1
00:00:00,000 --> 00:00:02,400
在上一个视频中 你们学习了如何定义

2
00:00:02,400 --> 00:00:04,980
神经风格转移(算法)的内容代价函数

3
00:00:04,980 --> 00:00:09,317
接下来 让我们来看看风格代价函数

4
00:00:09,317 --> 00:00:12,373
那么 对于图像而言风格是什么意思呢？

5
00:00:12,373 --> 00:00:14,633
比如说 你有一张这样的图像作为输入

6
00:00:14,633 --> 00:00:16,750
它们一般会通过这样的卷积网络

7
00:00:16,750 --> 00:00:20,091
计算不同隐藏层中的特征

8
00:00:20,091 --> 00:00:22,692
假设你选择了某一层L

9
00:00:22,692 --> 00:00:29,020
也许是这一层来衡量图像的风格

10
00:00:29,020 --> 00:00:34,095
我们要做的是将风格定义为层中

11
00:00:34,095 --> 00:00:40,635
不同激活通道之间的相关系数

12
00:00:40,635 --> 00:00:42,190
具体是这样的

13
00:00:42,190 --> 00:00:44,480
假设你选择了激活层L

14
00:00:44,480 --> 00:00:50,936
这是一个nh乘nw乘nc的激活阵

15
00:00:50,936 --> 00:00:55,995
然后我们想知道的是
不同的激活通道间的相关性有多大

16
00:00:55,995 --> 00:00:59,966
所以让我来解释一下这个有些隐晦的说法

17
00:00:59,966 --> 00:01:02,850
在这个激活阵上

18
00:01:02,850 --> 00:01:06,575
让我用不同的颜色表示不同的通道

19
00:01:06,575 --> 00:01:08,295
所以在这个例子中

20
00:01:08,295 --> 00:01:14,286
我们有5个通道 所以分别用5种颜色表示

21
00:01:14,286 --> 00:01:15,375
当然 在实践中

22
00:01:15,375 --> 00:01:18,514
在神经网络中 通道往往有远多于5个

23
00:01:18,514 --> 00:01:22,056
但用5个通道使得图示更为方便

24
00:01:22,056 --> 00:01:24,765
为了得到图像的风格

25
00:01:24,765 --> 00:01:26,850
你需要这样做

26
00:01:26,850 --> 00:01:28,910
让我们看一下前两个通道

27
00:01:28,910 --> 00:01:32,970
让我们看一下红色的和黄色的通道

28
00:01:32,970 --> 00:01:37,450
这两个激活通道的相关性有多大

29
00:01:37,450 --> 00:01:40,575
例如 在右下角

30
00:01:40,575 --> 00:01:45,820
你在第一个通道和第二个通道分别有一些激活元

31
00:01:45,820 --> 00:01:47,655
所以这会给你一个数组

32
00:01:47,655 --> 00:01:51,510
你需要做的是遍历这个激活块中不同的位置

33
00:01:51,510 --> 00:01:55,435
看这样一对一对的数

34
00:01:55,435 --> 00:01:57,232
其中一个在第一个通道 也就是红色的通道

35
00:01:57,232 --> 00:02:00,000
另一个在黄色的通道 也就是第二个通道

36
00:02:00,000 --> 00:02:02,370
然后你就看这些成对的数

37
00:02:02,370 --> 00:02:04,816
看看当你遍历所有这些位置

38
00:02:04,816 --> 00:02:07,320
即所有这些nh乘nw的位置时

39
00:02:07,320 --> 00:02:10,205
这些成对的数之间的相关性有多大

40
00:02:10,205 --> 00:02:12,550
所以 为什么这能表示风格呢？

41
00:02:12,550 --> 00:02:14,405
让我们看一个例子

42
00:02:14,405 --> 00:02:17,844
这是之前视频中的一个图示

43
00:02:17,844 --> 00:02:20,280
它来自于我之前提到的

44
00:02:20,280 --> 00:02:23,350
Matthew Zeiler和Rob Fergus的论文

45
00:02:23,350 --> 00:02:25,360
为了方便解释

46
00:02:25,360 --> 00:02:28,300
我们假设红色的神经元对应于

47
00:02:28,300 --> 00:02:30,170
为了方便解释

48
00:02:30,170 --> 00:02:36,600
我们假设红色的通道对应于这个

49
00:02:36,600 --> 00:02:40,320
用于分辨在图像中某些位置是否有

50
00:02:40,320 --> 00:02:46,185
竖条纹的神经元，我们再假设第二个通道

51
00:02:46,185 --> 00:02:51,795
这个第二个黄色的通道对应于这个

52
00:02:51,795 --> 00:02:56,515
分辨橙色色块的神经元

53
00:02:56,515 --> 00:03:01,104
那么这两个通道高度相关意味着什么呢？

54
00:03:01,104 --> 00:03:04,560
如果它们高度相关，那么这意味着图像中

55
00:03:04,560 --> 00:03:08,430
任何有这种微妙的竖条纹的部分

56
00:03:08,430 --> 00:03:12,980
那个部分也很有可能有这种橙色色调

57
00:03:12,980 --> 00:03:15,755
那么如果它们不相关呢？

58
00:03:15,755 --> 00:03:19,635
这意味着，任何有竖条纹的部分

59
00:03:19,635 --> 00:03:22,625
很有可能不会有橙色的色调

60
00:03:22,625 --> 00:03:25,710
所以相关性可以告诉你

61
00:03:25,710 --> 00:03:31,020
哪一些高层的纹理元素倾向于同时或不同时出现

62
00:03:31,020 --> 00:03:35,550
在图像中的某一部分，相关程度提供了一种

63
00:03:35,550 --> 00:03:40,455
衡量这些高层特征

64
00:03:40,455 --> 00:03:45,441
例如竖条纹或橙色调或其他特征

65
00:03:45,441 --> 00:03:48,180
是否经常出现，以及是否经常同时出现在

66
00:03:48,180 --> 00:03:51,740
图像中的某一部分的方式

67
00:03:51,740 --> 00:03:57,180
所以，如果我们使用通道间的相关性作为量化风格的方式

68
00:03:57,180 --> 00:04:02,670
那么你可以做的是在生成的图像中测量

69
00:04:02,670 --> 00:04:06,810
第一个通道与第二个通道的相关程度

70
00:04:06,810 --> 00:04:12,090
这可以告诉你在生成的图像中

71
00:04:12,090 --> 00:04:14,820
这种竖条纹是否经常和这种橙色调同时出现

72
00:04:14,820 --> 00:04:18,450
然后这会告诉你生成的图像的风格

73
00:04:18,450 --> 00:04:25,675
和输入图像的风格有多相似

74
00:04:25,675 --> 00:04:28,600
现在，让我们用更数学的语言来表达刚才直观的描述

75
00:04:28,600 --> 00:04:34,620
所以，你要做的是通过给定的图像计算出风格矩阵

76
00:04:34,620 --> 00:04:38,960
风格矩阵会记录上一张幻灯片中我们提到的所有相关系数

77
00:04:38,960 --> 00:04:44,280
所以，更正式地，让我们用a 上标l 下标i j k

78
00:04:44,280 --> 00:04:47,868
来表示在隐藏层l中位于i j k的激活元

79
00:04:47,868 --> 00:04:53,610
所以指标i表示高度

80
00:04:53,610 --> 00:04:54,850
指标j表示宽度

81
00:04:54,850 --> 00:04:58,050
指标k表示不同的通道

82
00:04:58,050 --> 00:05:00,045
所以在之前的幻灯片中

83
00:05:00,045 --> 00:05:05,165
我们有5个通道，那么指标k将会遍历这5个通道

84
00:05:05,165 --> 00:05:09,635
所以，你要做的是计算一个风格矩阵G上标l

85
00:05:09,635 --> 00:05:17,390
这会是一个nc乘nc的矩阵

86
00:05:17,390 --> 00:05:18,755
也就是一个方阵

87
00:05:18,755 --> 00:05:23,390
因为你有nc个通道，所以你需要一个

88
00:05:23,390 --> 00:05:29,490
nc乘nc大小的矩阵来记录每一对通道间的相关性

89
00:05:29,490 --> 00:05:32,585
所以G 上标l 下标k k'

90
00:05:32,585 --> 00:05:36,954
将会表示通道k和通道k'

91
00:05:36,954 --> 00:05:41,755
的激活元之间的相关性

92
00:05:41,755 --> 00:05:46,250
这里 k和k'会遍历1到nc

93
00:05:46,250 --> 00:05:49,630
即那一层中总共的通道数

94
00:05:49,630 --> 00:05:55,820
所以，更正式地，你会这样计算G 上标l

95
00:05:55,820 --> 00:06:00,840
我只写一下计算某一个元素的公式

96
00:06:00,840 --> 00:06:03,283
即其中第k行第k'列的元素

97
00:06:03,283 --> 00:06:06,210
它会是l层中

98
00:06:06,210 --> 00:06:08,987
位于i j k的激活元

99
00:06:08,987 --> 00:06:13,979
和位于i j k'的激活元的乘积

100
00:06:13,979 --> 00:06:22,078
遍历所有可能的i j所得到的和

101
00:06:22,078 --> 00:06:27,989
这里要记住，指标i j遍历了这整块中不同的位置

102
00:06:27,989 --> 00:06:30,453
即遍历了高度和宽度

103
00:06:30,453 --> 00:06:39,755
所以指标i要从1加到nh 指标j要从1加到nw

104
00:06:39,755 --> 00:06:45,200
还有，这里的指标k和k'要遍历不同的通道

105
00:06:45,200 --> 00:06:47,870
所以k和k'是从1到

106
00:06:47,870 --> 00:06:51,913
这个神经网络的第l层的总通道数

107
00:06:51,913 --> 00:06:55,967
所以，所要做的就是

108
00:06:55,967 --> 00:07:00,225
遍历图像中各个高度和宽度的位置

109
00:07:00,225 --> 00:07:03,640
将第k个和第k'个通道上的激活元相乘再求和

110
00:07:03,640 --> 00:07:08,853
这就是G下标k k'的定义

111
00:07:08,853 --> 00:07:14,450
然后对每一个k和k'都做同样的运算，来得到这个矩阵G

112
00:07:14,450 --> 00:07:17,585
即风格矩阵

113
00:07:17,585 --> 00:07:23,435
值得注意的是，如果这些对应激活元都很大

114
00:07:23,435 --> 00:07:26,325
那么G下标k k'也会很大

115
00:07:26,325 --> 00:07:28,510
而如果它们不相关，那么G下标k k'

116
00:07:28,510 --> 00:07:30,305
可能会较小

117
00:07:30,305 --> 00:07:32,060
从技术上来说，我一直在用

118
00:07:32,060 --> 00:07:36,170
相关系数这个术语来直观地表述，但实际上

119
00:07:36,170 --> 00:07:40,130
所计算的是未标准化的互协方差，因为我们没有

120
00:07:40,130 --> 00:07:46,130
将平均值减去，只是直接将对应的元素相乘

121
00:07:46,130 --> 00:07:50,370
所以，上述就是如何计算图像的风格

122
00:07:50,370 --> 00:07:54,030
实际上，你要对风格图像s和

123
00:07:54,030 --> 00:08:01,020
生成图像g都做同样的运算，所以
为了区分这是风格图像(的风格矩阵)

124
00:08:01,020 --> 00:08:07,630
让我在这里加一个圆括号S

125
00:08:07,630 --> 00:08:10,105
来表示这是风格图像S(的风格矩阵)

126
00:08:10,105 --> 00:08:12,715
以及这些是图像S上的激活元

127
00:08:12,715 --> 00:08:21,085
然后你要做的是对生成图像做同样的计算

128
00:08:21,085 --> 00:08:28,581
所以，就是一样的，对i j遍历求和

129
00:08:28,581 --> 00:08:32,670
a下标i j k上标l a下标i j k'上标l

130
00:08:32,670 --> 00:08:36,678
然后求和指标是和上面一样的

131
00:08:36,678 --> 00:08:46,130
最后你需要表示这个是生成图像的

132
00:08:46,130 --> 00:08:51,710
我就在这里加上圆括号G

133
00:08:51,710 --> 00:08:55,540
所以，现在，你有两个矩阵分别代表着

134
00:08:55,540 --> 00:08:59,770
图像S和图像G的风格

135
00:08:59,770 --> 00:09:05,260
顺便提一下，我一直用大写字母G来表示这些矩阵

136
00:09:05,260 --> 00:09:09,445
因为在线性代数中，这些矩阵也被称之为

137
00:09:09,445 --> 00:09:14,030
格拉姆矩阵，虽然在这个视频中

138
00:09:14,030 --> 00:09:17,680
我称它们为风格矩阵，但格拉姆矩阵的说法

139
00:09:17,680 --> 00:09:23,630
促使我们用大写的G来表示这些矩阵

140
00:09:23,630 --> 00:09:26,035
最后，代价函数

141
00:09:26,035 --> 00:09:28,875
即风格代价函数

142
00:09:28,875 --> 00:09:34,570
如果你用图像S和图像G的第l层(来计算)

143
00:09:34,570 --> 00:09:37,050
现在，你可以将其定义为

144
00:09:37,050 --> 00:09:44,610
这两个矩阵

145
00:09:44,610 --> 00:09:48,675
G上标l S和G上标l G

146
00:09:48,675 --> 00:09:54,265
的差的平方，这两个就是

147
00:09:54,265 --> 00:09:55,754
上面提到的两个矩阵

148
00:09:55,754 --> 00:10:00,660
这就是这两个矩阵对应元素的差的平方和

149
00:10:00,660 --> 00:10:07,065
把它写出来就是

150
00:10:07,065 --> 00:10:12,964
G上标l S下标k k'减去G上标l G下标k k'

151
00:10:12,964 --> 00:10:17,450
的差的平方

152
00:10:17,450 --> 00:10:24,530
遍历所有的k和k' 对所有的元素求和

153
00:10:24,530 --> 00:10:32,715
论文的作者还使用了这样的标准化系数 2乘以nh

154
00:10:32,715 --> 00:10:34,890
乘以这一层的nw

155
00:10:34,890 --> 00:10:40,015
乘以这一层的nc 然后平方一下，这里也要加上

156
00:10:40,015 --> 00:10:43,600
不过这个标准化常数并没有太大的影响

157
00:10:43,600 --> 00:10:47,485
因为这一项代价函数反正都需要乘以超参数b

158
00:10:47,485 --> 00:10:48,910
所以，总结一下

159
00:10:48,910 --> 00:10:51,970
这是通过l层定义的风格代价函数

160
00:10:51,970 --> 00:10:55,645
正如你在之前的幻灯片中看到的

161
00:10:55,645 --> 00:11:02,440
这基本上是基于图像S和图像G计算出的两个风格矩阵

162
00:11:02,440 --> 00:11:05,953
之间的弗罗贝尼乌斯范数(Frobenius norm)

163
00:11:05,953 --> 00:11:10,810
的平方，再额外乘以一个

164
00:11:10,810 --> 00:11:13,255
不那么重要的标准化常数

165
00:11:13,255 --> 00:11:18,400
最后，如果你用几个不同的层来计算风格代价矩阵

166
00:11:18,400 --> 00:11:23,443
你会得到看上去更好的结果

167
00:11:23,443 --> 00:11:27,095
所以，总的风格代价函数

168
00:11:27,095 --> 00:11:31,305
可以被定义为

169
00:11:31,305 --> 00:11:37,640
所有不同层的风格代价函数的总和

170
00:11:37,640 --> 00:11:41,820
我们需要使用一组参数，一组额外的超参数

171
00:11:41,820 --> 00:11:44,160
给上述公式加入权重

172
00:11:44,160 --> 00:11:46,895
也就是这里的lambda上标l所表示的

173
00:11:46,895 --> 00:11:51,595
这里所做的是，允许你用一个神经网络中不同的层

174
00:11:51,595 --> 00:11:52,815
包括那些早期的层

175
00:11:52,815 --> 00:11:55,800
用来捕捉相对简单的低层特征

176
00:11:55,800 --> 00:11:59,050
比如边界的，以及一些后期的

177
00:11:59,050 --> 00:12:03,000
用来捕捉高层特征的层，这样可以使得

178
00:12:03,000 --> 00:12:08,475
在计算风格的时候，将神经网络中低层和高层的相关性都考虑到

179
00:12:08,475 --> 00:12:10,845
在编程练习中

180
00:12:10,845 --> 00:12:13,980
你会对如何合理选择这种参数lambda

181
00:12:13,980 --> 00:12:19,080
有更多的感觉

182
00:12:19,080 --> 00:12:20,790
所以，最后总结一下

183
00:12:20,790 --> 00:12:24,660
你现在可以定义总的代价函数为

184
00:12:24,660 --> 00:12:30,720
alpha乘以C和G之间的内容代价函数加上

185
00:12:30,720 --> 00:12:37,515
beta乘以S和G之间的风格代价函数，然后用梯度下降法

186
00:12:37,515 --> 00:12:40,785
或者，如果你愿意的话，采用更复杂的优化算法

187
00:12:40,785 --> 00:12:44,696
来试图找到一个图像G

188
00:12:44,696 --> 00:12:49,590
使得代价函数J(G)尽可能地被最小化，如果你这么做了

189
00:12:49,590 --> 00:12:53,730
你可以生成相当不错的基于神经网络的艺术作品

190
00:12:53,730 --> 00:12:59,220
如果你这么做了，你可以得到一些非常不错的独特的艺术品

191
00:12:59,220 --> 00:13:02,010
所以关于神经风格转移，我们就讲到这里

192
00:13:02,010 --> 00:13:05,235
我希望你们在这周的编程练习中编程愉快

193
00:13:05,235 --> 00:13:06,625
在结束这周(的课)之前

194
00:13:06,625 --> 00:13:08,575
我还想和你们分享一件事

195
00:13:08,575 --> 00:13:11,100
那就是，如何在一维或三维的数据

196
00:13:11,100 --> 00:13:17,000
而不是二维的图像上实现卷积，让我们进入最后一个视频