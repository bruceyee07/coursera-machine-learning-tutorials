前回のビデオでは ニューラル スタイル変換の 内容コスト関数の定義を見た 次に スタイルコスト関数を見てみよう 画像のスタイルとは 何を意味するのか？ このような画像があるとする それに このような ConvNet があり 色んな層で 特徴量を計算する ある層 l を選んでみよう この層かな ここで 画像スタイルの測り方を定義する 定義するのは この層 l においての 異なるチャンネルに跨った 活性間の相関だ どういうことか 説明する 層 l の活性を取り出そう その活性は nH x nW x nC のブロックになる 異なるチャンネルに跨った 活性間には どのくらい相関があるだろうか？ この謎めいた言葉の意味するものを説明するために この活性のブロックを取り上げ 異なるチャンネルに 異なる色を付けよう この下の例では ５つのチャンネルがあるので ５つの色を付ける 実際には もちろん 通常のニューラルネットワークでは ５個より もっと多いチャンネルを持つ でも 描くのを簡単にするため ５個にする 画像のスタイルを把握するためには 次のようにする 最初の２つのチャンネルを見よう 赤と黄色のチャンネルを見よう この最初の２つのチャンネルの活性は どのくらい相関しているか？ 例えば この右下角では 最初のチャンネルの ある活性値と ２番目のチャンネルの ある活性値がある つまり 数のペアがある この活性ブロックに跨る 異なる場所それぞれで この数のペアを見る 最初のチャンネル 赤いチャンネルのと 黄色いチャンネル ２番目のチャンネルのだ これらの数のペアを 全ての場所で見て 全ての nH x nW 位置で見て この２つの数がどのくらい相関しているのか調べる なぜ これで スタイルが把握できるんだろう？ 別の例を見よう 前のビデオで使った可視化の例だ これは またもや 前にも触れた Matthew Zeiler そして Rob Fergus の論文からだ 議論のために こうしよう 赤いニューロンは 議論のために こうしよう 赤いチャンネルは このニューロンに対応していて この小さな垂直の質感を探し出そうとしている 画像の とある場所でね そして ２番目のチャンネルは この黄色い２番目のチャンネルは このニューロンに対応していて ぼんやりと オレンジ色の小領域を探している この２つのチャンネルが高く相関していると どういうことを意味するか？ もし それらが 高く相関していれば このような 微妙な縦の質感を持っている画像の部分は 多分 オレンジの色合いを 持っているだろうね それらが 相関していない場合は 何を意味するのか？ この縦の質感がある場所は 多分 このオレンジの色合いでは無いだろうね つまり 相関は これらの高レベルの質感が 同時に起こるかどうか その傾向を表す 画像の部分部分でね そして 相関度は これらの 異なる高レベル特徴が 一緒に起きる頻度を測る方法になる 垂直の質感や このオレンジの色合いとか 他のもののね それらが どのくらい起きるか どのくらい一緒に起きるか そして どのくらい一緒に起きないか 画像の色んな場所でね そして スタイルを測るものとして チャンネル間の相関度を使うなら 生成画像においても 相関度を測る この最初のチャンネルが この２番目のチャンネルと どのくらい 相関しているか していないかを測る
それで 生成画像において このような縦の質感が このオレンジの色合いと どのくらい頻繁に 一緒に 起こるのか 起こらないのか 分かる これにより 生成画像のスタイルと 入力したスタイル画像のスタイルが どのくらい似ているか測れる この考察を 定式化しよう ある画像を元にして スタイル行列と呼ばれるものを計算する それは 前のスライドで話した 全ての相関を測るものだ a[l] i,j,k は 位置 i, j, k における活性を表す 隠れ層 l でのね そして i は 高さ j は 幅 k は 各チャンネルを表す 前のスライドでは ５つのチャンネルがあったから k は ５チャンネルに跨るインデックスだ スタイル行列は こういうものだ まず G[l] で表される行列を計算する これは nC x nC 行列になる つまり 正方行列だ nC 個のチャンネルがあるわけで nC x nC 次元の行列は それらのペアがどれくらい相関しているか測るものだ 例えば G[l]kk' は チャンネル k の活性と チャンネル k' の活性が どのくらい相関しているか測るものだ ここで k と k' は １~ nC までの値を取る その層での チャンネル数だ もっと正式に言うと G[l] を計算する方法は １要素を計算する式だけを書くと これの k k' 要素だ これは i の合計 j の合計 その層の活性 i, j ,k 掛ける 活性 i,j,k' ここで i と j は ブロックの異なる場所に跨るインデックスだ 高さと幅のインデックスだ よって i は １~ nH で j は １~ nW だ そして k と k' は チャンネルのインデックスだ よって k と k' の範囲は １から その層のチャンネル数までだ これは 画像の異なる場所を 高さと幅に跨って 合計している そして チャンネル k と k' の活性を掛け算している これが Gkk' の定義だ これを 全ての k と k' について行うと この行列 G を得る これは スタイル行列とも呼ばれる この２つの活性が一緒に高くなる傾向があれば Gkk' は大きくなる 一方 それらに相関が無ければ Gkk' は 小さくなる 洞察を伝えるために 相関 という用語を使ったが 正確に言うと 正規化されていない 相互共分散 だ なぜなら 平均値を引いておらず 単に 要素同士を直接掛け算しただけだからだ これが 画像のスタイルを計算する方法だ そして これを スタイル画像 S と 生成画像 G に行う 区別したいので これは スタイル画像のだとしよう ここに (S) を加えよう これが スタイル画像 S のだという印だ そして これらが 画像 S の活性だ それから 同じことを 生成画像に対しても行う 全く同じに 合計する a[l]ijk a[l]ijk (訳注：k に ' を付け忘れていると思われる) 合計するインデックスは同じ そして これが 生成画像のだと示すため ここに (G) を付ける ここに ２つの行列を得た 画像 S のスタイル と 画像 G のスタイルを 取り出すものだ ところで これらの行列を表すのに 大文字の G を使ってきたけど 線形行列では これは グラム行列と呼ばれる しかし このビデオでは スタイル行列という用語を使っている グラム行列という用語なので この行列を表すのに 大文字の G を使っている 最後に コスト関数 スタイルコスト関数 この層で S と G のを計算するなら 今度は こう定義できる 違いとして定義できる これらの 行列の違いとして ...G[l](G) の２乗 これは行列だ よって フロベニウス ノルムにしよう これは 要素毎の違いの２乗の合計だ この２つの行列のね それから こう書こう k の合計 k' の合計することの この違い G[l](S)kk' - G[l](G)kk' そして 要素の２乗の合計 著者らは 正規化の常数として これを使った 2 x nH x nW その層のだ その層の nC そして ２乗する 上にも これを置く ただし 正規化定数は そんなに重要じゃない なぜなら このコストは どうせハイパーパラメータ b が掛けられるから じゃ 仕上げよう これが 層 l を使って定義した スタイルコスト関数だ そして 前のスライドで見たように これは 基本的には ２つのスタイル行列の フロベニウス ノルム 画像 S と 画像 G で計算したものだ フロベニウス ノルムの２乗だ 正規化定数は置かないけど 重要じゃないから そして 最後に より視覚的に望ましい結果を得るには 複数の異なる層の スタイルコスト関数を使うといい つまり 全ての スタイルコスト関数は こう定義できる 全ての異なる層のスタイルコスト関数の合計 パラメータで 重みも 入れるべきだ 追加のハイパーパラメータのセットだ ここでは λ[l] で示そう これで ニューラルネットワークの異なる層が使える 初めの方では 相対的に単純な低レベルの特徴 エッジ等を測り 同様に 後の方の層では 高レベルの特徴を測る そして ニューラルネットワークが スタイルを計算する際 低レベルの相関も 高レベルのも 考慮に入れる プログラミング演習では もっと多くの洞察を得られるよ ハイパーパラメータ λ の適切な選び方等についてね まとめよう 今は 全コスト関数を定義できるよね α x 内容関数 それは C G 間ので それに 足すことの β x スタイルコスト S G 間のだ そして 勾配降下法を使って もしくは やりたいなら もっと洗練されたアルゴリズムをを使って 画像 G を見つけるためにね それで これを最小化する コスト関数 J(G) を最小化する そうすると とてもいい感じの ニューラル芸術 を生成できる そうしたら 何か とても良い 今までにない 芸術作品を 生み出せるだろう 以上が ニューラル スタイル変換だ 今週のプログラミング演習の実装が 楽しいといいね 今週をまとめる前に １つだけ共有しておきたいことがある それは 2D画像だけでなく 1Dや3Dデータに どのように畳み込みを行うかだ 最後のビデオに進もう