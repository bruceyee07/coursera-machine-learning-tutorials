1
00:00:00,000 --> 00:00:02,400
在上部影片，我們學到如何定義

2
00:00:02,400 --> 00:00:04,980
神經風格轉換的「內容成本」

3
00:00:04,980 --> 00:00:09,317
接下來，讓我們看看「風格成本」

4
00:00:09,317 --> 00:00:12,373
那麼，一張圖片的「風格」是什麼意思？

5
00:00:12,373 --> 00:00:14,633
假設你有像這樣的輸入圖片

6
00:00:14,633 --> 00:00:16,750
這種 ConvNet 架構你應該不陌生

7
00:00:16,750 --> 00:00:20,091
在各個不同的層都會算出特徵

8
00:00:20,091 --> 00:00:22,692
假設你選擇了某層 l

9
00:00:22,692 --> 00:00:29,020
— 例如這一層，來定義如何衡量一張圖片的風格

10
00:00:29,020 --> 00:00:34,095
我們要做的是，將「風格」定義成

11
00:00:34,095 --> 00:00:40,635
在第 l 層各個通道的啟動值之間的相關度

12
00:00:40,635 --> 00:00:42,190
我的意思是這樣的

13
00:00:42,190 --> 00:00:44,480
假設你取出第 l 層的啟動值

14
00:00:44,480 --> 00:00:50,936
所以這會是一塊 n_h乘n_w乘n_c 的啟動值

15
00:00:50,936 --> 00:00:55,995
我們會問：橫跨各個不同通道，他們的啟動值有多相關？

16
00:00:55,995 --> 00:00:59,966
這說的有點天書，要解釋的話

17
00:00:59,966 --> 00:01:02,850
讓我們拿這一塊啟動值

18
00:01:02,850 --> 00:01:06,575
把不同通道塗上不同的顏色

19
00:01:06,575 --> 00:01:08,295
所以在這例子

20
00:01:08,295 --> 00:01:14,286
假設我們有 5 個通道，所以我畫了五種顏色

21
00:01:14,286 --> 00:01:15,375
當然實務上

22
00:01:15,375 --> 00:01:18,514
一個神經網路的通道常常會比五個還多

23
00:01:18,514 --> 00:01:22,056
不過這邊舉例五個只是為了畫圖簡單。

24
00:01:22,056 --> 00:01:24,765
而為了要獲得一張圖片的風格

25
00:01:24,765 --> 00:01:26,850
你會這樣子做：

26
00:01:26,850 --> 00:01:28,910
讓我們看看頭兩個通道

27
00:01:28,910 --> 00:01:32,970
讓我們看紅色的和黃色的通道，

28
00:01:32,970 --> 00:01:37,450
看看這兩個通道的啟動值，兩者多相關

29
00:01:37,450 --> 00:01:40,575
例如在最右下角

30
00:01:40,575 --> 00:01:45,820
第一個通道會有某個啟動值，第二個通道有某個啟動值

31
00:01:45,820 --> 00:01:47,655
所以這給了你一對數字

32
00:01:47,655 --> 00:01:51,510
你要做的是看過不同的位置

33
00:01:51,510 --> 00:01:55,435
在這堆啟動值只看這兩組數字

34
00:01:55,435 --> 00:01:57,232
一組在第一個通道內、紅色的

35
00:01:57,232 --> 00:02:00,000
另一組是黃色的通道、第二個的

36
00:02:00,000 --> 00:02:02,370
你觀察這兩對的數字們

37
00:02:02,370 --> 00:02:04,816
當你看過所有這些位置

38
00:02:04,816 --> 00:02:07,320
n_h乘n_w 個位置

39
00:02:07,320 --> 00:02:10,205
這兩組數字會多相關？

40
00:02:10,205 --> 00:02:12,550
那麼，為何這樣能捕捉出「風格」呢？

41
00:02:12,550 --> 00:02:14,405
讓我們看個例子

42
00:02:14,405 --> 00:02:17,844
這邊是之前影片的一個視覺化例子

43
00:02:17,844 --> 00:02:20,280
這邊是從之前講過的

44
00:02:20,280 --> 00:02:23,350
Matthew Zeiler 和 Rob Fergus 的論文來的

45
00:02:23,350 --> 00:02:25,360
為了討論方便，假設

46
00:02:25,360 --> 00:02:28,300
這個紅色的神經元對應到...

47
00:02:28,300 --> 00:02:30,170
為了討論方便，假設

48
00:02:30,170 --> 00:02:36,600
那個紅色的通道對應到這一個神經元 — 我們想知道

49
00:02:36,600 --> 00:02:40,320
在圖片中某個地方

50
00:02:40,320 --> 00:02:46,185
有沒有這種小小垂直的花樣。再假設第二個通道

51
00:02:46,185 --> 00:02:51,795
這個黃色的通道會對應這個神經元

52
00:02:51,795 --> 00:02:56,515
也就是大約去找出橘色系的圖塊。

53
00:02:56,515 --> 00:03:01,104
那麼，如果兩個通道很相關的話，這代表什麼意思？

54
00:03:01,104 --> 00:03:04,560
如果兩者很相關，表示無論在圖片的哪個區域

55
00:03:04,560 --> 00:03:08,430
含有這種細微垂直的花樣的時候，

56
00:03:08,430 --> 00:03:12,980
那一個區域也很可能有這種橘色色調

57
00:03:12,980 --> 00:03:15,755
那如果兩者不相關是什麼意思？

58
00:03:15,755 --> 00:03:19,635
這代表當觀察到這種垂直的花樣

59
00:03:19,635 --> 00:03:22,625
可能並不一定有橘色的色調

60
00:03:22,625 --> 00:03:25,710
因此，相關度可以告訴你

61
00:03:25,710 --> 00:03:31,020
這些抽象的花樣材質在圖片的某處

62
00:03:31,020 --> 00:03:35,550
會傾向於一起出現還是不會。
而相關度的大小

63
00:03:35,550 --> 00:03:40,455
可以衡量這些不同的高層次的特徵

64
00:03:40,455 --> 00:03:45,441
— 例如垂直的花樣、這種橘色色調、諸如此類的

65
00:03:45,441 --> 00:03:48,180
在圖片的各處他們多常會出現？

66
00:03:48,180 --> 00:03:51,740
會多常一起出現或不常一起出現？

67
00:03:51,740 --> 00:03:57,180
因此，如果我們用「通道間的相關度」作為一種風格的度量標準

68
00:03:57,180 --> 00:04:02,670
那麼，我們可以在生成的圖片中，去衡量

69
00:04:02,670 --> 00:04:06,810
這邊第一個通道有沒有跟

70
00:04:06,810 --> 00:04:12,090
第二個通道相關，這樣會告訴你在生成圖片中

71
00:04:12,090 --> 00:04:14,820
這種垂直花樣常不常

72
00:04:14,820 --> 00:04:18,450
和橘色色調一起出現 — 這給了你一種衡量方法

73
00:04:18,450 --> 00:04:25,675
看看生成圖片的風格，跟輸入風格圖片的風格，兩者有多相似

74
00:04:25,675 --> 00:04:28,600
所以，讓我們把這概念寫成公式

75
00:04:28,600 --> 00:04:34,620
給定一張圖片，你可以計算某個叫「風格矩陣」的東西

76
00:04:34,620 --> 00:04:38,960
他會衡量出上一張投影片談到的那些相關度

77
00:04:38,960 --> 00:04:44,280
更正式地說，讓 a 上標 [l] 下標 i,j,k

78
00:04:44,280 --> 00:04:47,868
表示在第 l 層、位置 (i, j, k) 的啟動值

79
00:04:47,868 --> 00:04:53,610
所以 i 是高度方向的索引

80
00:04:53,610 --> 00:04:54,850
j 是寬度方向的索引

81
00:04:54,850 --> 00:04:58,050
而 k 橫跨了不同的通道

82
00:04:58,050 --> 00:05:00,045
在前一張投影片

83
00:05:00,045 --> 00:05:05,165
我們有 5 個通道，所以 k 會那五個通道的索引

84
00:05:05,165 --> 00:05:09,635
那麼「風格矩陣」所做的是，你會計算一個矩陣

85
00:05:09,635 --> 00:05:17,390
叫他 G^[l]，這個矩陣維度會是 n_c乘n_c

86
00:05:17,390 --> 00:05:18,755
所以是正方的矩陣

87
00:05:18,755 --> 00:05:23,390
還記得你有 n_c 個通道，所以你會有

88
00:05:23,390 --> 00:05:29,490
n_c乘n_c 維度的矩陣，是為了要衡量任意兩個之間的相關度

89
00:05:29,490 --> 00:05:32,585
也就是說

90
00:05:32,585 --> 00:05:36,954
G^[l]_k k' 會度量通道 k 和通道 k'

91
00:05:36,954 --> 00:05:41,755
的啟動值，兩者之間有多相關

92
00:05:41,755 --> 00:05:46,250
其中 k 和 k' 會從 1 到 n_c

93
00:05:46,250 --> 00:05:49,630
n_c 是那一層的通道的數量

94
00:05:49,630 --> 00:05:55,820
更正式地說，要計算 G^[l]...

95
00:05:55,820 --> 00:06:00,840
— 讓我先寫出計算一個元素的公式

96
00:06:00,840 --> 00:06:03,283
也就是這個的下標 k,k' 元素

97
00:06:03,283 --> 00:06:06,210
這會是對 i 做加總

98
00:06:06,210 --> 00:06:08,987
對 j 做加總

99
00:06:08,987 --> 00:06:13,979
裡面是該層在 i, j, k 的啟動值

100
00:06:13,979 --> 00:06:22,078
乘上在 i, j, k' 的啟動值

101
00:06:22,078 --> 00:06:27,989
這邊請記得，i 和 j 是那一大塊啟動值的索引

102
00:06:27,989 --> 00:06:30,453
在高度和寬度上的索引

103
00:06:30,453 --> 00:06:39,755
所以 i 會從 1 加到 n_h，而 j 從 1 加到 n_w

104
00:06:39,755 --> 00:06:45,200
而這邊的 k 和 k' 是通道的索引

105
00:06:45,200 --> 00:06:47,870
所以 k 和 k' 範圍是 1 到

106
00:06:47,870 --> 00:06:51,913
網路該層的通道數量

107
00:06:51,913 --> 00:06:55,967
所以這一整件事在做的

108
00:06:55,967 --> 00:07:00,225
是對圖片高度和寬度方向的每一個位置加起來

109
00:07:00,225 --> 00:07:03,640
把通道 k 和 通道 k' 的啟動值的乘積

110
00:07:03,640 --> 00:07:08,853
加起來，這個就是 G_k,k' 的定義

111
00:07:08,853 --> 00:07:14,450
然後你對每個 k 和 k' 可能的值都這樣做，就計算出矩陣 G

112
00:07:14,450 --> 00:07:17,585
也就是「風格矩陣」

113
00:07:17,585 --> 00:07:23,435
注意一下，如果這兩個啟動值傾向於同時變大

114
00:07:23,435 --> 00:07:26,325
那 G_k,k' 就會很大

115
00:07:26,325 --> 00:07:28,510
而如果兩者無相關度

116
00:07:28,510 --> 00:07:30,305
那 G_k,k' 就會小

117
00:07:30,305 --> 00:07:32,060
技術上來說，我一直在用

118
00:07:32,060 --> 00:07:36,170
「相關度」這個詞來教你們概念，不過

119
00:07:36,170 --> 00:07:40,130
這其實是未正規化的交叉共變異數 (cross-covariance)

120
00:07:40,130 --> 00:07:46,130
因為沒有減去平均，這也單純是元素直接相乘

121
00:07:46,130 --> 00:07:50,370
那麼，這就是計算一張圖片「風格」的方法

122
00:07:50,370 --> 00:07:54,030
你其實會對風格圖片 S 和生成圖片 G

123
00:07:54,030 --> 00:08:01,020
都做這樣子做。只是為了區別，讓這個是風格圖片

124
00:08:01,020 --> 00:08:07,630
或許讓我在上面加個括號 (S)

125
00:08:07,630 --> 00:08:10,105
只是為了表示這是圖片 S 的風格矩陣

126
00:08:10,105 --> 00:08:12,715
而那些是圖片 S 的啟動值

127
00:08:12,715 --> 00:08:21,085
那接下來，你也對生成圖片計算同樣的東西

128
00:08:21,085 --> 00:08:28,581
所以這其實是同樣的東西... 加過 i 加過 j

129
00:08:28,581 --> 00:08:32,670
a_i,j,k^[l]

130
00:08:32,670 --> 00:08:36,678
加總的索引也一樣

131
00:08:36,678 --> 00:08:46,130
像這樣... 然後你要表示這屬於生成圖片

132
00:08:46,130 --> 00:08:51,710
我就放個括號 (G)

133
00:08:51,710 --> 00:08:55,540
因此，現在你有兩個矩陣捕捉了圖片 S 的風格

134
00:08:55,540 --> 00:08:59,770
和圖片 G 的風格

135
00:08:59,770 --> 00:09:05,260
題外話，我們一直用大寫 G 來表示這些矩陣

136
00:09:05,260 --> 00:09:09,445
在線性代數中，這些也叫

137
00:09:09,445 --> 00:09:14,030
"Gram matrix" (格拉姆矩陣)，不過在這部影片

138
00:09:14,030 --> 00:09:17,680
我就用「風格矩陣」這個詞。但是是因為

139
00:09:17,680 --> 00:09:23,630
Gram矩陣這個術語，使得我們用大寫 G 來表示這些矩陣

140
00:09:23,630 --> 00:09:26,035
最後呢，成本函數

141
00:09:26,035 --> 00:09:28,875
這個風格成本函數

142
00:09:28,875 --> 00:09:34,570
如果做在第 l 層，比較 S 和 G

143
00:09:34,570 --> 00:09:37,050
你可定義這個為

144
00:09:37,050 --> 00:09:44,610
定義為某種相差

145
00:09:44,610 --> 00:09:48,675
這兩個矩陣的相差

146
00:09:48,675 --> 00:09:54,265
... G^[l](G) 然後平方。因為是矩陣

147
00:09:54,265 --> 00:09:55,754
所以我取 Frobenius norm

148
00:09:55,754 --> 00:10:00,660
這單純是兩個矩陣的逐元素差距的平方和

149
00:10:00,660 --> 00:10:07,065
把他展開寫下來，這會是對 k 加總、

150
00:10:07,065 --> 00:10:12,964
對 k' 加總，把這些差距加起來

151
00:10:12,964 --> 00:10:17,450
... k,k' 減 G^[l](G)_k,k'

152
00:10:17,450 --> 00:10:24,530
然後取平方加起來

153
00:10:24,530 --> 00:10:32,715
作者其實用了這個標準化的常數：2 乘以 n_h

154
00:10:32,715 --> 00:10:34,890
n_w 在該層上

155
00:10:34,890 --> 00:10:40,015
還有該層的 n_c 然後平方。在上面也放這個

156
00:10:40,015 --> 00:10:43,600
不過標準化常數不大重要

157
00:10:43,600 --> 00:10:47,485
反正成本會乘上某個超參數 b

158
00:10:47,485 --> 00:10:48,910
最後

159
00:10:48,910 --> 00:10:51,970
這是定義在第 l 層的

160
00:10:51,970 --> 00:10:55,645
風格成本函數。如前張投影片說的

161
00:10:55,645 --> 00:11:02,440
這基本上是兩個風格矩陣之間的 Frobenius norm
(弗比尼斯範數)

162
00:11:02,440 --> 00:11:05,953
在圖片 S 和圖片 G 上的

163
00:11:05,953 --> 00:11:10,810
... 取範數取平方，然後會有個標準化的常數

164
00:11:10,810 --> 00:11:13,255
不過不是太重要

165
00:11:13,255 --> 00:11:18,400
最後呢，其實為了讓結果在視覺上效果好，

166
00:11:18,400 --> 00:11:23,443
你可以用不只一層的風格成本

167
00:11:23,443 --> 00:11:27,095
因此，整個風格成本函數

168
00:11:27,095 --> 00:11:31,305
你可以定義成

169
00:11:31,305 --> 00:11:37,640
把每一層的風格成本函數加起來

170
00:11:37,640 --> 00:11:41,820
— 我們應該要定義一些參數做加權

171
00:11:41,820 --> 00:11:44,160
有這群額外的超參數

172
00:11:44,160 --> 00:11:46,895
我在這裡定義乘 lambda^[l]

173
00:11:46,895 --> 00:11:51,595
所以這可以讓你利用網路不同的層

174
00:11:51,595 --> 00:11:52,815
初期的層

175
00:11:52,815 --> 00:11:55,800
會衡量比較簡單、低階的特徵，

176
00:11:55,800 --> 00:11:59,050
例如邊線。也可以用後面的層

177
00:11:59,050 --> 00:12:03,000
去衡量高階的特徵。這讓神經網路

178
00:12:03,000 --> 00:12:08,475
計算風格的時候可以同時考量低階和高階的相關度

179
00:12:08,475 --> 00:12:10,845
而在程式作業當中

180
00:12:10,845 --> 00:12:13,980
你會獲得更多的感覺

181
00:12:13,980 --> 00:12:19,080
知道超參數 lambda 合理的選擇是多少

182
00:12:19,080 --> 00:12:20,790
所以總結一下

183
00:12:20,790 --> 00:12:24,660
現在你可以定義一整個成本函數

184
00:12:24,660 --> 00:12:30,720
等於 alpha 乘上 C 和 G 之間的「內容成本」

185
00:12:30,720 --> 00:12:37,515
加上 beta 乘上 S 和 G 之間的「風格成本」，
然後用梯度下降法

186
00:12:37,515 --> 00:12:40,785
或者用更複雜的最佳化演算法

187
00:12:40,785 --> 00:12:44,696
來嘗試找到一張圖片 G

188
00:12:44,696 --> 00:12:49,590
讓成本函數 J(G) 變最小。如此一來

189
00:12:49,590 --> 00:12:53,730
你就能產生很好看的類神經藝術品

190
00:12:53,730 --> 00:12:59,220
如此一來，你就可以生出很厲害並且嶄新的藝術品

191
00:12:59,220 --> 00:13:02,010
那麼，這就是神經風格轉換。我希望

192
00:13:02,010 --> 00:13:05,235
在這周的作業裡，你能從實作中獲得樂趣

193
00:13:05,235 --> 00:13:06,625
在結束這周的課程前

194
00:13:06,625 --> 00:13:08,575
還有一件事情我想分享

195
00:13:08,575 --> 00:13:11,100
就是如何做卷積運算

196
00:13:11,100 --> 00:13:17,000
做在 1D 或 3D 的資料，而不光是 2D 的圖片。
讓我們來看最後一部影片