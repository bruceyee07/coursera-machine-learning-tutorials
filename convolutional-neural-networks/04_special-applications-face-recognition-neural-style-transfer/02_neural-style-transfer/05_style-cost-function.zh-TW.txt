在上部影片，我們學到如何定義 神經風格轉換的「內容成本」 接下來，讓我們看看「風格成本」 那麼，一張圖片的「風格」是什麼意思？ 假設你有像這樣的輸入圖片 這種 ConvNet 架構你應該不陌生 在各個不同的層都會算出特徵 假設你選擇了某層 l — 例如這一層，來定義如何衡量一張圖片的風格 我們要做的是，將「風格」定義成 在第 l 層各個通道的啟動值之間的相關度 我的意思是這樣的 假設你取出第 l 層的啟動值 所以這會是一塊 n_h乘n_w乘n_c 的啟動值 我們會問：橫跨各個不同通道，他們的啟動值有多相關？ 這說的有點天書，要解釋的話 讓我們拿這一塊啟動值 把不同通道塗上不同的顏色 所以在這例子 假設我們有 5 個通道，所以我畫了五種顏色 當然實務上 一個神經網路的通道常常會比五個還多 不過這邊舉例五個只是為了畫圖簡單。 而為了要獲得一張圖片的風格 你會這樣子做： 讓我們看看頭兩個通道 讓我們看紅色的和黃色的通道， 看看這兩個通道的啟動值，兩者多相關 例如在最右下角 第一個通道會有某個啟動值，第二個通道有某個啟動值 所以這給了你一對數字 你要做的是看過不同的位置 在這堆啟動值只看這兩組數字 一組在第一個通道內、紅色的 另一組是黃色的通道、第二個的 你觀察這兩對的數字們 當你看過所有這些位置 n_h乘n_w 個位置 這兩組數字會多相關？ 那麼，為何這樣能捕捉出「風格」呢？ 讓我們看個例子 這邊是之前影片的一個視覺化例子 這邊是從之前講過的 Matthew Zeiler 和 Rob Fergus 的論文來的 為了討論方便，假設 這個紅色的神經元對應到... 為了討論方便，假設 那個紅色的通道對應到這一個神經元 — 我們想知道 在圖片中某個地方 有沒有這種小小垂直的花樣。再假設第二個通道 這個黃色的通道會對應這個神經元 也就是大約去找出橘色系的圖塊。 那麼，如果兩個通道很相關的話，這代表什麼意思？ 如果兩者很相關，表示無論在圖片的哪個區域 含有這種細微垂直的花樣的時候， 那一個區域也很可能有這種橘色色調 那如果兩者不相關是什麼意思？ 這代表當觀察到這種垂直的花樣 可能並不一定有橘色的色調 因此，相關度可以告訴你 這些抽象的花樣材質在圖片的某處 會傾向於一起出現還是不會。
而相關度的大小 可以衡量這些不同的高層次的特徵 — 例如垂直的花樣、這種橘色色調、諸如此類的 在圖片的各處他們多常會出現？ 會多常一起出現或不常一起出現？ 因此，如果我們用「通道間的相關度」作為一種風格的度量標準 那麼，我們可以在生成的圖片中，去衡量 這邊第一個通道有沒有跟 第二個通道相關，這樣會告訴你在生成圖片中 這種垂直花樣常不常 和橘色色調一起出現 — 這給了你一種衡量方法 看看生成圖片的風格，跟輸入風格圖片的風格，兩者有多相似 所以，讓我們把這概念寫成公式 給定一張圖片，你可以計算某個叫「風格矩陣」的東西 他會衡量出上一張投影片談到的那些相關度 更正式地說，讓 a 上標 [l] 下標 i,j,k 表示在第 l 層、位置 (i, j, k) 的啟動值 所以 i 是高度方向的索引 j 是寬度方向的索引 而 k 橫跨了不同的通道 在前一張投影片 我們有 5 個通道，所以 k 會那五個通道的索引 那麼「風格矩陣」所做的是，你會計算一個矩陣 叫他 G^[l]，這個矩陣維度會是 n_c乘n_c 所以是正方的矩陣 還記得你有 n_c 個通道，所以你會有 n_c乘n_c 維度的矩陣，是為了要衡量任意兩個之間的相關度 也就是說 G^[l]_k k' 會度量通道 k 和通道 k' 的啟動值，兩者之間有多相關 其中 k 和 k' 會從 1 到 n_c n_c 是那一層的通道的數量 更正式地說，要計算 G^[l]... — 讓我先寫出計算一個元素的公式 也就是這個的下標 k,k' 元素 這會是對 i 做加總 對 j 做加總 裡面是該層在 i, j, k 的啟動值 乘上在 i, j, k' 的啟動值 這邊請記得，i 和 j 是那一大塊啟動值的索引 在高度和寬度上的索引 所以 i 會從 1 加到 n_h，而 j 從 1 加到 n_w 而這邊的 k 和 k' 是通道的索引 所以 k 和 k' 範圍是 1 到 網路該層的通道數量 所以這一整件事在做的 是對圖片高度和寬度方向的每一個位置加起來 把通道 k 和 通道 k' 的啟動值的乘積 加起來，這個就是 G_k,k' 的定義 然後你對每個 k 和 k' 可能的值都這樣做，就計算出矩陣 G 也就是「風格矩陣」 注意一下，如果這兩個啟動值傾向於同時變大 那 G_k,k' 就會很大 而如果兩者無相關度 那 G_k,k' 就會小 技術上來說，我一直在用 「相關度」這個詞來教你們概念，不過 這其實是未正規化的交叉共變異數 (cross-covariance) 因為沒有減去平均，這也單純是元素直接相乘 那麼，這就是計算一張圖片「風格」的方法 你其實會對風格圖片 S 和生成圖片 G 都做這樣子做。只是為了區別，讓這個是風格圖片 或許讓我在上面加個括號 (S) 只是為了表示這是圖片 S 的風格矩陣 而那些是圖片 S 的啟動值 那接下來，你也對生成圖片計算同樣的東西 所以這其實是同樣的東西... 加過 i 加過 j a_i,j,k^[l] 加總的索引也一樣 像這樣... 然後你要表示這屬於生成圖片 我就放個括號 (G) 因此，現在你有兩個矩陣捕捉了圖片 S 的風格 和圖片 G 的風格 題外話，我們一直用大寫 G 來表示這些矩陣 在線性代數中，這些也叫 "Gram matrix" (格拉姆矩陣)，不過在這部影片 我就用「風格矩陣」這個詞。但是是因為 Gram矩陣這個術語，使得我們用大寫 G 來表示這些矩陣 最後呢，成本函數 這個風格成本函數 如果做在第 l 層，比較 S 和 G 你可定義這個為 定義為某種相差 這兩個矩陣的相差 ... G^[l](G) 然後平方。因為是矩陣 所以我取 Frobenius norm 這單純是兩個矩陣的逐元素差距的平方和 把他展開寫下來，這會是對 k 加總、 對 k' 加總，把這些差距加起來 ... k,k' 減 G^[l](G)_k,k' 然後取平方加起來 作者其實用了這個標準化的常數：2 乘以 n_h n_w 在該層上 還有該層的 n_c 然後平方。在上面也放這個 不過標準化常數不大重要 反正成本會乘上某個超參數 b 最後 這是定義在第 l 層的 風格成本函數。如前張投影片說的 這基本上是兩個風格矩陣之間的 Frobenius norm
(弗比尼斯範數) 在圖片 S 和圖片 G 上的 ... 取範數取平方，然後會有個標準化的常數 不過不是太重要 最後呢，其實為了讓結果在視覺上效果好， 你可以用不只一層的風格成本 因此，整個風格成本函數 你可以定義成 把每一層的風格成本函數加起來 — 我們應該要定義一些參數做加權 有這群額外的超參數 我在這裡定義乘 lambda^[l] 所以這可以讓你利用網路不同的層 初期的層 會衡量比較簡單、低階的特徵， 例如邊線。也可以用後面的層 去衡量高階的特徵。這讓神經網路 計算風格的時候可以同時考量低階和高階的相關度 而在程式作業當中 你會獲得更多的感覺 知道超參數 lambda 合理的選擇是多少 所以總結一下 現在你可以定義一整個成本函數 等於 alpha 乘上 C 和 G 之間的「內容成本」 加上 beta 乘上 S 和 G 之間的「風格成本」，
然後用梯度下降法 或者用更複雜的最佳化演算法 來嘗試找到一張圖片 G 讓成本函數 J(G) 變最小。如此一來 你就能產生很好看的類神經藝術品 如此一來，你就可以生出很厲害並且嶄新的藝術品 那麼，這就是神經風格轉換。我希望 在這周的作業裡，你能從實作中獲得樂趣 在結束這周的課程前 還有一件事情我想分享 就是如何做卷積運算 做在 1D 或 3D 的資料，而不光是 2D 的圖片。
讓我們來看最後一部影片