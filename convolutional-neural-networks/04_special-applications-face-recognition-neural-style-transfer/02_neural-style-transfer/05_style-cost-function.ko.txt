마지막 비디오에서는 신경 스타일 변형을 위한 콘텐츠 비용 함수를 정의하는 방법을 살펴 보았습니다. 다음으로 스타일 비용 함수를 살펴 보겠습니다. 그렇다면 이미지의 스타일이란 무엇입니까? 이와 같은 인풋 이미지가 있다고 가정 해 봅시다. 여러분은 이런 컨볼네트를 보는 데에 익숙할 겁니다. 다른 레이어가 있는 피쳐를 계산하는 것이죠. 이미지 스타일의 척도를 정의하기 위해. L 레이어를 선택했다고 가정 해 봅시다 우리가 해야 할 일은 다른 채널을 가로질러 분포하는 활성 사이의 상관 관계로 스타일을 정의하는 것입니다. 이 레이어 L 활성에서 말이죠. 그것이 의미하는 바는 이렇습니다. 레이어 L 활성을 수행한다고 가정 해 보겠습니다. 이것은 nh x nw x nc 블록의 활성이 될 것이고, 우리는 어떻게 서로 다른 채널을 통해 활성화가 상호 연관되어 있는지를 물어볼 것입니다. 이것이 의미하는 바를 설명하는 것이 어쩌면 약간 아리송하게 들릴지도 모릅니다. 이 활성 블럭을 가지고 다른 채널을 다른 색상으로 색칠해보겠습니다. 이 아래 예시에서, 우리는 다섯 개의 채널을 가지고 있고, 따라서 다섯 개의 색상을 사용하고 있죠. 실제로, 물론, 신경망에는 보통 다섯 개 보다 훨씬 많은 채널이 있습니다. 하지만 다섯 개를 사용하는 것이 더 쉽게 그릴 수 있겠죠. 이 이미지 스타일을 캡쳐하기 위해 우리가 다음과 같은 일을 해야 합니다. 첫 번째 두 채널을 봅시다. 빨간색 채널과 노란 채널, 즉 첫 번째 두 채널에서 활성이 어떤 관계를 가지는지 봅시다. 예를 들어, 오른쪽 하단 코너에 보면 첫 번째 채널의 있는 활성이 보이고, 두 번째 채널에도 활성이 있습니다. 이것이 여러분에게 번호 쌍을 주게 됩니다. 이 블록 활성을 가로질러 분포하고 있는 다른 위치를 보십시오. 그리고 두 쌍의 숫자를 보십시오. 하나는 빨간색인 첫 번째 채널이고 다른 하나는 노란색인, 두 번째 채널입니다. 이 두 쌍의 수를 보고 이 모든 위치를 보시면 이 모든 nh x nw 위치 말이죠, 이 두 숫자가 어떤 상관 관계를 가지는지 알 수 있습니다. 이 캡처는 왜 스타일을 하는 건가요? 또 다른 예시를 봅시다. 예전 강의에서 보신 시각화의 하나입니다. 이것은 Matthew Zeiler와 Rob Fergus의 논문에서 인용 한 것입니다. 논증을 위해서, 빨간 신경은 논의를 위한 것이죠, 빨간 채널은 이 신경에 대응한다고 하면, 그럼 nh 이 특정 위치에 있는 이 작은 수직 텍스처가 있다는 것을 알아낼 것입니다. 그리고 이 두 번째 채널, 노란색 두 번째 채널은 이 신경에 상응한다고 하면, 이것은 이것은 흐릿한 주황색 패치를 보여줄 것입니다. 이 두 채널이 강한 상관 관계가 있다는 것은 무엇을 의미하나요? 그것들이 강한 상관 관계가 있다면, 이 이미지의 이 이미지 부분이 미세한 수직 텍스처 유형을 가지고 있으면, 이 이미지 부분도 이 주황빛 계열을 가지게 된다는 뜻입니다. 그렇다면 상관 관계가 없다는 뜻은 무엇인가요? 그것이 의미하는 바는 이 수직 텍스처가 있는 부분이 있더라도, 이것은 주황빛 계열이 아닐 수도 있다는 의미입니다. 그리고 이 상관 관계는 이 높은 레벨의 텍스처 구성요소들이 이미지 부분에서 함께 발생할 수도 있고 그렇지 않을 수도 있다는 뜻입니다. 이 상관 관계의 정도는 하나의 측정 방법입니다. 다시 말해서, 높은 레벨의 피처들 예를 들어, 수직 텍스처나 주황빛 혹은 다른 물체들이 이런 한 이미지 상의 다른 부분에 위치할 때 얼마나 자주 같이 발생하고 얼마나 자주 같이 발생하지 않는지 측정하는 것입니다. 만약 스타일의 측정하기 위해 채널 간 상관 관계의 정도를 사용한다면, 여러분이 할 수 있는 일은 생성된 이미지에서 정도를 측정하는 것입니다. 이 첫 번째 채널은 두 번째 채널과 상관관계가 있거나 상관 관계가 없습니다. 그리고 이는 생성된 이미지 상에서 얼마나 자주 이런 유형의 수직 텍스처가 주황빛과 같이 발생하고 혹은 발생하지 않는지를 알려줍니다. 그리고 이는 생성된 스타일이 인풋 스타일 이미지와는 얼마나 비슷한지 그 측정치를 알려주는 것이죠. 이 직관을 공식화 해봅시다. 여러분이 할 수 있는 일은 주어진 이미지가 스타일 매트릭스라고 불리는 것을 계산하는 것입니다. 그것은 앞의 슬라이드에서 언급한 이 모든 상관 관계를 측정할 것 입니다. 따라서, 공식으로 말하자면, 위 첨자 l, 아래 첨자 i, j, k는 히든 레이어에서 포지션 i, j, k 에서의 활성을 표시할 것입니다. 따라서 i 는 높이를 가리키고, j 는 너비를 그리고 k는 다른 채널들을 가로질러 분포하는 것을 가리킵니다. 이전 슬라이드에서, 다섯 개의 채널을 봤었는데요, k 는 5개 채널을 가로지르는 것을 가리켰었죠. 따라서 스타일 매트릭스가 할 일은 매트릭스 클래스 G의 [l]제곱입니다. 그리고 이것은 nc x nc 차원의 매트릭스가 될 것이고, 바로 제곱 매트릭스가 되겠죠. 여러분에겐 nc 채널들이 있다는 것을 기억하십시오 그리고 또한 그들 각각의 쌍이 얼마나 상관되어 있는지를 측정하기 위해 nc x nc 차원의 매트릭스도 있습니다. 특별한 G, l, k, k프라임은 채널 k에서의 활성화와 채널 k프라임 에서의 활성화가 얼마나 상관 관계가 있는지를 측정 할 것입니다. 여기서는 k와 k프라임은 1부터 nc까지의 범위가 되고, 이는 곧 이 쪽 위에 있는 레이어의 그 채널의 개수입니다. 그래서 더 공식적으로, G와 I를 계산하는 방법, 이 원소들을 계산하는 공식을 여기에 적어보겠습니다. 이것의 k와 k프라임(k프라임) 이것은 i의 합, j 의 합, i, j, k 레이어의 액티베이션 곱하기 액티베이선 i, j, k프라임. 자, 여기서 i와 j는 블럭에서 다른 위치를 가르 지르는 것을 가리킨다는 것을 기억하세요. 즉, 높이와 너비를 가리킨다는 것을요. 따라서 i 는 1부터 nh까지의 합이고, j는 1부터 nw까지의 합. k 와 k프라임은 이 채널을 가리키므로 k 와 k프라임 는 1부터 신경망 레이어에 있는 채널의 총 개수 가 되는 것입니다. 그래서 이 모든 것이 하는 것은 이미지의 서로 다른 위치에서 높이와 너비에 대한 합이고, 채널 k와 k프라임 의 액티베이션을 곱하는 것입니다. 이것이 G kk프라임의 정의입니다. 이 매트릭스 G를 계산하기 위해 k 및 k프라임 모든 값에 대해 이것을 하는 것입니다. 이는 스타일 매트릭스라고도 불립니다. 만약 이 액티베이션 둘 다 함께 커지는 경향이 있으면 Gkk프라임 도 커질 것이고, 반면에 이 둘이 상관이 안되면, Gkk프라임 는 작아질 것입니다. 그리고 기술적으로 저는 직감을 전달하기 위해 상관 관계라는 용어를 사용 해왔습니다. 그러나 이것은 실제로 비 표준화된 십자 영역입니다. 왜냐하면 우리는 평균을 뺄셈을 하고 있지 않고 이것은 단지 이 요소들에 의해 직접 곱해지고 있기 때문입니다. 이것이 이미지의 스타일을 계산하는 방법입니다. 그리고 여러분은 실제로 두 스타일 이미지 s, n을 생성 된 이미지 G에 대해 이렇게 할 것입니다. 그래서 이것이 스타일 이미지라는 점을 구별하기 위해 S 에 둥근 괄호를 써넣겠습니다. 그러면 이것이 이미지 S에 대한 스타일 이미지 임을 표시할 수 있고, 이것들은 이미지 S의 활성이 되는 것입니다. 그리고 나서는 생성 된 이미지에 대해 똑같은 것을 계산하십시오. 따라서 이것은 정말로 똑같은 것입니다. i 의 합, j 의 합, a의 ijk 그리고 l, a의 ijk 그리고 l 그리고 이 합계 부분도 같습니다. 이것을 따라 해봅시다 그러면 이것이 생성된 이미지 라는 것을 표시하길 원할 것입니다. G 에 둥근 괄호를 표시하겠습니다. 그래서, 여러분은 두 개의 매트릭스를 가지고 있습니다. 이 매트릭스은 이미지 S가 가진 스타일이고, 이것은 이미지 G의 스타일이고, 그런데 이 매트릭스를 표시하기 위해서 알파벳 대문자 G를 사용해왔군요. 선형 대수학에서는 이것들을 그랜드 매트릭스라고 부릅니다. 하지만 이 강의에서 저는 스타일 매트릭스라는 용어를 사용하겠습니다. 이 용어 그랜드 매트릭스는 대부분 대문자 G를 사용하고 이 매트릭스들을 표시하기 때문입니다. 마지막으로, 비용 함수, 스타일 비용함수입니다. 여러분이 만약 S 와 G사이의 레이어 l 에서 이걸 하고 있다면, 여러분은 이 차이점을, 이 두 매트릭스 사이의, Gl G 제곱 그리고 이 매트릭스, 이건 그냥 이전 강의들에서 가져온 것들입니다. 이것은 이 두 매트릭스 사이에서 생긴 요소 간 차이 값에 제곱해서 모두 합한 것입니다. 그리고 이것을 k의 합, k프라임의 합, s, k, k프라임 에서 Gl 을 빼고 G, k, k프라임 그리고 요소를 제곱한 것의 합. 이 작가들은 사실 정규화 상수에 이것을 사용했는데요, 2 x nh, 이 레이어에 nw, 이 레이어의 nc 그리고 이것을 제곱하고 이 쪽 위에도 적어 주시면 됩니다. 그러나 정규화 상수는 그다지 중요하지 않습니다. 왜냐하면 이것이 어떤 하이퍼 파라미터 b를 곱하기 때문입니다. 마무리 짓자면, 이전 슬라이드에서 보아 듯이 이것은 레이어 l을 사용하여 정의된 스타일 비용 함수입니다. 이것은 기본적으로 이미지 s와 이미지 G에서 계산 된 두 개의 스타일 매트릭스 사이의 Frobenius 표준입니다. Frobenius의 F, 그리고 제곱을 합니다. 이것은 그다지 중요하지 않은 단지 낮은 정규화 상수에 의해서가 아닙니다. 중요하지 않은 정규화 상수 그리고 마지막으로, 만약 여러분이 여러 개의 다른 레이어로부터 스타일 비용 함수를 사용한다면 시각적으로 더 훌륭한 결과를 얻을 수 있습니다. 따라서, 여러분이 정의할 수 있는 전체 스타일 비용 함수는 모든 다른 레이어들의 합, 그 레이어의 스타일 비용 함수, 몇몇 파라미터 세트로, 추가적인 하이퍼 파라미터로 가중치, 여기에 람다 ㅣ로 표시하여 가중치를 주어 정의 내려야 합니다. 따라서 이것이 하는 일은 여러분이 신경망에서 다른 레이어를 사용하도록 하는 것입니다. 앞의 것들은 모서리와 같은 비교적 간단한 낮은 레벨 피처를 측정합니다. 또한, 후기 레이어에서는, 높은 수준의 피처를 측정하고, 스타일을 계산할 때 신경망이 낮은 레벨과 높은 레벨의 상관 관계를 모두 고려하도록 합니다. 다음 연습에서는 이 유형의 매개 변수 람다에 대해 합리적인 선택이 될 수 있는 것에 대해 더 많은 직관을 얻을 수 있을 것입니다. 정리해보면, 여러분은 이제 전반적인 비용 함수를 정의 내릴 수 있습니다. 알파 곱하기 C와 G 사이의 컨텐츠 비용, 더하기, 베타 곱하기 S와 G 사이의 스타일 비용. 이렇게 하면 정교한 방법으로 최적화된 알고리즘을 얻을 수 있습니다. 그럼 정규화된 이미지 G를 찾을 수 있고, 이 비용함수 J의 (G)를 최소화 할 수 있습니다. 그리고 이렇게 하시면 멋져 보이는 신경 예술 작품을 만들어낼 수 있습니다. 또한, 이렇게 하시면 예쁘고 멋진 신기한 예술 작품을 만들어 낼 수 있을 것입니다. 신경 스타일 변형은 여기까지 입니다. 이번 주 프린팅 연습에서 즐겁게 실행해보는 시간 가지시길 바랍니다. 이번 주를 마무리 하기 전에, 한 가지 공유 드리고 싶은 점은, 2D 이미지에서뿐 아니라 1D 혹은 3D 에서 어떻게 컨볼루션을 할 수 있는가 입니다. 다음 강의로 가보시죠.