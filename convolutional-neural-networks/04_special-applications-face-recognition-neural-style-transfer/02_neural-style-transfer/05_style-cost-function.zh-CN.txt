在上一个视频中 你们学习了如何定义 神经风格转移(算法)的内容代价函数 接下来 让我们来看看风格代价函数 那么 对于图像而言风格是什么意思呢？ 比如说 你有一张这样的图像作为输入 它们一般会通过这样的卷积网络 计算不同隐藏层中的特征 假设你选择了某一层L 也许是这一层来衡量图像的风格 我们要做的是将风格定义为层中 不同激活通道之间的相关系数 具体是这样的 假设你选择了激活层L 这是一个nh乘nw乘nc的激活阵 然后我们想知道的是
不同的激活通道间的相关性有多大 所以让我来解释一下这个有些隐晦的说法 在这个激活阵上 让我用不同的颜色表示不同的通道 所以在这个例子中 我们有5个通道 所以分别用5种颜色表示 当然 在实践中 在神经网络中 通道往往有远多于5个 但用5个通道使得图示更为方便 为了得到图像的风格 你需要这样做 让我们看一下前两个通道 让我们看一下红色的和黄色的通道 这两个激活通道的相关性有多大 例如 在右下角 你在第一个通道和第二个通道分别有一些激活元 所以这会给你一个数组 你需要做的是遍历这个激活块中不同的位置 看这样一对一对的数 其中一个在第一个通道 也就是红色的通道 另一个在黄色的通道 也就是第二个通道 然后你就看这些成对的数 看看当你遍历所有这些位置 即所有这些nh乘nw的位置时 这些成对的数之间的相关性有多大 所以 为什么这能表示风格呢？ 让我们看一个例子 这是之前视频中的一个图示 它来自于我之前提到的 Matthew Zeiler和Rob Fergus的论文 为了方便解释 我们假设红色的神经元对应于 为了方便解释 我们假设红色的通道对应于这个 用于分辨在图像中某些位置是否有 竖条纹的神经元，我们再假设第二个通道 这个第二个黄色的通道对应于这个 分辨橙色色块的神经元 那么这两个通道高度相关意味着什么呢？ 如果它们高度相关，那么这意味着图像中 任何有这种微妙的竖条纹的部分 那个部分也很有可能有这种橙色色调 那么如果它们不相关呢？ 这意味着，任何有竖条纹的部分 很有可能不会有橙色的色调 所以相关性可以告诉你 哪一些高层的纹理元素倾向于同时或不同时出现 在图像中的某一部分，相关程度提供了一种 衡量这些高层特征 例如竖条纹或橙色调或其他特征 是否经常出现，以及是否经常同时出现在 图像中的某一部分的方式 所以，如果我们使用通道间的相关性作为量化风格的方式 那么你可以做的是在生成的图像中测量 第一个通道与第二个通道的相关程度 这可以告诉你在生成的图像中 这种竖条纹是否经常和这种橙色调同时出现 然后这会告诉你生成的图像的风格 和输入图像的风格有多相似 现在，让我们用更数学的语言来表达刚才直观的描述 所以，你要做的是通过给定的图像计算出风格矩阵 风格矩阵会记录上一张幻灯片中我们提到的所有相关系数 所以，更正式地，让我们用a 上标l 下标i j k 来表示在隐藏层l中位于i j k的激活元 所以指标i表示高度 指标j表示宽度 指标k表示不同的通道 所以在之前的幻灯片中 我们有5个通道，那么指标k将会遍历这5个通道 所以，你要做的是计算一个风格矩阵G上标l 这会是一个nc乘nc的矩阵 也就是一个方阵 因为你有nc个通道，所以你需要一个 nc乘nc大小的矩阵来记录每一对通道间的相关性 所以G 上标l 下标k k' 将会表示通道k和通道k' 的激活元之间的相关性 这里 k和k'会遍历1到nc 即那一层中总共的通道数 所以，更正式地，你会这样计算G 上标l 我只写一下计算某一个元素的公式 即其中第k行第k'列的元素 它会是l层中 位于i j k的激活元 和位于i j k'的激活元的乘积 遍历所有可能的i j所得到的和 这里要记住，指标i j遍历了这整块中不同的位置 即遍历了高度和宽度 所以指标i要从1加到nh 指标j要从1加到nw 还有，这里的指标k和k'要遍历不同的通道 所以k和k'是从1到 这个神经网络的第l层的总通道数 所以，所要做的就是 遍历图像中各个高度和宽度的位置 将第k个和第k'个通道上的激活元相乘再求和 这就是G下标k k'的定义 然后对每一个k和k'都做同样的运算，来得到这个矩阵G 即风格矩阵 值得注意的是，如果这些对应激活元都很大 那么G下标k k'也会很大 而如果它们不相关，那么G下标k k' 可能会较小 从技术上来说，我一直在用 相关系数这个术语来直观地表述，但实际上 所计算的是未标准化的互协方差，因为我们没有 将平均值减去，只是直接将对应的元素相乘 所以，上述就是如何计算图像的风格 实际上，你要对风格图像s和 生成图像g都做同样的运算，所以
为了区分这是风格图像(的风格矩阵) 让我在这里加一个圆括号S 来表示这是风格图像S(的风格矩阵) 以及这些是图像S上的激活元 然后你要做的是对生成图像做同样的计算 所以，就是一样的，对i j遍历求和 a下标i j k上标l a下标i j k'上标l 然后求和指标是和上面一样的 最后你需要表示这个是生成图像的 我就在这里加上圆括号G 所以，现在，你有两个矩阵分别代表着 图像S和图像G的风格 顺便提一下，我一直用大写字母G来表示这些矩阵 因为在线性代数中，这些矩阵也被称之为 格拉姆矩阵，虽然在这个视频中 我称它们为风格矩阵，但格拉姆矩阵的说法 促使我们用大写的G来表示这些矩阵 最后，代价函数 即风格代价函数 如果你用图像S和图像G的第l层(来计算) 现在，你可以将其定义为 这两个矩阵 G上标l S和G上标l G 的差的平方，这两个就是 上面提到的两个矩阵 这就是这两个矩阵对应元素的差的平方和 把它写出来就是 G上标l S下标k k'减去G上标l G下标k k' 的差的平方 遍历所有的k和k' 对所有的元素求和 论文的作者还使用了这样的标准化系数 2乘以nh 乘以这一层的nw 乘以这一层的nc 然后平方一下，这里也要加上 不过这个标准化常数并没有太大的影响 因为这一项代价函数反正都需要乘以超参数b 所以，总结一下 这是通过l层定义的风格代价函数 正如你在之前的幻灯片中看到的 这基本上是基于图像S和图像G计算出的两个风格矩阵 之间的弗罗贝尼乌斯范数(Frobenius norm) 的平方，再额外乘以一个 不那么重要的标准化常数 最后，如果你用几个不同的层来计算风格代价矩阵 你会得到看上去更好的结果 所以，总的风格代价函数 可以被定义为 所有不同层的风格代价函数的总和 我们需要使用一组参数，一组额外的超参数 给上述公式加入权重 也就是这里的lambda上标l所表示的 这里所做的是，允许你用一个神经网络中不同的层 包括那些早期的层 用来捕捉相对简单的低层特征 比如边界的，以及一些后期的 用来捕捉高层特征的层，这样可以使得 在计算风格的时候，将神经网络中低层和高层的相关性都考虑到 在编程练习中 你会对如何合理选择这种参数lambda 有更多的感觉 所以，最后总结一下 你现在可以定义总的代价函数为 alpha乘以C和G之间的内容代价函数加上 beta乘以S和G之间的风格代价函数，然后用梯度下降法 或者，如果你愿意的话，采用更复杂的优化算法 来试图找到一个图像G 使得代价函数J(G)尽可能地被最小化，如果你这么做了 你可以生成相当不错的基于神经网络的艺术作品 如果你这么做了，你可以得到一些非常不错的独特的艺术品 所以关于神经风格转移，我们就讲到这里 我希望你们在这周的编程练习中编程愉快 在结束这周(的课)之前 我还想和你们分享一件事 那就是，如何在一维或三维的数据 而不是二维的图像上实现卷积，让我们进入最后一个视频