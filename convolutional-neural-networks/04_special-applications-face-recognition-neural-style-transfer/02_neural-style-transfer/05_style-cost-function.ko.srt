1
00:00:00,000 --> 00:00:02,400
마지막 비디오에서는 신경 스타일 변형을 위한

2
00:00:02,400 --> 00:00:04,980
콘텐츠 비용 함수를 정의하는 방법을 살펴 보았습니다.

3
00:00:04,980 --> 00:00:09,317
다음으로 스타일 비용 함수를 살펴 보겠습니다.

4
00:00:09,317 --> 00:00:12,373
그렇다면 이미지의 스타일이란 무엇입니까?

5
00:00:12,373 --> 00:00:14,633
이와 같은 인풋 이미지가 있다고 가정 해 봅시다.

6
00:00:14,633 --> 00:00:16,750
여러분은 이런 컨볼네트를 보는 데에 익숙할 겁니다.

7
00:00:16,750 --> 00:00:20,091
다른 레이어가 있는 피쳐를 계산하는 것이죠.

8
00:00:20,091 --> 00:00:22,692
이미지 스타일의 척도를 정의하기 위해.

9
00:00:22,692 --> 00:00:29,020
L 레이어를 선택했다고 가정 해 봅시다

10
00:00:29,020 --> 00:00:34,095
우리가 해야 할 일은 다른 채널을 가로질러 분포하는 활성 사이의 상관 관계로

11
00:00:34,095 --> 00:00:40,635
스타일을 정의하는 것입니다. 이 레이어 L 활성에서 말이죠.

12
00:00:40,635 --> 00:00:42,190
그것이 의미하는 바는 이렇습니다.

13
00:00:42,190 --> 00:00:44,480
레이어 L 활성을 수행한다고 가정 해 보겠습니다.

14
00:00:44,480 --> 00:00:50,936
이것은 nh x nw x nc 블록의 활성이 될 것이고,

15
00:00:50,936 --> 00:00:55,995
우리는 어떻게 서로 다른 채널을 통해 활성화가 상호 연관되어 있는지를 물어볼 것입니다.

16
00:00:55,995 --> 00:00:59,966
이것이 의미하는 바를 설명하는 것이 어쩌면 약간 아리송하게 들릴지도 모릅니다.

17
00:00:59,966 --> 00:01:02,850
이 활성 블럭을 가지고

18
00:01:02,850 --> 00:01:06,575
다른 채널을 다른 색상으로 색칠해보겠습니다.

19
00:01:06,575 --> 00:01:08,295
이 아래 예시에서,

20
00:01:08,295 --> 00:01:14,286
우리는 다섯 개의 채널을 가지고 있고, 따라서 다섯 개의 색상을 사용하고 있죠.

21
00:01:14,286 --> 00:01:15,375
실제로, 물론,

22
00:01:15,375 --> 00:01:18,514
신경망에는 보통 다섯 개 보다 훨씬 많은 채널이 있습니다.

23
00:01:18,514 --> 00:01:22,056
하지만 다섯 개를 사용하는 것이 더 쉽게 그릴 수 있겠죠.

24
00:01:22,056 --> 00:01:24,765
이 이미지 스타일을 캡쳐하기 위해

25
00:01:24,765 --> 00:01:26,850
우리가 다음과 같은 일을 해야 합니다.

26
00:01:26,850 --> 00:01:28,910
첫 번째 두 채널을 봅시다.

27
00:01:28,910 --> 00:01:32,970
빨간색 채널과 노란 채널, 즉

28
00:01:32,970 --> 00:01:37,450
첫 번째 두 채널에서 활성이 어떤 관계를 가지는지 봅시다.

29
00:01:37,450 --> 00:01:40,575
예를 들어, 오른쪽 하단 코너에 보면

30
00:01:40,575 --> 00:01:45,820
첫 번째 채널의 있는 활성이 보이고, 두 번째 채널에도 활성이 있습니다.

31
00:01:45,820 --> 00:01:47,655
이것이 여러분에게 번호 쌍을 주게 됩니다.

32
00:01:47,655 --> 00:01:51,510
이 블록 활성을 가로질러 분포하고 있는 다른 위치를 보십시오.

33
00:01:51,510 --> 00:01:55,435
그리고 두 쌍의 숫자를 보십시오.

34
00:01:55,435 --> 00:01:57,232
하나는 빨간색인 첫 번째 채널이고

35
00:01:57,232 --> 00:02:00,000
다른 하나는 노란색인, 두 번째 채널입니다.

36
00:02:00,000 --> 00:02:02,370
이 두 쌍의 수를 보고

37
00:02:02,370 --> 00:02:04,816
이 모든 위치를 보시면

38
00:02:04,816 --> 00:02:07,320
이 모든 nh x nw 위치 말이죠,

39
00:02:07,320 --> 00:02:10,205
이 두 숫자가 어떤 상관 관계를 가지는지 알 수 있습니다.

40
00:02:10,205 --> 00:02:12,550
이 캡처는 왜 스타일을 하는 건가요?

41
00:02:12,550 --> 00:02:14,405
또 다른 예시를 봅시다.

42
00:02:14,405 --> 00:02:17,844
예전 강의에서 보신 시각화의 하나입니다.

43
00:02:17,844 --> 00:02:20,280
이것은 Matthew Zeiler와 Rob Fergus의

44
00:02:20,280 --> 00:02:23,350
논문에서 인용 한 것입니다.

45
00:02:23,350 --> 00:02:25,360
논증을 위해서,

46
00:02:25,360 --> 00:02:28,300
빨간 신경은

47
00:02:28,300 --> 00:02:30,170
논의를 위한 것이죠,

48
00:02:30,170 --> 00:02:36,600
빨간 채널은 이 신경에 대응한다고 하면, 그럼 nh 이 특정 위치에 있는

49
00:02:36,600 --> 00:02:40,320
이 작은 수직 텍스처가 있다는 것을 알아낼 것입니다.

50
00:02:40,320 --> 00:02:46,185
그리고 이 두 번째 채널, 노란색 두 번째 채널은

51
00:02:46,185 --> 00:02:51,795
이 신경에 상응한다고 하면, 이것은

52
00:02:51,795 --> 00:02:56,515
이것은 흐릿한 주황색 패치를 보여줄 것입니다.

53
00:02:56,515 --> 00:03:01,104
이 두 채널이 강한 상관 관계가 있다는 것은 무엇을 의미하나요?

54
00:03:01,104 --> 00:03:04,560
그것들이 강한 상관 관계가 있다면, 이 이미지의 이 이미지 부분이

55
00:03:04,560 --> 00:03:08,430
미세한 수직 텍스처 유형을 가지고 있으면,

56
00:03:08,430 --> 00:03:12,980
이 이미지 부분도 이 주황빛 계열을 가지게 된다는 뜻입니다.

57
00:03:12,980 --> 00:03:15,755
그렇다면 상관 관계가 없다는 뜻은 무엇인가요?

58
00:03:15,755 --> 00:03:19,635
그것이 의미하는 바는 이 수직 텍스처가 있는 부분이 있더라도,

59
00:03:19,635 --> 00:03:22,625
이것은 주황빛 계열이 아닐 수도 있다는 의미입니다.

60
00:03:22,625 --> 00:03:25,710
그리고 이 상관 관계는

61
00:03:25,710 --> 00:03:31,020
이 높은 레벨의 텍스처 구성요소들이 이미지 부분에서 함께 발생할 수도 있고

62
00:03:31,020 --> 00:03:35,550
그렇지 않을 수도 있다는 뜻입니다. 이 상관 관계의 정도는 하나의 측정 방법입니다.

63
00:03:35,550 --> 00:03:40,455
다시 말해서, 높은 레벨의 피처들

64
00:03:40,455 --> 00:03:45,441
예를 들어, 수직 텍스처나 주황빛 혹은 다른 물체들이 이런 한 이미지 상의 다른 부분에 위치할 때

65
00:03:45,441 --> 00:03:48,180
얼마나 자주 같이 발생하고

66
00:03:48,180 --> 00:03:51,740
얼마나 자주 같이 발생하지 않는지 측정하는 것입니다.

67
00:03:51,740 --> 00:03:57,180
만약 스타일의 측정하기 위해 채널 간 상관 관계의 정도를 사용한다면,

68
00:03:57,180 --> 00:04:02,670
여러분이 할 수 있는 일은 생성된 이미지에서 정도를 측정하는 것입니다.

69
00:04:02,670 --> 00:04:06,810
이 첫 번째 채널은 두 번째 채널과 상관관계가 있거나

70
00:04:06,810 --> 00:04:12,090
상관 관계가 없습니다. 그리고 이는 생성된 이미지 상에서 얼마나 자주

71
00:04:12,090 --> 00:04:14,820
이런 유형의 수직 텍스처가 주황빛과 같이 발생하고

72
00:04:14,820 --> 00:04:18,450
혹은 발생하지 않는지를 알려줍니다. 그리고 이는

73
00:04:18,450 --> 00:04:25,675
생성된 스타일이 인풋 스타일 이미지와는 얼마나 비슷한지 그 측정치를 알려주는 것이죠.

74
00:04:25,675 --> 00:04:28,600
이 직관을 공식화 해봅시다.

75
00:04:28,600 --> 00:04:34,620
여러분이 할 수 있는 일은 주어진 이미지가 스타일 매트릭스라고 불리는 것을 계산하는 것입니다.

76
00:04:34,620 --> 00:04:38,960
그것은 앞의 슬라이드에서 언급한 이 모든 상관 관계를 측정할 것 입니다.

77
00:04:38,960 --> 00:04:44,280
따라서, 공식으로 말하자면, 위 첨자 l, 아래 첨자 i, j, k는

78
00:04:44,280 --> 00:04:47,868
히든 레이어에서 포지션 i, j, k 에서의 활성을 표시할 것입니다.

79
00:04:47,868 --> 00:04:53,610
따라서 i 는 높이를 가리키고,

80
00:04:53,610 --> 00:04:54,850
j 는 너비를

81
00:04:54,850 --> 00:04:58,050
그리고 k는 다른 채널들을 가로질러 분포하는 것을 가리킵니다.

82
00:04:58,050 --> 00:05:00,045
이전 슬라이드에서,

83
00:05:00,045 --> 00:05:05,165
다섯 개의 채널을 봤었는데요, k 는 5개 채널을 가로지르는 것을 가리켰었죠.

84
00:05:05,165 --> 00:05:09,635
따라서 스타일 매트릭스가 할 일은 매트릭스 클래스

85
00:05:09,635 --> 00:05:17,390
G의 [l]제곱입니다. 그리고 이것은 nc x nc 차원의 매트릭스가 될 것이고,

86
00:05:17,390 --> 00:05:18,755
바로 제곱 매트릭스가 되겠죠.

87
00:05:18,755 --> 00:05:23,390
여러분에겐 nc 채널들이 있다는 것을 기억하십시오 그리고 또한

88
00:05:23,390 --> 00:05:29,490
그들 각각의 쌍이 얼마나 상관되어 있는지를 측정하기 위해 nc x nc 차원의 매트릭스도 있습니다.

89
00:05:29,490 --> 00:05:32,585
특별한 G, l, k, k프라임은

90
00:05:32,585 --> 00:05:36,954
채널 k에서의 활성화와 채널 k프라임 에서의 활성화가

91
00:05:36,954 --> 00:05:41,755
얼마나 상관 관계가 있는지를 측정 할 것입니다.

92
00:05:41,755 --> 00:05:46,250
여기서는 k와 k프라임은 1부터 nc까지의 범위가 되고,

93
00:05:46,250 --> 00:05:49,630
이는 곧 이 쪽 위에 있는 레이어의 그 채널의 개수입니다.

94
00:05:49,630 --> 00:05:55,820
그래서 더 공식적으로, G와 I를 계산하는 방법,

95
00:05:55,820 --> 00:06:00,840
이 원소들을 계산하는 공식을 여기에 적어보겠습니다.

96
00:06:00,840 --> 00:06:03,283
이것의 k와 k프라임(k프라임)

97
00:06:03,283 --> 00:06:06,210
이것은 i의 합,

98
00:06:06,210 --> 00:06:08,987
j 의 합,

99
00:06:08,987 --> 00:06:13,979
i, j, k 레이어의 액티베이션

100
00:06:13,979 --> 00:06:22,078
곱하기 액티베이선 i, j, k프라임.

101
00:06:22,078 --> 00:06:27,989
자, 여기서 i와 j는 블럭에서 다른 위치를 가르 지르는 것을 가리킨다는 것을 기억하세요.

102
00:06:27,989 --> 00:06:30,453
즉, 높이와 너비를 가리킨다는 것을요.

103
00:06:30,453 --> 00:06:39,755
따라서 i 는 1부터 nh까지의 합이고, j는 1부터 nw까지의 합.

104
00:06:39,755 --> 00:06:45,200
k 와 k프라임은 이 채널을 가리키므로

105
00:06:45,200 --> 00:06:47,870
k 와 k프라임 는 1부터

106
00:06:47,870 --> 00:06:51,913
신경망 레이어에 있는 채널의 총 개수 가 되는 것입니다.

107
00:06:51,913 --> 00:06:55,967
그래서 이 모든 것이 하는 것은

108
00:06:55,967 --> 00:07:00,225
이미지의 서로 다른 위치에서 높이와 너비에 대한 합이고,

109
00:07:00,225 --> 00:07:03,640
채널 k와 k프라임 의 액티베이션을 곱하는 것입니다.

110
00:07:03,640 --> 00:07:08,853
이것이 G kk프라임의 정의입니다.

111
00:07:08,853 --> 00:07:14,450
이 매트릭스 G를 계산하기 위해 k 및 k프라임 모든 값에 대해 이것을 하는 것입니다.

112
00:07:14,450 --> 00:07:17,585
이는 스타일 매트릭스라고도 불립니다.

113
00:07:17,585 --> 00:07:23,435
만약 이 액티베이션 둘 다 함께 커지는 경향이 있으면

114
00:07:23,435 --> 00:07:26,325
Gkk프라임 도 커질 것이고,

115
00:07:26,325 --> 00:07:28,510
반면에 이 둘이 상관이 안되면,

116
00:07:28,510 --> 00:07:30,305
Gkk프라임 는 작아질 것입니다.

117
00:07:30,305 --> 00:07:32,060
그리고 기술적으로 저는 직감을 전달하기 위해

118
00:07:32,060 --> 00:07:36,170
상관 관계라는 용어를 사용 해왔습니다. 그러나 이것은 실제로

119
00:07:36,170 --> 00:07:40,130
비 표준화된 십자 영역입니다. 왜냐하면 우리는

120
00:07:40,130 --> 00:07:46,130
평균을 뺄셈을 하고 있지 않고 이것은 단지 이 요소들에 의해 직접 곱해지고 있기 때문입니다.

121
00:07:46,130 --> 00:07:50,370
이것이 이미지의 스타일을 계산하는 방법입니다.

122
00:07:50,370 --> 00:07:54,030
그리고 여러분은 실제로 두 스타일 이미지 s, n을

123
00:07:54,030 --> 00:08:01,020
생성 된 이미지 G에 대해 이렇게 할 것입니다. 그래서 이것이 스타일 이미지라는 점을 구별하기 위해

124
00:08:01,020 --> 00:08:07,630
S 에 둥근 괄호를 써넣겠습니다.

125
00:08:07,630 --> 00:08:10,105
그러면 이것이 이미지 S에 대한 스타일 이미지 임을 표시할 수 있고,

126
00:08:10,105 --> 00:08:12,715
이것들은 이미지 S의 활성이 되는 것입니다.

127
00:08:12,715 --> 00:08:21,085
그리고 나서는 생성 된 이미지에 대해 똑같은 것을 계산하십시오.

128
00:08:21,085 --> 00:08:28,581
따라서 이것은 정말로 똑같은 것입니다. i 의 합, j 의 합,

129
00:08:28,581 --> 00:08:32,670
a의 ijk 그리고 l, a의 ijk 그리고 l

130
00:08:32,670 --> 00:08:36,678
그리고 이 합계 부분도 같습니다.

131
00:08:36,678 --> 00:08:46,130
이것을 따라 해봅시다 그러면 이것이 생성된 이미지 라는 것을 표시하길 원할 것입니다.

132
00:08:46,130 --> 00:08:51,710
G 에 둥근 괄호를 표시하겠습니다.

133
00:08:51,710 --> 00:08:55,540
그래서, 여러분은 두 개의 매트릭스를 가지고 있습니다.

134
00:08:55,540 --> 00:08:59,770
이 매트릭스은 이미지 S가 가진 스타일이고, 이것은 이미지 G의 스타일이고,

135
00:08:59,770 --> 00:09:05,260
그런데 이 매트릭스를 표시하기 위해서 알파벳 대문자 G를 사용해왔군요.

136
00:09:05,260 --> 00:09:09,445
선형 대수학에서는 이것들을 그랜드 매트릭스라고 부릅니다.

137
00:09:09,445 --> 00:09:14,030
하지만 이 강의에서

138
00:09:14,030 --> 00:09:17,680
저는 스타일 매트릭스라는 용어를 사용하겠습니다. 이 용어 그랜드 매트릭스는

139
00:09:17,680 --> 00:09:23,630
대부분 대문자 G를 사용하고 이 매트릭스들을 표시하기 때문입니다.

140
00:09:23,630 --> 00:09:26,035
마지막으로, 비용 함수,

141
00:09:26,035 --> 00:09:28,875
스타일 비용함수입니다.

142
00:09:28,875 --> 00:09:34,570
여러분이 만약 S 와 G사이의 레이어 l 에서 이걸 하고 있다면,

143
00:09:34,570 --> 00:09:37,050
여러분은

144
00:09:37,050 --> 00:09:44,610
이 차이점을,

145
00:09:44,610 --> 00:09:48,675
이 두 매트릭스 사이의,

146
00:09:48,675 --> 00:09:54,265
Gl G 제곱 그리고 이 매트릭스,

147
00:09:54,265 --> 00:09:55,754
이건 그냥 이전 강의들에서 가져온 것들입니다.

148
00:09:55,754 --> 00:10:00,660
이것은 이 두 매트릭스 사이에서 생긴 요소 간 차이 값에 제곱해서 모두 합한 것입니다.

149
00:10:00,660 --> 00:10:07,065
그리고 이것을 k의 합,

150
00:10:07,065 --> 00:10:12,964
k프라임의 합, s, k, k프라임 에서

151
00:10:12,964 --> 00:10:17,450
Gl 을 빼고

152
00:10:17,450 --> 00:10:24,530
G, k, k프라임 그리고 요소를 제곱한 것의 합.

153
00:10:24,530 --> 00:10:32,715
이 작가들은 사실 정규화 상수에 이것을 사용했는데요, 2 x nh,

154
00:10:32,715 --> 00:10:34,890
이 레이어에 nw,

155
00:10:34,890 --> 00:10:40,015
이 레이어의 nc 그리고 이것을 제곱하고 이 쪽 위에도 적어 주시면 됩니다.

156
00:10:40,015 --> 00:10:43,600
그러나 정규화 상수는 그다지 중요하지 않습니다.

157
00:10:43,600 --> 00:10:47,485
왜냐하면 이것이 어떤 하이퍼 파라미터 b를 곱하기 때문입니다.

158
00:10:47,485 --> 00:10:48,910
마무리 짓자면,

159
00:10:48,910 --> 00:10:51,970
이전 슬라이드에서 보아 듯이

160
00:10:51,970 --> 00:10:55,645
이것은 레이어 l을 사용하여 정의된 스타일 비용 함수입니다.

161
00:10:55,645 --> 00:11:02,440
이것은 기본적으로 이미지 s와 이미지 G에서 계산 된

162
00:11:02,440 --> 00:11:05,953
두 개의 스타일 매트릭스 사이의 Frobenius 표준입니다.

163
00:11:05,953 --> 00:11:10,810
Frobenius의 F, 그리고 제곱을 합니다. 이것은 그다지 중요하지 않은 단지 낮은 정규화 상수에 의해서가 아닙니다.

164
00:11:10,810 --> 00:11:13,255
중요하지 않은 정규화 상수

165
00:11:13,255 --> 00:11:18,400
그리고 마지막으로, 만약 여러분이 여러 개의 다른 레이어로부터 스타일 비용 함수를 사용한다면

166
00:11:18,400 --> 00:11:23,443
시각적으로 더 훌륭한 결과를 얻을 수 있습니다.

167
00:11:23,443 --> 00:11:27,095
따라서, 여러분이 정의할 수 있는

168
00:11:27,095 --> 00:11:31,305
전체 스타일 비용 함수는

169
00:11:31,305 --> 00:11:37,640
모든 다른 레이어들의 합, 그 레이어의 스타일 비용 함수,

170
00:11:37,640 --> 00:11:41,820
몇몇 파라미터 세트로,

171
00:11:41,820 --> 00:11:44,160
추가적인 하이퍼 파라미터로 가중치,

172
00:11:44,160 --> 00:11:46,895
여기에 람다 ㅣ로 표시하여 가중치를 주어 정의 내려야 합니다.

173
00:11:46,895 --> 00:11:51,595
따라서 이것이 하는 일은 여러분이 신경망에서 다른 레이어를 사용하도록 하는 것입니다.

174
00:11:51,595 --> 00:11:52,815
앞의 것들은

175
00:11:52,815 --> 00:11:55,800
모서리와 같은 비교적 간단한 낮은 레벨 피처를

176
00:11:55,800 --> 00:11:59,050
측정합니다. 또한, 후기 레이어에서는,

177
00:11:59,050 --> 00:12:03,000
높은 수준의 피처를 측정하고, 스타일을 계산할 때 신경망이

178
00:12:03,000 --> 00:12:08,475
낮은 레벨과 높은 레벨의 상관 관계를 모두 고려하도록 합니다.

179
00:12:08,475 --> 00:12:10,845
다음 연습에서는

180
00:12:10,845 --> 00:12:13,980
이 유형의 매개 변수 람다에 대해 합리적인 선택이 될 수 있는 것에 대해

181
00:12:13,980 --> 00:12:19,080
더 많은 직관을 얻을 수 있을 것입니다.

182
00:12:19,080 --> 00:12:20,790
정리해보면,

183
00:12:20,790 --> 00:12:24,660
여러분은 이제 전반적인 비용 함수를 정의 내릴 수 있습니다.

184
00:12:24,660 --> 00:12:30,720
알파 곱하기 C와 G 사이의 컨텐츠 비용,

185
00:12:30,720 --> 00:12:37,515
더하기, 베타 곱하기 S와 G 사이의 스타일 비용. 이렇게 하면

186
00:12:37,515 --> 00:12:40,785
정교한 방법으로 최적화된 알고리즘을 얻을 수 있습니다.

187
00:12:40,785 --> 00:12:44,696
그럼 정규화된 이미지 G를 찾을 수 있고,

188
00:12:44,696 --> 00:12:49,590
이 비용함수 J의 (G)를 최소화 할 수 있습니다. 그리고 이렇게 하시면

189
00:12:49,590 --> 00:12:53,730
멋져 보이는 신경 예술 작품을 만들어낼 수 있습니다.

190
00:12:53,730 --> 00:12:59,220
또한, 이렇게 하시면 예쁘고 멋진 신기한 예술 작품을 만들어 낼 수 있을 것입니다.

191
00:12:59,220 --> 00:13:02,010
신경 스타일 변형은 여기까지 입니다.

192
00:13:02,010 --> 00:13:05,235
이번 주 프린팅 연습에서 즐겁게 실행해보는 시간 가지시길 바랍니다.

193
00:13:05,235 --> 00:13:06,625
이번 주를 마무리 하기 전에,

194
00:13:06,625 --> 00:13:08,575
한 가지 공유 드리고 싶은 점은,

195
00:13:08,575 --> 00:13:11,100
2D 이미지에서뿐 아니라 1D 혹은 3D 에서

196
00:13:11,100 --> 00:13:17,000
어떻게 컨볼루션을 할 수 있는가 입니다. 다음 강의로 가보시죠.