ニューラルネットワークに そのパラメータを学ばせ 顔写真を上手に符号化する方法の１つは Triplet Loss 関数に勾配降下法を適用することだ それがどういうことか、見てみましょう Triplet Loss を適用するには 画像のペアを比較する必要がある 例えば この写真で ニューラルネットワークのパラメータを学習するには 同時に いくつかの写真を見なければならない 例えば この画像のペアでは これらは 同じ人だから 同じような符号にしたい 一方 この画像のペアでは これらは 違う人だから 全く異なる符号化をしたい Triplet Loss の用語では １つのアンカー画像を常に見ることになる そして アンカーと 正の画像間の 距離を出したい 正の例とは 同じ人と言えるくらい似ているということだ 一方 アンカーと比較されるペアが 負の場合は その距離は大きく離れることになる これが Triplet Loss という用語の元だ 常に ３つの画像を一度に見る アンカー画像を見て 正の画像を見て 同じく 負の画像も見る アンカー 正 そして 負 を短縮して A P そして N とする これを形式化するのに 符号化用ニューラルネットワークの パラメータには こんな特性が欲しい 欲しいのは アンカーの符号 引く 正の例の符号 これを小さくしたい 特に アンカーの符号と 負の例の符号 間の２乗ノルム の距離以下にしたい ここで これは もちろん d(A, P) で これは d(A, Ｎ) だ
d は 距離関数と考えることができる だから それに アルファベットの d という名を付けた この式の 右の項を 左へ動かすと 結局 |f(A) - f(P)|^2 - 右の項となる |f(A) - f(N)|^2 これを 0 以下にしたい ただし この式を少し変更しよう これを満たすことを確実にするための ちょっとした変更だ もし 全てを０と学んでしまうと もし f が常に０と等しければ これは 0 - 0 で これは 0
これは 0 - 0 で 0 そして どの画像の f も 全てが０のベクトルなら この式を ほぼ自明に満たすことができる だから ニューラルネットワークが 全ての符号に単に０を出さないことを保証したい だから 全ての符号が お互いに同じにならないことを保証したい ニューラルネットワークが自明な出力となるもう一つの場合がある 全ての画像の符号が 他の全ての画像の符号と一致する場合 その場合は また 0 - 0 になる だから ニューラルネットワークがそうなるのを防ぐために この目的関数を修正する ただの０以下にする必要があるだけでなく ０よりも かなり小さくする必要がある 特に 負の α よりも小さくする必要があるとすれば α は 別のハイパーパラメータになる そうして ニューラルネットワークが自明な解を出力するのを防ぐ そして 慣例では 通常 ここに + α と書いて - α とはしない これは マージンとも呼ばれる この用語は サポートベクターマシンの文献を見たことがあるなら 馴染みがあるだろう もし そうじゃないなら 気にしなくていい そして マージン パラメータを加えたのに加えて この式をさらに修正できる 例を挙げると マージンを 0.2 にしたとする この例では アンカーと正の d は 0.5 だとしたら そうしたら 条件を満たせない場合がある もしも アンカーと負の d が ちょっとだけ大きかったら 例えば 0.51 0.51 は 0.5 より大きいけれど 十分じゃない d(A, N) は d(A, P) よりも とても大きくしたい 特に 少なくとも これは 0.7以上にしたい 言い換えれば このマージン このギャップを 少なくとも 0.2にするには これを上げて これを下げて 少なくとも この α のギャップがあるようにする ハイパーパラメータ α 0.2 は アンカーと正の間の距離 と アンカーと負の間の距離 の差だ これが マージンパラメータをここに持つ意味だ それは アンカーと正のペア と アンカーと負のペア を互いに引き離す この 下にある 式を 次のスライドで 形式化して Triplet Loss 関数を定義しよう Triplet Loss 関数は ３つの画像で定義される ３つの画像で A P そして N アンカー 正の例 そして 負の例 正の例は アンカーと同じ人ので 負の例は アンカーとは違う人のだ 損失を次のように定義する この例の損失は ３つの画像で定義されて 最初に前のスライドのをコピーさせて そして |f(A) - f(P)|^2 - |f(A) - f(N)|^2 そして + α マージンパラメータ そして これを０以下にしたい では 損失関数を定義するのに これと０との最大値を取ろう 最大値を取る効果は これが ０以下である限りは 損失は０になる なぜなら 最大値は ０以下の何かだから この０が０になる 緑の下線を実現するという目的を達成している限りはね それを ０以下にするという目的を達成している限りはね そうなら この例の損失は０となる しかし 一方 これが０より大きいと 最大値を取ると max は この緑の下線部を選ぶ そして 正の損失が出るだろう よって これを最小化することで これを０以下にしようとする 効果がある そして ここが０以下である限りは ニューラルネットワークは これが どのくらい大きな負かは 気にしない それでは これが １Triplet の損失を定義する方法だ ニューラルネットワーク全体のコスト関数は 学習セットに跨って 異なる Triplet の 個別損失を 合計する よって もし 1,000人の10,000枚の写真の学習セットがあれば 10,000枚の写真から このような Triplet を作り それから このような コスト関数で 勾配降下法を使って アルゴリズムを訓練しなくてはならない コスト関数は 学習セットから取り出した画像の Triplet で定義される Triplet のこのデータセットを定義するために A と P のペアが必要だ 同じ人の写真のペアだ システムを訓練する目的のために 同じ人の 複数の写真があるデータセットが必要だ だから この例では 1,000人の10,000枚の写真と言った 1,000人それぞれが 平均で 1人当たり10枚の写真を持ち 全データセットを作っている １人当たり １枚の写真しか無い場合は このシステムを訓練できない ただし 訓練した後は これを適用することになる ただし システムを訓練した後なら もちろん これを適用できる One Shot Learning 問題に 顔認識システムでは 認識しようとしている誰かの写真は １つしかない しかし 学習セットでは 同じ人の複数の画像があることを保証する必要がある 少なくとも 学習セットの中の何人かは そうなれば アンカーと正の画像のペアができる さて 実際には どうやって これらの Triplet を学習セットから選ぶのか？ １つの問題は A P そして N を 学習セットから ランダムに選ぶ場合 A と P が同じ人になるように A と N が違う人になるようにだ 問題の１つは もし それらをランダムに選ぶ場合は この制約は とても簡単に満たせてしまう なぜなら ２つのランダムに選んだ人々の写真では A と N が A と P よりも大きく異なる確率が高い まだ この表記を覚えているといいけど この d(A, P) は 最後の数枚のスライドで この符号化のために 使った これは これと同じで 前のスライドで使ったのは 符号間の２乗ノルム距離だ しかし A と N が ランダムに選ばれた人の２つだったら その場合は とても高い確率で これが 左にあるマージン α よりも とても大きくなる そうすると ニューラルネットワークは そこから 多くを学ばなくなる よって 学習セットを作るために Triplet A P そして N は 学習し難いもの にしたい 特に この制約を満たす全ての Tripletに対し 難しい Triplet は A P そして N を選ぶ場合 d(A, P) が d(A, N) に とても近いだろう よって この場合 学習アルゴリズムは とても苦労して 正しく ことを行い これを上げて 左のは下げる そうしたら 少なくとも 左側と右側の間は マージン α になる これらの Triplet を選ぶ効果は 学習アルゴリズムの計算効率を上げることだ もし Triplet をランダムに選ぶと 非常に多くの Triplet は 簡単で 勾配降下法は 何もしない なぜなら ニューラルネットワークは 全ての場合で とても良い解を得てしまうからだ 難しい Triplet を使うことによってのみ 勾配降下法は この値を この値から引き離そうと 何らかの仕事をせざるを得なくなる もし 興味があるなら 詳細は この論文にある Florian Schroff, Dmitry Kalinichenko, そして James Philbin によるもので かれらが FaceNet と呼んだものだ このビデオで説明したアイデアの多くは そこから来ている ところで 面白い事実がある ディープラーニングの世界では しばしば アルゴリズムは こんな風に命名される もし ある領域で働いているなら それを "何々" と呼ぼう すると "何々Net" もしくは "Deep何々" と呼ばれるシステムになる 顔認識について話してきた だから この論文は FaceNet と呼ばれる そして 前回のビデオでは Deep Face を見た でも "何々Net" や "Deep何々" というアイデアは ディープラーニングの世界でアルゴリズムを命名する とても多いやり方だ そして 気軽に 論文を見てほしい アルゴリズムを速くするための その他の詳細を 何か学びたいのなら それは 学習に使うための 最も有益な Triplet の選び方だ これは いい論文だ まとめよう Triplet Loss で学習する 学習セットから 沢山の Triplet を組み合わせて作る 必要がある ここに Triplet があり アンカーと正がある 両者は 同じ人で 異なる人のは負だ ここに 別の アンカーがある 正は 同じ人で アンカーと負は 違う人だ 行うのは このような学習セットを定義することだ アンカー 正 そして 負 のTriplet に 勾配降下法を使い 前のビデオで定義した コスト関数 J を最小化する 逆伝播の作用で ニューラルネットワークの全てのパラメータを 学び 符号化して ２画像が同じ人なら その２画像の d は小さくなる それらが 違う人なら 大きくなる これが Triplet Loss だ ニューラルネットワークを 顔認識の学習と符号化のために 訓練する方法だ さて 周知されていることに 商業利用の顔認識システムは とても巨大なデータセットで訓練してあるということがある 現時点では しばしば 100万画像を使い 時々 1,000万画像を使い ある商業企業 は 1億を超える画像を使っていると言っている これは 現在の標準においても 非常に巨大なデータセットだ Triplet Loss と それを使って ニューラルネットワークを訓練し 顔認識用に良い符号化を行う方法は 以上だ そして 現在の顔認識システムは 特に 大規模な商用顔認識システムは 巨大なデータセットで訓練されていることが分かった 100万画像のデータセットでさえ 普通じゃないけど ある企業は 1,000万画像を使っていて ある企業は １億画像で これらのシステムを訓練している 現在の標準からしても これらは 非常に巨大なデータセットだ これらのデータセットは 簡単には得られない 幸運なことに これらの企業のいくつかは これらの巨大なネットワークを訓練して オンラインにパラメータを上げている だから スクラッチで これらのネットワークを訓練しようとするよりも その共有するデータ量ゆえに しばしば 有益であるのは 誰か他の人の 訓練済みモデルをダウンロードすることだ 自分自身で スクラッチで全てをやるより しかし 誰か他の人の訓練済みモデルをダウンロードしても どのように これらのアルゴリズムが訓練されたか知ることは 有益だ これらのアイデアを スクラッチで 自分自身で 何かのアプリケーションに必要とする場合にね Triplet Loss は以上だ 次のビデオでは 他の種類の シャム ネットワークを いくつか見せたい そして どのように それらを訓練するかを 次のビデオに進みましょう