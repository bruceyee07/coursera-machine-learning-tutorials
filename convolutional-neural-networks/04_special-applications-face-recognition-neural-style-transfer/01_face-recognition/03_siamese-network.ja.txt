最後のビデオで学んだ 関数 d の仕事は ２つの顔を入力して どのくらい似ているか 違っているかを 言うものだ これをやるのにいいのは シャム ネットワークを使うことだ 次の例をみてみましょう。 このような ConvNet の絵をよく見るよね 画像を入力して それを x(1) としよう 一連の 畳み込み層や プーリング層 そして 全結合層を通して このような特徴ベクトルを得る 時々 これは ソフトマックスに喰わせて 分類を出す このビデオでは これは 使わない 代わりに このベクトルに注目する それを128個の数としよう ネットワークの深い場所にある全結合層が計算するものだ この 128個の数に 名前を付けよう これを f(x(1)) と呼ぶ f(x(1)) は 入力画像 x(1) を符号化したものだと考える ここの入力画像 Kianの写真が 128個の数として 再表現される 顔認識システムを作るには ２つの写真を比較したい 例えば この最初の写真と この２番目の写真があるなら ２番目の写真を 同じパラメータを持つ 同じニューラルネットワークに喰わせて 異なるベクトルの 128個の数を得る それは この２番目の写真を表す もしくは 符号化したものだ この２番目の写真を この２番目の写真を符号化したものを f(x(2)) と呼ぶことにする ここに x(1) と x(2) という２つの画像を表すものがある それらは 必ずしも 学習セットの中で １番目 ２番目になっている必要は無い どの２つの写真でもよい 結局 これら 符号化したものは ２つの画像の良い表現だと言える それから やるのは x(1) と x(2) の距離 d を定義することだ 違いのノルムとしてね この２つの画像を符号化したものの違いのノルムだ この２つの同じ 畳み込みニューラルネットワークを ２つの異なる入力で走らせて 比較するというアイデアは 時々 シャム ネットワーク構造と呼ばれる それから ここに挙げた多くのアイデアは この論文から来ている Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, そして Lior Wolf の論文で 彼らが発展させたシステムでは Deep Face と呼んでいた それから ここに挙げた多くのアイデアは Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, そして Lior Wolf の論文から来ていて 彼らが発展させたシステムでは Deep Face と呼んだ では どうやって シャム ネットワークを訓練するか？ この２つのニューラルネットワークは 同じパラメータを持つ 本当にやりたいのは このニューラルネットワークを訓練して これを符号化して 関数 d で結果を計算して ２つの写真が同じ人か判断することだ 形式上 ニューラルネットワークのパラメータが 符号化 f(x(i)) を定義する どんな画像 x(i) が与えられても ニューラルネットワークは この 128次元の符号 f(x(i)) を出力する 形式上 やりたいことは パラメータを学んで もし ２つの写真 x(i) と x(j) が同じ人なら この符号間の距離を小さくしたい 前のスライドでは x(1) と x(2) を使っていたけど それは 実際には 学習セットの どの x(i) と x(j) のペアにもなり得る 対照的に x(i) と x(j) が違う人なら この符号間の距離を大きくしたい ニューラルネットワークの全ての層のパラメータを変化させて 異なる符号を得る そして 誤差逆伝播を使い この全てのパラメータを変化させて この条件を満たすようにする さぁ シャム ネットワーク構造について学んだ ニューラルネットワークに出力させたいものの感覚は掴んだだろう 良い符号化を行うものについてはね しかし 実際に目的関数を定義するには どうする？ ニューラルネットワークに ここで議論したことをさせるには？ 次のビデオで Triplet Loss 関数を使う そのやり方を見よう