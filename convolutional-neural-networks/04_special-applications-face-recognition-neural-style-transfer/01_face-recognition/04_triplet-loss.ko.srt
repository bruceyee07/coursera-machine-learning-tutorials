1
00:00:00,000 --> 00:00:04,650
신경망의 파라미터를 학습하여

2
00:00:04,650 --> 00:00:07,875
얼굴 그림을 위한 훌륭한 인코딩을 제공하는 한 가지 방법은

3
00:00:07,875 --> 00:00:11,930
triplet loss function(삼중 항 손실 함수)에 적용된 기울기 강하를 정의하는 것입니다. 이게 무슨 뜻인지 보시죠.

4
00:00:11,930 --> 00:00:13,425
이게 무슨 말인지 한번 보도록 하겠습니다.

5
00:00:13,425 --> 00:00:15,341
삼중 항 손실을 적용하려면

6
00:00:15,341 --> 00:00:18,630
이미지 쌍을 비교해야 합니다.

7
00:00:18,630 --> 00:00:21,365
예를 들어, 주어진 이 그림에서,

8
00:00:21,365 --> 00:00:24,075
신경망의 파라미터를 배우려면

9
00:00:24,075 --> 00:00:27,605
동시에 여러 그림을 봐야 합니다.

10
00:00:27,605 --> 00:00:29,685
예를 들어, 이 이미지 쌍이 주어지면

11
00:00:29,685 --> 00:00:33,895
이들은 같은 사람 때문에, 인코딩이 동일해야 합니다.

12
00:00:33,895 --> 00:00:35,925
반면, 이 이미지 쌍이 주어지면

13
00:00:35,925 --> 00:00:41,500
이들은 다른 사람이기 때문에 인코딩이 상당히 달라지게 됩니다.

14
00:00:41,500 --> 00:00:44,770
삼중 손실의 용어적으로,

15
00:00:44,770 --> 00:00:49,290
여러분이 할 일은, 언제나 하나의 앵커 이미지를 보는 것입니다.

16
00:00:49,290 --> 00:00:53,820
그리고 나서 앵커와 포지티브 이미지 사이에 거리를 주는 겁니다.

17
00:00:53,820 --> 00:00:55,200
이건 정말 포지티브 예시이죠,

18
00:00:55,200 --> 00:00:58,025
같은 사람이긴 하지만, 비슷하다는 뜻입니다.

19
00:00:58,025 --> 00:01:01,470
반면에, 이미지 쌍들이 네거티브 예시와 비교될 때는,

20
00:01:01,470 --> 00:01:07,455
앵커가 그들과 훨씬 더 많이 떨어져서 거리를 가지게 해야 합니다.

21
00:01:07,455 --> 00:01:10,755
이것은 한 번에 세 개의 이미지를 항상 보게 된다는 의미의 용어인

22
00:01:10,755 --> 00:01:15,145
'삼중 항 손실'을 가져오는 것이죠.

23
00:01:15,145 --> 00:01:17,415
여러분은 네거티브 이미지뿐 아니라

24
00:01:17,415 --> 00:01:22,185
앵커 이미지, 포지티브 이미지를 보게 될 것입니다.

25
00:01:22,185 --> 00:01:25,993
이제 앵커 양수와 음수를

26
00:01:25,993 --> 00:01:29,561
A, P, N으로 축약하겠습니다. 이렇게 공식화하기 위해서

27
00:01:29,561 --> 00:01:32,700
여러분이 원하는 것은 인코딩의 신경망 파라미터가.

28
00:01:32,700 --> 00:01:36,060
다음 속성을 갖도록 하는 것입니다

29
00:01:36,060 --> 00:01:39,735
앵커 인코딩에서

30
00:01:39,735 --> 00:01:44,445
포지티브 예제의 인코딩을 뺀 값을

31
00:01:44,445 --> 00:01:47,865
작게 하고, 특히

32
00:01:47,865 --> 00:01:52,950
앵커의 인코딩과 음수의 인코딩 사이의

33
00:01:52,950 --> 00:01:58,900
제곱 된 norm의 거리보다 작거나 같게 하려면

34
00:01:58,900 --> 00:02:02,485
물론 이것은 d의 A, P이고

35
00:02:02,485 --> 00:02:06,610
이것은 d의 A, N입니다

36
00:02:06,610 --> 00:02:11,280
여기서 d는 거리 함수로 생각할 수 있습니다.

37
00:02:11,280 --> 00:02:14,700
알파벳 d로 시작하므로 d라고 이름을 붙였습니다.

38
00:02:14,700 --> 00:02:18,825
이제 이 방정식의 오른쪽에서 왼쪽으로 이동하면

39
00:02:18,825 --> 00:02:25,140
llf(A)-f(P)ll의 제곱, 빼기,

40
00:02:25,140 --> 00:02:27,675
오른쪽을 가보면,

41
00:02:27,675 --> 00:02:31,404
llf(A) - f(N)ll 이고

42
00:02:31,404 --> 00:02:34,800
이것이 0 보다 작거나 같게 하면 됩니다.

43
00:02:34,800 --> 00:02:38,280
그러나 이제, 우리는 이 표현에 약간의 변화를 줄 텐데요

44
00:02:38,280 --> 00:02:41,420
이것은 이 표현이 만족되는지 확인하는 간단한 방법이며

45
00:02:41,420 --> 00:02:44,245
모든 것이 0과 같다는 것을 배우는 것 입니다.

46
00:02:44,245 --> 00:02:46,275
f가 항상 0이면,

47
00:02:46,275 --> 00:02:47,955
0 빼기 0 은 0 이고,

48
00:02:47,955 --> 00:02:50,545
0 빼기 0 은 0 이고

49
00:02:50,545 --> 00:02:57,140
어떤 이미지의 f가 모두 0의 벡터라면,

50
00:02:57,140 --> 00:03:01,410
여러분은 거의 이 방정식을 만족시킬 수 있습니다.

51
00:03:01,410 --> 00:03:07,465
따라서 신경망이 모든 인코딩에 대해 0을 출력하지 않도록 하기 위해

52
00:03:07,465 --> 00:03:12,480
모든 인코딩을 서로 동일하게 설정되지 않도록 해야 합니다.

53
00:03:12,480 --> 00:03:16,285
신경망에 대한 또 다른 방법은

54
00:03:16,285 --> 00:03:20,246
모든 이미지의 인코딩이 다른 모든 이미지의 인코딩과 동일한 지 여부입니다.

55
00:03:20,246 --> 00:03:25,400
이 경우 다시 0 빼기 0 이 되므로

56
00:03:25,400 --> 00:03:28,075
신경망이 그러한 것을 하지 못하도록 하기 위해

57
00:03:28,075 --> 00:03:32,370
우리가 할 일은, 이 목표를 수정하여

58
00:03:32,370 --> 00:03:36,985
이것이 0보다 작거나 같을 필요 없다고 알려주는 것입니다.

59
00:03:36,985 --> 00:03:40,575
0보다 약간 작아야 합니다.

60
00:03:40,575 --> 00:03:45,755
따라서, 특히 negative alpha보다 작을 필요가 있다고 하면

61
00:03:45,755 --> 00:03:49,235
알파는 또 다른 하이퍼 파라미터인데요,

62
00:03:49,235 --> 00:03:53,475
이것은 신경망이 trivial solution를 출력하지 못하게 합니다.

63
00:03:53,475 --> 00:03:55,230
관례 상, 일반적으

64
00:03:55,230 --> 00:03:59,550
우리는 음수 알파 대신, 이쪽에 플러스 알파라고 씁니다.

65
00:03:59,550 --> 00:04:02,763
그리고 이것을 margin (마진)이라 부르는데,

66
00:04:02,763 --> 00:04:05,190
이것은 지지 벡터 기계에 대한 문헌을 보았다면,

67
00:04:05,190 --> 00:04:10,320
익숙한 용어이겠지만, 그렇지 않은 경우라 해도

68
00:04:10,320 --> 00:04:12,675
걱정하지 않아도 됩니다.

69
00:04:12,675 --> 00:04:17,995
또한 이 마진 파라미터를 추가하여, 이 위에 있는 수색을 수정할 수도 있습니다.

70
00:04:17,995 --> 00:04:19,395
예를 들어

71
00:04:19,395 --> 00:04:23,260
마진을 0.2로 설정했다고 합시다.

72
00:04:23,260 --> 00:04:24,855
이 예에서

73
00:04:24,855 --> 00:04:29,550
앵커의 d와 포지티브가 0.5 인 경우

74
00:04:29,550 --> 00:04:32,580
앵커와 네거티브 사이의 d가 조금 더 크면, 예를 들어 0.51 이면

75
00:04:32,580 --> 00:04:37,087
이는 만족되지 않을 것입니다

76
00:04:37,087 --> 00:04:41,610
0.51이 0.5보다 크더라도,

77
00:04:41,610 --> 00:04:43,917
충분하지 않다는 것을 말하고 있습니다.

78
00:04:43,917 --> 00:04:47,610
d(A, N) 이 d(A, P) 보다 훨씬 큰 것을 원하고 있죠.

79
00:04:47,610 --> 00:04:49,245
특히나,

80
00:04:49,245 --> 00:04:53,520
우린 이것이 적어도 0.7 이거나 혹은 그 이상이 되기를 원합니다.

81
00:04:53,520 --> 00:04:58,740
이 마진 또는 이 간격을 0.2 이상으로 설정하려면

82
00:04:58,740 --> 00:05:02,330
이 값을 밀어 올리거나 아래로 밀어서

83
00:05:02,330 --> 00:05:06,305
이 알파 값의 이 갭,

84
00:05:06,305 --> 00:05:09,350
즉, 앵커와 포지티브 대 앵커와 네거티브 이 둘 사이의 거리인

85
00:05:09,350 --> 00:05:14,530
하이퍼 파라미터 알파가 0.2 가 되도록 하면 됩니다.

86
00:05:14,530 --> 00:05:17,491
여기에 마진 파라미터가 하는 일입니다.

87
00:05:17,491 --> 00:05:19,055
이것은 앵커 포지티브 쌍과 앵커 네거티브 쌍을

88
00:05:19,055 --> 00:05:25,720
서로서로 더 멀어질 수 있도록 밀어내는 것이죠.

89
00:05:25,720 --> 00:05:29,435
자, 여기 아래의 방정식과 다음 슬라이드를 봅시다.

90
00:05:29,435 --> 00:05:31,160
다음 슬라이드에서

91
00:05:31,160 --> 00:05:35,225
그것을 공식화하고, 삼중 항 손실 함수를 정의 내려봅시다.

92
00:05:35,225 --> 00:05:40,810
삼중 항 손실 함수는 이미지의 세 개에 정의됩니다

93
00:05:40,810 --> 00:05:43,465
그래서, 세 이미지,

94
00:05:43,465 --> 00:05:46,350
A, P, N 이 주어지면

95
00:05:46,350 --> 00:05:48,990
앵커, 포지티브와 네거티브의 예시들입니다.

96
00:05:48,990 --> 00:05:53,245
포지티브 예시는 앵커와 같은 사람의 것이지만

97
00:05:53,245 --> 00:05:58,040
네거티브는 앵커와 다른 사람입니다.

98
00:05:58,040 --> 00:06:01,515
손실을 다음과 같이 정의 할 것입니다.

99
00:06:01,515 --> 00:06:03,055
이 예제에서 손실은,

100
00:06:03,055 --> 00:06:06,465
실제로 세 개의 이미지에서 정의되어 있습니다.

101
00:06:06,465 --> 00:06:10,169
이전 슬라이드에서 봤던 것을 우선 복사하겠습니다.

102
00:06:10,169 --> 00:06:16,045
그래서 llf(A) - f(N)ll 의 제곱에서

103
00:06:16,045 --> 00:06:23,790
llf(A) - f(N)ll 의 제곱을 뺀 것입니다.

104
00:06:23,790 --> 00:06:26,755
그리고 나서 알파, 즉 마진 파라미터를 더하면

105
00:06:26,755 --> 00:06:31,600
이 부분이 0 보다 작거나 같아야만 합니다.

106
00:06:31,600 --> 00:06:34,365
따라서 손실 함수를 정의하려면

107
00:06:34,365 --> 00:06:39,040
이 값과 0 사이의 최대 값을 취해 봅시다.

108
00:06:39,040 --> 00:06:42,030
따라서 여기에서 최대 값을 취하는 효과는

109
00:06:42,030 --> 00:06:45,225
0보다 작으면.

110
00:06:45,225 --> 00:06:47,070
손실은 0입니다

111
00:06:47,070 --> 00:06:49,847
왜냐하면, 최대 값은 0 보다 작거나 같으며

112
00:06:49,847 --> 00:06:52,705
0 이 0 일 될 때이기 때문입니다

113
00:06:52,705 --> 00:06:56,370
초록색으로 밑줄 그어 놓은 목표를 달성하는 한,

114
00:06:56,370 --> 00:07:00,950
여러분이 0보다 작거나 같게 만드는 목표를 달성했다면

115
00:07:00,950 --> 00:07:04,150
이 예제의 손실은 0이됩니다.

116
00:07:04,150 --> 00:07:05,355
그러나 다른 한편으로

117
00:07:05,355 --> 00:07:07,650
이것이 0보다 클 때,

118
00:07:07,650 --> 00:07:09,120
최대 값을 취하면,

119
00:07:09,120 --> 00:07:10,665
우리가 선택한 최대값,

120
00:07:10,665 --> 00:07:12,343
이는 녹색으로 밑줄 그은 이 부분이므로

121
00:07:12,343 --> 00:07:15,455
포지티브 손실을 보게 됩니다

122
00:07:15,455 --> 00:07:17,475
따라서 이것을 최소화하려고 시도하면

123
00:07:17,475 --> 00:07:22,130
이 값을 0보다 작거나 같게

124
00:07:22,130 --> 00:07:23,450
보내려고 하는 효과가 있습니다.

125
00:07:23,450 --> 00:07:26,550
그리고 영 (0) 이하이거나 0과 같을 때,

126
00:07:26,550 --> 00:07:31,980
신경망은 그것이 얼마나 더 네거티브한지는 신경 쓰지 않습니다.

127
00:07:31,980 --> 00:07:33,990
따라서, 이것은 단일 삼중에 대한 손실을 정의하는 방법이며,

128
00:07:33,990 --> 00:07:38,970
신경망에서 전체 비용 함수는

129
00:07:38,970 --> 00:07:47,575
서로 다른 트리플렛상에서 개별 손실의 트레이닝 세트에 대한 총액이 될 것입니다.

130
00:07:47,575 --> 00:07:55,080
그래서 1,000 명의 사람을 사진 찍어서, 10,000 장의 사진의 트레이닝 세트를 가지고 있다면

131
00:07:55,080 --> 00:08:00,360
여러분이 해야 할 일은 10,000장의 사진을 찍어서, 그것을 이런 트리플렛을 생성하고

132
00:08:00,360 --> 00:08:03,720
선택한 후, 이 비용함수 유형에서 기울기 강하를 사용하여

133
00:08:03,720 --> 00:08:08,005
학습 알고리즘을 훈련시키는 것입니다.

134
00:08:08,005 --> 00:08:14,145
이는 실제로 트레이닝 세트에서 가져온 이미지의 세 장으로 정의됩니다..

135
00:08:14,145 --> 00:08:19,635
이 세 장의 데이터 집합을 정의하려면

136
00:08:19,635 --> 00:08:25,405
A와 P의 몇 쌍이 필요합니다. 같은 사람의 그림 쌍이 필요하죠.

137
00:08:25,405 --> 00:08:27,510
시스템을 훈련할 목표를 위해서는,

138
00:08:27,510 --> 00:08:32,600
같은 사람의 사진을 여러 개 가지고 있는 데이터 세트가 필요합니다.

139
00:08:32,600 --> 00:08:33,960
때문에 이 예에서

140
00:08:33,960 --> 00:08:38,040
1,000 명의 다른 사람의 사진이 10,000 개라면

141
00:08:38,040 --> 00:08:40,965
1,000 명당 평균 10 장의 사진을 가지고

142
00:08:40,965 --> 00:08:44,310
전체 데이터 집합을 구성한다고 말씀 드렸습니다.

143
00:08:44,310 --> 00:08:47,085
각 사람의 사진이 하나뿐이라면

144
00:08:47,085 --> 00:08:49,725
이 시스템을 실제로 훈련 할 수 없습니다.

145
00:08:49,725 --> 00:08:52,080
물론 교육을 한 후에,

146
00:08:52,080 --> 00:08:54,205
만약 여러분이 이것을 적용한다면,

147
00:08:54,205 --> 00:08:56,565
물론 시스템을 훈련한 후에,

148
00:08:56,565 --> 00:08:58,200
여러분의 얼굴 인식 시스템을 위한 원샷 학습 문제에

149
00:08:58,200 --> 00:09:02,685
그것을 적용 할 수 있습니다.

150
00:09:02,685 --> 00:09:06,945
어쩌면 여러분이 인식하려고 하는 사람의 사진을 단 한장만 가지고 있을지도 모릅니다.

151
00:09:06,945 --> 00:09:08,335
하지만 여러분의 트레이닝 세트에서는

152
00:09:08,335 --> 00:09:12,780
앵커 이미지와 포지티브 이미지가 쌍을 이룰 수 있도록

153
00:09:12,780 --> 00:09:14,940
트레이닝 세트에 있는 적어도 몇몇 사람들만이라도

154
00:09:14,940 --> 00:09:19,000
동일한 사람의 이미지가 여러 개 있는지 확인해야 합니다.

155
00:09:19,000 --> 00:09:25,240
자, 실제로 트레이닝 세트를 구성하려면 어떻게 이 트리플렛을 골라야 할까요?

156
00:09:25,240 --> 00:09:28,635
문제점 중의 하나는,

157
00:09:28,635 --> 00:09:34,260
A, P 는 같은 사람이고, A, N 은 다른 사람이라는 전제하에

158
00:09:34,260 --> 00:09:36,270
A, P, N 을 트레이닝 세트에서 무작위로 고르면

159
00:09:36,270 --> 00:09:40,140
문제점은 무작위로 고르면

160
00:09:40,140 --> 00:09:44,160
이 제약 조건을 매우 쉽게 만족할 수 있습니다.

161
00:09:44,160 --> 00:09:48,405
두 개의 무작위로 선택된 사람들의 그림이 주어 졌기 때문에

162
00:09:48,405 --> 00:09:51,945
A와 P 보다 A와 N은 아주 많이 다를 가능성을 가지고 있습니다.

163
00:09:51,945 --> 00:09:55,740
이 표기법을 여전히 기억하고 있기를 바라는데요,

164
00:09:55,740 --> 00:10:02,710
이 d(A, P)는 인코딩으로 작년 슬라이드에서 썼던 것입니다.

165
00:10:02,710 --> 00:10:06,190
이것은 지난 슬라이드에서 본 인코딩사이의

166
00:10:06,190 --> 00:10:11,040
제곱 된 norm의 값과 같습니다.

167
00:10:11,040 --> 00:10:14,640
그러나 A와 N이 무작위로 선택된 두 사람이라면

168
00:10:14,640 --> 00:10:17,610
왼쪽에 있는 마진 알파보다

169
00:10:17,610 --> 00:10:21,630
훨씬 더 큰 확률을 가질 가능성이 매우 높습니다.

170
00:10:21,630 --> 00:10:24,405
따라서, 신경망은 이것으로부터 많은 것을 배울 수 없습니다.

171
00:10:24,405 --> 00:10:25,940
따라서 훈련 세트를 구성하기 위해

172
00:10:25,940 --> 00:10:28,340
여러분이 할 일은 훈련하기 어려운

173
00:10:28,340 --> 00:10:31,280
A, P, N 중 하나를 선택하는 것입니다.

174
00:10:31,280 --> 00:10:38,685
그래서, 특히, 여러분이 원하는 것은 이 제약이 만족되는 트리플렛이죠.

175
00:10:38,685 --> 00:10:44,995
그래서, 아마도 d (A, P)가 실제로 d (A, N)에 아주 가깝도록

176
00:10:44,995 --> 00:10:47,454
A, P, N에 대한 값을 선택한다면,

177
00:10:47,454 --> 00:10:52,550
삼중 항에게는 어려운 일일 것입니다.

178
00:10:52,550 --> 00:10:54,020
그래서 그 경우,

179
00:10:54,020 --> 00:10:57,230
학습 알고리즘은

180
00:10:57,230 --> 00:11:00,740
오른쪽에 있는 이것을 가져 가서 밀어 올리거나

181
00:11:00,740 --> 00:11:04,030
왼쪽에 있는 이 것을 가져가서 아래로 밀어 내릴 수 있게 더 열심히 노력해서

182
00:11:04,030 --> 00:11:08,430
적어도 왼쪽과 오른쪽 사이의 알파 마진이 생기도록 해야 합니다.

183
00:11:08,430 --> 00:11:11,900
이 트리플렛을 선택하는 것의 효과는

184
00:11:11,900 --> 00:11:16,460
학습 알고리즘의 계산 효율성이 높아지는 것 입니다.

185
00:11:16,460 --> 00:11:18,500
트리플렛을 무작위로 선택하면

186
00:11:18,500 --> 00:11:21,725
너무 많이 삼중 쌍이 정밀 쉬워질 것이고,

187
00:11:21,725 --> 00:11:25,970
기울기 하강은 신경망이 그것들을 올바르게 수행하기 때문에,

188
00:11:25,970 --> 00:11:27,090
기울기 하강은 꽤 언제나 아무것도 하지 않습니다.

189
00:11:27,090 --> 00:11:32,270
기울기 하강 과정이 이러한 양으로부터 이런 양을 더 멀리 떨어 드리려는 노력을 해야 하는 것은

190
00:11:32,270 --> 00:11:38,700
강한 삼중 항을 사용해서만 가능합니다.

191
00:11:38,700 --> 00:11:40,155
관심이 있으시다면,

192
00:11:40,155 --> 00:11:46,295
Florian Schroff, Dmitry Kalinichenko,

193
00:11:46,295 --> 00:11:51,305
James Philbin 에 의해 쓰여진 논문에서 자세히 설명합니다.

194
00:11:51,305 --> 00:11:55,860
FaceNet 이라는 시스템을 가지고 있는데 저 또한 여기서 나온 많은 아이디어들을 이 강의에서 보여드리고 있습니다.

195
00:11:55,860 --> 00:11:58,220
그건 그렇고, 이것은 알고리즘이 딥러닝 세계에서 어떻게 명명되는지에 대한

196
00:11:58,220 --> 00:12:02,030
재미있는 사실을 보여줍니다.

197
00:12:02,030 --> 00:12:05,810
이는 블랭크라고 부르는 어느 특정 도메인에서 작업하신다면,

198
00:12:05,810 --> 00:12:10,710
블랭크넷 혹은 딥블랭크라고 불리는 시스템을 종종 보실 수 있을 겁니다.

199
00:12:10,710 --> 00:12:13,095
우리는 얼굴 인식에 대해 이야기 해 왔습니다.

200
00:12:13,095 --> 00:12:16,123
이 논문은 FaceNet이라고 불리는 것인데요

201
00:12:16,123 --> 00:12:17,465
마지막 비디오에서

202
00:12:17,465 --> 00:12:19,910
여러분은 단지 딥페이스만 보았습니다.

203
00:12:19,910 --> 00:12:23,600
그러나 블랭크네트 또는 딥블랭크의 이 아이디어는

204
00:12:23,600 --> 00:12:28,370
딥러닝 세계에서 알고리즘을 명명하는 매우 인기 있는 방법입니다.

205
00:12:28,370 --> 00:12:32,780
그리고 가장 유용한 삼중 항을 선택하여

206
00:12:32,780 --> 00:12:34,940
알고리즘 속도를 높이기 위한 다른 세부 정보를 배우고 싶다면

207
00:12:34,940 --> 00:12:38,745
이 논문을 살펴보십시오.

208
00:12:38,745 --> 00:12:40,025
이건 좋은 논문입니다.

209
00:12:40,025 --> 00:12:41,240
이제 마무리 지어보자면,

210
00:12:41,240 --> 00:12:42,670
삼중 손실을 훈련하려면

211
00:12:42,670 --> 00:12:47,060
트레이닝 세트를 가져 와서 많은 트리플에 매핑해야합니다.

212
00:12:47,060 --> 00:12:50,550
여기에 앵커와 포지티브를 가진 트리플이 있는데요

213
00:12:50,550 --> 00:12:54,375
둘 다 같은 사람이죠, 그리고 다른 사람에 대한 네거티브가 있습니다.

214
00:12:54,375 --> 00:12:58,445
앵커와 포지티브가 같은 사람이지만,

215
00:12:58,445 --> 00:13:04,315
앵커와 네거티브가 다른 사람인 또 다른 경우가 있고, 계속 이어집니다.

216
00:13:04,315 --> 00:13:07,000
앵커 포지티브와 네거티브 삼중항의 트레이닝 세트를 가지고

217
00:13:07,000 --> 00:13:09,920
이전 슬라이드에서 정의된 비용함수 J를 최소화 하기 위해

218
00:13:09,920 --> 00:13:16,740
기울기 하강을 사용하십시오.

219
00:13:16,740 --> 00:13:20,090
그렇게 하면 인코딩을 배우기 위해서

220
00:13:20,090 --> 00:13:23,640
신경망의 모든 파라미터에 후 방향 전파되는 효과가 있을 것입니다.

221
00:13:23,640 --> 00:13:27,435
그럼으로써, 이 두 이미지가 같은 사람일 때,

222
00:13:27,435 --> 00:13:33,395
두 이미지의 d는 작아질 것이고

223
00:13:33,395 --> 00:13:40,286
이 두 이미지가 다른 사람일 때, d는 커질 것 입니다.

224
00:13:40,286 --> 00:13:43,805
삼중손실은 여기까지입니다. 그리고 어떻게 얼굴인식을 위한

225
00:13:43,805 --> 00:13:48,200
인코딩을 배우도록 신경망을 훈련할 수 있을지 알아보았습니다.

226
00:13:48,200 --> 00:13:49,715
이제,

227
00:13:49,715 --> 00:13:54,556
상업적인 얼굴 인식 시스템들은 매우 큰 데이터세트에서 훈련되는 것이 분명합니다.

228
00:13:54,556 --> 00:13:56,630
종종, 백 만장의 이미지

229
00:13:56,630 --> 00:14:00,275
때로는, 흔하진 않지만, 천만 장의 이미지도 사용하죠.

230
00:14:00,275 --> 00:14:05,210
그리고 1 억 개 이상의 이미지를 사용하는 상업 회사가 있습니다.

231
00:14:05,210 --> 00:14:09,300
이것들은 매우 큰 데이터 집합입니다.

232
00:14:09,300 --> 00:14:12,800
삼중손실은 여기까지입니다. 그리고 어떻게

233
00:14:12,800 --> 00:14:18,230
얼굴인식을 위한 좋은 인코딩을 작동시키도록 신경망을 훈련할 수 있을지 알아보았습니다.

234
00:14:18,230 --> 00:14:21,500
이제 오늘날의 얼굴 인식 시스템, 특히 손실 치료 상용 얼굴 인식 시스템이

235
00:14:21,500 --> 00:14:24,830
매우 큰 데이터 세트에 대해

236
00:14:24,830 --> 00:14:27,360
훈련된 것으로 나타났습니다.

237
00:14:27,360 --> 00:14:30,350
백만 개 이미지의 데이터세트는 드문 일이 아니며

238
00:14:30,350 --> 00:14:34,040
일부 회사는 1,000 만 개의 이미지를 사용하고

239
00:14:34,040 --> 00:14:38,135
일부 회사는 이 시스템을 교육하기 위해 1 억 개의 이미지를 가지고 있습니다.

240
00:14:38,135 --> 00:14:41,730
따라서 이들은 현대 표준에 의해서조차 도 매우 큰 데이터 세트이며,

241
00:14:41,730 --> 00:14:45,230
이러한 데이터 세트 자산은 획득하기 쉽지 않습니다.

242
00:14:45,230 --> 00:14:48,140
다행히도 일부 회사는 이러한 대규모 네트워크를 훈련하고

243
00:14:48,140 --> 00:14:51,875
파라미터를 온라인에 게시했습니다

244
00:14:51,875 --> 00:14:54,790
따라서 처음부터 이러한 네트워크 중 하나를 교육하기보다는

245
00:14:54,790 --> 00:14:59,280
공유 데이터 볼륨 사이즈로 인해

246
00:14:59,280 --> 00:15:02,390
종종 다른 사람의 기존 훈련 모델을 다운로드 하는 것이

247
00:15:02,390 --> 00:15:05,313
유용할지도 모르는 영역입니다.

248
00:15:05,313 --> 00:15:07,685
모든 걸 처음부터 혼자서 하려고 하기보다는 말이죠.

249
00:15:07,685 --> 00:15:10,130
다른 사람의 기존 훈련 모델을 다운로드 한다 해도,

250
00:15:10,130 --> 00:15:14,195
제 생각엔 알고리즘들이 어떻게 훈련되는지 배우고

251
00:15:14,195 --> 00:15:19,225
몇 응용프로그램을 위해서 처음부터 혼자서 이 모든 아이들을 응용해야만 하는 경우에 여전히 유용할 것입니다.

252
00:15:19,225 --> 00:15:21,405
삼중 손실에 대한 강의를 마쳤습니다. 다음 강의에서는

253
00:15:21,405 --> 00:15:22,640
다음 비디오에서는

254
00:15:22,640 --> 00:15:25,280
Siamese 네트워크에 대한 다른 변형과

255
00:15:25,280 --> 00:15:28,510
이러한 시스템을 교육하는 방법을 보여 드리고자 합니다. 다음 강의로 이동하시죠

256
00:15:28,510 --> 00:15:30,000
다음 비디오로 넘어가겠습니다.