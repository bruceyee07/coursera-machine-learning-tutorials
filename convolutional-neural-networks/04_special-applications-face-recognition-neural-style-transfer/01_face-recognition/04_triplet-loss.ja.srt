1
00:00:00,000 --> 00:00:04,650
ニューラルネットワークに そのパラメータを学ばせ

2
00:00:04,650 --> 00:00:07,875
顔写真を上手に符号化する方法の１つは

3
00:00:07,875 --> 00:00:11,930
Triplet Loss 関数に勾配降下法を適用することだ

4
00:00:11,930 --> 00:00:13,425
それがどういうことか、見てみましょう

5
00:00:13,425 --> 00:00:15,341
Triplet Loss を適用するには

6
00:00:15,341 --> 00:00:18,630
画像のペアを比較する必要がある

7
00:00:18,630 --> 00:00:21,365
例えば この写真で

8
00:00:21,365 --> 00:00:24,075
ニューラルネットワークのパラメータを学習するには

9
00:00:24,075 --> 00:00:27,605
同時に いくつかの写真を見なければならない

10
00:00:27,605 --> 00:00:29,685
例えば この画像のペアでは

11
00:00:29,685 --> 00:00:33,895
これらは 同じ人だから 同じような符号にしたい

12
00:00:33,895 --> 00:00:35,925
一方 この画像のペアでは

13
00:00:35,925 --> 00:00:41,500
これらは 違う人だから 全く異なる符号化をしたい

14
00:00:41,500 --> 00:00:44,770
Triplet Loss の用語では

15
00:00:44,770 --> 00:00:49,290
１つのアンカー画像を常に見ることになる

16
00:00:49,290 --> 00:00:53,820
そして アンカーと 正の画像間の 距離を出したい

17
00:00:53,820 --> 00:00:55,200
正の例とは

18
00:00:55,200 --> 00:00:58,025
同じ人と言えるくらい似ているということだ

19
00:00:58,025 --> 00:01:01,470
一方 アンカーと比較されるペアが

20
00:01:01,470 --> 00:01:07,455
負の場合は その距離は大きく離れることになる

21
00:01:07,455 --> 00:01:10,755
これが Triplet Loss という用語の元だ

22
00:01:10,755 --> 00:01:15,145
常に ３つの画像を一度に見る

23
00:01:15,145 --> 00:01:17,415
アンカー画像を見て

24
00:01:17,415 --> 00:01:22,185
正の画像を見て 同じく 負の画像も見る

25
00:01:22,185 --> 00:01:25,993
アンカー 正 そして 負 を短縮して

26
00:01:25,993 --> 00:01:29,561
A P そして N とする これを形式化するのに

27
00:01:29,561 --> 00:01:32,700
符号化用ニューラルネットワークの

28
00:01:32,700 --> 00:01:36,060
パラメータには こんな特性が欲しい

29
00:01:36,060 --> 00:01:39,735
欲しいのは

30
00:01:39,735 --> 00:01:44,445
アンカーの符号 引く 正の例の符号

31
00:01:44,445 --> 00:01:47,865
これを小さくしたい 特に

32
00:01:47,865 --> 00:01:52,950
アンカーの符号と 負の例の符号 間の２乗ノルム

33
00:01:52,950 --> 00:01:58,900
の距離以下にしたい

34
00:01:58,900 --> 00:02:02,485
ここで これは もちろん

35
00:02:02,485 --> 00:02:06,610
d(A, P) で これは

36
00:02:06,610 --> 00:02:11,280
d(A, Ｎ) だ
d は 距離関数と考えることができる

37
00:02:11,280 --> 00:02:14,700
だから それに アルファベットの d という名を付けた

38
00:02:14,700 --> 00:02:18,825
この式の 右の項を 左へ動かすと

39
00:02:18,825 --> 00:02:25,140
結局 |f(A) - f(P)|^2 -

40
00:02:25,140 --> 00:02:27,675
右の項となる

41
00:02:27,675 --> 00:02:31,404
|f(A) - f(N)|^2

42
00:02:31,404 --> 00:02:34,800
これを 0 以下にしたい

43
00:02:34,800 --> 00:02:38,280
ただし この式を少し変更しよう

44
00:02:38,280 --> 00:02:41,420
これを満たすことを確実にするための ちょっとした変更だ

45
00:02:41,420 --> 00:02:44,245
もし 全てを０と学んでしまうと

46
00:02:44,245 --> 00:02:46,275
もし f が常に０と等しければ

47
00:02:46,275 --> 00:02:47,955
これは 0 - 0 で

48
00:02:47,955 --> 00:02:50,545
これは 0
これは 0 - 0 で 0

49
00:02:50,545 --> 00:02:57,140
そして どの画像の f も 全てが０のベクトルなら

50
00:02:57,140 --> 00:03:01,410
この式を ほぼ自明に満たすことができる

51
00:03:01,410 --> 00:03:07,465
だから ニューラルネットワークが 全ての符号に単に０を出さないことを保証したい

52
00:03:07,465 --> 00:03:12,480
だから 全ての符号が お互いに同じにならないことを保証したい

53
00:03:12,480 --> 00:03:16,285
ニューラルネットワークが自明な出力となるもう一つの場合がある

54
00:03:16,285 --> 00:03:20,246
全ての画像の符号が 他の全ての画像の符号と一致する場合

55
00:03:20,246 --> 00:03:25,400
その場合は また 0 - 0 になる

56
00:03:25,400 --> 00:03:28,075
だから ニューラルネットワークがそうなるのを防ぐために

57
00:03:28,075 --> 00:03:32,370
この目的関数を修正する

58
00:03:32,370 --> 00:03:36,985
ただの０以下にする必要があるだけでなく

59
00:03:36,985 --> 00:03:40,575
０よりも かなり小さくする必要がある

60
00:03:40,575 --> 00:03:45,755
特に 負の α よりも小さくする必要があるとすれば

61
00:03:45,755 --> 00:03:49,235
α は 別のハイパーパラメータになる

62
00:03:49,235 --> 00:03:53,475
そうして ニューラルネットワークが自明な解を出力するのを防ぐ

63
00:03:53,475 --> 00:03:55,230
そして 慣例では 通常

64
00:03:55,230 --> 00:03:59,550
ここに + α と書いて - α とはしない

65
00:03:59,550 --> 00:04:02,763
これは マージンとも呼ばれる

66
00:04:02,763 --> 00:04:05,190
この用語は

67
00:04:05,190 --> 00:04:10,320
サポートベクターマシンの文献を見たことがあるなら 馴染みがあるだろう

68
00:04:10,320 --> 00:04:12,675
もし そうじゃないなら 気にしなくていい

69
00:04:12,675 --> 00:04:17,995
そして マージン パラメータを加えたのに加えて この式をさらに修正できる

70
00:04:17,995 --> 00:04:19,395
例を挙げると

71
00:04:19,395 --> 00:04:23,260
マージンを 0.2 にしたとする

72
00:04:23,260 --> 00:04:24,855
この例では

73
00:04:24,855 --> 00:04:29,550
アンカーと正の d は 0.5 だとしたら

74
00:04:29,550 --> 00:04:32,580
そうしたら 条件を満たせない場合がある もしも

75
00:04:32,580 --> 00:04:37,087
アンカーと負の d が ちょっとだけ大きかったら 例えば 0.51

76
00:04:37,087 --> 00:04:41,610
0.51 は 0.5 より大きいけれど

77
00:04:41,610 --> 00:04:43,917
十分じゃない

78
00:04:43,917 --> 00:04:47,610
d(A, N) は d(A, P) よりも とても大きくしたい

79
00:04:47,610 --> 00:04:49,245
特に

80
00:04:49,245 --> 00:04:53,520
少なくとも これは 0.7以上にしたい

81
00:04:53,520 --> 00:04:58,740
言い換えれば このマージン このギャップを 少なくとも 0.2にするには

82
00:04:58,740 --> 00:05:02,330
これを上げて これを下げて

83
00:05:02,330 --> 00:05:06,305
少なくとも この α のギャップがあるようにする

84
00:05:06,305 --> 00:05:09,350
ハイパーパラメータ α 0.2 は

85
00:05:09,350 --> 00:05:14,530
アンカーと正の間の距離 と アンカーと負の間の距離 の差だ

86
00:05:14,530 --> 00:05:17,491
これが マージンパラメータをここに持つ意味だ

87
00:05:17,491 --> 00:05:19,055
それは

88
00:05:19,055 --> 00:05:25,720
アンカーと正のペア と アンカーと負のペア を互いに引き離す

89
00:05:25,720 --> 00:05:29,435
この 下にある 式を

90
00:05:29,435 --> 00:05:31,160
次のスライドで

91
00:05:31,160 --> 00:05:35,225
形式化して Triplet Loss 関数を定義しよう

92
00:05:35,225 --> 00:05:40,810
Triplet Loss 関数は ３つの画像で定義される

93
00:05:40,810 --> 00:05:43,465
３つの画像で

94
00:05:43,465 --> 00:05:46,350
A P そして N

95
00:05:46,350 --> 00:05:48,990
アンカー 正の例 そして 負の例

96
00:05:48,990 --> 00:05:53,245
正の例は アンカーと同じ人ので

97
00:05:53,245 --> 00:05:58,040
負の例は アンカーとは違う人のだ

98
00:05:58,040 --> 00:06:01,515
損失を次のように定義する

99
00:06:01,515 --> 00:06:03,055
この例の損失は

100
00:06:03,055 --> 00:06:06,465
３つの画像で定義されて

101
00:06:06,465 --> 00:06:10,169
最初に前のスライドのをコピーさせて

102
00:06:10,169 --> 00:06:16,045
そして |f(A) - f(P)|^2

103
00:06:16,045 --> 00:06:23,790
- |f(A) - f(N)|^2

104
00:06:23,790 --> 00:06:26,755
そして + α マージンパラメータ

105
00:06:26,755 --> 00:06:31,600
そして これを０以下にしたい

106
00:06:31,600 --> 00:06:34,365
では 損失関数を定義するのに

107
00:06:34,365 --> 00:06:39,040
これと０との最大値を取ろう

108
00:06:39,040 --> 00:06:42,030
最大値を取る効果は

109
00:06:42,030 --> 00:06:45,225
これが ０以下である限りは

110
00:06:45,225 --> 00:06:47,070
損失は０になる

111
00:06:47,070 --> 00:06:49,847
なぜなら 最大値は ０以下の何かだから

112
00:06:49,847 --> 00:06:52,705
この０が０になる

113
00:06:52,705 --> 00:06:56,370
緑の下線を実現するという目的を達成している限りはね

114
00:06:56,370 --> 00:07:00,950
それを ０以下にするという目的を達成している限りはね

115
00:07:00,950 --> 00:07:04,150
そうなら この例の損失は０となる

116
00:07:04,150 --> 00:07:05,355
しかし 一方

117
00:07:05,355 --> 00:07:07,650
これが０より大きいと

118
00:07:07,650 --> 00:07:09,120
最大値を取ると

119
00:07:09,120 --> 00:07:10,665
max は

120
00:07:10,665 --> 00:07:12,343
この緑の下線部を選ぶ

121
00:07:12,343 --> 00:07:15,455
そして 正の損失が出るだろう

122
00:07:15,455 --> 00:07:17,475
よって これを最小化することで

123
00:07:17,475 --> 00:07:22,130
これを０以下にしようとする

124
00:07:22,130 --> 00:07:23,450
効果がある

125
00:07:23,450 --> 00:07:26,550
そして ここが０以下である限りは

126
00:07:26,550 --> 00:07:31,980
ニューラルネットワークは これが どのくらい大きな負かは 気にしない

127
00:07:31,980 --> 00:07:33,990
それでは これが １Triplet の損失を定義する方法だ

128
00:07:33,990 --> 00:07:38,970
ニューラルネットワーク全体のコスト関数は

129
00:07:38,970 --> 00:07:47,575
学習セットに跨って 異なる Triplet の 個別損失を 合計する

130
00:07:47,575 --> 00:07:55,080
よって もし 1,000人の10,000枚の写真の学習セットがあれば

131
00:07:55,080 --> 00:08:00,360
10,000枚の写真から

132
00:08:00,360 --> 00:08:03,720
このような Triplet を作り それから

133
00:08:03,720 --> 00:08:08,005
このような コスト関数で 勾配降下法を使って アルゴリズムを訓練しなくてはならない

134
00:08:08,005 --> 00:08:14,145
コスト関数は 学習セットから取り出した画像の Triplet で定義される

135
00:08:14,145 --> 00:08:19,635
Triplet のこのデータセットを定義するために

136
00:08:19,635 --> 00:08:25,405
A と P のペアが必要だ 同じ人の写真のペアだ

137
00:08:25,405 --> 00:08:27,510
システムを訓練する目的のために

138
00:08:27,510 --> 00:08:32,600
同じ人の 複数の写真があるデータセットが必要だ

139
00:08:32,600 --> 00:08:33,960
だから この例では

140
00:08:33,960 --> 00:08:38,040
1,000人の10,000枚の写真と言った

141
00:08:38,040 --> 00:08:40,965
1,000人それぞれが 平均で

142
00:08:40,965 --> 00:08:44,310
1人当たり10枚の写真を持ち 全データセットを作っている

143
00:08:44,310 --> 00:08:47,085
１人当たり １枚の写真しか無い場合は

144
00:08:47,085 --> 00:08:49,725
このシステムを訓練できない

145
00:08:49,725 --> 00:08:52,080
ただし 訓練した後は

146
00:08:52,080 --> 00:08:54,205
これを適用することになる

147
00:08:54,205 --> 00:08:56,565
ただし システムを訓練した後なら もちろん

148
00:08:56,565 --> 00:08:58,200
これを適用できる

149
00:08:58,200 --> 00:09:02,685
One Shot Learning 問題に 顔認識システムでは

150
00:09:02,685 --> 00:09:06,945
認識しようとしている誰かの写真は １つしかない

151
00:09:06,945 --> 00:09:08,335
しかし 学習セットでは

152
00:09:08,335 --> 00:09:12,780
同じ人の複数の画像があることを保証する必要がある 少なくとも

153
00:09:12,780 --> 00:09:14,940
学習セットの中の何人かは そうなれば

154
00:09:14,940 --> 00:09:19,000
アンカーと正の画像のペアができる

155
00:09:19,000 --> 00:09:25,240
さて 実際には どうやって これらの Triplet を学習セットから選ぶのか？

156
00:09:25,240 --> 00:09:28,635
１つの問題は A P そして N を

157
00:09:28,635 --> 00:09:34,260
学習セットから ランダムに選ぶ場合 A と P が同じ人になるように

158
00:09:34,260 --> 00:09:36,270
A と N が違う人になるようにだ

159
00:09:36,270 --> 00:09:40,140
問題の１つは もし それらをランダムに選ぶ場合は

160
00:09:40,140 --> 00:09:44,160
この制約は とても簡単に満たせてしまう

161
00:09:44,160 --> 00:09:48,405
なぜなら ２つのランダムに選んだ人々の写真では

162
00:09:48,405 --> 00:09:51,945
A と N が A と P よりも大きく異なる確率が高い

163
00:09:51,945 --> 00:09:55,740
まだ この表記を覚えているといいけど

164
00:09:55,740 --> 00:10:02,710
この d(A, P) は 最後の数枚のスライドで この符号化のために 使った

165
00:10:02,710 --> 00:10:06,190
これは これと同じで

166
00:10:06,190 --> 00:10:11,040
前のスライドで使ったのは 符号間の２乗ノルム距離だ

167
00:10:11,040 --> 00:10:14,640
しかし A と N が ランダムに選ばれた人の２つだったら

168
00:10:14,640 --> 00:10:17,610
その場合は とても高い確率で 

169
00:10:17,610 --> 00:10:21,630
これが 左にあるマージン α よりも とても大きくなる

170
00:10:21,630 --> 00:10:24,405
そうすると ニューラルネットワークは そこから 多くを学ばなくなる

171
00:10:24,405 --> 00:10:25,940
よって 学習セットを作るために

172
00:10:25,940 --> 00:10:28,340
Triplet

173
00:10:28,340 --> 00:10:31,280
A P そして N は 学習し難いもの にしたい

174
00:10:31,280 --> 00:10:38,685
特に この制約を満たす全ての Tripletに対し

175
00:10:38,685 --> 00:10:44,995
難しい Triplet は A P そして N を選ぶ場合

176
00:10:44,995 --> 00:10:47,454
d(A, P) が

177
00:10:47,454 --> 00:10:52,550
d(A, N) に とても近いだろう

178
00:10:52,550 --> 00:10:54,020
よって この場合

179
00:10:54,020 --> 00:10:57,230
学習アルゴリズムは とても苦労して

180
00:10:57,230 --> 00:11:00,740
正しく ことを行い これを上げて

181
00:11:00,740 --> 00:11:04,030
左のは下げる そうしたら

182
00:11:04,030 --> 00:11:08,430
少なくとも 左側と右側の間は マージン α になる

183
00:11:08,430 --> 00:11:11,900
これらの Triplet を選ぶ効果は

184
00:11:11,900 --> 00:11:16,460
学習アルゴリズムの計算効率を上げることだ

185
00:11:16,460 --> 00:11:18,500
もし Triplet をランダムに選ぶと

186
00:11:18,500 --> 00:11:21,725
非常に多くの Triplet は 簡単で

187
00:11:21,725 --> 00:11:25,970
勾配降下法は 何もしない なぜなら ニューラルネットワークは 全ての場合で

188
00:11:25,970 --> 00:11:27,090
とても良い解を得てしまうからだ

189
00:11:27,090 --> 00:11:32,270
難しい Triplet を使うことによってのみ 勾配降下法は

190
00:11:32,270 --> 00:11:38,700
この値を この値から引き離そうと 何らかの仕事をせざるを得なくなる

191
00:11:38,700 --> 00:11:40,155
もし 興味があるなら

192
00:11:40,155 --> 00:11:46,295
詳細は この論文にある Florian Schroff,

193
00:11:46,295 --> 00:11:51,305
Dmitry Kalinichenko, そして James Philbin によるもので かれらが FaceNet と呼んだものだ

194
00:11:51,305 --> 00:11:55,860
このビデオで説明したアイデアの多くは そこから来ている

195
00:11:55,860 --> 00:11:58,220
ところで 面白い事実がある

196
00:11:58,220 --> 00:12:02,030
ディープラーニングの世界では しばしば アルゴリズムは こんな風に命名される

197
00:12:02,030 --> 00:12:05,810
もし ある領域で働いているなら それを "何々" と呼ぼう

198
00:12:05,810 --> 00:12:10,710
すると "何々Net" もしくは "Deep何々" と呼ばれるシステムになる

199
00:12:10,710 --> 00:12:13,095
顔認識について話してきた

200
00:12:13,095 --> 00:12:16,123
だから この論文は FaceNet と呼ばれる

201
00:12:16,123 --> 00:12:17,465
そして 前回のビデオでは

202
00:12:17,465 --> 00:12:19,910
Deep Face を見た

203
00:12:19,910 --> 00:12:23,600
でも "何々Net" や "Deep何々" というアイデアは

204
00:12:23,600 --> 00:12:28,370
ディープラーニングの世界でアルゴリズムを命名する とても多いやり方だ

205
00:12:28,370 --> 00:12:32,780
そして 気軽に 論文を見てほしい

206
00:12:32,780 --> 00:12:34,940
アルゴリズムを速くするための その他の詳細を 何か学びたいのなら

207
00:12:34,940 --> 00:12:38,745
それは 学習に使うための 最も有益な Triplet の選び方だ

208
00:12:38,745 --> 00:12:40,025
これは いい論文だ

209
00:12:40,025 --> 00:12:41,240
まとめよう

210
00:12:41,240 --> 00:12:42,670
Triplet Loss で学習する

211
00:12:42,670 --> 00:12:47,060
学習セットから 沢山の Triplet を組み合わせて作る 必要がある

212
00:12:47,060 --> 00:12:50,550
ここに Triplet があり アンカーと正がある

213
00:12:50,550 --> 00:12:54,375
両者は 同じ人で 異なる人のは負だ

214
00:12:54,375 --> 00:12:58,445
ここに 別の アンカーがある 正は 同じ人で

215
00:12:58,445 --> 00:13:04,315
アンカーと負は 違う人だ

216
00:13:04,315 --> 00:13:07,000
行うのは このような学習セットを定義することだ

217
00:13:07,000 --> 00:13:09,920
アンカー 正 そして 負 のTriplet に

218
00:13:09,920 --> 00:13:16,740
勾配降下法を使い 前のビデオで定義した コスト関数 J を最小化する

219
00:13:16,740 --> 00:13:20,090
逆伝播の作用で

220
00:13:20,090 --> 00:13:23,640
ニューラルネットワークの全てのパラメータを 学び

221
00:13:23,640 --> 00:13:27,435
符号化して

222
00:13:27,435 --> 00:13:33,395
２画像が同じ人なら その２画像の d は小さくなる 

223
00:13:33,395 --> 00:13:40,286
それらが 違う人なら 大きくなる

224
00:13:40,286 --> 00:13:43,805
これが Triplet Loss だ ニューラルネットワークを

225
00:13:43,805 --> 00:13:48,200
顔認識の学習と符号化のために 訓練する方法だ

226
00:13:48,200 --> 00:13:49,715
さて 周知されていることに

227
00:13:49,715 --> 00:13:54,556
商業利用の顔認識システムは とても巨大なデータセットで訓練してあるということがある 現時点では

228
00:13:54,556 --> 00:13:56,630
しばしば 100万画像を使い

229
00:13:56,630 --> 00:14:00,275
時々 1,000万画像を使い

230
00:14:00,275 --> 00:14:05,210
ある商業企業 は 1億を超える画像を使っていると言っている

231
00:14:05,210 --> 00:14:09,300
これは 現在の標準においても 非常に巨大なデータセットだ

232
00:14:09,300 --> 00:14:12,800
Triplet Loss と それを使って ニューラルネットワークを訓練し

233
00:14:12,800 --> 00:14:18,230
顔認識用に良い符号化を行う方法は 以上だ

234
00:14:18,230 --> 00:14:21,500
そして 現在の顔認識システムは

235
00:14:21,500 --> 00:14:24,830
特に 大規模な商用顔認識システムは

236
00:14:24,830 --> 00:14:27,360
巨大なデータセットで訓練されていることが分かった

237
00:14:27,360 --> 00:14:30,350
100万画像のデータセットでさえ 普通じゃないけど

238
00:14:30,350 --> 00:14:34,040
ある企業は 1,000万画像を使っていて ある企業は

239
00:14:34,040 --> 00:14:38,135
１億画像で これらのシステムを訓練している

240
00:14:38,135 --> 00:14:41,730
現在の標準からしても これらは 非常に巨大なデータセットだ

241
00:14:41,730 --> 00:14:45,230
これらのデータセットは 簡単には得られない

242
00:14:45,230 --> 00:14:48,140
幸運なことに これらの企業のいくつかは

243
00:14:48,140 --> 00:14:51,875
これらの巨大なネットワークを訓練して オンラインにパラメータを上げている

244
00:14:51,875 --> 00:14:54,790
だから スクラッチで これらのネットワークを訓練しようとするよりも

245
00:14:54,790 --> 00:14:59,280
その共有するデータ量ゆえに

246
00:14:59,280 --> 00:15:02,390
しばしば 有益であるのは

247
00:15:02,390 --> 00:15:05,313
誰か他の人の 訓練済みモデルをダウンロードすることだ

248
00:15:05,313 --> 00:15:07,685
自分自身で スクラッチで全てをやるより

249
00:15:07,685 --> 00:15:10,130
しかし 誰か他の人の訓練済みモデルをダウンロードしても

250
00:15:10,130 --> 00:15:14,195
どのように これらのアルゴリズムが訓練されたか知ることは 有益だ

251
00:15:14,195 --> 00:15:19,225
これらのアイデアを スクラッチで 自分自身で 何かのアプリケーションに必要とする場合にね

252
00:15:19,225 --> 00:15:21,405
Triplet Loss は以上だ

253
00:15:21,405 --> 00:15:22,640
次のビデオでは

254
00:15:22,640 --> 00:15:25,280
他の種類の シャム ネットワークを いくつか見せたい

255
00:15:25,280 --> 00:15:28,510
そして どのように それらを訓練するかを

256
00:15:28,510 --> 00:15:30,000
次のビデオに進みましょう