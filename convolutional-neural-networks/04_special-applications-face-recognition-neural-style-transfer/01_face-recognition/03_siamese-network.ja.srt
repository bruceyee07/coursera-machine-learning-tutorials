1
00:00:00,300 --> 00:00:04,300
最後のビデオで学んだ 関数 d の仕事は

2
00:00:04,300 --> 00:00:08,860
２つの顔を入力して どのくらい似ているか 違っているかを 言うものだ

3
00:00:08,860 --> 00:00:12,605
これをやるのにいいのは シャム ネットワークを使うことだ

4
00:00:12,605 --> 00:00:13,105
次の例をみてみましょう。

5
00:00:15,117 --> 00:00:19,098
このような ConvNet の絵をよく見るよね

6
00:00:19,098 --> 00:00:20,635
画像を入力して それを x(1) としよう

7
00:00:20,635 --> 00:00:24,165
一連の 畳み込み層や プーリング層 そして

8
00:00:24,165 --> 00:00:28,465
全結合層を通して このような特徴ベクトルを得る

9
00:00:30,260 --> 00:00:35,940
時々 これは ソフトマックスに喰わせて 分類を出す

10
00:00:35,940 --> 00:00:39,000
このビデオでは これは 使わない

11
00:00:39,000 --> 00:00:44,090
代わりに このベクトルに注目する それを128個の数としよう

12
00:00:44,090 --> 00:00:49,900
ネットワークの深い場所にある全結合層が計算するものだ

13
00:00:50,930 --> 00:00:54,850
この 128個の数に 名前を付けよう

14
00:00:54,850 --> 00:01:00,220
これを f(x(1)) と呼ぶ

15
00:01:00,220 --> 00:01:07,476
f(x(1)) は 入力画像 x(1) を符号化したものだと考える

16
00:01:07,476 --> 00:01:12,653
ここの入力画像 Kianの写真が

17
00:01:12,653 --> 00:01:17,750
128個の数として 再表現される

18
00:01:17,750 --> 00:01:22,840
顔認識システムを作るには ２つの写真を比較したい

19
00:01:22,840 --> 00:01:27,890
例えば この最初の写真と この２番目の写真があるなら

20
00:01:27,890 --> 00:01:31,860
２番目の写真を 同じパラメータを持つ 同じニューラルネットワークに喰わせて

21
00:01:31,860 --> 00:01:37,625
異なるベクトルの 128個の数を得る

22
00:01:37,625 --> 00:01:41,740
それは この２番目の写真を表す もしくは 符号化したものだ

23
00:01:41,740 --> 00:01:43,560
この２番目の写真を

24
00:01:44,570 --> 00:01:50,480
この２番目の写真を符号化したものを f(x(2)) と呼ぶことにする

25
00:01:50,480 --> 00:01:55,680
ここに x(1) と x(2) という２つの画像を表すものがある

26
00:01:55,680 --> 00:01:58,030
それらは 必ずしも 学習セットの中で

27
00:01:58,030 --> 00:02:00,010
１番目 ２番目になっている必要は無い

28
00:02:00,010 --> 00:02:02,230
どの２つの写真でもよい

29
00:02:02,230 --> 00:02:07,320
結局 これら 符号化したものは ２つの画像の良い表現だと言える

30
00:02:07,320 --> 00:02:11,430
それから やるのは

31
00:02:12,670 --> 00:02:16,463
x(1) と x(2) の距離 d を定義することだ

32
00:02:16,463 --> 00:02:21,820
違いのノルムとしてね

33
00:02:21,820 --> 00:02:25,750
この２つの画像を符号化したものの違いのノルムだ

34
00:02:26,990 --> 00:02:29,434
この２つの同じ

35
00:02:29,434 --> 00:02:34,544
畳み込みニューラルネットワークを ２つの異なる入力で走らせて 比較するというアイデアは

36
00:02:34,544 --> 00:02:39,380
時々 シャム ネットワーク構造と呼ばれる

37
00:02:39,380 --> 00:02:43,530
それから ここに挙げた多くのアイデアは

38
00:02:43,530 --> 00:02:48,490
この論文から来ている Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, そして

39
00:02:48,490 --> 00:02:53,690
Lior Wolf の論文で 彼らが発展させたシステムでは Deep Face と呼んでいた

40
00:02:54,890 --> 00:03:00,741
それから ここに挙げた多くのアイデアは Yaniv Taigman,

41
00:03:00,741 --> 00:03:02,920
Ming Yang, Marc'Aurelio Ranzato, そして

42
00:03:02,920 --> 00:03:07,155
Lior Wolf の論文から来ていて 彼らが発展させたシステムでは Deep Face と呼んだ

43
00:03:08,520 --> 00:03:11,830
では どうやって シャム ネットワークを訓練するか？

44
00:03:11,830 --> 00:03:15,380
この２つのニューラルネットワークは 同じパラメータを持つ

45
00:03:16,730 --> 00:03:19,600
本当にやりたいのは このニューラルネットワークを訓練して

46
00:03:19,600 --> 00:03:24,250
これを符号化して 関数 d で結果を計算して

47
00:03:24,250 --> 00:03:27,420
２つの写真が同じ人か判断することだ

48
00:03:27,420 --> 00:03:33,350
形式上 ニューラルネットワークのパラメータが 符号化 f(x(i)) を定義する

49
00:03:33,350 --> 00:03:35,548
どんな画像 x(i) が与えられても

50
00:03:35,548 --> 00:03:40,968
ニューラルネットワークは この 128次元の符号 f(x(i)) を出力する

51
00:03:40,968 --> 00:03:45,655
形式上 やりたいことは パラメータを学んで

52
00:03:45,655 --> 00:03:50,152
もし ２つの写真 x(i) と x(j) が同じ人なら

53
00:03:50,152 --> 00:03:55,347
この符号間の距離を小さくしたい

54
00:03:55,347 --> 00:03:59,583
前のスライドでは x(1) と x(2) を使っていたけど

55
00:03:59,583 --> 00:04:03,841
それは 実際には 学習セットの どの x(i) と x(j) のペアにもなり得る

56
00:04:03,841 --> 00:04:07,959
対照的に x(i) と x(j) が違う人なら

57
00:04:07,959 --> 00:04:13,340
この符号間の距離を大きくしたい

58
00:04:13,340 --> 00:04:18,160
ニューラルネットワークの全ての層のパラメータを変化させて

59
00:04:18,160 --> 00:04:20,665
異なる符号を得る

60
00:04:20,665 --> 00:04:23,639
そして 誤差逆伝播を使い

61
00:04:23,639 --> 00:04:29,590
この全てのパラメータを変化させて この条件を満たすようにする

62
00:04:29,590 --> 00:04:33,460
さぁ シャム ネットワーク構造について学んだ

63
00:04:33,460 --> 00:04:36,890
ニューラルネットワークに出力させたいものの感覚は掴んだだろう

64
00:04:36,890 --> 00:04:39,830
良い符号化を行うものについてはね

65
00:04:39,830 --> 00:04:42,790
しかし 実際に目的関数を定義するには どうする？

66
00:04:42,790 --> 00:04:46,700
ニューラルネットワークに ここで議論したことをさせるには？

67
00:04:46,700 --> 00:04:50,940
次のビデオで Triplet Loss 関数を使う そのやり方を見よう