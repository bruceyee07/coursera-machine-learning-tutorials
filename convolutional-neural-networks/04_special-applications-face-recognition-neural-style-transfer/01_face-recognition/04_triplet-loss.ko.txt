신경망의 파라미터를 학습하여 얼굴 그림을 위한 훌륭한 인코딩을 제공하는 한 가지 방법은 triplet loss function(삼중 항 손실 함수)에 적용된 기울기 강하를 정의하는 것입니다. 이게 무슨 뜻인지 보시죠. 이게 무슨 말인지 한번 보도록 하겠습니다. 삼중 항 손실을 적용하려면 이미지 쌍을 비교해야 합니다. 예를 들어, 주어진 이 그림에서, 신경망의 파라미터를 배우려면 동시에 여러 그림을 봐야 합니다. 예를 들어, 이 이미지 쌍이 주어지면 이들은 같은 사람 때문에, 인코딩이 동일해야 합니다. 반면, 이 이미지 쌍이 주어지면 이들은 다른 사람이기 때문에 인코딩이 상당히 달라지게 됩니다. 삼중 손실의 용어적으로, 여러분이 할 일은, 언제나 하나의 앵커 이미지를 보는 것입니다. 그리고 나서 앵커와 포지티브 이미지 사이에 거리를 주는 겁니다. 이건 정말 포지티브 예시이죠, 같은 사람이긴 하지만, 비슷하다는 뜻입니다. 반면에, 이미지 쌍들이 네거티브 예시와 비교될 때는, 앵커가 그들과 훨씬 더 많이 떨어져서 거리를 가지게 해야 합니다. 이것은 한 번에 세 개의 이미지를 항상 보게 된다는 의미의 용어인 '삼중 항 손실'을 가져오는 것이죠. 여러분은 네거티브 이미지뿐 아니라 앵커 이미지, 포지티브 이미지를 보게 될 것입니다. 이제 앵커 양수와 음수를 A, P, N으로 축약하겠습니다. 이렇게 공식화하기 위해서 여러분이 원하는 것은 인코딩의 신경망 파라미터가. 다음 속성을 갖도록 하는 것입니다 앵커 인코딩에서 포지티브 예제의 인코딩을 뺀 값을 작게 하고, 특히 앵커의 인코딩과 음수의 인코딩 사이의 제곱 된 norm의 거리보다 작거나 같게 하려면 물론 이것은 d의 A, P이고 이것은 d의 A, N입니다 여기서 d는 거리 함수로 생각할 수 있습니다. 알파벳 d로 시작하므로 d라고 이름을 붙였습니다. 이제 이 방정식의 오른쪽에서 왼쪽으로 이동하면 llf(A)-f(P)ll의 제곱, 빼기, 오른쪽을 가보면, llf(A) - f(N)ll 이고 이것이 0 보다 작거나 같게 하면 됩니다. 그러나 이제, 우리는 이 표현에 약간의 변화를 줄 텐데요 이것은 이 표현이 만족되는지 확인하는 간단한 방법이며 모든 것이 0과 같다는 것을 배우는 것 입니다. f가 항상 0이면, 0 빼기 0 은 0 이고, 0 빼기 0 은 0 이고 어떤 이미지의 f가 모두 0의 벡터라면, 여러분은 거의 이 방정식을 만족시킬 수 있습니다. 따라서 신경망이 모든 인코딩에 대해 0을 출력하지 않도록 하기 위해 모든 인코딩을 서로 동일하게 설정되지 않도록 해야 합니다. 신경망에 대한 또 다른 방법은 모든 이미지의 인코딩이 다른 모든 이미지의 인코딩과 동일한 지 여부입니다. 이 경우 다시 0 빼기 0 이 되므로 신경망이 그러한 것을 하지 못하도록 하기 위해 우리가 할 일은, 이 목표를 수정하여 이것이 0보다 작거나 같을 필요 없다고 알려주는 것입니다. 0보다 약간 작아야 합니다. 따라서, 특히 negative alpha보다 작을 필요가 있다고 하면 알파는 또 다른 하이퍼 파라미터인데요, 이것은 신경망이 trivial solution를 출력하지 못하게 합니다. 관례 상, 일반적으 우리는 음수 알파 대신, 이쪽에 플러스 알파라고 씁니다. 그리고 이것을 margin (마진)이라 부르는데, 이것은 지지 벡터 기계에 대한 문헌을 보았다면, 익숙한 용어이겠지만, 그렇지 않은 경우라 해도 걱정하지 않아도 됩니다. 또한 이 마진 파라미터를 추가하여, 이 위에 있는 수색을 수정할 수도 있습니다. 예를 들어 마진을 0.2로 설정했다고 합시다. 이 예에서 앵커의 d와 포지티브가 0.5 인 경우 앵커와 네거티브 사이의 d가 조금 더 크면, 예를 들어 0.51 이면 이는 만족되지 않을 것입니다 0.51이 0.5보다 크더라도, 충분하지 않다는 것을 말하고 있습니다. d(A, N) 이 d(A, P) 보다 훨씬 큰 것을 원하고 있죠. 특히나, 우린 이것이 적어도 0.7 이거나 혹은 그 이상이 되기를 원합니다. 이 마진 또는 이 간격을 0.2 이상으로 설정하려면 이 값을 밀어 올리거나 아래로 밀어서 이 알파 값의 이 갭, 즉, 앵커와 포지티브 대 앵커와 네거티브 이 둘 사이의 거리인 하이퍼 파라미터 알파가 0.2 가 되도록 하면 됩니다. 여기에 마진 파라미터가 하는 일입니다. 이것은 앵커 포지티브 쌍과 앵커 네거티브 쌍을 서로서로 더 멀어질 수 있도록 밀어내는 것이죠. 자, 여기 아래의 방정식과 다음 슬라이드를 봅시다. 다음 슬라이드에서 그것을 공식화하고, 삼중 항 손실 함수를 정의 내려봅시다. 삼중 항 손실 함수는 이미지의 세 개에 정의됩니다 그래서, 세 이미지, A, P, N 이 주어지면 앵커, 포지티브와 네거티브의 예시들입니다. 포지티브 예시는 앵커와 같은 사람의 것이지만 네거티브는 앵커와 다른 사람입니다. 손실을 다음과 같이 정의 할 것입니다. 이 예제에서 손실은, 실제로 세 개의 이미지에서 정의되어 있습니다. 이전 슬라이드에서 봤던 것을 우선 복사하겠습니다. 그래서 llf(A) - f(N)ll 의 제곱에서 llf(A) - f(N)ll 의 제곱을 뺀 것입니다. 그리고 나서 알파, 즉 마진 파라미터를 더하면 이 부분이 0 보다 작거나 같아야만 합니다. 따라서 손실 함수를 정의하려면 이 값과 0 사이의 최대 값을 취해 봅시다. 따라서 여기에서 최대 값을 취하는 효과는 0보다 작으면. 손실은 0입니다 왜냐하면, 최대 값은 0 보다 작거나 같으며 0 이 0 일 될 때이기 때문입니다 초록색으로 밑줄 그어 놓은 목표를 달성하는 한, 여러분이 0보다 작거나 같게 만드는 목표를 달성했다면 이 예제의 손실은 0이됩니다. 그러나 다른 한편으로 이것이 0보다 클 때, 최대 값을 취하면, 우리가 선택한 최대값, 이는 녹색으로 밑줄 그은 이 부분이므로 포지티브 손실을 보게 됩니다 따라서 이것을 최소화하려고 시도하면 이 값을 0보다 작거나 같게 보내려고 하는 효과가 있습니다. 그리고 영 (0) 이하이거나 0과 같을 때, 신경망은 그것이 얼마나 더 네거티브한지는 신경 쓰지 않습니다. 따라서, 이것은 단일 삼중에 대한 손실을 정의하는 방법이며, 신경망에서 전체 비용 함수는 서로 다른 트리플렛상에서 개별 손실의 트레이닝 세트에 대한 총액이 될 것입니다. 그래서 1,000 명의 사람을 사진 찍어서, 10,000 장의 사진의 트레이닝 세트를 가지고 있다면 여러분이 해야 할 일은 10,000장의 사진을 찍어서, 그것을 이런 트리플렛을 생성하고 선택한 후, 이 비용함수 유형에서 기울기 강하를 사용하여 학습 알고리즘을 훈련시키는 것입니다. 이는 실제로 트레이닝 세트에서 가져온 이미지의 세 장으로 정의됩니다.. 이 세 장의 데이터 집합을 정의하려면 A와 P의 몇 쌍이 필요합니다. 같은 사람의 그림 쌍이 필요하죠. 시스템을 훈련할 목표를 위해서는, 같은 사람의 사진을 여러 개 가지고 있는 데이터 세트가 필요합니다. 때문에 이 예에서 1,000 명의 다른 사람의 사진이 10,000 개라면 1,000 명당 평균 10 장의 사진을 가지고 전체 데이터 집합을 구성한다고 말씀 드렸습니다. 각 사람의 사진이 하나뿐이라면 이 시스템을 실제로 훈련 할 수 없습니다. 물론 교육을 한 후에, 만약 여러분이 이것을 적용한다면, 물론 시스템을 훈련한 후에, 여러분의 얼굴 인식 시스템을 위한 원샷 학습 문제에 그것을 적용 할 수 있습니다. 어쩌면 여러분이 인식하려고 하는 사람의 사진을 단 한장만 가지고 있을지도 모릅니다. 하지만 여러분의 트레이닝 세트에서는 앵커 이미지와 포지티브 이미지가 쌍을 이룰 수 있도록 트레이닝 세트에 있는 적어도 몇몇 사람들만이라도 동일한 사람의 이미지가 여러 개 있는지 확인해야 합니다. 자, 실제로 트레이닝 세트를 구성하려면 어떻게 이 트리플렛을 골라야 할까요? 문제점 중의 하나는, A, P 는 같은 사람이고, A, N 은 다른 사람이라는 전제하에 A, P, N 을 트레이닝 세트에서 무작위로 고르면 문제점은 무작위로 고르면 이 제약 조건을 매우 쉽게 만족할 수 있습니다. 두 개의 무작위로 선택된 사람들의 그림이 주어 졌기 때문에 A와 P 보다 A와 N은 아주 많이 다를 가능성을 가지고 있습니다. 이 표기법을 여전히 기억하고 있기를 바라는데요, 이 d(A, P)는 인코딩으로 작년 슬라이드에서 썼던 것입니다. 이것은 지난 슬라이드에서 본 인코딩사이의 제곱 된 norm의 값과 같습니다. 그러나 A와 N이 무작위로 선택된 두 사람이라면 왼쪽에 있는 마진 알파보다 훨씬 더 큰 확률을 가질 가능성이 매우 높습니다. 따라서, 신경망은 이것으로부터 많은 것을 배울 수 없습니다. 따라서 훈련 세트를 구성하기 위해 여러분이 할 일은 훈련하기 어려운 A, P, N 중 하나를 선택하는 것입니다. 그래서, 특히, 여러분이 원하는 것은 이 제약이 만족되는 트리플렛이죠. 그래서, 아마도 d (A, P)가 실제로 d (A, N)에 아주 가깝도록 A, P, N에 대한 값을 선택한다면, 삼중 항에게는 어려운 일일 것입니다. 그래서 그 경우, 학습 알고리즘은 오른쪽에 있는 이것을 가져 가서 밀어 올리거나 왼쪽에 있는 이 것을 가져가서 아래로 밀어 내릴 수 있게 더 열심히 노력해서 적어도 왼쪽과 오른쪽 사이의 알파 마진이 생기도록 해야 합니다. 이 트리플렛을 선택하는 것의 효과는 학습 알고리즘의 계산 효율성이 높아지는 것 입니다. 트리플렛을 무작위로 선택하면 너무 많이 삼중 쌍이 정밀 쉬워질 것이고, 기울기 하강은 신경망이 그것들을 올바르게 수행하기 때문에, 기울기 하강은 꽤 언제나 아무것도 하지 않습니다. 기울기 하강 과정이 이러한 양으로부터 이런 양을 더 멀리 떨어 드리려는 노력을 해야 하는 것은 강한 삼중 항을 사용해서만 가능합니다. 관심이 있으시다면, Florian Schroff, Dmitry Kalinichenko, James Philbin 에 의해 쓰여진 논문에서 자세히 설명합니다. FaceNet 이라는 시스템을 가지고 있는데 저 또한 여기서 나온 많은 아이디어들을 이 강의에서 보여드리고 있습니다. 그건 그렇고, 이것은 알고리즘이 딥러닝 세계에서 어떻게 명명되는지에 대한 재미있는 사실을 보여줍니다. 이는 블랭크라고 부르는 어느 특정 도메인에서 작업하신다면, 블랭크넷 혹은 딥블랭크라고 불리는 시스템을 종종 보실 수 있을 겁니다. 우리는 얼굴 인식에 대해 이야기 해 왔습니다. 이 논문은 FaceNet이라고 불리는 것인데요 마지막 비디오에서 여러분은 단지 딥페이스만 보았습니다. 그러나 블랭크네트 또는 딥블랭크의 이 아이디어는 딥러닝 세계에서 알고리즘을 명명하는 매우 인기 있는 방법입니다. 그리고 가장 유용한 삼중 항을 선택하여 알고리즘 속도를 높이기 위한 다른 세부 정보를 배우고 싶다면 이 논문을 살펴보십시오. 이건 좋은 논문입니다. 이제 마무리 지어보자면, 삼중 손실을 훈련하려면 트레이닝 세트를 가져 와서 많은 트리플에 매핑해야합니다. 여기에 앵커와 포지티브를 가진 트리플이 있는데요 둘 다 같은 사람이죠, 그리고 다른 사람에 대한 네거티브가 있습니다. 앵커와 포지티브가 같은 사람이지만, 앵커와 네거티브가 다른 사람인 또 다른 경우가 있고, 계속 이어집니다. 앵커 포지티브와 네거티브 삼중항의 트레이닝 세트를 가지고 이전 슬라이드에서 정의된 비용함수 J를 최소화 하기 위해 기울기 하강을 사용하십시오. 그렇게 하면 인코딩을 배우기 위해서 신경망의 모든 파라미터에 후 방향 전파되는 효과가 있을 것입니다. 그럼으로써, 이 두 이미지가 같은 사람일 때, 두 이미지의 d는 작아질 것이고 이 두 이미지가 다른 사람일 때, d는 커질 것 입니다. 삼중손실은 여기까지입니다. 그리고 어떻게 얼굴인식을 위한 인코딩을 배우도록 신경망을 훈련할 수 있을지 알아보았습니다. 이제, 상업적인 얼굴 인식 시스템들은 매우 큰 데이터세트에서 훈련되는 것이 분명합니다. 종종, 백 만장의 이미지 때로는, 흔하진 않지만, 천만 장의 이미지도 사용하죠. 그리고 1 억 개 이상의 이미지를 사용하는 상업 회사가 있습니다. 이것들은 매우 큰 데이터 집합입니다. 삼중손실은 여기까지입니다. 그리고 어떻게 얼굴인식을 위한 좋은 인코딩을 작동시키도록 신경망을 훈련할 수 있을지 알아보았습니다. 이제 오늘날의 얼굴 인식 시스템, 특히 손실 치료 상용 얼굴 인식 시스템이 매우 큰 데이터 세트에 대해 훈련된 것으로 나타났습니다. 백만 개 이미지의 데이터세트는 드문 일이 아니며 일부 회사는 1,000 만 개의 이미지를 사용하고 일부 회사는 이 시스템을 교육하기 위해 1 억 개의 이미지를 가지고 있습니다. 따라서 이들은 현대 표준에 의해서조차 도 매우 큰 데이터 세트이며, 이러한 데이터 세트 자산은 획득하기 쉽지 않습니다. 다행히도 일부 회사는 이러한 대규모 네트워크를 훈련하고 파라미터를 온라인에 게시했습니다 따라서 처음부터 이러한 네트워크 중 하나를 교육하기보다는 공유 데이터 볼륨 사이즈로 인해 종종 다른 사람의 기존 훈련 모델을 다운로드 하는 것이 유용할지도 모르는 영역입니다. 모든 걸 처음부터 혼자서 하려고 하기보다는 말이죠. 다른 사람의 기존 훈련 모델을 다운로드 한다 해도, 제 생각엔 알고리즘들이 어떻게 훈련되는지 배우고 몇 응용프로그램을 위해서 처음부터 혼자서 이 모든 아이들을 응용해야만 하는 경우에 여전히 유용할 것입니다. 삼중 손실에 대한 강의를 마쳤습니다. 다음 강의에서는 다음 비디오에서는 Siamese 네트워크에 대한 다른 변형과 이러한 시스템을 교육하는 방법을 보여 드리고자 합니다. 다음 강의로 이동하시죠 다음 비디오로 넘어가겠습니다.