在前面的影片中 你知道怎麼讓一個神經網路輸出四個數字 b_x, b_y, b_h 和 b_w， 來指定一個物件的邊界框，讓神經網路定位之 更一般來說 你可以讓神經網路輸出 圖片中重要的點的座標 (x,y) 有時叫作「特徵點」(landmark)，我們想要神經網路辨認之 讓我來舉一些例子 假設你在做臉部辨認的應用，基於一些原因 你想讓演算法告訴你某個人的眼角在哪 那麼，眼角這個點有座標 (x,y) 所以你可以讓神經網路 的最後一層多輸出兩個數字 — 我要叫他們 l_x, l_y 來告訴你那個眼角的座標 但是，如果要他告訴你眼睛的四個角落呢？ 也就是兩隻眼睛的眼角 那麼，如果我們叫這些點 由左到右是第 1, 2, 3, 4 點 那你可以修改神經網路，讓他輸出 l_1x, l_1y 表示第一點 l_2x, l_2y 表示第二點，依此類推 讓這神經網路能輸出 這個人臉上這四個點的位置 可是，如果你不只要四個點呢？ 如果你想要輸出這個點 眼睛周圍的這個和這個點，該怎麼辦呢 也可能我要在嘴巴周圍放些關鍵點 讓你可以取得嘴巴的形狀，辨認一個人在笑還是不開心 或許在鼻子周圍取一些點 你可以在臉上定義若干個點 舉例來說，假設是 64 個點、64 個特徵點 甚至可能一些點去定出臉龐的邊緣、 下巴的線條... 總之定義了一些特徵點 並且產生有標記、含有這些特徵點的訓練集之後 你就能讓神經網路告訴你 一張臉上的關鍵點、特徵點在哪裡 那麼你要做的是把這張圖片 這張人臉作為輸入 讓他通過 ConvNet 得到一些特徵 讓他輸出 0 或 1 表示「這張圖有沒有臉」；還讓他輸出 l_1x、 l_1y、一直到 l_64x, l64y 這裡我用 l 表示特徵點 所以以這例子，會有 129 個輸出單元 一個代表「是不是臉」 然後如果有 64 個特徵點 就會有 64 乘 2 也就是 128 再加一個輸出單元 這些能告訴你圖中有沒有臉，以及臉上所有的特徵點在哪 所以，要做臉部表情辨識的話 這是滿關鍵的要素。如果你玩過 Snapchat 或是其他的 像是擴增實境的濾鏡 像 Snapchat 能在照片的臉上頭畫個皇冠或是其他特效 能偵測到臉的特徵點是很關鍵的元件 像是那些繪圖特效，將臉變形 弄出各種特效，讓人戴頂皇冠或帽子一類的。 當然，為了要能訓練出這樣的網路 你要有標記好的訓練資料 我們要有一堆圖片，還有一堆標記 y 而這需要有人看過這些圖片 手動努力地標出這些特徵點 最後一個例子：如果你對人的姿勢偵測感興趣 你也可以定義一些關鍵點，例如胸部中間 左肩、左手肘、手腕等等 然後讓神經網路去標記 一個人姿勢的特徵點，讓他輸出 所有我這邊點出的點 這樣神經網路也能輸出一個人的姿勢 所以當然要這樣的話，一樣也必須要標好 這堆特徵點，例如 l_1x, l1_y 可能是胸部中點，一直可能到 l_32x, l_32y — 如果你想用 32 個座標來表示一個人的姿勢 那麼，這個主意可能聽起來很簡單：只要加一堆輸出單元 輸出各個想偵測的特徵點座標 (x,y) 就好。 這邊提醒一下，在所有圖片中 特徵點的定義都要一致 例如特徵點1都是這個眼角 特徵點2都是這個眼角 特徵點3, 特徵點4, 等等等 橫跨各個圖片，標記都要一致 如果你能僱用專門標記的人，或你自己能標出足夠多的資料 那神經網路就能預測出所有的特徵點 讓你能跟著做其他有趣的事，像是姿勢辨認、 了解某個人的情緒，諸如此類 那麼，這就是「特徵點偵測」(Landmark detection) 接下來，讓我們從這個元件出發 往物件偵測 (object detection) 邁進