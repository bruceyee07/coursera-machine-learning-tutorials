為什麼深度殘差網路作用這麼好？ 我們來進行一個例子說明
為什麼深度殘差網路作用這麼棒 至少在於您如何可以讓您的網路更深更深而真的 不會傷害且能在訓練集上至少做好訓練 也希望您從這個學程的第三課程中學到 在訓練集上做好訓練，通常先決條件是在 測試集上能保持或更深入都能做好 所以能夠訓練好深度殘差網路 在測試集上是好的第一步，我們來看一個例子 我們在前面的影片中看到，如果您讓網路更深 它可能會傷害到您的網路
在訓練集上做訓練 這是為什麼有時候
您不想讓您的網路太深 但當您訓練深度殘差網路時，
這不再是事實，或者至少不一定是事實 讓我們來跑個實際的例子 假設您餵一個 X 進入 一個大的神經網路，輸出一個啟動值 a[l] 假設對於這個例子，您要修改 這個神經網路，讓它更深一些 使用相同的神經網路 然後這個輸出 a[l] 我們要加入一些額外的層到這個網路 我們加一層在這裡，然後另一層在這裡 這時的輸出會是 a[l+2] 只是現在讓它成為深度殘差網路區塊 一個殘差區塊，有額外的捷徑 為了便於討論 假設整個網路我們都使用 ReLU 啟動函數 所以所有啟動將會是大於或等於 0 只有輸入值  Ｘ 例外 因為 ReLU 啟動函數的輸出值不是 0 就是正數 現在我們來看看 a[l+2] 會是什麼？ 從前面的影片複製過來 a[l+2] 會是 ReLU 應用在 z[l+2] 上 然後再加入  a[l]，這個多出來的 a[l] 是從我們剛加入的捷徑中跳過連結來的 而如果我們將此展開 這個會等於 g of w[l+2] 乘上 a[l+1] 加上 b[l+2] 所以這是等於 z[l+2]，再加上 a[l] 請注意，如果您使用 L2 正則化，權重衰減 這個會縮小 w[l+2] 的值 如果您使用權重衰減到 b，
它也會縮小，雖然 我猜實作上，有時候您會有時候不會
把權重衰減用在 b 上 但這裡要注意的重點是 w 而如果 w[l+2] 等於 0 假設為了討論方便起見，假設 b 也是為 0 這一項可以去掉，因為等於 0 然後 g of a[l] 這個就等於 a[l]，因為我們假設使用 ReLU 啟動函數 所以所有啟動都是非負值  g of a[l] 應用 ReLU 到一個非負的項 您會得到 a[l] 這裡顯示的是恆等函數
對於殘差區塊是容易去學習的 而得到 a[l+2] 等於 a[l] 很容易的，
因為使用了跳過連結 而這表示加入這兩層到您的神經網路 它們並不會傷害到您神經網路的能力，跟 原來的簡單網路沒有這額外這兩層的一樣 因為對它而言
學習恆等函數是很容易的，只要複製 a[l] 到 a[l+2] 即使多了兩層 而這是為什麼增加額外兩層 加入這個殘差區塊到 中間或者後面的神經網路，不會傷害到它的績效 但當然我們的目標
不只是不傷害績效 而是幫助績效，您可以想像如果所有 這些隱藏單元
如果真的學習到有用的東西 或許您可以學得比恆等函數更好 而一般網路到很深層時會出錯
如果不使用 殘差區塊或跳過連結時 當您讓網路越來越深時 實際上很難對它選擇參數 即使是學習恆等函數
也因為這樣，很多層 的結果造成越來越糟，而不是越來越好 而我想殘差網路可行的主要原因是 對於這些額外的層很容易學習 恆等函數，好像保證他不會傷害 績效，而常常或許您很幸運
這樣做會有助於績效 至少立於不敗之地，不至於 傷害績效，而梯度下降可以從此改進，只會更好 有關殘差網路有另一個細節 值得討論的是透過這個『加入』 我們假設 z[l+2] 跟 a[l] 是同維度 所以您在深度殘差網路
會見到很多使用相同卷積 所以這個維度會 等於這個維度，我猜這一層是輸出層 所以我們真的可以做這種捷徑連結 因為相同卷積保持維度 所以讓您很容易的應用 這種捷徑，進行兩個同維度向量的加法 假使輸入跟輸出有不同的維度，舉個例 如果這個 Z 是 128 維度， a[l] 是 256 維度，舉個例 您需要做的是加上一個額外的矩陣稱為 Ws 而 Ws 在這個例子會是 a[l] 256 乘 128 維度的矩陣 所以 Ws 乘上 a[l] 會變成 256 維度 這個加法現在是 兩個 256 維度的向量，有一些事您可以對 Ws 做 它可以是一個我們要學習的矩陣 它可以是一個固定的矩陣，我們只要 填 0 也就是拿 a[l] 然後用 0 填入成為 256 維度，這兩種方式我想都可以用 最後，讓我們來看深度殘差網路
作用在影像上 這些影像來自於 Harlow 的論文 這是一個一般網路的例子，您輸入一個影像 然後有一些卷積層 直到最終您有一個 softmax 輸出 將這個轉換為深度殘差網路 您加入這些額外的跳過連結 我只會提到一些細節 這裡面有很多 3乘3 卷積，大部分都是 3乘3 相同卷積 這是為什麼您加入了
相同維度的特徵向量 與其使用全連結層 這些確實是卷積層但因為是相同卷積 維度被保持著，所以 z[l+2] 加 a[l] 用加法是有意義的 跟您之前看過很多的網路類似 您有一些卷積層然後 偶而用池層，或是類似池層 而每當發生這種層 您需要調整維度，
就像我們在前面的投影片一樣 您可以使用 Ws 矩陣 然後在這些網路常見到的 您會有卷積，卷積，池層，卷積，卷積，池層<unknown> 最後您會用 一個全連結層，跟 Softmax 來做預測 這就是深度殘差網路 接下來，有一種很有趣的方式 在神經網路使用 1乘1 的過濾器 1乘1 卷積 為什麼要用 1乘1 卷積 我們在下一段影片來看看