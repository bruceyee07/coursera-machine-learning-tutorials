在设计卷积网络的架构时 其中一种很有用的想法是用1*1的卷积 现在你可能在想 1*1的卷积能做些什么呢？ 难道不就是数的相乘吗？ 这看起来是一件很滑稽的事 事实并非完全如此 让我们来看看 这是一个1*1的过滤器 我们会把数字2放进去 如果你把这个6*6的图像 这个6*6*1的图像去和这个1*1*1的滤波器做卷积 你最终只是把这个图像乘以2 所以 1 2 3最终变成2 4 6 如此类推 所以通过一个1*1的过滤器做卷积 似乎不是特别有用 你只是把它乘以某个数字 但这是一个6*6*1一个通道图像的例子 如果你有一个6*6*32 而不是6*6*1(的图像） 那么和一个1*1的过滤器做卷积会更有意义 特别是一个1*1的卷积将会 逐一扫过这里的36个不同的位置 然后进行对应元素间的乘法 将左边的32个数和过滤器中的32个数相乘 (做内积) 然后将一个非线性映射 ReLU(线性整流函数)作用于它 以这36个位置中的一个为例 比如这个立方体中的这一面 你将这36个数乘以这整面中的这一条 最后你会得到 一个实数，就像这里所画的，是其中一个输出值 事实上你可以这样理解 这个1*1*32的过滤器中的32个数 它类似于你有一个神经元 接收一个32个数的输入向量 将这一条位于相同位置，即相同高度和宽度，但位于32个不同通道的数 和32个权重相乘 然后将线性整流函数(ReLU)作用于它，再把对应的结果输出到这里 更通常地 而有多个过滤器 那么这类似于，你有不止一个单元，而有多个单元 接受用一面中的所有数为输入 然后将它们生成为一个6*6*滤波器数的输出量 所以对1*1卷积的一种理解是 它本质上是一个完全连接的神经网络 逐一作用于这36个不同的位置 这个完全连接的神经网络所做的是 它接收32个数的输入，然后输出过滤器数个输出值 所以要标记的话 它其实是nc上标l+1 (即l+1层的通道数) 如果这是往后一层的话 然后对这36个位置的中每一个都进行相同的操作 也就是对这6乘6个位置中的每一个(都进行相同的操作) 你最终得到的输出是6*6*过滤器数 这对你的输入量所进行的是一个非常不平凡的计算 这个想法经常被称为1*1卷积 但有时也被称为网中网(Network in Network) 它源于这篇由 Min Lin, Qiang Chen和Shuicheng Yan所著的论文 虽然这篇论文中的结构细节没有被广泛地使用 这个1*1卷积的想法 有时也被称为网中网的想法，有很大的影响力 影响了许多其他的神经网络架构 包括我们在下一个视频中会看到的inception network 不过举一个1*1卷积有应用价值的例子 这是一个你会用到它的时候 假设你有一个28*28*192的立方体 如果你想缩小它的高度和宽度 你可以使用一层池化层 我们知道怎样做 但如果通道数过大，我们就想缩小它 你会如何将它缩小到28*28*32的大小呢？ 那么你可以使用32个1*1的过滤器 从技术上说，每一个滤波器将会是1*1*192大小的 因为过滤器的通道数 需要和输入量的通道数一致 但由于你使用了32个滤波器，这个过程的输出将会是28*28*32大小的立方体 这是一种让你缩小nc的方式 而池化层我只可以用它来缩小nh和nw 即这个立方体的高度和宽度 我们会在之后看到 这个1*1卷积的想法是如何缩小通道数 从而达到在某些网络中减少计算量的目的的 当然如果你想要保持192个通道数，这也是可行的 这时1*1卷积的效果是增加非线性性 它通过添加一层输入28*28*192 再输出28*28*192(的操作) 使得你的网络可以学习到更复杂函数形式 因此这就是一个1*1的卷积层 如何可以做一些不平凡的操作 以及在神经网络中增加非线性性 以及允许你减少，或不改变或如果你愿意的话 增加输入的通道数 接下来你会看到这在构建inception network中是十分有用 让我们在下一个视频中继续 因此你现在看到了一个1*1的卷积操作 实际上是一个非常不平凡的操作，它使得你可以缩小 输入体积的通道数 或不改变它，或如果你想的话，甚至增加它 在下一视频中 你会看到它可以被用来构建 inception network 让我们进入下一个视频