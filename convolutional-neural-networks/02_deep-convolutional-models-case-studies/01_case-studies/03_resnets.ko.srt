1
00:00:00,000 --> 00:00:03,390
아주 아주 깊은 심층 신경망 학습시키기 어렵습니다.

2
00:00:03,390 --> 00:00:07,215
왜냐하면 Vanishing Gradient 와 Exploding Gradient 유형의 문제들 때문입니다.

3
00:00:07,215 --> 00:00:08,790
이 영상에서, 여러분은

4
00:00:08,790 --> 00:00:12,150
하나의 레이어로부터 액티베이션을 취하게 해 줄 뿐만 아니라

5
00:00:12,150 --> 00:00:17,498
이것을 신경망 안에 있는 훨씬 깊은 또 다른 래이어에 제공하는 skip 레이어에 대해 배워보겠습니다.

6
00:00:17,498 --> 00:00:22,600
이것을 사용하면, 아주 아주 깊이 심층 신경망을 학습시키도록 해주는 ResNet을 만들게 될 것입니다.

7
00:00:22,600 --> 00:00:26,865
때로는 심지어 100 개가 넘는 레이어로 된 네트워크도 있습니다. 한 번 보시죠.

8
00:00:26,865 --> 00:00:30,390
ResNet는 residual block (잔류블럭)이라고 불리는 것으로 만들어졌습니다.

9
00:00:30,390 --> 00:00:33,185
먼저 그 블럭이 무엇인지 설명해 보겠습니다.

10
00:00:33,185 --> 00:00:35,370
신경망 레이어가 두 개 있습니다.

11
00:00:35,370 --> 00:00:38,005
레이어 액티베이션 a[l] 에서 시작하시면 됩니다.

12
00:00:38,005 --> 00:00:43,940
그리고 a[l+1], 그리고 나서 비활성화된 두 개 레이어 뒤에는 a[l+2]가 됩니다.

13
00:00:43,940 --> 00:00:48,798
이 계산 단계를 통해 a[1]을 얻은 다음

14
00:00:48,798 --> 00:00:54,459
이 linear operator를 적용하시면,

15
00:00:54,459 --> 00:00:57,660
이것은 이 방정식에 의해 좌우됩니다.

16
00:00:57,660 --> 00:01:01,690
a[l] 으로부터 시작해서 z[l+1] 과.

17
00:01:01,690 --> 00:01:07,975
가중치 매트릭스를 곱하고, 바이어스 벡터 값을 더해주면 됩니다.

18
00:01:07,975 --> 00:01:17,945
그리고 나서, a[l+1]를 얻으려면 ReLU 비선형성을 적용하십시오.

19
00:01:17,945 --> 00:01:24,750
그러면 이것은 a[l+1] = g(z[l+1]) 방정식에 의해 계산될 것입니다.

20
00:01:24,750 --> 00:01:26,280
그런 다음, 다음 레이어에서

21
00:01:26,280 --> 00:01:30,540
이 선형 단계를 다시 수행하시면,

22
00:01:30,540 --> 00:01:33,432
이 방정식에 의해 제어될 것입니다.

23
00:01:33,432 --> 00:01:38,040
이건 바로 왼쪽에 있는 이 방정식하고 많이 비슷하죠

24
00:01:38,040 --> 00:01:43,890
그리고 마지막으로, 이 방정식으로 산출되는 또 다른 ReLU operation을 적용하시면

25
00:01:43,890 --> 00:01:52,105
여기 G는 ReLU 비선형성이 될 것입니다.

26
00:01:52,105 --> 00:01:56,880
이렇게 하면 a[l+2] 값을 구하게 되죠. 다시 말해서,

27
00:01:56,880 --> 00:01:57,900
자 다시 말해

28
00:01:57,900 --> 00:02:03,035
[l]에서 [l + 2]로 이동하기 위해서는

29
00:02:03,035 --> 00:02:07,455
이 모든 단계를 거쳐야 하는데요, 이걸

30
00:02:07,455 --> 00:02:13,140
레이어 세트의 주요 경로라고 부르도록 하죠.

31
00:02:13,140 --> 00:02:14,550
잔류블럭에서,

32
00:02:14,550 --> 00:02:16,900
이걸 바꿔봅시다,

33
00:02:16,900 --> 00:02:18,495
a[l]을 가져다가

34
00:02:18,495 --> 00:02:22,805
우선 앞으로 쭉 빼서, 복사하시고,

35
00:02:22,805 --> 00:02:26,200
바로 여기 신경망에 매치시켜 줍니다.

36
00:02:26,200 --> 00:02:28,860
그러면 이 a[l]이 위치하게 됩니다.

37
00:02:28,860 --> 00:02:34,080
비선형성, 즉, ReLU 비선형성에 적용시키기 전에 말이죠.

38
00:02:34,080 --> 00:02:37,730
이걸 우린 shortcut 이라고 부릅시다.

39
00:02:37,730 --> 00:02:40,725
주 통로를 따라갈 필요 없이

40
00:02:40,725 --> 00:02:43,335
a[1]의 정보는 이제 신경망로

41
00:02:43,335 --> 00:02:46,910
훨씬 더 깊숙이 들어가는 지름길을 따라갈 수 있습니다.

42
00:02:46,910 --> 00:02:49,680
이 마지막 방정식은 없애버리고, 대신에

43
00:02:49,680 --> 00:02:52,760
아웃풋 a[l + 2] 는 ReLU 비선형성 g(z[l + 2] 라고

44
00:02:52,760 --> 00:03:00,810
앞에 했던 것이랑 똑같이 쓰고

45
00:03:00,810 --> 00:03:02,830
이제는 a [l] 을 더하면 됩니다.

46
00:03:02,830 --> 00:03:05,515
따라서, 여기서 이 a[l]을 덧셈하면

47
00:03:05,515 --> 00:03:07,355
이게 잔여 블럭을 만들게 되는 거죠.

48
00:03:07,355 --> 00:03:11,070
이 그림에서, shortcut 이 이쪽에 가도록 그려서

49
00:03:11,070 --> 00:03:15,945
이 사진의 위 부분을 바꿀 수 있습니다.

50
00:03:15,945 --> 00:03:20,805
그리고 shortcut 실제로 ReLU 비선형성 앞쪽에 추가되기 때문에

51
00:03:20,805 --> 00:03:26,220
이 두 번째 레이어에 그리겠습니다.

52
00:03:26,220 --> 00:03:27,570
이렇게 하면 여기 있는 이 각각의 노드들에

53
00:03:27,570 --> 00:03:30,560
선형함수와 ReLU 적용됩니다.

54
00:03:30,560 --> 00:03:34,320
따라서 a[l]은 선형 부분이면서, ReLU 부분 앞쪽인 곳에 들어가게 됩니다.

55
00:03:34,320 --> 00:03:37,815
때로는 shortcut이라는 용어 대신,

56
00:03:37,815 --> 00:03:40,485
skip connection 이라는 용어를 듣게 될 텐데요,

57
00:03:40,485 --> 00:03:44,835
이것은 신경망 속으로 더 깊숙이 들어가 정보를 처리하기 위해

58
00:03:44,835 --> 00:03:51,090
a[l]이 레이어를 하나씩 혹은 두 개씩 건너뛰는 것을 가리키는 것입니다.

59
00:03:51,090 --> 00:03:54,030
ResNet을 발명한

60
00:03:54,030 --> 00:03:55,950
Kaiming He 와 Xiangyu Zhang, Shaoqing Ren, Jian Sun

61
00:03:55,950 --> 00:03:58,925
그리고 Shaoqing Ren 과 Jian Sun.

62
00:03:58,925 --> 00:04:02,010
이들이 찾아낸 것은 잔류블럭을 사용해서

63
00:04:02,010 --> 00:04:05,920
더 깊게 신경망을 훈련할 수 있게 해 준 것입니다.

64
00:04:05,920 --> 00:04:10,785
ResNet을 구축하는 방법은 이러한 잔여 블럭을 많이 사용하여

65
00:04:10,785 --> 00:04:15,695
심층 신경망을 형성하기 위해 이렇게 겹쳐 쌓아 올리는 것입니다.

66
00:04:15,695 --> 00:04:18,150
이 네트워크를 보시죠.

67
00:04:18,150 --> 00:04:19,730
이것은 잔류블럭이 아니라

68
00:04:19,730 --> 00:04:22,950
plain network (평범한 망)라고 부릅니다.

69
00:04:22,950 --> 00:04:26,830
이는 ResNet 논문 용어입니다.

70
00:04:26,830 --> 00:04:28,675
이것을 ResNet으로 바꾸려면

71
00:04:28,675 --> 00:04:31,050
이런 식으로 연결이 짧긴 하지만,

72
00:04:31,050 --> 00:04:36,475
모든 skip connection을 추가해야 합니다.

73
00:04:36,475 --> 00:04:39,875
따라서 레이어를 하나씩 건너뛰면

74
00:04:39,875 --> 00:04:44,710
이전 슬라이드에 보셨듯이,

75
00:04:44,710 --> 00:04:49,520
각각을 잔류블럭으로 바꾸기 위해서는 추가적인 변경을 하게 됩니다.

76
00:04:49,520 --> 00:04:53,770
따라서 이 그림은 5개의 잔류블럭이 같이 쌓여있는 것이고,

77
00:04:53,770 --> 00:04:56,565
이는 곧 잔류블럭이 됩니다.

78
00:04:56,565 --> 00:04:59,615
그리고 만약 여러분이

79
00:04:59,615 --> 00:05:02,620
기울기 강하 혹은 더 괜찮은 적합 알고리즘 등의

80
00:05:02,620 --> 00:05:04,120
표준 적합 알고리즘을 사용해서

81
00:05:04,120 --> 00:05:07,340
평범한 망을 학습시키기 고자 한다면,

82
00:05:07,340 --> 00:05:10,270
모든 부가적인 레지듀얼이나,

83
00:05:10,270 --> 00:05:14,030
방금 그린 모든 부가적인 shortcut 즉 skip connection 없이도,

84
00:05:14,030 --> 00:05:18,965
경험에 기인하여, 레이어 개수가 늘어남에 따라

85
00:05:18,965 --> 00:05:21,100
훈련 오류는 시간이 지나면서 줄어드는 걸 볼 수 있을 겁니다.

86
00:05:21,100 --> 00:05:24,240
하지만 그리고 나서 다시 올라가는 경향을 보입니다.

87
00:05:24,240 --> 00:05:29,170
이론적으로 신경망을 더 깊게 만들면

88
00:05:29,170 --> 00:05:32,935
트레이닝 세트에서 더 잘 수행해야 합니다

89
00:05:32,935 --> 00:05:35,155
그렇습니다. 따라서 이론적으로

90
00:05:35,155 --> 00:05:37,815
더 깊은 네트워크를 갖는 이론은 도움이 됩니다

91
00:05:37,815 --> 00:05:40,435
그러나 사실상 현실적으로

92
00:05:40,435 --> 00:05:42,925
평범한 망을 사용하고 ResNet은 없기 때문에

93
00:05:42,925 --> 00:05:45,890
깊게 평범한 망을 사용하는 것

94
00:05:45,890 --> 00:05:50,220
여러분의 최적화 알고리즘이 훨씬 혹독하게 학습하고 있다는 것을 의미합니다.

95
00:05:50,220 --> 00:05:51,685
현실적으로

96
00:05:51,685 --> 00:05:55,865
너무 깊은 네트워크를 고르면 학습 오류가 점점 더 심해지지만

97
00:05:55,865 --> 00:06:01,530
ResNet의 경우, 레이어 숫자가 많아진다 하더라도

98
00:06:01,530 --> 00:06:06,120
학습 수행 오류를 저하시킬 수 있습니다.

99
00:06:06,120 --> 00:06:10,030
우리가 100 개가 넘는 레이어로 네트워크를 훈련하더라도.

100
00:06:10,030 --> 00:06:12,820
지금은 실제로 많은 부분을 사용했는지는 모르겠지만

101
00:06:12,820 --> 00:06:17,845
레이어가 천 개가 넘는 네트워크로 실험하는 사람들이 있습니다.

102
00:06:17,845 --> 00:06:20,230
이런 액티베이션을 취함으로써

103
00:06:20,230 --> 00:06:24,950
중간에 있는 액티베이션의 X가 신경망에 더 깊이 들어갈 수 있게 해줍니다.

104
00:06:24,950 --> 00:06:30,355
이는 사라지거나 폭발적으로 증가하는 gradient 문제점들을 처리하도록 도와주고,

105
00:06:30,355 --> 00:06:31,930
수행 시 눈에 띄는 손실 없이

106
00:06:31,930 --> 00:06:36,220
신경망을 훨씬 깊게 훈련하도록 해줍니다.

107
00:06:36,220 --> 00:06:39,370
어느 시점에 이건 안정화되어, 수평 하게 되겠죠.

108
00:06:39,370 --> 00:06:43,090
그리고 이건 더 깊은 네트워크에는 그리 도움이 되지 않을 겁니다.

109
00:06:43,090 --> 00:06:49,120
하지만 ResNet은 심층 신경망 깊이 훈련하도록 도와주는 데에는 그리 효과적이지는 않습니다.

110
00:06:49,120 --> 00:06:52,645
이제 ResNets의 작동 방식을 개괄적으로 살펴 보았습니다.

111
00:06:52,645 --> 00:06:55,495
그리고 사실, 이번 주 프로그래밍 연습에서는

112
00:06:55,495 --> 00:06:59,205
이러한 아이디어를 구현하고, 여러분을 위해 작동하는 것을 보게 될 것입니다.

113
00:06:59,205 --> 00:07:02,350
지만 다음 강의에서는, ResNet이 왜 그렇게 잘 작동하는지에 대해서

114
00:07:02,350 --> 00:07:06,160
훨씬 나은 관점들을 공유하고 싶습니다.

115
00:07:06,160 --> 00:07:07,730
다음 강의로 가보시죠.