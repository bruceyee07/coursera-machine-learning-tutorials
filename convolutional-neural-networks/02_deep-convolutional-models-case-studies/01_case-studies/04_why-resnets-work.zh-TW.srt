1
00:00:00,000 --> 00:00:02,970
為什麼深度殘差網路作用這麼好？

2
00:00:02,970 --> 00:00:07,830
我們來進行一個例子說明
為什麼深度殘差網路作用這麼棒

3
00:00:07,830 --> 00:00:12,390
至少在於您如何可以讓您的網路更深更深而真的

4
00:00:12,390 --> 00:00:17,540
不會傷害且能在訓練集上至少做好訓練

5
00:00:17,540 --> 00:00:22,185
也希望您從這個學程的第三課程中學到

6
00:00:22,185 --> 00:00:25,845
在訓練集上做好訓練，通常先決條件是在

7
00:00:25,845 --> 00:00:29,800
測試集上能保持或更深入都能做好

8
00:00:29,800 --> 00:00:33,570
所以能夠訓練好深度殘差網路

9
00:00:33,570 --> 00:00:38,650
在測試集上是好的第一步，我們來看一個例子

10
00:00:38,650 --> 00:00:43,910
我們在前面的影片中看到，如果您讓網路更深

11
00:00:43,910 --> 00:00:49,195
它可能會傷害到您的網路
在訓練集上做訓練

12
00:00:49,195 --> 00:00:53,420
這是為什麼有時候
您不想讓您的網路太深

13
00:00:53,420 --> 00:00:58,555
但當您訓練深度殘差網路時，
這不再是事實，或者至少不一定是事實

14
00:00:58,555 --> 00:01:01,365
讓我們來跑個實際的例子

15
00:01:01,365 --> 00:01:04,980
假設您餵一個 X 進入

16
00:01:04,980 --> 00:01:10,365
一個大的神經網路，輸出一個啟動值 a[l]

17
00:01:10,365 --> 00:01:14,080
假設對於這個例子，您要修改

18
00:01:14,080 --> 00:01:19,150
這個神經網路，讓它更深一些

19
00:01:19,150 --> 00:01:20,755
使用相同的神經網路

20
00:01:20,755 --> 00:01:23,035
然後這個輸出 a[l]

21
00:01:23,035 --> 00:01:28,240
我們要加入一些額外的層到這個網路

22
00:01:28,240 --> 00:01:33,635
我們加一層在這裡，然後另一層在這裡

23
00:01:33,635 --> 00:01:38,195
這時的輸出會是 a[l+2]

24
00:01:38,195 --> 00:01:40,930
只是現在讓它成為深度殘差網路區塊

25
00:01:40,930 --> 00:01:46,060
一個殘差區塊，有額外的捷徑

26
00:01:46,060 --> 00:01:47,380
為了便於討論

27
00:01:47,380 --> 00:01:52,690
假設整個網路我們都使用 ReLU 啟動函數

28
00:01:52,690 --> 00:01:58,330
所以所有啟動將會是大於或等於 0

29
00:01:58,330 --> 00:02:01,955
只有輸入值  Ｘ 例外

30
00:02:01,955 --> 00:02:07,685
因為 ReLU 啟動函數的輸出值不是 0 就是正數

31
00:02:07,685 --> 00:02:10,510
現在我們來看看 a[l+2] 會是什麼？

32
00:02:10,510 --> 00:02:15,025
從前面的影片複製過來

33
00:02:15,025 --> 00:02:21,715
a[l+2] 會是 ReLU 應用在 z[l+2] 上

34
00:02:21,715 --> 00:02:26,505
然後再加入  a[l]，這個多出來的 a[l]

35
00:02:26,505 --> 00:02:31,750
是從我們剛加入的捷徑中跳過連結來的

36
00:02:31,750 --> 00:02:33,300
而如果我們將此展開

37
00:02:33,300 --> 00:02:34,710
這個會等於 g of w[l+2]

38
00:02:34,710 --> 00:02:43,570
乘上 a[l+1] 加上 b[l+2]

39
00:02:43,570 --> 00:02:48,810
所以這是等於 z[l+2]，再加上 a[l]

40
00:02:48,810 --> 00:02:53,450
請注意，如果您使用 L2 正則化，權重衰減

41
00:02:53,450 --> 00:02:58,435
這個會縮小 w[l+2] 的值

42
00:02:58,435 --> 00:03:02,540
如果您使用權重衰減到 b，
它也會縮小，雖然

43
00:03:02,540 --> 00:03:06,665
我猜實作上，有時候您會有時候不會
把權重衰減用在 b 上

44
00:03:06,665 --> 00:03:12,050
但這裡要注意的重點是 w

45
00:03:12,050 --> 00:03:16,175
而如果 w[l+2] 等於 0

46
00:03:16,175 --> 00:03:20,975
假設為了討論方便起見，假設 b 也是為 0

47
00:03:20,975 --> 00:03:25,820
這一項可以去掉，因為等於 0

48
00:03:25,820 --> 00:03:27,995
然後 g of a[l]

49
00:03:27,995 --> 00:03:36,805
這個就等於 a[l]，因為我們假設使用 ReLU 啟動函數

50
00:03:36,805 --> 00:03:39,635
所以所有啟動都是非負值 

51
00:03:39,635 --> 00:03:43,930
g of a[l] 應用 ReLU 到一個非負的項

52
00:03:43,930 --> 00:03:46,800
您會得到 a[l]

53
00:03:46,800 --> 00:03:56,270
這裡顯示的是恆等函數
對於殘差區塊是容易去學習的

54
00:03:56,270 --> 00:04:03,515
而得到 a[l+2] 等於 a[l] 很容易的，
因為使用了跳過連結

55
00:04:03,515 --> 00:04:08,060
而這表示加入這兩層到您的神經網路

56
00:04:08,060 --> 00:04:11,885
它們並不會傷害到您神經網路的能力，跟

57
00:04:11,885 --> 00:04:16,205
原來的簡單網路沒有這額外這兩層的一樣

58
00:04:16,205 --> 00:04:20,930
因為對它而言
學習恆等函數是很容易的，只要複製

59
00:04:20,930 --> 00:04:26,305
a[l] 到 a[l+2] 即使多了兩層

60
00:04:26,305 --> 00:04:30,250
而這是為什麼增加額外兩層

61
00:04:30,250 --> 00:04:34,910
加入這個殘差區塊到

62
00:04:34,910 --> 00:04:40,392
中間或者後面的神經網路，不會傷害到它的績效

63
00:04:40,392 --> 00:04:43,910
但當然我們的目標
不只是不傷害績效

64
00:04:43,910 --> 00:04:48,650
而是幫助績效，您可以想像如果所有

65
00:04:48,650 --> 00:04:51,770
這些隱藏單元
如果真的學習到有用的東西

66
00:04:51,770 --> 00:04:55,745
或許您可以學得比恆等函數更好

67
00:04:55,745 --> 00:05:00,970
而一般網路到很深層時會出錯
如果不使用

68
00:05:00,970 --> 00:05:03,650
殘差區塊或跳過連結時

69
00:05:03,650 --> 00:05:06,555
當您讓網路越來越深時

70
00:05:06,555 --> 00:05:10,865
實際上很難對它選擇參數

71
00:05:10,865 --> 00:05:14,570
即使是學習恆等函數
也因為這樣，很多層

72
00:05:14,570 --> 00:05:19,025
的結果造成越來越糟，而不是越來越好

73
00:05:19,025 --> 00:05:22,430
而我想殘差網路可行的主要原因是

74
00:05:22,430 --> 00:05:26,360
對於這些額外的層很容易學習

75
00:05:26,360 --> 00:05:30,260
恆等函數，好像保證他不會傷害

76
00:05:30,260 --> 00:05:35,450
績效，而常常或許您很幸運
這樣做會有助於績效

77
00:05:35,450 --> 00:05:39,530
至少立於不敗之地，不至於

78
00:05:39,530 --> 00:05:44,435
傷害績效，而梯度下降可以從此改進，只會更好

79
00:05:44,435 --> 00:05:46,450
有關殘差網路有另一個細節

80
00:05:46,450 --> 00:05:50,570
值得討論的是透過這個『加入』

81
00:05:50,570 --> 00:05:54,995
我們假設 z[l+2] 跟 a[l] 是同維度

82
00:05:54,995 --> 00:06:01,175
所以您在深度殘差網路
會見到很多使用相同卷積

83
00:06:01,175 --> 00:06:03,665
所以這個維度會

84
00:06:03,665 --> 00:06:08,280
等於這個維度，我猜這一層是輸出層

85
00:06:08,280 --> 00:06:12,450
所以我們真的可以做這種捷徑連結

86
00:06:12,450 --> 00:06:15,615
因為相同卷積保持維度

87
00:06:15,615 --> 00:06:19,205
所以讓您很容易的應用

88
00:06:19,205 --> 00:06:25,575
這種捷徑，進行兩個同維度向量的加法

89
00:06:25,575 --> 00:06:30,635
假使輸入跟輸出有不同的維度，舉個例

90
00:06:30,635 --> 00:06:36,420
如果這個 Z 是 128 維度，

91
00:06:36,420 --> 00:06:40,660
a[l] 是 256 維度，舉個例

92
00:06:40,660 --> 00:06:46,640
您需要做的是加上一個額外的矩陣稱為 Ws 

93
00:06:46,640 --> 00:06:54,905
而 Ws 在這個例子會是 a[l] 256 乘 128 維度的矩陣

94
00:06:54,905 --> 00:06:59,930
所以 Ws 乘上 a[l] 會變成 256 維度

95
00:06:59,930 --> 00:07:01,550
這個加法現在是

96
00:07:01,550 --> 00:07:05,300
兩個 256 維度的向量，有一些事您可以對 Ws 做

97
00:07:05,300 --> 00:07:07,940
它可以是一個我們要學習的矩陣

98
00:07:07,940 --> 00:07:10,490
它可以是一個固定的矩陣，我們只要

99
00:07:10,490 --> 00:07:13,220
填 0 也就是拿 a[l] 然後用 0

100
00:07:13,220 --> 00:07:20,000
填入成為 256 維度，這兩種方式我想都可以用

101
00:07:20,000 --> 00:07:23,650
最後，讓我們來看深度殘差網路
作用在影像上

102
00:07:23,650 --> 00:07:27,870
這些影像來自於 Harlow 的論文

103
00:07:27,870 --> 00:07:35,465
這是一個一般網路的例子，您輸入一個影像

104
00:07:35,465 --> 00:07:39,155
然後有一些卷積層

105
00:07:39,155 --> 00:07:44,300
直到最終您有一個 softmax 輸出

106
00:07:44,300 --> 00:07:46,010
將這個轉換為深度殘差網路

107
00:07:46,010 --> 00:07:50,945
您加入這些額外的跳過連結

108
00:07:50,945 --> 00:07:53,620
我只會提到一些細節

109
00:07:53,620 --> 00:07:57,744
這裡面有很多 3乘3 卷積，大部分都是

110
00:07:57,744 --> 00:08:01,435
3乘3 相同卷積

111
00:08:01,435 --> 00:08:05,990
這是為什麼您加入了
相同維度的特徵向量

112
00:08:05,990 --> 00:08:08,360
與其使用全連結層

113
00:08:08,360 --> 00:08:11,820
這些確實是卷積層但因為是相同卷積

114
00:08:11,820 --> 00:08:21,360
維度被保持著，所以 z[l+2] 加 a[l] 用加法是有意義的

115
00:08:21,360 --> 00:08:25,085
跟您之前看過很多的網路類似

116
00:08:25,085 --> 00:08:29,540
您有一些卷積層然後

117
00:08:29,540 --> 00:08:34,410
偶而用池層，或是類似池層

118
00:08:34,410 --> 00:08:36,170
而每當發生這種層

119
00:08:36,170 --> 00:08:42,200
您需要調整維度，
就像我們在前面的投影片一樣

120
00:08:42,200 --> 00:08:44,815
您可以使用 Ws 矩陣

121
00:08:44,815 --> 00:08:46,955
然後在這些網路常見到的

122
00:08:46,955 --> 00:08:50,724
您會有卷積，卷積，池層，卷積，卷積，池層<unknown>

123
00:08:50,724 --> 00:08:52,220
最後您會用

124
00:08:52,220 --> 00:08:56,915
一個全連結層，跟 Softmax 來做預測

125
00:08:56,915 --> 00:08:58,340
這就是深度殘差網路

126
00:08:58,340 --> 00:09:02,060
接下來，有一種很有趣的方式

127
00:09:02,060 --> 00:09:05,955
在神經網路使用 1乘1 的過濾器

128
00:09:05,955 --> 00:09:07,490
1乘1 卷積

129
00:09:07,490 --> 00:09:10,280
為什麼要用 1乘1 卷積

130
00:09:10,280 --> 00:09:12,220
我們在下一段影片來看看