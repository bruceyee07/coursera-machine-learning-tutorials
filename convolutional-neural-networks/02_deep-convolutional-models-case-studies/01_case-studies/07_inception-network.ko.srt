1
00:00:00,190 --> 00:00:01,320
이번 비디오에서는, 

2
00:00:01,320 --> 00:00:06,420
인셉션망의 기본적인 빌딩블럭들을 보았습니다.

3
00:00:06,420 --> 00:00:10,680
이번 강좌에서는, 이러한 빌딩블럭을 결합하여

4
00:00:10,680 --> 00:00:12,780
고유한 인셉션망을 구축하는 방법을 알아보겠습니다.

5
00:00:13,910 --> 00:00:17,970
인셉션 모듈은 이전 레이어로부터 나온 액티베이션이나

6
00:00:17,970 --> 00:00:20,260
아웃풋을 인풋으로 사용합니다.

7
00:00:20,260 --> 00:00:25,860
논의를 위해, 우리의 이전 강의에서 본

8
00:00:25,860 --> 00:00:28,030
28 x 192 x 192를 사용합시다.

9
00:00:28,030 --> 00:00:35,130
심도 있게 작업했던 이 예제는 1 x 1 다음에 5 x 5 레이어로 이어져 있습니다.

10
00:00:35,130 --> 00:00:41,120
따라서, 1 x 1 은 16개의 채널을 가지고 있고,

11
00:00:41,120 --> 00:00:48,170
그러면, 5 x 5는 28 x 28의 32개 채널을 아웃풋 할 것입니다.

12
00:00:49,700 --> 00:00:53,790
이것은 이전 강의의 마지막 슬라이드에서 작업 한 예입니다.

13
00:00:54,900 --> 00:01:02,260
그런 다음 3 x 3컨볼루션에 계산을 저장하려면 
여기에서도 같은 작업을 수행 할 수 있습니다.

14
00:01:02,260 --> 00:01:08,310
그리고 나서 3 x 3 는 28 x 28 x 80 을 아웃풋합니다.

15
00:01:09,390 --> 00:01:14,100
그리고 나면, 아마도 1 x 1 
컨볼루션도 고려하고 싶어 질 것입니다.

16
00:01:14,100 --> 00:01:18,481
1 x 1 컨볼루션 뒤에 나오는 또 다른 
1 x 1 컨볼루션은 작업할 필요가 없습니다.

17
00:01:18,481 --> 00:01:23,409
따라서, 하나의 작업 단계만 있는 것이죠. 
그리고 이것이 28 x 28 x 64를 아웃풋한다고 가정 해 봅니다

18
00:01:23,409 --> 00:01:31,900
그리고 마지막으로 pooling 레이어입니다.

19
00:01:34,000 --> 00:01:35,730
이제 여기서 우리는 재미있는 것을 해 볼 텐데요,

20
00:01:35,730 --> 00:01:40,154
마지막에 이러한 모든 아웃풋을 연결하기 위해

21
00:01:40,154 --> 00:01:44,073
동일한 유형의 패딩을 pooling에 사용하므로

22
00:01:44,073 --> 00:01:47,480
아웃풋 높이와 너비는 여전히 28 x 28입니다

23
00:01:47,480 --> 00:01:53,300
따라서 이 출력을 여기 다른 출력과 연결할 수 있습니다.

24
00:01:53,300 --> 00:01:57,842
max pooling을 수행하는 경우 
동일한 패딩을 사용하더라도

25
00:01:57,842 --> 00:01:59,917
3 x 3의 필터가 맞습니다.

26
00:01:59,917 --> 00:02:07,016
여기 아웃풋은 28 x 28 x 192 입니다.

27
00:02:07,016 --> 00:02:10,790
여기에 인풋한 것과 동일한 수의 채널과

28
00:02:10,790 --> 00:02:15,570
동일한 깊이를 갖게 됩니다.

29
00:02:15,570 --> 00:02:19,252
그래서 이것은 많은 채널을 
가지고 있는 것처럼 보입니다.

30
00:02:19,252 --> 00:02:23,752
그래서 우리가 할 일은 채널의 수를 줄이기 위해

31
00:02:23,752 --> 00:02:28,607
1 x 1 컨볼루션 강의에서 본 대로

32
00:02:28,607 --> 00:02:31,541
1 x 1 컨볼 레이어를
하나 추가하는 것입니다.

33
00:02:31,541 --> 00:02:37,730
그러면 이 28 x 28 x 32 로 줄일 수 있죠.

34
00:02:37,730 --> 00:02:44,770
말하자면, 여러분이 하는 방식은 1 x 1 x 192

35
00:02:44,770 --> 00:02:49,660
차원의 32개의 필터를 사용하는 것입니다.

36
00:02:49,660 --> 00:02:54,110
그래서 아웃풋 다이멘션이 32로 
줄어든 채널의 수가 되는 것입니다.

37
00:02:54,110 --> 00:02:58,460
이렇게 해야 최종 아웃풋에서 모든 채널을 차지하고

38
00:02:58,460 --> 00:03:00,920
있는 pooling 레이어로 되지 않는 것입니다.

39
00:03:02,310 --> 00:03:08,121
마지막으로, 이 모든 블록을 
가져 와서 채널 연결을 수행하고

40
00:03:08,121 --> 00:03:12,865
이 64 + 128 + 32 + 32를 연결하면됩니다

41
00:03:12,865 --> 00:03:18,165
그리고 이걸 다 더하면

42
00:03:18,165 --> 00:03:24,055
28 × 28 × 256 차원 아웃풋이 됩니다.

43
00:03:24,055 --> 00:03:30,879
채널 연결이란 이전 강의에서 본 
대로 블록들을 연결하는 것입니다.

44
00:03:33,918 --> 00:03:39,598
그래서 이것은 하나의 인셉션 모듈입니다. 그리고 인셉션망이

45
00:03:39,598 --> 00:03:44,057
하는 일은 이 모듈들을 함께 모으는 것입니다.

46
00:03:45,624 --> 00:03:50,671
다음은 Szegety et al.의 
논문에서 가져온 인셉션 그림입니다.

47
00:03:53,615 --> 00:03:56,570
그리고 이것에서 여러 번 
반복되는 블록들을 발견하게 됩니다.

48
00:03:56,570 --> 00:03:58,580
어쩌면 이 그림은 정말 복잡해 보일 수도 있지만,

49
00:03:58,580 --> 00:04:03,563
그 블록 중 하나를 보시면, 이 블록은 기본적으로

50
00:04:03,563 --> 00:04:07,920
이전 슬라이드에서 본 인셉션 모듈입니다.

51
00:04:10,046 --> 00:04:17,085
그리고 제가 설명하지 않을 세부 
사항임을 전제로 해서,

52
00:04:17,085 --> 00:04:19,460
이것은 다른 인셉션 블록입니다.

53
00:04:19,460 --> 00:04:23,200
여기에는 높이와 너비의 크기를 변경하기위한

54
00:04:24,250 --> 00:04:25,800
max pooling 레이어가 추가로 있습니다

55
00:04:25,800 --> 00:04:28,140
하지만 또 다른 인셉션 블록이 있고,

56
00:04:28,140 --> 00:04:31,020
그리고 높이와 너비를 바꿀 수 
있는 또 다른 max pooling이 있습니다.

57
00:04:31,020 --> 00:04:33,280
하지만, 기본적으로 또 다른 인셉션 블록이 있고,

58
00:04:33,280 --> 00:04:37,370
그 인셉션망은 네트워크의 여러 위치에 대해

59
00:04:37,370 --> 00:04:40,930
반복적으로 학습 한 많은 블록들입니다.

60
00:04:40,930 --> 00:04:44,509
이전 슬라이드의 인셉션블록을 이해하셨다면

61
00:04:44,509 --> 00:04:46,914
인셉션망도 이해할 수 있습니다.

62
00:04:49,518 --> 00:04:53,403
원래의 연구 논문을 읽어보면 이 
인셉션망에 대한 마지막 세부사항이 나오는데요,

63
00:04:53,403 --> 00:04:55,430
원래의 연구 논문을 읽어보면 이 
인셉션망에 대한 마지막 세부사항이 나오는데요,

64
00:04:55,430 --> 00:04:59,311
이게 바로 제가 여기에 
덧붙여놓은 추가적인 곁가지들입니다.

65
00:05:01,835 --> 00:05:03,440
자, 그래서 이 곁가지들은 무엇을 합니까?

66
00:05:03,440 --> 00:05:07,800
음, 네트워크의 마지막 몇 레이어는 완전 연결 레이어이고,

67
00:05:07,800 --> 00:05:11,360
그 뒤에 예측을 해보려 하는 softmax 레이어가 있습니다.

68
00:05:11,360 --> 00:05:15,430
이 곁가지들은 숨겨진 레이어를 가져 와서

69
00:05:15,430 --> 00:05:17,760
예측하는 데에 그 레이어를 사용하는 것입니다.

70
00:05:17,760 --> 00:05:22,140
그래서 이것은 실제로 softmax 
아웃풋이며, 이것 역시 그렇습니다,

71
00:05:22,140 --> 00:05:23,952
그리고이 다른 쪽 곁가지는

72
00:05:23,952 --> 00:05:29,173
숨겨져 있는 레이어를 취해서, 
몇몇 레이어 즉, 완전 연결 레이어를 통과합니다.

73
00:05:29,173 --> 00:05:33,243
그리고 softmax가 아웃풋 라벨이 
무엇인지를 예측하려고 시도했습니다.

74
00:05:35,545 --> 00:05:38,798
이것을 인셉션망의 또 다른 
세부 사항으로 생각해야 합니다.

75
00:05:38,798 --> 00:05:40,000
이것을 인셉션망의 또 다른 
세부 사항으로 생각해야 합니다.

76
00:05:40,000 --> 00:05:44,460
하지만 그것이 하는 일은 히든유닛이나 중간 레이어에서 조차

77
00:05:44,460 --> 00:05:48,060
컴퓨터의 피처들이 이미지의 아웃풋 예측능력이

78
00:05:48,060 --> 00:05:52,490
그리 나쁘지만은 않게 도와주는 것입니다.

79
00:05:52,490 --> 00:05:56,875
그리고 이것은 인셉션망에 규칙적인 영향을 미치는 것으로 보이며

80
00:05:56,875 --> 00:05:59,667
이 네트워크가 초과 적용되는 것을 방지합니다.

81
00:06:03,048 --> 00:06:07,907
그런데, 이 특정 인셉션망은

82
00:06:07,907 --> 00:06:11,770
Google에서 일하는 
작가들에 의해 만들어졌습니다.

83
00:06:11,770 --> 00:06:18,850
그들은 이걸 이렇게 쓰고, GoogLenet이라고 
불렀는데요, 이는 여러분이 이전 강의에서 배우기도

84
00:06:18,850 --> 00:06:21,350
했던 LeNet network 에 경의를
 표하기 위함이었다고 합니다.

85
00:06:23,460 --> 00:06:29,086
그래서 저는 딥러닝 단체가 매우 협조적이며,

86
00:06:29,086 --> 00:06:30,436
그래서 저는 딥러닝 단체가 매우 협조적이며,

87
00:06:30,436 --> 00:06:32,593
서로의 작업에 대한 강한 건강한 존중이 있는 점이

88
00:06:32,593 --> 00:06:35,865
정말 좋다고 생각합니다

89
00:06:35,865 --> 00:06:37,895
마지막으로 재미있는 사실 하나 있습니다.

90
00:06:37,895 --> 00:06:40,425
인셉션망이라는 용어는 어디에서 왔을까요?

91
00:06:41,585 --> 00:06:47,375
우리가 더 깊게 알아볼 필요가 있기 때문에, 
이 인셉션 논문은 이 사진을 인용하고 있습니다.

92
00:06:47,375 --> 00:06:52,315
이 URL은 실제 논문에서 사용된 참조이며,

93
00:06:52,315 --> 00:06:54,320
이 이미지로 연결해줍니다.

94
00:06:54,320 --> 00:06:57,610
인셉션 이라는 제목의 이 영화를 보셨다면,

95
00:06:57,610 --> 00:07:00,070
아마도이 meme이 의미가 있을지 모르지만,

96
00:07:00,070 --> 00:07:05,380
저자들은 이 이미지를 실제로 
더 심층 신경망을 구축해야 할

97
00:07:05,380 --> 00:07:09,040
필요성에 대한 동기로서 인용하고 있습니다.

98
00:07:09,040 --> 00:07:12,890
그리고 이것이 그들이 인셉션 
아키텍처를 제시 한 방법입니다.

99
00:07:12,890 --> 00:07:17,830
연구 논문이 인용문에서 인터넷 
meme를 인용하는 것은 그리 흔한 일은

100
00:07:17,830 --> 00:07:19,030
아닌데요, 하지만 이 경우에는

101
00:07:19,030 --> 00:07:22,165
상당히 효과적이라는 생각이 듭니다.

102
00:07:23,285 --> 00:07:27,015
요약하면, 인셉션 모듈을 이해하면

103
00:07:27,015 --> 00:07:29,765
인셉션망도 이해할 수 있습니다.

104
00:07:29,765 --> 00:07:33,865
인셉션망은 대체로 네트워크 전체에 
걸쳐 여러 번 반복되는 인셉션모듈입니다.

105
00:07:33,865 --> 00:07:35,435
인셉션망은 대체로 네트워크 전체에 
걸쳐 여러 번 반복되는 인셉션모듈입니다.

106
00:07:35,435 --> 00:07:40,025
인셉션 모듈을 개발 한 이후로

107
00:07:40,025 --> 00:07:43,760
저자와 다른 사람들은 이 모듈을 
기반으로 다른 버전을 만들었습니다.

108
00:07:43,760 --> 00:07:49,090
따라서 인셉션 알고리즘의 최신 
버전에 대한 연구 논문이 있으며,

109
00:07:49,090 --> 00:07:53,380
Inception V2, Inception V3, 
Inception V4와 같은 최신 버전의 일부를

110
00:07:53,380 --> 00:07:57,040
사람들이 사용하는 것을 볼 수 있을 것입니다.

111
00:07:57,040 --> 00:07:59,360
또한 skipping connection을 가진

112
00:07:59,360 --> 00:08:02,800
레지던트 아이디어와 결합 
된 인셉션버전이 있으며

113
00:08:02,800 --> 00:08:05,740
이것이 때로는 더 잘 
효과적으로 작동합니다.

114
00:08:05,740 --> 00:08:10,600
그러나 이러한 모든 
변형은 인셉션 모듈과 그것들 

115
00:08:10,600 --> 00:08:14,710
함께 쌓는 것에 대한 강의에서 배운,

116
00:08:14,710 --> 00:08:17,510
기본 기념에 근거해 구축됩니다.

117
00:08:17,510 --> 00:08:20,530
이 강의들로서, 여러분은 인셉션 논문이나

118
00:08:20,530 --> 00:08:23,940
후속 변형들을 설명하는 다른 논문들도

119
00:08:23,940 --> 00:08:28,790
후속 변형들을 설명하는 다른 논문들도

120
00:08:28,790 --> 00:08:30,020
자, 여기까지입니다.

121
00:08:30,020 --> 00:08:34,820
전문화된 신경망 아키텍처를 공부했습니다.

122
00:08:34,820 --> 00:08:39,650
다음 강의에서는, 여러분 자신의 컴퓨터 
비전 시스템을 만드는 이 알고리즘을

123
00:08:39,650 --> 00:08:43,830
어떻게 사용할 수 있을지에 대해 
실제적인 조언을 드리면서 시작해보고 싶습니다.

124
00:08:43,830 --> 00:08:45,090
다음 비디오로 넘어가겠습니다.