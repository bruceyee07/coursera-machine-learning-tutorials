このビデオでは クラシックな ニューラルネットワーク構造を学ぼう LeNet-5, Alexnet, そしてVGGNet の3つだ これが LeNet-5の構造だ 初めの画像は 32 x 32 x 1 だ LetNet-5は 手書き文字の認識のために作られた このような手書きの数字の画像だ そのためLeNet-5は グレースケール画像で学習する 32 x 32 x 1 の画像で このニューラルネットワークの構造は 先週のビデオで見たものとよく似ている 最初のステップでは ６つの ストライド１の 5 x 5 のフィルターを使う ６フィルターを使うので ここにあるように 28 x 28 x 6 となる ストライド１で パディング無しなので 画像の次元は 32 x 32 から 28 x 28 に削減される 次に LeNetニューラルネットワークは プーリングを適応する この論文が書かれた時は、 平均プーリングが より多く使われていた 今なら 多分 最大プーリングを使うだろう このオリジナルの例では 平均プールは フィルター幅が２でストライドが２のものが使われている その結果 データの次元は 因数２で削減される よって 14 x 14 x 6 ボリュームになる このボリュームの高さと幅は 完全な縮尺で描かれてない 正確な縮尺で このボリュームを描けば 高さと幅は 因数２で縮むからね 次に 次の畳み込み層を適応する 今度は 16個の 5 x 5 フィルターを使う その結果 16チャンネルになる この論文が書かれた1998年には パディングが使わなかった つまりValid畳み込みのみだった そのため 畳み込み層を適用する度に 高さと幅は縮んだ だから 14 x 14 から 10 x 10 に小さくなる それから 別のプーリング層が来て 高さと幅は 因数２で縮小する 結局 5 x 5 になる この 5 x 5 x 16 を掛け算すると 掛け算の結果 400 になる 25 x 16 は 400 だ 次の層は 全結合層だ これら400ノードが 120ニューロンに それぞれ全て接続する そう 全結合層だ 時々 こんな風に 間引いて 400ノードの層を描くよ ここに全結合層が来て 次も全結合層だ そして 最終ステップでは この本質的な84個の特徴を使い 1つの出力にする ここに yハットを出すための もう1つのノードを 描こうとすれば 描ける そして yハットは １０種類の値を取り得る ０~９のどの数字を認識したかによって このニューラルネットワークの現代版では 10通りの分類を出力する ソフトマックス層を使うだろう しかし 当時は LeNet-5は 出力層に異なる分類器を使う 今日では 役に立たないものだ このニューラルネットワークは 現在の標準では 小さいもので 約６万個のパラメータを持っている 今日では ニューラルネットワークは １千万から１億のパラメータを持つこともしばしばだ そして このネットワークよりも 文字通り 千倍大きいネットワークを見ることも珍しくない しかし 1つ分かるのは
ネットワークが深くなるにつれ 左から右へ行くにつれ 高さと幅は 減る傾向があるということだ 32 x 32 が 28 になり 14 になり 10 になり 5 となった
一方で チャンネル数は増加する それは ネットワークの階層が深くなるにつれ
1 から 6 になり 16 になる このニューラルネットワークのもう1つのパターンは
今日でも繰り返し使われているけど １つ もしくは 複数の 畳み込み層を置き
次に 1つのプーリング増を置く そして さらに １つ もしくは 複数の 畳み込み層を置き
1つのプーリング増を置く そして いくつかの全結合層を置き 出力を得る
ってことだ そう こんな風に 層を配置するのは 非常に一般的なんだ では 最後に この論文を読んでみたい人向けになるかもしれないけど 他に異なる点がいくるかある このスライドの残りで より上級者向けの提言をするけど このクラシック論文を読みたい人向けのね 赤で書いたものは 全て 飛ばしてしまっても問題ないからね ここに 面白い歴史の脚注があるけど 全部 見る必要は無い オリジナルの論文を読めば 分かるけど sigmoidとtanhで非線形化していた 当時は ReLU非線形は 使っていなかった だから 論文を見れば sigmoidとtanh が出てくる また このネットワークには 少しおかしな所もある ちょっと変 少なくとも現在の基準から見ればおかしい 例えば nH x nW x nC ネットワークがあり nC チャンネルで
f x f x nC 次元のフィルターがあると 全てのフィルターは これらのチャンネル全てを見るが 当時 コンピュータがとても遅かったので 計算やパラメータの保存のため オリジナルの LeNet-5は いくつかの狂気じみた方法を使っている 異なるフィルターが 入力ブロックの異なるチャンネルを見る そして 論文では それらの詳細に触れている しかし 現在の実装には 今日では この種の複雑さは存在しない そして 現在は行われていないが 当時は行われていた 最後の1つは オリジナルの LeNet-5は プーリングの後に非線形化していたことだ 実際には プーリング層の後に sigmoid を使っていた よって もし この論文を読めば 読むのが 難しいものの1つになろう これから見ていく何本かのビデオに比べて 次のは 簡単に始められるだろう このスライドの多くは その論文の２節と３節から得たものだが 論文の後の節では 他のアイデアについても書かれている "Graph Transformer Networks" と呼ばれるものについて書いてあり それは 今日では 広く使われている よって もし この論文を読もうとするなら 構造について書かれている第２節に絞ることを薦める そして 第３節は ざっと目を通すとよいだろう そこには 沢山の実験と結果が載っており とても興味深い 見せたい ニューラルネットワークの２番目の例は AlexNet だ Alex Krizhevsky の名から名付けられた
彼は この仕事の論文の最初の著者だ 他の著者は Ilya Sutskever と Geoffrey Hinton だ
AlexNet は 227 x 227 x 3 画像から始まる もし 論文を読めば 論文では 224 x 224 x 3 画像となっている しかし 数を見ると 実際には 227 x 227 とする方が 筋が通っている 最初の層では 96個の 11 x 11 フィルターを適用する そして ４という大きめのストライドを使うので 次元数は 55 x 55 に縮小する 大きなストライドにより 大雑把に言って 因数４で小さくなる それから 3 x 3 の最大プーリングを行う f = 3 で ２のストライドだ よって これは ボリュームを 27 x 27 x 96 に落とす それから 5 x 5 のSame畳み込みをすると そうパディングすると 27 x 27 x 276 となる 再び最大プーリング そうすると 高さと幅は 13 となる それから もう1つSame畳み込み Sameパディング すると 13 x 13 x 今度は384フィルター それから 再び 3 x 3 のSame畳み込みで こうなって それから 3 x 3 のSame畳み込みで こうなって それから 最大プールで 6 x 6 x 256 に落ちる これを全て掛けると 6 x 6 x 256 は 9216 よって これを展開すると 9216ノードになる それから ついに 全結合層を入れて それから 最後に ソフトマックスで出力する それは 物体が取り得る1000クラスの内の1つだ このニューラルネットワークには 実は LeNetと多くの共通点がある ただし とても大きいけど 前のスライドの LeNet LeNet-5には 約6万のパラメータがあったけど この AlexNet には 約６千万のパラメータがある そして 実際は とても良く似た基本構成要素があるが より多くの隠れ層を持ち より多くのデータで学習し ImageNet データセットで学習し 本当に 目を見張る性能を持つに至った この構造を LeNetよりも大きく改善させた１面には ReLU 活性化関数を使っている ということがある そして もう一度言うけど もし 論文を読む場合 より進んだ詳細が書いてあるけど それは 論文を読まない場合は関わる必要の無いようなことだ 1つは この論文が書かれた時は GPUは まだ少し遅かった なので ２つのGPUで学習するのに 複雑な方法を取っていた 基本的なアイデアはこうだ 沢山のこれらの層を２つのGPU向けに分割し ２つのGPUが互いにやり取りできるような考え抜かれた方法を使ったんだ 論文では また オリジナルのAlexNet構造は もう一つ別の層を持っていた それは 局所応答正規化 と呼ばれる この種の層は 多くは使われていない それなので それについては話さなかった しかし 局所応答正規化 の基本アイデアは もし これらのブロックの1つを見れば 上段のこれらのボリュームの1つを見れば 話のために この1つを見ると 13 x 13 x 256 局所応答正規化(LRN)がするのは 1つの位置を見て 高さと幅のある1つの位置で 全てのチャンネルに跨って 256個全てをもって 正規化することだ この 13 x 13 画像の各場所で 局所応答正規化を行おうとするのは 非常に高く活性化するニューロンを数多く欲しくないからだ しかし 後で 多くの研究者によって これはそれほど助けにはならないことが分かった だから これは赤で書かれるだろうアイデアの1つだ なぜなら あなたが理解べき重要事項ではないからだ そして 実際 私は 局所応答正規化を使わない
今日学習させるネットワークでは使わない もし ディープラーニングの歴史に興味があるなら AlexNetよりも前でさえ ディープラーニングは 音声認識や他の分野で 勢いを増しつつあった でも 本当にこの論文なんだ 多くのコンピュータ ビジョン会で ディープラーニングが 真剣に見られるようになり コンピュータ ビジョンに使えると人々を信じさせたのは それから 非常に大きな衝撃を持つまでになった コンピュータ ビジョンだけでなく それ以外でも もし これらの論文を 自分で読もうとするなら 本当は このコースでやる必要は無いけど でも もし これらの論文を 自分で読もうとするなら これは 読むのが易しい方だし ちょっと目を通すのもいいだろう AlexNetは 比較的 複雑な構造だったけど ハイパーパラメータが沢山あるしね そこの全ての数は Alex Krizhevsky と共著者が 思いついたものだ このビデオにおける ３番目の 最後の例
VGG もしくは VGG-16 ネットワークと呼ばれるものを見よう VGG-16 ネットについて特筆すべき点は 多くのハイパーパラメータを持つ代わりに 畳み込み層を持つことに重きを置き もっと単純なネットワークを使う その層は 単に ストライド１の3 x 3 フィルターで 常にSameパディングを使う そして 全ての最大プーリング層を ストライド２の 2 x 2 にする そして VGGネットワークの 非常に良い点は ニューラルネットワークの構造を 本当に単純にすることだ では 構造を見てみよう 画像から始まり
最初は ２層の畳み込みだ 3 x 3 フィルターを使う 最初の２層は 64個のフィルターを使う 結果は 224 x 224 なぜなら Same畳み込みだから
そして 64チャンネルだ VGG-16は 比較的深いネットワークなので ここには 全てのボリュームを描かない この行が意味しているのは 前に 224 x 224 x 3 として描いたのと 224 x 224 x 64 に畳み込むもので (より深いボリュームで描かないと) そして もう１つの 224 x 224 x 64 に畳み込むものだ つまり この "[CONV 64] x 2" は ２つの層を
64個のフィルターを持つ２つの畳み込み層を表す そして 前に言ったように フィルターは 常に 3 x 3 で ストライドは１ そして 常に Same畳み込みだ よって 全てのボリュームを描くよりも このネットワークを表すのに 文字を使おう 次に プーリング層を使う プーリング層は 縮小させる 224 x 224 から 何になる？ そう 112 x 112 x 64 それから さらに数個の畳み込み層 これは 128個のフィルターを意味していて
Same畳み込みだから 新しい次元は？ 分かる？ 112 x 112 x 128 だ それから プーリング層
で 新たな次元が何になるか 分かるでしょ？ そして ３つの畳み込み層 256フィルターのね
プーリング層 それから もういくつか畳み込み層 プーリング層 さらにいくつか畳み込み層 プーリング層 それから この最後の 7 x 7 x 512 を得て 全結合層に喰わせる 4096ユニットの全結合層 それから ソフトマックスで 1000クラスの中から１つの出力を得る ところで VGG-16の16は 重みを持っている層が16あることから来ている そして これは かなり大きなネットワークだ このネットワークは 全部で １億３８百万個のパラメータを持っている これは 現在の標準から言っても とても大きい しかし VGG-16構造の単純さが 非常に魅力的にした これの構造は 極めて均質だと言ってよいだろう 数個の畳み込み層に 1つのプーリング層が続き 高さと幅を減らす プーリング層は高さと幅を減らすからね ここに あるね しかし 畳み込み層のフィルター数を見ると ここに64フィルターがあり それから 倍の128 倍の256 倍の512 思うに 著者は 512が十分に大きいと考えて ここでさらに倍にはしなかった しかし このように ステップの度に ほぼ倍にすること 畳み込み層の塊を通る度に 倍にすることは このネットワーク構造の設計の もう1つの 単純な原則だ この構造の 相対的な均一さは 研究者にとって とても魅力的だ 主な欠点は 非常に多くのパラメータを持つ とても大きなネットワークを
学習させなくてはならないことだ もし 文献を読むならば 人々は 時々 VGG-19について話しているのが分かるだろう それは このネットワークの より大きいバージョンだ そして その詳細は 下の Karen Simonyan と Andrew Zisserman から引用した論文で 見ることができる ただし VGG-16は VGG-19 と 殆ど同じで 多くの人は VGG-16を使うだろう しかし これの私が最も気に入っている点は このパターンを持っているとういことだ 深くなるにつれ 高さと幅が減っていき それは プーリング層で 因数２で毎回 減っていき 一方 チャンネル数は 増加していく 新しい畳み込み層の塊りの度に 大体２の因数で増えていく つまり その割合にしたことで とてもシステマティックに 減っていき 増えていく この点が この論文を非常に魅力的にしていると思う これで ３つのクラシック構造は終わりだ これらの論文を読みたいのなら 今読むべきだ AlexNet論文から初めて 次にVGGネット論文を読むことを薦める それから LeNet論文は 読むのが 難しいが 一見する価値のある 良いクラシックだ でも 次に これらのクラシックネットワークを超えて より進んだものを見てみよう さらに強力なニューラルネットワーク構造を
さぁ 次のビデオに進もう