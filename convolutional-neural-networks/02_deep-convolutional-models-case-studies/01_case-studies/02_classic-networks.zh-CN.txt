这节课中 你会学到一些 经典神经网络结构 如LeNet-5 AlexNet和VGGNet 我们来看一下 这是LeNet-5的网络结构 你以一幅图像开始 即 32乘32乘1 而LeNet-5的任务是识别手写数字 可能就像这幅数字图像 LeNet-5就在灰度图像上训练 这就是为什么它是32乘32乘1 该神经网络实际上 （这些灰度图像）与你上周所见的样本类似 第一步， 你用一组6个 你用一组6个5乘5 步长为1的滤波器 因为用 因为用6个滤波器可以生成28乘28乘6的结果 同时步长为1 零填充 图像的尺寸将从32x32降低到28x28 然后LeNet用了一个池化层 而写这篇文章那时 人们更常用均值池化 如果你正在构建一个现代化的变体 你可能会改用最大池化 但在这个例子中 你用宽为2 步长为2的滤波器做均值池化 结果是降维 即高和宽都以2的因子降维 因此我们得到14乘14乘6的体积结果 我猜这些体积的高度和宽度并不是完全按比例绘制的 从技术上讲，如果我将这些体积绘制成比例 高度和宽度会增加一倍 接着你用另一层卷积层 这次你用一组16个滤波器 16个滤波器尺寸为5乘5 因此得到16通道的体积结果 而在这篇文章完成的1998年 人们不怎么用填充 或你总用有效卷积计算 这也就是为什么每次你用卷积层 结果的尺寸都会缩小 所以这就是为什么这里 你会得到从14乘14降到10乘10的结果 然后接着另一池化层 因此高和宽都以2的因子降低 然后得到5乘5的结果 如果你将这些数字相乘 即5乘5乘16 等于400 也就是25乘16 等于400 然后接下来一层是全连接层 其完全连接了 其自身120个神经元中每个都全连接了这400个节点 因此叫做全连接层 有时这里会专门画出 一个有400个节点的图层, 我在这里跳过。 这是一层全连接层 接着是另一层全连接层 然后最后一步是用 这84维特征生成一个最终结果 我想你可在这里再画一个节点来预测y 即ŷ 而ŷ包括10个预测值 与从0到9的10个数字相对应 作为该网络结构的现代版 我们用softmax层来生成10个分类结果 尽管LeNet-5原先是用别的分类器做输出层 而这个分类器现在已经不用了 因此用现在标准来看 这是个小型神经网络 有大概60,000个参数 而如今你经常会见到 包含千万到亿量级参数的神经网络 现在见这些网络已经很正常了 这些字面上比LeNet-5大千倍的神经网络 但当你深入研究网络 值得注意的一个模式是 从左到右 高度和宽度往往会下降 随着尺寸从32乘32降到28 到14 到10和到5 通道数是增加的 随着网络层数的增加 通道数从1增到6到16 另一个神经网络至今依然使用的模式是 你会用一层或多层卷积层 随后是池化层 然后再是一层或多层卷积层 随后池化层 然后是几层全连接层 然后输出 这种层次排列很常见 最后 针对你们中想读这篇论文的人 这里有些不同的变化 在这张幻灯片的剩余部分 我要多做一些说明 只针对你们中那些想读这篇经典文章的人来说 因此我用红色来标记要说的话 (不读的)你可以稳妥地跳过这些 这可能是个有趣的历史相关的脚注 如果你没完全理解也没关系 所以如果你读了原始文献 那时候 人们用sigmoid和tanh非线性函数 而不用ReLU非线性函数 因此你读文章会发现提及的sigmoid和tanh 此外还有些有趣的方式 即以现代标准来看这个网络级联的方式很有趣 例如你会发现 如果有一个nh乘nw乘nc的网络结构 其中nc是通道数 然后用f乘f乘nc维的滤波器 其中每个滤波器是处理了每一维通道 但那时计算机非常慢 为了节省计算量和参数数量 原始LeNet-5有种比较疯狂的计算方法 其中不同滤波器会处理输入块的不同通道 这篇文章会讲到这些细节 而现在先进些的应用则不会有这种复杂方法 最后一件以前做现在没有再做的事是 原始LeNet-5在池化后有非线性处理 我想应是池化层后用了sigmoid非线性 所以如果你读了这篇论文 这是一篇较难的论文 与随后一些视频中讲的论文相比 随后这篇论文更容易开始 这张幻灯片中大部分算法属于论文第二三章 后面几章讨论了其他想法 即图转换网络(GTN) 该网络现在已不再广泛使用 所以如果你想读这篇论文 我推荐多关注讨论LeNet-5结构的第二章 也可以快速看下第三章 第三章很有趣 讲了实验和结果 第二个我想讲的是AlexNet 以一作Alex Krizhevsky命名 另两位是Ilya Sutskever和Geoffrey Hinton AlexNet以227乘227乘3的图像开始 如果你读了这篇论文 本文提及的是224乘224乘3的图像 但如果你检查数字 我想227乘227才合理 第一层用一组96个11乘11 步长4的滤波器 而因为用了大的步长4 图像尺寸缩到55乘55 因为步长大 维度以4的因子快速降低 随后的最大池化层用了3乘3的滤波器 所以f等于3 步长为2 结果体积降到27乘27乘96 然后用相同的5乘5卷积 相同填充 得到27乘27乘256 再次做最大池化 高和宽降到13 然后相同的卷积 相同填充 得到13乘13乘384 然后再次3乘3相同卷积 虽然再次3乘3相同卷积 随后最大池化 结果降到6乘6乘256 如果你把所有数字相乘 6乘6乘256等于9216 因此我们把其展开成9216个节点 然后有几层全连接层 最后用softmax输出结果 即目标可能是1000类中的哪一类 所以这个网络结构实际上很像LeNet 但更大 鉴于之前幻灯片中LeNet-5有60,000个参数 AlexNet大概有6千万参数 而且AlexNet采用 非常相似的构造版块 拥有更多隐藏神经元 在更多数据上训练 AlexNet在ImageNet数据库上训练 (大数据库)使它能有优秀的性能 使AlexNet比LeNet更好的另一因素 是ReLU激活函数的使用 然后 如果你读了这篇文章 文中有些你不需担忧的较难方法 如果你没读文章 其中一个是 写这篇文章的时候 GPU还比较慢 因此该文有在两块GPU上训练的复杂方法 其中基本思想是 网络中很多层被分割到两块不同GPU上 两块GPU可以互相通讯 而且这篇文章也 原始的AlexNet结构还有另一种层 叫做局部响应归一层(LRN) 这种层实际上用得很少 因此我不讲它 但局部响应归一(LRN)的基本思想是 如果你观察这些块 顶上这些体积块中之一 为讨论方便 这块 13乘13乘256 局部响应归一(LRN) 局部响应归一(LRN)做的就是 当你看一个位置 一个位置的高和宽 然后看跨越通道的这个位置 (这个位置)跨越整个256通道然后归一化 这个局部响应归一(LRN)操作是针对处理 该13乘13图像中的每一个位置 但也许你不想要太多高激活率的神经元 随后许多研究者发现该层并不太管用 所以我用红色把这些内容标出来 因为能否理解这个不太重要 实际中在我训练的网络结构中我不会用 局部响应归一 如果你对深度学习的历史感兴趣 我想在AlexNet之前 深度学习开始在语音识别和其他一些领域获得关注 但这篇文章让大部分 计算机视觉领域的研究者开始认真对待深度学习 确信深度学习对计算机视觉有用 随后深度学习产生了巨大的影响力 对计算机视觉和其他领域产生了巨大影响 如果你自己想读一些这方面文章 其实你不需为这门课专门读文章 但如果你确实想读文章 这篇文章比较易懂 值得一读 所以鉴于AlexNet有个比较复杂的结构 有许多超参数 是吧 在哪放置这些参数 是Alex Krizhevsky和搭档必须考虑的问题 下面介绍第三个也是最后一个VGG或叫VGG-16网络 按作者所说关于VGG-16非常值得注意的一点是 与大量的超参数不同 VGG-16结构更简单 更能关注卷积层 即3乘3 步长为1 用相同填充的卷积滤波器 所有最大池化层滤波器都是2乘2 步长为2 VGG有个优点 VGG有个优点是真正简化了神经网络结构 所以我们来认真分析下这个结构 以图像开始 前两层是卷积层 即3乘3的滤波器 且前两层用64个滤波器 得到224乘224乘64的结果 因为用了相同卷积 所以因为VGG-16是个相对深的网络 我不打算把所有体积块画出来 所以这块小图像表示我们曾经 (我们曾经)画过的224乘224乘3的体积 然后是一个卷积生成 224乘224乘64的更深体积 然后是另一层生成224乘224乘64 这里conv64乘2表示用两层64个滤波器的卷积层 如我之前所说 滤波器通常是3乘3 步长为1 因此是相同卷积 不同于画出所有的体积块 我会用文字表示该网络 接下来是池化层 这样池化层会降低 我想 把224乘224降低到多少? 对 降到112乘112乘64 然后又是一对卷积层 这表示128个滤波器 因为是相同卷积 我们看下新维度是多少 对 结果是112乘112乘128 随后是池化层 所以你可以算出新维度是多少 现在再三层卷积层 256个卷积滤波器 然后池化层 然后又一组卷积层 池化层 卷积层 池化层 最后得到7乘7乘512的结果输入全连接层 两层4096个神经元的全连接层 然后是softmax函数输出1000类结果 顺便说 VGG-16中的16 指该网络有16层带权重的层 这是个相当大的网络 该网络总共有1亿3千8百万个参数 即使以现在标准衡量也是很大了 但VGG-16结构的简洁性也非常吸引人 看得出该结构相当统一 有几组卷积层 随后是池化层 以降低高和宽 对吧? 所以池化层来减少高和宽 这样的模式有好几组 而另一方面 如果你看卷积层的滤波器数量 你有64个滤波器 然后两倍到128到256到512 这里我想作者认为512够大 所以没再两倍增加 但这种在每步粗略地双倍增加 或在每组卷积层双倍增加的方式 是设计这个网络时用的另一个简单原则 所以我认为这种相对统一性 这种结构的相对统一性对研究者来说很有趣 而该结构的主要缺点是 以训练参数的数量来看是个非常大的网络 如果你读过文献 有时会看到提及VGG-19 那是VGG-16的更大版本 你可以看论文中的技术细节 引自Karen Simonyan和Andrew Zisserman 然而由于VGG-16跟VGG-19的性能差不多 很多人会用VGG-16 但我最喜欢的是 这种模式 这种高和宽随着网络深入而减少的模式 尺寸每次会(因为池化层)按因子2减少 因为池化层 与此同时通道数量增加 每次用一组卷积层时(通道数)会按因子2增加 所以按这种比率(网络)减少或增加
是非常有条理性的 从这个角度看这篇文章是非常吸引人的 所以这就是三种经典网络结构 如果愿意 你可以读其中一些论文 个人推荐先从AlexNet论文开始 然后VGGNet 然后LeNet论文有点难懂 但一旦读懂它是非常经典的 接下来除了这些经典网络 我们会学一些更先进 更强大的神经网络 让我们开始下节吧