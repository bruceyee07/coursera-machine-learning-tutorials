1
00:00:00,000 --> 00:00:02,970
那么为什么ResNets这么好用呢？

2
00:00:02,970 --> 00:00:07,830
让我们通过一个例子来说明为什么ResNets工作得这么好,

3
00:00:07,830 --> 00:00:12,390
至少在这个意义上,你可以让他们越来越深入而又没有真正地

4
00:00:12,390 --> 00:00:17,540
损害你最起码能让他们在训练集上好好工作的能力

5
00:00:17,540 --> 00:00:22,185
你应该已经从这个课程系列的第三门课理解到，

6
00:00:22,185 --> 00:00:25,845
在训练集上的出色表现通常来是

7
00:00:25,845 --> 00:00:29,800
你出色地控制你在训练集上的深度的前提条件

8
00:00:29,800 --> 00:00:33,570
所以训练ResNet使其能对付

9
00:00:33,570 --> 00:00:38,650
训练集是第一步。我们来看一个例子，

10
00:00:38,650 --> 00:00:43,910
从上一集视频中我们看到，如果你设计了更深层的网络，

11
00:00:43,910 --> 00:00:49,195
它会使得你用训练集训练神经网络的能力下降，

12
00:00:49,195 --> 00:00:53,420
这也是为什么有时你不希望有太深层的神经网络。

13
00:00:53,420 --> 00:00:58,555
但当你训练ResNet的时候就不一样了，

14
00:00:58,555 --> 00:01:01,365
我们来看个例子

15
00:01:01,365 --> 00:01:04,980
如果你有X作为输入，

16
00:01:04,980 --> 00:01:10,365
输入到某个大型神经网络，并输出a[l]，

17
00:01:10,365 --> 00:01:14,080
如果在这里你要调整

18
00:01:14,080 --> 00:01:19,150
神经网络使其层数更深一点，

19
00:01:19,150 --> 00:01:20,755
相同的Big NN，

20
00:01:20,755 --> 00:01:23,035
然后输出a[l]，

21
00:01:23,035 --> 00:01:28,240
我们将要再额外增加几层

22
00:01:28,240 --> 00:01:33,635
先加一层，然后再一层，

23
00:01:33,635 --> 00:01:38,195
然后输出a[l+2]，

24
00:01:38,195 --> 00:01:40,930
我们把这个做成 ResNet块，

25
00:01:40,930 --> 00:01:46,060
一个有着快捷路径的残差块。

26
00:01:46,060 --> 00:01:47,380
为了证明我们的观点，

27
00:01:47,380 --> 00:01:52,690
在这整个神经网络中我们用Relu激活函数，

28
00:01:52,690 --> 00:01:58,330
那么所有激活 (a) 都将会大于等于0，

29
00:01:58,330 --> 00:02:01,955
除了某些特定的输入X。

30
00:02:01,955 --> 00:02:07,685
因为激活输出的数只会是0或者正数。

31
00:02:07,685 --> 00:02:10,510
现在我们来看a[l+2]会是什么。

32
00:02:10,510 --> 00:02:15,025
从之前的视频复制相同的表达式，

33
00:02:15,025 --> 00:02:21,715
a[l+2]将会是由z[l+2]得来，

34
00:02:21,715 --> 00:02:26,505
再加上a[l]，这里加入的a[l]

35
00:02:26,505 --> 00:02:31,750
来自于我们之前添加的跳跃连接。

36
00:02:31,750 --> 00:02:33,300
如果我们将其展开，

37
00:02:33,300 --> 00:02:34,710
就等于

38
00:02:34,710 --> 00:02:43,570
g(w[l+2]*a[l+1]+b[l+2])。

39
00:02:43,570 --> 00:02:48,810
这也就是z[l+2]，再加上a[l]。

40
00:02:48,810 --> 00:02:53,450
这里要注意一下，如果你这里用远离K的L2正则化(regularisation)

41
00:02:53,450 --> 00:02:58,435
这将会减小 w[l+2]的值。

42
00:02:58,435 --> 00:03:02,540
如果你应用正则化于b的话也会导致b值减小，

43
00:03:02,540 --> 00:03:06,665
尽管我觉得在现实中有时候并不总是需要应用于b，

44
00:03:06,665 --> 00:03:12,050
但在这里w才是最需要关注的项。

45
00:03:12,050 --> 00:03:16,175
如果w[l+2]等于0，

46
00:03:16,175 --> 00:03:20,975
同时我们假设b[l+2]也等于0

47
00:03:20,975 --> 00:03:25,820
那么这里所有的项都会消失因为它们都等于0，

48
00:03:25,820 --> 00:03:27,995
那么g(a[l])，

49
00:03:27,995 --> 00:03:36,805
就等于a[l]，因为我们假设我们使用激活函数的值。

50
00:03:36,805 --> 00:03:39,635
所有激活都不是负数，

51
00:03:39,635 --> 00:03:43,930
那么g(a[l])会是对于非负数，

52
00:03:43,930 --> 00:03:46,800
所以你就重新得到了a[l]。

53
00:03:46,800 --> 00:03:56,270
这意味着残差块比较容易学习恒等函数。

54
00:03:56,270 --> 00:04:03,515
由于这个跳跃连接也很容易得到a[l+2]等于a[l]。

55
00:04:03,515 --> 00:04:08,060
这意味着将这两层加入到你的神经网络，

56
00:04:08,060 --> 00:04:11,885
与上面这个没有这两层的网络相比

57
00:04:11,885 --> 00:04:16,205
并不会非常影响神经网络的能力，

58
00:04:16,205 --> 00:04:20,930
因为对于它来说学习恒等函数非常容易，

59
00:04:20,930 --> 00:04:26,305
只需要复制a[l]到a[l+2]，即便它中间有额外两层。

60
00:04:26,305 --> 00:04:30,250
所以这就是为什么添加这额外的两层，

61
00:04:30,250 --> 00:04:34,910
这个残差块到大型神经网络

62
00:04:34,910 --> 00:04:40,392
中间或者尾部并不会影响神经网络的表现。

63
00:04:40,392 --> 00:04:43,910
当然我们的目标并不只是维持原有的表现，

64
00:04:43,910 --> 00:04:48,650
而是帮助获得更好的表现，你可以想象当

65
00:04:48,650 --> 00:04:51,770
这里所有项都学习到有意义的东西的话，

66
00:04:51,770 --> 00:04:55,745
可能要比学习恒等函数更好。

67
00:04:55,745 --> 00:05:00,970
而有极多层的神经网络里比较深层的平织网在没有

68
00:05:00,970 --> 00:05:03,650
这些残茶块代表的跳跃连接情况下，容易出错的地方是

69
00:05:03,650 --> 00:05:06,555
当你的神经网络越来越深层，

70
00:05:06,555 --> 00:05:10,865
它就很难选择参数来学习，

71
00:05:10,865 --> 00:05:14,570
即便是恒等函数也很难，这也是为什么

72
00:05:14,570 --> 00:05:19,025
有很多层数会使你的结果更糟而不是更好。

73
00:05:19,025 --> 00:05:22,430
我认为残差块网络有效的主要原因是

74
00:05:22,430 --> 00:05:26,360
这些额外层学习起恒等函数非常简单，

75
00:05:26,360 --> 00:05:30,260
你几乎总能保证它不会影响总体的表现，

76
00:05:30,260 --> 00:05:35,450
甚至许多时候幸运的话可以提升网络的表现。

77
00:05:35,450 --> 00:05:39,530
至少有个合理的底线，

78
00:05:39,530 --> 00:05:44,435
不会影响其表现，然后梯度下降从这里开始可以改善最终的结果。

79
00:05:44,435 --> 00:05:46,450
残差网络的另一个值得讨论的细节是

80
00:05:46,450 --> 00:05:50,570
在这一项中，

81
00:05:50,570 --> 00:05:54,995
我们是在假定z[l+2]和a[l]是在同一维度的。

82
00:05:54,995 --> 00:06:01,175
所以你将会看到再ResNet中许多相同卷积的应用，

83
00:06:01,175 --> 00:06:03,665
因此这里的维度

84
00:06:03,665 --> 00:06:08,280
和这里，或者是输出层的维度相同。

85
00:06:08,280 --> 00:06:12,450
所以我们可以做这个快捷圈的连接，

86
00:06:12,450 --> 00:06:15,615
因为相同的卷积会保留维度，

87
00:06:15,615 --> 00:06:19,205
也使你运算这个快捷圈

88
00:06:19,205 --> 00:06:25,575
变得更方便，再运算这两个相同维度矢量的和。

89
00:06:25,575 --> 00:06:30,635
如果输入和输出有不同的维度，比如这个例子，

90
00:06:30,635 --> 00:06:36,420
如果这里是128维，

91
00:06:36,420 --> 00:06:40,660
那么a[l+2]是256维。

92
00:06:40,660 --> 00:06:46,640
你需要做的是增加一个额外的矩阵，称它为Ws，

93
00:06:46,640 --> 00:06:54,905
那么这里Ws将会是一个256*128尺寸的矩阵。

94
00:06:54,905 --> 00:06:59,930
使得Ws乘以a[l]变为256维，

95
00:06:59,930 --> 00:07:01,550
这时这边的加法运算

96
00:07:01,550 --> 00:07:05,300
的矢量尺寸都为256维了，与此同时对于Ws你可以做一些其他的事情，

97
00:07:05,300 --> 00:07:07,940
它可以是一个包含已学习到的参数的矩阵，

98
00:07:07,940 --> 00:07:10,490
它可以是一个固定的矩阵，

99
00:07:10,490 --> 00:07:13,220
其中包含许多0的项，即除了a[l]以外

100
00:07:13,220 --> 00:07:20,000
由0来填满256维。这两种办法我觉得都会有效。

101
00:07:20,000 --> 00:07:23,650
最后，让我们来看看ResNet在图像上的应用。

102
00:07:23,650 --> 00:07:27,870
这些是我从Harlow文献中拿来的图像。

103
00:07:27,870 --> 00:07:35,465
这是一个平铺网络的例子，在其中你输入一张图像，

104
00:07:35,465 --> 00:07:39,155
然后有一系列卷基层，

105
00:07:39,155 --> 00:07:44,300
直到最终你有一个softmax输出。

106
00:07:44,300 --> 00:07:46,010
要把这个变成ResNet，

107
00:07:46,010 --> 00:07:50,945
你需要添加那些额外的跳跃连接，

108
00:07:50,945 --> 00:07:53,620
我只提及一小些细节，

109
00:07:53,620 --> 00:07:57,744
这里有许多3＊3卷积，其中大部分

110
00:07:57,744 --> 00:08:01,435
都是相同的3＊3卷积，

111
00:08:01,435 --> 00:08:05,990
这也是为什么你需要做相同维度矢量的相加。

112
00:08:05,990 --> 00:08:08,360
所以与其说是相互充分连接的层，

113
00:08:08,360 --> 00:08:11,820
还不如说这些是卷基层，但由于它们是相同的卷基层，

114
00:08:11,820 --> 00:08:21,360
这里的维度相同，因此z[l+2]+a[l]相加的算数得以成立。

115
00:08:21,360 --> 00:08:25,085
和你之前见过的许多ResNet相同的是，

116
00:08:25,085 --> 00:08:29,540
你有一堆卷基层，然后

117
00:08:29,540 --> 00:08:34,410
偶尔有池化层或者类似池化的层。

118
00:08:34,410 --> 00:08:36,170
每当这些事情发生时，

119
00:08:36,170 --> 00:08:42,200
你可以用我们之前提到的方法调整维度，

120
00:08:42,200 --> 00:08:44,815
你可以用矩阵Ws，

121
00:08:44,815 --> 00:08:46,955
然后在这些网络中常见的是，

122
00:08:46,955 --> 00:08:50,724
你有<unknown>个池

123
00:08:50,724 --> 00:08:52,220
然后在最后你会有

124
00:08:52,220 --> 00:08:56,915
一个充分连接的层来最终用softmax来做出预测。

125
00:08:56,915 --> 00:08:58,340
那么这就是ResNet了。

126
00:08:58,340 --> 00:09:02,060
接下来，会是一个非常有趣的概念，

127
00:09:02,060 --> 00:09:05,955
其中用到有1＊1过滤，1＊1卷积

128
00:09:05,955 --> 00:09:07,490
的神经网络。

129
00:09:07,490 --> 00:09:10,280
所以什么是1＊1卷积？如何使用它？

130
00:09:10,280 --> 00:09:12,220
我们会在下一个视频中提到。