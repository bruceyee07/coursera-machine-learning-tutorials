1
00:00:00,000 --> 00:00:02,470
이번 강의에서는

2
00:00:02,470 --> 00:00:05,418
LeNet-5, AlexNet, VGGNet으로 시작하는 고전 신경망 아키텍처에 대해

3
00:00:05,418 --> 00:00:10,122
배우겠습니다. 한 번 보시죠.

4
00:00:10,122 --> 00:00:12,935
이것이 LeNet-5 아키텍처입니다.

5
00:00:12,935 --> 00:00:15,700
32 x 32 x 1 의 이미지로

6
00:00:15,700 --> 00:00:17,540
시작해보겠습니다.

7
00:00:17,540 --> 00:00:21,520
LeNet-5의 목표는 손으로 쓴 숫자를 인식하는 것이었습니다.

8
00:00:21,520 --> 00:00:25,490
그런 숫자의 이미지 일 것입니다.

9
00:00:25,490 --> 00:00:28,815
그리고 LeNet-5는 그레이 스케일 이미지에 대해 학습되었기 때문에

10
00:00:28,815 --> 00:00:32,180
이것은 32 x 32 x 1입니다

11
00:00:32,180 --> 00:00:34,400
이 신경망 아키텍처는 실제로

12
00:00:34,400 --> 00:00:38,315
지난 주에 본 마지막 예제와 매우 유사합니다. 첫 번째 단계에서는.

13
00:00:38,315 --> 00:00:39,847
먼저

14
00:00:39,847 --> 00:00:41,765
스트라이드 1인여섯 개의 5 x 5 필터를 사용합니다

15
00:00:41,765 --> 00:00:45,220
여섯 개의 필터를 사용합니다.

16
00:00:45,220 --> 00:00:50,218
여기는 곧 20 x 20 x 6이됩니다.

17
00:00:50,218 --> 00:00:52,730
그리고 패딩이 없고, 스트라이드가 1인

18
00:00:52,730 --> 00:00:58,640
이미지 크기는 32 x 32로 28 x 28로 줄어 듭니다.

19
00:00:58,640 --> 00:01:02,295
그런 다음 LeNet 신경망이 pooling을 적용합니다.

20
00:01:02,295 --> 00:01:04,970
그리고 나서 이 논문이 작성됐던 그 때에는

21
00:01:04,970 --> 00:01:07,576
사람들은 평균 pooling을 훨씬 더 많이 사용했었습니다.

22
00:01:07,576 --> 00:01:09,290
하지만, 현대적인 변형을 만드는 경우

23
00:01:09,290 --> 00:01:12,145
아마 max pooling을 대신 사용하게 될 것입니다.

24
00:01:12,145 --> 00:01:13,360
하지만 이 예제에서는

25
00:01:13,360 --> 00:01:17,825
필터 폭 2와 스트라이드 2를 사용하여 풀의 평균을 구하고

26
00:01:17,825 --> 00:01:20,780
크기, 높이 및 너비를

27
00:01:20,780 --> 00:01:22,730
2 배 줄이면

28
00:01:22,730 --> 00:01:28,784
이제 14 x 14 x 6볼륨을 얻게 됩니다.

29
00:01:28,784 --> 00:01:32,180
이 볼륨의 높이와 너비가 완전히 일정한 비례대로 그려지지는 않았을 것입니다.

30
00:01:32,180 --> 00:01:35,210
기술적으로, 이 비례대로 볼륨을 그리면

31
00:01:35,210 --> 00:01:38,150
크기와 높이가 두 배 더 강해집니다.

32
00:01:38,150 --> 00:01:41,180
그런 다음 다른 컨볼루션 레이서를 적용합니다.

33
00:01:41,180 --> 00:01:44,070
이번에는 5 x 5, 16 개의 필터 세트를 사용하므로

34
00:01:44,070 --> 00:01:48,515
다음 볼륨으로 16개의 채널을 가지게 됩니다.

35
00:01:48,515 --> 00:01:52,355
그리고 이 논문 가 1998 년에 작성되었을 때

36
00:01:52,355 --> 00:01:57,200
사람들은 패딩이나 유효한 컨볼루션을 사용하지 않았었습니다.

37
00:01:57,200 --> 00:01:59,635
이는 컨볼루션 레이어를 적용 할 때마다

38
00:01:59,635 --> 00:02:01,965
장점이 많아졌기 때문입니다.

39
00:02:01,965 --> 00:02:03,380
이게 바로 여기처럼

40
00:02:03,380 --> 00:02:06,393
14 x 14 에서 10 x 10 으로 바뀌는 이유입니다.

41
00:02:06,393 --> 00:02:08,580
그런 다음 다른 pooling 레이어를 사용하여

42
00:02:08,580 --> 00:02:11,060
높이와 너비를 2 배로 줄이면

43
00:02:11,060 --> 00:02:13,715
여기서 5 x 5 가 되겠죠.

44
00:02:13,715 --> 00:02:16,640
그리고 이 모든 숫자 이렇게 5 x 5 x 16 곱하면

45
00:02:16,640 --> 00:02:20,375
이것은 400 됩니다.

46
00:02:20,375 --> 00:02:24,020
25의 16 배는 400 이니까요.

47
00:02:24,020 --> 00:02:29,900
그리고 나서 다음 레이어는 400 개의 각 노드와 120 개의 뉴런 모두를 연결하는

48
00:02:29,900 --> 00:02:36,840
완전 연결 레이어이다.

49
00:02:36,840 --> 00:02:38,385
그리고 완전 연결 레이어가 있습니다.

50
00:02:38,385 --> 00:02:41,955
때로는 400 개의 노드가 있는 레이어만을

51
00:02:41,955 --> 00:02:46,080
끌어오는데요, 그건 생각하도록 하겠습니다.

52
00:02:46,080 --> 00:02:49,590
완전 연결 레이어가 있고, 또 다른 완전 연결 레이어가 있습니다.

53
00:02:49,590 --> 00:02:51,690
그리고 나서 마지막 단계는

54
00:02:51,690 --> 00:02:57,280
본질적으로 84 개의 피쳐를 사용하고 그걸 최종아웃풋으로 사용하는 것입니다.

55
00:02:57,280 --> 00:03:01,375
ŷ에 대한 예측을 하기 위해 여기에 노드를 하나 더 그려도 됩니다.

56
00:03:01,375 --> 00:03:04,560
0에서 9까지의 숫자를 인식 할 때 가능한

57
00:03:04,560 --> 00:03:09,090
10 가지 값을 ŷ이 취했습니다

58
00:03:09,090 --> 00:03:11,100
이 신경망의 최신 버전에서는

59
00:03:11,100 --> 00:03:17,300
10 방향 분류 아웃풋이 있는 softmax 레이어를 사용합니다.

60
00:03:17,300 --> 00:03:23,385
그 당시에도, LeNet-5는 사실 아웃풋 레이어에서 다른 분류기를 사용했습니다.

61
00:03:23,385 --> 00:03:25,633
이는 현재 쓸모 없는 것입니다.

62
00:03:25,633 --> 00:03:29,220
그래서 이 신경망은 최신 표준에서는 작았고

63
00:03:29,220 --> 00:03:32,645
약 60,000 개의 매개 변수를 가지고 있었습니다.

64
00:03:32,645 --> 00:03:35,934
오늘날에는 1 천만에서 1 억 개의 파라미터를 가진

65
00:03:35,934 --> 00:03:39,690
신경망을 자주 보게 됩니다.

66
00:03:39,690 --> 00:03:41,850
문자 그대로 이 네트워크보다 약 1,000 배

67
00:03:41,850 --> 00:03:45,295
큰 네트워크를 보는 것은 드문 일이 아닙니다.

68
00:03:45,295 --> 00:03:49,600
그러나 여러분이 보게 되는 것은

69
00:03:49,600 --> 00:03:51,790
네트워크에서 더 깊숙이 왼쪽에서 오른쪽으로 갈수록

70
00:03:51,790 --> 00:03:55,360
높이와 폭이 줄어드는 경향이 있다는 것입니다.

71
00:03:55,360 --> 00:03:57,690
채널 수는 늘어나는 반면, 이렇게 32 x 32에서 28 x 28,

72
00:03:57,690 --> 00:04:03,100
14, 10에서 5로 이렇게 변화된 것이죠

73
00:04:03,100 --> 00:04:11,250
네트워크 레이어로 깊숙이 들어감에 따라, 1에서 6에서 16으로 갑니다.

74
00:04:11,250 --> 00:04:15,400
이 신경망에서 여전히 반복되고 볼 수 있는 또 다른 패턴이 있는데요,

75
00:04:15,400 --> 00:04:20,500
이 신경망은 하나 이상의 컨볼 레이어와 pooling 레이어 있습니다.

76
00:04:20,500 --> 00:04:25,758
그리고 나서 하나 또는 때로는 하나 이상의 컨볼 레이어, pooling 레이어와

77
00:04:25,758 --> 00:04:29,940
열어 완전 연결 레이어 및 아웃풋을 차례로 이어집니다.

78
00:04:29,940 --> 00:04:34,090
따라서 이 유형의 배열은 꽤 일반적입니다.

79
00:04:34,090 --> 00:04:39,515
이제 마침내 이것은 논문을 읽어보려고 하는 사람들을 위한 것 같습니다.

80
00:04:39,515 --> 00:04:41,880
다른 몇 가지가 있습니다.

81
00:04:41,880 --> 00:04:43,690
이 슬라이드의 나머지 부분에서는

82
00:04:43,690 --> 00:04:47,065
이 정통 논문을 읽고자 하는 사람들만을 위해서

83
00:04:47,065 --> 00:04:52,265
좀 더 깊이 있는 내용을 이야기 하려고 합니다.

84
00:04:52,265 --> 00:04:54,903
제가 빨간색으로 쓰는 모든 것은

85
00:04:54,903 --> 00:04:57,490
슬라이드에서 건너뛰셔도 무방합니다.

86
00:04:57,490 --> 00:05:00,520
완전히 이해하지 않아도 괜찮지만

87
00:05:00,520 --> 00:05:04,350
그래도 조금 흥미로운 역사적인 각주도 있을 겁니다.

88
00:05:04,350 --> 00:05:07,990
그 당시의 원본 논문을 읽어보시면

89
00:05:07,990 --> 00:05:12,453
사람들은 Sigmoid(시그모이드)와 Tanh non-linearities(탄비선형성)을 사용했고,

90
00:05:12,453 --> 00:05:16,330
당시 사람들은 가치 비선형성을 사용하지 않고 있었습니다

91
00:05:16,330 --> 00:05:20,065
그래서 논문을 보면, Sigmoid와 Tanh가 언급 된 것을 볼 수 있습니다

92
00:05:20,065 --> 00:05:23,260
그리고 이 네트워크에 대한 재미있는 것들이 있는데요,

93
00:05:23,260 --> 00:05:26,835
현대 표준방식에 의한 유선 방식이라는 점입니다.

94
00:05:26,835 --> 00:05:33,775
예를 들어, nc 채널이 있는 nh x nc x nc 네트워크가 있고,

95
00:05:33,775 --> 00:05:40,985
여러분이 같은 채널이 있는 f x f x nc 차원의 필터를 사용한다면,

96
00:05:40,985 --> 00:05:44,480
채널은 전부 이 채널을 사용하고 있는 것으로 보이죠.

97
00:05:44,480 --> 00:05:47,195
하지만 당시 컴퓨터는 훨씬 느렸습니다.

98
00:05:47,195 --> 00:05:50,230
그래서 일부 파라미터 뿐만 아니라 계산에서도 저장하기 위해

99
00:05:50,230 --> 00:05:53,785
본래의 LeNet-5는 매우 복잡한 방법을 가지고 있습니다.

100
00:05:53,785 --> 00:05:58,040
다른 필터들이 인풋블럭의 다른 채널들을 보도록 하는 것이었죠.

101
00:05:58,040 --> 00:06:00,343
그리고 이 논문은 그 세부 사항에 관해 이야기하지만,

102
00:06:00,343 --> 00:06:07,090
요즘 현대적 실행 법에서는 그런 유형의 복잡성을 가지지 않습니다.

103
00:06:07,090 --> 00:06:12,280
그 당시에는 했지만 요즘은 그다지 하고 있지 않는 한 가지는

104
00:06:12,280 --> 00:06:19,705
원래의 LeNet-5가 pooling을 한 후에 비선형성을 갖지 않는다는 것입니다.

105
00:06:19,705 --> 00:06:25,005
실제로 이것은 pooling 레이어 이후에 Sigmoid 비선형성을 사용한다고 생각합니다.

106
00:06:25,005 --> 00:06:27,130
그래서 여러분이 이 논문을 읽어 보시면 아시겠지만,

107
00:06:27,130 --> 00:06:29,345
다음 후속 강의들에서 다룰 논문보다도

108
00:06:29,345 --> 00:06:32,100
이것은 더 읽기 어려운 논문입니다.

109
00:06:32,100 --> 00:06:34,670
다음 것은 좀 더 시작하기 수월할 것 같습니다.

110
00:06:34,670 --> 00:06:40,135
이 슬라이드의 아이디어 중 대부분은 논문의 섹션 2 와 섹션 3에 대한 것들이고,

111
00:06:40,135 --> 00:06:44,485
논문의 뒤 부분에서는 다른 내용에 대해 다루고 있는데요,

112
00:06:44,485 --> 00:06:47,260
오늘날 널리 사용되지 않는

113
00:06:47,260 --> 00:06:49,215
그래프 변압기 네트워크에 대해 다루고 있습니다.

114
00:06:49,215 --> 00:06:50,935
따라서 이 논문을 읽어 보시려면,

115
00:06:50,935 --> 00:06:55,660
아키텍처에 대해 이야기하는 두 번째 섹션에 초점을 맞추고

116
00:06:55,660 --> 00:06:58,165
흥미로운 실험 및 결과가 많이 있는 세 번째 섹션을

117
00:06:58,165 --> 00:07:01,720
간략하게 살펴 보는 것이 좋습니다.

118
00:07:01,720 --> 00:07:06,155
이제 보여드리려고 하는 신경망의 두 번째 예제는 AlexNet (알렉스넷)입니다.

119
00:07:06,155 --> 00:07:12,510
Alex Krizhevsky의 이름을 따서 명명 것인데요, 이 분은 이 작업을 설명하는 이 논문의 첫 번째 저자입니다.

120
00:07:12,510 --> 00:07:13,725
다른 저자는 Ilya Sutskever와 Geoffrey Hinton이었습니다.

121
00:07:13,725 --> 00:07:21,048
따라서, 알렉스넷 입력은 227 x 227 x 3 이미지로 시작합니다.

122
00:07:21,048 --> 00:07:22,525
그리고 논문을 읽어보시면

123
00:07:22,525 --> 00:07:27,010
224 x 224 x 3의 이미지를 보여줍니다

124
00:07:27,010 --> 00:07:28,120
그러나 숫자를 보면

125
00:07:28,120 --> 00:07:33,100
숫자가 사실 227 x 227 인 경우에만 잘 들어맞습니다.

126
00:07:33,100 --> 00:07:40,230
첫 번째 레이어는 96세트의 스트라이드 4를 가진 11 x 11 필터를 적용합니다.

127
00:07:40,230 --> 00:07:42,740
그래서 큰 스트라이드 4를 사용하기 때문에.

128
00:07:42,740 --> 00:07:45,574
디멘션은 55 x 55로 줄어듭니다.

129
00:07:45,574 --> 00:07:50,930
대략적으로, 스트라이드가 커서 4배로 내려가는 것이죠.

130
00:07:50,930 --> 00:07:55,110
그리고 나서, 3 x 3필터의 max pooling을 적용합니다.

131
00:07:55,110 --> 00:07:57,925
f=3, stride=2

132
00:07:57,925 --> 00:08:04,570
이것을 볼륨을 27 x 27 x 96으로 줄이고,

133
00:08:04,570 --> 00:08:08,530
5 x 5의 같은 컨볼루션

134
00:08:08,530 --> 00:08:14,730
같은 패딩을 수행해서, 결국 27 x 27 x 276 이 됩니다.

135
00:08:14,730 --> 00:08:20,025
다시 max pooling을 적용해서, 높이와 너비를 13으로 줄입니다.

136
00:08:20,025 --> 00:08:23,860
그리고 또 다른 동일한 컨볼루션, 같은 패딩으로

137
00:08:23,860 --> 00:08:29,805
13 x 13 x 384 필터가 됩니다.

138
00:08:29,805 --> 00:08:35,275
3 x 3 의 같은 컨볼루션을 적용하면 이렇게 됩니다.

139
00:08:35,275 --> 00:08:39,680
다시 3 x 3 의 동일한 컨볼류션을 적용해서 이렇게 되죠.

140
00:08:39,680 --> 00:08:45,285
max pooling을 적용하면 이렇게 6 x 6 x 256으로 작아지고

141
00:08:45,285 --> 00:08:52,020
이 숫자들, 6 x 6 x 256 이렇게 곱하면 9,216입니다

142
00:08:52,020 --> 00:08:56,947
그래서 이것을 9216 노드로 풀어 보겠습니다.

143
00:08:56,947 --> 00:09:00,790
그리고 마지막으로 완전 연결 레이어가 몇 개 있습니다.

144
00:09:00,790 --> 00:09:04,250
마지막으로 softmax를 사용하여 아웃풋을 만들도록 합니다.

145
00:09:04,250 --> 00:09:09,515
1000 개 중 하나가 객체를 생성 할 수 있도록 말이죠.

146
00:09:09,515 --> 00:09:16,920
이제, 이 신경망은 실제로 LeNet과 많은 유사점을 가졌지만

147
00:09:16,920 --> 00:09:20,210
훨씬 더 커졌습니다.

148
00:09:20,210 --> 00:09:27,740
이전 슬라이드의 LeNet-5에는 약 60,000 개의 파라미터가 있었지만

149
00:09:27,740 --> 00:09:31,935
알렉스넷에는 약 6 천만 개의 파라미터가 있었습니다.

150
00:09:31,935 --> 00:09:34,024
그리고 그것들은

151
00:09:34,024 --> 00:09:36,925
많은 유사한 기본 빌딩 블럭을 취할 수 있지만

152
00:09:36,925 --> 00:09:40,270
매우 많은 히든 유닛을 가지고 있고, 더 많은 데이터 상에서 학습하고 있습니다.

153
00:09:40,270 --> 00:09:42,820
그들은 이미지에서 학습해서

154
00:09:42,820 --> 00:09:46,255
데이터 세트가 엄청난 수행능력을 가질 수 있도록 합니다.

155
00:09:46,255 --> 00:09:49,810
LeNet보다 훨씬 나은이 아키텍처의 또 다른 측면은

156
00:09:49,810 --> 00:09:53,575
밸류 액티베이션 기능을 사용하고 있다는 것입니다.

157
00:09:53,575 --> 00:09:56,425
다시 말해서, 논문을 읽으면

158
00:09:56,425 --> 00:09:59,020
이걸 읽지 않으면 신경 쓰지 않아도 될만

159
00:09:59,020 --> 00:10:01,840
그런 고급 세부사항들이 나옵니다. 그 중 하나는

160
00:10:01,840 --> 00:10:03,445
이 논문이 작성되었을 때,

161
00:10:03,445 --> 00:10:06,197
GPU는 여전히 약간 느려서

162
00:10:06,197 --> 00:10:11,135
2 개의 GPU에서 복잡한 교육 방법을 가졌습니다.

163
00:10:11,135 --> 00:10:13,310
기본적인 개념은

164
00:10:13,310 --> 00:10:18,250
이 많은 레이어들이 서로 다른 두 개의 GPU를 교차해서 분리되었고,

165
00:10:18,250 --> 00:10:23,497
두 GPU가 서로 통신 할 때 좀 더 좋은 방법이 있었다는 것 입니다.

166
00:10:23,497 --> 00:10:25,360
이 논문에서는,

167
00:10:25,360 --> 00:10:29,650
원래의 알렉스넷 아키텍처에도 Local Response Normalization이라는

168
00:10:29,650 --> 00:10:34,125
레이어 세트가 추가되었습니다.

169
00:10:34,125 --> 00:10:36,820
이 유형의 레이어는 실제로 별로 사용되지 않습니다.

170
00:10:36,820 --> 00:10:38,830
제가 많이 설명하지 않는 이유이기도 하죠.

171
00:10:38,830 --> 00:10:42,220
그러나 Local Response Normalization의 기본 개념은

172
00:10:42,220 --> 00:10:44,845
이러한 블럭 중 하나를 살펴보면

173
00:10:44,845 --> 00:10:46,940
위에 있는 볼륨 중 하나를 살펴 보시죠

174
00:10:46,940 --> 00:10:49,360
논의를 위해서,

175
00:10:49,360 --> 00:10:52,380
이 13 x 13 x 256 으로 생각해봅시다.

176
00:10:52,380 --> 00:10:54,765
Local Response Normalization, LRN (로컬 응답 정규화)가하는 일은

177
00:10:54,765 --> 00:10:57,805
한 위치를 보는 것 입니다.

178
00:10:57,805 --> 00:10:59,570
그 하나의 위치의 높이와 너비를 보는 것이죠.

179
00:10:59,570 --> 00:11:02,935
그리고 이 모든 채널에 걸쳐 내려다 보고

180
00:11:02,935 --> 00:11:07,195
이 모든 256 숫자를 보고 모두를 정상화 시키는 것입니다.

181
00:11:07,195 --> 00:11:10,750
그리고이 로컬 응답 정규화의 동기는

182
00:11:10,750 --> 00:11:14,934
이 13 x 13 이미지의 각 위치에 대해

183
00:11:14,934 --> 00:11:20,123
매우 높은 액티베이션을 가진 너무 많은 뉴런을 갖지 않게 하기 위함입니다.

184
00:11:20,123 --> 00:11:25,730
그러나 많은 연구자들은 이것이 이것이 많은 도움이 되지 않는다는 것을 발견했습니다.

185
00:11:25,730 --> 00:11:27,995
그래서 이 개념들은 빨간색으로 그리고 있는 있는데요,

186
00:11:27,995 --> 00:11:31,880
왜냐하면 여러분이 이걸 이해하는 게 그리 중요하지 않기 때문입니다.

187
00:11:31,880 --> 00:11:33,940
그리고 실제로, 저는 요즘 학습된 네트워크 언어에서

188
00:11:33,940 --> 00:11:38,760
지역 응답 정규화를 실제로 사용하지 않습니다.

189
00:11:38,760 --> 00:11:41,380
따라서 딥러닝의 역사에 관심이 있다면

190
00:11:41,380 --> 00:11:43,395
알렉스넷 이전에도,

191
00:11:43,395 --> 00:11:48,978
딥러닝은 음성 인식 및 기타 분야에서 큰 영향을 받기 시작했다고 생각합니다.

192
00:11:48,978 --> 00:11:52,690
그러나 이것은 컴퓨터 비전 커뮤니티에 확신을 준 것입니다.

193
00:11:52,690 --> 00:11:56,350
컴퓨터 비전에서 딥러닝이 효과적임을 주장해서

194
00:11:56,350 --> 00:12:00,280
딥러닝을 심각하게 받아들이도록 만든 것입니다.

195
00:12:00,280 --> 00:12:02,710
그리고 나서 컴퓨터 비전뿐 아니라

196
00:12:02,710 --> 00:12:05,508
컴퓨터 비전을 넘어서는 분야에서도 커다란 영향을 미칠 만큼 성장했습니다.

197
00:12:05,508 --> 00:12:08,170
그리고 만약 여러분이 논문들을 직접 읽어 보시면,

198
00:12:08,170 --> 00:12:11,635
사실 이 강의를 위해 읽을 필요는 없습니다만,

199
00:12:11,635 --> 00:12:14,200
이 논문 몇몇을 읽어보시고 싶다면,

200
00:12:14,200 --> 00:12:19,354
여기 있는 이것이 읽기 수월해서, 보시기 좋을 겁니다.

201
00:12:19,354 --> 00:12:23,257
알렉스넷은 상대적으로 복잡한 아키텍처를 가지고 있었지만

202
00:12:23,257 --> 00:12:25,585
많은 하이퍼 파라미터가있었습니다, 그렇죠?

203
00:12:25,585 --> 00:12:28,255
그리고 Alex Krizhevsky와 그의 공동 저자들이 생각해 냈던

204
00:12:28,255 --> 00:12:33,240
이 모든 숫자들이죠.

205
00:12:33,240 --> 00:12:39,765
이 강의에서 세 번째이자 마지막 예시인 VGG 또는 VGG-16 네트워크라는 거를 보여 드리겠습니다

206
00:12:39,765 --> 00:12:44,820
VGG-16 네트에 대한 주목할만한 점은

207
00:12:44,820 --> 00:12:46,966
많은 하이퍼 파라미터를 가지고 있는 대신에,

208
00:12:46,966 --> 00:12:52,495
더 단순한 네트워크를 사용합시다.

209
00:12:52,495 --> 00:12:58,690
단지 스트라이드 1, 언제나 동일한 패딩의 3 x 3 필터인 컨볼레이어를 사용하는 것에 초점을 맞추고

210
00:12:58,690 --> 00:13:03,640
그리고 스트라이드 2 를 가진 2 x 2 , max pooling레이어를 만든다는 점입니다.

211
00:13:03,640 --> 00:13:06,250
그래서, VGG 네트워크에 대한 아주 좋은 점 중 하나는

212
00:13:06,250 --> 00:13:12,224
이 신경망 아키텍처를 단순화 하는 것입니다.

213
00:13:12,224 --> 00:13:14,494
이제 아키텍처를 살펴 보겠습니다.

214
00:13:14,494 --> 00:13:19,660
그들을 위해 이미지로 풀면, 첫 번째 두 레이어가 컨볼루션이므로

215
00:13:19,660 --> 00:13:24,315
3 x 3 필터가 되죠.

216
00:13:24,315 --> 00:13:27,930
그리고 처음 두 레이어에서는 64 개의 필터를 사용합니다.

217
00:13:27,930 --> 00:13:35,830
동일한 컨볼루션과 64 채널을 사용하기 때문에 224 x 224 가 됩니다.

218
00:13:35,830 --> 00:13:39,345
VGG-16은 상대적으로 깊은 네트워크이기 때문에

219
00:13:39,345 --> 00:13:42,335
모든 볼륨을 그리지는 않겠습니다.

220
00:13:42,335 --> 00:13:46,270
그래서이 작은 그림이 나타내는 것은

221
00:13:46,270 --> 00:13:50,890
이전에 우리가 224 x 224 x 3로 그렸던 것입니다.

222
00:13:50,890 --> 00:13:55,362
224 x 224 x 64가 되는 컨볼루션은

223
00:13:55,362 --> 00:14:00,535
더 깊은 볼륨으로 그려지게 될 것입니다.

224
00:14:00,535 --> 00:14:07,227
224 x 224 x 64가 되는 또 다른 레이어로 그려집니다

225
00:14:07,227 --> 00:14:15,730
그래서 이 [conv64]x2는 64필터로 여기 이 두 개의 conv를 수행하고 있음을 나타냅니다.

226
00:14:15,730 --> 00:14:17,380
그리고 앞에서 언급했듯이

227
00:14:17,380 --> 00:14:20,555
필터는 항상 스트라이드 1 의

228
00:14:20,555 --> 00:14:24,455
3x3 이므로 그것들은 항상 동일한 컨볼루션입니다.

229
00:14:24,455 --> 00:14:26,395
따라서 이러한 모든 볼륨을 그리기보다는

230
00:14:26,395 --> 00:14:28,400
텍스트를 사용하여 이 네트워크를 나타내겠습니다.

231
00:14:28,400 --> 00:14:31,413
pooling 레이어를 사용하고

232
00:14:31,413 --> 00:14:33,580
그러면 pooling 레이어는 줄어들겠죠

233
00:14:33,580 --> 00:14:36,725
224 x 224 에서 줄어들겠죠?

234
00:14:36,725 --> 00:14:40,755
그렇습니다. 112 x 112 x 64 가 되고

235
00:14:40,755 --> 00:14:44,339
몇 개 더 많은 컨볼레이어를 갖게 됩니다.

236
00:14:44,339 --> 00:14:50,426
128 개의 필터를 가지고 있다는 것을 의미합니다. 그리고 이것들은 동일한 컨볼루션이기 때문입니다.

237
00:14:50,426 --> 00:14:52,365
새로운 디멘션은 무엇인지 보시죠.

238
00:14:52,365 --> 00:14:57,020
이것은 112 x 112x 128이 될 겁니다. 그리고 나서

239
00:14:57,020 --> 00:15:02,205
풀리 레이어를 하면 이것의 새로운 디멘션이 무엇인지 알 수 있겠죠.

240
00:15:02,205 --> 00:15:07,210
이제, 256필터를 가진 3개의 컨볼레이어가

241
00:15:07,210 --> 00:15:14,300
pooling레이어를 거쳐서 몇 개 더 많은 컨볼레이어를 거치고

242
00:15:14,300 --> 00:15:18,945
pooling 레이어, 더 많은 컨볼레이어, pooling 레이어 이런 식으로 됩니다.

243
00:15:18,945 --> 00:15:26,345
그러면 이것은 이 최종 7 x 7 x 512를 취해서 완전 연결 레이어로,

244
00:15:26,345 --> 00:15:30,230
4096 유닛의 FC,

245
00:15:30,230 --> 00:15:36,080
그리고 1000개의 softmax 아웃풋이 됩니다.

246
00:15:36,080 --> 00:15:39,875
하지만, VGG-16의 16은

247
00:15:39,875 --> 00:15:45,080
이것이 16 개의 레이어에 가중치가 있음을 나타냅니다.

248
00:15:45,080 --> 00:15:47,470
그리고 이것은 꽤 큰 네트워크입니다.

249
00:15:47,470 --> 00:15:52,415
이 네트워크는 약 1 억 3 천 8 백만 개의 파라미터를 가지고 있습니다.

250
00:15:52,415 --> 00:15:55,615
그것은 현대 표준에 의해서조차 도 꽤 큽니다.

251
00:15:55,615 --> 00:16:00,673
그러나 VGG-16 아키텍처의 단순성으로 인해 매우 매력적이었습니다.

252
00:16:00,673 --> 00:16:03,935
여러분은 이 아키텍처가 정말로 아주 균일하다는 것을 알 수 있습니다.

253
00:16:03,935 --> 00:16:07,130
컨볼레이어에 뒤이어서

254
00:16:07,130 --> 00:16:09,590
높이와 너비를 줄이는 pooling레이어가 있습니다, 그렇죠?

255
00:16:09,590 --> 00:16:13,396
pooling 레이어는 높이와 너비를 줄입니다.

256
00:16:13,396 --> 00:16:15,570
여기에 그들 중 몇 개가 있습니다.

257
00:16:15,570 --> 00:16:20,260
그러나 컨볼레이어에서 필터의 수를 보면

258
00:16:20,260 --> 00:16:28,675
64 개의 필터가 있고, 256으로, 256으로 계속 두 배로 커지고 있습니다.

259
00:16:28,675 --> 00:16:33,160
저자들은 512가 충분히 크고, 두 배로 늘어난 거라 생각했던 것 같습니다.

260
00:16:33,160 --> 00:16:36,410
그러나 모든 단계에서 대략 두 배가되는 것

261
00:16:36,410 --> 00:16:39,915
혹은 컨볼레이어의 모든 쌓아진 것을 관통해서 2배가 되는 것은

262
00:16:39,915 --> 00:16:45,040
이 네트워크의 아키텍처를 디자인 하는 데에 사용된 또 다른 원칙이었던 것이죠.

263
00:16:45,040 --> 00:16:48,230
그래서이 건축물의 상대적 균일 성이

264
00:16:48,230 --> 00:16:52,460
연구자들에게 상당히 매력적인 네트워크입니다.

265
00:16:52,460 --> 00:16:54,680
주된 단점은

266
00:16:54,680 --> 00:16:58,910
여러분이 학습시켜야 하는 파라미터의 개수 측면에서 볼 때, 이게 매우 큰 네트워크라는 점 입니다.

267
00:16:58,910 --> 00:17:00,995
그리고 논문을 읽으면

268
00:17:00,995 --> 00:17:04,700
VGG-19가 이 네트워크의 더 큰 버전이라고 말하는

269
00:17:04,700 --> 00:17:08,600
사람들을 볼 수 있습니다.

270
00:17:08,600 --> 00:17:11,780
그리고 Karen Simonyan과 Andrew Zisserman에 의해

271
00:17:11,780 --> 00:17:16,595
하단에 인용 된 논문의 세부 사항도 볼 수 있습니다.

272
00:17:16,595 --> 00:17:20,875
그러나 VGG-16은 VGG-19와 거의 비슷하기 때문에

273
00:17:20,875 --> 00:17:23,570
많은 사람들이 VGG-16을 사용할 것입니다.

274
00:17:23,570 --> 00:17:26,090
하지만 이것에서 제가 가장 좋아했던 부분은

275
00:17:26,090 --> 00:17:28,540
이게 이 패턴을 만드는 건데요,

276
00:17:28,540 --> 00:17:31,100
더 깊게 높이와 너비가 내려감에 따라

277
00:17:31,100 --> 00:17:33,540
pooling레이어가 매번 2배씩 내려간다는 것입니다.

278
00:17:33,540 --> 00:17:36,890
채널의 수는 증가하지만 말이죠.

279
00:17:36,890 --> 00:17:42,855
여기에 새로운 컨볼레이어 세트가 있을 때마다 대략 2 배씩 올라갑니다.

280
00:17:42,855 --> 00:17:49,155
그것이 내려 가고 올라가는 비율을 체계적으로 만들어서,

281
00:17:49,155 --> 00:17:54,410
저는 이 논문이 그 관점에서 매우 매력적이라고 생각했습니다.

282
00:17:54,410 --> 00:17:57,845
자, 여기까지 3 가지 고전적인 아키텍처에 대한 내용이었습니다.

283
00:17:57,845 --> 00:18:00,931
원하시면, 이 논문들을 읽어보시면 좋을 것 같습니다.

284
00:18:00,931 --> 00:18:05,270
알렉스넷 논문을 시작으로 시작해서 VGGNet 논문으로 넘어가시는 것을 추천 드립니다.

285
00:18:05,270 --> 00:18:07,460
LeNet 논문은 읽기가 조금 더 어렵습니다.

286
00:18:07,460 --> 00:18:09,984
하지만 살펴본다면 이 역시 좋은 고전 문헌이죠.

287
00:18:09,984 --> 00:18:14,725
다음 강의에서는, 이러한 고전적 네트워크를 뛰어 넘어, 좀 더 발전되고

288
00:18:14,725 --> 00:18:18,040
더욱 강력한 신경망 아키텍처를 살펴 보겠습니다. 다음 강의로 넘어가시죠.