1
00:00:00,000 --> 00:00:02,470
这节课中 你会学到一些

2
00:00:02,470 --> 00:00:05,418
经典神经网络结构 如LeNet-5

3
00:00:05,418 --> 00:00:10,122
AlexNet和VGGNet 我们来看一下

4
00:00:10,122 --> 00:00:12,935
这是LeNet-5的网络结构

5
00:00:12,935 --> 00:00:15,700
你以一幅图像开始 即

6
00:00:15,700 --> 00:00:17,540
32乘32乘1

7
00:00:17,540 --> 00:00:21,520
而LeNet-5的任务是识别手写数字

8
00:00:21,520 --> 00:00:25,490
可能就像这幅数字图像

9
00:00:25,490 --> 00:00:28,815
LeNet-5就在灰度图像上训练

10
00:00:28,815 --> 00:00:32,180
这就是为什么它是32乘32乘1

11
00:00:32,180 --> 00:00:34,400
该神经网络实际上

12
00:00:34,400 --> 00:00:38,315
（这些灰度图像）与你上周所见的样本类似

13
00:00:38,315 --> 00:00:39,847
第一步，

14
00:00:39,847 --> 00:00:41,765
你用一组6个

15
00:00:41,765 --> 00:00:45,220
你用一组6个5乘5 步长为1的滤波器 因为用

16
00:00:45,220 --> 00:00:50,218
因为用6个滤波器可以生成28乘28乘6的结果

17
00:00:50,218 --> 00:00:52,730
同时步长为1 零填充

18
00:00:52,730 --> 00:00:58,640
图像的尺寸将从32x32降低到28x28

19
00:00:58,640 --> 00:01:02,295
然后LeNet用了一个池化层

20
00:01:02,295 --> 00:01:04,970
而写这篇文章那时

21
00:01:04,970 --> 00:01:07,576
人们更常用均值池化

22
00:01:07,576 --> 00:01:09,290
如果你正在构建一个现代化的变体

23
00:01:09,290 --> 00:01:12,145
你可能会改用最大池化

24
00:01:12,145 --> 00:01:13,360
但在这个例子中

25
00:01:13,360 --> 00:01:17,825
你用宽为2 步长为2的滤波器做均值池化

26
00:01:17,825 --> 00:01:20,780
结果是降维

27
00:01:20,780 --> 00:01:22,730
即高和宽都以2的因子降维

28
00:01:22,730 --> 00:01:28,784
因此我们得到14乘14乘6的体积结果

29
00:01:28,784 --> 00:01:32,180
我猜这些体积的高度和宽度并不是完全按比例绘制的

30
00:01:32,180 --> 00:01:35,210
从技术上讲，如果我将这些体积绘制成比例

31
00:01:35,210 --> 00:01:38,150
高度和宽度会增加一倍

32
00:01:38,150 --> 00:01:41,180
接着你用另一层卷积层

33
00:01:41,180 --> 00:01:44,070
这次你用一组16个滤波器

34
00:01:44,070 --> 00:01:48,515
16个滤波器尺寸为5乘5 因此得到16通道的体积结果

35
00:01:48,515 --> 00:01:52,355
而在这篇文章完成的1998年

36
00:01:52,355 --> 00:01:57,200
人们不怎么用填充 或你总用有效卷积计算

37
00:01:57,200 --> 00:01:59,635
这也就是为什么每次你用卷积层

38
00:01:59,635 --> 00:02:01,965
结果的尺寸都会缩小

39
00:02:01,965 --> 00:02:03,380
所以这就是为什么这里

40
00:02:03,380 --> 00:02:06,393
你会得到从14乘14降到10乘10的结果

41
00:02:06,393 --> 00:02:08,580
然后接着另一池化层

42
00:02:08,580 --> 00:02:11,060
因此高和宽都以2的因子降低

43
00:02:11,060 --> 00:02:13,715
然后得到5乘5的结果

44
00:02:13,715 --> 00:02:16,640
如果你将这些数字相乘 即5乘5乘16

45
00:02:16,640 --> 00:02:20,375
等于400

46
00:02:20,375 --> 00:02:24,020
也就是25乘16 等于400

47
00:02:24,020 --> 00:02:29,900
然后接下来一层是全连接层 其完全连接了

48
00:02:29,900 --> 00:02:36,840
其自身120个神经元中每个都全连接了这400个节点

49
00:02:36,840 --> 00:02:38,385
因此叫做全连接层

50
00:02:38,385 --> 00:02:41,955
有时这里会专门画出

51
00:02:41,955 --> 00:02:46,080
一个有400个节点的图层, 我在这里跳过。

52
00:02:46,080 --> 00:02:49,590
这是一层全连接层 接着是另一层全连接层

53
00:02:49,590 --> 00:02:51,690
然后最后一步是用

54
00:02:51,690 --> 00:02:57,280
这84维特征生成一个最终结果

55
00:02:57,280 --> 00:03:01,375
我想你可在这里再画一个节点来预测y 即ŷ

56
00:03:01,375 --> 00:03:04,560
而ŷ包括10个预测值

57
00:03:04,560 --> 00:03:09,090
与从0到9的10个数字相对应

58
00:03:09,090 --> 00:03:11,100
作为该网络结构的现代版

59
00:03:11,100 --> 00:03:17,300
我们用softmax层来生成10个分类结果

60
00:03:17,300 --> 00:03:23,385
尽管LeNet-5原先是用别的分类器做输出层

61
00:03:23,385 --> 00:03:25,633
而这个分类器现在已经不用了

62
00:03:25,633 --> 00:03:29,220
因此用现在标准来看 这是个小型神经网络

63
00:03:29,220 --> 00:03:32,645
有大概60,000个参数

64
00:03:32,645 --> 00:03:35,934
而如今你经常会见到

65
00:03:35,934 --> 00:03:39,690
包含千万到亿量级参数的神经网络

66
00:03:39,690 --> 00:03:41,850
现在见这些网络已经很正常了

67
00:03:41,850 --> 00:03:45,295
这些字面上比LeNet-5大千倍的神经网络

68
00:03:45,295 --> 00:03:49,600
但当你深入研究网络 值得注意的一个模式是

69
00:03:49,600 --> 00:03:51,790
从左到右

70
00:03:51,790 --> 00:03:55,360
高度和宽度往往会下降

71
00:03:55,360 --> 00:03:57,690
随着尺寸从32乘32降到28 到14

72
00:03:57,690 --> 00:04:03,100
到10和到5 通道数是增加的

73
00:04:03,100 --> 00:04:11,250
随着网络层数的增加 通道数从1增到6到16

74
00:04:11,250 --> 00:04:15,400
另一个神经网络至今依然使用的模式是

75
00:04:15,400 --> 00:04:20,500
你会用一层或多层卷积层 随后是池化层

76
00:04:20,500 --> 00:04:25,758
然后再是一层或多层卷积层 随后池化层

77
00:04:25,758 --> 00:04:29,940
然后是几层全连接层 然后输出

78
00:04:29,940 --> 00:04:34,090
这种层次排列很常见

79
00:04:34,090 --> 00:04:39,515
最后 针对你们中想读这篇论文的人

80
00:04:39,515 --> 00:04:41,880
这里有些不同的变化

81
00:04:41,880 --> 00:04:43,690
在这张幻灯片的剩余部分

82
00:04:43,690 --> 00:04:47,065
我要多做一些说明

83
00:04:47,065 --> 00:04:52,265
只针对你们中那些想读这篇经典文章的人来说

84
00:04:52,265 --> 00:04:54,903
因此我用红色来标记要说的话

85
00:04:54,903 --> 00:04:57,490
(不读的)你可以稳妥地跳过这些

86
00:04:57,490 --> 00:05:00,520
这可能是个有趣的历史相关的脚注

87
00:05:00,520 --> 00:05:04,350
如果你没完全理解也没关系

88
00:05:04,350 --> 00:05:07,990
所以如果你读了原始文献 那时候

89
00:05:07,990 --> 00:05:12,453
人们用sigmoid和tanh非线性函数

90
00:05:12,453 --> 00:05:16,330
而不用ReLU非线性函数

91
00:05:16,330 --> 00:05:20,065
因此你读文章会发现提及的sigmoid和tanh

92
00:05:20,065 --> 00:05:23,260
此外还有些有趣的方式

93
00:05:23,260 --> 00:05:26,835
即以现代标准来看这个网络级联的方式很有趣

94
00:05:26,835 --> 00:05:33,775
例如你会发现 如果有一个nh乘nw乘nc的网络结构

95
00:05:33,775 --> 00:05:40,985
其中nc是通道数 然后用f乘f乘nc维的滤波器

96
00:05:40,985 --> 00:05:44,480
其中每个滤波器是处理了每一维通道

97
00:05:44,480 --> 00:05:47,195
但那时计算机非常慢

98
00:05:47,195 --> 00:05:50,230
为了节省计算量和参数数量

99
00:05:50,230 --> 00:05:53,785
原始LeNet-5有种比较疯狂的计算方法

100
00:05:53,785 --> 00:05:58,040
其中不同滤波器会处理输入块的不同通道

101
00:05:58,040 --> 00:06:00,343
这篇文章会讲到这些细节

102
00:06:00,343 --> 00:06:07,090
而现在先进些的应用则不会有这种复杂方法

103
00:06:07,090 --> 00:06:12,280
最后一件以前做现在没有再做的事是

104
00:06:12,280 --> 00:06:19,705
原始LeNet-5在池化后有非线性处理

105
00:06:19,705 --> 00:06:25,005
我想应是池化层后用了sigmoid非线性

106
00:06:25,005 --> 00:06:27,130
所以如果你读了这篇论文

107
00:06:27,130 --> 00:06:29,345
这是一篇较难的论文

108
00:06:29,345 --> 00:06:32,100
与随后一些视频中讲的论文相比

109
00:06:32,100 --> 00:06:34,670
随后这篇论文更容易开始

110
00:06:34,670 --> 00:06:40,135
这张幻灯片中大部分算法属于论文第二三章

111
00:06:40,135 --> 00:06:44,485
后面几章讨论了其他想法

112
00:06:44,485 --> 00:06:47,260
即图转换网络(GTN)

113
00:06:47,260 --> 00:06:49,215
该网络现在已不再广泛使用

114
00:06:49,215 --> 00:06:50,935
所以如果你想读这篇论文

115
00:06:50,935 --> 00:06:55,660
我推荐多关注讨论LeNet-5结构的第二章

116
00:06:55,660 --> 00:06:58,165
也可以快速看下第三章

117
00:06:58,165 --> 00:07:01,720
第三章很有趣 讲了实验和结果

118
00:07:01,720 --> 00:07:06,155
第二个我想讲的是AlexNet

119
00:07:06,155 --> 00:07:12,510
以一作Alex Krizhevsky命名

120
00:07:12,510 --> 00:07:13,725
另两位是Ilya Sutskever和Geoffrey Hinton

121
00:07:13,725 --> 00:07:21,048
AlexNet以227乘227乘3的图像开始

122
00:07:21,048 --> 00:07:22,525
如果你读了这篇论文

123
00:07:22,525 --> 00:07:27,010
本文提及的是224乘224乘3的图像

124
00:07:27,010 --> 00:07:28,120
但如果你检查数字

125
00:07:28,120 --> 00:07:33,100
我想227乘227才合理

126
00:07:33,100 --> 00:07:40,230
第一层用一组96个11乘11 步长4的滤波器

127
00:07:40,230 --> 00:07:42,740
而因为用了大的步长4

128
00:07:42,740 --> 00:07:45,574
图像尺寸缩到55乘55

129
00:07:45,574 --> 00:07:50,930
因为步长大 维度以4的因子快速降低

130
00:07:50,930 --> 00:07:55,110
随后的最大池化层用了3乘3的滤波器

131
00:07:55,110 --> 00:07:57,925
所以f等于3 步长为2

132
00:07:57,925 --> 00:08:04,570
结果体积降到27乘27乘96

133
00:08:04,570 --> 00:08:08,530
然后用相同的5乘5卷积

134
00:08:08,530 --> 00:08:14,730
相同填充 得到27乘27乘256

135
00:08:14,730 --> 00:08:20,025
再次做最大池化 高和宽降到13

136
00:08:20,025 --> 00:08:23,860
然后相同的卷积 相同填充

137
00:08:23,860 --> 00:08:29,805
得到13乘13乘384

138
00:08:29,805 --> 00:08:35,275
然后再次3乘3相同卷积

139
00:08:35,275 --> 00:08:39,680
虽然再次3乘3相同卷积

140
00:08:39,680 --> 00:08:45,285
随后最大池化 结果降到6乘6乘256

141
00:08:45,285 --> 00:08:52,020
如果你把所有数字相乘 6乘6乘256等于9216

142
00:08:52,020 --> 00:08:56,947
因此我们把其展开成9216个节点

143
00:08:56,947 --> 00:09:00,790
然后有几层全连接层

144
00:09:00,790 --> 00:09:04,250
最后用softmax输出结果

145
00:09:04,250 --> 00:09:09,515
即目标可能是1000类中的哪一类

146
00:09:09,515 --> 00:09:16,920
所以这个网络结构实际上很像LeNet

147
00:09:16,920 --> 00:09:20,210
但更大

148
00:09:20,210 --> 00:09:27,740
鉴于之前幻灯片中LeNet-5有60,000个参数

149
00:09:27,740 --> 00:09:31,935
AlexNet大概有6千万参数

150
00:09:31,935 --> 00:09:34,024
而且AlexNet采用

151
00:09:34,024 --> 00:09:36,925
非常相似的构造版块

152
00:09:36,925 --> 00:09:40,270
拥有更多隐藏神经元 在更多数据上训练

153
00:09:40,270 --> 00:09:42,820
AlexNet在ImageNet数据库上训练

154
00:09:42,820 --> 00:09:46,255
(大数据库)使它能有优秀的性能

155
00:09:46,255 --> 00:09:49,810
使AlexNet比LeNet更好的另一因素

156
00:09:49,810 --> 00:09:53,575
是ReLU激活函数的使用

157
00:09:53,575 --> 00:09:56,425
然后 如果你读了这篇文章

158
00:09:56,425 --> 00:09:59,020
文中有些你不需担忧的较难方法

159
00:09:59,020 --> 00:10:01,840
如果你没读文章 其中一个是

160
00:10:01,840 --> 00:10:03,445
写这篇文章的时候 

161
00:10:03,445 --> 00:10:06,197
GPU还比较慢

162
00:10:06,197 --> 00:10:11,135
因此该文有在两块GPU上训练的复杂方法

163
00:10:11,135 --> 00:10:13,310
其中基本思想是

164
00:10:13,310 --> 00:10:18,250
网络中很多层被分割到两块不同GPU上

165
00:10:18,250 --> 00:10:23,497
两块GPU可以互相通讯

166
00:10:23,497 --> 00:10:25,360
而且这篇文章也

167
00:10:25,360 --> 00:10:29,650
原始的AlexNet结构还有另一种层

168
00:10:29,650 --> 00:10:34,125
叫做局部响应归一层(LRN)

169
00:10:34,125 --> 00:10:36,820
这种层实际上用得很少

170
00:10:36,820 --> 00:10:38,830
因此我不讲它

171
00:10:38,830 --> 00:10:42,220
但局部响应归一(LRN)的基本思想是

172
00:10:42,220 --> 00:10:44,845
如果你观察这些块

173
00:10:44,845 --> 00:10:46,940
顶上这些体积块中之一

174
00:10:46,940 --> 00:10:49,360
为讨论方便 这块

175
00:10:49,360 --> 00:10:52,380
13乘13乘256

176
00:10:52,380 --> 00:10:54,765
局部响应归一(LRN)

177
00:10:54,765 --> 00:10:57,805
局部响应归一(LRN)做的就是 当你看一个位置

178
00:10:57,805 --> 00:10:59,570
一个位置的高和宽

179
00:10:59,570 --> 00:11:02,935
然后看跨越通道的这个位置

180
00:11:02,935 --> 00:11:07,195
(这个位置)跨越整个256通道然后归一化

181
00:11:07,195 --> 00:11:10,750
这个局部响应归一(LRN)操作是针对处理

182
00:11:10,750 --> 00:11:14,934
该13乘13图像中的每一个位置

183
00:11:14,934 --> 00:11:20,123
但也许你不想要太多高激活率的神经元

184
00:11:20,123 --> 00:11:25,730
随后许多研究者发现该层并不太管用

185
00:11:25,730 --> 00:11:27,995
所以我用红色把这些内容标出来

186
00:11:27,995 --> 00:11:31,880
因为能否理解这个不太重要

187
00:11:31,880 --> 00:11:33,940
实际中在我训练的网络结构中我不会用

188
00:11:33,940 --> 00:11:38,760
局部响应归一

189
00:11:38,760 --> 00:11:41,380
如果你对深度学习的历史感兴趣

190
00:11:41,380 --> 00:11:43,395
我想在AlexNet之前

191
00:11:43,395 --> 00:11:48,978
深度学习开始在语音识别和其他一些领域获得关注

192
00:11:48,978 --> 00:11:52,690
但这篇文章让大部分

193
00:11:52,690 --> 00:11:56,350
计算机视觉领域的研究者开始认真对待深度学习

194
00:11:56,350 --> 00:12:00,280
确信深度学习对计算机视觉有用

195
00:12:00,280 --> 00:12:02,710
随后深度学习产生了巨大的影响力

196
00:12:02,710 --> 00:12:05,508
对计算机视觉和其他领域产生了巨大影响

197
00:12:05,508 --> 00:12:08,170
如果你自己想读一些这方面文章

198
00:12:08,170 --> 00:12:11,635
其实你不需为这门课专门读文章

199
00:12:11,635 --> 00:12:14,200
但如果你确实想读文章

200
00:12:14,200 --> 00:12:19,354
这篇文章比较易懂 值得一读

201
00:12:19,354 --> 00:12:23,257
所以鉴于AlexNet有个比较复杂的结构

202
00:12:23,257 --> 00:12:25,585
有许多超参数 是吧

203
00:12:25,585 --> 00:12:28,255
在哪放置这些参数

204
00:12:28,255 --> 00:12:33,240
是Alex Krizhevsky和搭档必须考虑的问题

205
00:12:33,240 --> 00:12:39,765
下面介绍第三个也是最后一个VGG或叫VGG-16网络

206
00:12:39,765 --> 00:12:44,820
按作者所说关于VGG-16非常值得注意的一点是

207
00:12:44,820 --> 00:12:46,966
与大量的超参数不同

208
00:12:46,966 --> 00:12:52,495
VGG-16结构更简单 更能关注卷积层

209
00:12:52,495 --> 00:12:58,690
即3乘3 步长为1 用相同填充的卷积滤波器

210
00:12:58,690 --> 00:13:03,640
所有最大池化层滤波器都是2乘2 步长为2

211
00:13:03,640 --> 00:13:06,250
VGG有个优点

212
00:13:06,250 --> 00:13:12,224
VGG有个优点是真正简化了神经网络结构

213
00:13:12,224 --> 00:13:14,494
所以我们来认真分析下这个结构

214
00:13:14,494 --> 00:13:19,660
以图像开始 前两层是卷积层

215
00:13:19,660 --> 00:13:24,315
即3乘3的滤波器

216
00:13:24,315 --> 00:13:27,930
且前两层用64个滤波器

217
00:13:27,930 --> 00:13:35,830
得到224乘224乘64的结果 因为用了相同卷积

218
00:13:35,830 --> 00:13:39,345
所以因为VGG-16是个相对深的网络

219
00:13:39,345 --> 00:13:42,335
我不打算把所有体积块画出来

220
00:13:42,335 --> 00:13:46,270
所以这块小图像表示我们曾经

221
00:13:46,270 --> 00:13:50,890
(我们曾经)画过的224乘224乘3的体积

222
00:13:50,890 --> 00:13:55,362
然后是一个卷积生成

223
00:13:55,362 --> 00:14:00,535
224乘224乘64的更深体积

224
00:14:00,535 --> 00:14:07,227
然后是另一层生成224乘224乘64

225
00:14:07,227 --> 00:14:15,730
这里conv64乘2表示用两层64个滤波器的卷积层

226
00:14:15,730 --> 00:14:17,380
如我之前所说

227
00:14:17,380 --> 00:14:20,555
滤波器通常是3乘3

228
00:14:20,555 --> 00:14:24,455
步长为1 因此是相同卷积

229
00:14:24,455 --> 00:14:26,395
不同于画出所有的体积块

230
00:14:26,395 --> 00:14:28,400
我会用文字表示该网络

231
00:14:28,400 --> 00:14:31,413
接下来是池化层

232
00:14:31,413 --> 00:14:33,580
这样池化层会降低

233
00:14:33,580 --> 00:14:36,725
我想 把224乘224降低到多少?

234
00:14:36,725 --> 00:14:40,755
对 降到112乘112乘64

235
00:14:40,755 --> 00:14:44,339
然后又是一对卷积层

236
00:14:44,339 --> 00:14:50,426
这表示128个滤波器 因为是相同卷积

237
00:14:50,426 --> 00:14:52,365
我们看下新维度是多少

238
00:14:52,365 --> 00:14:57,020
对 结果是112乘112乘128

239
00:14:57,020 --> 00:15:02,205
随后是池化层 所以你可以算出新维度是多少

240
00:15:02,205 --> 00:15:07,210
现在再三层卷积层

241
00:15:07,210 --> 00:15:14,300
256个卷积滤波器 然后池化层 然后又一组卷积层

242
00:15:14,300 --> 00:15:18,945
池化层 卷积层 池化层

243
00:15:18,945 --> 00:15:26,345
最后得到7乘7乘512的结果输入全连接层

244
00:15:26,345 --> 00:15:30,230
两层4096个神经元的全连接层

245
00:15:30,230 --> 00:15:36,080
然后是softmax函数输出1000类结果

246
00:15:36,080 --> 00:15:39,875
顺便说 VGG-16中的16

247
00:15:39,875 --> 00:15:45,080
指该网络有16层带权重的层

248
00:15:45,080 --> 00:15:47,470
这是个相当大的网络

249
00:15:47,470 --> 00:15:52,415
该网络总共有1亿3千8百万个参数

250
00:15:52,415 --> 00:15:55,615
即使以现在标准衡量也是很大了

251
00:15:55,615 --> 00:16:00,673
但VGG-16结构的简洁性也非常吸引人

252
00:16:00,673 --> 00:16:03,935
看得出该结构相当统一

253
00:16:03,935 --> 00:16:07,130
有几组卷积层 随后是池化层

254
00:16:07,130 --> 00:16:09,590
以降低高和宽 对吧?

255
00:16:09,590 --> 00:16:13,396
所以池化层来减少高和宽

256
00:16:13,396 --> 00:16:15,570
这样的模式有好几组

257
00:16:15,570 --> 00:16:20,260
而另一方面 如果你看卷积层的滤波器数量

258
00:16:20,260 --> 00:16:28,675
你有64个滤波器 然后两倍到128到256到512

259
00:16:28,675 --> 00:16:33,160
这里我想作者认为512够大 所以没再两倍增加

260
00:16:33,160 --> 00:16:36,410
但这种在每步粗略地双倍增加

261
00:16:36,410 --> 00:16:39,915
或在每组卷积层双倍增加的方式

262
00:16:39,915 --> 00:16:45,040
是设计这个网络时用的另一个简单原则

263
00:16:45,040 --> 00:16:48,230
所以我认为这种相对统一性

264
00:16:48,230 --> 00:16:52,460
这种结构的相对统一性对研究者来说很有趣

265
00:16:52,460 --> 00:16:54,680
而该结构的主要缺点是

266
00:16:54,680 --> 00:16:58,910
以训练参数的数量来看是个非常大的网络

267
00:16:58,910 --> 00:17:00,995
如果你读过文献

268
00:17:00,995 --> 00:17:04,700
有时会看到提及VGG-19

269
00:17:04,700 --> 00:17:08,600
那是VGG-16的更大版本

270
00:17:08,600 --> 00:17:11,780
你可以看论文中的技术细节

271
00:17:11,780 --> 00:17:16,595
引自Karen Simonyan和Andrew Zisserman

272
00:17:16,595 --> 00:17:20,875
然而由于VGG-16跟VGG-19的性能差不多

273
00:17:20,875 --> 00:17:23,570
很多人会用VGG-16

274
00:17:23,570 --> 00:17:26,090
但我最喜欢的是

275
00:17:26,090 --> 00:17:28,540
这种模式

276
00:17:28,540 --> 00:17:31,100
这种高和宽随着网络深入而减少的模式

277
00:17:31,100 --> 00:17:33,540
尺寸每次会(因为池化层)按因子2减少

278
00:17:33,540 --> 00:17:36,890
因为池化层 与此同时通道数量增加

279
00:17:36,890 --> 00:17:42,855
每次用一组卷积层时(通道数)会按因子2增加

280
00:17:42,855 --> 00:17:49,155
所以按这种比率(网络)减少或增加
是非常有条理性的

281
00:17:49,155 --> 00:17:54,410
从这个角度看这篇文章是非常吸引人的

282
00:17:54,410 --> 00:17:57,845
所以这就是三种经典网络结构

283
00:17:57,845 --> 00:18:00,931
如果愿意 你可以读其中一些论文

284
00:18:00,931 --> 00:18:05,270
个人推荐先从AlexNet论文开始 然后VGGNet

285
00:18:05,270 --> 00:18:07,460
然后LeNet论文有点难懂

286
00:18:07,460 --> 00:18:09,984
但一旦读懂它是非常经典的

287
00:18:09,984 --> 00:18:14,725
接下来除了这些经典网络 我们会学一些更先进

288
00:18:14,725 --> 00:18:18,040
更强大的神经网络 让我们开始下节吧