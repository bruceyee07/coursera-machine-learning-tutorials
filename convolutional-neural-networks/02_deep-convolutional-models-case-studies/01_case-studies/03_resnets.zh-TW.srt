1
00:00:00,000 --> 00:00:03,390
很深，很深的網路很難去訓練因為

2
00:00:03,390 --> 00:00:07,215
資料消失跟梯度爆炸的問題

3
00:00:07,215 --> 00:00:08,790
在這段影片中，您會學到

4
00:00:08,790 --> 00:00:12,150
如何跳過連結，讓您可以用一層的啟動值

5
00:00:12,150 --> 00:00:17,498
突然餵進另ㄧ個在神經網路的更深層

6
00:00:17,498 --> 00:00:22,600
使用這種方式，您會發現
『深度殘差網路』讓您可以訓練很深很深的網路

7
00:00:22,600 --> 00:00:26,865
有時候甚至超過 100 層的網路，我們來看看

8
00:00:26,865 --> 00:00:30,390
深度殘差網路建立在稱為『殘差區塊』上

9
00:00:30,390 --> 00:00:33,185
我們先來描述那是什麼

10
00:00:33,185 --> 00:00:35,370
這裡是神經網路的兩層

11
00:00:35,370 --> 00:00:38,005
開始從 l 層的一些啟動值 a[l]

12
00:00:38,005 --> 00:00:43,940
然後經過 a[l+1] 兩層後的啟動值為 a[l+2]

13
00:00:43,940 --> 00:00:48,798
所以經過這兩層的計算，您有 a[l]

14
00:00:48,798 --> 00:00:54,459
第一件事是您應用這個線性運算在這上面

15
00:00:54,459 --> 00:00:57,660
也就是用這個方程式來控制

16
00:00:57,660 --> 00:01:01,690
所以您從 a[l] 來計算 z[l+1]

17
00:01:01,690 --> 00:01:07,975
使用權重矩陣來相乘在加上偏差向量

18
00:01:07,975 --> 00:01:17,945
接著，您應用 ReLU 非線性來得到 a[l+1]

19
00:01:17,945 --> 00:01:24,750
用這個方程式 a[l+1] 是 g(z[l+1]) 來控制

20
00:01:24,750 --> 00:01:26,280
接著下一層

21
00:01:26,280 --> 00:01:30,540
您再次用線性的步驟

22
00:01:30,540 --> 00:01:33,432
用這個方程式控制

23
00:01:33,432 --> 00:01:38,040
這個相當類似於左邊的方程式

24
00:01:38,040 --> 00:01:43,890
最後，您應用了另一個 ReLU 運算也就是

25
00:01:43,890 --> 00:01:52,105
這個方程式來控制，而 G 會是 ReLU 非線性

26
00:01:52,105 --> 00:01:56,880
而這給您 a[l+2]

27
00:01:56,880 --> 00:01:57,900
換句話說

28
00:01:57,900 --> 00:02:03,035
從 a[l] 流到 a[l+2] 的資訊中

29
00:02:03,035 --> 00:02:07,455
它需要經過所有這些步驟，我將它稱為

30
00:02:07,455 --> 00:02:13,140
在這組網路層中的『主要路徑』

31
00:02:13,140 --> 00:02:14,550
在殘差網路中

32
00:02:14,550 --> 00:02:16,900
我們要改變這些

33
00:02:16,900 --> 00:02:18,495
我們拿這個 a[l]

34
00:02:18,495 --> 00:02:22,805
快速前進，複製它

35
00:02:22,805 --> 00:02:26,200
前進到神經網路的這裡

36
00:02:26,200 --> 00:02:28,860
只是加入這個 a[l]

37
00:02:28,860 --> 00:02:34,080
在應用非線性， ReLU 非線性以前

38
00:02:34,080 --> 00:02:37,730
我將其稱為『捷徑』

39
00:02:37,730 --> 00:02:40,725
與其需要順著主要路徑

40
00:02:40,725 --> 00:02:43,335
這個 a[l] 的資訊可以順著

41
00:02:43,335 --> 00:02:46,910
捷徑到神經網路的更深層

42
00:02:46,910 --> 00:02:49,680
這個意思是在最後這個方程式

43
00:02:49,680 --> 00:02:52,760
去掉，與其我們的輸出

44
00:02:52,760 --> 00:03:00,810
a[l+2] 為 ReLU 非線性 g 應用到 z[l+2]

45
00:03:00,810 --> 00:03:02,830
現在再加上 a[l]

46
00:03:02,830 --> 00:03:05,515
所以加上一個 a[l] 在這裡

47
00:03:05,515 --> 00:03:07,355
造成了一個殘差區塊

48
00:03:07,355 --> 00:03:11,070
在圖形上，您也可以修改上面圖形

49
00:03:11,070 --> 00:03:15,945
畫這個捷徑到這裡

50
00:03:15,945 --> 00:03:20,805
而我們要畫的方式是它進入到這第二層這裡

51
00:03:20,805 --> 00:03:26,220
因為捷徑實際上是在
ReLU 非線性之前加入的

52
00:03:26,220 --> 00:03:27,570
所以每一個節點

53
00:03:27,570 --> 00:03:30,560
有應用線性函數跟 ReLU 函數

54
00:03:30,560 --> 00:03:34,320
所以 a[l] 注入在線性函數之後，在 ReLU 之前

55
00:03:34,320 --> 00:03:37,815
有時候與其用捷徑這個名詞

56
00:03:37,815 --> 00:03:40,485
您也會聽到跳過連結 (skip connection) 這個名詞

57
00:03:40,485 --> 00:03:44,835
而這指的是 a[l] 跳過一層，或者說是跳過

58
00:03:44,835 --> 00:03:51,090
幾乎兩層為了要
在更深層的神經網路上處理資訊

59
00:03:51,090 --> 00:03:54,030
所以深度殘差網路的發明者

60
00:03:54,030 --> 00:03:55,950
也就是  Kaiming He, Xiangyu Zhang

61
00:03:55,950 --> 00:03:58,925
Shaoqing Ren, Jian Sun.

62
00:03:58,925 --> 00:04:02,010
他們發現到使用殘差區塊

63
00:04:02,010 --> 00:04:05,920
讓您可以訓練更深層的神經網路

64
00:04:05,920 --> 00:04:10,785
而建置深度殘差網路方式是用很多這樣的殘差區塊

65
00:04:10,785 --> 00:04:15,695
像這樣，一起疊起來變成一個深度網路

66
00:04:15,695 --> 00:04:18,150
我們來看看這個網路

67
00:04:18,150 --> 00:04:19,730
這不是一個殘差網路

68
00:04:19,730 --> 00:04:22,950
這是稱為『一般網路』(plain network)

69
00:04:22,950 --> 00:04:26,830
這個專有名詞用在深度殘差網路論文中

70
00:04:26,830 --> 00:04:28,675
將這個轉為深度殘差網路

71
00:04:28,675 --> 00:04:31,050
您要做的是加入這些

72
00:04:31,050 --> 00:04:36,475
跳過連結或者捷徑連結像這樣

73
00:04:36,475 --> 00:04:39,875
所以每兩層終點會

74
00:04:39,875 --> 00:04:44,710
加入這種改變就像我們在

75
00:04:44,710 --> 00:04:49,520
前面投影片看到的
轉成殘差區塊

76
00:04:49,520 --> 00:04:53,770
所以這個圖形顯示了五個殘差區塊疊在一起

77
00:04:53,770 --> 00:04:56,565
而這就是殘差網路

78
00:04:56,565 --> 00:04:59,615
實際上如果您使用了

79
00:04:59,615 --> 00:05:02,620
您的標準最佳化演算法像是

80
00:05:02,620 --> 00:05:04,120
梯度下降法或者

81
00:05:04,120 --> 00:05:07,340
比較時髦的最佳化演算法來訓練這個『一般網路』

82
00:05:07,340 --> 00:05:10,270
不使用所有這些額外的殘差

83
00:05:10,270 --> 00:05:14,030
不使用所有這些我剛畫的
捷徑或者說跳過連結

84
00:05:14,030 --> 00:05:18,965
經驗上來講，您會發現到當您增加層數時

85
00:05:18,965 --> 00:05:21,100
訓練誤差會趨向於降低

86
00:05:21,100 --> 00:05:24,240
經過一會兒，它又會反轉向上

87
00:05:24,240 --> 00:05:29,170
理論上當您用更深層的神經網路

88
00:05:29,170 --> 00:05:32,935
它應該在訓練集的表現越來越好

89
00:05:32,935 --> 00:05:35,155
對吧，理論上

90
00:05:35,155 --> 00:05:37,815
用更深層的網路應該只會有幫助

91
00:05:37,815 --> 00:05:40,435
但實際上

92
00:05:40,435 --> 00:05:42,925
在一般網路上，非深度殘差網路中

93
00:05:42,925 --> 00:05:45,890
用一個很深的一般網路意思是

94
00:05:45,890 --> 00:05:50,220
您的最佳化演算法越來越難訓練

95
00:05:50,220 --> 00:05:51,685
所以實際上

96
00:05:51,685 --> 00:05:55,865
您的訓練誤差會越來越差
如果您選的網路太深的話

97
00:05:55,865 --> 00:06:01,530
但如果是用深度殘差網路的話，即使層數越來越深

98
00:06:01,530 --> 00:06:06,120
您還是可以有訓練誤差越來越低的效果

99
00:06:06,120 --> 00:06:10,030
即使訓練的網路超過一百層

100
00:06:10,030 --> 00:06:12,820
也一些人實驗一個網路

101
00:06:12,820 --> 00:06:17,845
超過一千層，雖然我覺得還不實用

102
00:06:17,845 --> 00:06:20,230
但拿這些啟動值加入

103
00:06:20,230 --> 00:06:24,950
到中間啟動值讓它可以在神經網路中走得更深

104
00:06:24,950 --> 00:06:30,355
這真的幫助了資料消失跟梯度爆炸的問題

105
00:06:30,355 --> 00:06:31,930
讓您可以訓練

106
00:06:31,930 --> 00:06:36,220
更深層的網路而不會真的失去績效

107
00:06:36,220 --> 00:06:39,370
或許到某一點後，這個會攤平，會平坦出去

108
00:06:39,370 --> 00:06:43,090
但它無法幫助到太深太深的網路

109
00:06:43,090 --> 00:06:49,120
但深度殘差網路有效地幫助到深層網路

110
00:06:49,120 --> 00:06:52,645
所以您看到了深度殘差網路如何作用的簡介

111
00:06:52,645 --> 00:06:55,495
實際上，在這個禮拜的程式練習

112
00:06:55,495 --> 00:06:59,205
您要建置這個觀念
然後自己親眼看它如何作用

113
00:06:59,205 --> 00:07:02,350
在下一段，我想分享您更好的直觀

114
00:07:02,350 --> 00:07:06,160
甚至於為什麼深度殘差網路
作用如此好的直觀

115
00:07:06,160 --> 00:07:07,730
我們往下一段影片邁進