1
00:00:00,010 --> 00:00:03,935
在设计卷积网络的架构时

2
00:00:03,935 --> 00:00:09,140
其中一种很有用的想法是用1*1的卷积

3
00:00:09,140 --> 00:00:10,685
现在你可能在想

4
00:00:10,685 --> 00:00:13,035
1*1的卷积能做些什么呢？

5
00:00:13,035 --> 00:00:15,170
难道不就是数的相乘吗？

6
00:00:15,170 --> 00:00:17,150
这看起来是一件很滑稽的事

7
00:00:17,150 --> 00:00:18,680
事实并非完全如此

8
00:00:18,680 --> 00:00:20,223
让我们来看看

9
00:00:20,223 --> 00:00:22,740
这是一个1*1的过滤器

10
00:00:22,740 --> 00:00:24,970
我们会把数字2放进去

11
00:00:24,970 --> 00:00:27,460
如果你把这个6*6的图像

12
00:00:27,460 --> 00:00:31,310
这个6*6*1的图像去和这个1*1*1的滤波器做卷积

13
00:00:31,310 --> 00:00:33,940
你最终只是把这个图像乘以2

14
00:00:33,940 --> 00:00:37,580
所以 1 2 3最终变成2 4 6

15
00:00:37,580 --> 00:00:40,190
如此类推

16
00:00:40,190 --> 00:00:43,940
所以通过一个1*1的过滤器做卷积

17
00:00:43,940 --> 00:00:45,350
似乎不是特别有用

18
00:00:45,350 --> 00:00:47,615
你只是把它乘以某个数字

19
00:00:47,615 --> 00:00:53,635
但这是一个6*6*1一个通道图像的例子

20
00:00:53,635 --> 00:00:58,415
如果你有一个6*6*32 而不是6*6*1(的图像）

21
00:00:58,415 --> 00:01:04,995
那么和一个1*1的过滤器做卷积会更有意义

22
00:01:04,995 --> 00:01:08,945
特别是一个1*1的卷积将会

23
00:01:08,945 --> 00:01:13,620
逐一扫过这里的36个不同的位置

24
00:01:13,620 --> 00:01:16,720
然后进行对应元素间的乘法

25
00:01:16,720 --> 00:01:21,185
将左边的32个数和过滤器中的32个数相乘 (做内积)

26
00:01:21,185 --> 00:01:26,230
然后将一个非线性映射 ReLU(线性整流函数)作用于它

27
00:01:26,230 --> 00:01:29,405
以这36个位置中的一个为例

28
00:01:29,405 --> 00:01:32,840
比如这个立方体中的这一面

29
00:01:32,840 --> 00:01:41,525
你将这36个数乘以这整面中的这一条

30
00:01:41,525 --> 00:01:43,535
最后你会得到

31
00:01:43,535 --> 00:01:49,945
一个实数，就像这里所画的，是其中一个输出值

32
00:01:49,945 --> 00:01:52,535
事实上你可以这样理解

33
00:01:52,535 --> 00:01:56,645
这个1*1*32的过滤器中的32个数

34
00:01:56,645 --> 00:02:01,020
它类似于你有一个神经元

35
00:02:01,020 --> 00:02:06,770
接收一个32个数的输入向量

36
00:02:06,770 --> 00:02:12,285
将这一条位于相同位置，即相同高度和宽度，但位于32个不同通道的数

37
00:02:12,285 --> 00:02:15,665
和32个权重相乘

38
00:02:15,665 --> 00:02:22,115
然后将线性整流函数(ReLU)作用于它，再把对应的结果输出到这里

39
00:02:22,115 --> 00:02:28,875
更通常地

40
00:02:28,875 --> 00:02:31,020
而有多个过滤器

41
00:02:31,020 --> 00:02:36,240
那么这类似于，你有不止一个单元，而有多个单元

42
00:02:36,240 --> 00:02:40,950
接受用一面中的所有数为输入

43
00:02:40,950 --> 00:02:49,530
然后将它们生成为一个6*6*滤波器数的输出量

44
00:02:49,530 --> 00:02:53,260
所以对1*1卷积的一种理解是

45
00:02:53,260 --> 00:02:59,115
它本质上是一个完全连接的神经网络

46
00:02:59,115 --> 00:03:04,015
逐一作用于这36个不同的位置

47
00:03:04,015 --> 00:03:05,920
这个完全连接的神经网络所做的是

48
00:03:05,920 --> 00:03:13,210
它接收32个数的输入，然后输出过滤器数个输出值

49
00:03:13,210 --> 00:03:14,555
所以要标记的话

50
00:03:14,555 --> 00:03:15,910
它其实是nc上标l+1 (即l+1层的通道数)

51
00:03:15,910 --> 00:03:19,440
如果这是往后一层的话

52
00:03:19,440 --> 00:03:22,710
然后对这36个位置的中每一个都进行相同的操作

53
00:03:22,710 --> 00:03:24,160
也就是对这6乘6个位置中的每一个(都进行相同的操作)

54
00:03:24,160 --> 00:03:29,850
你最终得到的输出是6*6*过滤器数

55
00:03:29,850 --> 00:03:35,830
这对你的输入量所进行的是一个非常不平凡的计算

56
00:03:35,830 --> 00:03:40,755
这个想法经常被称为1*1卷积

57
00:03:40,755 --> 00:03:46,655
但有时也被称为网中网(Network in Network)

58
00:03:46,655 --> 00:03:49,468
它源于这篇由

59
00:03:49,468 --> 00:03:53,485
Min Lin, Qiang Chen和Shuicheng Yan所著的论文

60
00:03:53,485 --> 00:03:58,400
虽然这篇论文中的结构细节没有被广泛地使用

61
00:03:58,400 --> 00:04:01,460
这个1*1卷积的想法

62
00:04:01,460 --> 00:04:05,300
有时也被称为网中网的想法，有很大的影响力

63
00:04:05,300 --> 00:04:08,090
影响了许多其他的神经网络架构

64
00:04:08,090 --> 00:04:11,860
包括我们在下一个视频中会看到的inception network

65
00:04:11,860 --> 00:04:16,180
不过举一个1*1卷积有应用价值的例子

66
00:04:16,180 --> 00:04:18,443
这是一个你会用到它的时候

67
00:04:18,443 --> 00:04:23,070
假设你有一个28*28*192的立方体

68
00:04:23,070 --> 00:04:25,715
如果你想缩小它的高度和宽度

69
00:04:25,715 --> 00:04:27,310
你可以使用一层池化层

70
00:04:27,310 --> 00:04:28,800
我们知道怎样做

71
00:04:28,800 --> 00:04:34,265
但如果通道数过大，我们就想缩小它

72
00:04:34,265 --> 00:04:40,260
你会如何将它缩小到28*28*32的大小呢？

73
00:04:40,260 --> 00:04:48,058
那么你可以使用32个1*1的过滤器

74
00:04:48,058 --> 00:04:52,700
从技术上说，每一个滤波器将会是1*1*192大小的

75
00:04:52,700 --> 00:04:55,440
因为过滤器的通道数

76
00:04:55,440 --> 00:04:58,570
需要和输入量的通道数一致

77
00:04:58,570 --> 00:05:08,035
但由于你使用了32个滤波器，这个过程的输出将会是28*28*32大小的立方体

78
00:05:08,035 --> 00:05:12,850
这是一种让你缩小nc的方式

79
00:05:12,850 --> 00:05:17,750
而池化层我只可以用它来缩小nh和nw

80
00:05:17,750 --> 00:05:20,436
即这个立方体的高度和宽度

81
00:05:20,436 --> 00:05:23,510
我们会在之后看到

82
00:05:23,510 --> 00:05:28,670
这个1*1卷积的想法是如何缩小通道数

83
00:05:28,670 --> 00:05:31,450
从而达到在某些网络中减少计算量的目的的

84
00:05:31,450 --> 00:05:37,165
当然如果你想要保持192个通道数，这也是可行的

85
00:05:37,165 --> 00:05:41,470
这时1*1卷积的效果是增加非线性性

86
00:05:41,470 --> 00:05:45,740
它通过添加一层输入28*28*192 再输出28*28*192(的操作)

87
00:05:45,740 --> 00:05:52,423
使得你的网络可以学习到更复杂函数形式

88
00:05:52,423 --> 00:05:54,620
因此这就是一个1*1的卷积层

89
00:05:54,620 --> 00:05:58,280
如何可以做一些不平凡的操作

90
00:05:58,280 --> 00:06:01,190
以及在神经网络中增加非线性性

91
00:06:01,190 --> 00:06:04,525
以及允许你减少，或不改变或如果你愿意的话

92
00:06:04,525 --> 00:06:08,565
增加输入的通道数

93
00:06:08,565 --> 00:06:13,960
接下来你会看到这在构建inception network中是十分有用

94
00:06:13,960 --> 00:06:16,860
让我们在下一个视频中继续

95
00:06:16,860 --> 00:06:22,145
因此你现在看到了一个1*1的卷积操作

96
00:06:22,145 --> 00:06:26,255
实际上是一个非常不平凡的操作，它使得你可以缩小

97
00:06:26,255 --> 00:06:28,640
输入体积的通道数

98
00:06:28,640 --> 00:06:31,270
或不改变它，或如果你想的话，甚至增加它

99
00:06:31,270 --> 00:06:33,077
在下一视频中

100
00:06:33,077 --> 00:06:36,140
你会看到它可以被用来构建

101
00:06:36,140 --> 00:06:39,670
inception network 让我们进入下一个视频