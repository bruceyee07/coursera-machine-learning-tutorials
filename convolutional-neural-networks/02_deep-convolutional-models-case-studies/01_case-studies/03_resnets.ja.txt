とても とても 深いニューラルネットワークは 訓練するのが難しい なぜなら 勾配消失や発散 問題があるためだ このビデオでは スキップ コネクションについて学ぶ
それは １つの層からの活性を 突然 ニューラルネットワークの別のもっと深い層に与える そして それを使うと ResNetができて とても とても 深いディープなネットワークを訓練できるようになる 時折 ネットワークは 100層を超える
見ていこう ResNet は 残差(Residual)ブロック と呼ばれるものから作られている 最初に それが何なのか説明しよう ここに ２層のニューラルネットワークがある 層 a[l] における 活性から始まり それから a[l+1] そして 2層後の活性 a[l+2] この計算ステップを追ってみよう
a[l] があり それから 最初にやるのは これを線形処理することだ それは この方程式で決まる そして a[l] から z[l+1] を計算する 重み行列を掛けて バイアス ベクトルを足すことでね その後 ReLU 非線形を適用し a[l+1] を得る それは この方程式だ
a[l+1] は g(z[l+1]) だ それから 次の層では この線形ステップを再び行い この式で決まるやつだ これは 左のと 殆ど同じだ そして 最後に 別のReLU処理を適用する この式で計算される
g() は ReLU非線形だ そして これで a[l+2] を得る 言い換えると、 a[l] から a[l+2] へ流れる情報は これらのステップを全て経由する必要がある この一連の層の 本道だ 残差(residual)ネットでは これに変更を加える a[l] を取り 先送りする それをコピーして ニューラルネットワークのもっと先 ここへ そして 単に a[l] を足す 非線形 ReLU非線形を適用する前に これは ショートカット だ 本道に沿わずに a[l] からの情報は 今度は ショートカットを流れ ニューラルネットワークに深く入って行く そして これが意味するのは
この最後の式は 消えてなくなり その代わり a[l+2] は ReLU非線形 g に 前と同じく z[l+2] を適用させるけど 今度は a[l] を加える ここに加えた a[l] が 残差(residual)ブロックを作る それと 絵で 上にあるこの絵を ここに 追加のショートカットを描いて 変更する ここの２番目の層の箱に入るように なぜなら ショートカットは ReLU非線形の前に 足されるからだ それから ここの各ノードは 線形関数に掛けられて それから ReLUを掛けられるけど a[l] は 線形関数の後 ReLUの前に 挿入される 時々 "ショートカット"という名前ではなく "スキップ コネクション"という言葉を聞くことがある それは a[l] が １層スキップしている もしくは ほぼ２層スキップしているようなものだからだ
ニューラルネットワークの深い方へ情報を処理するのにね ResNetの発明者は？ Kaiming He に Xiangyu Zhang Shaoqing Ren に Jian Sun だろう 彼らが見つけたのは 残差ブロックを使うことで より深いニューラルネットワークを訓練できるようになるということだ そして ResNetの作り方は
この様な 残差ブロックを 多く使い 積み重ねて 深いネットワークを作るんだ じゃ このネットワークを見てみよう これは Residual Network じゃない これは 平坦な(プレーン)ネットワークと呼ばれる これは ResNet 論文の用語だけど これを ResNet に変えよう やることは これらの スキップ コネクション を
もしくは ショートカットを こんな風に 加えることだ よって 各２層が 前のスライドのように 追加の変更をされて 残差ブロックとなる それで この絵では ５つの残差ブロックが積み上げられている そして これが Residual Network だ 分かったことがある
もし 標準的な 最適化アルゴリズムを使う場合 勾配降下法や もっと凝った最適化アルゴリズムを 平坦なネットワークの訓練に使うなら つまり このような 追加の残差無し 追加のショートカット もしくは スキップ コネクション無しなら 経験的には 層の数が増えるほど 学習誤差は しばらく減った後 また 増え始める傾向がある 理論的には ニューラルネットワークを深くすれば 学習セットに対して どんどん 良くなっていくはずだ そうそう 理論ではね 理論では より深いネットワークは より良いと しかし 実際には 平坦なネットワークでは ResNetではない場合は とても深い平坦なネットワークでは 最適化アルゴリズムによる学習が難しい 現実はそうなんだ そんなに深いネットワークを選ぶと 学習誤差は増加しちゃう しかし ResNetでは 層の数が深くなっていっても 学習誤差を下げ続けるような性能を得ることができる 100層を超えるネットワークを訓練したとしてもね それから 1000層を超えるネットワークの研究をしている人たちもいる 実際に使われているのは あまり見ないけど でも これらの活性 Xや これらの 中間の活性を使って ニューラルネットワークをより深くすることにより これが 本当に 勾配の消失や発散問題の助けになり とても深いニューラルネットワークを訓練できるようにする それほど大きく性能を損なわずに ある時点で これは平らになる そして これは とても深い深いネットワークでは そんなに役立たない しかし ResNetは それでも有効で とても深いネットワークの学習を助ける さぁ これで どのように ResNetが動くのか分かった 実際 今週のプログラミング演習では 自分で これらのアイデアを実装し 動くのを見てもらう しかし 次は より良い直観 もしくは より多くの直観を
あなたと共有したい なぜ ResNetは そんなにうまく行くのかについて 次のビデオに進もう