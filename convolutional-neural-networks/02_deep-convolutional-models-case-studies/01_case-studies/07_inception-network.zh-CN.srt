1
00:00:00,190 --> 00:00:01,320
在之前的视频中

2
00:00:01,320 --> 00:00:06,420
你已经学到了所有构建Inception网络的基本模块

3
00:00:06,420 --> 00:00:10,680
在这个视频中我们把这些基本模块连接起来

4
00:00:10,680 --> 00:00:12,780
从而建立自己的Inception 网络

5
00:00:13,910 --> 00:00:17,970
Inception 模块的输入一般是激活值

6
00:00:17,970 --> 00:00:20,260
或者是来自上次层的输出

7
00:00:20,260 --> 00:00:25,860
比如 我们有 28 x 28 x192 (的输入）

8
00:00:25,860 --> 00:00:28,030
等同上一个视频中的例子

9
00:00:28,030 --> 00:00:35,130
那个例子中 首先是1x1的卷积核 然后是5x5卷积核

10
00:00:35,130 --> 00:00:41,120
可能 1x1 的 卷积核有 16个通道

11
00:00:41,120 --> 00:00:48,170
5x5 的卷积核 输出28x28x32 个通道

12
00:00:49,700 --> 00:00:53,790
这是前面视频中我们讲的例子

13
00:00:54,900 --> 00:01:02,260
这里 为了节省3x3的卷积计算 我们也可以做同样的事情

14
00:01:02,260 --> 00:01:08,310
这样 3x3 的输入就是28x28x128

15
00:01:09,390 --> 00:01:14,100
如果你考虑一个1x1的卷积

16
00:01:14,100 --> 00:01:18,481
我们没有必要去做两个连续的1x1 的卷积

17
00:01:18,481 --> 00:01:23,409
这里只需要一步 比如这里输出是28x28x64

18
00:01:23,409 --> 00:01:31,900
接着最后是池化层

19
00:01:34,000 --> 00:01:35,730
这里我们可以做一些有意思的操作

20
00:01:35,730 --> 00:01:40,154
为了把最后所有的输出合并起来

21
00:01:40,154 --> 00:01:44,073
我们将会用同样的padding来做池化

22
00:01:44,073 --> 00:01:47,480
因此这里的输出的高和宽维度仍旧是28x28

23
00:01:47,480 --> 00:01:53,300
所以我们能把其他的所有输出都连接起来

24
00:01:53,300 --> 00:01:57,842
这里需要注意的是如果你做最大化池化
即使用了一样的padding

25
00:01:57,842 --> 00:01:59,917
3x3 过滤器

26
00:01:59,917 --> 00:02:07,016
这里的输出是28x28x192

27
00:02:07,016 --> 00:02:10,790
它的通道的个数 

28
00:02:10,790 --> 00:02:15,570
以及深度和我们给的输入是一样的

29
00:02:15,570 --> 00:02:19,252
这个看上去好像有很多通道

30
00:02:19,252 --> 00:02:23,752
因此我们要做的是加一个1x1的卷积层

31
00:02:23,752 --> 00:02:28,607
像之前我们在1x1卷积神经核的视频中看到的一样

32
00:02:28,607 --> 00:02:31,541
来增强通道的个数

33
00:02:31,541 --> 00:02:37,730
因此这样我们得到了28x28x32

34
00:02:37,730 --> 00:02:44,770
这样我们用32个过滤器

35
00:02:44,770 --> 00:02:49,660
他们的维度是1x1x192

36
00:02:49,660 --> 00:02:54,110
这就是为什么输出的维度缩减到了32个通道

37
00:02:54,110 --> 00:02:58,460
这样的话我们避免了在最后的输出中把所有的

38
00:02:58,460 --> 00:03:00,920
通道到加到池化层

39
00:03:02,310 --> 00:03:08,121
最后 所有的模块都用来做渠道连接

40
00:03:08,121 --> 00:03:12,865
就是连接 64x128x32x32

41
00:03:12,865 --> 00:03:18,165
然后加起来以后你会得到一个

42
00:03:18,165 --> 00:03:24,055
28x28x256的输出

43
00:03:24,055 --> 00:03:30,879
渠道链接就是像我们在前面的视频中提到的
链接这些模块

44
00:03:33,918 --> 00:03:39,598
这就是1个inception 模块 inception 网络所做的

45
00:03:39,598 --> 00:03:44,057
差不多就是把这些模块放在一起

46
00:03:45,624 --> 00:03:50,671
这里有一张inception网络的图片 来自Szegedy et al

47
00:03:53,615 --> 00:03:56,570
你可能已经看到了 在这个图里有很多重复的模块

48
00:03:56,570 --> 00:03:58,580
可能这个图片看起来很复杂

49
00:03:58,580 --> 00:04:03,563
不过如果你看只看其中一个模块的化
这个模块基本上就是

50
00:04:03,563 --> 00:04:07,920
前一张图中的一个inception模块

51
00:04:10,046 --> 00:04:17,085
至于其中的小细节 我就不详细讲了
比方这是另一个inception 模块

52
00:04:17,085 --> 00:04:19,460
这里是另一个inception 模块

53
00:04:19,460 --> 00:04:23,200
这里还有一些额外的最大池化层来改变高度

54
00:04:24,250 --> 00:04:25,800
的维度的

55
00:04:25,800 --> 00:04:28,140
但是这也是另一个inception模块

56
00:04:28,140 --> 00:04:31,020
还有另一个池化层来改变高和宽的维度

57
00:04:31,020 --> 00:04:33,280
基本上来说者还是一个inception 模块

58
00:04:33,280 --> 00:04:37,370
但是inception网络不过就是很多这些我们刚学到的模块

59
00:04:37,370 --> 00:04:40,930
重复在这个网络的不同位置

60
00:04:40,930 --> 00:04:44,509
你从前一节已经理解了inception模块

61
00:04:44,509 --> 00:04:46,914
因此这里你会相应的理解inception 网络

62
00:04:49,518 --> 00:04:53,403
最后一个细节就是 加入你读了选读的

63
00:04:53,403 --> 00:04:55,430
论文

64
00:04:55,430 --> 00:04:59,311
里面有讲到这些额外的旁枝 我们刚加上

65
00:05:01,835 --> 00:05:03,440
那么体育数据分析员们怎么应对呢？

66
00:05:03,440 --> 00:05:07,800
这个网络的最后这些层是一个完全连接层

67
00:05:07,800 --> 00:05:11,360
接着是一个softmax层 用来做预测

68
00:05:11,360 --> 00:05:15,430
那么这个旁枝的作用就是 它把隐藏层作为输入

69
00:05:15,430 --> 00:05:17,760
来做预测

70
00:05:17,760 --> 00:05:22,140
因此这是一个softmax输出

71
00:05:22,140 --> 00:05:23,952
这里另一个旁枝

72
00:05:23,952 --> 00:05:29,173
也是把一个隐藏层作为输入 经过一些层 比方一些连接层

73
00:05:29,173 --> 00:05:33,243
然后通过softmax方程来预测输出的种类

74
00:05:35,545 --> 00:05:38,798
你可以把这个看作是inception网络的

75
00:05:38,798 --> 00:05:40,000
另一个细节

76
00:05:40,000 --> 00:05:44,460
但是它的作用是用来保证所计算的特征值

77
00:05:44,460 --> 00:05:48,060
即使他们是在最头部的单元里 或者在中间层里

78
00:05:48,060 --> 00:05:52,490
他们对于预测结果来说不算太差

79
00:05:52,490 --> 00:05:56,875
这是对inception网络的正则化

80
00:05:56,875 --> 00:05:59,667
用来防止这个网络的过度学习

81
00:06:03,048 --> 00:06:07,907
顺便 这里的这个inception网络

82
00:06:07,907 --> 00:06:11,770
是google的工程师开发的

83
00:06:11,770 --> 00:06:18,850
把它称作 GoogleNet 为了致敬我们之前 

84
00:06:18,850 --> 00:06:21,350
在前一节中学习的LeNet

85
00:06:23,460 --> 00:06:29,086
因此我觉得非常好的就是深度学习的社区非常

86
00:06:29,086 --> 00:06:30,436
合作

87
00:06:30,436 --> 00:06:32,593
并展现了对社区中其他人的工作

88
00:06:32,593 --> 00:06:35,865
积极健康的尊重

89
00:06:35,865 --> 00:06:37,895
最后 一个有趣的实习是

90
00:06:37,895 --> 00:06:40,425
这个名字 inception网络是从哪里来的呢

91
00:06:41,585 --> 00:06:47,375
发表Inception的论文实际上提到了一个meme: 
我们需要深度

92
00:06:47,375 --> 00:06:52,315
并且论文里有这个

93
00:06:52,315 --> 00:06:54,320
图片的链接

94
00:06:54,320 --> 00:06:57,610
假如你看过电影Inception的化

95
00:06:57,610 --> 00:07:00,070
可能你会对这个Meme有同感

96
00:07:00,070 --> 00:07:05,380
但是这些作者们实际上把这个图片当做了

97
00:07:05,380 --> 00:07:09,040
构建更加有深度的神经网络的动机

98
00:07:09,040 --> 00:07:12,890
这就是他们提出了inceptipn架构的背后的故事

99
00:07:12,890 --> 00:07:17,830
我想 这里论文中引用网络meme的例子可能不常见

100
00:07:17,830 --> 00:07:19,030
在他们的引文中

101
00:07:19,030 --> 00:07:22,165
可是在这个例子中
它(网络meme)的作用还是很明显的

102
00:07:23,285 --> 00:07:27,015
这里我们来总结一下
如果你理解了inception 模块的化

103
00:07:27,015 --> 00:07:29,765
这里你也会相应的理解inception 网络

104
00:07:29,765 --> 00:07:33,865
所谓的inception 网络 就是重复多次使用inception 模块

105
00:07:33,865 --> 00:07:35,435
的网络结构

106
00:07:35,435 --> 00:07:40,025
自从inception模块开发以来

107
00:07:40,025 --> 00:07:43,760
其作者和其他的研究人员
也开发了不同版本的inception网络

108
00:07:43,760 --> 00:07:49,090
有一些的新的inception算法在不同的研究论文提出

109
00:07:49,090 --> 00:07:53,380
可能有时候你会看到有人会使用最新版本的算法

110
00:07:53,380 --> 00:07:57,040
比方inception v2 inception v3 inception v4

111
00:07:57,040 --> 00:07:59,360
另外 有一个版本的inception网络

112
00:07:59,360 --> 00:08:02,800
结合了resonant的想法 来跳过一些连接层

113
00:08:02,800 --> 00:08:05,740
有时候会有更好的效果

114
00:08:05,740 --> 00:08:10,600
但是这些不同的版本都是建立在我们在之前的视频中

115
00:08:10,600 --> 00:08:14,710
学习到的inception模块的基本思想之上

116
00:08:14,710 --> 00:08:17,510
然后叠加了一定数目的inception 模块

117
00:08:17,510 --> 00:08:20,530
通过这个视频 你应该能够

118
00:08:20,530 --> 00:08:23,940
读懂inception 论文

119
00:08:23,940 --> 00:08:28,790
还有其他介绍新的版本的inception算法的论文

120
00:08:28,790 --> 00:08:30,020
这就是我们今天要讲的

121
00:08:30,020 --> 00:08:34,820
你已经学习了一定数量的神经网络

122
00:08:34,820 --> 00:08:39,650
在下一个视频中 我会展示一些使用的技巧

123
00:08:39,650 --> 00:08:43,830
这些算法可以用来构建你自己的计算机视觉系统

124
00:08:43,830 --> 00:08:45,090
我们下一节再见