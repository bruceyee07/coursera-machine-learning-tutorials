ConvNet構造の設計面では 本当に助けになるアイデアに 1 x 1 畳み込みがある 今 疑問に思ったでしょ？ 1 x 1 畳み込みが何をやるのか 数を掛けるだけじゃないの？ おかしな事をやるように思える 全く そんなことは無い 次の例をみてみましょう。 ここに 1 x 1 フィルターがある そこれに 数２を置く そして もし この 6 x 6 画像を 6 x 6 x 1 を この 1 x 1 x 1 フィルターで畳み込むと 単に 画像を取って それに２を掛けただけの結果を得る そう 1 2 3 は 2 4 6 等となる よって 1 x 1 フィルターによる畳み込みは 特に有益には見えない 単に ある数を掛けただけだから しかし それは 6 x 6 x "１"チャンネル画像だからだ もし "1" ではなく 6 x 6 x "32" だったら 1 x 1 フィルターによる畳み込みは もっと意味を持った何かをできる 特に 1 x 1 畳み込みがすることは ここの36個の異なる位置を見て 要素ごとの掛け算を行う 左の32個の数を このフィルターで その後に ReLU非線形を適用する 36個の位置の内の１つを見てみよう このボリュームを貫く１断片だ これらの36(訳注: 32の言い間違いと思われる)個の数を
このように ボリューム全体を通した 1 x 1 断片と掛け算すると 結局こうなる 出力の各々は このようにプロットされる 単一の実数になる 実際 考え方としては この 1 x 1 x 32 フィルターにある 32個の数は ニューロンがあるようなもので 32個の数 これらの32個の各々 同じ位置 高さと場所にある 32の異なるチャンネルを持った１つの断片を 32の重みと掛け算して ReLU非線形に適用して それから ここに出力する さらに一般的には フィルターは１つだけではなく もし 複数のフィルターを持っていた場合は １ユニットではなく 複数ユニットがあるようなものだ １断片の全ての数を入力とし 6 x 6 x フィルター数 の出力を作る つまり 1 x 1 畳み込みの考え方の１つは 基本的には 全結合ニューロン ネットワークであり それは 62(訳注:36の言い間違いと思われる)個の異なる位置に適用される 全結合ニューロン ネットワークが行うのは 32個の数を入力として フィルター数分の出力を行う 表記にもよるが これは nC[l+1] それが 次の層にあるとすればね 36個の位置それぞれで これを行うと この 6 x 6 位置で行なうと 6 x 6 x フィルター数 の出力を得る これは 入力ボリュームに対して とても重要な計算となるかもしれない このアイデアは よく 1 x 1 畳み込みと呼ばれる でも 時々 Network in Network とも呼ばれる そして この論文に書かれているが Min Lin, Qiang Chen, そして Schuicheng Yan の論文に書かれているが この論文にある構造のアイデアは 広く使われていないにも関わらず 1 x 1 畳み込みのアイデア もしくは 時折 Network in Network と呼ばれるアイデアは 大きな影響を与えてきた それは 他のニューラルネットワーク構造の多くに影響を与えてきた 次のビデオで見る Inception ネットワーク も その内の一つだ でも 1 x 1 畳み込みが有効である例を示すため ここに それができるのを用意した 28 x 28 x 192 ボリュームがあるとする 高さと幅を縮小したいなら プーリング層を使うことができる その方法は知っているよね しかし チャンネル数が大きくて それを縮小したい場合はどうする？ 28 x 28 x 32 次元に どうやって 縮小する？ 32個の 1 x 1 フィルターを使えばできる 詳細に言えば 各フィルターは 1 x 1 x 192 次元だ なぜなら フィルターのチャンネル数は 入力ボリュームのチャンネル数と合っていないといけないからね でも 32個のフィルターを使うので 出力は 28 x 28 x 32 ボリュームになる これが nC を減らす方法の1つだ 一方 プーリング層は nH と nW のみを減らす ボリュームの高さと幅をね 後で やり方を見るけど 1 x 1 畳み込みのアイデアで チャンネル数を減らせる だから ネットワークの計算量を減らせる ただし 勿論 チャンネル数を192のままにしておきたいなら それもいい その場合 1 x 1 畳み込みの効果は 単に非線形化をしたことになる それにより ネットワークに より複雑な関数を学習させることができる 28 x 28 x 192 入力から 28 x 28 x 192 出力をする層を加えることでね これが 1 x 1 畳み込みが どのように 重要な何かを行うかということだ そして ニューラルネットワークに 非線形化を加えて 思うように ボリュームのチャンネル数を 減らしたり 元のままにしたりできる 次回では Inception ネットワークを作るのに これが本当に有益であるのが分かるだろう 次のビデオに行こう 今見てきたのは
1 x 1 畳み込み処理が 本当に 重要な処理を行い ボリュームのチャンネル数を減らしたり 保ったり やりければ 増やす事もできる ということだ 見ての通り、ここある。 次のビデオでは これが使われて Inception ネットワークを作るのに一役買っているのが分かるだろう
次のビデオに進もう