1
00:00:00,000 --> 00:00:03,370
为卷积网络设计某一层时，您可能需要选择，

2
00:00:03,370 --> 00:00:05,430
你想要一个1x3的滤波器（又译做卷积核）

3
00:00:05,430 --> 00:00:07,385
或者3x3，或5x5

4
00:00:07,385 --> 00:00:09,180
或者你想要一个池化层？

5
00:00:09,180 --> 00:00:11,920
inception网络是说

6
00:00:11,920 --> 00:00:13,095
为什么不全用呢？

7
00:00:13,095 --> 00:00:16,255
这使得网络架构更加复杂，

8
00:00:16,255 --> 00:00:18,380
但它的效果也变得更好。

9
00:00:18,380 --> 00:00:19,720
让我们来看几个例子

10
00:00:19,720 --> 00:00:23,380
举例来说，您已经输入了一个

11
00:00:23,380 --> 00:00:26,990
28x28x192的体积

12
00:00:26,990 --> 00:00:32,500
所以inception网络或inception层是指

13
00:00:32,500 --> 00:00:36,600
与其在卷积神经网络中选择一个你想使用的卷积核尺寸

14
00:00:36,600 --> 00:00:40,205
乃至选择你是否需要一个卷积层还是一个池化层

15
00:00:40,205 --> 00:00:46,795
让我们来全部都做吧。那么如果你可以使用1*1卷积核

16
00:00:46,795 --> 00:00:50,750
则会输出28*28的一个结果

17
00:00:50,750 --> 00:00:56,180
我们称之为28*28*64大小的输出

18
00:00:56,180 --> 00:00:58,405
也就是如图所示的这个量

19
00:00:58,405 --> 00:01:06,510
但是也许你也想尝试一个3*3大小的（卷积核），然后结果就是20*20*128

20
00:01:06,510 --> 00:01:13,975
然后你要做的是，把第二个量并列的堆叠在第一个量旁边

21
00:01:13,975 --> 00:01:16,415
并且保证维度是匹配的

22
00:01:16,415 --> 00:01:19,470
让我们使用same填充（same padding）

23
00:01:19,470 --> 00:01:23,500
所以输出的维度仍然是28*28

24
00:01:23,500 --> 00:01:26,965
输出是与输入的高度和宽度相同的

25
00:01:26,965 --> 00:01:31,300
在本例中的是28*28*128

26
00:01:31,300 --> 00:01:34,510
你也可能会想，我还想要同时使用一些其他的技巧来“对冲”一下

27
00:01:34,510 --> 00:01:36,710
也许5*5的卷积核效果更好

28
00:01:36,710 --> 00:01:44,755
所以我们也那样做了，进而得到输出结果为28*28*32维

29
00:01:44,755 --> 00:01:49,435
并且再使用same填充让维度变得匹配

30
00:01:49,435 --> 00:01:52,715
也许你不希望使用卷基层

31
00:01:52,715 --> 00:01:58,560
那么我们使用池化层，这会导致其他的输出，再把他们堆在一块儿

32
00:01:58,560 --> 00:02:05,070
在这里池化的输出为28*28*32

33
00:02:05,070 --> 00:02:09,195
为了使所有的维度匹配

34
00:02:09,195 --> 00:02:12,560
事实上，你需要为最大池化操作使用“填充”

35
00:02:12,560 --> 00:02:15,950
所以这是一种不一般的池化<br />因为如果你想

36
00:02:15,950 --> 00:02:19,985
输入图像和输出的大于都是28*28

37
00:02:19,985 --> 00:02:24,395
你要调整所有其他的维度也成为28*28

38
00:02:24,395 --> 00:02:31,020
然后你需要使用same填充，并且池化操作中的滑动步幅为1

39
00:02:31,020 --> 00:02:34,230
这一个细节也许现在看起来比较可笑

40
00:02:34,230 --> 00:02:35,520
但是让我们先继续讲下去

41
00:02:35,520 --> 00:02:39,310
最后我们会让所有部分正常工作起来

42
00:02:39,310 --> 00:02:43,170
但是对于像这样一个inception模块

43
00:02:43,170 --> 00:02:46,080
你可以输入一些量并且获得一输出

44
00:02:46,080 --> 00:02:48,640
在这种情况下，我想，如果你把所有这些数字加起来

45
00:02:48,640 --> 00:02:51,705
32 + 32 + 128 + 64

46
00:02:51,705 --> 00:02:54,915
等于256

47
00:02:54,915 --> 00:03:01,515
所以你将有一个inception模块，其输入28*28*129 <br />（这应该是口误，根据幻灯片，应该是192）

48
00:03:01,515 --> 00:03:06,275
输出为28*28*256

49
00:03:06,275 --> 00:03:11,300
这是inception网络的核心

50
00:03:11,300 --> 00:03:13,260
并且这是Christian Szegedy, Wei Liu

51
00:03:13,260 --> 00:03:15,130
Yangqing Jia, Pierre Sermanet

52
00:03:15,130 --> 00:03:17,750
Scott, Reed, Dragomir Anguelov, Dumitru Erhan,

53
00:03:17,750 --> 00:03:20,660
Vincent Canhoucke和Andrew Rabinovich做出的贡献

54
00:03:20,660 --> 00:03:23,950
并且这个网络最基础的一个特点就是

55
00:03:23,950 --> 00:03:30,000
你不用去只挑选一个卷积核的大小或者是pooling

56
00:03:30,000 --> 00:03:33,200
你可以所有可能都做 然后把所有的输出结果都连接起来

57
00:03:33,200 --> 00:03:36,270
然后让神经网络去学习它想要用到的参数

58
00:03:36,270 --> 00:03:40,050
以及它想要用到的卷积核大小

59
00:03:40,050 --> 00:03:42,420
但是这里会出现一个关于我们现在说的

60
00:03:42,420 --> 00:03:44,745
inception网络的问题：

61
00:03:44,745 --> 00:03:46,795
计算成本问题

62
00:03:46,795 --> 00:03:48,060
在下一个幻灯片

63
00:03:48,060 --> 00:03:54,485
我们来看一下使用这个5x5的卷积核导致的

64
00:03:54,485 --> 00:03:56,655
计算成本是多少

65
00:03:56,655 --> 00:04:02,735
我们只关注5x5的卷积核

66
00:04:02,735 --> 00:04:07,010
我们现在有一个28x28x192大小的输入

67
00:04:07,010 --> 00:04:14,620
然后你使用32个5x5的卷积核 使用same填充 然后输出一个28x28x32的张量

68
00:04:14,620 --> 00:04:18,750
在之前的那一个幻灯片 我把它画成了紫色并且很窄

69
00:04:18,750 --> 00:04:23,035
而我在这里就将它画成一个普通的立方体

70
00:04:23,035 --> 00:04:30,700
所以，让我们现在来看一下获得这个28x28x32输出的计算成本

71
00:04:30,700 --> 00:04:38,065
你有32个卷积核，因为你有32个通道

72
00:04:38,065 --> 00:04:44,805
然后每一个卷积核大小将是5x5x192

73
00:04:44,805 --> 00:04:48,410
那么输出张量的大小将是28x28x32

74
00:04:48,410 --> 00:04:53,600
所以你需要算28x28x32这么多的数

75
00:04:53,600 --> 00:04:58,685
所以每一个你都需要做这么多乘法：

76
00:04:58,685 --> 00:05:01,185
5x5x192

77
00:05:01,185 --> 00:05:03,550
所以你做乘法的总数

78
00:05:03,550 --> 00:05:07,010
就是你算每一个输出所需的乘法次数

79
00:05:07,010 --> 00:05:12,615
乘以你的输出数

80
00:05:12,615 --> 00:05:15,330
你乘完这一串数字以后

81
00:05:15,330 --> 00:05:18,790
结果等于1.2亿

82
00:05:18,790 --> 00:05:24,725
虽然你可以在现代的计算机里做1.2亿次乘法运算

83
00:05:24,725 --> 00:05:27,385
但是这个计算成本还是相当大的

84
00:05:27,385 --> 00:05:32,390
在下一个幻灯片里，你将会看到，

85
00:05:32,390 --> 00:05:34,210
通过使用你之前学过的

86
00:05:34,210 --> 00:05:38,630
1x1卷积运算，这能够降低计算成本到大约1/10左右

87
00:05:38,630 --> 00:05:44,400
从1.2亿次乘法运算到它的1/10

88
00:05:44,400 --> 00:05:48,575
请记住1.2亿这个数字

89
00:05:48,575 --> 00:05:52,045
以便和下一张幻灯片中的数字进行比较

90
00:05:52,045 --> 00:05:58,540
接下来我们就讲另一种结构

91
00:05:58,540 --> 00:06:03,020
它同样接受28x28x192的输入并且输出大小为28x28x32。

92
00:06:03,020 --> 00:06:05,175
你仍将输入28x28x192的向量

93
00:06:05,175 --> 00:06:14,580
但紧接着用1x1的卷积运算将通道的个数从192减小为16

94
00:06:14,580 --> 00:06:17,370
然后我们用这个相对较小的量

95
00:06:17,370 --> 00:06:21,915
去做5x5的卷积运算并得出最终结果

96
00:06:21,915 --> 00:06:24,665
注意第一种方法和第二种方法的输入和输出的维度都是相同的

97
00:06:24,665 --> 00:06:31,280
两种方法中，你都是输入28x28x192，然后输出28x28x32

98
00:06:31,280 --> 00:06:33,865
第二种和第一种一样

99
00:06:33,865 --> 00:06:37,340
但是第二种方法中，我们先将这个较大的输入

100
00:06:37,340 --> 00:06:42,110
减小成一个较小的中间值

101
00:06:42,110 --> 00:06:46,740
也就是这个只有16个通道而不是192个通道

102
00:06:46,740 --> 00:06:53,000
有时，我们会把中间的这个层叫做瓶颈层

103
00:06:53,600 --> 00:06:59,890
我猜啊，这是因为瓶颈是东西中最细的那个部分

104
00:06:59,890 --> 00:07:04,820
所以我想如果你有一个像这样的玻璃瓶

105
00:07:04,820 --> 00:07:09,065
然后这个是瓶塞...

106
00:07:09,065 --> 00:07:13,615
那么瓶颈就是这个细细的部分

107
00:07:13,615 --> 00:07:18,035
所以，一样的道理，瓶颈层也是这个网络中最小的部分

108
00:07:18,035 --> 00:07:22,625
在这里，我们将输入在变大前缩小

109
00:07:22,625 --> 00:07:26,945
现在，我们再来看一下这次的计算成本

110
00:07:26,945 --> 00:07:30,320
我们有16个卷积核

111
00:07:30,320 --> 00:07:32,510
去调用这个1x1的卷积运算

112
00:07:32,510 --> 00:07:37,145
每一个卷积核将会是1x1x192这么大

113
00:07:37,145 --> 00:07:40,170
这里的192对应输入层里的192

114
00:07:40,170 --> 00:07:43,300
那么生成这个28x28x16瓶颈层的计算成本

115
00:07:43,300 --> 00:07:45,870
将会是....

116
00:07:45,870 --> 00:07:48,205
你需要这么多输出，对吧

117
00:07:48,205 --> 00:07:54,900
然后每一个你都需要做192次乘法

118
00:07:54,900 --> 00:07:58,515
我能写成1x1x192，对吧

119
00:07:58,515 --> 00:08:00,960
如果你把这些数乘起来

120
00:08:00,960 --> 00:08:02,650
那么结果是240万

121
00:08:02,650 --> 00:08:04,370
大约是240万哈

122
00:08:04,370 --> 00:08:05,655
那么第二个运算的计算成本大概多大呢？

123
00:08:05,655 --> 00:08:11,325
这个是第一个卷积层的计算成本

124
00:08:11,325 --> 00:08:15,670
那么第二个运算的成本.....额

125
00:08:15,670 --> 00:08:17,290
你有这么多输出：

126
00:08:17,290 --> 00:08:19,340
28x28x32

127
00:08:19,340 --> 00:08:28,200
然后每一个输出你都需要调用一个5x5x16的卷积核

128
00:08:28,200 --> 00:08:31,305
然后5x5x16....

129
00:08:31,305 --> 00:08:36,520
然后相乘.....结果是1000万

130
00:08:36,520 --> 00:08:41,160
所以总的计算成本大概是240万加上1千万

131
00:08:41,160 --> 00:08:45,020
1240万次乘法

132
00:08:45,020 --> 00:08:47,955
然后你和我们第一种方法的计算成本比较

133
00:08:47,955 --> 00:08:53,095
就会发现，运算成本从1.2亿次运算

134
00:08:53,095 --> 00:08:55,810
减小到了大概1240万次运算

135
00:08:55,810 --> 00:08:59,335
大概是第一个方法的1/10

136
00:08:59,335 --> 00:09:02,345
然后在第二种方法中，你相加的次数

137
00:09:02,345 --> 00:09:06,305
和你相乘的次数差不多

138
00:09:06,305 --> 00:09:10,230
所以这就是为什么我只计算了相乘的次数

139
00:09:10,230 --> 00:09:13,490
所以，我们总结一下：如果你在建立一个卷积网络层

140
00:09:13,490 --> 00:09:16,140
并且你不想去决定

141
00:09:16,140 --> 00:09:17,820
到底是用1x1

142
00:09:17,820 --> 00:09:20,095
3x3还是5x5的卷积核或者是否使用的pooling

143
00:09:20,095 --> 00:09:23,560
那么就用这个inception模型，它会做所有的工作

144
00:09:23,560 --> 00:09:25,645
并会将所有的结果都连接起来

145
00:09:25,645 --> 00:09:28,720
然后我们讨论了计算成本的问题

146
00:09:28,720 --> 00:09:32,460
在这堂课里你看到了1x1卷积运算

147
00:09:32,460 --> 00:09:34,585
是如何生成了一个瓶颈层

148
00:09:34,585 --> 00:09:38,115
并且显著降低计算成本的

149
00:09:38,115 --> 00:09:39,725
你现在可能会有疑问

150
00:09:39,725 --> 00:09:43,370
你可能会问：如此剧烈地缩小特征表示的大小

151
00:09:43,370 --> 00:09:46,730
会不会影响神经网路的性能

152
00:09:46,730 --> 00:09:52,440
答案就是：如果你只要合理地去实现这个瓶颈层

153
00:09:52,440 --> 00:09:56,530
你既可以缩小输入张量的维度

154
00:09:56,530 --> 00:09:59,240
又不会影响到整体的性能

155
00:09:59,240 --> 00:10:01,700
还能给你节省计算成本

156
00:10:01,700 --> 00:10:07,835
所以这些就是inception模型的一些关键点

157
00:10:07,835 --> 00:10:09,790
让我们将这些放在一起

158
00:10:09,790 --> 00:10:14,100
然后在下一个视频中你会看到一个完整的inception网络是怎样的