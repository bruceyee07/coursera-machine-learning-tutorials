ResNet은 왜 잘 작동되는 걸까요? ResNet 이 왜 잘 작동되는지를 설명하는 예시를 살펴봅시다. 트레이닝 세트에서 그것들이 잘 수행할 수 있게 만드는 여러분의 능력을 다치게 하지 않 점점 더 깊어지게 만들 수 있는 지 그 방법적인 측면에서 보도록 하죠. 이 일련의 과정 중 세 번째 강의에서 여러분이 이해했기를 바라는데요, 트레이닝 세트에서 잘 수행하는 것이 그 다음에 있는 홀드아웃 와 테스트 세트를 잘 수행할 수 있는 조건입니다. 트레이닝 세트에서 ResNet을 잘 훈련시키는 것은 그것을 향해 제대로 된 첫 걸음을 내딛는 것이죠. 예시를 하나 보시죠. 마지막 강의에서 보셨던 것은 네트워크를 더 깊이 만들면 트레이닝 세트에서 잘 수행할 수 있게 하려다가 네트워크를 학습시키는 능력에 손상을 줄 수 있다는 것이었습니다. 그런 이유로 때로는 지나치게 깊은 네트워크는 원치 않게 되는 것이죠. 그러나 여러분이 ResNet를 학습시킬 때에는 이 말은 사실이 아니거나 적어도 사실일 가능성이 매우 적습니다. 예시를 통해서 보시죠. X가 큰 신경망에 입력되어 있고 아웃풋으로 액티베이션 a[1]을 가지고 있다고 가정해봅시다. 이 예제에서 신경망을 조금 더 깊게 만들고자 수정한다고 가정 해 봅시다. 따라서 동일한 큰 NN을 사용하면 이 아웃풋은 a[1]이 되고, 이 네트워크에 레이어를 추가적으로 몇 개 더할 것입니다. 그리고 나서 여기에 레이어를 하나 더 추가하고, 또 다른 레이어는 여기에 둡시다. 그럼 이제 아웃풋은 a[l + 2]가 되는군요. 이걸 추가된 shortcut을 이용해서 ResNet 블럭, 즉 잔류블럭으로 만들어봅시다. 토론을 위해서 이 네트워크 전반에 걸쳐 밸류 액티베이션 함수를 사용한다고 가정 해 봅시다. 모든 액티베이션 0보다 크거나 같을 것입니다. 인풋 X에 가능한 예외사항이 있을 수 있습니다 밸류 액티베이션 아웃풋 숫자들은 0 또는 양수이기 때문입니다. 자, [l + 2]가 무엇으로 될지 살펴 봅시다. 이전 강의에서 나왔던 expression을 똑같이 써보면, a[l + 2] = g(z [l + 2] + a [l]) 이 됩니다 a [l]을 덧셈하는 것은 우리가 추가했던 skip connection의 short circle에서 나온 것입니다 이걸 확장시키면, 이것은 g(w[l + 2]a[l + 1] + b[l + 2] 이고 따라서, 여기 z[l + 2]는 이것과 같고, 여기에 a[1]를 더해줍니다. 만약 여러분이 K의 방법으로서 L2 일반화를 사용하고 있다면 그것은 w[l + 2]값이 줄어드는 경향이 있을 것이다, K에서 B로 적용하고 있다면, 이것 또한 이것을 줄어들게 할 것입니다. 실제로 여러분이 K에서 B로 적용을 해보기도 하고 하지 않기도 할 테지만, W는 정말로 우리가 주목해야 할 핵심포인트 입니다. w[l + 2] =0 이고, 토론을 위해, B[ㅣ + 2]=0 라고 가정해봅시다. 그러면 이것들은 0이 되기 때문에, 이 부분은 없어질 것입니다. g(a[l])은 a[l]과 같아질 것입니다. 왜냐하면 밸류 액티베이션 함수를 사용하고 있다고 가정했기 때문이죠. 모든 액티베이션 이 음수이면, g(a[l]) 은 음의 아닌 양에 적용된 값이 됩니다. 따라서 다시 a[l]을 얻게 됩니다. 이는 레시듀얼 블럭이 항등 함수를 배우는 것은 쉽고 여기 이 skip connection이 있으므로 a[l + 2]= a[l] 임을 얻어내는 것은 쉽다는 걸 보여줍니다. 이는 이 두 추가적인 레이어가 없는 더 단순한 네트워크에서든 신경망에서 있는 이 두 레이어를 더하던 지간에 이것이 신경망의 능력을 저하시키지 않는다는 것을 의미합니다. 왜냐하면 항등 함수를 학습하는 것을 매우 쉽기 때문입니다. 여기 이 두 레이어를 더해서 a[l] 를 a[l + 2]으로 똑같이 복사해주만 하면 되니까요. 이게 바로 두 개의 추가 레이어를 더하고 이 잔류블럭을 여기 큰 신경망의 중간이나 끝 어딘가에 더해도 수행능력은 저해되지 않는 이유인 것입니다. 하지만 물론 우리의 목표는 수행능력을 떨어뜨리지 않는 게 아니고 수행능력을 도와주는 것입니다. 그러면 이 모든 히든 유닛이, 유용한 것을 학습한다면, 여러분이 항등 함수를 학습하는 것 보다 훨씬 더 잘 할 수 있을 것입니다. skip connection의 잔류가 없는 매우 심한 심층 신경망의 평범한 망에서 오류가 날 수 있는 건 네트워크를 점점 깊게 만들면, 이 네트워크가 항등 함수를 학습하는 파라미터를 선택하게 하기란 사실 매우 어려운 일입니다. 그리고 많은 레이어들이 결국 결과를 더 좋게 하기보다 더 악화시키는 이유이기도 합니다. 레지듀얼 네트워크가 작동하는 주된 이유는 이 추가적인 레이어들이 항등 함수를 학습하는 게 너무 쉽기 때문에, 수행이 저해되는 일이 없고 심지어 더 잘 수행할 것임을 보장받을 수 있기 때문입니다. 적어도 수행능력을 저해하지 않는다는 기준치에서 벗어나는 것은 더 쉽고, 그러면 좋은 수행치는 해상도를 개선할 수 있을 겁니다. 이 편집 본을 통해 논의할 가치가 있는 잔류 네트워크의 또 하나의 세부사항은 우리가 z[l + 2] 와 a[l] 가 같은 차원을 가질 거라고 가정하는 것입니다. ResNet에서 봐야 할 것은 같은 컨볼루션을 많이 사용해서 이 다이멘션이 이쪽 레이어나 아웃풋 레이어의 다이멘션과 같아지도록 하는 겁니다. 또한 우리가 이 short circle connection 할 수 있도록 해주기도 하죠. 같은 컨볼루션은 다이멘션을 그래도 유지해주고, 여러분이 이 short circle을 수행하거나 두 동일한 다이멘션 벡터의 덧셈을 하는 것을 더욱 쉽게 만들어줍니다. 인풋과 아웃풋이 다른 다이멘션 을 가지는 경우를 예시로 들면, 만약 이게 128 차원이고, Z 니까, 이 a[l]이 256 차원이라면, 추가 매트릭스 즉 Ws를 여기에 더해주십시오 이 예시의 Ws는 256 x 128 차원의 매트릭스가 될 것입니다. 따라서, Ws x a[l] = 256 차원이 되고 이 덧셈은 이제 2개의 256 다이멘션의 벡터 사이에 있는 것입니다. Ws로 할 수 있는 게 몇 가지 있는데요, 우리가 배웠던 파라미터 매트릭스일 수도 있고 고정된 매트릭스일 수도 있습니다. 이 고정되 매트릭스는 a[l]을 취하는 제로 패딩을 실행시킬 수도 있고 256 차원이 되도록 0을 추가할 수도 있습니다. 혹은 이 두 가지 버전 중 그 어느 것이라도 작동할 것입니다. 자 마지막으로, 이미지 상의 ResNet을 살펴봅시다. 이것들은 Harlow의 논문에서 가져온 이미지들입니다. 평범한 망의 예시로서, 이미지를 인풋하고 softmax 아웃풋을 얻을 때까지 많은 컨볼레이어를 쭉 가지는 것입니다. 이걸 ResNet으로 전환시키려면, 이 추가 skip connection을 더하십시오. 몇 가지 세부사항을 언급하려고 합니다. 여기에 3 x 3 컨볼루션이 많이 있습니다. 그리고 이 대부분은 똑같은 3 x 3 컨볼루션이죠. 따라서 여러분을 동일한 차원의 피쳐 벡터를 더하고 있는 것입니다. 따라서 완전 연결 레이어보다, 이것들이 진짜 컨볼루션 레이어들입니다. 하지만 차원들이 유지되고 있는 동일한 컨볼루션이기 때문에, z[l + 2] + a[l] 에서 이 덧셈이 이치에 맞게 되는 겁니다. 앞서 NetRes에서 보았던 것과 유사하게, 상당한 컨볼루션 레이어가 있고, 간헐적으로 pooling layer들도 보입니다, 혹은 pooling 같아 보이는 것들도 있죠. 이런 것들이 생겨날 때마다, 이전 슬라이드에서 보았던 차원을 조정해야 할 필요가 있습니다. Ws 매트릭스를 할 수 있고, 이 네트워크에 보이는 대로 컨볼, 컨볼, 컨볼, pool, 컨볼, 컨볼, 컨볼, pool 이런 식으로 되어있습니다.<unknown> 그리고 마지막에는 softmax를 이용하여 예측을 하는 완전 연결 레이어가 있습니다. 자, 여기까지가 ResNet 에 대한 것입니다. 신경망을 사용하는 것에는 아주 흥미로운 아이디어가 있습니다. 바로 1 x 1 필터, 1 x 1 컨볼루션입니다. 따라서, 1 x 1 컨볼루션을 사용할 수 있게 될 것입니다. 다음 강의를 한 번 보시죠.