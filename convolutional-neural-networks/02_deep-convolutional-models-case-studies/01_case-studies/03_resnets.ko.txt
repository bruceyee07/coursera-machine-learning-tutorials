아주 아주 깊은 심층 신경망 학습시키기 어렵습니다. 왜냐하면 Vanishing Gradient 와 Exploding Gradient 유형의 문제들 때문입니다. 이 영상에서, 여러분은 하나의 레이어로부터 액티베이션을 취하게 해 줄 뿐만 아니라 이것을 신경망 안에 있는 훨씬 깊은 또 다른 래이어에 제공하는 skip 레이어에 대해 배워보겠습니다. 이것을 사용하면, 아주 아주 깊이 심층 신경망을 학습시키도록 해주는 ResNet을 만들게 될 것입니다. 때로는 심지어 100 개가 넘는 레이어로 된 네트워크도 있습니다. 한 번 보시죠. ResNet는 residual block (잔류블럭)이라고 불리는 것으로 만들어졌습니다. 먼저 그 블럭이 무엇인지 설명해 보겠습니다. 신경망 레이어가 두 개 있습니다. 레이어 액티베이션 a[l] 에서 시작하시면 됩니다. 그리고 a[l+1], 그리고 나서 비활성화된 두 개 레이어 뒤에는 a[l+2]가 됩니다. 이 계산 단계를 통해 a[1]을 얻은 다음 이 linear operator를 적용하시면, 이것은 이 방정식에 의해 좌우됩니다. a[l] 으로부터 시작해서 z[l+1] 과. 가중치 매트릭스를 곱하고, 바이어스 벡터 값을 더해주면 됩니다. 그리고 나서, a[l+1]를 얻으려면 ReLU 비선형성을 적용하십시오. 그러면 이것은 a[l+1] = g(z[l+1]) 방정식에 의해 계산될 것입니다. 그런 다음, 다음 레이어에서 이 선형 단계를 다시 수행하시면, 이 방정식에 의해 제어될 것입니다. 이건 바로 왼쪽에 있는 이 방정식하고 많이 비슷하죠 그리고 마지막으로, 이 방정식으로 산출되는 또 다른 ReLU operation을 적용하시면 여기 G는 ReLU 비선형성이 될 것입니다. 이렇게 하면 a[l+2] 값을 구하게 되죠. 다시 말해서, 자 다시 말해 [l]에서 [l + 2]로 이동하기 위해서는 이 모든 단계를 거쳐야 하는데요, 이걸 레이어 세트의 주요 경로라고 부르도록 하죠. 잔류블럭에서, 이걸 바꿔봅시다, a[l]을 가져다가 우선 앞으로 쭉 빼서, 복사하시고, 바로 여기 신경망에 매치시켜 줍니다. 그러면 이 a[l]이 위치하게 됩니다. 비선형성, 즉, ReLU 비선형성에 적용시키기 전에 말이죠. 이걸 우린 shortcut 이라고 부릅시다. 주 통로를 따라갈 필요 없이 a[1]의 정보는 이제 신경망로 훨씬 더 깊숙이 들어가는 지름길을 따라갈 수 있습니다. 이 마지막 방정식은 없애버리고, 대신에 아웃풋 a[l + 2] 는 ReLU 비선형성 g(z[l + 2] 라고 앞에 했던 것이랑 똑같이 쓰고 이제는 a [l] 을 더하면 됩니다. 따라서, 여기서 이 a[l]을 덧셈하면 이게 잔여 블럭을 만들게 되는 거죠. 이 그림에서, shortcut 이 이쪽에 가도록 그려서 이 사진의 위 부분을 바꿀 수 있습니다. 그리고 shortcut 실제로 ReLU 비선형성 앞쪽에 추가되기 때문에 이 두 번째 레이어에 그리겠습니다. 이렇게 하면 여기 있는 이 각각의 노드들에 선형함수와 ReLU 적용됩니다. 따라서 a[l]은 선형 부분이면서, ReLU 부분 앞쪽인 곳에 들어가게 됩니다. 때로는 shortcut이라는 용어 대신, skip connection 이라는 용어를 듣게 될 텐데요, 이것은 신경망 속으로 더 깊숙이 들어가 정보를 처리하기 위해 a[l]이 레이어를 하나씩 혹은 두 개씩 건너뛰는 것을 가리키는 것입니다. ResNet을 발명한 Kaiming He 와 Xiangyu Zhang, Shaoqing Ren, Jian Sun 그리고 Shaoqing Ren 과 Jian Sun. 이들이 찾아낸 것은 잔류블럭을 사용해서 더 깊게 신경망을 훈련할 수 있게 해 준 것입니다. ResNet을 구축하는 방법은 이러한 잔여 블럭을 많이 사용하여 심층 신경망을 형성하기 위해 이렇게 겹쳐 쌓아 올리는 것입니다. 이 네트워크를 보시죠. 이것은 잔류블럭이 아니라 plain network (평범한 망)라고 부릅니다. 이는 ResNet 논문 용어입니다. 이것을 ResNet으로 바꾸려면 이런 식으로 연결이 짧긴 하지만, 모든 skip connection을 추가해야 합니다. 따라서 레이어를 하나씩 건너뛰면 이전 슬라이드에 보셨듯이, 각각을 잔류블럭으로 바꾸기 위해서는 추가적인 변경을 하게 됩니다. 따라서 이 그림은 5개의 잔류블럭이 같이 쌓여있는 것이고, 이는 곧 잔류블럭이 됩니다. 그리고 만약 여러분이 기울기 강하 혹은 더 괜찮은 적합 알고리즘 등의 표준 적합 알고리즘을 사용해서 평범한 망을 학습시키기 고자 한다면, 모든 부가적인 레지듀얼이나, 방금 그린 모든 부가적인 shortcut 즉 skip connection 없이도, 경험에 기인하여, 레이어 개수가 늘어남에 따라 훈련 오류는 시간이 지나면서 줄어드는 걸 볼 수 있을 겁니다. 하지만 그리고 나서 다시 올라가는 경향을 보입니다. 이론적으로 신경망을 더 깊게 만들면 트레이닝 세트에서 더 잘 수행해야 합니다 그렇습니다. 따라서 이론적으로 더 깊은 네트워크를 갖는 이론은 도움이 됩니다 그러나 사실상 현실적으로 평범한 망을 사용하고 ResNet은 없기 때문에 깊게 평범한 망을 사용하는 것 여러분의 최적화 알고리즘이 훨씬 혹독하게 학습하고 있다는 것을 의미합니다. 현실적으로 너무 깊은 네트워크를 고르면 학습 오류가 점점 더 심해지지만 ResNet의 경우, 레이어 숫자가 많아진다 하더라도 학습 수행 오류를 저하시킬 수 있습니다. 우리가 100 개가 넘는 레이어로 네트워크를 훈련하더라도. 지금은 실제로 많은 부분을 사용했는지는 모르겠지만 레이어가 천 개가 넘는 네트워크로 실험하는 사람들이 있습니다. 이런 액티베이션을 취함으로써 중간에 있는 액티베이션의 X가 신경망에 더 깊이 들어갈 수 있게 해줍니다. 이는 사라지거나 폭발적으로 증가하는 gradient 문제점들을 처리하도록 도와주고, 수행 시 눈에 띄는 손실 없이 신경망을 훨씬 깊게 훈련하도록 해줍니다. 어느 시점에 이건 안정화되어, 수평 하게 되겠죠. 그리고 이건 더 깊은 네트워크에는 그리 도움이 되지 않을 겁니다. 하지만 ResNet은 심층 신경망 깊이 훈련하도록 도와주는 데에는 그리 효과적이지는 않습니다. 이제 ResNets의 작동 방식을 개괄적으로 살펴 보았습니다. 그리고 사실, 이번 주 프로그래밍 연습에서는 이러한 아이디어를 구현하고, 여러분을 위해 작동하는 것을 보게 될 것입니다. 지만 다음 강의에서는, ResNet이 왜 그렇게 잘 작동하는지에 대해서 훨씬 나은 관점들을 공유하고 싶습니다. 다음 강의로 가보시죠.