太深的神经网络训练起来很难，因为有 梯度消失和爆炸这类问题。 在这个视频中，你将会学到 跳跃连接（skip connection），<br />它能让你从一层中得到激活 并突然把它递给下一层，<br />甚至更深的神经网络层 利用它，你就可以训练<br />网络层很深很深的残差网络(ResNet)。 有时甚至可以超过100层的网络。<br />让我们看看吧。 残差网络（ResNet）<br />是使用了残差结构的网络。 我们先来看看那是什么。 这里有两层神经网络，开始于 a[l]代表第l层中的激活函数 然后到了a[l+1]，两层后是a[l+2] 所以在这个计算步骤里，你先有一个a[l] 之后你做的第一件事是<br />将这个线性运算应用在它上面 是由这个等式得到的。 所以你用a[l]来计算z[l+1] 通过乘一个加权矩阵，并加上一个偏置（Bias）向量。 之后，你应用非线性ReLU来得到a[l+1]。 是通过这个等式，a[l+1] = g(z[l+1])。 然后 在下一层 再次应用这个线性步骤, 所以是由这个方程式。 这与我们在左边看到的等式很相似。 最后，你再用一次ReLU变换，这是 现在 是由这个等式得到的，<br />G在这里是非线性ReLU。 你可以得到一个a[l+2] 换言之 从a[l]流向a[l+2]的信息， 它需要经过所有这些步骤，我会把这称作 这组层的主路径。 在残差网络（ResNet）中, 我们要做个改变 我们要把a[l], 然后把它往前提，复制 提到神经网络很往后的位置， 然后把它加在这。 最后应用一些非线性处理，<br />比如线性整流函数(ReLU) 我将把这叫做快捷路径。 所以，无需遵循主路径, a[l]中的信息现在可以遵循 快捷路径进入到更深层的神经网络中。 这意味着最后一个等式 消失了，取而代之，我们有了 一个a[l+2]作为ReLU非线性g，<br />和以前一样，应用到z[l+2]上 但现在加上a[l]。 所以，在这里添加一个a[l], 使这成为了一个残差块（Residual Block）。 在这幅图中，你还可以在这个图片上修改 把这个快捷路径画到这。 我们打算画成 它进入到第二层的这里 是因为实际上 快捷路径是加在ReLU非线性之前的。 所以这里的每个节点, 都应用一个线性方程和一个ReLU。 所以a[l]插入于线性部分之后，<br />ReLU部分之前。 有时这个术语不叫做快捷路径（shortcut）， 你也会听到"跳跃连接"（skip connection）这个词 这是指a[l]跳过一层或者跳过 几乎两层把信息传递到更深的神经网络中去。 ResNet的发明者 是Kaiming He，Xiangyu Zhang， Shaoqing Ren，和Jian Sun。 他们发现使用残差块 让你可以训练更深层的神经网络。 而你建立一个ResNet的方法<br />就是是通过大量的这些残差块 例如这个，然后把他们堆叠起来，<br />形成一个深层网络。 那么，让我们看看这个网络。 这个还不是残差网络 这个被称为普通（plain）网络。 这是ResNet论文的术语。 要把这个变成ResNet 你做的是添加所有这些 跳跃连接（或称为快捷路径连接） 所以每两层结束于 额外的改变，如同我们 在之前幻灯片中看到的 将这些每个都变成残差块 这张图片显示了5个残差块堆积在一起, 这就是一个残差网络。 事实证明，如果你使用 标准的优化算法，如 梯度下降法，或者 以下很高级的优化算法之一，来训练普通网络 没有这些额外的残差 没有所有额外的快速路径或跳跃连接，<br />比如我刚刚画的那些。 从经验上来说，你会发现当你增加层数时, 训练误差会在下降一段时间后， 但它们又会回升上去。 在理论上，当你使神经网络更深, 它在训练数据上的性能应该只会更好 是的。这个理论，理论上来讲 一个更深层次的网络只会有帮助。 但在实践中，或在现实中, 有一个普通网络，没有ResNet, 有一个很深的纯网络意味着 你的优化算法训练起来会更困难。 所以，在现实中, 如果你选择的网络太深，则训练误差会更糟。 但有了ResNet的情况是，即使层数越来越深, 你仍可以让训练误差继续下降， 即使我们训练一个超过100层的网络。 然后现在有些人甚至拿超过1000层的网络做实验。 尽管我很少看到有人在实际应用中用到那么多层的网络 但把这些激活X 或者这些中间层的激活输出，连接到更后面的层去 这确实对解决梯度消失和爆炸问题非常有帮助 使得我们可以训练 深得多的神经元网络而不会看到性能倒退的现象 尽管可能在某一个点会达到平原阶段 这时候就算再加层也不会有帮助 但是Resnet确实对训练很深的网络有很大的帮助 所以你现在已经了解了 ResNets 是如何工作的。 在本周的编程练习中, 你会自己去实现这个结构，<br />可以自己看看它到底怎样有效 下一节我将与你分享一些如何建立 为什么Resnet那么有效的原因 我们下一节再见