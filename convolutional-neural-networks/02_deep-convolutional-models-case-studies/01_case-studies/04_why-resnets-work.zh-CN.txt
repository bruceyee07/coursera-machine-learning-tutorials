那么为什么ResNets这么好用呢？ 让我们通过一个例子来说明为什么ResNets工作得这么好, 至少在这个意义上,你可以让他们越来越深入而又没有真正地 损害你最起码能让他们在训练集上好好工作的能力 你应该已经从这个课程系列的第三门课理解到， 在训练集上的出色表现通常来是 你出色地控制你在训练集上的深度的前提条件 所以训练ResNet使其能对付 训练集是第一步。我们来看一个例子， 从上一集视频中我们看到，如果你设计了更深层的网络， 它会使得你用训练集训练神经网络的能力下降， 这也是为什么有时你不希望有太深层的神经网络。 但当你训练ResNet的时候就不一样了， 我们来看个例子 如果你有X作为输入， 输入到某个大型神经网络，并输出a[l]， 如果在这里你要调整 神经网络使其层数更深一点， 相同的Big NN， 然后输出a[l]， 我们将要再额外增加几层 先加一层，然后再一层， 然后输出a[l+2]， 我们把这个做成 ResNet块， 一个有着快捷路径的残差块。 为了证明我们的观点， 在这整个神经网络中我们用Relu激活函数， 那么所有激活 (a) 都将会大于等于0， 除了某些特定的输入X。 因为激活输出的数只会是0或者正数。 现在我们来看a[l+2]会是什么。 从之前的视频复制相同的表达式， a[l+2]将会是由z[l+2]得来， 再加上a[l]，这里加入的a[l] 来自于我们之前添加的跳跃连接。 如果我们将其展开， 就等于 g(w[l+2]*a[l+1]+b[l+2])。 这也就是z[l+2]，再加上a[l]。 这里要注意一下，如果你这里用远离K的L2正则化(regularisation) 这将会减小 w[l+2]的值。 如果你应用正则化于b的话也会导致b值减小， 尽管我觉得在现实中有时候并不总是需要应用于b， 但在这里w才是最需要关注的项。 如果w[l+2]等于0， 同时我们假设b[l+2]也等于0 那么这里所有的项都会消失因为它们都等于0， 那么g(a[l])， 就等于a[l]，因为我们假设我们使用激活函数的值。 所有激活都不是负数， 那么g(a[l])会是对于非负数， 所以你就重新得到了a[l]。 这意味着残差块比较容易学习恒等函数。 由于这个跳跃连接也很容易得到a[l+2]等于a[l]。 这意味着将这两层加入到你的神经网络， 与上面这个没有这两层的网络相比 并不会非常影响神经网络的能力， 因为对于它来说学习恒等函数非常容易， 只需要复制a[l]到a[l+2]，即便它中间有额外两层。 所以这就是为什么添加这额外的两层， 这个残差块到大型神经网络 中间或者尾部并不会影响神经网络的表现。 当然我们的目标并不只是维持原有的表现， 而是帮助获得更好的表现，你可以想象当 这里所有项都学习到有意义的东西的话， 可能要比学习恒等函数更好。 而有极多层的神经网络里比较深层的平织网在没有 这些残茶块代表的跳跃连接情况下，容易出错的地方是 当你的神经网络越来越深层， 它就很难选择参数来学习， 即便是恒等函数也很难，这也是为什么 有很多层数会使你的结果更糟而不是更好。 我认为残差块网络有效的主要原因是 这些额外层学习起恒等函数非常简单， 你几乎总能保证它不会影响总体的表现， 甚至许多时候幸运的话可以提升网络的表现。 至少有个合理的底线， 不会影响其表现，然后梯度下降从这里开始可以改善最终的结果。 残差网络的另一个值得讨论的细节是 在这一项中， 我们是在假定z[l+2]和a[l]是在同一维度的。 所以你将会看到再ResNet中许多相同卷积的应用， 因此这里的维度 和这里，或者是输出层的维度相同。 所以我们可以做这个快捷圈的连接， 因为相同的卷积会保留维度， 也使你运算这个快捷圈 变得更方便，再运算这两个相同维度矢量的和。 如果输入和输出有不同的维度，比如这个例子， 如果这里是128维， 那么a[l+2]是256维。 你需要做的是增加一个额外的矩阵，称它为Ws， 那么这里Ws将会是一个256*128尺寸的矩阵。 使得Ws乘以a[l]变为256维， 这时这边的加法运算 的矢量尺寸都为256维了，与此同时对于Ws你可以做一些其他的事情， 它可以是一个包含已学习到的参数的矩阵， 它可以是一个固定的矩阵， 其中包含许多0的项，即除了a[l]以外 由0来填满256维。这两种办法我觉得都会有效。 最后，让我们来看看ResNet在图像上的应用。 这些是我从Harlow文献中拿来的图像。 这是一个平铺网络的例子，在其中你输入一张图像， 然后有一系列卷基层， 直到最终你有一个softmax输出。 要把这个变成ResNet， 你需要添加那些额外的跳跃连接， 我只提及一小些细节， 这里有许多3＊3卷积，其中大部分 都是相同的3＊3卷积， 这也是为什么你需要做相同维度矢量的相加。 所以与其说是相互充分连接的层， 还不如说这些是卷基层，但由于它们是相同的卷基层， 这里的维度相同，因此z[l+2]+a[l]相加的算数得以成立。 和你之前见过的许多ResNet相同的是， 你有一堆卷基层，然后 偶尔有池化层或者类似池化的层。 每当这些事情发生时， 你可以用我们之前提到的方法调整维度， 你可以用矩阵Ws， 然后在这些网络中常见的是， 你有<unknown>个池 然后在最后你会有 一个充分连接的层来最终用softmax来做出预测。 那么这就是ResNet了。 接下来，会是一个非常有趣的概念， 其中用到有1＊1过滤，1＊1卷积 的神经网络。 所以什么是1＊1卷积？如何使用它？ 我们会在下一个视频中提到。