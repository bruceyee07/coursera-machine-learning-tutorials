为卷积网络设计某一层时，您可能需要选择， 你想要一个1x3的滤波器（又译做卷积核） 或者3x3，或5x5 或者你想要一个池化层？ inception网络是说 为什么不全用呢？ 这使得网络架构更加复杂， 但它的效果也变得更好。 让我们来看几个例子 举例来说，您已经输入了一个 28x28x192的体积 所以inception网络或inception层是指 与其在卷积神经网络中选择一个你想使用的卷积核尺寸 乃至选择你是否需要一个卷积层还是一个池化层 让我们来全部都做吧。那么如果你可以使用1*1卷积核 则会输出28*28的一个结果 我们称之为28*28*64大小的输出 也就是如图所示的这个量 但是也许你也想尝试一个3*3大小的（卷积核），然后结果就是20*20*128 然后你要做的是，把第二个量并列的堆叠在第一个量旁边 并且保证维度是匹配的 让我们使用same填充（same padding） 所以输出的维度仍然是28*28 输出是与输入的高度和宽度相同的 在本例中的是28*28*128 你也可能会想，我还想要同时使用一些其他的技巧来“对冲”一下 也许5*5的卷积核效果更好 所以我们也那样做了，进而得到输出结果为28*28*32维 并且再使用same填充让维度变得匹配 也许你不希望使用卷基层 那么我们使用池化层，这会导致其他的输出，再把他们堆在一块儿 在这里池化的输出为28*28*32 为了使所有的维度匹配 事实上，你需要为最大池化操作使用“填充” 所以这是一种不一般的池化<br />因为如果你想 输入图像和输出的大于都是28*28 你要调整所有其他的维度也成为28*28 然后你需要使用same填充，并且池化操作中的滑动步幅为1 这一个细节也许现在看起来比较可笑 但是让我们先继续讲下去 最后我们会让所有部分正常工作起来 但是对于像这样一个inception模块 你可以输入一些量并且获得一输出 在这种情况下，我想，如果你把所有这些数字加起来 32 + 32 + 128 + 64 等于256 所以你将有一个inception模块，其输入28*28*129 <br />（这应该是口误，根据幻灯片，应该是192） 输出为28*28*256 这是inception网络的核心 并且这是Christian Szegedy, Wei Liu Yangqing Jia, Pierre Sermanet Scott, Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Canhoucke和Andrew Rabinovich做出的贡献 并且这个网络最基础的一个特点就是 你不用去只挑选一个卷积核的大小或者是pooling 你可以所有可能都做 然后把所有的输出结果都连接起来 然后让神经网络去学习它想要用到的参数 以及它想要用到的卷积核大小 但是这里会出现一个关于我们现在说的 inception网络的问题： 计算成本问题 在下一个幻灯片 我们来看一下使用这个5x5的卷积核导致的 计算成本是多少 我们只关注5x5的卷积核 我们现在有一个28x28x192大小的输入 然后你使用32个5x5的卷积核 使用same填充 然后输出一个28x28x32的张量 在之前的那一个幻灯片 我把它画成了紫色并且很窄 而我在这里就将它画成一个普通的立方体 所以，让我们现在来看一下获得这个28x28x32输出的计算成本 你有32个卷积核，因为你有32个通道 然后每一个卷积核大小将是5x5x192 那么输出张量的大小将是28x28x32 所以你需要算28x28x32这么多的数 所以每一个你都需要做这么多乘法： 5x5x192 所以你做乘法的总数 就是你算每一个输出所需的乘法次数 乘以你的输出数 你乘完这一串数字以后 结果等于1.2亿 虽然你可以在现代的计算机里做1.2亿次乘法运算 但是这个计算成本还是相当大的 在下一个幻灯片里，你将会看到， 通过使用你之前学过的 1x1卷积运算，这能够降低计算成本到大约1/10左右 从1.2亿次乘法运算到它的1/10 请记住1.2亿这个数字 以便和下一张幻灯片中的数字进行比较 接下来我们就讲另一种结构 它同样接受28x28x192的输入并且输出大小为28x28x32。 你仍将输入28x28x192的向量 但紧接着用1x1的卷积运算将通道的个数从192减小为16 然后我们用这个相对较小的量 去做5x5的卷积运算并得出最终结果 注意第一种方法和第二种方法的输入和输出的维度都是相同的 两种方法中，你都是输入28x28x192，然后输出28x28x32 第二种和第一种一样 但是第二种方法中，我们先将这个较大的输入 减小成一个较小的中间值 也就是这个只有16个通道而不是192个通道 有时，我们会把中间的这个层叫做瓶颈层 我猜啊，这是因为瓶颈是东西中最细的那个部分 所以我想如果你有一个像这样的玻璃瓶 然后这个是瓶塞... 那么瓶颈就是这个细细的部分 所以，一样的道理，瓶颈层也是这个网络中最小的部分 在这里，我们将输入在变大前缩小 现在，我们再来看一下这次的计算成本 我们有16个卷积核 去调用这个1x1的卷积运算 每一个卷积核将会是1x1x192这么大 这里的192对应输入层里的192 那么生成这个28x28x16瓶颈层的计算成本 将会是.... 你需要这么多输出，对吧 然后每一个你都需要做192次乘法 我能写成1x1x192，对吧 如果你把这些数乘起来 那么结果是240万 大约是240万哈 那么第二个运算的计算成本大概多大呢？ 这个是第一个卷积层的计算成本 那么第二个运算的成本.....额 你有这么多输出： 28x28x32 然后每一个输出你都需要调用一个5x5x16的卷积核 然后5x5x16.... 然后相乘.....结果是1000万 所以总的计算成本大概是240万加上1千万 1240万次乘法 然后你和我们第一种方法的计算成本比较 就会发现，运算成本从1.2亿次运算 减小到了大概1240万次运算 大概是第一个方法的1/10 然后在第二种方法中，你相加的次数 和你相乘的次数差不多 所以这就是为什么我只计算了相乘的次数 所以，我们总结一下：如果你在建立一个卷积网络层 并且你不想去决定 到底是用1x1 3x3还是5x5的卷积核或者是否使用的pooling 那么就用这个inception模型，它会做所有的工作 并会将所有的结果都连接起来 然后我们讨论了计算成本的问题 在这堂课里你看到了1x1卷积运算 是如何生成了一个瓶颈层 并且显著降低计算成本的 你现在可能会有疑问 你可能会问：如此剧烈地缩小特征表示的大小 会不会影响神经网路的性能 答案就是：如果你只要合理地去实现这个瓶颈层 你既可以缩小输入张量的维度 又不会影响到整体的性能 还能给你节省计算成本 所以这些就是inception模型的一些关键点 让我们将这些放在一起 然后在下一个视频中你会看到一个完整的inception网络是怎样的