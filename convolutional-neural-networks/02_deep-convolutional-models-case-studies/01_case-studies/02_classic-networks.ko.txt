이번 강의에서는 LeNet-5, AlexNet, VGGNet으로 시작하는 고전 신경망 아키텍처에 대해 배우겠습니다. 한 번 보시죠. 이것이 LeNet-5 아키텍처입니다. 32 x 32 x 1 의 이미지로 시작해보겠습니다. LeNet-5의 목표는 손으로 쓴 숫자를 인식하는 것이었습니다. 그런 숫자의 이미지 일 것입니다. 그리고 LeNet-5는 그레이 스케일 이미지에 대해 학습되었기 때문에 이것은 32 x 32 x 1입니다 이 신경망 아키텍처는 실제로 지난 주에 본 마지막 예제와 매우 유사합니다. 첫 번째 단계에서는. 먼저 스트라이드 1인여섯 개의 5 x 5 필터를 사용합니다 여섯 개의 필터를 사용합니다. 여기는 곧 20 x 20 x 6이됩니다. 그리고 패딩이 없고, 스트라이드가 1인 이미지 크기는 32 x 32로 28 x 28로 줄어 듭니다. 그런 다음 LeNet 신경망이 pooling을 적용합니다. 그리고 나서 이 논문이 작성됐던 그 때에는 사람들은 평균 pooling을 훨씬 더 많이 사용했었습니다. 하지만, 현대적인 변형을 만드는 경우 아마 max pooling을 대신 사용하게 될 것입니다. 하지만 이 예제에서는 필터 폭 2와 스트라이드 2를 사용하여 풀의 평균을 구하고 크기, 높이 및 너비를 2 배 줄이면 이제 14 x 14 x 6볼륨을 얻게 됩니다. 이 볼륨의 높이와 너비가 완전히 일정한 비례대로 그려지지는 않았을 것입니다. 기술적으로, 이 비례대로 볼륨을 그리면 크기와 높이가 두 배 더 강해집니다. 그런 다음 다른 컨볼루션 레이서를 적용합니다. 이번에는 5 x 5, 16 개의 필터 세트를 사용하므로 다음 볼륨으로 16개의 채널을 가지게 됩니다. 그리고 이 논문 가 1998 년에 작성되었을 때 사람들은 패딩이나 유효한 컨볼루션을 사용하지 않았었습니다. 이는 컨볼루션 레이어를 적용 할 때마다 장점이 많아졌기 때문입니다. 이게 바로 여기처럼 14 x 14 에서 10 x 10 으로 바뀌는 이유입니다. 그런 다음 다른 pooling 레이어를 사용하여 높이와 너비를 2 배로 줄이면 여기서 5 x 5 가 되겠죠. 그리고 이 모든 숫자 이렇게 5 x 5 x 16 곱하면 이것은 400 됩니다. 25의 16 배는 400 이니까요. 그리고 나서 다음 레이어는 400 개의 각 노드와 120 개의 뉴런 모두를 연결하는 완전 연결 레이어이다. 그리고 완전 연결 레이어가 있습니다. 때로는 400 개의 노드가 있는 레이어만을 끌어오는데요, 그건 생각하도록 하겠습니다. 완전 연결 레이어가 있고, 또 다른 완전 연결 레이어가 있습니다. 그리고 나서 마지막 단계는 본질적으로 84 개의 피쳐를 사용하고 그걸 최종아웃풋으로 사용하는 것입니다. ŷ에 대한 예측을 하기 위해 여기에 노드를 하나 더 그려도 됩니다. 0에서 9까지의 숫자를 인식 할 때 가능한 10 가지 값을 ŷ이 취했습니다 이 신경망의 최신 버전에서는 10 방향 분류 아웃풋이 있는 softmax 레이어를 사용합니다. 그 당시에도, LeNet-5는 사실 아웃풋 레이어에서 다른 분류기를 사용했습니다. 이는 현재 쓸모 없는 것입니다. 그래서 이 신경망은 최신 표준에서는 작았고 약 60,000 개의 매개 변수를 가지고 있었습니다. 오늘날에는 1 천만에서 1 억 개의 파라미터를 가진 신경망을 자주 보게 됩니다. 문자 그대로 이 네트워크보다 약 1,000 배 큰 네트워크를 보는 것은 드문 일이 아닙니다. 그러나 여러분이 보게 되는 것은 네트워크에서 더 깊숙이 왼쪽에서 오른쪽으로 갈수록 높이와 폭이 줄어드는 경향이 있다는 것입니다. 채널 수는 늘어나는 반면, 이렇게 32 x 32에서 28 x 28, 14, 10에서 5로 이렇게 변화된 것이죠 네트워크 레이어로 깊숙이 들어감에 따라, 1에서 6에서 16으로 갑니다. 이 신경망에서 여전히 반복되고 볼 수 있는 또 다른 패턴이 있는데요, 이 신경망은 하나 이상의 컨볼 레이어와 pooling 레이어 있습니다. 그리고 나서 하나 또는 때로는 하나 이상의 컨볼 레이어, pooling 레이어와 열어 완전 연결 레이어 및 아웃풋을 차례로 이어집니다. 따라서 이 유형의 배열은 꽤 일반적입니다. 이제 마침내 이것은 논문을 읽어보려고 하는 사람들을 위한 것 같습니다. 다른 몇 가지가 있습니다. 이 슬라이드의 나머지 부분에서는 이 정통 논문을 읽고자 하는 사람들만을 위해서 좀 더 깊이 있는 내용을 이야기 하려고 합니다. 제가 빨간색으로 쓰는 모든 것은 슬라이드에서 건너뛰셔도 무방합니다. 완전히 이해하지 않아도 괜찮지만 그래도 조금 흥미로운 역사적인 각주도 있을 겁니다. 그 당시의 원본 논문을 읽어보시면 사람들은 Sigmoid(시그모이드)와 Tanh non-linearities(탄비선형성)을 사용했고, 당시 사람들은 가치 비선형성을 사용하지 않고 있었습니다 그래서 논문을 보면, Sigmoid와 Tanh가 언급 된 것을 볼 수 있습니다 그리고 이 네트워크에 대한 재미있는 것들이 있는데요, 현대 표준방식에 의한 유선 방식이라는 점입니다. 예를 들어, nc 채널이 있는 nh x nc x nc 네트워크가 있고, 여러분이 같은 채널이 있는 f x f x nc 차원의 필터를 사용한다면, 채널은 전부 이 채널을 사용하고 있는 것으로 보이죠. 하지만 당시 컴퓨터는 훨씬 느렸습니다. 그래서 일부 파라미터 뿐만 아니라 계산에서도 저장하기 위해 본래의 LeNet-5는 매우 복잡한 방법을 가지고 있습니다. 다른 필터들이 인풋블럭의 다른 채널들을 보도록 하는 것이었죠. 그리고 이 논문은 그 세부 사항에 관해 이야기하지만, 요즘 현대적 실행 법에서는 그런 유형의 복잡성을 가지지 않습니다. 그 당시에는 했지만 요즘은 그다지 하고 있지 않는 한 가지는 원래의 LeNet-5가 pooling을 한 후에 비선형성을 갖지 않는다는 것입니다. 실제로 이것은 pooling 레이어 이후에 Sigmoid 비선형성을 사용한다고 생각합니다. 그래서 여러분이 이 논문을 읽어 보시면 아시겠지만, 다음 후속 강의들에서 다룰 논문보다도 이것은 더 읽기 어려운 논문입니다. 다음 것은 좀 더 시작하기 수월할 것 같습니다. 이 슬라이드의 아이디어 중 대부분은 논문의 섹션 2 와 섹션 3에 대한 것들이고, 논문의 뒤 부분에서는 다른 내용에 대해 다루고 있는데요, 오늘날 널리 사용되지 않는 그래프 변압기 네트워크에 대해 다루고 있습니다. 따라서 이 논문을 읽어 보시려면, 아키텍처에 대해 이야기하는 두 번째 섹션에 초점을 맞추고 흥미로운 실험 및 결과가 많이 있는 세 번째 섹션을 간략하게 살펴 보는 것이 좋습니다. 이제 보여드리려고 하는 신경망의 두 번째 예제는 AlexNet (알렉스넷)입니다. Alex Krizhevsky의 이름을 따서 명명 것인데요, 이 분은 이 작업을 설명하는 이 논문의 첫 번째 저자입니다. 다른 저자는 Ilya Sutskever와 Geoffrey Hinton이었습니다. 따라서, 알렉스넷 입력은 227 x 227 x 3 이미지로 시작합니다. 그리고 논문을 읽어보시면 224 x 224 x 3의 이미지를 보여줍니다 그러나 숫자를 보면 숫자가 사실 227 x 227 인 경우에만 잘 들어맞습니다. 첫 번째 레이어는 96세트의 스트라이드 4를 가진 11 x 11 필터를 적용합니다. 그래서 큰 스트라이드 4를 사용하기 때문에. 디멘션은 55 x 55로 줄어듭니다. 대략적으로, 스트라이드가 커서 4배로 내려가는 것이죠. 그리고 나서, 3 x 3필터의 max pooling을 적용합니다. f=3, stride=2 이것을 볼륨을 27 x 27 x 96으로 줄이고, 5 x 5의 같은 컨볼루션 같은 패딩을 수행해서, 결국 27 x 27 x 276 이 됩니다. 다시 max pooling을 적용해서, 높이와 너비를 13으로 줄입니다. 그리고 또 다른 동일한 컨볼루션, 같은 패딩으로 13 x 13 x 384 필터가 됩니다. 3 x 3 의 같은 컨볼루션을 적용하면 이렇게 됩니다. 다시 3 x 3 의 동일한 컨볼류션을 적용해서 이렇게 되죠. max pooling을 적용하면 이렇게 6 x 6 x 256으로 작아지고 이 숫자들, 6 x 6 x 256 이렇게 곱하면 9,216입니다 그래서 이것을 9216 노드로 풀어 보겠습니다. 그리고 마지막으로 완전 연결 레이어가 몇 개 있습니다. 마지막으로 softmax를 사용하여 아웃풋을 만들도록 합니다. 1000 개 중 하나가 객체를 생성 할 수 있도록 말이죠. 이제, 이 신경망은 실제로 LeNet과 많은 유사점을 가졌지만 훨씬 더 커졌습니다. 이전 슬라이드의 LeNet-5에는 약 60,000 개의 파라미터가 있었지만 알렉스넷에는 약 6 천만 개의 파라미터가 있었습니다. 그리고 그것들은 많은 유사한 기본 빌딩 블럭을 취할 수 있지만 매우 많은 히든 유닛을 가지고 있고, 더 많은 데이터 상에서 학습하고 있습니다. 그들은 이미지에서 학습해서 데이터 세트가 엄청난 수행능력을 가질 수 있도록 합니다. LeNet보다 훨씬 나은이 아키텍처의 또 다른 측면은 밸류 액티베이션 기능을 사용하고 있다는 것입니다. 다시 말해서, 논문을 읽으면 이걸 읽지 않으면 신경 쓰지 않아도 될만 그런 고급 세부사항들이 나옵니다. 그 중 하나는 이 논문이 작성되었을 때, GPU는 여전히 약간 느려서 2 개의 GPU에서 복잡한 교육 방법을 가졌습니다. 기본적인 개념은 이 많은 레이어들이 서로 다른 두 개의 GPU를 교차해서 분리되었고, 두 GPU가 서로 통신 할 때 좀 더 좋은 방법이 있었다는 것 입니다. 이 논문에서는, 원래의 알렉스넷 아키텍처에도 Local Response Normalization이라는 레이어 세트가 추가되었습니다. 이 유형의 레이어는 실제로 별로 사용되지 않습니다. 제가 많이 설명하지 않는 이유이기도 하죠. 그러나 Local Response Normalization의 기본 개념은 이러한 블럭 중 하나를 살펴보면 위에 있는 볼륨 중 하나를 살펴 보시죠 논의를 위해서, 이 13 x 13 x 256 으로 생각해봅시다. Local Response Normalization, LRN (로컬 응답 정규화)가하는 일은 한 위치를 보는 것 입니다. 그 하나의 위치의 높이와 너비를 보는 것이죠. 그리고 이 모든 채널에 걸쳐 내려다 보고 이 모든 256 숫자를 보고 모두를 정상화 시키는 것입니다. 그리고이 로컬 응답 정규화의 동기는 이 13 x 13 이미지의 각 위치에 대해 매우 높은 액티베이션을 가진 너무 많은 뉴런을 갖지 않게 하기 위함입니다. 그러나 많은 연구자들은 이것이 이것이 많은 도움이 되지 않는다는 것을 발견했습니다. 그래서 이 개념들은 빨간색으로 그리고 있는 있는데요, 왜냐하면 여러분이 이걸 이해하는 게 그리 중요하지 않기 때문입니다. 그리고 실제로, 저는 요즘 학습된 네트워크 언어에서 지역 응답 정규화를 실제로 사용하지 않습니다. 따라서 딥러닝의 역사에 관심이 있다면 알렉스넷 이전에도, 딥러닝은 음성 인식 및 기타 분야에서 큰 영향을 받기 시작했다고 생각합니다. 그러나 이것은 컴퓨터 비전 커뮤니티에 확신을 준 것입니다. 컴퓨터 비전에서 딥러닝이 효과적임을 주장해서 딥러닝을 심각하게 받아들이도록 만든 것입니다. 그리고 나서 컴퓨터 비전뿐 아니라 컴퓨터 비전을 넘어서는 분야에서도 커다란 영향을 미칠 만큼 성장했습니다. 그리고 만약 여러분이 논문들을 직접 읽어 보시면, 사실 이 강의를 위해 읽을 필요는 없습니다만, 이 논문 몇몇을 읽어보시고 싶다면, 여기 있는 이것이 읽기 수월해서, 보시기 좋을 겁니다. 알렉스넷은 상대적으로 복잡한 아키텍처를 가지고 있었지만 많은 하이퍼 파라미터가있었습니다, 그렇죠? 그리고 Alex Krizhevsky와 그의 공동 저자들이 생각해 냈던 이 모든 숫자들이죠. 이 강의에서 세 번째이자 마지막 예시인 VGG 또는 VGG-16 네트워크라는 거를 보여 드리겠습니다 VGG-16 네트에 대한 주목할만한 점은 많은 하이퍼 파라미터를 가지고 있는 대신에, 더 단순한 네트워크를 사용합시다. 단지 스트라이드 1, 언제나 동일한 패딩의 3 x 3 필터인 컨볼레이어를 사용하는 것에 초점을 맞추고 그리고 스트라이드 2 를 가진 2 x 2 , max pooling레이어를 만든다는 점입니다. 그래서, VGG 네트워크에 대한 아주 좋은 점 중 하나는 이 신경망 아키텍처를 단순화 하는 것입니다. 이제 아키텍처를 살펴 보겠습니다. 그들을 위해 이미지로 풀면, 첫 번째 두 레이어가 컨볼루션이므로 3 x 3 필터가 되죠. 그리고 처음 두 레이어에서는 64 개의 필터를 사용합니다. 동일한 컨볼루션과 64 채널을 사용하기 때문에 224 x 224 가 됩니다. VGG-16은 상대적으로 깊은 네트워크이기 때문에 모든 볼륨을 그리지는 않겠습니다. 그래서이 작은 그림이 나타내는 것은 이전에 우리가 224 x 224 x 3로 그렸던 것입니다. 224 x 224 x 64가 되는 컨볼루션은 더 깊은 볼륨으로 그려지게 될 것입니다. 224 x 224 x 64가 되는 또 다른 레이어로 그려집니다 그래서 이 [conv64]x2는 64필터로 여기 이 두 개의 conv를 수행하고 있음을 나타냅니다. 그리고 앞에서 언급했듯이 필터는 항상 스트라이드 1 의 3x3 이므로 그것들은 항상 동일한 컨볼루션입니다. 따라서 이러한 모든 볼륨을 그리기보다는 텍스트를 사용하여 이 네트워크를 나타내겠습니다. pooling 레이어를 사용하고 그러면 pooling 레이어는 줄어들겠죠 224 x 224 에서 줄어들겠죠? 그렇습니다. 112 x 112 x 64 가 되고 몇 개 더 많은 컨볼레이어를 갖게 됩니다. 128 개의 필터를 가지고 있다는 것을 의미합니다. 그리고 이것들은 동일한 컨볼루션이기 때문입니다. 새로운 디멘션은 무엇인지 보시죠. 이것은 112 x 112x 128이 될 겁니다. 그리고 나서 풀리 레이어를 하면 이것의 새로운 디멘션이 무엇인지 알 수 있겠죠. 이제, 256필터를 가진 3개의 컨볼레이어가 pooling레이어를 거쳐서 몇 개 더 많은 컨볼레이어를 거치고 pooling 레이어, 더 많은 컨볼레이어, pooling 레이어 이런 식으로 됩니다. 그러면 이것은 이 최종 7 x 7 x 512를 취해서 완전 연결 레이어로, 4096 유닛의 FC, 그리고 1000개의 softmax 아웃풋이 됩니다. 하지만, VGG-16의 16은 이것이 16 개의 레이어에 가중치가 있음을 나타냅니다. 그리고 이것은 꽤 큰 네트워크입니다. 이 네트워크는 약 1 억 3 천 8 백만 개의 파라미터를 가지고 있습니다. 그것은 현대 표준에 의해서조차 도 꽤 큽니다. 그러나 VGG-16 아키텍처의 단순성으로 인해 매우 매력적이었습니다. 여러분은 이 아키텍처가 정말로 아주 균일하다는 것을 알 수 있습니다. 컨볼레이어에 뒤이어서 높이와 너비를 줄이는 pooling레이어가 있습니다, 그렇죠? pooling 레이어는 높이와 너비를 줄입니다. 여기에 그들 중 몇 개가 있습니다. 그러나 컨볼레이어에서 필터의 수를 보면 64 개의 필터가 있고, 256으로, 256으로 계속 두 배로 커지고 있습니다. 저자들은 512가 충분히 크고, 두 배로 늘어난 거라 생각했던 것 같습니다. 그러나 모든 단계에서 대략 두 배가되는 것 혹은 컨볼레이어의 모든 쌓아진 것을 관통해서 2배가 되는 것은 이 네트워크의 아키텍처를 디자인 하는 데에 사용된 또 다른 원칙이었던 것이죠. 그래서이 건축물의 상대적 균일 성이 연구자들에게 상당히 매력적인 네트워크입니다. 주된 단점은 여러분이 학습시켜야 하는 파라미터의 개수 측면에서 볼 때, 이게 매우 큰 네트워크라는 점 입니다. 그리고 논문을 읽으면 VGG-19가 이 네트워크의 더 큰 버전이라고 말하는 사람들을 볼 수 있습니다. 그리고 Karen Simonyan과 Andrew Zisserman에 의해 하단에 인용 된 논문의 세부 사항도 볼 수 있습니다. 그러나 VGG-16은 VGG-19와 거의 비슷하기 때문에 많은 사람들이 VGG-16을 사용할 것입니다. 하지만 이것에서 제가 가장 좋아했던 부분은 이게 이 패턴을 만드는 건데요, 더 깊게 높이와 너비가 내려감에 따라 pooling레이어가 매번 2배씩 내려간다는 것입니다. 채널의 수는 증가하지만 말이죠. 여기에 새로운 컨볼레이어 세트가 있을 때마다 대략 2 배씩 올라갑니다. 그것이 내려 가고 올라가는 비율을 체계적으로 만들어서, 저는 이 논문이 그 관점에서 매우 매력적이라고 생각했습니다. 자, 여기까지 3 가지 고전적인 아키텍처에 대한 내용이었습니다. 원하시면, 이 논문들을 읽어보시면 좋을 것 같습니다. 알렉스넷 논문을 시작으로 시작해서 VGGNet 논문으로 넘어가시는 것을 추천 드립니다. LeNet 논문은 읽기가 조금 더 어렵습니다. 하지만 살펴본다면 이 역시 좋은 고전 문헌이죠. 다음 강의에서는, 이러한 고전적 네트워크를 뛰어 넘어, 좀 더 발전되고 더욱 강력한 신경망 아키텍처를 살펴 보겠습니다. 다음 강의로 넘어가시죠.