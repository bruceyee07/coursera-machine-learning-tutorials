1
00:00:00,000 --> 00:00:02,470
このビデオでは

2
00:00:02,470 --> 00:00:05,418
クラシックな ニューラルネットワーク構造を学ぼう

3
00:00:05,418 --> 00:00:10,122
LeNet-5, Alexnet, そしてVGGNet の3つだ

4
00:00:10,122 --> 00:00:12,935
これが LeNet-5の構造だ

5
00:00:12,935 --> 00:00:15,700
初めの画像は

6
00:00:15,700 --> 00:00:17,540
32 x 32 x 1 だ

7
00:00:17,540 --> 00:00:21,520
LetNet-5は 手書き文字の認識のために作られた

8
00:00:21,520 --> 00:00:25,490
このような手書きの数字の画像だ

9
00:00:25,490 --> 00:00:28,815
そのためLeNet-5は グレースケール画像で学習する

10
00:00:28,815 --> 00:00:32,180
32 x 32 x 1 の画像で

11
00:00:32,180 --> 00:00:34,400
このニューラルネットワークの構造は

12
00:00:34,400 --> 00:00:38,315
先週のビデオで見たものとよく似ている

13
00:00:38,315 --> 00:00:39,847
最初のステップでは

14
00:00:39,847 --> 00:00:41,765
６つの

15
00:00:41,765 --> 00:00:45,220
ストライド１の 5 x 5 のフィルターを使う

16
00:00:45,220 --> 00:00:50,218
６フィルターを使うので ここにあるように 28 x 28 x 6 となる

17
00:00:50,218 --> 00:00:52,730
ストライド１で パディング無しなので

18
00:00:52,730 --> 00:00:58,640
画像の次元は 32 x 32 から 28 x 28 に削減される

19
00:00:58,640 --> 00:01:02,295
次に LeNetニューラルネットワークは プーリングを適応する

20
00:01:02,295 --> 00:01:04,970
この論文が書かれた時は、

21
00:01:04,970 --> 00:01:07,576
平均プーリングが より多く使われていた

22
00:01:07,576 --> 00:01:09,290
今なら

23
00:01:09,290 --> 00:01:12,145
多分 最大プーリングを使うだろう

24
00:01:12,145 --> 00:01:13,360
このオリジナルの例では

25
00:01:13,360 --> 00:01:17,825
平均プールは フィルター幅が２でストライドが２のものが使われている

26
00:01:17,825 --> 00:01:20,780
その結果 データの次元は

27
00:01:20,780 --> 00:01:22,730
因数２で削減される

28
00:01:22,730 --> 00:01:28,784
よって 14 x 14 x 6 ボリュームになる

29
00:01:28,784 --> 00:01:32,180
このボリュームの高さと幅は 完全な縮尺で描かれてない

30
00:01:32,180 --> 00:01:35,210
正確な縮尺で このボリュームを描けば

31
00:01:35,210 --> 00:01:38,150
高さと幅は 因数２で縮むからね

32
00:01:38,150 --> 00:01:41,180
次に 次の畳み込み層を適応する

33
00:01:41,180 --> 00:01:44,070
今度は 16個の 5 x 5 フィルターを使う

34
00:01:44,070 --> 00:01:48,515
その結果 16チャンネルになる

35
00:01:48,515 --> 00:01:52,355
この論文が書かれた1998年には

36
00:01:52,355 --> 00:01:57,200
パディングが使わなかった つまりValid畳み込みのみだった

37
00:01:57,200 --> 00:01:59,635
そのため 畳み込み層を適用する度に

38
00:01:59,635 --> 00:02:01,965
高さと幅は縮んだ

39
00:02:01,965 --> 00:02:03,380
だから

40
00:02:03,380 --> 00:02:06,393
14 x 14 から 10 x 10 に小さくなる

41
00:02:06,393 --> 00:02:08,580
それから 別のプーリング層が来て

42
00:02:08,580 --> 00:02:11,060
高さと幅は 因数２で縮小する

43
00:02:11,060 --> 00:02:13,715
結局 5 x 5 になる

44
00:02:13,715 --> 00:02:16,640
この 5 x 5 x 16 を掛け算すると

45
00:02:16,640 --> 00:02:20,375
掛け算の結果 400 になる

46
00:02:20,375 --> 00:02:24,020
25 x 16 は 400 だ

47
00:02:24,020 --> 00:02:29,900
次の層は 全結合層だ

48
00:02:29,900 --> 00:02:36,840
これら400ノードが 120ニューロンに それぞれ全て接続する

49
00:02:36,840 --> 00:02:38,385
そう 全結合層だ

50
00:02:38,385 --> 00:02:41,955
時々 こんな風に

51
00:02:41,955 --> 00:02:46,080
間引いて 400ノードの層を描くよ

52
00:02:46,080 --> 00:02:49,590
ここに全結合層が来て 次も全結合層だ

53
00:02:49,590 --> 00:02:51,690
そして 最終ステップでは

54
00:02:51,690 --> 00:02:57,280
この本質的な84個の特徴を使い 1つの出力にする

55
00:02:57,280 --> 00:03:01,375
ここに yハットを出すための もう1つのノードを 描こうとすれば 描ける

56
00:03:01,375 --> 00:03:04,560
そして yハットは １０種類の値を取り得る

57
00:03:04,560 --> 00:03:09,090
０~９のどの数字を認識したかによって

58
00:03:09,090 --> 00:03:11,100
このニューラルネットワークの現代版では

59
00:03:11,100 --> 00:03:17,300
10通りの分類を出力する ソフトマックス層を使うだろう

60
00:03:17,300 --> 00:03:23,385
しかし 当時は LeNet-5は 出力層に異なる分類器を使う

61
00:03:23,385 --> 00:03:25,633
今日では 役に立たないものだ

62
00:03:25,633 --> 00:03:29,220
このニューラルネットワークは 現在の標準では 小さいもので

63
00:03:29,220 --> 00:03:32,645
約６万個のパラメータを持っている

64
00:03:32,645 --> 00:03:35,934
今日では ニューラルネットワークは

65
00:03:35,934 --> 00:03:39,690
１千万から１億のパラメータを持つこともしばしばだ

66
00:03:39,690 --> 00:03:41,850
そして このネットワークよりも 文字通り

67
00:03:41,850 --> 00:03:45,295
千倍大きいネットワークを見ることも珍しくない

68
00:03:45,295 --> 00:03:49,600
しかし 1つ分かるのは
ネットワークが深くなるにつれ

69
00:03:49,600 --> 00:03:51,790
左から右へ行くにつれ

70
00:03:51,790 --> 00:03:55,360
高さと幅は 減る傾向があるということだ

71
00:03:55,360 --> 00:03:57,690
32 x 32 が 28 になり

72
00:03:57,690 --> 00:04:03,100
14 になり 10 になり 5 となった
一方で チャンネル数は増加する

73
00:04:03,100 --> 00:04:11,250
それは ネットワークの階層が深くなるにつれ
1 から 6 になり 16 になる

74
00:04:11,250 --> 00:04:15,400
このニューラルネットワークのもう1つのパターンは
今日でも繰り返し使われているけど

75
00:04:15,400 --> 00:04:20,500
１つ もしくは 複数の 畳み込み層を置き
次に 1つのプーリング増を置く

76
00:04:20,500 --> 00:04:25,758
そして さらに １つ もしくは 複数の 畳み込み層を置き
1つのプーリング増を置く

77
00:04:25,758 --> 00:04:29,940
そして いくつかの全結合層を置き 出力を得る
ってことだ

78
00:04:29,940 --> 00:04:34,090
そう こんな風に 層を配置するのは 非常に一般的なんだ

79
00:04:34,090 --> 00:04:39,515
では 最後に この論文を読んでみたい人向けになるかもしれないけど

80
00:04:39,515 --> 00:04:41,880
他に異なる点がいくるかある

81
00:04:41,880 --> 00:04:43,690
このスライドの残りで

82
00:04:43,690 --> 00:04:47,065
より上級者向けの提言をするけど

83
00:04:47,065 --> 00:04:52,265
このクラシック論文を読みたい人向けのね

84
00:04:52,265 --> 00:04:54,903
赤で書いたものは 全て

85
00:04:54,903 --> 00:04:57,490
飛ばしてしまっても問題ないからね

86
00:04:57,490 --> 00:05:00,520
ここに 面白い歴史の脚注があるけど

87
00:05:00,520 --> 00:05:04,350
全部 見る必要は無い

88
00:05:04,350 --> 00:05:07,990
オリジナルの論文を読めば 分かるけど

89
00:05:07,990 --> 00:05:12,453
sigmoidとtanhで非線形化していた

90
00:05:12,453 --> 00:05:16,330
当時は ReLU非線形は 使っていなかった

91
00:05:16,330 --> 00:05:20,065
だから 論文を見れば sigmoidとtanh が出てくる

92
00:05:20,065 --> 00:05:23,260
また このネットワークには 少しおかしな所もある

93
00:05:23,260 --> 00:05:26,835
ちょっと変 少なくとも現在の基準から見ればおかしい

94
00:05:26,835 --> 00:05:33,775
例えば nH x nW x nC ネットワークがあり

95
00:05:33,775 --> 00:05:40,985
nC チャンネルで
f x f x nC 次元のフィルターがあると

96
00:05:40,985 --> 00:05:44,480
全てのフィルターは これらのチャンネル全てを見るが

97
00:05:44,480 --> 00:05:47,195
当時 コンピュータがとても遅かったので

98
00:05:47,195 --> 00:05:50,230
計算やパラメータの保存のため

99
00:05:50,230 --> 00:05:53,785
オリジナルの LeNet-5は いくつかの狂気じみた方法を使っている

100
00:05:53,785 --> 00:05:58,040
異なるフィルターが 入力ブロックの異なるチャンネルを見る

101
00:05:58,040 --> 00:06:00,343
そして 論文では それらの詳細に触れている

102
00:06:00,343 --> 00:06:07,090
しかし 現在の実装には 今日では この種の複雑さは存在しない

103
00:06:07,090 --> 00:06:12,280
そして 現在は行われていないが 当時は行われていた 最後の1つは

104
00:06:12,280 --> 00:06:19,705
オリジナルの LeNet-5は プーリングの後に非線形化していたことだ

105
00:06:19,705 --> 00:06:25,005
実際には プーリング層の後に sigmoid を使っていた

106
00:06:25,005 --> 00:06:27,130
よって もし この論文を読めば

107
00:06:27,130 --> 00:06:29,345
読むのが 難しいものの1つになろう

108
00:06:29,345 --> 00:06:32,100
これから見ていく何本かのビデオに比べて

109
00:06:32,100 --> 00:06:34,670
次のは 簡単に始められるだろう

110
00:06:34,670 --> 00:06:40,135
このスライドの多くは その論文の２節と３節から得たものだが

111
00:06:40,135 --> 00:06:44,485
論文の後の節では 他のアイデアについても書かれている

112
00:06:44,485 --> 00:06:47,260
"Graph Transformer Networks" と呼ばれるものについて書いてあり

113
00:06:47,260 --> 00:06:49,215
それは 今日では 広く使われている

114
00:06:49,215 --> 00:06:50,935
よって もし この論文を読もうとするなら

115
00:06:50,935 --> 00:06:55,660
構造について書かれている第２節に絞ることを薦める

116
00:06:55,660 --> 00:06:58,165
そして 第３節は ざっと目を通すとよいだろう

117
00:06:58,165 --> 00:07:01,720
そこには 沢山の実験と結果が載っており とても興味深い

118
00:07:01,720 --> 00:07:06,155
見せたい ニューラルネットワークの２番目の例は AlexNet だ

119
00:07:06,155 --> 00:07:12,510
Alex Krizhevsky の名から名付けられた
彼は この仕事の論文の最初の著者だ

120
00:07:12,510 --> 00:07:13,725
他の著者は

121
00:07:13,725 --> 00:07:21,048
Ilya Sutskever と Geoffrey Hinton だ
AlexNet は 227 x 227 x 3 画像から始まる

122
00:07:21,048 --> 00:07:22,525
もし 論文を読めば

123
00:07:22,525 --> 00:07:27,010
論文では 224 x 224 x 3 画像となっている

124
00:07:27,010 --> 00:07:28,120
しかし 数を見ると

125
00:07:28,120 --> 00:07:33,100
実際には 227 x 227 とする方が 筋が通っている

126
00:07:33,100 --> 00:07:40,230
最初の層では 96個の 11 x 11 フィルターを適用する

127
00:07:40,230 --> 00:07:42,740
そして ４という大きめのストライドを使うので

128
00:07:42,740 --> 00:07:45,574
次元数は 55 x 55 に縮小する

129
00:07:45,574 --> 00:07:50,930
大きなストライドにより 大雑把に言って 因数４で小さくなる

130
00:07:50,930 --> 00:07:55,110
それから 3 x 3 の最大プーリングを行う

131
00:07:55,110 --> 00:07:57,925
f = 3 で ２のストライドだ

132
00:07:57,925 --> 00:08:04,570
よって これは ボリュームを 27 x 27 x 96 に落とす

133
00:08:04,570 --> 00:08:08,530
それから 5 x 5 のSame畳み込みをすると

134
00:08:08,530 --> 00:08:14,730
そうパディングすると 27 x 27 x 276 となる

135
00:08:14,730 --> 00:08:20,025
再び最大プーリング そうすると 高さと幅は 13 となる

136
00:08:20,025 --> 00:08:23,860
それから もう1つSame畳み込み Sameパディング

137
00:08:23,860 --> 00:08:29,805
すると 13 x 13 x 今度は384フィルター

138
00:08:29,805 --> 00:08:35,275
それから 再び 3 x 3 のSame畳み込みで こうなって

139
00:08:35,275 --> 00:08:39,680
それから 3 x 3 のSame畳み込みで こうなって

140
00:08:39,680 --> 00:08:45,285
それから 最大プールで 6 x 6 x 256 に落ちる

141
00:08:45,285 --> 00:08:52,020
これを全て掛けると 6 x 6 x 256 は 9216

142
00:08:52,020 --> 00:08:56,947
よって これを展開すると 9216ノードになる

143
00:08:56,947 --> 00:09:00,790
それから ついに 全結合層を入れて

144
00:09:00,790 --> 00:09:04,250
それから 最後に ソフトマックスで出力する

145
00:09:04,250 --> 00:09:09,515
それは 物体が取り得る1000クラスの内の1つだ

146
00:09:09,515 --> 00:09:16,920
このニューラルネットワークには 実は LeNetと多くの共通点がある

147
00:09:16,920 --> 00:09:20,210
ただし とても大きいけど

148
00:09:20,210 --> 00:09:27,740
前のスライドの LeNet LeNet-5には 約6万のパラメータがあったけど

149
00:09:27,740 --> 00:09:31,935
この AlexNet には 約６千万のパラメータがある

150
00:09:31,935 --> 00:09:34,024
そして 実際は

151
00:09:34,024 --> 00:09:36,925
とても良く似た基本構成要素があるが

152
00:09:36,925 --> 00:09:40,270
より多くの隠れ層を持ち より多くのデータで学習し

153
00:09:40,270 --> 00:09:42,820
ImageNet データセットで学習し

154
00:09:42,820 --> 00:09:46,255
本当に 目を見張る性能を持つに至った

155
00:09:46,255 --> 00:09:49,810
この構造を LeNetよりも大きく改善させた１面には

156
00:09:49,810 --> 00:09:53,575
ReLU 活性化関数を使っている ということがある

157
00:09:53,575 --> 00:09:56,425
そして もう一度言うけど もし 論文を読む場合

158
00:09:56,425 --> 00:09:59,020
より進んだ詳細が書いてあるけど それは

159
00:09:59,020 --> 00:10:01,840
論文を読まない場合は関わる必要の無いようなことだ 1つは

160
00:10:01,840 --> 00:10:03,445
この論文が書かれた時は

161
00:10:03,445 --> 00:10:06,197
GPUは まだ少し遅かった

162
00:10:06,197 --> 00:10:11,135
なので ２つのGPUで学習するのに 複雑な方法を取っていた

163
00:10:11,135 --> 00:10:13,310
基本的なアイデアはこうだ

164
00:10:13,310 --> 00:10:18,250
沢山のこれらの層を２つのGPU向けに分割し

165
00:10:18,250 --> 00:10:23,497
２つのGPUが互いにやり取りできるような考え抜かれた方法を使ったんだ

166
00:10:23,497 --> 00:10:25,360
論文では また

167
00:10:25,360 --> 00:10:29,650
オリジナルのAlexNet構造は もう一つ別の層を持っていた

168
00:10:29,650 --> 00:10:34,125
それは 局所応答正規化 と呼ばれる

169
00:10:34,125 --> 00:10:36,820
この種の層は 多くは使われていない

170
00:10:36,820 --> 00:10:38,830
それなので それについては話さなかった

171
00:10:38,830 --> 00:10:42,220
しかし 局所応答正規化 の基本アイデアは

172
00:10:42,220 --> 00:10:44,845
もし これらのブロックの1つを見れば

173
00:10:44,845 --> 00:10:46,940
上段のこれらのボリュームの1つを見れば

174
00:10:46,940 --> 00:10:49,360
話のために この1つを見ると

175
00:10:49,360 --> 00:10:52,380
13 x 13 x 256

176
00:10:52,380 --> 00:10:54,765
局所応答正規化(LRN)がするのは

177
00:10:54,765 --> 00:10:57,805
1つの位置を見て

178
00:10:57,805 --> 00:10:59,570
高さと幅のある1つの位置で

179
00:10:59,570 --> 00:11:02,935
全てのチャンネルに跨って

180
00:11:02,935 --> 00:11:07,195
256個全てをもって 正規化することだ

181
00:11:07,195 --> 00:11:10,750
この 13 x 13 画像の各場所で

182
00:11:10,750 --> 00:11:14,934
局所応答正規化を行おうとするのは

183
00:11:14,934 --> 00:11:20,123
非常に高く活性化するニューロンを数多く欲しくないからだ

184
00:11:20,123 --> 00:11:25,730
しかし 後で 多くの研究者によって これはそれほど助けにはならないことが分かった

185
00:11:25,730 --> 00:11:27,995
だから これは赤で書かれるだろうアイデアの1つだ

186
00:11:27,995 --> 00:11:31,880
なぜなら あなたが理解べき重要事項ではないからだ

187
00:11:31,880 --> 00:11:33,940
そして 実際 私は

188
00:11:33,940 --> 00:11:38,760
局所応答正規化を使わない
今日学習させるネットワークでは使わない

189
00:11:38,760 --> 00:11:41,380
もし ディープラーニングの歴史に興味があるなら

190
00:11:41,380 --> 00:11:43,395
AlexNetよりも前でさえ

191
00:11:43,395 --> 00:11:48,978
ディープラーニングは 音声認識や他の分野で 勢いを増しつつあった

192
00:11:48,978 --> 00:11:52,690
でも 本当にこの論文なんだ

193
00:11:52,690 --> 00:11:56,350
多くのコンピュータ ビジョン会で ディープラーニングが

194
00:11:56,350 --> 00:12:00,280
真剣に見られるようになり コンピュータ ビジョンに使えると人々を信じさせたのは

195
00:12:00,280 --> 00:12:02,710
それから 非常に大きな衝撃を持つまでになった

196
00:12:02,710 --> 00:12:05,508
コンピュータ ビジョンだけでなく それ以外でも

197
00:12:05,508 --> 00:12:08,170
もし これらの論文を 自分で読もうとするなら

198
00:12:08,170 --> 00:12:11,635
本当は このコースでやる必要は無いけど

199
00:12:11,635 --> 00:12:14,200
でも もし これらの論文を 自分で読もうとするなら

200
00:12:14,200 --> 00:12:19,354
これは 読むのが易しい方だし ちょっと目を通すのもいいだろう

201
00:12:19,354 --> 00:12:23,257
AlexNetは 比較的 複雑な構造だったけど

202
00:12:23,257 --> 00:12:25,585
ハイパーパラメータが沢山あるしね

203
00:12:25,585 --> 00:12:28,255
そこの全ての数は

204
00:12:28,255 --> 00:12:33,240
Alex Krizhevsky と共著者が 思いついたものだ

205
00:12:33,240 --> 00:12:39,765
このビデオにおける ３番目の 最後の例
VGG もしくは VGG-16 ネットワークと呼ばれるものを見よう

206
00:12:39,765 --> 00:12:44,820
VGG-16 ネットについて特筆すべき点は

207
00:12:44,820 --> 00:12:46,966
多くのハイパーパラメータを持つ代わりに

208
00:12:46,966 --> 00:12:52,495
畳み込み層を持つことに重きを置き もっと単純なネットワークを使う

209
00:12:52,495 --> 00:12:58,690
その層は 単に ストライド１の3 x 3 フィルターで 常にSameパディングを使う

210
00:12:58,690 --> 00:13:03,640
そして 全ての最大プーリング層を ストライド２の 2 x 2 にする

211
00:13:03,640 --> 00:13:06,250
そして VGGネットワークの 非常に良い点は

212
00:13:06,250 --> 00:13:12,224
ニューラルネットワークの構造を 本当に単純にすることだ

213
00:13:12,224 --> 00:13:14,494
では 構造を見てみよう

214
00:13:14,494 --> 00:13:19,660
画像から始まり
最初は ２層の畳み込みだ

215
00:13:19,660 --> 00:13:24,315
3 x 3 フィルターを使う

216
00:13:24,315 --> 00:13:27,930
最初の２層は 64個のフィルターを使う

217
00:13:27,930 --> 00:13:35,830
結果は 224 x 224 なぜなら Same畳み込みだから
そして 64チャンネルだ

218
00:13:35,830 --> 00:13:39,345
VGG-16は 比較的深いネットワークなので

219
00:13:39,345 --> 00:13:42,335
ここには 全てのボリュームを描かない

220
00:13:42,335 --> 00:13:46,270
この行が意味しているのは 前に

221
00:13:46,270 --> 00:13:50,890
224 x 224 x 3 として描いたのと

222
00:13:50,890 --> 00:13:55,362
224 x 224 x 64 に畳み込むもので

223
00:13:55,362 --> 00:14:00,535
(より深いボリュームで描かないと)

224
00:14:00,535 --> 00:14:07,227
そして もう１つの 224 x 224 x 64 に畳み込むものだ

225
00:14:07,227 --> 00:14:15,730
つまり この "[CONV 64] x 2" は ２つの層を
64個のフィルターを持つ２つの畳み込み層を表す

226
00:14:15,730 --> 00:14:17,380
そして 前に言ったように

227
00:14:17,380 --> 00:14:20,555
フィルターは 常に 3 x 3 で

228
00:14:20,555 --> 00:14:24,455
ストライドは１ そして 常に Same畳み込みだ

229
00:14:24,455 --> 00:14:26,395
よって 全てのボリュームを描くよりも

230
00:14:26,395 --> 00:14:28,400
このネットワークを表すのに 文字を使おう

231
00:14:28,400 --> 00:14:31,413
次に プーリング層を使う

232
00:14:31,413 --> 00:14:33,580
プーリング層は 縮小させる

233
00:14:33,580 --> 00:14:36,725
224 x 224 から 何になる？

234
00:14:36,725 --> 00:14:40,755
そう 112 x 112 x 64

235
00:14:40,755 --> 00:14:44,339
それから さらに数個の畳み込み層

236
00:14:44,339 --> 00:14:50,426
これは 128個のフィルターを意味していて
Same畳み込みだから

237
00:14:50,426 --> 00:14:52,365
新しい次元は？

238
00:14:52,365 --> 00:14:57,020
分かる？ 112 x 112 x 128 だ それから

239
00:14:57,020 --> 00:15:02,205
プーリング層
で 新たな次元が何になるか 分かるでしょ？

240
00:15:02,205 --> 00:15:07,210
そして ３つの畳み込み層

241
00:15:07,210 --> 00:15:14,300
256フィルターのね
プーリング層 それから もういくつか畳み込み層

242
00:15:14,300 --> 00:15:18,945
プーリング層 さらにいくつか畳み込み層 プーリング層

243
00:15:18,945 --> 00:15:26,345
それから この最後の 7 x 7 x 512 を得て 全結合層に喰わせる

244
00:15:26,345 --> 00:15:30,230
4096ユニットの全結合層

245
00:15:30,230 --> 00:15:36,080
それから ソフトマックスで 1000クラスの中から１つの出力を得る

246
00:15:36,080 --> 00:15:39,875
ところで VGG-16の16は

247
00:15:39,875 --> 00:15:45,080
重みを持っている層が16あることから来ている

248
00:15:45,080 --> 00:15:47,470
そして これは かなり大きなネットワークだ

249
00:15:47,470 --> 00:15:52,415
このネットワークは 全部で １億３８百万個のパラメータを持っている

250
00:15:52,415 --> 00:15:55,615
これは 現在の標準から言っても とても大きい

251
00:15:55,615 --> 00:16:00,673
しかし VGG-16構造の単純さが 非常に魅力的にした

252
00:16:00,673 --> 00:16:03,935
これの構造は 極めて均質だと言ってよいだろう

253
00:16:03,935 --> 00:16:07,130
数個の畳み込み層に 1つのプーリング層が続き

254
00:16:07,130 --> 00:16:09,590
高さと幅を減らす

255
00:16:09,590 --> 00:16:13,396
プーリング層は高さと幅を減らすからね

256
00:16:13,396 --> 00:16:15,570
ここに あるね

257
00:16:15,570 --> 00:16:20,260
しかし 畳み込み層のフィルター数を見ると

258
00:16:20,260 --> 00:16:28,675
ここに64フィルターがあり それから 倍の128 倍の256 倍の512

259
00:16:28,675 --> 00:16:33,160
思うに 著者は 512が十分に大きいと考えて ここでさらに倍にはしなかった

260
00:16:33,160 --> 00:16:36,410
しかし このように ステップの度に ほぼ倍にすること

261
00:16:36,410 --> 00:16:39,915
畳み込み層の塊を通る度に 倍にすることは

262
00:16:39,915 --> 00:16:45,040
このネットワーク構造の設計の もう1つの 単純な原則だ

263
00:16:45,040 --> 00:16:48,230
この構造の 相対的な均一さは

264
00:16:48,230 --> 00:16:52,460
研究者にとって とても魅力的だ

265
00:16:52,460 --> 00:16:54,680
主な欠点は

266
00:16:54,680 --> 00:16:58,910
非常に多くのパラメータを持つ とても大きなネットワークを
学習させなくてはならないことだ

267
00:16:58,910 --> 00:17:00,995
もし 文献を読むならば

268
00:17:00,995 --> 00:17:04,700
人々は 時々 VGG-19について話しているのが分かるだろう

269
00:17:04,700 --> 00:17:08,600
それは このネットワークの より大きいバージョンだ

270
00:17:08,600 --> 00:17:11,780
そして その詳細は

271
00:17:11,780 --> 00:17:16,595
下の Karen Simonyan と Andrew Zisserman から引用した論文で 見ることができる

272
00:17:16,595 --> 00:17:20,875
ただし VGG-16は VGG-19 と 殆ど同じで

273
00:17:20,875 --> 00:17:23,570
多くの人は VGG-16を使うだろう

274
00:17:23,570 --> 00:17:26,090
しかし これの私が最も気に入っている点は

275
00:17:26,090 --> 00:17:28,540
このパターンを持っているとういことだ

276
00:17:28,540 --> 00:17:31,100
深くなるにつれ 高さと幅が減っていき

277
00:17:31,100 --> 00:17:33,540
それは プーリング層で 因数２で毎回 減っていき

278
00:17:33,540 --> 00:17:36,890
一方 チャンネル数は 増加していく

279
00:17:36,890 --> 00:17:42,855
新しい畳み込み層の塊りの度に 大体２の因数で増えていく

280
00:17:42,855 --> 00:17:49,155
つまり その割合にしたことで とてもシステマティックに 減っていき 増えていく

281
00:17:49,155 --> 00:17:54,410
この点が この論文を非常に魅力的にしていると思う

282
00:17:54,410 --> 00:17:57,845
これで ３つのクラシック構造は終わりだ

283
00:17:57,845 --> 00:18:00,931
これらの論文を読みたいのなら 今読むべきだ

284
00:18:00,931 --> 00:18:05,270
AlexNet論文から初めて 次にVGGネット論文を読むことを薦める

285
00:18:05,270 --> 00:18:07,460
それから LeNet論文は 読むのが

286
00:18:07,460 --> 00:18:09,984
難しいが 一見する価値のある 良いクラシックだ

287
00:18:09,984 --> 00:18:14,725
でも 次に これらのクラシックネットワークを超えて より進んだものを見てみよう

288
00:18:14,725 --> 00:18:18,040
さらに強力なニューラルネットワーク構造を
さぁ 次のビデオに進もう