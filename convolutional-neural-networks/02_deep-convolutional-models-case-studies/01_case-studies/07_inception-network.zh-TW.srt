1
00:00:00,190 --> 00:00:01,320
在上一段影片中

2
00:00:01,320 --> 00:00:06,420
你已經看到所有 Inception 神經網絡的基本建構區塊

3
00:00:06,420 --> 00:00:10,680
在接下來的影片中，讓我們來看如何運用這些建構區塊

4
00:00:10,680 --> 00:00:12,780
來建立您的 Inception 網路

5
00:00:13,910 --> 00:00:17,970
Inception 模組輸入前一層的啟動值

6
00:00:17,970 --> 00:00:20,260
或者說前一層的輸出

7
00:00:20,260 --> 00:00:25,860
為了討論方便
假設是 28乘28乘192

8
00:00:25,860 --> 00:00:28,030
跟前面影片中一樣

9
00:00:28,030 --> 00:00:35,130
我們要詳細跑的例子是 1乘1 接著 5乘5 層

10
00:00:35,130 --> 00:00:41,120
或許 1乘1 有 16 個通道

11
00:00:41,120 --> 00:00:48,170
然後 5乘5 會輸出 28乘28 然後 32 個通道

12
00:00:49,700 --> 00:00:53,790
而這是前段影片
最後投影片中的例子

13
00:00:54,900 --> 00:01:02,260
然後為了節省計算，您的也可以在這裡做 3乘3 卷積

14
00:01:02,260 --> 00:01:08,310
然後這個 3乘3 輸出會是 28乘28乘128

15
00:01:09,390 --> 00:01:14,100
然後或許您也考慮做單獨的 1乘1 卷積

16
00:01:14,100 --> 00:01:18,481
這時就不需要用一個 1乘1卷積
 再接著一個 1乘1 卷積

17
00:01:18,481 --> 00:01:23,409
這裡只需要一個步驟，假設輸出是 28乘28乘64

18
00:01:23,409 --> 00:01:31,900
最後是池層

19
00:01:34,000 --> 00:01:35,730
這裡我們要做一件有趣的事

20
00:01:35,730 --> 00:01:40,154
為了最後要連結所有這些輸出

21
00:01:40,154 --> 00:01:44,073
我們將要使用相同填充在這個池層

22
00:01:44,073 --> 00:01:47,480
所以輸出維度依然是 28乘28

23
00:01:47,480 --> 00:01:53,300
這樣我們才可以跟其他的輸出連結

24
00:01:53,300 --> 00:01:57,842
但請注意，即使您做最大池層，即使是相同填充

25
00:01:57,842 --> 00:01:59,917
3乘3 過濾器跨步為 1

26
00:01:59,917 --> 00:02:07,016
輸出依然會是 28乘28乘192

27
00:02:07,016 --> 00:02:10,790
它會有相同的通道數

28
00:02:10,790 --> 00:02:15,570
跟我們的輸入有相同的深度

29
00:02:15,570 --> 00:02:19,252
這個看起來有很多的通道

30
00:02:19,252 --> 00:02:23,752
我們要做的是加入一個 1乘1 卷積層

31
00:02:23,752 --> 00:02:28,607
就像我們在 1乘1 卷積影片中見到的

32
00:02:28,607 --> 00:02:31,541
來縮短通道的數目

33
00:02:31,541 --> 00:02:37,730
我們讓它降至 28乘28乘32

34
00:02:37,730 --> 00:02:44,770
方式是使用 32 個過濾器

35
00:02:44,770 --> 00:02:49,660
維度是 1乘1乘192

36
00:02:49,660 --> 00:02:54,110
這是為什麼輸出維度會縮為 32 個通道

37
00:02:54,110 --> 00:02:58,460
所以我們不會因為池層

38
00:02:58,460 --> 00:03:00,920
而用了所有輸出的通道

39
00:03:02,310 --> 00:03:08,121
最後我們把這些區塊做通道連結

40
00:03:08,121 --> 00:03:12,865
只是連結這些 64 加 128 加

41
00:03:12,865 --> 00:03:18,165
32 加 32 ，如果您將他們加總

42
00:03:18,165 --> 00:03:24,055
會是28乘28乘256 維度的輸出

43
00:03:24,055 --> 00:03:30,879
連結就只是將這些區塊連在一起，
像我們在前面影片中見到的

44
00:03:33,918 --> 00:03:39,598
所以這就是一個 inception 模組，而 inception 網路做的

45
00:03:39,598 --> 00:03:44,057
或多或少，就是將一些這樣的模組放在一起

46
00:03:45,624 --> 00:03:50,671
這是一個 inception 網路的圖片，從 Szegedy et al 的論文中節錄

47
00:03:53,615 --> 00:03:56,570
您注意到有很多重複的區塊

48
00:03:56,570 --> 00:03:58,580
或許這個圖片看起來很複雜

49
00:03:58,580 --> 00:04:03,563
但如果您看這一個區塊，這個區塊基本上

50
00:04:03,563 --> 00:04:07,920
就是您在前面投影片看到的 inception 模組

51
00:04:10,046 --> 00:04:17,085
我不想討論裡面的細節，但這是另一個 inception 區塊

52
00:04:17,085 --> 00:04:19,460
這使另一個 inception 區塊

53
00:04:19,460 --> 00:04:23,200
有一些額外的池層在這裡來改變維度

54
00:04:24,250 --> 00:04:25,800
有關於高度跟寬度

55
00:04:25,800 --> 00:04:28,140
但這是另一個 inception 區塊

56
00:04:28,140 --> 00:04:31,020
而這是另一個最大池層來改變高度跟寬度

57
00:04:31,020 --> 00:04:33,280
但基本上這是另一個 inception 區塊

58
00:04:33,280 --> 00:04:37,370
inception 網路就是有很多
您學過這樣的區塊

59
00:04:37,370 --> 00:04:40,930
重複地在網路中不同的位置出現

60
00:04:40,930 --> 00:04:44,509
所以您只要理解前面投影片中的 inception 區塊

61
00:04:44,509 --> 00:04:46,914
您就會理解 inception 網路

62
00:04:49,518 --> 00:04:53,403
實際上還有一個 inception 網路的細節

63
00:04:53,403 --> 00:04:55,430
如果您讀研究論文的話

64
00:04:55,430 --> 00:04:59,311
也就是我剛加入的這些額外的旁枝

65
00:05:01,835 --> 00:05:03,440
他們有什麼作用？

66
00:05:03,440 --> 00:05:07,800
因為網路最後幾層時全連結層

67
00:05:07,800 --> 00:05:11,360
接著一個 softmax 層來做預測

68
00:05:11,360 --> 00:05:15,430
這些旁枝做的是拿一些隱藏層

69
00:05:15,430 --> 00:05:17,760
試著用它來做預測

70
00:05:17,760 --> 00:05:22,140
所以這實際上是 softmax 輸出

71
00:05:22,140 --> 00:05:23,952
這是另一個旁枝

72
00:05:23,952 --> 00:05:29,173
一樣，拿這個隱藏層經過一些層像是全連結層

73
00:05:29,173 --> 00:05:33,243
然後一個 softmax 試著去預測輸出標籤

74
00:05:35,545 --> 00:05:38,798
您應該想成這只是另一個

75
00:05:38,798 --> 00:05:40,000
inception 網路的細節

76
00:05:40,000 --> 00:05:44,460
它的作用是確保計算的特徵值

77
00:05:44,460 --> 00:05:48,060
即使在隱藏層，即使是中間層

78
00:05:48,060 --> 00:05:52,490
在影像的預測輸出時也不差

79
00:05:52,490 --> 00:05:56,875
這是一種對 inception 網路的影響正規化

80
00:05:56,875 --> 00:05:59,667
幫助避免這個網路過適

81
00:06:03,048 --> 00:06:07,907
順便提一下，這個特定的 inception 網路

82
00:06:07,907 --> 00:06:11,770
是由 Google 員工開發出來的

83
00:06:11,770 --> 00:06:18,850
所以稱為 GoogleNet 這樣拼法是為了對這個網路致敬

84
00:06:18,850 --> 00:06:21,350
您從前面影片中也學到了這個網路

85
00:06:23,460 --> 00:06:29,086
我想這非常棒，深度學習社群

86
00:06:29,086 --> 00:06:30,436
是如此協同努力

87
00:06:30,436 --> 00:06:32,593
有這種強烈，健康的尊重

88
00:06:32,593 --> 00:06:35,865
在深度學習社群中每個人的貢獻

89
00:06:35,865 --> 00:06:37,895
最後一點有趣的事實

90
00:06:37,895 --> 00:06:40,425
這個 inception 網路的名稱何來？

91
00:06:41,585 --> 00:06:47,375
inception 論文實際引用這個網路爆紅事件(meme) 
是因為我們要進入更深層

92
00:06:47,375 --> 00:06:52,315
而在 inception 論文中的這個網址真的指到

93
00:06:52,315 --> 00:06:54,320
連結到這個圖片

94
00:06:54,320 --> 00:06:57,610
如果您看過 Inception（全面啟動）這個電影

95
00:06:57,610 --> 00:07:00,070
或許這個名稱會對您有意義

96
00:07:00,070 --> 00:07:05,380
但作者實際上是引用這個名稱來

97
00:07:05,380 --> 00:07:09,040
表達需要建立更深層神經網路的動機

98
00:07:09,040 --> 00:07:12,890
這是為什麼他們要提出這種 inception 架構的用意

99
00:07:12,890 --> 00:07:17,830
我想在研究論文中引用網路爆紅事件 (internet meme)

100
00:07:17,830 --> 00:07:19,030
是比較少見的

101
00:07:19,030 --> 00:07:22,165
但在這個例子中，我覺得他用得很好

102
00:07:23,285 --> 00:07:27,015
總結一下，如果您理解 inception 模組

103
00:07:27,015 --> 00:07:29,765
那您也會理解 inception 網路

104
00:07:29,765 --> 00:07:33,865
也就是很多的 inception 模組重複很多次

105
00:07:33,865 --> 00:07:35,435
在這個網路上

106
00:07:35,435 --> 00:07:40,025
自從原始 inception 模組開發後，作者跟

107
00:07:40,025 --> 00:07:43,760
一些人在這個基礎上建立其他版本

108
00:07:43,760 --> 00:07:49,090
所以有一些研究論文在
新版本的 inception 演算法上

109
00:07:49,090 --> 00:07:53,380
您有時候會見到一些人使用後來的版本

110
00:07:53,380 --> 00:07:57,040
您會見到像是 inception v2, inception v3, inception v4 

111
00:07:57,040 --> 00:07:59,360
也有一種 inception 版本

112
00:07:59,360 --> 00:08:02,800
結合了殘差網路 (ResNets) 的觀念，加入了跳過連結

113
00:08:02,800 --> 00:08:05,740
有時候作用得更好

114
00:08:05,740 --> 00:08:10,600
但所有這些變形都建立在
您剛學到的基本觀念上

115
00:08:10,600 --> 00:08:14,710
也就是前面影片中介紹的 inception 模組

116
00:08:14,710 --> 00:08:17,510
然後將它們疊在一起建立的

117
00:08:17,510 --> 00:08:20,530
而透過這段影片，您應該可以讀懂

118
00:08:20,530 --> 00:08:23,940
跟理解這個 inception 論文

119
00:08:23,940 --> 00:08:28,790
跟或許一些論文
是使用我描述一些後面的變形

120
00:08:28,790 --> 00:08:30,020
就這樣了

121
00:08:30,020 --> 00:08:34,820
您看到了許多特定的神經網路架構

122
00:08:34,820 --> 00:08:39,650
在下一段影片，我想分享您一些實作的建議

123
00:08:39,650 --> 00:08:43,830
您如何真正使用這些演算法來
建立您自己的電腦視覺系統

124
00:08:43,830 --> 00:08:45,090
讓我們進入下一段影片