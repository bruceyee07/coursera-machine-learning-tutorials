1
00:00:00,010 --> 00:00:03,935
ConvNet構造の設計面では

2
00:00:03,935 --> 00:00:09,140
本当に助けになるアイデアに 1 x 1 畳み込みがある

3
00:00:09,140 --> 00:00:10,685
今 疑問に思ったでしょ？

4
00:00:10,685 --> 00:00:13,035
1 x 1 畳み込みが何をやるのか

5
00:00:13,035 --> 00:00:15,170
数を掛けるだけじゃないの？

6
00:00:15,170 --> 00:00:17,150
おかしな事をやるように思える

7
00:00:17,150 --> 00:00:18,680
全く そんなことは無い

8
00:00:18,680 --> 00:00:20,223
次の例をみてみましょう。

9
00:00:20,223 --> 00:00:22,740
ここに 1 x 1 フィルターがある

10
00:00:22,740 --> 00:00:24,970
そこれに 数２を置く

11
00:00:24,970 --> 00:00:27,460
そして もし この 6 x 6 画像を

12
00:00:27,460 --> 00:00:31,310
6 x 6 x 1 を この 1 x 1 x 1 フィルターで畳み込むと

13
00:00:31,310 --> 00:00:33,940
単に 画像を取って それに２を掛けただけの結果を得る

14
00:00:33,940 --> 00:00:37,580
そう 1 2 3 は

15
00:00:37,580 --> 00:00:40,190
2 4 6 等となる

16
00:00:40,190 --> 00:00:43,940
よって 1 x 1 フィルターによる畳み込みは

17
00:00:43,940 --> 00:00:45,350
特に有益には見えない

18
00:00:45,350 --> 00:00:47,615
単に ある数を掛けただけだから

19
00:00:47,615 --> 00:00:53,635
しかし それは 6 x 6 x "１"チャンネル画像だからだ

20
00:00:53,635 --> 00:00:58,415
もし "1" ではなく 6 x 6 x "32" だったら

21
00:00:58,415 --> 00:01:04,995
1 x 1 フィルターによる畳み込みは もっと意味を持った何かをできる

22
00:01:04,995 --> 00:01:08,945
特に 1 x 1 畳み込みがすることは

23
00:01:08,945 --> 00:01:13,620
ここの36個の異なる位置を見て

24
00:01:13,620 --> 00:01:16,720
要素ごとの掛け算を行う

25
00:01:16,720 --> 00:01:21,185
左の32個の数を このフィルターで

26
00:01:21,185 --> 00:01:26,230
その後に ReLU非線形を適用する

27
00:01:26,230 --> 00:01:29,405
36個の位置の内の１つを見てみよう

28
00:01:29,405 --> 00:01:32,840
このボリュームを貫く１断片だ

29
00:01:32,840 --> 00:01:41,525
これらの36(訳注: 32の言い間違いと思われる)個の数を
このように ボリューム全体を通した 1 x 1 断片と掛け算すると

30
00:01:41,525 --> 00:01:43,535
結局こうなる

31
00:01:43,535 --> 00:01:49,945
出力の各々は このようにプロットされる 単一の実数になる

32
00:01:49,945 --> 00:01:52,535
実際 考え方としては

33
00:01:52,535 --> 00:01:56,645
この 1 x 1 x 32 フィルターにある 32個の数は

34
00:01:56,645 --> 00:02:01,020
ニューロンがあるようなもので

35
00:02:01,020 --> 00:02:06,770
32個の数 これらの32個の各々

36
00:02:06,770 --> 00:02:12,285
同じ位置 高さと場所にある 32の異なるチャンネルを持った１つの断片を

37
00:02:12,285 --> 00:02:15,665
32の重みと掛け算して

38
00:02:15,665 --> 00:02:22,115
ReLU非線形に適用して それから ここに出力する

39
00:02:22,115 --> 00:02:28,875
さらに一般的には フィルターは１つだけではなく

40
00:02:28,875 --> 00:02:31,020
もし 複数のフィルターを持っていた場合は

41
00:02:31,020 --> 00:02:36,240
１ユニットではなく 複数ユニットがあるようなものだ

42
00:02:36,240 --> 00:02:40,950
１断片の全ての数を入力とし

43
00:02:40,950 --> 00:02:49,530
6 x 6 x フィルター数 の出力を作る

44
00:02:49,530 --> 00:02:53,260
つまり 1 x 1 畳み込みの考え方の１つは

45
00:02:53,260 --> 00:02:59,115
基本的には 全結合ニューロン ネットワークであり

46
00:02:59,115 --> 00:03:04,015
それは 62(訳注:36の言い間違いと思われる)個の異なる位置に適用される

47
00:03:04,015 --> 00:03:05,920
全結合ニューロン ネットワークが行うのは

48
00:03:05,920 --> 00:03:13,210
32個の数を入力として フィルター数分の出力を行う

49
00:03:13,210 --> 00:03:14,555
表記にもよるが

50
00:03:14,555 --> 00:03:15,910
これは nC[l+1]

51
00:03:15,910 --> 00:03:19,440
それが 次の層にあるとすればね

52
00:03:19,440 --> 00:03:22,710
36個の位置それぞれで これを行うと

53
00:03:22,710 --> 00:03:24,160
この 6 x 6 位置で行なうと

54
00:03:24,160 --> 00:03:29,850
6 x 6 x フィルター数 の出力を得る

55
00:03:29,850 --> 00:03:35,830
これは 入力ボリュームに対して とても重要な計算となるかもしれない

56
00:03:35,830 --> 00:03:40,755
このアイデアは よく 1 x 1 畳み込みと呼ばれる

57
00:03:40,755 --> 00:03:46,655
でも 時々 Network in Network とも呼ばれる

58
00:03:46,655 --> 00:03:49,468
そして この論文に書かれているが

59
00:03:49,468 --> 00:03:53,485
Min Lin, Qiang Chen, そして Schuicheng Yan の論文に書かれているが

60
00:03:53,485 --> 00:03:58,400
この論文にある構造のアイデアは 広く使われていないにも関わらず

61
00:03:58,400 --> 00:04:01,460
1 x 1 畳み込みのアイデア もしくは

62
00:04:01,460 --> 00:04:05,300
時折 Network in Network と呼ばれるアイデアは 大きな影響を与えてきた

63
00:04:05,300 --> 00:04:08,090
それは 他のニューラルネットワーク構造の多くに影響を与えてきた

64
00:04:08,090 --> 00:04:11,860
次のビデオで見る Inception ネットワーク も その内の一つだ

65
00:04:11,860 --> 00:04:16,180
でも 1 x 1 畳み込みが有効である例を示すため

66
00:04:16,180 --> 00:04:18,443
ここに それができるのを用意した

67
00:04:18,443 --> 00:04:23,070
28 x 28 x 192 ボリュームがあるとする

68
00:04:23,070 --> 00:04:25,715
高さと幅を縮小したいなら

69
00:04:25,715 --> 00:04:27,310
プーリング層を使うことができる

70
00:04:27,310 --> 00:04:28,800
その方法は知っているよね

71
00:04:28,800 --> 00:04:34,265
しかし チャンネル数が大きくて それを縮小したい場合はどうする？

72
00:04:34,265 --> 00:04:40,260
28 x 28 x 32 次元に どうやって 縮小する？

73
00:04:40,260 --> 00:04:48,058
32個の 1 x 1 フィルターを使えばできる

74
00:04:48,058 --> 00:04:52,700
詳細に言えば 各フィルターは 1 x 1 x 192 次元だ

75
00:04:52,700 --> 00:04:55,440
なぜなら フィルターのチャンネル数は

76
00:04:55,440 --> 00:04:58,570
入力ボリュームのチャンネル数と合っていないといけないからね

77
00:04:58,570 --> 00:05:08,035
でも 32個のフィルターを使うので 出力は 28 x 28 x 32 ボリュームになる

78
00:05:08,035 --> 00:05:12,850
これが nC を減らす方法の1つだ

79
00:05:12,850 --> 00:05:17,750
一方 プーリング層は nH と nW のみを減らす

80
00:05:17,750 --> 00:05:20,436
ボリュームの高さと幅をね

81
00:05:20,436 --> 00:05:23,510
後で やり方を見るけど 1 x 1 畳み込みのアイデアで

82
00:05:23,510 --> 00:05:28,670
チャンネル数を減らせる だから

83
00:05:28,670 --> 00:05:31,450
ネットワークの計算量を減らせる

84
00:05:31,450 --> 00:05:37,165
ただし 勿論 チャンネル数を192のままにしておきたいなら それもいい

85
00:05:37,165 --> 00:05:41,470
その場合 1 x 1 畳み込みの効果は 単に非線形化をしたことになる

86
00:05:41,470 --> 00:05:45,740
それにより ネットワークに より複雑な関数を学習させることができる

87
00:05:45,740 --> 00:05:52,423
28 x 28 x 192 入力から 28 x 28 x 192 出力をする層を加えることでね

88
00:05:52,423 --> 00:05:54,620
これが

89
00:05:54,620 --> 00:05:58,280
1 x 1 畳み込みが どのように 重要な何かを行うかということだ

90
00:05:58,280 --> 00:06:01,190
そして ニューラルネットワークに 非線形化を加えて

91
00:06:01,190 --> 00:06:04,525
思うように ボリュームのチャンネル数を

92
00:06:04,525 --> 00:06:08,565
減らしたり 元のままにしたりできる

93
00:06:08,565 --> 00:06:13,960
次回では Inception ネットワークを作るのに これが本当に有益であるのが分かるだろう

94
00:06:13,960 --> 00:06:16,860
次のビデオに行こう

95
00:06:16,860 --> 00:06:22,145
今見てきたのは
1 x 1 畳み込み処理が 本当に

96
00:06:22,145 --> 00:06:26,255
重要な処理を行い

97
00:06:26,255 --> 00:06:28,640
ボリュームのチャンネル数を減らしたり 保ったり

98
00:06:28,640 --> 00:06:31,270
やりければ 増やす事もできる ということだ

99
00:06:31,270 --> 00:06:33,077
見ての通り、ここある。

100
00:06:33,077 --> 00:06:36,140
次のビデオでは これが使われて

101
00:06:36,140 --> 00:06:39,670
Inception ネットワークを作るのに一役買っているのが分かるだろう
次のビデオに進もう