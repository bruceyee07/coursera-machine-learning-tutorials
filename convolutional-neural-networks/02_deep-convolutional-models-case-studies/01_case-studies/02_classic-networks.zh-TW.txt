在這段影片中, 你將開始學習一些 經典的神經網路架構，從 LeNet-5 到 AlexNet, 然後是 VGGNet, 我們來看看 這個是 LeNet-5 架構 您從一個影像開始 32乘32乘1 這個LeNet-5 的目標是辨識手寫數字 或許是一個數字影像像這樣 而 LeNet-5 訓練在灰階的影像上 也就是為什麼 32乘32乘1 這個神經網路架構實際上 相當類似於您上個禮拜看過的例子 第一步 您用一組六個 5乘5 過濾器，跨步為 1, 因為您用了 6 個過濾器，您最終會有 28乘28乘6 的輸出 而跨步 1, 沒有填充 影像維度會從 32乘32 降為 28乘28 然後 LeNet 神經網路應用池層 回到這份論文寫作時 人們大多使用平均池層 如果您在現在建置 您或許會使用最大池化 不過在這例子 您使用平均池層，過濾器的寬度為 2, 跨步為 2 這樣您會降低維度 高度跟寬度都減半 我們現在會是 14乘14乘6 容積 我猜我畫的高度跟寬度不符合比例原則 技術上而言，如果您畫的容積符合比例 高度跟寬度應該要減半 接下來，您應用另一個卷積層 這一次，您用了一組 16 個過濾器 5乘5 ，您最終會有 16 個通道在下一個容積 回到這篇論文寫作的時間 1998 年 人們實際上不用填充，或者說人們都用有效卷積 也就是每次您應用卷積層 高度跟寬度都會縮水 所以這是為什麼，這裡 您從 14乘14 降為 10乘10 然後加入另一個池層 因此將高度跟寬度減半 最終輸出是 5乘5 而如果您將所有的 5乘5乘16 乘起來 這個乘積是 400 25乘 16 是 400 然後下一層是全連結層，連結所有 這些 400 個節點跟這 120 個節點 所以這是一個全連結層 有時候會另外畫 一層有 400 個節點，我跳過這個步驟 這是一個全連結層，然後另一個全連結層 最後的步驟是使用 這些是基本的 84 個特徵，用來估算一個輸出 我猜您可以畫另一個節點在這裡來做 y-hat 的預估 而 y-hat 有 10 個可能值 對應辨識從 0 到 9 的每一個數字 一個現代的神經網路版本 我們會使用  softmax 層有 10 個分類輸出 雖然回到當初，
 LeNet-5 實際上使用不同的分類器在輸出層 現在已經不用了 這個神經網路
依現在的標準來看算小的 有大約 60,000 個參數 在今天，您通常看到的神經網路 可能從一千萬到一億個參數 也不是不常看到那樣的網路 真的是千倍於這個網路 但當您進入深層網路時您會見到 當您從左到右 高度跟寬度趨向於降低 所以您從 32乘32 到 28 到 14 到 10 到 5，而通道數目則一直增加 它從 1 到 6 到 16 當您進入深層的網路時 另一個模式即使在今天的神經網路
也依然重複著的是 您或許有一個或多個卷積層接著一個池層 然後一個或多個卷積層接著一個池層 然後一些全連結層然後是輸出層 這樣網路層的安排是很常見的 最後這或許只針對於那些想讀論文的人 有一些東西是不同的 這張投影片的其餘部分 我將做一些進階的註解 只針對那些想讀經典論文的人 所有用紅色字寫的 您可以安全地跳過 或許是一個有趣的歷史註解 如果您跟不上也沒關係 實際上如果您讀原來的論文，那時 人們使用 S 形函數跟  Tanh 非線性函數 人們那時候還沒有使用 ReLU 非線性函數 所以如果您看論文的話，
您會看到 s形函數 跟 tanh 被引用 然後也有一些有趣的方式有關 這些網路如何連結，用現在的標準來看 舉個例，您見到如果您有  nh 乘 nw 乘 nc 網路 有 nc 個通道，您會使用  f乘f乘nc 維度的過濾器 每一個過濾器都看著所有的通道 但那時，電腦相對比較慢 為了節省計算跟一些參數 原本 LeNet-5 有一些瘋狂複雜的方式 不同的過濾器看管不同的輸入通道 論文裡談到了有關這些的細節 但在現在的建置中已經不需要這樣的複雜 最後一件事我猜那時並沒有作對的一件事 就是原本的 LeNet-5 在池層後應用非線性 我想它實際上使用非線性  S 型函數在池層之後 如果您讀這份論文 這個應該算是難的一份比起 下幾個影片要談的 下一個應該比較容易開始 這張投影片大部分的觀念
我用的是論文的第二跟第三節 論文後面幾節談到其他觀念 它談論到有關圖形轉換網路 在今日不常用到 如果您試著讀這份論文 我建議您聚焦在第二節談到有關架構部分 或許很快看過第三節 有一些實驗跟結果，還相當有趣 第二個神經網路的例子我要展示的是 AlexNet 以 Alex Krizhevsky 為名，
是第一位作者論文描述這種方式 其他的作者是 Ilya Sutskever 跟 Geoffrey Hinton 所以 AlexNet 的輸入是 227乘227乘3 影像 如果您讀論文的話 論文指的是 224乘224乘3 影像 但如果您看這些數字 我想這些數字應該是 227乘277 才有道理 然後第一層使用一組 96 個
11乘11 的過濾器，跨步為 4 因為用了大的跨步 4 維度降低到了 55乘55 大約，降低了將近四分之一，因為大的跨步 然後應用了一個最大化池層 3乘3 過濾器 所以 f 等於 3 跨步為 2 這個降低容積到 27乘27乘96 然後做一個 5乘5 相同卷積 一些填充，所以最終是 27乘27乘276 (應該是 256) 再一次最大池化，降低高度跟寬度到 13 然後另外一個相同卷積，所以相同填充 所以是 13乘13乘384 過濾器 然後再一次 3乘3 相同卷積，給您這個 然後再一次 3乘3 相同卷積，給您這個 再一個最大池化，降低到  6乘6乘256 如果您將這些數字乘起來 6乘6乘 256 會是 9216 所以我們會攤平這些成為 9216 個節點 然後最後，會有一些全連結層 最後，使用 softmax 來輸出 也就是 1000 種分類之一 這個神經網路實際上跟 LeNet 有很多的相似性 但是大很多 儘管前面投影片的 LeNet-5 有大約 60,000 個參數 這個 AlexNet 大約有 6 千萬個參數 實際上它們可以 用相當類似的建構基石是因為 它用了相當多的隱藏單元跟訓練在大量的資料上 它們使用 ImageNet 的資料集 讓它可以有很好的績效 另外一方面這個架構讓它比 LeNet 更好是因為用了 ReLU 啟動函數 再提醒一次，如果您讀了這份論文 會有一些進階的細節 如果您不讀這些論文的話您不用擔心，一個是 當這份論文寫作時 GPU 還比較慢 它用了複雜的方式在兩顆 GPU 上訓練 基本的觀念是 很多層實際上拆開來在不同的  ＧＰＵ 上，有一些 考慮到當兩顆 GPU 互相溝通的方式 論文也同時 在原本 AlexNet 架構同時也用了一些其他種的層 稱為 區域反應正規化 (Local Response Nomalization) 這類的網路層不常使用 所以我並沒有提到 但區域反應正規化(LRN)基本觀念是 如果看一個建構基石 上面的其中一個容積 為了討論方便起見，用這個 13乘13乘256 區域反應正規化 做的是，當您看一個位置 一個同一高度跟寬度的位置 看穿所有的通道 看這些 256 個數字，正規化它們 區域反應正規化的動機在於 在這 13乘13 影像中的每一個位置 或許您不要太多的神經元有太高的啟動值 但後來，很多研究學者發現
這樣做並沒有太大幫助 這是為什麼我用紅色來標這個想法 因為對您而言這個較不重要 實際上，我也不會使用 區域反應正規化在神經網路上 如果您對深度學習的歷史有興趣 我想即使在 AlexNet 之前 深度學習開始在語音辨識及其他區域獲得青睞 但真的從這份論文開始，說服很多 電腦視覺社群認真地看待 深度學習，說服他們
深度學習的確可以在電腦視覺成功 然後長大茁壯不只影響到 電腦視覺，更超乎電腦視覺 而如果您試著自己讀這些論文 這個課程並不要求這樣做 但如果您想自己讀這些論文 這份論文算是容易的，或許可以看一下 既然 AlexNet 有相對複雜的架構 就會有很多的超參數，對吧？ 您要所有這些數字 Alex Krizenvsky 跟其他作者要能提供 讓我來展示第三種，這影片的最後一個例子
稱為 VGG 或者 VGG-16 網路 VGG-16 值得注意的是，他們說 與其使用這麼多的超參數 我們用簡單許多的網路，您只要聚焦在卷積層 就只是 3乘3 過濾器，
跨步為 1, 都使用相同填充 讓您所有的最大池層都是 2乘2，跨步為 2 所以，一件很棒的事在 VGG 網路是真的簡化了神經網路的架構 讓我們看看這個架構 您開始一個影像，然後前兩層是卷積層 也就是這些 3乘3 過濾器 而在前兩層用 64 個過濾器 您最終會有 224乘224 
因為使用了相同卷積，然後有 64 個通道 因為 VGG-16 相對於言是比較深的深層網路 我們就不畫所有的容積 所以這小張圖，描述的是我們先前 畫的 224乘224乘3 然後卷積結果，我猜會是 224乘224乘64 畫成這樣的深度容積 然後另一層的結果是 224乘224乘64 所以這個 conv64 乘 2 指的是
您使用兩個卷積層有 64 個過濾器 就像我之前提過的 過濾器永遠都是 3乘3  跨步為 1，都是使用相同卷積 所以與其畫出所有容積 我只用文字來代表這個網路 接下來，用一個池層 所以池層會降低 我想從 224乘224 降到多少？ 降到 112乘112乘64 然後有一些卷積層 這個代表它使用 128 個過濾器，因為是相同卷積 我們來看看新的維度是？ 它會是 112乘112乘128 然後是池層，您可以發現到新的維度是這樣 然後三個卷積層 256 過濾器，到池層，然後一些卷積層 池層，一些卷積層，池層 然後它用最後這個 7乘7乘512 到全連結層 這全連結層有 4,096 個 單元，然後用 softmax 輸出一千個類別 順道提一下， VGG-16 的 16 指的是這裡有 16 層裡有權重 這是相當大的網路 這個網路全部有 一億三千八百萬個參數 而即使在今天也是很大的網路 但 VGG-16 的簡單架構讓它很吸引人 您可以看出來它的架構很一致 一些卷積層接著一個池層 來降低高度跟寬度 池層用來降低高度跟寬度 您看到一些這種型態 同時，如果您看卷積層的過濾器個數 這裡是64個過濾器，然後加倍到 128，
加倍到 256，加倍到 512 而我猜作者想說 512 已經足夠大，沒有再加倍 但這樣大約每次加倍 或者說每次卷積層加倍的方式 在設計這個網路架構時
是另一個簡單的規則 所以我想這種相對一致 的架構對於研究人員很具有吸引力 最主要的缺點是它 是相當大的網路，如果說您需要訓練這麼多的參數 而如果您讀論文的話 有時候您會見到人們談 VGG-19 比這個網路還要大 您可以在論文的細節看到在 底端被 Karen Simonyan 跟 Andrew Zisserman 引用 但因為 VGG-16 幾乎跟 VGG-19 一樣好 很多人會使用 VGG-16 我最喜歡的一件事是 這種模式 當您網路越深，高度跟寬度越低 它每次降低一半在 池層，而通道的數目增加 這裡幾乎是一倍的增加
每次您用一組新的卷積層 所以很系統化地讓這個比率降低，同時讓這個增加 我想這份論文從這個觀點看很吸引人 所以這就是經典的三個架構 如果要的話，您可以讀這一下這些論文 我建議從  AlexNet 開始，然後是  VGG net 論文 然後 LeNet 論文有點難讀 但是是只要您讀過，會是相當經典 接下來，超越這些經典網路，看一些更進階的 更強大的神經網路架構，我們繼續前進