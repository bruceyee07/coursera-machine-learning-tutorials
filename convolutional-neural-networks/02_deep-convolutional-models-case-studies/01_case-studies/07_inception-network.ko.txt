이번 비디오에서는, 인셉션망의 기본적인 빌딩블럭들을 보았습니다. 이번 강좌에서는, 이러한 빌딩블럭을 결합하여 고유한 인셉션망을 구축하는 방법을 알아보겠습니다. 인셉션 모듈은 이전 레이어로부터 나온 액티베이션이나 아웃풋을 인풋으로 사용합니다. 논의를 위해, 우리의 이전 강의에서 본 28 x 192 x 192를 사용합시다. 심도 있게 작업했던 이 예제는 1 x 1 다음에 5 x 5 레이어로 이어져 있습니다. 따라서, 1 x 1 은 16개의 채널을 가지고 있고, 그러면, 5 x 5는 28 x 28의 32개 채널을 아웃풋 할 것입니다. 이것은 이전 강의의 마지막 슬라이드에서 작업 한 예입니다. 그런 다음 3 x 3컨볼루션에 계산을 저장하려면 
여기에서도 같은 작업을 수행 할 수 있습니다. 그리고 나서 3 x 3 는 28 x 28 x 80 을 아웃풋합니다. 그리고 나면, 아마도 1 x 1 
컨볼루션도 고려하고 싶어 질 것입니다. 1 x 1 컨볼루션 뒤에 나오는 또 다른 
1 x 1 컨볼루션은 작업할 필요가 없습니다. 따라서, 하나의 작업 단계만 있는 것이죠. 
그리고 이것이 28 x 28 x 64를 아웃풋한다고 가정 해 봅니다 그리고 마지막으로 pooling 레이어입니다. 이제 여기서 우리는 재미있는 것을 해 볼 텐데요, 마지막에 이러한 모든 아웃풋을 연결하기 위해 동일한 유형의 패딩을 pooling에 사용하므로 아웃풋 높이와 너비는 여전히 28 x 28입니다 따라서 이 출력을 여기 다른 출력과 연결할 수 있습니다. max pooling을 수행하는 경우 
동일한 패딩을 사용하더라도 3 x 3의 필터가 맞습니다. 여기 아웃풋은 28 x 28 x 192 입니다. 여기에 인풋한 것과 동일한 수의 채널과 동일한 깊이를 갖게 됩니다. 그래서 이것은 많은 채널을 
가지고 있는 것처럼 보입니다. 그래서 우리가 할 일은 채널의 수를 줄이기 위해 1 x 1 컨볼루션 강의에서 본 대로 1 x 1 컨볼 레이어를
하나 추가하는 것입니다. 그러면 이 28 x 28 x 32 로 줄일 수 있죠. 말하자면, 여러분이 하는 방식은 1 x 1 x 192 차원의 32개의 필터를 사용하는 것입니다. 그래서 아웃풋 다이멘션이 32로 
줄어든 채널의 수가 되는 것입니다. 이렇게 해야 최종 아웃풋에서 모든 채널을 차지하고 있는 pooling 레이어로 되지 않는 것입니다. 마지막으로, 이 모든 블록을 
가져 와서 채널 연결을 수행하고 이 64 + 128 + 32 + 32를 연결하면됩니다 그리고 이걸 다 더하면 28 × 28 × 256 차원 아웃풋이 됩니다. 채널 연결이란 이전 강의에서 본 
대로 블록들을 연결하는 것입니다. 그래서 이것은 하나의 인셉션 모듈입니다. 그리고 인셉션망이 하는 일은 이 모듈들을 함께 모으는 것입니다. 다음은 Szegety et al.의 
논문에서 가져온 인셉션 그림입니다. 그리고 이것에서 여러 번 
반복되는 블록들을 발견하게 됩니다. 어쩌면 이 그림은 정말 복잡해 보일 수도 있지만, 그 블록 중 하나를 보시면, 이 블록은 기본적으로 이전 슬라이드에서 본 인셉션 모듈입니다. 그리고 제가 설명하지 않을 세부 
사항임을 전제로 해서, 이것은 다른 인셉션 블록입니다. 여기에는 높이와 너비의 크기를 변경하기위한 max pooling 레이어가 추가로 있습니다 하지만 또 다른 인셉션 블록이 있고, 그리고 높이와 너비를 바꿀 수 
있는 또 다른 max pooling이 있습니다. 하지만, 기본적으로 또 다른 인셉션 블록이 있고, 그 인셉션망은 네트워크의 여러 위치에 대해 반복적으로 학습 한 많은 블록들입니다. 이전 슬라이드의 인셉션블록을 이해하셨다면 인셉션망도 이해할 수 있습니다. 원래의 연구 논문을 읽어보면 이 
인셉션망에 대한 마지막 세부사항이 나오는데요, 원래의 연구 논문을 읽어보면 이 
인셉션망에 대한 마지막 세부사항이 나오는데요, 이게 바로 제가 여기에 
덧붙여놓은 추가적인 곁가지들입니다. 자, 그래서 이 곁가지들은 무엇을 합니까? 음, 네트워크의 마지막 몇 레이어는 완전 연결 레이어이고, 그 뒤에 예측을 해보려 하는 softmax 레이어가 있습니다. 이 곁가지들은 숨겨진 레이어를 가져 와서 예측하는 데에 그 레이어를 사용하는 것입니다. 그래서 이것은 실제로 softmax 
아웃풋이며, 이것 역시 그렇습니다, 그리고이 다른 쪽 곁가지는 숨겨져 있는 레이어를 취해서, 
몇몇 레이어 즉, 완전 연결 레이어를 통과합니다. 그리고 softmax가 아웃풋 라벨이 
무엇인지를 예측하려고 시도했습니다. 이것을 인셉션망의 또 다른 
세부 사항으로 생각해야 합니다. 이것을 인셉션망의 또 다른 
세부 사항으로 생각해야 합니다. 하지만 그것이 하는 일은 히든유닛이나 중간 레이어에서 조차 컴퓨터의 피처들이 이미지의 아웃풋 예측능력이 그리 나쁘지만은 않게 도와주는 것입니다. 그리고 이것은 인셉션망에 규칙적인 영향을 미치는 것으로 보이며 이 네트워크가 초과 적용되는 것을 방지합니다. 그런데, 이 특정 인셉션망은 Google에서 일하는 
작가들에 의해 만들어졌습니다. 그들은 이걸 이렇게 쓰고, GoogLenet이라고 
불렀는데요, 이는 여러분이 이전 강의에서 배우기도 했던 LeNet network 에 경의를
 표하기 위함이었다고 합니다. 그래서 저는 딥러닝 단체가 매우 협조적이며, 그래서 저는 딥러닝 단체가 매우 협조적이며, 서로의 작업에 대한 강한 건강한 존중이 있는 점이 정말 좋다고 생각합니다 마지막으로 재미있는 사실 하나 있습니다. 인셉션망이라는 용어는 어디에서 왔을까요? 우리가 더 깊게 알아볼 필요가 있기 때문에, 
이 인셉션 논문은 이 사진을 인용하고 있습니다. 이 URL은 실제 논문에서 사용된 참조이며, 이 이미지로 연결해줍니다. 인셉션 이라는 제목의 이 영화를 보셨다면, 아마도이 meme이 의미가 있을지 모르지만, 저자들은 이 이미지를 실제로 
더 심층 신경망을 구축해야 할 필요성에 대한 동기로서 인용하고 있습니다. 그리고 이것이 그들이 인셉션 
아키텍처를 제시 한 방법입니다. 연구 논문이 인용문에서 인터넷 
meme를 인용하는 것은 그리 흔한 일은 아닌데요, 하지만 이 경우에는 상당히 효과적이라는 생각이 듭니다. 요약하면, 인셉션 모듈을 이해하면 인셉션망도 이해할 수 있습니다. 인셉션망은 대체로 네트워크 전체에 
걸쳐 여러 번 반복되는 인셉션모듈입니다. 인셉션망은 대체로 네트워크 전체에 
걸쳐 여러 번 반복되는 인셉션모듈입니다. 인셉션 모듈을 개발 한 이후로 저자와 다른 사람들은 이 모듈을 
기반으로 다른 버전을 만들었습니다. 따라서 인셉션 알고리즘의 최신 
버전에 대한 연구 논문이 있으며, Inception V2, Inception V3, 
Inception V4와 같은 최신 버전의 일부를 사람들이 사용하는 것을 볼 수 있을 것입니다. 또한 skipping connection을 가진 레지던트 아이디어와 결합 
된 인셉션버전이 있으며 이것이 때로는 더 잘 
효과적으로 작동합니다. 그러나 이러한 모든 
변형은 인셉션 모듈과 그것들 함께 쌓는 것에 대한 강의에서 배운, 기본 기념에 근거해 구축됩니다. 이 강의들로서, 여러분은 인셉션 논문이나 후속 변형들을 설명하는 다른 논문들도 후속 변형들을 설명하는 다른 논문들도 자, 여기까지입니다. 전문화된 신경망 아키텍처를 공부했습니다. 다음 강의에서는, 여러분 자신의 컴퓨터 
비전 시스템을 만드는 이 알고리즘을 어떻게 사용할 수 있을지에 대해 
실제적인 조언을 드리면서 시작해보고 싶습니다. 다음 비디오로 넘어가겠습니다.