1
00:00:00,000 --> 00:00:02,970
ResNet은 왜 잘 작동되는 걸까요?

2
00:00:02,970 --> 00:00:07,830
ResNet 이 왜 잘 작동되는지를 설명하는 예시를 살펴봅시다.

3
00:00:07,830 --> 00:00:12,390
트레이닝 세트에서 그것들이 잘 수행할 수 있게 만드는 여러분의 능력을 다치게 하지 않

4
00:00:12,390 --> 00:00:17,540
점점 더 깊어지게 만들 수 있는 지 그 방법적인 측면에서 보도록 하죠.

5
00:00:17,540 --> 00:00:22,185
이 일련의 과정 중 세 번째 강의에서 여러분이 이해했기를 바라는데요,

6
00:00:22,185 --> 00:00:25,845
트레이닝 세트에서 잘 수행하는 것이 그 다음에 있는

7
00:00:25,845 --> 00:00:29,800
홀드아웃 와 테스트 세트를 잘 수행할 수 있는 조건입니다.

8
00:00:29,800 --> 00:00:33,570
트레이닝 세트에서 ResNet을 잘 훈련시키는 것은

9
00:00:33,570 --> 00:00:38,650
그것을 향해 제대로 된 첫 걸음을 내딛는 것이죠. 예시를 하나 보시죠.

10
00:00:38,650 --> 00:00:43,910
마지막 강의에서 보셨던 것은 네트워크를 더 깊이 만들면

11
00:00:43,910 --> 00:00:49,195
트레이닝 세트에서 잘 수행할 수 있게 하려다가 네트워크를 학습시키는 능력에 손상을 줄 수 있다는 것이었습니다.

12
00:00:49,195 --> 00:00:53,420
그런 이유로 때로는 지나치게 깊은 네트워크는 원치 않게 되는 것이죠.

13
00:00:53,420 --> 00:00:58,555
그러나 여러분이 ResNet를 학습시킬 때에는 이 말은 사실이 아니거나 적어도 사실일 가능성이 매우 적습니다.

14
00:00:58,555 --> 00:01:01,365
예시를 통해서 보시죠.

15
00:01:01,365 --> 00:01:04,980
X가 큰 신경망에 입력되어 있고

16
00:01:04,980 --> 00:01:10,365
아웃풋으로 액티베이션 a[1]을 가지고 있다고 가정해봅시다.

17
00:01:10,365 --> 00:01:14,080
이 예제에서 신경망을 조금 더 깊게 만들고자

18
00:01:14,080 --> 00:01:19,150
수정한다고 가정 해 봅시다.

19
00:01:19,150 --> 00:01:20,755
따라서 동일한 큰 NN을 사용하면

20
00:01:20,755 --> 00:01:23,035
이 아웃풋은 a[1]이 되고,

21
00:01:23,035 --> 00:01:28,240
이 네트워크에 레이어를 추가적으로 몇 개 더할 것입니다.

22
00:01:28,240 --> 00:01:33,635
그리고 나서 여기에 레이어를 하나 더 추가하고, 또 다른 레이어는 여기에 둡시다.

23
00:01:33,635 --> 00:01:38,195
그럼 이제 아웃풋은 a[l + 2]가 되는군요.

24
00:01:38,195 --> 00:01:40,930
이걸 추가된 shortcut을 이용해서

25
00:01:40,930 --> 00:01:46,060
ResNet 블럭, 즉 잔류블럭으로 만들어봅시다.

26
00:01:46,060 --> 00:01:47,380
토론을 위해서

27
00:01:47,380 --> 00:01:52,690
이 네트워크 전반에 걸쳐 밸류 액티베이션 함수를 사용한다고 가정 해 봅시다.

28
00:01:52,690 --> 00:01:58,330
모든 액티베이션 0보다 크거나 같을 것입니다.

29
00:01:58,330 --> 00:02:01,955
인풋 X에 가능한 예외사항이 있을 수 있습니다

30
00:02:01,955 --> 00:02:07,685
밸류 액티베이션 아웃풋 숫자들은 0 또는 양수이기 때문입니다.

31
00:02:07,685 --> 00:02:10,510
자, [l + 2]가 무엇으로 될지 살펴 봅시다.

32
00:02:10,510 --> 00:02:15,025
이전 강의에서 나왔던 expression을 똑같이 써보면,

33
00:02:15,025 --> 00:02:21,715
a[l + 2] = g(z [l + 2] + a [l]) 이 됩니다

34
00:02:21,715 --> 00:02:26,505
a [l]을 덧셈하는 것은

35
00:02:26,505 --> 00:02:31,750
우리가 추가했던 skip connection의 short circle에서 나온 것입니다

36
00:02:31,750 --> 00:02:33,300
이걸 확장시키면,

37
00:02:33,300 --> 00:02:34,710
이것은 g(w[l + 2]a[l + 1]

38
00:02:34,710 --> 00:02:43,570
+ b[l + 2] 이고

39
00:02:43,570 --> 00:02:48,810
따라서, 여기 z[l + 2]는 이것과 같고, 여기에 a[1]를 더해줍니다.

40
00:02:48,810 --> 00:02:53,450
만약 여러분이 K의 방법으로서 L2 일반화를 사용하고 있다면

41
00:02:53,450 --> 00:02:58,435
그것은 w[l + 2]값이 줄어드는 경향이 있을 것이다,

42
00:02:58,435 --> 00:03:02,540
K에서 B로 적용하고 있다면, 이것 또한 이것을 줄어들게 할 것입니다.

43
00:03:02,540 --> 00:03:06,665
실제로 여러분이 K에서 B로 적용을 해보기도 하고 하지 않기도 할 테지만,

44
00:03:06,665 --> 00:03:12,050
W는 정말로 우리가 주목해야 할 핵심포인트 입니다.

45
00:03:12,050 --> 00:03:16,175
w[l + 2] =0 이고,

46
00:03:16,175 --> 00:03:20,975
토론을 위해, B[ㅣ + 2]=0 라고 가정해봅시다.

47
00:03:20,975 --> 00:03:25,820
그러면 이것들은 0이 되기 때문에, 이 부분은 없어질 것입니다.

48
00:03:25,820 --> 00:03:27,995
g(a[l])은

49
00:03:27,995 --> 00:03:36,805
a[l]과 같아질 것입니다. 왜냐하면 밸류 액티베이션 함수를 사용하고 있다고 가정했기 때문이죠.

50
00:03:36,805 --> 00:03:39,635
모든 액티베이션 이 음수이면,

51
00:03:39,635 --> 00:03:43,930
g(a[l]) 은 음의 아닌 양에 적용된 값이 됩니다.

52
00:03:43,930 --> 00:03:46,800
따라서 다시 a[l]을 얻게 됩니다.

53
00:03:46,800 --> 00:03:56,270
이는 레시듀얼 블럭이 항등 함수를 배우는 것은 쉽고

54
00:03:56,270 --> 00:04:03,515
여기 이 skip connection이 있으므로 a[l + 2]= a[l] 임을 얻어내는 것은 쉽다는 걸 보여줍니다.

55
00:04:03,515 --> 00:04:08,060
이는 이 두 추가적인 레이어가 없는 더 단순한 네트워크에서든

56
00:04:08,060 --> 00:04:11,885
신경망에서 있는 이 두 레이어를 더하던 지간에

57
00:04:11,885 --> 00:04:16,205
이것이 신경망의 능력을 저하시키지 않는다는 것을 의미합니다.

58
00:04:16,205 --> 00:04:20,930
왜냐하면 항등 함수를 학습하는 것을 매우 쉽기 때문입니다.

59
00:04:20,930 --> 00:04:26,305
여기 이 두 레이어를 더해서 a[l] 를 a[l + 2]으로 똑같이 복사해주만 하면 되니까요.

60
00:04:26,305 --> 00:04:30,250
이게 바로 두 개의 추가 레이어를 더하고

61
00:04:30,250 --> 00:04:34,910
이 잔류블럭을

62
00:04:34,910 --> 00:04:40,392
여기 큰 신경망의 중간이나 끝 어딘가에 더해도 수행능력은 저해되지 않는 이유인 것입니다.

63
00:04:40,392 --> 00:04:43,910
하지만 물론 우리의 목표는 수행능력을 떨어뜨리지 않는 게 아니고

64
00:04:43,910 --> 00:04:48,650
수행능력을 도와주는 것입니다. 그러면 이 모든 히든 유닛이,

65
00:04:48,650 --> 00:04:51,770
유용한 것을 학습한다면,

66
00:04:51,770 --> 00:04:55,745
여러분이 항등 함수를 학습하는 것 보다 훨씬 더 잘 할 수 있을 것입니다.

67
00:04:55,745 --> 00:05:00,970
skip connection의 잔류가 없는

68
00:05:00,970 --> 00:05:03,650
매우 심한 심층 신경망의 평범한 망에서 오류가 날 수 있는 건

69
00:05:03,650 --> 00:05:06,555
네트워크를 점점 깊게 만들면,

70
00:05:06,555 --> 00:05:10,865
이 네트워크가 항등 함수를 학습하는 파라미터를 선택하게 하기란

71
00:05:10,865 --> 00:05:14,570
사실 매우 어려운 일입니다. 그리고 많은 레이어들이 결국

72
00:05:14,570 --> 00:05:19,025
결과를 더 좋게 하기보다 더 악화시키는 이유이기도 합니다.

73
00:05:19,025 --> 00:05:22,430
레지듀얼 네트워크가 작동하는 주된 이유는

74
00:05:22,430 --> 00:05:26,360
이 추가적인 레이어들이 항등 함수를 학습하는 게

75
00:05:26,360 --> 00:05:30,260
너무 쉽기 때문에, 수행이 저해되는 일이 없고

76
00:05:30,260 --> 00:05:35,450
심지어 더 잘 수행할 것임을 보장받을 수 있기 때문입니다.

77
00:05:35,450 --> 00:05:39,530
적어도 수행능력을 저해하지 않는다는 기준치에서 벗어나는 것은 더 쉽고,

78
00:05:39,530 --> 00:05:44,435
그러면 좋은 수행치는 해상도를 개선할 수 있을 겁니다.

79
00:05:44,435 --> 00:05:46,450
이 편집 본을 통해 논의할 가치가 있는

80
00:05:46,450 --> 00:05:50,570
잔류 네트워크의 또 하나의 세부사항은

81
00:05:50,570 --> 00:05:54,995
우리가 z[l + 2] 와 a[l] 가 같은 차원을 가질 거라고 가정하는 것입니다.

82
00:05:54,995 --> 00:06:01,175
ResNet에서 봐야 할 것은 같은 컨볼루션을 많이 사용해서

83
00:06:01,175 --> 00:06:03,665
이 다이멘션이

84
00:06:03,665 --> 00:06:08,280
이쪽 레이어나 아웃풋 레이어의 다이멘션과 같아지도록 하는 겁니다.

85
00:06:08,280 --> 00:06:12,450
또한 우리가 이 short circle connection 할 수 있도록 해주기도 하죠.

86
00:06:12,450 --> 00:06:15,615
같은 컨볼루션은 다이멘션을 그래도 유지해주고,

87
00:06:15,615 --> 00:06:19,205
여러분이 이 short circle을 수행하거나

88
00:06:19,205 --> 00:06:25,575
두 동일한 다이멘션 벡터의 덧셈을 하는 것을 더욱 쉽게 만들어줍니다.

89
00:06:25,575 --> 00:06:30,635
인풋과 아웃풋이 다른 다이멘션 을 가지는 경우를 예시로 들면,

90
00:06:30,635 --> 00:06:36,420
만약 이게 128 차원이고, Z 니까,

91
00:06:36,420 --> 00:06:40,660
이 a[l]이 256 차원이라면,

92
00:06:40,660 --> 00:06:46,640
추가 매트릭스 즉 Ws를 여기에 더해주십시오

93
00:06:46,640 --> 00:06:54,905
이 예시의 Ws는 256 x 128 차원의 매트릭스가 될 것입니다.

94
00:06:54,905 --> 00:06:59,930
따라서, Ws x a[l] = 256 차원이 되고

95
00:06:59,930 --> 00:07:01,550
이 덧셈은 이제

96
00:07:01,550 --> 00:07:05,300
2개의 256 다이멘션의 벡터 사이에 있는 것입니다. Ws로 할 수 있는 게 몇 가지 있는데요,

97
00:07:05,300 --> 00:07:07,940
우리가 배웠던 파라미터 매트릭스일 수도 있고

98
00:07:07,940 --> 00:07:10,490
고정된 매트릭스일 수도 있습니다.

99
00:07:10,490 --> 00:07:13,220
이 고정되 매트릭스는 a[l]을 취하는 제로 패딩을 실행시킬 수도 있고

100
00:07:13,220 --> 00:07:20,000
256 차원이 되도록 0을 추가할 수도 있습니다. 혹은 이 두 가지 버전 중 그 어느 것이라도 작동할 것입니다.

101
00:07:20,000 --> 00:07:23,650
자 마지막으로, 이미지 상의 ResNet을 살펴봅시다.

102
00:07:23,650 --> 00:07:27,870
이것들은 Harlow의 논문에서 가져온 이미지들입니다.

103
00:07:27,870 --> 00:07:35,465
평범한 망의 예시로서, 이미지를 인풋하고

104
00:07:35,465 --> 00:07:39,155
softmax 아웃풋을 얻을 때까지

105
00:07:39,155 --> 00:07:44,300
많은 컨볼레이어를 쭉 가지는 것입니다.

106
00:07:44,300 --> 00:07:46,010
이걸 ResNet으로 전환시키려면,

107
00:07:46,010 --> 00:07:50,945
이 추가 skip connection을 더하십시오.

108
00:07:50,945 --> 00:07:53,620
몇 가지 세부사항을 언급하려고 합니다.

109
00:07:53,620 --> 00:07:57,744
여기에 3 x 3 컨볼루션이 많이 있습니다. 그리고 이 대부분은

110
00:07:57,744 --> 00:08:01,435
똑같은 3 x 3 컨볼루션이죠.

111
00:08:01,435 --> 00:08:05,990
따라서 여러분을 동일한 차원의 피쳐 벡터를 더하고 있는 것입니다.

112
00:08:05,990 --> 00:08:08,360
따라서 완전 연결 레이어보다,

113
00:08:08,360 --> 00:08:11,820
이것들이 진짜 컨볼루션 레이어들입니다. 하지만 차원들이 유지되고 있는

114
00:08:11,820 --> 00:08:21,360
동일한 컨볼루션이기 때문에, z[l + 2] + a[l] 에서 이 덧셈이 이치에 맞게 되는 겁니다.

115
00:08:21,360 --> 00:08:25,085
앞서 NetRes에서 보았던 것과 유사하게,

116
00:08:25,085 --> 00:08:29,540
상당한 컨볼루션 레이어가 있고,

117
00:08:29,540 --> 00:08:34,410
간헐적으로 pooling layer들도 보입니다, 혹은 pooling 같아 보이는 것들도 있죠.

118
00:08:34,410 --> 00:08:36,170
이런 것들이 생겨날 때마다,

119
00:08:36,170 --> 00:08:42,200
이전 슬라이드에서 보았던 차원을 조정해야 할 필요가 있습니다.

120
00:08:42,200 --> 00:08:44,815
Ws 매트릭스를 할 수 있고,

121
00:08:44,815 --> 00:08:46,955
이 네트워크에 보이는 대로

122
00:08:46,955 --> 00:08:50,724
컨볼, 컨볼, 컨볼, pool, 컨볼, 컨볼, 컨볼, pool 이런 식으로 되어있습니다.<unknown>

123
00:08:50,724 --> 00:08:52,220
그리고 마지막에는

124
00:08:52,220 --> 00:08:56,915
softmax를 이용하여 예측을 하는 완전 연결 레이어가 있습니다.

125
00:08:56,915 --> 00:08:58,340
자, 여기까지가 ResNet 에 대한 것입니다.

126
00:08:58,340 --> 00:09:02,060
신경망을 사용하는 것에는

127
00:09:02,060 --> 00:09:05,955
아주 흥미로운 아이디어가 있습니다.

128
00:09:05,955 --> 00:09:07,490
바로 1 x 1 필터, 1 x 1 컨볼루션입니다.

129
00:09:07,490 --> 00:09:10,280
따라서, 1 x 1 컨볼루션을 사용할 수 있게 될 것입니다.

130
00:09:10,280 --> 00:09:12,220
다음 강의를 한 번 보시죠.