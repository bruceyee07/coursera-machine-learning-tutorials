1
00:00:00,000 --> 00:00:03,390
とても とても 深いニューラルネットワークは 訓練するのが難しい

2
00:00:03,390 --> 00:00:07,215
なぜなら 勾配消失や発散 問題があるためだ

3
00:00:07,215 --> 00:00:08,790
このビデオでは

4
00:00:08,790 --> 00:00:12,150
スキップ コネクションについて学ぶ
それは １つの層からの活性を 

5
00:00:12,150 --> 00:00:17,498
突然 ニューラルネットワークの別のもっと深い層に与える

6
00:00:17,498 --> 00:00:22,600
そして それを使うと ResNetができて とても とても 深いディープなネットワークを訓練できるようになる

7
00:00:22,600 --> 00:00:26,865
時折 ネットワークは 100層を超える
見ていこう

8
00:00:26,865 --> 00:00:30,390
ResNet は 残差(Residual)ブロック と呼ばれるものから作られている

9
00:00:30,390 --> 00:00:33,185
最初に それが何なのか説明しよう

10
00:00:33,185 --> 00:00:35,370
ここに ２層のニューラルネットワークがある

11
00:00:35,370 --> 00:00:38,005
層 a[l] における 活性から始まり

12
00:00:38,005 --> 00:00:43,940
それから a[l+1] そして 2層後の活性 a[l+2]

13
00:00:43,940 --> 00:00:48,798
この計算ステップを追ってみよう
a[l] があり

14
00:00:48,798 --> 00:00:54,459
それから 最初にやるのは これを線形処理することだ

15
00:00:54,459 --> 00:00:57,660
それは この方程式で決まる

16
00:00:57,660 --> 00:01:01,690
そして a[l] から z[l+1] を計算する

17
00:01:01,690 --> 00:01:07,975
重み行列を掛けて バイアス ベクトルを足すことでね

18
00:01:07,975 --> 00:01:17,945
その後 ReLU 非線形を適用し a[l+1] を得る

19
00:01:17,945 --> 00:01:24,750
それは この方程式だ
a[l+1] は g(z[l+1]) だ

20
00:01:24,750 --> 00:01:26,280
それから 次の層では

21
00:01:26,280 --> 00:01:30,540
この線形ステップを再び行い

22
00:01:30,540 --> 00:01:33,432
この式で決まるやつだ

23
00:01:33,432 --> 00:01:38,040
これは 左のと 殆ど同じだ

24
00:01:38,040 --> 00:01:43,890
そして 最後に 別のReLU処理を適用する

25
00:01:43,890 --> 00:01:52,105
この式で計算される
g() は ReLU非線形だ

26
00:01:52,105 --> 00:01:56,880
そして これで a[l+2] を得る

27
00:01:56,880 --> 00:01:57,900
言い換えると、

28
00:01:57,900 --> 00:02:03,035
a[l] から a[l+2] へ流れる情報は

29
00:02:03,035 --> 00:02:07,455
これらのステップを全て経由する必要がある

30
00:02:07,455 --> 00:02:13,140
この一連の層の 本道だ

31
00:02:13,140 --> 00:02:14,550
残差(residual)ネットでは

32
00:02:14,550 --> 00:02:16,900
これに変更を加える

33
00:02:16,900 --> 00:02:18,495
a[l] を取り

34
00:02:18,495 --> 00:02:22,805
先送りする それをコピーして

35
00:02:22,805 --> 00:02:26,200
ニューラルネットワークのもっと先 ここへ

36
00:02:26,200 --> 00:02:28,860
そして 単に a[l] を足す

37
00:02:28,860 --> 00:02:34,080
非線形 ReLU非線形を適用する前に

38
00:02:34,080 --> 00:02:37,730
これは ショートカット だ

39
00:02:37,730 --> 00:02:40,725
本道に沿わずに

40
00:02:40,725 --> 00:02:43,335
a[l] からの情報は 今度は

41
00:02:43,335 --> 00:02:46,910
ショートカットを流れ ニューラルネットワークに深く入って行く

42
00:02:46,910 --> 00:02:49,680
そして これが意味するのは
この最後の式は

43
00:02:49,680 --> 00:02:52,760
消えてなくなり その代わり

44
00:02:52,760 --> 00:03:00,810
a[l+2] は ReLU非線形 g に 前と同じく z[l+2] を適用させるけど

45
00:03:00,810 --> 00:03:02,830
今度は a[l] を加える

46
00:03:02,830 --> 00:03:05,515
ここに加えた a[l] が

47
00:03:05,515 --> 00:03:07,355
残差(residual)ブロックを作る

48
00:03:07,355 --> 00:03:11,070
それと 絵で 上にあるこの絵を

49
00:03:11,070 --> 00:03:15,945
ここに 追加のショートカットを描いて 変更する

50
00:03:15,945 --> 00:03:20,805
ここの２番目の層の箱に入るように

51
00:03:20,805 --> 00:03:26,220
なぜなら ショートカットは ReLU非線形の前に 足されるからだ

52
00:03:26,220 --> 00:03:27,570
それから ここの各ノードは

53
00:03:27,570 --> 00:03:30,560
線形関数に掛けられて それから ReLUを掛けられるけど

54
00:03:30,560 --> 00:03:34,320
a[l] は 線形関数の後 ReLUの前に 挿入される

55
00:03:34,320 --> 00:03:37,815
時々 "ショートカット"という名前ではなく

56
00:03:37,815 --> 00:03:40,485
"スキップ コネクション"という言葉を聞くことがある

57
00:03:40,485 --> 00:03:44,835
それは a[l] が １層スキップしている もしくは

58
00:03:44,835 --> 00:03:51,090
ほぼ２層スキップしているようなものだからだ
ニューラルネットワークの深い方へ情報を処理するのにね

59
00:03:51,090 --> 00:03:54,030
ResNetの発明者は？

60
00:03:54,030 --> 00:03:55,950
Kaiming He に Xiangyu Zhang

61
00:03:55,950 --> 00:03:58,925
Shaoqing Ren に Jian Sun だろう

62
00:03:58,925 --> 00:04:02,010
彼らが見つけたのは 残差ブロックを使うことで

63
00:04:02,010 --> 00:04:05,920
より深いニューラルネットワークを訓練できるようになるということだ

64
00:04:05,920 --> 00:04:10,785
そして ResNetの作り方は
この様な 残差ブロックを 多く使い

65
00:04:10,785 --> 00:04:15,695
積み重ねて 深いネットワークを作るんだ

66
00:04:15,695 --> 00:04:18,150
じゃ このネットワークを見てみよう

67
00:04:18,150 --> 00:04:19,730
これは Residual Network じゃない

68
00:04:19,730 --> 00:04:22,950
これは 平坦な(プレーン)ネットワークと呼ばれる

69
00:04:22,950 --> 00:04:26,830
これは ResNet 論文の用語だけど

70
00:04:26,830 --> 00:04:28,675
これを ResNet に変えよう

71
00:04:28,675 --> 00:04:31,050
やることは 

72
00:04:31,050 --> 00:04:36,475
これらの スキップ コネクション を
もしくは ショートカットを こんな風に 加えることだ

73
00:04:36,475 --> 00:04:39,875
よって 各２層が

74
00:04:39,875 --> 00:04:44,710
前のスライドのように 追加の変更をされて

75
00:04:44,710 --> 00:04:49,520
残差ブロックとなる

76
00:04:49,520 --> 00:04:53,770
それで この絵では ５つの残差ブロックが積み上げられている

77
00:04:53,770 --> 00:04:56,565
そして これが Residual Network だ

78
00:04:56,565 --> 00:04:59,615
分かったことがある
もし

79
00:04:59,615 --> 00:05:02,620
標準的な 最適化アルゴリズムを使う場合

80
00:05:02,620 --> 00:05:04,120
勾配降下法や

81
00:05:04,120 --> 00:05:07,340
もっと凝った最適化アルゴリズムを 平坦なネットワークの訓練に使うなら

82
00:05:07,340 --> 00:05:10,270
つまり このような 追加の残差無し

83
00:05:10,270 --> 00:05:14,030
追加のショートカット もしくは スキップ コネクション無しなら

84
00:05:14,030 --> 00:05:18,965
経験的には 層の数が増えるほど

85
00:05:18,965 --> 00:05:21,100
学習誤差は しばらく減った後

86
00:05:21,100 --> 00:05:24,240
また 増え始める傾向がある

87
00:05:24,240 --> 00:05:29,170
理論的には ニューラルネットワークを深くすれば

88
00:05:29,170 --> 00:05:32,935
学習セットに対して どんどん 良くなっていくはずだ

89
00:05:32,935 --> 00:05:35,155
そうそう 理論ではね 理論では

90
00:05:35,155 --> 00:05:37,815
より深いネットワークは より良いと

91
00:05:37,815 --> 00:05:40,435
しかし 実際には

92
00:05:40,435 --> 00:05:42,925
平坦なネットワークでは ResNetではない場合は

93
00:05:42,925 --> 00:05:45,890
とても深い平坦なネットワークでは

94
00:05:45,890 --> 00:05:50,220
最適化アルゴリズムによる学習が難しい

95
00:05:50,220 --> 00:05:51,685
現実はそうなんだ

96
00:05:51,685 --> 00:05:55,865
そんなに深いネットワークを選ぶと 学習誤差は増加しちゃう

97
00:05:55,865 --> 00:06:01,530
しかし ResNetでは 層の数が深くなっていっても

98
00:06:01,530 --> 00:06:06,120
学習誤差を下げ続けるような性能を得ることができる

99
00:06:06,120 --> 00:06:10,030
100層を超えるネットワークを訓練したとしてもね

100
00:06:10,030 --> 00:06:12,820
それから 1000層を超えるネットワークの研究をしている人たちもいる

101
00:06:12,820 --> 00:06:17,845
実際に使われているのは あまり見ないけど

102
00:06:17,845 --> 00:06:20,230
でも これらの活性 Xや

103
00:06:20,230 --> 00:06:24,950
これらの 中間の活性を使って ニューラルネットワークをより深くすることにより

104
00:06:24,950 --> 00:06:30,355
これが 本当に 勾配の消失や発散問題の助けになり

105
00:06:30,355 --> 00:06:31,930
とても深いニューラルネットワークを訓練できるようにする

106
00:06:31,930 --> 00:06:36,220
それほど大きく性能を損なわずに

107
00:06:36,220 --> 00:06:39,370
ある時点で これは平らになる

108
00:06:39,370 --> 00:06:43,090
そして これは とても深い深いネットワークでは そんなに役立たない

109
00:06:43,090 --> 00:06:49,120
しかし ResNetは それでも有効で とても深いネットワークの学習を助ける

110
00:06:49,120 --> 00:06:52,645
さぁ これで どのように ResNetが動くのか分かった

111
00:06:52,645 --> 00:06:55,495
実際 今週のプログラミング演習では

112
00:06:55,495 --> 00:06:59,205
自分で これらのアイデアを実装し 動くのを見てもらう

113
00:06:59,205 --> 00:07:02,350
しかし 次は より良い直観 もしくは より多くの直観を
あなたと共有したい

114
00:07:02,350 --> 00:07:06,160
なぜ ResNetは そんなにうまく行くのかについて

115
00:07:06,160 --> 00:07:07,730
次のビデオに進もう