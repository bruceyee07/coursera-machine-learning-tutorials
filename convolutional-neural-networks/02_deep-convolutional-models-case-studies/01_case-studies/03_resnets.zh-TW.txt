很深，很深的網路很難去訓練因為 資料消失跟梯度爆炸的問題 在這段影片中，您會學到 如何跳過連結，讓您可以用一層的啟動值 突然餵進另ㄧ個在神經網路的更深層 使用這種方式，您會發現
『深度殘差網路』讓您可以訓練很深很深的網路 有時候甚至超過 100 層的網路，我們來看看 深度殘差網路建立在稱為『殘差區塊』上 我們先來描述那是什麼 這裡是神經網路的兩層 開始從 l 層的一些啟動值 a[l] 然後經過 a[l+1] 兩層後的啟動值為 a[l+2] 所以經過這兩層的計算，您有 a[l] 第一件事是您應用這個線性運算在這上面 也就是用這個方程式來控制 所以您從 a[l] 來計算 z[l+1] 使用權重矩陣來相乘在加上偏差向量 接著，您應用 ReLU 非線性來得到 a[l+1] 用這個方程式 a[l+1] 是 g(z[l+1]) 來控制 接著下一層 您再次用線性的步驟 用這個方程式控制 這個相當類似於左邊的方程式 最後，您應用了另一個 ReLU 運算也就是 這個方程式來控制，而 G 會是 ReLU 非線性 而這給您 a[l+2] 換句話說 從 a[l] 流到 a[l+2] 的資訊中 它需要經過所有這些步驟，我將它稱為 在這組網路層中的『主要路徑』 在殘差網路中 我們要改變這些 我們拿這個 a[l] 快速前進，複製它 前進到神經網路的這裡 只是加入這個 a[l] 在應用非線性， ReLU 非線性以前 我將其稱為『捷徑』 與其需要順著主要路徑 這個 a[l] 的資訊可以順著 捷徑到神經網路的更深層 這個意思是在最後這個方程式 去掉，與其我們的輸出 a[l+2] 為 ReLU 非線性 g 應用到 z[l+2] 現在再加上 a[l] 所以加上一個 a[l] 在這裡 造成了一個殘差區塊 在圖形上，您也可以修改上面圖形 畫這個捷徑到這裡 而我們要畫的方式是它進入到這第二層這裡 因為捷徑實際上是在
ReLU 非線性之前加入的 所以每一個節點 有應用線性函數跟 ReLU 函數 所以 a[l] 注入在線性函數之後，在 ReLU 之前 有時候與其用捷徑這個名詞 您也會聽到跳過連結 (skip connection) 這個名詞 而這指的是 a[l] 跳過一層，或者說是跳過 幾乎兩層為了要
在更深層的神經網路上處理資訊 所以深度殘差網路的發明者 也就是  Kaiming He, Xiangyu Zhang Shaoqing Ren, Jian Sun. 他們發現到使用殘差區塊 讓您可以訓練更深層的神經網路 而建置深度殘差網路方式是用很多這樣的殘差區塊 像這樣，一起疊起來變成一個深度網路 我們來看看這個網路 這不是一個殘差網路 這是稱為『一般網路』(plain network) 這個專有名詞用在深度殘差網路論文中 將這個轉為深度殘差網路 您要做的是加入這些 跳過連結或者捷徑連結像這樣 所以每兩層終點會 加入這種改變就像我們在 前面投影片看到的
轉成殘差區塊 所以這個圖形顯示了五個殘差區塊疊在一起 而這就是殘差網路 實際上如果您使用了 您的標準最佳化演算法像是 梯度下降法或者 比較時髦的最佳化演算法來訓練這個『一般網路』 不使用所有這些額外的殘差 不使用所有這些我剛畫的
捷徑或者說跳過連結 經驗上來講，您會發現到當您增加層數時 訓練誤差會趨向於降低 經過一會兒，它又會反轉向上 理論上當您用更深層的神經網路 它應該在訓練集的表現越來越好 對吧，理論上 用更深層的網路應該只會有幫助 但實際上 在一般網路上，非深度殘差網路中 用一個很深的一般網路意思是 您的最佳化演算法越來越難訓練 所以實際上 您的訓練誤差會越來越差
如果您選的網路太深的話 但如果是用深度殘差網路的話，即使層數越來越深 您還是可以有訓練誤差越來越低的效果 即使訓練的網路超過一百層 也一些人實驗一個網路 超過一千層，雖然我覺得還不實用 但拿這些啟動值加入 到中間啟動值讓它可以在神經網路中走得更深 這真的幫助了資料消失跟梯度爆炸的問題 讓您可以訓練 更深層的網路而不會真的失去績效 或許到某一點後，這個會攤平，會平坦出去 但它無法幫助到太深太深的網路 但深度殘差網路有效地幫助到深層網路 所以您看到了深度殘差網路如何作用的簡介 實際上，在這個禮拜的程式練習 您要建置這個觀念
然後自己親眼看它如何作用 在下一段，我想分享您更好的直觀 甚至於為什麼深度殘差網路
作用如此好的直觀 我們往下一段影片邁進