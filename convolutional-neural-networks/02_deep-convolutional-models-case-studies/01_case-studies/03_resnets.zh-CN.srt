1
00:00:00,000 --> 00:00:03,390
太深的神经网络训练起来很难，因为有

2
00:00:03,390 --> 00:00:07,215
梯度消失和爆炸这类问题。

3
00:00:07,215 --> 00:00:08,790
在这个视频中，你将会学到

4
00:00:08,790 --> 00:00:12,150
跳跃连接（skip connection），<br />它能让你从一层中得到激活

5
00:00:12,150 --> 00:00:17,498
并突然把它递给下一层，<br />甚至更深的神经网络层

6
00:00:17,498 --> 00:00:22,600
利用它，你就可以训练<br />网络层很深很深的残差网络(ResNet)。

7
00:00:22,600 --> 00:00:26,865
有时甚至可以超过100层的网络。<br />让我们看看吧。

8
00:00:26,865 --> 00:00:30,390
残差网络（ResNet）<br />是使用了残差结构的网络。

9
00:00:30,390 --> 00:00:33,185
我们先来看看那是什么。

10
00:00:33,185 --> 00:00:35,370
这里有两层神经网络，开始于

11
00:00:35,370 --> 00:00:38,005
a[l]代表第l层中的激活函数

12
00:00:38,005 --> 00:00:43,940
然后到了a[l+1]，两层后是a[l+2]

13
00:00:43,940 --> 00:00:48,798
所以在这个计算步骤里，你先有一个a[l]

14
00:00:48,798 --> 00:00:54,459
之后你做的第一件事是<br />将这个线性运算应用在它上面

15
00:00:54,459 --> 00:00:57,660
是由这个等式得到的。

16
00:00:57,660 --> 00:01:01,690
所以你用a[l]来计算z[l+1]

17
00:01:01,690 --> 00:01:07,975
通过乘一个加权矩阵，并加上一个偏置（Bias）向量。

18
00:01:07,975 --> 00:01:17,945
之后，你应用非线性ReLU来得到a[l+1]。

19
00:01:17,945 --> 00:01:24,750
是通过这个等式，a[l+1] = g(z[l+1])。

20
00:01:24,750 --> 00:01:26,280
然后 在下一层

21
00:01:26,280 --> 00:01:30,540
再次应用这个线性步骤,

22
00:01:30,540 --> 00:01:33,432
所以是由这个方程式。

23
00:01:33,432 --> 00:01:38,040
这与我们在左边看到的等式很相似。

24
00:01:38,040 --> 00:01:43,890
最后，你再用一次ReLU变换，这是

25
00:01:43,890 --> 00:01:52,105
现在 是由这个等式得到的，<br />G在这里是非线性ReLU。

26
00:01:52,105 --> 00:01:56,880
你可以得到一个a[l+2]

27
00:01:56,880 --> 00:01:57,900
换言之

28
00:01:57,900 --> 00:02:03,035
从a[l]流向a[l+2]的信息，

29
00:02:03,035 --> 00:02:07,455
它需要经过所有这些步骤，我会把这称作

30
00:02:07,455 --> 00:02:13,140
这组层的主路径。

31
00:02:13,140 --> 00:02:14,550
在残差网络（ResNet）中,

32
00:02:14,550 --> 00:02:16,900
我们要做个改变

33
00:02:16,900 --> 00:02:18,495
我们要把a[l],

34
00:02:18,495 --> 00:02:22,805
然后把它往前提，复制

35
00:02:22,805 --> 00:02:26,200
提到神经网络很往后的位置，

36
00:02:26,200 --> 00:02:28,860
然后把它加在这。

37
00:02:28,860 --> 00:02:34,080
最后应用一些非线性处理，<br />比如线性整流函数(ReLU)

38
00:02:34,080 --> 00:02:37,730
我将把这叫做快捷路径。

39
00:02:37,730 --> 00:02:40,725
所以，无需遵循主路径,

40
00:02:40,725 --> 00:02:43,335
a[l]中的信息现在可以遵循

41
00:02:43,335 --> 00:02:46,910
快捷路径进入到更深层的神经网络中。

42
00:02:46,910 --> 00:02:49,680
这意味着最后一个等式

43
00:02:49,680 --> 00:02:52,760
消失了，取而代之，我们有了

44
00:02:52,760 --> 00:03:00,810
一个a[l+2]作为ReLU非线性g，<br />和以前一样，应用到z[l+2]上

45
00:03:00,810 --> 00:03:02,830
但现在加上a[l]。

46
00:03:02,830 --> 00:03:05,515
所以，在这里添加一个a[l],

47
00:03:05,515 --> 00:03:07,355
使这成为了一个残差块（Residual Block）。

48
00:03:07,355 --> 00:03:11,070
在这幅图中，你还可以在这个图片上修改

49
00:03:11,070 --> 00:03:15,945
把这个快捷路径画到这。

50
00:03:15,945 --> 00:03:20,805
我们打算画成 它进入到第二层的这里

51
00:03:20,805 --> 00:03:26,220
是因为实际上 快捷路径是加在ReLU非线性之前的。

52
00:03:26,220 --> 00:03:27,570
所以这里的每个节点,

53
00:03:27,570 --> 00:03:30,560
都应用一个线性方程和一个ReLU。

54
00:03:30,560 --> 00:03:34,320
所以a[l]插入于线性部分之后，<br />ReLU部分之前。

55
00:03:34,320 --> 00:03:37,815
有时这个术语不叫做快捷路径（shortcut），

56
00:03:37,815 --> 00:03:40,485
你也会听到"跳跃连接"（skip connection）这个词

57
00:03:40,485 --> 00:03:44,835
这是指a[l]跳过一层或者跳过

58
00:03:44,835 --> 00:03:51,090
几乎两层把信息传递到更深的神经网络中去。

59
00:03:51,090 --> 00:03:54,030
ResNet的发明者

60
00:03:54,030 --> 00:03:55,950
是Kaiming He，Xiangyu Zhang，

61
00:03:55,950 --> 00:03:58,925
Shaoqing Ren，和Jian Sun。

62
00:03:58,925 --> 00:04:02,010
他们发现使用残差块

63
00:04:02,010 --> 00:04:05,920
让你可以训练更深层的神经网络。

64
00:04:05,920 --> 00:04:10,785
而你建立一个ResNet的方法<br />就是是通过大量的这些残差块

65
00:04:10,785 --> 00:04:15,695
例如这个，然后把他们堆叠起来，<br />形成一个深层网络。

66
00:04:15,695 --> 00:04:18,150
那么，让我们看看这个网络。

67
00:04:18,150 --> 00:04:19,730
这个还不是残差网络

68
00:04:19,730 --> 00:04:22,950
这个被称为普通（plain）网络。

69
00:04:22,950 --> 00:04:26,830
这是ResNet论文的术语。

70
00:04:26,830 --> 00:04:28,675
要把这个变成ResNet

71
00:04:28,675 --> 00:04:31,050
你做的是添加所有这些

72
00:04:31,050 --> 00:04:36,475
跳跃连接（或称为快捷路径连接）

73
00:04:36,475 --> 00:04:39,875
所以每两层结束于

74
00:04:39,875 --> 00:04:44,710
额外的改变，如同我们

75
00:04:44,710 --> 00:04:49,520
在之前幻灯片中看到的 将这些每个都变成残差块

76
00:04:49,520 --> 00:04:53,770
这张图片显示了5个残差块堆积在一起,

77
00:04:53,770 --> 00:04:56,565
这就是一个残差网络。

78
00:04:56,565 --> 00:04:59,615
事实证明，如果你使用

79
00:04:59,615 --> 00:05:02,620
标准的优化算法，如

80
00:05:02,620 --> 00:05:04,120
梯度下降法，或者

81
00:05:04,120 --> 00:05:07,340
以下很高级的优化算法之一，来训练普通网络

82
00:05:07,340 --> 00:05:10,270
没有这些额外的残差

83
00:05:10,270 --> 00:05:14,030
没有所有额外的快速路径或跳跃连接，<br />比如我刚刚画的那些。

84
00:05:14,030 --> 00:05:18,965
从经验上来说，你会发现当你增加层数时,

85
00:05:18,965 --> 00:05:21,100
训练误差会在下降一段时间后，

86
00:05:21,100 --> 00:05:24,240
但它们又会回升上去。

87
00:05:24,240 --> 00:05:29,170
在理论上，当你使神经网络更深,

88
00:05:29,170 --> 00:05:32,935
它在训练数据上的性能应该只会更好

89
00:05:32,935 --> 00:05:35,155
是的。这个理论，理论上来讲

90
00:05:35,155 --> 00:05:37,815
一个更深层次的网络只会有帮助。

91
00:05:37,815 --> 00:05:40,435
但在实践中，或在现实中,

92
00:05:40,435 --> 00:05:42,925
有一个普通网络，没有ResNet,

93
00:05:42,925 --> 00:05:45,890
有一个很深的纯网络意味着

94
00:05:45,890 --> 00:05:50,220
你的优化算法训练起来会更困难。

95
00:05:50,220 --> 00:05:51,685
所以，在现实中,

96
00:05:51,685 --> 00:05:55,865
如果你选择的网络太深，则训练误差会更糟。

97
00:05:55,865 --> 00:06:01,530
但有了ResNet的情况是，即使层数越来越深,

98
00:06:01,530 --> 00:06:06,120
你仍可以让训练误差继续下降，

99
00:06:06,120 --> 00:06:10,030
即使我们训练一个超过100层的网络。

100
00:06:10,030 --> 00:06:12,820
然后现在有些人甚至拿超过1000层的网络做实验。

101
00:06:12,820 --> 00:06:17,845
尽管我很少看到有人在实际应用中用到那么多层的网络

102
00:06:17,845 --> 00:06:20,230
但把这些激活X

103
00:06:20,230 --> 00:06:24,950
或者这些中间层的激活输出，连接到更后面的层去

104
00:06:24,950 --> 00:06:30,355
这确实对解决梯度消失和爆炸问题非常有帮助

105
00:06:30,355 --> 00:06:31,930
使得我们可以训练

106
00:06:31,930 --> 00:06:36,220
深得多的神经元网络而不会看到性能倒退的现象

107
00:06:36,220 --> 00:06:39,370
尽管可能在某一个点会达到平原阶段

108
00:06:39,370 --> 00:06:43,090
这时候就算再加层也不会有帮助

109
00:06:43,090 --> 00:06:49,120
但是Resnet确实对训练很深的网络有很大的帮助

110
00:06:49,120 --> 00:06:52,645
所以你现在已经了解了 ResNets 是如何工作的。

111
00:06:52,645 --> 00:06:55,495
在本周的编程练习中,

112
00:06:55,495 --> 00:06:59,205
你会自己去实现这个结构，<br />可以自己看看它到底怎样有效

113
00:06:59,205 --> 00:07:02,350
下一节我将与你分享一些如何建立

114
00:07:02,350 --> 00:07:06,160
为什么Resnet那么有效的原因

115
00:07:06,160 --> 00:07:07,730
我们下一节再见