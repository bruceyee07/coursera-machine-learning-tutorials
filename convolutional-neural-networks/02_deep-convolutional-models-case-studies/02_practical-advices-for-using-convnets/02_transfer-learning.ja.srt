1
00:00:00,270 --> 00:00:04,240
コンピュータ ビジョン アプリケーションを構築する場合

2
00:00:04,240 --> 00:00:07,630
スクラッチで ランダム初期化で 学習するよりも

3
00:00:07,630 --> 00:00:10,010
もっと速く進められる
もし 誰か他の人が

4
00:00:10,010 --> 00:00:12,530
既に学習を行った ニューラルネットワークをダウンロードし

5
00:00:12,530 --> 00:00:15,640
それを 事前学習済みのものとして使い

6
00:00:15,640 --> 00:00:19,525
自分が興味を持っている新しいタスクに転移させるなら

7
00:00:19,525 --> 00:00:24,230
コンピュータ ビジョン研究コミュニティでは 非常に沢山の

8
00:00:24,230 --> 00:00:28,860
データセットをインターネットに投稿してきた
ImageNet や MS COCO もしくは

9
00:00:28,860 --> 00:00:31,082
Pascal のようなデータセットのことを聞いたことが有るなら

10
00:00:31,082 --> 00:00:33,995
これらは 人々がオンライン投稿してきた異なるデータセットの名前で

11
00:00:33,995 --> 00:00:39,640
多くのコンピュータ ビジョン研究者が 自分のアルゴリズムの訓練に使ってきた

12
00:00:39,640 --> 00:00:44,760
時折 このような訓練は数週間かかったり とても多くのGPUを使ったりする

13
00:00:44,760 --> 00:00:47,020
他の誰かが これを行い

14
00:00:47,020 --> 00:00:50,005
痛みを伴う 高性能の研究過程を経てきた ということは

15
00:00:50,005 --> 00:00:55,360
他の誰かが 見つけ出すのに 何週間も 何か月もかかった オープンソースの重みを

16
00:00:55,360 --> 00:00:57,570
ダウンロードできて それを使うことができる ということだ

17
00:00:57,570 --> 00:01:01,550
自分自身のニューラルネットワークの非常に良い初期化として

18
00:01:01,550 --> 00:01:05,012
そして 転移学習 を使うんだ
これらの巨大なパブリックデータセットから

19
00:01:05,012 --> 00:01:08,761
自分自身の問題へ 知識を転移させるんだ

20
00:01:08,761 --> 00:01:11,710
どうやるのか 深く見ていこう

21
00:01:11,710 --> 00:01:13,760
例から始めよう

22
00:01:13,760 --> 00:01:18,820
自分のペットの猫を認識する 猫検出器を作っているとしよう

23
00:01:18,820 --> 00:01:22,570
インターネットによれば

24
00:01:22,570 --> 00:01:32,855
Tigger は よくある猫の名前だ
Misty も また よくある猫の名前だ

25
00:01:32,855 --> 00:01:41,480
あなたの猫は Tigger と Misty だとする どちらでも無い場合もある

26
00:01:41,480 --> 00:01:44,570
３つの場合がある 分類問題だ

27
00:01:44,570 --> 00:01:46,300
この写真は Tigger か

28
00:01:46,300 --> 00:01:49,075
Misty か それとも どちらでも無いか？

29
00:01:49,075 --> 00:01:53,875
それに 両方の猫が 写真に写っている場合がある

30
00:01:53,875 --> 00:01:57,020
さて あなたは 多分 Tigger の写真を たくさんは持っていないだろう

31
00:01:57,020 --> 00:02:00,745
Misty のもね
つまり あなたの学習セットは 小さい

32
00:02:00,745 --> 00:02:02,170
何ができるだろう？

33
00:02:02,170 --> 00:02:06,920
私が勧めるのは こうだ
オンラインで、いくつかの ニューラルネットワークのオープン ソース実装のダウンロードする

34
00:02:06,920 --> 00:02:12,600
コードだけではなく 重み もダウンロードするんだ

35
00:02:12,600 --> 00:02:19,900
たくさんの訓練済みのネットワークをダウンロードできる 例えば

36
00:02:19,900 --> 00:02:25,130
ImageNet データセットには 1000個のことなクラスがあって だから

37
00:02:25,130 --> 00:02:31,180
ネットワークは ソフトマックスで 1000の内の1つのクラスを出力するだろう

38
00:02:31,180 --> 00:02:36,110
できるのは 最後のソフトマックス層を取り除き

39
00:02:36,110 --> 00:02:46,025
Tigger か Misty か それ以外かを 出力する自分のソフトマックスユニットを作ることだ

40
00:02:46,025 --> 00:02:48,710
ネットワークの方では

41
00:02:48,710 --> 00:02:52,350
これらの全ての層は 固定されたものと考えるとよい

42
00:02:52,350 --> 00:02:56,415
つまり ネットワークのこの全ての層では

43
00:02:56,415 --> 00:03:00,300
パラメータが固定され それから

44
00:03:00,300 --> 00:03:05,700
ソフトマックス層に関係するパラメータだけを訓練するんだ

45
00:03:05,700 --> 00:03:08,982
これは ３つの出力が有り得るソフトマックス層だ

46
00:03:08,982 --> 00:03:11,790
Tigger, Misty, もしくは どちらでも無い

47
00:03:11,790 --> 00:03:16,560
他の誰かの 無料の学習済み重み を使うことで

48
00:03:16,560 --> 00:03:22,600
小さなデータセットでも とても良い性能を得ることだろう

49
00:03:22,600 --> 00:03:25,100
幸運なことに 多くのディープラーニング フレームワークは

50
00:03:25,100 --> 00:03:28,000
この処理を行うモードを備えている
実際に

51
00:03:28,000 --> 00:03:35,085
フレームワークにもよるが trainableParameter = 0 とすることで そうできたりする

52
00:03:35,085 --> 00:03:37,660
これらを もっと前の方の層にセットするかもしれない

53
00:03:37,660 --> 00:03:39,117
こう言うためにだよ

54
00:03:39,117 --> 00:03:42,885
"これらの重みを訓練するな"
または パラメータは

55
00:03:42,885 --> 00:03:47,295
freeze = 1 とするかもしれない

56
00:03:47,295 --> 00:03:50,730
異なるディープラーニング プログラミング フレームワークでは 異なるやり方で

57
00:03:50,730 --> 00:03:56,245
特定の層の重みの 訓練可否を指定する

58
00:03:56,245 --> 00:03:58,440
この例では

59
00:03:58,440 --> 00:04:04,945
ソフトマックス層の重みのみ訓練し それより前の層の重みは全て固定する

60
00:04:04,945 --> 00:04:07,930
実装によっては役に立つ 別の巧みなトリックがある

61
00:04:07,930 --> 00:04:12,380
これらの前の方の層は 全て固定されているので

62
00:04:12,380 --> 00:04:16,345
これは 固定された関数であって 変更できないので

63
00:04:16,345 --> 00:04:20,040
これは訓練されずに 入力画像 x を取り

64
00:04:20,040 --> 00:04:24,305
この層で ある活性に割り付けする

65
00:04:24,305 --> 00:04:30,455
学習速度を上げるトリックの1つは この層を事前に計算しておくことだ

66
00:04:30,455 --> 00:04:36,615
この層の活性化された特徴量を計算しておき ディスクに保存しておく

67
00:04:36,615 --> 00:04:40,620
今やったのは この固定関数を使い

68
00:04:40,620 --> 00:04:43,030
このニューラルネットワークの最初の部分を使い

69
00:04:43,030 --> 00:04:49,680
入力画像 x から 特徴ベクトルを計算して それから

70
00:04:49,680 --> 00:04:56,445
その特徴ベクトルで 薄いソフトマックス モデルを訓練した
予測を行うためにね

71
00:04:56,445 --> 00:05:04,170
計算を助ける１ステップは これらの層の活性を事前計算し

72
00:05:04,170 --> 00:05:06,330
学習セットの全てのサンプルについて行い ディスクに保存し

73
00:05:06,330 --> 00:05:10,525
それから それらを使って ソフトマックス識別器の訓練だけを行うことだ

74
00:05:10,525 --> 00:05:12,585
ディスクに保存しておく利点は

75
00:05:12,585 --> 00:05:15,990
事前計算してディスクに保存しておく利点は

76
00:05:15,990 --> 00:05:19,020
毎回 これらの活性を 再計算をしなくてよいことだ

77
00:05:19,020 --> 00:05:23,150
エポックの度に 訓練セットを通さなくてよい

78
00:05:23,150 --> 00:05:28,585
自分のタスクに とても少ない学習セットしか持っていない場合は こうすればいい

79
00:05:28,585 --> 00:05:31,215
もっと沢山の学習セットを持っていたら どうだろうか？

80
00:05:31,215 --> 00:05:33,810
一つの指針は もし

81
00:05:33,810 --> 00:05:39,164
多くのデータセットがあり
そう TiggerとMisty の写真を山ほどもっているかもしれない

82
00:05:39,164 --> 00:05:41,940
同じく ２つが移っていないのも沢山あるだろう

83
00:05:41,940 --> 00:05:45,935
できることの一つは より少ない層を固定することだ

84
00:05:45,935 --> 00:05:52,761
ここの層だけを固定し この後の方の層を訓練する

85
00:05:52,761 --> 00:05:57,540
ただし 出力層に 異なるクラスを持つ場合は

86
00:05:57,540 --> 00:06:04,321
"Tigger, Misty, 何れでも無い" となる 何らかの方法の 自分の出力ユニットが必要だ

87
00:06:04,321 --> 00:06:07,550
これを行うには いくつかのやり方がある

88
00:06:07,550 --> 00:06:10,980
終わりの方の何層かの

89
00:06:10,980 --> 00:06:17,346
重みは初期化に使い それから勾配降下法を始めることができる

90
00:06:17,346 --> 00:06:22,050
もしくは ここの終わりの何層かを消してしまい

91
00:06:22,050 --> 00:06:27,990
自分自身の新しい隠れ層と 自分自身の最終ソフトマックス出力にもできる

92
00:06:27,990 --> 00:06:32,000
どちらも やってみる価値がある

93
00:06:32,000 --> 00:06:35,220
ただし もし より多くのデータを持っているなら

94
00:06:35,220 --> 00:06:39,090
固定する層の数を より少なくし

95
00:06:39,090 --> 00:06:43,810
訓練する先頭の層の数を より大きくするだろう

96
00:06:43,810 --> 00:06:46,710
考え方はこうだ
もし 大きなデータセットがあるなら

97
00:06:46,710 --> 00:06:51,090
十分なデータがあるなら
ソフトマックス１つだけを訓練するのではなく

98
00:06:51,090 --> 00:06:54,960
ニューラルネットワークのより多くの層を訓練する

99
00:06:54,960 --> 00:07:00,400
最終的に使うネットワークの最後の数層を

100
00:07:00,400 --> 00:07:03,965
最後に もし とても多くのデータがあるなら

101
00:07:03,965 --> 00:07:09,710
行なうであろうことは このオープンソースのネットワークと重みを

102
00:07:09,710 --> 00:07:15,430
全て 初期化にだけ使い ネットワーク全体を訓練することだ

103
00:07:15,430 --> 00:07:20,945
でも また言うけど 1000のソフトマックスだった場合は ３つの出力しかしないのだから

104
00:07:20,945 --> 00:07:23,610
自分のソフトマックス出力が必要だよ

105
00:07:23,610 --> 00:07:26,133
その出力ラベルは 自分が扱おうとしているものにしてね

106
00:07:26,133 --> 00:07:29,760
より多くのデータがあるなら

107
00:07:29,760 --> 00:07:33,630
Tigger, Misty, その他 の写真がもっとあるなら

108
00:07:33,630 --> 00:07:37,005
より多くの層を訓練できる そして究極では

109
00:07:37,005 --> 00:07:40,365
ダウンロードした重みを

110
00:07:40,365 --> 00:07:42,405
初期化としてのみ使い

111
00:07:42,405 --> 00:07:45,260
ランダム初期化の代わりでね
そして 勾配降下法を使って

112
00:07:45,260 --> 00:07:50,838
ネットワークの 全ての重み 全ての層を 訓練 更新できる

113
00:07:50,838 --> 00:07:54,510
これが ConvNet を訓練するための 転移学習だ

114
00:07:54,510 --> 00:08:00,090
実際 インターネット上のオープン データセットは膨大で

115
00:08:00,090 --> 00:08:05,580
ダウンロードできる重みは 誰かが 数週間かけて 非常に沢山のデータで訓練したものなので

116
00:08:05,580 --> 00:08:08,385
多くのコンピュータ ビジョン アプリケーション用のが見つかるし

117
00:08:08,385 --> 00:08:10,980
とても上手く行く
もし 自身の問題のため

118
00:08:10,980 --> 00:08:16,080
他の誰かの オープンソース重みを ダウンロードして 初期化に使うならね

119
00:08:16,080 --> 00:08:18,410
あらゆる分野において

120
00:08:18,410 --> 00:08:21,720
あらゆる種類のディープラーニングアプリケーションにおいて

121
00:08:21,720 --> 00:08:25,220
コンピュータ ビジョンには 転移学習を

122
00:08:25,220 --> 00:08:28,815
常に行うべきだ

123
00:08:28,815 --> 00:08:35,145
格別に大きなデータセットを持っていて 自分自身で スクラッチで全てを訓練する場合を除いて

124
00:08:35,145 --> 00:08:40,560
転移学習は 真剣に検討する価値が 本当にある

125
00:08:40,560 --> 00:08:43,745
自分自身で スクラッチで 全てを訓練するための

126
00:08:43,745 --> 00:08:47,350
特別に巨大なデータセットと 巨大なコンピュータ予算がある場合を除いてね