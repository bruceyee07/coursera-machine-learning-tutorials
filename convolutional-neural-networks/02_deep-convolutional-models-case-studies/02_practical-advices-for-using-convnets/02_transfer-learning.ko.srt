1
00:00:00,270 --> 00:00:04,240
여러분이 컴퓨터 비전 응용 프로그램을 아예 처음부터, 무작위 초기화(random initialization) 단계부터, 개발하는 것이 아니라, 

2
00:00:04,240 --> 00:00:07,630
그것을 구축하려고 하는 경우,

3
00:00:07,630 --> 00:00:10,010
여러분은 다른 사람이 이미 트레이닝 시켜놓은 

4
00:00:10,010 --> 00:00:12,530
네트워크 아키테처를 다운받아서,

5
00:00:12,530 --> 00:00:15,640
사전 훈련으로 사용하고 

6
00:00:15,640 --> 00:00:19,525
그것을 여러분이 관심있는 새로운 작업으로 전환하면 훨씬 더 빠른 진전을 보이게 됩니다. 

7
00:00:19,525 --> 00:00:24,230
컴퓨터 비전 연구 커뮤니티는 인터넷에서 많은 데이터 세트를

8
00:00:24,230 --> 00:00:28,860
게시하는 데 능숙합니다. 그래서 여러분이 만약 Image Net, MS COCO,

9
00:00:28,860 --> 00:00:31,082
또는 파스칼 유형의 데이터 세트와 같은 정보가 있으면,

10
00:00:31,082 --> 00:00:33,995
이건 사람들이 온라인에 올려놓은 다른 데이터 세트들의 이름입니다. 

11
00:00:33,995 --> 00:00:39,640
그리고 많은 컴퓨터 연구가들은 그 데이터 세트상에서 계속해서 알고리즘을 훈련시켜 왔습니다. 

12
00:00:39,640 --> 00:00:44,760
때로는 이러한 훈련에 몇 주가 걸리고, 많은 GP를 사용합니다. 

13
00:00:44,760 --> 00:00:47,020
누군가가 이렇게 해서

14
00:00:47,020 --> 00:00:50,005
고통스러운 고성능 연구 프로세스를 진행했다는 사실은

15
00:00:50,005 --> 00:00:55,360
여러분은 누군가가 이걸 수 주 또는 수 개월 걸려서 알아낸 

16
00:00:55,360 --> 00:00:57,570
오픈 소스 방법들을 다운로드하고,

17
00:00:57,570 --> 00:01:01,550
그것을 여러분의 신경망을 위한 매우 좋은 초기화로서 사용할 수 있다는 것을 의미합니다.

18
00:01:01,550 --> 00:01:05,012
또한, 전이 지식을 이러한 매우 큰 공용 데이터 세트의 일부로부터 자신의 문제로 분류시키는 것에 

19
00:01:05,012 --> 00:01:08,761
전이 학습(transfer learning)을 사용할 수 있다는 뜻입니다. 

20
00:01:08,761 --> 00:01:11,710
이렇게하는 방법에 대해 자세히 살펴 보겠습니다. 

21
00:01:11,710 --> 00:01:13,760
이 예제로 시작해 보시죠. 

22
00:01:13,760 --> 00:01:18,820
여러분의 애완 고양이를 알아볼 수있는 고양이 탐지기를 제작한다고 가정해 봅시다. 

23
00:01:18,820 --> 00:01:22,570
인터넷에 따르면,

24
00:01:22,570 --> 00:01:32,855
Tigger는 일반적인 고양이 이름이고 Misty는 또 다른 흔한 고양이 이름입니다. 

25
00:01:32,855 --> 00:01:41,480
여러분의 고양이 이름이 티거, 미스티, 둘 다 아님 이라고 합시다. 

26
00:01:41,480 --> 00:01:44,570
세 클래스에 대한 분류 문제가 있습니다. 

27
00:01:44,570 --> 00:01:46,300
이 사진은 티거 (Tigger)인가요?

28
00:01:46,300 --> 00:01:49,075
아니면 미스티인가요, 아니면 둘 다 아닌 고양이인가요?

29
00:01:49,075 --> 00:01:53,875
그리고 두 고양이의 모든 경우에 고양이가 그림에 등장합니다.

30
00:01:53,875 --> 00:01:57,020
자, 당신은 아마도 티거 또는 미스티의 사진이 그리 많지 않으므로 

31
00:01:57,020 --> 00:02:00,745
트레이닝 세트는 작을 것입니다. 여러분은 뭘 할 수 있나요? 

32
00:02:00,745 --> 00:02:02,170
어떻게 해야 할까요?

33
00:02:02,170 --> 00:02:06,920
온라인으로 가서 신경망의 오픈 소스 구현을 다운로드하고 

34
00:02:06,920 --> 00:02:12,600
코드뿐만 아니라 가중치도 다운로드하는 것이 좋습니다. 

35
00:02:12,600 --> 00:02:19,900
이미 트레이닝 된 다운로드 받을 수 있는 네트워크들이 많이 있습니다. 예를 들자면, 

36
00:02:19,900 --> 00:02:25,130
1000 개의 다른 클래스들이 있는 Init Net 데이터 세트죠. 그러면 그 네트워크는

37
00:02:25,130 --> 00:02:31,180
가능한 1000개 중의 하나를 아웃풋하는 Softmax 유닛을 가지고 있을 수도 있습니다. 

38
00:02:31,180 --> 00:02:36,110
여러분이 할 수 있는 일은 softmax 레이어를 없애고

39
00:02:36,110 --> 00:02:46,025
Tigger 나 Misty를 출력하는 softmax 유닛을 생성하는 것입니다. 

40
00:02:46,025 --> 00:02:48,710
네트워크의 관점에서 볼 때 ,

41
00:02:48,710 --> 00:02:52,350
이러한 모든 레이어를 고정 된 상태로 생각하여,

42
00:02:52,350 --> 00:02:56,415
네트워크의 모든 레이어에서 

43
00:02:56,415 --> 00:03:00,300
파라미터를 고정시키고, 

44
00:03:00,300 --> 00:03:05,700
softmax 레이어와 관련된 파라미터를 조정할 것을 권장합니다. 

45
00:03:05,700 --> 00:03:08,982
세 가지 가능한 아웃풋인 Softmax 레이어는 어느 것인가요, 

46
00:03:08,982 --> 00:03:11,790
티거, 미스티 또는 둘 다 아님 인가요?

47
00:03:11,790 --> 00:03:16,560
다른 누군가의 자유 교환 방식을 사용하면 

48
00:03:16,560 --> 00:03:22,600
작은 데이터 세트로도 꽤 좋은 성능을 얻을 수 있습니다. 

49
00:03:22,600 --> 00:03:25,100
다행스럽게도 프레임 워크를 학습하는 많은 사람들이

50
00:03:25,100 --> 00:03:28,000
이 작업 모드를 지원합니다. 그리고 사실, 

51
00:03:28,000 --> 00:03:35,085
실제로 프레임 워크에 따라, 학습 가능한 파라미터는 0 일 수 있고, 

52
00:03:35,085 --> 00:03:37,660
이러한 초기 레이어들의 일부를 위해 그것을 설정할 수 있습니다. 

53
00:03:37,660 --> 00:03:39,117
다른 사람들은 단지, 

54
00:03:39,117 --> 00:03:42,885
그런 방법들을 훈련시키지 않거나 때로는 	

55
00:03:42,885 --> 00:03:47,295
프리즈가 0 인 파라미터를 가지게 됩니다. 

56
00:03:47,295 --> 00:03:50,730
그리고 그것들은 다른 방법과 다른 딥러닝 프로그램 프레임워크이기 때문에 

57
00:03:50,730 --> 00:03:56,245
특정 레이어와 관련된 방법을 교육할지 여부를 지정할 수 있게 해줍니다. 

58
00:03:56,245 --> 00:03:58,440
이 경우 softmax 레이어 방법 만 교육하고 

59
00:03:58,440 --> 00:04:04,945
이전 레이어 방법은 모두 고정합니다. 

60
00:04:04,945 --> 00:04:07,930
일부 구현에 도움이 될 수있는 또 다른 깔끔한 기술은

61
00:04:07,930 --> 00:04:12,380
이러한 모든 초기 리드가 고정되어 있기 때문에 

62
00:04:12,380 --> 00:04:16,345
변하지 않는 고정된 기능이 있습니다. 왜냐하면 여러분이 변경하지 않고, 

63
00:04:16,345 --> 00:04:20,040
이 입력 이미지 X를 가져 와서

64
00:04:20,040 --> 00:04:24,305
해당 레이어의 활성화 세트에 매핑하는 교육을하지 않습니다. 

65
00:04:24,305 --> 00:04:30,455
트레이닝 속도를 높일 수있는 트릭 중 하나는 해당 레이어와 그 레이어의 

66
00:04:30,455 --> 00:04:36,615
재활성화 기능을 사전 계산하고 디스크에 저장하는 것입니다.

67
00:04:36,615 --> 00:04:40,620
여러분이 하고 있는 것은 신경망의 첫 번째 부분에서 

68
00:04:40,620 --> 00:04:43,030
이 고정된 함수를 사용하여

69
00:04:43,030 --> 00:04:49,680
이 입력을 모든 이미지 X로 가져 와서, 그것을위한 몇 가지 특징 벡터를 계산 한 다음

70
00:04:49,680 --> 00:04:56,445
이 특징 벡터로부터 얕은 softmax 모델을 훈련시켜서 예측하도록 만드는 것입니다. 

71
00:04:56,445 --> 00:05:04,170
이는 트레이닝 세트의 모든 예제에 대해 레이어 활성화를 미리 계산하고 

72
00:05:04,170 --> 00:05:06,330
디스크에 저장 한 다음 

73
00:05:06,330 --> 00:05:10,525
거기에 softmax를 트레이닝 할 때 여러분의 계산을 도울 수있는 한 단계입니다. 

74
00:05:10,525 --> 00:05:12,585
안전 디스크,

75
00:05:12,585 --> 00:05:15,990
사전 계산 방법 혹은 안전 디스트의 장점은 

76
00:05:15,990 --> 00:05:19,020
트레이닝 세트를 통해 ____를 할 때마다

77
00:05:19,020 --> 00:05:23,150
그 활성화들을 다시 계산할 필요가 없다는 것입니다. 

78
00:05:23,150 --> 00:05:28,585
이것이 바로 작업을 위해 아주 작은 트레이닝 세트를 가지고 있을때 여러분이 하는 일입니다. 

79
00:05:28,585 --> 00:05:31,215
더 큰 트레이닝 세트가 있는지 여부를 위해,

80
00:05:31,215 --> 00:05:33,810
한 가지 최고의 규칙은

81
00:05:33,810 --> 00:05:39,164
여러분이 더 큰 레이블 데이터 세트를 가지고 있고, 

82
00:05:39,164 --> 00:05:41,940
티거와 미스티, 그리고 둘 다 아님의 아주 많은 사진을 가지고 있다면, 

83
00:05:41,940 --> 00:05:45,935
여러분이 할 일은 더 적은 레이어를 고정하는 것 입니다. 

84
00:05:45,935 --> 00:05:52,761
어쩌면 당신은 이 레이어들을 단지 고정하고, 나중에 이 레이어들을 훈련시킬 것입니다. 

85
00:05:52,761 --> 00:05:57,540
아웃풋 레이어에 다른 클래스가 있는 경우라면, 

86
00:05:57,540 --> 00:06:04,321
여러분의 티거, 미스티, 또는 둘 다 아님의 아웃풋 단위가 필요합니다. 

87
00:06:04,321 --> 00:06:07,550
이 작업에는 몇 가지 방법이 있습니다. 

88
00:06:07,550 --> 00:06:10,980
마지막 몇 개의 레이어 방법을 사용하여

89
00:06:10,980 --> 00:06:17,346
초기화(initialization)로 사용하고, 그곳에서 기울기 강하를 수행하거나

90
00:06:17,346 --> 00:06:22,050
마지막 몇 개의 레이어를 날려 버리는겁니다.

91
00:06:22,050 --> 00:06:27,990
그리고 자신만의 새로운 숨겨진 유닛과 자신의 최종 softmax 아웃풋을 사용할 수 있습니다. 

92
00:06:27,990 --> 00:06:32,000
이 중 어느 것이 든 노력할 가치가 있습니다. 

93
00:06:32,000 --> 00:06:35,220
그러나 하나의 패턴은 여러분이 더 많은 데이터를 가지고 있는 경우 

94
00:06:35,220 --> 00:06:39,090
고정된 레이어의 수가 더 작아 질 수 있고

95
00:06:39,090 --> 00:06:43,810
여러분이 위에서 훈련하는 레이어 수가 더 클 수 있다는 것입니다. 

96
00:06:43,810 --> 00:06:46,710
그리고 그 아이디어는 데이터 세트를 고르고 

97
00:06:46,710 --> 00:06:51,090
단일 softmax 장치를 교육하는 것뿐만 아니라 

98
00:06:51,090 --> 00:06:54,960
사용하는 최종 네트워크의 마지막 몇 개 레이어로 구성된

99
00:06:54,960 --> 00:07:00,400
다른 크기의 신경망을 훈련하기에 충분한 데이터를 보유하고있는 경우입니다. 

100
00:07:00,400 --> 00:07:03,965
마지막으로, 많은 양의 데이터를 가지고 있다면,

101
00:07:03,965 --> 00:07:09,710
이 오픈 소스 네트워크와 방법을 사용하고 

102
00:07:09,710 --> 00:07:15,430
초기화를 전체 네트워크로 사용하고, 네트워크 전체를 훈련시키는 것이 좋습니다. 

103
00:07:15,430 --> 00:07:20,945
다시, 이것이 softmax 1000 이고 

104
00:07:20,945 --> 00:07:23,610
여러분은 단지 3 개의 softmax 아웃풋,

105
00:07:23,610 --> 00:07:26,133
즉 여러분이 관심있는 레이블의 아웃풋을 필요로합니다. 

106
00:07:26,133 --> 00:07:29,760
하지만 작업에 필요한 레이블 데이터가 많고

107
00:07:29,760 --> 00:07:33,630
티거, 미스티 그리고 둘 다 아님의 사진이 많을수록, 

108
00:07:33,630 --> 00:07:37,005
여러분은 더 많은 레이어를 트레인할 수도 있고, 심한 경우

109
00:07:37,005 --> 00:07:40,365
초기화와 같이 다운로드한 방법들을 사용할 수도 있습니다. 

110
00:07:40,365 --> 00:07:42,405
따라서 그들은 랜덤 초기화를 대체하고

111
00:07:42,405 --> 00:07:45,260
기울기 강하를 할 수 있습니다. 

112
00:07:45,260 --> 00:07:50,838
네트워크의 모든 방법과 레이어를 업데이트 하는 훈련을 하면서 말이죠. 

113
00:07:50,838 --> 00:07:54,510
이것이 컨볼네트의 훈련을 위한 전이 학습입니다. 

114
00:07:54,510 --> 00:08:00,090
실제로는 인터넷상의 공개 데이터 세트가 매우 많고 

115
00:08:00,090 --> 00:08:05,580
누군가 여러 주 동안 트레이닝에 시간을 쓰면서 배운 많은 데이터를 다운로드 할 수 있으므로 

116
00:08:05,580 --> 00:08:08,385
여러분은 많은 컴퓨터 비전 응용 프로그램을 찾을 수 있고

117
00:08:08,385 --> 00:08:10,980
누군가의 오픈 소스 방법들을 다운받아서 그것을 여러분의 문제를 위한 초기화로서 사용하면 

118
00:08:10,980 --> 00:08:16,080
훨씬 더 잘 할 수 있을 겁니다. 

119
00:08:16,080 --> 00:08:18,410
이 모든 다른 훈련법에서, 

120
00:08:18,410 --> 00:08:21,720
이 모든 딥러닝 응용 프로그램에서

121
00:08:21,720 --> 00:08:25,220
컴퓨터 비전은 전이 학습이 

122
00:08:25,220 --> 00:08:28,815
거의 모든 일을 해야 한다는 것을 의미합니다.

123
00:08:28,815 --> 00:08:35,145
처음부터 혼자서 모든 것을 훈련해야 하는 유난히 큰 데이터 세트를 가지고 있지 않는 경우라면 말이죠. 

124
00:08:35,145 --> 00:08:40,560
하지만 혼자 처음부터 모든 것을 훈련해야 하는 

125
00:08:40,560 --> 00:08:43,745
유난하게 큰 데이터 세트와 매우 큰 계산 예산이 없다면

126
00:08:43,745 --> 00:08:47,350
전이 학습은 심각하게 고려해야 할 많은 가치가 있습니다.