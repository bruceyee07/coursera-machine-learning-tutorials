1
00:00:00,270 --> 00:00:04,240
Eğer bir bilgisayarlı görü uygulaması geliştiriyorsanız,

2
00:00:04,240 --> 00:00:07,630
ağırlıkları rastgele belirleyip sıfırdan eğitmek yerine

3
00:00:07,630 --> 00:00:10,010
başkasının daha önceden bir sinir ağır üzerinde eğittiği

4
00:00:10,010 --> 00:00:12,530
ağırlıkları indirip ön eğitim olarak kullanırsanız ve

5
00:00:12,530 --> 00:00:15,640
bunları ilgilendiğiniz yeni bir probleme transfer ederek

6
00:00:15,640 --> 00:00:19,525
genelde çok daha hızlı bir şekilde ilerleme kaydedersiniz.

7
00:00:19,525 --> 00:00:24,230
Bilgisayarlı görü araştırma topluluğu,
internet üzerinde birçok veri setini

8
00:00:24,230 --> 00:00:28,860
paylaşma konusunda bayağı iyi, bu sebeple eğer ImageNet,

9
00:00:28,860 --> 00:00:31,082
MS COCO veya PASCAL gibi şeyler duyarsanız

10
00:00:31,082 --> 00:00:33,995
bunlar birçok kişinin internette paylaştığı

11
00:00:33,995 --> 00:00:39,640
ve algoritmalarını eğitirken
kullandıkları veri setlerinin isimleridir.

12
00:00:39,640 --> 00:00:44,760
Bazen eğitim haftalarca sürebilmekte ve
birden çok ekran kartına (GPU) ihtiyaç duyulabilmekte.

13
00:00:44,760 --> 00:00:47,020
Başka bir kişinin, oldukça zahmetli olan
üst parametre (hyperparameter)

14
00:00:47,020 --> 00:00:50,005
arama sürecini gerçekleştirerek haftalarca veya

15
00:00:50,005 --> 00:00:55,360
aylarca süren bu eğitim sürecini tamamlaması, bu ağırlıkları

16
00:00:55,360 --> 00:00:57,570
indirebileceğiniz ve kendi sinir ağınız için oldukça güzel bir

17
00:00:57,570 --> 00:01:01,550
başlangıç olarak kullanabileceğiniz anlamına gelir

18
00:01:01,550 --> 00:01:05,012
ve transfer öğrenmesini kullanarak
bu oldukça büyük ve herkese açık

19
00:01:05,012 --> 00:01:08,761
veri setlerinden kendi probleminize
bir nevi bilgi transfer edin.

20
00:01:08,761 --> 00:01:11,710
Bunu nasıl yapabileceğimize daha yakından bakalım.

21
00:01:11,710 --> 00:01:13,760
Bir örnek ile başlayalım.

22
00:01:13,760 --> 00:01:18,820
Diyelim ki kendi kedinizi tanımak için
bir kedi dedektörü geliştiriyorsunuz.

23
00:01:18,820 --> 00:01:22,570
İnternete göre

24
00:01:22,570 --> 00:01:32,855
Tigger, yaygın bir kedi ismiymiş. Misty de öyle.

25
00:01:32,855 --> 00:01:41,480
Diyelim ki sizin kedilerinizin ismi Tigger ve Misty
ve ayrıca hiçbiri de var.

26
00:01:41,480 --> 00:01:44,570
Yani 3 sınıflı bir sınıflandırma probleminiz var.

27
00:01:44,570 --> 00:01:46,300
Bu fotoğraftaki Tigger mı,

28
00:01:46,300 --> 00:01:49,075
yoksa Misty mi, yoksa hiçbiri mi?

29
00:01:49,075 --> 00:01:53,875
Aynı zamanda iki kedinizin de
aynı fotoğrafta olma durumu da var.

30
00:01:53,875 --> 00:01:57,020
Muhtemelen elinizde Tigger'ın veya Misty'nin
çok sayıda fotoğrafı bulunmamaktadır,

31
00:01:57,020 --> 00:02:00,745
bu sebeple eğitim setiniz küçük olacaktır.

32
00:02:00,745 --> 00:02:02,170
Bunun için ne yapabilirsiniz?

33
00:02:02,170 --> 00:02:06,920
İnternete girip bir sinir ağının
açık kaynak kodlu uygulamasını bulup

34
00:02:06,920 --> 00:02:12,600
hem kodları hem de ağırlıkları indirmenizi öneririm.

35
00:02:12,600 --> 00:02:19,900
İnternette indirebileceğiniz, örneğin 1000 farklı sınıfa
sahip olan ImageNet veri seti ile eğitilmiş

36
00:02:19,900 --> 00:02:25,130
birçok sinir ağı bulunmakta ve böyle bir sinir ağında

37
00:02:25,130 --> 00:02:31,180
1000 olası sınıf arasından birini çıktı olarak veren
eşiksiz en büyük işlevi (softmax) katmanı bulunabilir.

38
00:02:31,180 --> 00:02:36,110
Burada yapabileceğiniz şey, bu katmanı çıkarıp Tigger, Misty veya hiçbiri

39
00:02:36,110 --> 00:02:46,025
olarak çıktı verebilen eşiksiz en büyük işlevi
katmanınızı kendiniz oluşturabilirsiniz.

40
00:02:46,025 --> 00:02:48,710
Olaya ağ açısından bakarsak,

41
00:02:48,710 --> 00:02:52,350
bütün bu katmanları donmuş gibi
düşünmenizi öneririm.

42
00:02:52,350 --> 00:02:56,415
Yani ağın bu katmanlarındaki parametreleri dondurur ve

43
00:02:56,415 --> 00:03:00,300
kendi eşiksiz en büyük işlevi katmanınız

44
00:03:00,300 --> 00:03:05,700
ile ilişkili parametreleri eğitirsiniz.

45
00:03:05,700 --> 00:03:08,982
Bu da Tigger, Misty veya hiçbiri olmak üzere

46
00:03:08,982 --> 00:03:11,790
3 olası çıktıya sahip olan bir katmandır.

47
00:03:11,790 --> 00:03:16,560
Başkasının önceden eğitilmiş ağırlıklarını kullanarak

48
00:03:16,560 --> 00:03:22,600
küçük bir veri seti kullanmanıza rağmen
güzel bir sonuç elde edebilirsiniz.

49
00:03:22,600 --> 00:03:25,100
Neyse ki birçok derin öğrenme çatıları

50
00:03:25,100 --> 00:03:28,000
böyle bir işlemi desteklemekte ve hatta

51
00:03:28,000 --> 00:03:35,085
çatıya bağlı olarak trainableParameter = 0 gibi
şeylere de sahip olabilmektedirler.

52
00:03:35,085 --> 00:03:37,660
Belki önceki katmanlar için böyle bir şey yapabilirsiniz.

53
00:03:37,660 --> 00:03:39,117
Diğerlerinde ise

54
00:03:39,117 --> 00:03:42,885
bu ağırlıkları eğitme veya bazen

55
00:03:42,885 --> 00:03:47,295
freeze = 1 gibi bir parametre vardır.

56
00:03:47,295 --> 00:03:50,730
Bunlar, herhangi bir katman ile ilişkili
ağırlıkları eğitip eğitmeyeceğinizi

57
00:03:50,730 --> 00:03:56,245
belirlemenizi sağlayan farklı yollar ve
farklı derin öğrenme çatılarıdır.

58
00:03:56,245 --> 00:03:58,440
Bu durumda, yalnızca

59
00:03:58,440 --> 00:04:04,945
eşiksiz en büyük işlevi katmanının ağırlıklarını
eğitip geri kalanları dondurursunuz.

60
00:04:04,945 --> 00:04:07,930
Bazı uygulamalarda yardımcı olabilecek güzel bir püf noktası,

61
00:04:07,930 --> 00:04:12,380
önceki katmanlar dondurulduğu için

62
00:04:12,380 --> 00:04:16,345
burada siz değiştirmediğiniz ve eğitmediğiniz
için değişmeyen fonksiyonlar vardır

63
00:04:16,345 --> 00:04:20,040
ve bu fonksiyonlar, girdi imgesi x'i alıp

64
00:04:20,040 --> 00:04:24,305
o katmandaki bazı etkilenimlere eşler.

65
00:04:24,305 --> 00:04:30,455
Eğitimi hızlandıracak başka bir püf noktası ise
o katmanı önceden hesaplayarak

66
00:04:30,455 --> 00:04:36,615
o katmandaki etkilenimlerin özniteliklerini buluruz
ve doğrudan diske kaydederiz.

67
00:04:36,615 --> 00:04:40,620
Burada yaptığınız şey, sinir ağının bu kısmındaki

68
00:04:40,620 --> 00:04:43,030
sabit fonksiyonu kullanarak

69
00:04:43,030 --> 00:04:49,680
herhangi bir girdi imgesi X'i alıp bunun için
bir öznitelik vektörü hesaplarsınız

70
00:04:49,680 --> 00:04:56,445
ve ardından bu vektörü kullanarak bir sığ eşiksiz
en büyük işlevi modeli eğiterek tahmin yaparsınız.

71
00:04:56,445 --> 00:05:04,170
Eğitim setindeki her örnek için
bu katmandaki etkilenimleri önceden hesaplayıp

72
00:05:04,170 --> 00:05:06,330
diske kaydetmek ve ardından eşiksiz
en büyük işlevi sınıflandırıcısını eğitmek

73
00:05:06,330 --> 00:05:10,525
hesaplama kısmında yardımcı olabilecek işlemlerden birisidir.

74
00:05:10,525 --> 00:05:12,585
Diske kaydetme veya önceden hesaplama yönteminin avantajı,

75
00:05:12,585 --> 00:05:15,990
eğitm seti üzerindeki her geçişte

76
00:05:15,990 --> 00:05:19,020
etkilenimleri tekrardan hesaplamak

77
00:05:19,020 --> 00:05:23,150
zorunda olmamanızdır.

78
00:05:23,150 --> 00:05:28,585
Probleminiz için oldukça küçük bir veri setine
sahip olduğunuzda yapacağınız şey budur.

79
00:05:28,585 --> 00:05:31,215
Peki ya daha büyük bir veri setiniz varsa?

80
00:05:31,215 --> 00:05:33,810
Tercih edilen yöntemlerden birisi,

81
00:05:33,810 --> 00:05:39,164
eğer Tigger'ın ve Misty'nin olduğu ve hiçbirinin
olmadığı çok sayıda fotoğraf içeren

82
00:05:39,164 --> 00:05:41,940
etiketlenmiş büyük bir veri setiniz varsa,

83
00:05:41,940 --> 00:05:45,935
bu durumda daha az sayıda katmanı dondurmaktır.

84
00:05:45,935 --> 00:05:52,761
Örneğin bu katmanları dondurup, bu katmanları eğitirsiniz.

85
00:05:52,761 --> 00:05:57,540
Ancak eğer çıktı katmanı farklı sınıflara sahip ise

86
00:05:57,540 --> 00:06:04,321
çıktısı Tigger, Misty veya hiçbiri olacak şekilde kendi çıktı
katmanınızı geliştirmeniz gerekmektedir.

87
00:06:04,321 --> 00:06:07,550
Bunu yapmak için birkaç farklı yol vardır.

88
00:06:07,550 --> 00:06:10,980
Son birkaç katmanın ağırlıklarını alıp başlangıç olarak kullanabilir

89
00:06:10,980 --> 00:06:17,346
ve buradan itibaren gradyan inişi yaparsınız veya

90
00:06:17,346 --> 00:06:22,050
bu son birkaç katmanı yok ederek kendi gizli birimlerinizi

91
00:06:22,050 --> 00:06:27,990
kullanıp kendi eşiksiz en büyük işlevi çıktınızı kullanırsınız.

92
00:06:27,990 --> 00:06:32,000
Bu yöntemlerin hepsi denemeye değer.

93
00:06:32,000 --> 00:06:35,220
Ancak eğer daha çok veriye sahipseniz,

94
00:06:35,220 --> 00:06:39,090
dondurabileceğiniz katmanların sayısı daha küçük olabilir

95
00:06:39,090 --> 00:06:43,810
ve eğitebileceğiniz katmanların sayısı daha büyük olabilir.

96
00:06:43,810 --> 00:06:46,710
Buradaki ana fikir, eğer daha büyük bir veri setiniz varsa

97
00:06:46,710 --> 00:06:51,090
yalnızca tek bir eşiksiz en büyük işlevi birimi eğitmek değil,

98
00:06:51,090 --> 00:06:54,960
aynı zamanda kullandığınız sinir ağının
sonundaki birkaç katmanı içeren

99
00:06:54,960 --> 00:07:00,400
modern boyutlardaki bir sinir ağını eğitmektir.

100
00:07:00,400 --> 00:07:03,965
Son olarak, eğer çok fazla veriye sahipseniz

101
00:07:03,965 --> 00:07:09,710
yapabileceğiniz şeylerden birisi, açık kaynaklı
ağlardan birini ve onun ağırlıklarını alıp

102
00:07:09,710 --> 00:07:15,430
bunların tamamını başlangıç olarak kullanabilir
ve tüm sinir ağını eğitebilirsiniz.

103
00:07:15,430 --> 00:07:20,945
Ancak eğer bu, 1000 olası çıktıya
sahip bir eşiksiz en büyük işlevi katmanına sahipse

104
00:07:20,945 --> 00:07:23,610
ve sizin yalnızca 3 olası çıktınız varsa, kendi
eşiksiz en büyük işlevi katmanınızı oluşturmanız gerekir.

105
00:07:23,610 --> 00:07:26,133
İlgilendiğiniz etiketlerin çıktısı.

106
00:07:26,133 --> 00:07:29,760
Ancak probleminiz için daha çok
etiketlenmiş veriye sahipseniz

107
00:07:29,760 --> 00:07:33,630
yani Tigger'ın ve Misty'nin olduğu ve
hiçbirinin olmadığı daha çok fotoğrafa sahipseniz,

108
00:07:33,630 --> 00:07:37,005
daha çok katman eğitebilirsiniz ve hatta daha da aşırı durumda

109
00:07:37,005 --> 00:07:40,365
indirdiğiniz ağırlıkları, rastgele belirlemek yerine

110
00:07:40,365 --> 00:07:42,405
başlangıç ağırlıkları olarak kullanırsınız.

111
00:07:42,405 --> 00:07:45,260
Ardından gradyan inişini uygulayıp sinir ağının

112
00:07:45,260 --> 00:07:50,838
tüm katmanlarını ve ağırlıklarını güncelleyerek eğitirsiniz.

113
00:07:50,838 --> 00:07:54,510
Bu evrişimli sinir ağları için transfer öğrenmesidir.

114
00:07:54,510 --> 00:08:00,090
İnternetteki indirdiğiniz veri setleri
oldukça büyük olduğu için ve

115
00:08:00,090 --> 00:08:05,580
başkasının haftalarca üzerinde uğraştığı bu
indirebileceğiniz ağırlıklar, çok fazla veriden öğrendiği için

116
00:08:05,580 --> 00:08:08,385
birçok bilgisayarlı görü uygulamasında

117
00:08:08,385 --> 00:08:10,980
bu ağırlıkları indirip kendi probleminiz için başlangıç olarak

118
00:08:10,980 --> 00:08:16,080
kullandığınızda daha iyi sonuçlar aldığınızı göreceksiniz.

119
00:08:16,080 --> 00:08:18,410
Tüm diğer uygulama alanları arasında,

120
00:08:18,410 --> 00:08:21,720
derin öğrenmenin diğer alanları arasında,

121
00:08:21,720 --> 00:08:25,220
bence bilgisayarlı görü, transfer öğrenmesinin neredeyse

122
00:08:25,220 --> 00:08:28,815
her durumda kullanıldığı alanlardan birisidir. Tek istisna,

123
00:08:28,815 --> 00:08:35,145
her şeyi sıfırdan eğitebilecek kadar
büyük bir veri setine sahip olduğunuz durumdur.

124
00:08:35,145 --> 00:08:40,560
Ancak transfer öğrenmesi, her şeyi sıfırdan eğitecek kadar

125
00:08:40,560 --> 00:08:43,745
oldukça büyük bir veri setine ve
kaynağa sahip olduğunuz durum dışında

126
00:08:43,745 --> 00:08:47,350
kesinlikle denemeye değer şeylerden birisidir.