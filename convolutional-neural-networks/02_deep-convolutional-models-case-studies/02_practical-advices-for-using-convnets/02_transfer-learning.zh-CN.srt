1
00:00:00,270 --> 00:00:04,240
如果你想实现一个计算机视觉应用 而不想

2
00:00:04,240 --> 00:00:07,630
从零开始训练权重 比方从随机初始化开始（训练）

3
00:00:07,630 --> 00:00:10,010
实现更快的方式通常是下载

4
00:00:10,010 --> 00:00:12,530
已经训练好权重的

5
00:00:12,530 --> 00:00:15,640
网络结构 把这个作为预训练

6
00:00:15,640 --> 00:00:19,525
迁移到你感兴趣的新任务上

7
00:00:19,525 --> 00:00:24,230
计算机视觉的研究社区已经很擅长把许多

8
00:00:24,230 --> 00:00:28,860
数据库发布在网络上 如ImageNet MSCOCO

9
00:00:28,860 --> 00:00:31,082
PASCAL等数据库

10
00:00:31,082 --> 00:00:33,995
这些是已经公开在线的不同数据库的名字

11
00:00:33,995 --> 00:00:39,640
许多计算机视觉的研究者已经在上面训练了自己的算法

12
00:00:39,640 --> 00:00:44,760
有时（算法）训练要耗费好几周时间 占据许多GPU

13
00:00:44,760 --> 00:00:47,020
事实上有人已经做过

14
00:00:47,020 --> 00:00:50,005
这种训练 也经历过这种痛苦的高性能调试过程

15
00:00:50,005 --> 00:00:55,360
这意味着你可以下载这些开源的权重 有人曾花数周

16
00:00:55,360 --> 00:00:57,570
或数月来训练和调试 （这些权重）可以用来

17
00:00:57,570 --> 00:01:01,550
为你自己的神经网络做好的初始化开端

18
00:01:01,550 --> 00:01:05,012
且可以用迁移学习来迁移知识

19
00:01:05,012 --> 00:01:08,761
从这些大型公共数据库迁移知识到你自己的问题上

20
00:01:08,761 --> 00:01:11,710
我们来仔细了解下如何实现迁移学习

21
00:01:11,710 --> 00:01:13,760
举个例子

22
00:01:13,760 --> 00:01:18,820
比如你要建立一个可识别自己宠物猫的猫检测器

23
00:01:18,820 --> 00:01:22,570
根据网络常识

24
00:01:22,570 --> 00:01:32,855
Tigger和Misty是两个常见的猫名字

25
00:01:32,855 --> 00:01:41,480
假设你的猫叫Tigger和Misty 或其他名字

26
00:01:41,480 --> 00:01:44,570
所以你就有一个三分类的分类问题

27
00:01:44,570 --> 00:01:46,300
这幅图究竟是Tigger

28
00:01:46,300 --> 00:01:49,075
还是Misty 或者两者都不是

29
00:01:49,075 --> 00:01:53,875
或者有种情况就是两只猫都在这幅图里

30
00:01:53,875 --> 00:01:57,020
现在你可能没有足够多的Tigger的照片

31
00:01:57,020 --> 00:02:00,745
或者Misty的照片 这样你的训练集很小

32
00:02:00,745 --> 00:02:02,170
这时你能做什么

33
00:02:02,170 --> 00:02:06,920
我建议你上网下载一些开源的应用

34
00:02:06,920 --> 00:02:12,600
神经网络的应用 不但下载源码 还要下载相应权重

35
00:02:12,600 --> 00:02:19,900
你可以下载许多已经训练好的网络 例如

36
00:02:19,900 --> 00:02:25,130
在有1000类物体的ImageNet数据库上训练

37
00:02:25,130 --> 00:02:31,180
因此该网络有一个可以输出千分之一类别概率的softmax神经元

38
00:02:31,180 --> 00:02:36,110
你能做的是去掉其softmax层 然后创造自己

39
00:02:36,110 --> 00:02:46,025
softmax层来输出Tigger/Misty/其他

40
00:02:46,025 --> 00:02:48,710
从网络方面来看

41
00:02:48,710 --> 00:02:52,350
我建议你考虑冻结（前面）这些层

42
00:02:52,350 --> 00:02:56,415
即冻结相应参数

43
00:02:56,415 --> 00:03:00,300
该网络前面所有层（的参数）

44
00:03:00,300 --> 00:03:05,700
因此可以只训练与你自己softmax层有关的参数

45
00:03:05,700 --> 00:03:08,982
即与三个类别输出有关的softmax层

46
00:03:08,982 --> 00:03:11,790
Tigger Misty 或两者皆非

47
00:03:11,790 --> 00:03:16,560
通过用别人训练好的权重

48
00:03:16,560 --> 00:03:22,600
即使在很小的数据库上也可能得到很好的性能

49
00:03:22,600 --> 00:03:25,100
幸运的是 许多深度学习框架

50
00:03:25,100 --> 00:03:28,000
支持这样的用法 事实上

51
00:03:28,000 --> 00:03:35,085
根据框架结构 它有点像所包含的可训练参数等于零

52
00:03:35,085 --> 00:03:37,660
你可以把前面某些层这样设置（可训练参数为零）

53
00:03:37,660 --> 00:03:39,117
另一种说法是

54
00:03:39,117 --> 00:03:42,885
不要训练这些权重 或有时你可以用一个参数

55
00:03:42,885 --> 00:03:47,295
比如freeze等于1（来设置网络）

56
00:03:47,295 --> 00:03:50,730
这些不同的方法和不同的深度学习框架可以让你

57
00:03:50,730 --> 00:03:56,245
来确定是否训练与某些具体层相关的权重

58
00:03:56,245 --> 00:03:58,440
所以在现在的例子中 你可以只训练

59
00:03:58,440 --> 00:04:04,945
softmax层的权重 同时冻结前面所有层

60
00:04:04,945 --> 00:04:07,930
另一种应用中比较巧妙的方法

61
00:04:07,930 --> 00:04:12,380
是由于前面所有层被冻结了

62
00:04:12,380 --> 00:04:16,345
之前有些固定函数不会变 因为你现在也不改动它

63
00:04:16,345 --> 00:04:20,040
也不训练它 （因此）网络接受输入图像X

64
00:04:20,040 --> 00:04:24,305
并把X映射到该层的激活函数上

65
00:04:24,305 --> 00:04:30,455
因此这个方法可以加速训练 因为我们刚预计算过该层

66
00:04:30,455 --> 00:04:36,615
而该层重激活产生的特征就直接保存在硬盘上

67
00:04:36,615 --> 00:04:40,620
你所做的就是用这个固定函数

68
00:04:40,620 --> 00:04:43,030
在该神经网络的前半部分

69
00:04:43,030 --> 00:04:49,680
接受任一输入图像X 然后计算其特征向量 然后

70
00:04:49,680 --> 00:04:56,445
依据这个特征向量训练一个浅层softmax模型去预测

71
00:04:56,445 --> 00:05:04,170
因此预计算之前层的激活结果是有利于你计算的操作

72
00:05:04,170 --> 00:05:06,330
（预计算）训练集所有样本（激活结果）并存到硬盘上

73
00:05:06,330 --> 00:05:10,525
然后训练右边的softmax类别

74
00:05:10,525 --> 00:05:12,585
这样做的好处是

75
00:05:12,585 --> 00:05:15,990
预计算的好处是你不需要

76
00:05:15,990 --> 00:05:19,020
在训练集上每次迭代

77
00:05:19,020 --> 00:05:23,150
都重新计算这些激活结果

78
00:05:23,150 --> 00:05:28,585
所以 如果你的训练集比较小 以上就是你可以做的工作

79
00:05:28,585 --> 00:05:31,215
如果你的训练集更大

80
00:05:31,215 --> 00:05:33,810
一个经验之谈是

81
00:05:33,810 --> 00:05:39,164
如果你有更大的带标签数据集 比如许多Tigger图片

82
00:05:39,164 --> 00:05:41,940
或者Misty图片 或者别的猫图片

83
00:05:41,940 --> 00:05:45,935
你可以冻结更少的层数

84
00:05:45,935 --> 00:05:52,761
你可以只冻结这些层 然后训练后面这些层

85
00:05:52,761 --> 00:05:57,540
尽管输出层的类别与你需要的不同

86
00:05:57,540 --> 00:06:04,321
你的输出是Tigger Misty或其他

87
00:06:04,321 --> 00:06:07,550
有几种可以这样做的方法

88
00:06:07,550 --> 00:06:10,980
你可以用最后几层的权重

89
00:06:10,980 --> 00:06:17,346
作为初始化开始做梯度下降（训练）

90
00:06:17,346 --> 00:06:22,050
或者你也可以去掉最后几层

91
00:06:22,050 --> 00:06:27,990
然后用自己的新神经元和最终softmax输出（训练）

92
00:06:27,990 --> 00:06:32,000
两种方法都值得尝试

93
00:06:32,000 --> 00:06:35,220
但有个模式 即你数据越多

94
00:06:35,220 --> 00:06:39,090
所冻结的层数可以越少

95
00:06:39,090 --> 00:06:43,810
自己训练的层数可以越多

96
00:06:43,810 --> 00:06:46,710
其中的想法就是如果你选了个数据集

97
00:06:46,710 --> 00:06:51,090
有足够的数据 不仅可以训练单层softmax 还可以

98
00:06:51,090 --> 00:06:54,960
训练由所采用网络的最后几层组成的中型网络

99
00:06:54,960 --> 00:07:00,400
其中的最后几层你可以使用

100
00:07:00,400 --> 00:07:03,965
最后 如果你有许多数据

101
00:07:03,965 --> 00:07:09,710
你可以用该开源网络和权重

102
00:07:09,710 --> 00:07:15,430
（用它们）初始化整个网络然后训练

103
00:07:15,430 --> 00:07:20,945
尽管这是个1000类的softmax 而你只需要3类

104
00:07:20,945 --> 00:07:23,610
你需要自己的softmax输出

105
00:07:23,610 --> 00:07:26,133
自己想要的标签

106
00:07:26,133 --> 00:07:29,760
但是你有越多的带标签数据

107
00:07:29,760 --> 00:07:33,630
或者说有越多Tigger Misty或其他猫的图片

108
00:07:33,630 --> 00:07:37,005
你可以训练的层数就越多 极端的例子是

109
00:07:37,005 --> 00:07:40,365
你可以用下载的权重作为

110
00:07:40,365 --> 00:07:42,405
初始化 因此这些权重可以代替

111
00:07:42,405 --> 00:07:45,260
随机初始化 然后做梯度下降

112
00:07:45,260 --> 00:07:50,838
训练来更新所有权重和网络层

113
00:07:50,838 --> 00:07:54,510
所以 这就是卷积神经网络训练中的迁移学习

114
00:07:54,510 --> 00:08:00,090
实际中因为网上公开数据库非常大 你下载的权重

115
00:08:00,090 --> 00:08:05,580
你下载的权重也是别人从大量数据上花费数周训练而来的

116
00:08:05,580 --> 00:08:08,385
你会发现对许多计算机视觉应用来说

117
00:08:08,385 --> 00:08:10,980
你可以做的更好 如果

118
00:08:10,980 --> 00:08:16,080
如果你下载别人开源的权重来初始化自己的问题

119
00:08:16,080 --> 00:08:18,410
因此在所有不同领域中

120
00:08:18,410 --> 00:08:21,720
在所有深度学习应用的领域中

121
00:08:21,720 --> 00:08:25,220
我认为计算机视觉是其中一个迁移学习

122
00:08:25,220 --> 00:08:28,815
可以一直应用的领域 除非

123
00:08:28,815 --> 00:08:35,145
除非你有特别大的数据集来从零训练自己的网络

124
00:08:35,145 --> 00:08:40,560
而迁移学习值得认真考虑除非

125
00:08:40,560 --> 00:08:43,745
你的确有特别海量的数据和计算资源

126
00:08:43,745 --> 00:08:47,350
来自己从零开始