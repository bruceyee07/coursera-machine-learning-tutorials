您現在已經準備好來建置
一個一層的卷積神經網路 讓我們來看看這個例子。 您已經在前面的影片中看到如何將三維的資料值 用兩個不同的過濾器進行卷積處理 在這個例子中，為了要得到一個新的四乘四二維的輸出資料值 我們就先用第一個過濾器 卷積運算後獲得一個四乘四的輸出值 然後用第二個過濾器卷積運算，
得到另外一個四乘以四的輸出值 要完成這個卷積神經網路層的最後一個步驟 就是要在每個運算中加上一個偏差值 就是加上一個實數 因為python程式語言的運算廣播原則，這個實數就會跟所有 16 個元素的每一項相加 然後我們加上一個非線性函數運算 在這裡我們用了ReLU函數 會得到一個 4乘4 輸出值 加入了偏差值跟非線性函數運算之後 然後對於底下這一項也一樣，
您加入一個不同的偏差值 同樣地也是一個實數 所以您將一個實數加到所有這 16 項上 然後使用一個非線性函數，
假設也是 ReLU 函數 這會給您另外一個不同的 4乘4 輸出值 就如同前面的運算步驟，如果我們把這些輸出值 像這樣疊起來，
我們會得到一個 4乘4乘2 輸出值 那這個計算您從一個 6乘6乘3 
到一個 4乘4乘4 (應該是 4乘4乘2) 這就是一個一層的卷積神經網路 所以對應到一層的
標準神經網路的正向傳播 在非卷積神經網路中 記得在傳播前的一個步驟是像這樣，對吧？ z[1] = w[1] 乘 a[0],
a[0] 等於 x 然後加 b[1] 您應用一個非線性函數到這個 a[1], 所以是  g(z[1]) 在這裡的輸入，類似於這個是 a[0], 其實是 x 而這些過濾器 扮演的角色類似 w1 您記得在做卷積運算時，您拿這 27 個數字，或者實際上是  27乘2， 因為您有兩個過濾器 您拿所有這些數字跟這個相乘 所以您其實是在計算一個線性函數
來得到這個 4乘4 矩陣 所以這個 4乘4 矩陣，卷積運算的輸出 扮演的角色類似於 w[1] 乘 a[0] 輸出是這個 4乘4 
跟這個  4乘4 然後另外一件事
是您加入這個偏差 所以在應用 ReLU 之前這些東西 扮演的角色類似 z 最後透過應用非線性函數，我猜像這樣 所以，這個輸出扮演的角色 其實變成是您的下一層的啟動值 所以這是您如何從 a[0] 到 a[1],
首先是線性函數運算 然後卷積讓這些相乘 所以卷積實際上是應用了線性運算 然後加入了偏差，
再應用了 ReLU 運算 您從一個 6乘6乘3 維度的  a[0] 經過了一層的神經網路 到了一個 4乘4乘2 維度的  a[1] 所以 6乘6乘3 
變成 4乘4乘2 這就是一層的卷積網路 在這個例子中我們用了兩個過濾器，
可以當成是兩個特徵 這也是為什麼我們
最終我們的輸出會是 4乘4乘2 但假設我們如果用 10 個過濾器，
而不是 2 個過濾器 那我們最終會得到一個 
4乘4乘10 維度的輸出容積 因為我們會將這 10 個而不是 2 個，疊起來 變成一個 4乘4乘10 輸出容積，
這將會是我們的 a[1] 為了確保您理解這些，
我們再來一個例子 假設您有 10 個過濾器，
不只是兩個過濾器，是 3乘3乘3 一層的神經網路，這一層有多少個參數呢？ 讓我們看看 每一個過濾器都是 3乘3乘3 容積，所以是 3x3x3 所以每個過濾器有 27 個參數，對吧？ 總共有 27 個數字來運算，加上偏差 也就是 b 參數，總共給您 28 個參數 然後如果您想像在前面的投影片中，
我們畫了兩個過濾器 但現在如果您想像您實際上有 10 個過濾器 1,2, ... 10 個這樣的 總共您會有 28乘10 所以會是 280 個參數 請注意這樣有一個好處，
不管輸入的影像多大 輸入影像可以是 1000乘1000, 
或者 5000乘5000 但是參數的數目還是
維持固定的 280 個 您可以使用這 10 個過濾器來
偵測特徵，垂直邊緣 水平邊緣，或者其他特徵
在任何地方，即使是 很大很大的影像，
只需要小數目的參數 這真的是卷積神經網路的一個特性 比較不容易過適，然後您可以 一旦您成功學習了 10 個特徵偵測 您可以將這個應用在大的影像上 而參數的數目
還是固定且相對少的 像這個例子是 280 好的，總結一下，我們使用的記號 將用來描述一層的 卷積神經網路 所以 L 層是一個卷積層 我會用 f 上標 [l] 來記為過濾器大小 所以前面我們看過的過濾器是 f乘f 現在這個上標方括號 L 記為 這是一個過濾器
大小為 f乘f 在 L 層 和往常一樣, 上標方括弧 L 是我們使用的符號 指的是第 Ｌ層 將使用 p[l] 記為填充量 再一次，填充的大小
也可以說您要的是 有效卷積，也就是說沒有填充 相同卷積，也就是您選擇的填充 讓輸出的大小跟輸入的高度，寬度一樣 然後您使用 s[l] 來記跨步 現在這一層的輸入將會是一些維度 將會是一個 n乘n乘 在前一層的通道數目 現在我將稍微修改一下這個記號 我將使用上標 l-1 因為那是從 前一層來的啟動值，L-1 乘 nc L-1 我們目前使用的例子，
我只使用高度跟寬度相同的影像 假如高度跟寬度不同 我將會使用上標 h 跟上標 w 來記 來自於前面一層高度跟寬度 所以在第 Ｌ層，容積的大小會是 nh 乘nw乘nc 上標為方括號 L 只是因為在 L 層，
這一層的輸入會從 前面一層來的，
這是為什麼這裡是 L-1 然後這一層神經網路的輸出值 也就是 nh[L]乘nw[L]乘nc[L] 這就是輸出的大小 所以當我們顯示輸出的容積大小 或者至少高度跟寬度是這個公式 n+2p-f 除 s +1 然後用捨去法 在這個新的記號上，我們的輸出值是在 L 層 將是這個從前面一層的維度 加上我們用在Ｌ層的填充 減去Ｌ層的過濾器大小等等 基本上這個是高度，對吧？ 所以輸出容積的高度是這樣，
您可以從 右邊的公式計算得到，
寬度也是一樣 所以您可以劃掉 h 然後用 w , 同樣的公式
對於不管是高度或者 寬度，代入計算可以得到
輸出值的高度或者寬度 所以這是 nh[L-1] 跟 nh[L] 的關係
以及nw[L-1] 跟 nw[L] 的關係 那麼，通道的數目呢？這些數字從何而來？ 我們來看看，如果這個輸出值有這個深度 我們從前面的例子得知，這是等於 在那一層過濾器的數目，對吧？ 我們如果有兩個過濾器，
輸出值會是 4乘4乘2, 是 2 維 而如果您有 10 個過濾器，
您的輸出容積會是 4乘4乘10 所以在輸出值的通道數目 也就是我們在神經網路這一層
使用的過濾器數目 接下來，這個過濾器的大小為何？ 每一個過濾器都是 f[L]乘f[L]乘 一個數字 最後一個數字是多少？ 我們看到您需要卷積一個 6乘6乘3 影像 用一個 3乘3乘3 過濾器 所以過濾器的通道數目，必須等於 您輸入的通道數目，
所以這個數字要等於那個數字，對吧？ 這也就是每個過濾器將會是 
f[L]乘f[L]乘nc[L-1] 而這一層的輸出通常會
應用上偏差跟非線性 這將會是這一層的啟動值 a[L] 我們已經看過它的維度了，對吧？ a[L] 會是一個 3D 的容積 也就是  nh[L]乘nw[L]乘nc[L] 而當您使用向量化建置，或者批次梯度下降 或者小批次梯度下降，然後您真的輸出 a[L] 也會是一組 m 啟動值，
如果您有 m 個例子 所以會是  m乘nh[L]乘nw[L]乘nc[L], 對吧？ 假設您用的是批次梯度下降 在程式練習中，這會是變數的次序 我們會先放索引跟訓練例子 然後這三個變數 接下來權重或者參數呢？
或者像是  w 參數呢？ 我們在過濾器的維度時已經見過 過濾器將會是 f[L]乘f[L]乘nc[L-1] 但那是一個過濾器的維度 我們有多少過濾器？ 這是過濾器的總數， 所以權重其實是將所有的過濾器放在一起，
所以維度會是 這個乘上過濾器的數目，對吧？ 因為這個，最後這個數是過濾器的數目 在 L 層 最後，您有這個偏差參數 您有一個偏差參數，
一個實數對每一個過濾器 所以您將會有這個偏差，會有這些變數 這只是一個這個維度的向量 雖然後面我們在程式中會 比較方便的用 
1乘1乘1乘nc[L] 4 維度的矩陣，或者說 4 維度的張量 我知道這裡有很多的符號 這個是我大部分使用的慣例 我只是想提醒一下，
假設您在線上搜尋或者看開源程式 並沒有完全通用的標準公約有關於 高度，寬度跟通道的次序 所以如果您在 Github 看開源程式，
或者開源程式的建置 您會發現有些作者用另外一種順序，您先放 通道，有時候您會看到這種變數的次序 事實上一些通用的框架，實際上多個框架 實際在放變數或者說參數時 您想將通道的數目先放，或者 將通道的數目放在最後
當將索引放進這個容積時 我想兩種公約都可以，只要您用的時候一致 很不幸的，或許這個符號 在深度學習文獻上並不一致 但我會用這個公約在我們的影片中 也就是高度跟寬度，
然後通道的數目放最後 我知道突然有很多的符號您要用， 您或許會想，哇，
這麼長的符號，我要怎樣記住這些 不用擔心，您不需要去記這些符號 透過這個禮拜的練習，屆時您會變得比較熟悉 但這個影片的重點是 一個一層的卷積神經網路如何作用 跟相關的計算從拿一層的啟動值 對應到下一層的啟動值 接下來，您理解了一層的卷積神經網路如何作用 我們來將一堆這樣的東西疊起來變成一個 更深度的卷積神經網路 我們下一段影片見