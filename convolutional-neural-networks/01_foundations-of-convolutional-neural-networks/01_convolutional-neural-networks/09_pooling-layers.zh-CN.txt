除卷积层之外 ConvNets通常还使用池化层来减少展示量 以此来提高计算速度 并使一些特征的检测功能更强大 我们来看一个池化的例子 然后我们会讨论一下为什么要这么做 假设你有一个4x4的输入 并且你想使用一种池化类型，称作max pooling 这个max pooling的输出 将会是一个2x2的输出。 实现它的做法非常简单 将4*4的输入划分为不同的区域 如图所示，我将给四个区域赋予不同的颜色 然后 在它的输出里 一个2x2的输出 每个输出值将会是其对应颜色区域的最大值 所以在左上角 这四个数字的最大值是9 右上角蓝色背景数字的最大值是2 左下角最大的数字是6 而右下角最大的数字是3 所以当计算右手边每一个格子里的数字时 我们选取那些2x2区域的最大值 这如同你使用了一个大小为2的过滤器 因为你选取一个2x2的区域，并且使用的步长为2 这些实际上就是max pooling的超参数 因为我们是从这个过滤器大小开始的 这就像一个2x2的区域给出最大值9 然后向前走两步来看看个区域 给出2 然后来到下一行 你向下走两步 这里给出的是6 再向右走两步得到3 然后因为这些方块是2x2 f就等于2 又因为你的步长时2 s就等于2 这里来说一下max pooling背后的机制 如果你把这个4x4的区域看作某个特征的集合 即神经网络某个层中的激活状态 那么 一个大的数字 意味着它或许检测到了一个特定的特征 所以 左侧上方的四分之一区域有这样的特征 它或许是一个垂直的边沿 亦或一个更高或更弱 显然 左侧上方的四分之一区域有那个特征 然而这个特征 或许它不是猫眼检测 但是 右侧上方的四分之一区域没有这个特征 所以 max pooling做的是 检测到所有地方的特征 四个特征中的一个被保留在max pooling的输出中 所以，max pooling作所做的其实是 如果在滤波器中任何地方检测到了这些特征 就保留最大的数值 但是 如果这个特征没有被检测到 可能左侧上方的四分之一区域就没有这个特征 于是 那些数值的最大值仍然相当小 这或许就是max pooling背后的解释 但是 不得不承认 我认为大家使用max pooling的主要原因是 因为在很多实验中发现它的效果很好 以及我刚刚描述的机制 尽管它经常被引用 我不知道是否有人完全了解这是真正的根本原因 也不知道有谁知道这是否是 max pooling在卷积网络中效果很好的根本原因 max pooling的一个有趣的特性是 它有一套超参 但是它没有任何参数需要学习 实际上 没有任何需要梯度相加算法学习的东西 一旦确定了 f 和 s 就确定了计算 而且梯度下降算法不会对其有任何改变 让我们来看一个有某些不同超参的例子 这里我们打算使用一个5x5的输入 并且为max pooling应用一个大小为3x3的过滤器 于是 f 等于3 并且我们使用步长为1 在这个例子中 输出的大小将会是3x3 我们在之前的视频中用来计算卷基层输出的大小 而推到出的公式 那些公式对max pooling同样适用 那个公式是 n 加上 2p 减去 s 的 f 次方再加 1 这个公式同样可用于计算max pooling的输出的大小 但在这个例子中 让我们来计算这个3x3输出的每一个元素 左侧上方的元素 我们要来看看那个区域 注意到这是一个3x3的区域 因为滤波器的大小是3 并取其最大值 所以输出应该是9 然后我们移动一列 因为步长是1 于是 蓝色方块中的最大值是9 让我们再向右移动一列 蓝色方块中最大值是5 让我们去道下一行 步长为1 所以 我们仅仅向下走一步 那个区域的最大值是9 这个区域的最大值是9 再来看这个区域的最大 现在有两个5 所以有多个最大值都是5 最后，这个区域的最大值是8 这里的最大值是6 以及这个区域的最大值是9 到此，这个集合的超参有 f 等于3 s 等于1，给出如图所示的输出 截至目前，我演示了在二维输入上的max pooling 如果有一个三维输入 则输出会具有同样的维数 例如，如有有一个5x5x2的输入 则输出将是3x3x2，并且 计算最大值采样的方法是使用 我们刚刚描述的每个通道的计算过程 所以，第一个通道，即和这里显示的顶层一样 然后第二个通道，我猜测 这个我刚画在底下的通道 你应该在那一层上做相同的操作 这样会给出第二层 更一般得来讲，假如输入是5乘5乘一些通道数 输出应该是3乘3乘同样的通道数 并且最大值采样计算是在这些 N_C 个通道上独立进行的 这就是最大值采样(max pooling) 这个是一类并非很常使用的采样 但是我会简要提一下均值采样 它所做的基本上是你所预期的 不是在每个滤波器中取最大值 而是取其平均值 在这个例子里 紫色区域中数值的均值是3.75 然后是1.25 然后是4和2 这是均值采样，其超参为 f 等于2 s 等于2，我们也可以选择其它超参 目前，最大值采样的使用 通常比均值采样多得多，唯一的例外 是有时候在深度非常大的神经网络 你也许可以使用均值采样来合并表示 例如从7x7x1000 将它们整体取一个均值 得到1x1x1000 我们稍后会看到这样的一个例子 但是你看，在神经网络中最大值采样比均值采样用的更多 总结一下 采样的超参是滤波器的大小 f 以及步长 s 或许常见的参数选择可能是 f 等于2，s 等于2 这个相当常用并且其效果是 近似得把高度和宽度缩小了两倍以上 并且一个被普遍选用的超参可能是 f 等于2，s 等于2 它的效果是将表示的 高度和宽度缩小了两倍 我还见过 f 等于3，s 等于2 而其它的超参就像一个二进制位元 指示你是使用最大值采样还是均值采样 如果你愿意，你可以增加一个额外的补位超参 尽管这个是极其罕见被使用的 当你做最大值采样时，通常情况下 不会做任何补位 尽管有一个例外，这个我们下周会看到 但是，对于最大值采样的绝大多数部分 通常情况下，是不进行任何的补位 所以，截至目前 p 最为常用的取值是 p 等于零 最大值采样的输入是，你输入一个大小为 N_H 乘 N_W 乘 N_C 的体数据 它会输出一个如下所述大小的体数据 假设没有大小为 N_W 减去 s 的 f 次方的补位 这个是 N_C 因此，输入通道的数量等于输出通道的数量 因为采样是独立地应用于每一个通道 关于采样，一个值得注意的是，它没有需要学习的参数 所以，在实现剪切的时候 你会发现，没有参数需要通过最大值采样进行调整 反而是，这里只有这些超参需要你设定一次 或许是人工设定或者使用交叉检验 除此之外，你就不用做什么了 它就是一个神经网络在其中一层计算的确定函数 而这里实际上没有任何需要学习的 它仅仅是一个确定的函数 这就是采样的所有内容 现在你知道如何构建卷积层和采样层 这里这个 让我们看一个更为复杂的卷积网络的例子 这个例子还会让我们引入全连结曾