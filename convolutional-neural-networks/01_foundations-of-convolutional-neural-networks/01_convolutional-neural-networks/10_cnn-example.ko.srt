1
00:00:00,021 --> 00:00:02,721
여러분은 이제 전체 컨볼루션 신경망을 구축하는

2
00:00:02,721 --> 00:00:04,509
모든 빌딩 블록을 꽤 많이 알고 있습니다.

3
00:00:04,509 --> 00:00:07,336
예시를 한번 봐 볼게요.

4
00:00:07,336 --> 00:00:12,436
32 x 32 x 3 크기의 이미지를 입력한다고 가정 해 보겠습니다.

5
00:00:12,436 --> 00:00:18,855
이것은 RGB 이미지이고, 손으로 쓰는 숫자 인식을 하려고 합니다.

6
00:00:18,855 --> 00:00:24,396
32 x 32 RGB 시작점에서 7과 같은 숫자를 사용하여

7
00:00:24,396 --> 00:00:30,495
0부터 9까지의 10 자리 숫자 중 어느 것이 이것인지 인식하려고 합니다.

8
00:00:30,495 --> 00:00:32,791
이를 위해 신경망을 사용해 봅시다.

9
00:00:32,791 --> 00:00:35,827
이 슬라이드에서 사용할 내용은 영감을 받은 건데요,

10
00:00:35,827 --> 00:00:41,106
LeNet-5라는 고전적인 신경망중 하나와 실제로 비슷합니다.

11
00:00:41,106 --> 00:00:43,942
이 LeNet-5는 Yann LeCun이 수년 전에 만들었습니다.

12
00:00:43,942 --> 00:00:47,680
제가 보여드리려고 하는 것은 정확히 LeNet-5가 아니지만

13
00:00:47,680 --> 00:00:53,024
이것에서 영감을 얻었죠, 많은 파라미터 선택은 이것의 영감을 받은 겁니다.

14
00:00:53,024 --> 00:00:58,524
32 x 32 x 3 인풋, 이걸 첫 번째 레이어라고 부르도록 하죠.

15
00:00:58,524 --> 00:01:04,770
이것은 필터=5, 스트라이드=1, 패딩없음 을로 사용하겠습니다.

16
00:01:04,770 --> 00:01:08,240
6개의 필터를 사용한다면

17
00:01:08,240 --> 00:01:13,732
이 레이어의 아웃풋은 28 x 28 x 6 이 됩니다.

18
00:01:13,732 --> 00:01:18,803
그럼 이 레이어를 Conv1 이라고 부르도록 합시다.

19
00:01:18,803 --> 00:01:23,660
6개의 필터를 적용하고, 바이어스를 더해준 후, 비선형성을 적용해주세요.

20
00:01:23,660 --> 00:01:28,356
아마 진짜 비선형성 이겠죠, 그럼 Conv1 아웃풋을 생성하게 됩니다.

21
00:01:28,356 --> 00:01:32,678
다음으로, pooling 레이어를 적용해봅시다.

22
00:01:32,678 --> 00:01:40,280
여기에 대량 pooling을 적용하고 f=2, s=2를 사용하도록 하겠습니다.

23
00:01:40,280 --> 00:01:44,101
패딩을 쓰지 않을 때는 0으로 쉽게 패드를 사용하십시오.

24
00:01:44,101 --> 00:01:48,895
다음으로 pooling 레이어를 적용 해 보겠습니다.

25
00:01:48,895 --> 00:01:54,975
2 x 2 필터로 max pooling을 봅시다. 스트라이드=2 입니다.

26
00:01:54,975 --> 00:01:57,064
따라서 표현의 높이와 너비는

27
00:01:57,064 --> 00:01:59,614
2 배로 줄어들어야 합니다.

28
00:01:59,614 --> 00:02:04,138
따라서 28 x 28은 14 x 14가 됩니다. 그리고

29
00:02:04,138 --> 00:02:10,472
채널의 수는 14 x 14 x 6 으로 동일하게 남아있게 됩니다.

30
00:02:10,472 --> 00:02:15,536
이걸 Pool 1 아웃풋이라고 부르겠습니다.

31
00:02:15,536 --> 00:02:20,111
컨볼네트 서적에 보면

32
00:02:20,111 --> 00:02:25,562
레이어라고 부르는 것에 대해 일관성 없는 두 가지 규칙이 있음이 밝혀졌습니다.

33
00:02:25,562 --> 00:02:30,918
한 가지 규칙은 이것을 하나의 레이어라고 부르는 것입니다.

34
00:02:30,918 --> 00:02:35,900
따라서 이것은 신경망의 레이어 1이 될 것입니다. 그리고 다른 규칙들은,

35
00:02:35,900 --> 00:02:40,980
Conv 1 레이어를 레이어로 전달하고, Pool 1 레이어를 레이어 로 전달하는 것을 가리킵니다.

36
00:02:40,980 --> 00:02:45,223
사람들이 신경망에서 레이어의 숫자를 이야기할 때, 보통

37
00:02:45,223 --> 00:02:49,025
가중치가 있고, 파라미터가 있는 레이어의 숫자를 기록합니다.

38
00:02:49,025 --> 00:02:53,043
pooling 레이어는 가중치도 없고, 파라미터도 없고, 하이퍼 파라미터도 몇 개만 있기 때문에,

39
00:02:53,043 --> 00:02:57,418
Conv 1과 Pool 1 이

40
00:02:57,418 --> 00:02:59,015
함께 공유하는 규칙을 사용하도록 하겠습니다.

41
00:02:59,015 --> 00:03:03,551
이걸 레이어 1로 다루도록 하겠습니다. 비록 사람들이

42
00:03:03,551 --> 00:03:08,447
온라인으로 아티클 보고 연구논문을 읽으면

43
00:03:08,447 --> 00:03:11,703
conv 레이어 와 pooling 레이어가 두 개의 분리된 레이어인 것처럼 되어있습니다.

44
00:03:11,703 --> 00:03:16,788
하지만 이것은 약간 일관성이 없게 표기된 용어이기도 합니다.

45
00:03:16,788 --> 00:03:22,053
그래도 레이어를 셀 때, 가중치가 있는 레이어를 세어서,

46
00:03:22,053 --> 00:03:25,614
이 둘 모두 레이어 1으로 얻어낼 것입니다.

47
00:03:25,614 --> 00:03:30,822
Conv1 과 Pool1 은 이름에 1을 끝에 붙여서 사용하는데

48
00:03:30,822 --> 00:03:37,961
이는 제가 이 둘 모두를 신경망의 레이어 1의 일부로 생각한다는 것을 가리키는 것입니다.

49
00:03:37,961 --> 00:03:42,665
Pool 1 은 레이어 1으로 그룹화 됩니다. 자신의 가중치가 없기 때문이죠.

50
00:03:42,665 --> 00:03:47,585
다음으로, 여기 주어진 14 x 14 x 16 볼륨으로

51
00:03:47,585 --> 00:03:53,181
이것에 컨볼루션 레이어를 적용해 보죠. 그리고 5 x 5 필터를 사용합시다.

52
00:03:53,181 --> 00:03:58,796
스트라이드=1, 이번엔 필터가 10개라고 합시다.

53
00:03:58,796 --> 00:04:04,350
그럼 A 10 x 10 x 10 볼륨이 만들어 지고

54
00:04:04,350 --> 00:04:09,786
이제 이걸 Conv2라고 부르겠습니다.

55
00:04:09,786 --> 00:04:14,467
이 네트워크에서 f=2, s=2 로

56
00:04:14,467 --> 00:04:19,008
max pooling을 합시다.

57
00:04:19,008 --> 00:04:23,456
그러면 이 아웃풋을 추측해볼 수도 있을 텐데요, f=2

58
00:04:23,456 --> 00:04:26,769
s=2, 이것은 2배로 높이와 넓이를 줄여줄 것입니다.

59
00:04:26,769 --> 00:04:31,425
따라서 5 x 5 x 10으로 남게 됩니다.

60
00:04:31,425 --> 00:04:34,773
이제 이걸 Pool2 라고 부르겠습니다.

61
00:04:34,773 --> 00:04:39,652
우리 규칙으로 이것은 신경망의 레이어 2 입니다.

62
00:04:39,652 --> 00:04:42,293
이제 여기에 또 다른 컨볼루션 레이어 를 적용해봅시다.

63
00:04:42,293 --> 00:04:47,113
5 x 5 filter를 사용하니, f=5, 이렇게 하면

64
00:04:47,113 --> 00:04:51,962
1, 그리고 패딩은 없습니다.

65
00:04:51,962 --> 00:04:58,254
이것은 Conv2 아웃풋을 생성하고 이는 16개의 필터가 됩니다.

66
00:04:58,254 --> 00:05:03,860
따라서 이것은 10 x 10 x 16 차수의 아웃풋이죠.

67
00:05:03,860 --> 00:05:10,380
여기 보시면, 이것은 Conv2 레이어입니다.

68
00:05:10,380 --> 00:05:17,356
max pooling 을 f=2, s=2으로 여기에 적용해봅시다.

69
00:05:17,356 --> 00:05:20,227
여러분은 이 아웃풋을 추측해볼 수 있을 겁니다.

70
00:05:20,227 --> 00:05:24,555
우리는 f=2, s=2 로 된 max pooling으로 10 x 10 x 16이 되었습니다.

71
00:05:24,555 --> 00:05:26,667
이것은 높이와 넓이를 반으로 줄여줄 것입니다.

72
00:05:26,667 --> 00:05:31,075
이 결과는 추측해볼 수 있으시겠죠?

73
00:05:31,075 --> 00:05:32,463
왼쪽 pooling은 f=2, s=2가 됩니다.

74
00:05:32,463 --> 00:05:37,663
높이와 넓이를 반으로 줄여서 결국

75
00:05:37,663 --> 00:05:43,214
예전과 동일한 채널 수를 가지게 되는 것입니다.

76
00:05:43,214 --> 00:05:47,166
이것을 Pool2 라고 부르겠습니다.

77
00:05:47,166 --> 00:05:52,429
우리 규칙에서 이것은 레이어 2 이죠.

78
00:05:52,429 --> 00:05:57,203
왜냐하면 가중치와 Conv2 레이어를 가지고 있기 때문입니다.

79
00:05:57,203 --> 00:06:03,306
이제 5 x 5 x 16은 400이고

80
00:06:03,306 --> 00:06:10,895
이 Pool2를 400 x 1 차수의 벡터로 뚱뚱하게 만들어 봅시다.

81
00:06:10,895 --> 00:06:16,686
이것을 이런 뉴런 집합으로 뚱뚱하게 만든다고 생각해보십시오.

82
00:06:16,686 --> 00:06:22,373
우리가 할 일은 이 400 유닛을 취해서

83
00:06:22,373 --> 00:06:30,070
다음 레이어를 만들고, 120 유닛을 만드는 것입니다.

84
00:06:30,070 --> 00:06:33,243
그러면 이게 바로 첫 번째 완전히 연결된 레이어가 되는 것입니다.

85
00:06:33,243 --> 00:06:38,392
이걸 FC3라고 부르겠습니다.

86
00:06:38,392 --> 00:06:44,410
120 조각에 빽빽하게 연결된 400 조각이 있기 때문입니다.

87
00:06:46,245 --> 00:06:51,628
따라서 이 완전히 연결된 부분은

88
00:06:51,628 --> 00:06:56,660
과정 1, 2에서 보았던 하나의 신경망 레이어과 같습니다.

89
00:06:56,660 --> 00:07:01,710
이것은 곧 (120, 400)차수의 W[3]으로 불리는

90
00:07:01,710 --> 00:07:08,044
가중치 매트릭스를 가지고 있는 표준 신경망입니다.

91
00:07:08,044 --> 00:07:13,406
그리고 이것은 완전히 연결됩니다 왜냐하면 400 조각의 각각이

92
00:07:13,406 --> 00:07:18,354
120 조각에 개별적으로 연결되어 있기 때문입니다. 또한 바이어스 파라미터가 있는데,

93
00:07:18,354 --> 00:07:23,655
이는 120 차수, 120 아웃풋이 될 것입니다.

94
00:07:23,655 --> 00:07:28,715
마지막으로 120 유닛을 가져와 다른 레이어를 추가해봅시다.

95
00:07:28,715 --> 00:07:33,119
이번에는 더 작지만, 여기에 84 유닛이 있다고 가정 해 봅시다.

96
00:07:33,119 --> 00:07:36,883
저는 이걸 완전히 연결된 레이어 4라고 부르겠습니다.

97
00:07:36,883 --> 00:07:44,435
마지막으로, 여러분이 Softmax 유닛에 맞출 수 있는 84개의 진짜 숫자기 있습니다.

98
00:07:44,435 --> 00:07:48,215
손 글씨 디지털 인식을 하려 한다면,

99
00:07:48,215 --> 00:07:51,794
손을 인식할 수 있도록 이건 최대 0, 1, 2 등이 9가됩니다.

100
00:07:51,794 --> 00:07:56,680
그러면 이것은 10 개의 아웃풋 이 있는 Softmax가됩니다.

101
00:07:56,680 --> 00:08:00,969
따라서 이것은 컨볼루션 신경망가 어떻게 보일지에 대한

102
00:08:00,969 --> 00:08:05,482
전형적인 예를 들어 보겠습니다.

103
00:08:05,482 --> 00:08:09,367
이것이 많은 하이퍼 파라미터들처럼 보인다고 생각합니다.

104
00:08:09,367 --> 00:08:12,919
나중에 이러한 유형의 하이퍼 파라미터를 선택하는 방법에 대한

105
00:08:12,919 --> 00:08:15,882
몇 가지 구체적인 제안을 드릴 것입니다.

106
00:08:15,882 --> 00:08:20,388
한 가지 일반적인 가이드라인은 자기만의 하이퍼 파라미터를

107
00:08:20,388 --> 00:08:22,802
설계하려 하지 않고

108
00:08:22,802 --> 00:08:27,887
하이퍼 파라미터가 다른 사람들에게 어떻게 작용할지 서적에서 찾아보라는 것입니다.

109
00:08:27,887 --> 00:08:30,963
누군가에게 잘 작동하고 있는 설계도를 선택하면

110
00:08:30,963 --> 00:08:35,316
여러분이 적용하는 데에도 잘 될 가능성이 있을 테니까요.

111
00:08:35,316 --> 00:08:38,321
우리는 다음 주에 대해 더 많이 보게 될 것입니다.

112
00:08:38,321 --> 00:08:43,715
하지만 지금은 신경망에 깊이 알수록

113
00:08:43,715 --> 00:08:47,493
Nh와 Nw가 높이와 넓이를 줄여준다는 것을 지적하고 싶군요.

114
00:08:47,493 --> 00:08:52,432
앞서 말씀드림대로, 32 x 32는 20 x 20으로, 14 x 14 로

115
00:08:52,432 --> 00:08:53,934
또, 10 x 10 으로, 5 x 5로  줄어듭니다.

116
00:08:53,934 --> 00:08:57,870
보통 더 갈수록 높이와 너비가 줄어들지만

117
00:08:57,870 --> 00:09:00,852
반면에 채널 수는 증가합니다.

118
00:09:00,852 --> 00:09:07,277
3에서 6으로 16으로, 그리고 나서 완전히 연결된 레이어가 끝에 있습니다.

119
00:09:07,277 --> 00:09:13,135
신경망 에서 볼 수 있는 흔한 패턴은 conv 레이어 가 있고,

120
00:09:13,135 --> 00:09:17,426
하나 이상의 conv 레이어가 있고, 뒤따라 pooling 레이어가 있고,

121
00:09:17,426 --> 00:09:21,329
또 하나 이상의 conv 레이어가 있고, 뒤따라 pooling 레이어 있는 패턴입니다.

122
00:09:21,329 --> 00:09:24,731
그리고 마지막에는 완전히 연결된 몇 개의 레이어가 있고,

123
00:09:24,731 --> 00:09:26,756
Softmax가 그 뒤를 따라 나오는 겁니다.

124
00:09:26,756 --> 00:09:32,378
이것은 신경망에서 볼 수 있는 또 다른 흔한 패턴입니다.

125
00:09:32,378 --> 00:09:33,956
이 신경망의 몇 가지 세부적인 것을 살펴봅시다.

126
00:09:33,956 --> 00:09:37,968
activation 모양, activation크기, 그리고 이 네트워크에 있는 파라미터의 개수 같은 것들 말이죠.

127
00:09:37,968 --> 00:09:41,799
activation 모양, activation크기, 그리고 이 네트워크에 있는 파라미터의 개수 같은 것들 말이죠.

128
00:09:41,799 --> 00:09:44,181
인풋은 32 x 30 x 3 이었고,

129
00:09:44,181 --> 00:09:48,324
이 숫자들을 모두 곱하면 3,072가 됩니다.

130
00:09:48,324 --> 00:09:54,313
activation [a0]는 3,072 차원을 가지고 있는 거이죠

131
00:09:54,313 --> 00:09:58,005
이것은 진짜로 32 x 30 x 3 크기 입니다.

132
00:09:58,005 --> 00:10:02,562
제 생각엔 인풋 레이어에 파라미터는 없는 것 같습니다.

133
00:10:02,562 --> 00:10:05,672
이 다른 레이어들을 보면서,

134
00:10:05,672 --> 00:10:09,068
편하게 이 세부사항들을 살펴보십시오.

135
00:10:09,068 --> 00:10:10,975
이것들은 다른 레이어들의 activation모양과

136
00:10:10,975 --> 00:10:13,743
activation 사이즈입니다.

137
00:10:15,422 --> 00:10:16,957
몇 가지 확인해보자면,

138
00:10:16,957 --> 00:10:23,352
우선, max pooling 레이어는 그 어떤 파라미터도 가지지 않는다는 걸 기억하세요.

139
00:10:23,352 --> 00:10:28,202
둘째로, 이전 강의에서 말씀그린대로, conv 레이어는 상대적으로

140
00:10:28,202 --> 00:10:32,302
파라미터가 거의 없는 경향이 있다는 것을 기억해주십시오.

141
00:10:32,302 --> 00:10:36,414
사실 많은 파라미터가 신경망의

142
00:10:36,414 --> 00:10:39,426
완전히 집약된 레이어안에 있는 경향이 있습니다.

143
00:10:39,426 --> 00:10:44,584
또한 activation 사이즈는

144
00:10:44,584 --> 00:10:50,289
신경망 안에 깊게 진행될 수록 점차 줄어들 것입니다.

145
00:10:50,289 --> 00:10:55,198
너무 빨리 떨어지면, 수행 능력이 그다지 좋지 않습니다.

146
00:10:55,198 --> 00:11:00,349
그래서 처음에는 6,000으로 시작해서 1,600 정도로 떨어지고,

147
00:11:00,349 --> 00:11:06,405
천천히 84로 떨어지다가 Softmax 아웃풋까지 마침내 오게 되는 것입니다.

148
00:11:06,405 --> 00:11:10,683
여러분들은 많은 것들이 이것들과 유사한 패턴이나

149
00:11:10,683 --> 00:11:13,293
속성을 가질 것이라는 것을 알게 됩니다

150
00:11:13,293 --> 00:11:16,455
지금까지 신경망의, 즉 컨볼루션 신경망, 기본적인 빌딩블록,

151
00:11:16,455 --> 00:11:20,068
conv 레이어 와 pooling 레이어

152
00:11:20,068 --> 00:11:21,601
그리고 완전히 연결된 레이어들을 살펴보았습니다.

153
00:11:21,601 --> 00:11:25,693
많은 컴퓨터 비전연구가 효과적인 신경망을 구축하기 위해

154
00:11:25,693 --> 00:11:29,078
이러한 기본 빌딩 블록을 결합하는 방법을 알아내는 데에 진전을 보이고 있습니다.

155
00:11:29,078 --> 00:11:33,379
그리고 이 두 가지를 합치려면 실제로 통찰력이 필요합니다.

156
00:11:33,379 --> 00:11:35,213
이러한 것들을 어떻게 합쳐서 사용할 수 있을지에

157
00:11:35,213 --> 00:11:39,323
직감을 얻는 가장 좋은 방법 중 하나는

158
00:11:39,323 --> 00:11:41,804
다른 사람들이 했던 방법의 구체적인 예시들을 많이 보는 것입니다.

159
00:11:41,804 --> 00:11:46,268
그래서 제가 다음 주에 하고 싶은 것은 사람들이 어떻게 이것들을 성공적으로 묶어서

160
00:11:46,268 --> 00:11:50,183
효과적으로 신경망을 구축했는지

161
00:11:50,183 --> 00:11:53,637
처음에 보셨던 이 예를 넘어선 몇 가지 구체적인 예를 보여드리는 것입니다.

162
00:11:53,637 --> 00:11:58,532
다음 주 강의를 통해 이러한 것들이 어떻게 구축되는지에 대한

163
00:11:58,532 --> 00:12:00,098
여러분의 직감을 가지게 되길 바랍니다

164
00:12:00,098 --> 00:12:05,068
그 설계도의 구체적인 예들을 보게 되면

165
00:12:05,068 --> 00:12:09,120
아마도 여러분은 누군가 다른 사람이 발전시킨 것과 정확히 똑같이 혹은 여러분만의 응용을 하게 될 것입니다.

166
00:12:09,120 --> 00:12:10,971
자 그럼 다음 주에 그걸 하도록 하죠.

167
00:12:10,971 --> 00:12:15,499
마무리 짓기 전에 마지막으로

168
00:12:15,499 --> 00:12:19,840
다음 강의에서는 컨볼루션을 왜 사용해야 하는지에 대해 이야기하도록 하겠습니다.

169
00:12:19,840 --> 00:12:20,869
컨볼루션을 사용할 때의 혜택과 장점은 무엇이고

170
00:12:20,869 --> 00:12:25,133
그것들을 어떻게 모두 합쳐 사용할 수 있을지에 대해서 알아봅시다.

171
00:12:25,133 --> 00:12:29,021
방금 보신 신경망을 몇몇 과제를 위해

172
00:12:29,021 --> 00:12:32,735
이미지 인식을 수행하기 위해 훈련 세트대로 그것을 훈련할 수 있을지도 알아보겠습니다.

173
00:12:32,735 --> 00:12:35,700
그럼 이번 주 마지막 강의로 가 보시죠.