1
00:00:03,945 --> 00:00:07,666
您現在已經準備好來建置
一個一層的卷積神經網路

2
00:00:07,666 --> 00:00:09,223
讓我們來看看這個例子。

3
00:00:12,432 --> 00:00:17,146
您已經在前面的影片中看到如何將三維的資料值

4
00:00:17,146 --> 00:00:20,670
用兩個不同的過濾器進行卷積處理

5
00:00:21,810 --> 00:00:25,861
在這個例子中，為了要得到一個新的四乘四二維的輸出資料值

6
00:00:30,891 --> 00:00:35,548
我們就先用第一個過濾器

7
00:00:35,548 --> 00:00:40,769
卷積運算後獲得一個四乘四的輸出值

8
00:00:40,769 --> 00:00:49,108
然後用第二個過濾器卷積運算，
得到另外一個四乘以四的輸出值

9
00:00:49,108 --> 00:00:55,574
要完成這個卷積神經網路層的最後一個步驟

10
00:00:55,574 --> 00:01:00,566
就是要在每個運算中加上一個偏差值

11
00:01:00,566 --> 00:01:03,980
就是加上一個實數

12
00:01:03,980 --> 00:01:08,840
因為python程式語言的運算廣播原則，這個實數就會跟所有

13
00:01:08,840 --> 00:01:11,750
16 個元素的每一項相加

14
00:01:11,750 --> 00:01:16,805
然後我們加上一個非線性函數運算 在這裡我們用了ReLU函數

15
00:01:16,805 --> 00:01:23,430
會得到一個 4乘4 輸出值

16
00:01:23,430 --> 00:01:27,295
加入了偏差值跟非線性函數運算之後

17
00:01:27,295 --> 00:01:31,894
然後對於底下這一項也一樣，
您加入一個不同的偏差值

18
00:01:31,894 --> 00:01:33,154
同樣地也是一個實數

19
00:01:33,154 --> 00:01:36,473
所以您將一個實數加到所有這 16 項上

20
00:01:36,473 --> 00:01:40,934
然後使用一個非線性函數，
假設也是 ReLU 函數

21
00:01:40,934 --> 00:01:47,369
這會給您另外一個不同的 4乘4 輸出值

22
00:01:47,369 --> 00:01:52,461
就如同前面的運算步驟，如果我們把這些輸出值

23
00:01:52,461 --> 00:01:59,698
像這樣疊起來，
我們會得到一個 4乘4乘2 輸出值

24
00:01:59,698 --> 00:02:06,326
那這個計算您從一個 6乘6乘3 
到一個 4乘4乘4 (應該是 4乘4乘2)

25
00:02:06,326 --> 00:02:11,303
這就是一個一層的卷積神經網路

26
00:02:11,303 --> 00:02:15,634
所以對應到一層的
標準神經網路的正向傳播

27
00:02:15,634 --> 00:02:18,832
在非卷積神經網路中

28
00:02:18,832 --> 00:02:23,155
記得在傳播前的一個步驟是像這樣，對吧？

29
00:02:23,155 --> 00:02:28,461
z[1] = w[1] 乘 a[0],
a[0] 等於 x

30
00:02:28,461 --> 00:02:31,265
然後加 b[1]

31
00:02:31,265 --> 00:02:38,121
您應用一個非線性函數到這個 a[1], 所以是  g(z[1])

32
00:02:38,121 --> 00:02:43,400
在這裡的輸入，類似於這個是 a[0], 其實是 x

33
00:02:44,770 --> 00:02:48,076
而這些過濾器

34
00:02:48,076 --> 00:02:52,488
扮演的角色類似 w1

35
00:02:52,488 --> 00:02:56,377
您記得在做卷積運算時，您拿這

36
00:02:56,377 --> 00:03:01,149
27 個數字，或者實際上是  27乘2， 因為您有兩個過濾器

37
00:03:01,149 --> 00:03:03,900
您拿所有這些數字跟這個相乘

38
00:03:03,900 --> 00:03:09,631
所以您其實是在計算一個線性函數
來得到這個 4乘4 矩陣

39
00:03:09,631 --> 00:03:15,048
所以這個 4乘4 矩陣，卷積運算的輸出

40
00:03:15,048 --> 00:03:19,245
扮演的角色類似於 w[1] 乘 a[0]

41
00:03:19,245 --> 00:03:25,340
輸出是這個 4乘4 
跟這個  4乘4

42
00:03:25,340 --> 00:03:29,599
然後另外一件事
是您加入這個偏差

43
00:03:29,599 --> 00:03:35,138
所以在應用 ReLU 之前這些東西

44
00:03:35,138 --> 00:03:38,939
扮演的角色類似 z

45
00:03:38,939 --> 00:03:43,848
最後透過應用非線性函數，我猜像這樣

46
00:03:43,848 --> 00:03:49,740
所以，這個輸出扮演的角色

47
00:03:49,740 --> 00:03:53,390
其實變成是您的下一層的啟動值

48
00:03:53,390 --> 00:03:58,794
所以這是您如何從 a[0] 到 a[1],
首先是線性函數運算

49
00:03:58,794 --> 00:04:02,192
然後卷積讓這些相乘

50
00:04:02,192 --> 00:04:05,507
所以卷積實際上是應用了線性運算

51
00:04:05,507 --> 00:04:08,437
然後加入了偏差，
再應用了 ReLU 運算

52
00:04:08,437 --> 00:04:14,107
您從一個 6乘6乘3 維度的  a[0]

53
00:04:14,107 --> 00:04:18,210
經過了一層的神經網路

54
00:04:18,210 --> 00:04:22,693
到了一個 4乘4乘2 維度的  a[1]

55
00:04:22,693 --> 00:04:27,144
所以 6乘6乘3 
變成 4乘4乘2

56
00:04:27,144 --> 00:04:30,860
這就是一層的卷積網路

57
00:04:33,697 --> 00:04:40,504
在這個例子中我們用了兩個過濾器，
可以當成是兩個特徵

58
00:04:40,504 --> 00:04:45,270
這也是為什麼我們
最終我們的輸出會是 4乘4乘2

59
00:04:45,270 --> 00:04:49,248
但假設我們如果用 10 個過濾器，
而不是 2 個過濾器

60
00:04:49,248 --> 00:04:54,396
那我們最終會得到一個 
4乘4乘10 維度的輸出容積

61
00:04:54,396 --> 00:05:00,334
因為我們會將這 10 個而不是 2 個，疊起來

62
00:05:00,334 --> 00:05:05,598
變成一個 4乘4乘10 輸出容積，
這將會是我們的 a[1]

63
00:05:05,598 --> 00:05:09,630
為了確保您理解這些，
我們再來一個例子

64
00:05:09,630 --> 00:05:14,655
假設您有 10 個過濾器，
不只是兩個過濾器，是 3乘3乘3 

65
00:05:14,655 --> 00:05:19,600
一層的神經網路，這一層有多少個參數呢？

66
00:05:21,000 --> 00:05:22,984
讓我們看看

67
00:05:22,984 --> 00:05:29,439
每一個過濾器都是 3乘3乘3 容積，所以是 3x3x3

68
00:05:29,439 --> 00:05:35,318
所以每個過濾器有 27 個參數，對吧？

69
00:05:35,318 --> 00:05:39,800
總共有 27 個數字來運算，加上偏差

70
00:05:42,210 --> 00:05:46,818
也就是 b 參數，總共給您 28 個參數

71
00:05:50,125 --> 00:05:54,456
然後如果您想像在前面的投影片中，
我們畫了兩個過濾器

72
00:05:54,456 --> 00:05:58,329
但現在如果您想像您實際上有 10 個過濾器

73
00:05:58,329 --> 00:06:01,747
1,2, ... 10 個這樣的

74
00:06:01,747 --> 00:06:06,942
總共您會有 28乘10

75
00:06:06,942 --> 00:06:10,753
所以會是 280 個參數

76
00:06:10,753 --> 00:06:16,316
請注意這樣有一個好處，
不管輸入的影像多大

77
00:06:16,316 --> 00:06:22,051
輸入影像可以是 1000乘1000, 
或者 5000乘5000

78
00:06:22,051 --> 00:06:26,973
但是參數的數目還是
維持固定的 280 個

79
00:06:26,973 --> 00:06:31,453
您可以使用這 10 個過濾器來
偵測特徵，垂直邊緣

80
00:06:31,453 --> 00:06:35,485
水平邊緣，或者其他特徵
在任何地方，即使是

81
00:06:35,485 --> 00:06:39,240
很大很大的影像，
只需要小數目的參數

82
00:06:40,920 --> 00:06:44,410
這真的是卷積神經網路的一個特性

83
00:06:44,410 --> 00:06:48,000
比較不容易過適，然後您可以

84
00:06:48,000 --> 00:06:51,450
一旦您成功學習了 10 個特徵偵測

85
00:06:51,450 --> 00:06:54,770
您可以將這個應用在大的影像上

86
00:06:54,770 --> 00:06:58,300
而參數的數目
還是固定且相對少的

87
00:06:58,300 --> 00:07:00,494
像這個例子是 280

88
00:07:00,494 --> 00:07:05,130
好的，總結一下，我們使用的記號

89
00:07:05,130 --> 00:07:09,766
將用來描述一層的

90
00:07:09,766 --> 00:07:11,980
卷積神經網路

91
00:07:11,980 --> 00:07:14,302
所以 L 層是一個卷積層

92
00:07:14,302 --> 00:07:18,555
我會用 f 上標 [l] 來記為過濾器大小

93
00:07:18,555 --> 00:07:23,219
所以前面我們看過的過濾器是 f乘f

94
00:07:23,219 --> 00:07:28,163
現在這個上標方括號 L 記為

95
00:07:28,163 --> 00:07:31,074
這是一個過濾器
大小為 f乘f 在 L 層

96
00:07:31,074 --> 00:07:35,767
和往常一樣, 上標方括弧 L 是我們使用的符號

97
00:07:35,767 --> 00:07:37,611
指的是第 Ｌ層

98
00:07:39,812 --> 00:07:42,809
將使用 p[l] 記為填充量

99
00:07:42,809 --> 00:07:47,363
再一次，填充的大小
也可以說您要的是

100
00:07:47,363 --> 00:07:50,135
有效卷積，也就是說沒有填充

101
00:07:50,135 --> 00:07:53,240
相同卷積，也就是您選擇的填充

102
00:07:53,240 --> 00:07:57,910
讓輸出的大小跟輸入的高度，寬度一樣

103
00:07:59,000 --> 00:08:01,590
然後您使用 s[l] 來記跨步

104
00:08:03,250 --> 00:08:09,450
現在這一層的輸入將會是一些維度

105
00:08:09,450 --> 00:08:18,590
將會是一個 n乘n乘 在前一層的通道數目

106
00:08:18,590 --> 00:08:21,162
現在我將稍微修改一下這個記號

107
00:08:21,162 --> 00:08:25,385
我將使用上標 l-1

108
00:08:25,385 --> 00:08:29,902
因為那是從

109
00:08:29,902 --> 00:08:35,594
前一層來的啟動值，L-1 乘 nc L-1

110
00:08:35,594 --> 00:08:40,966
我們目前使用的例子，
我只使用高度跟寬度相同的影像

111
00:08:40,966 --> 00:08:43,990
假如高度跟寬度不同

112
00:08:43,990 --> 00:08:48,528
我將會使用上標 h 跟上標 w 來記

113
00:08:48,528 --> 00:08:51,949
來自於前面一層高度跟寬度

114
00:08:51,949 --> 00:08:56,418
所以在第 Ｌ層，容積的大小會是 nh

115
00:08:56,418 --> 00:09:01,134
乘nw乘nc 上標為方括號 L

116
00:09:01,134 --> 00:09:05,597
只是因為在 L 層，
這一層的輸入會從

117
00:09:05,597 --> 00:09:09,451
前面一層來的，
這是為什麼這裡是 L-1 

118
00:09:09,451 --> 00:09:16,730
然後這一層神經網路的輸出值

119
00:09:16,730 --> 00:09:23,065
也就是 nh[L]乘nw[L]乘nc[L]

120
00:09:23,065 --> 00:09:28,495
這就是輸出的大小

121
00:09:28,495 --> 00:09:34,941
所以當我們顯示輸出的容積大小

122
00:09:34,941 --> 00:09:40,657
或者至少高度跟寬度是這個公式

123
00:09:40,657 --> 00:09:47,971
n+2p-f 除 s +1 然後用捨去法 

124
00:09:47,971 --> 00:09:55,605
在這個新的記號上，我們的輸出值是在 L 層

125
00:09:55,605 --> 00:10:00,891
將是這個從前面一層的維度

126
00:10:00,891 --> 00:10:05,471
加上我們用在Ｌ層的填充

127
00:10:05,471 --> 00:10:11,760
減去Ｌ層的過濾器大小等等

128
00:10:11,760 --> 00:10:16,580
基本上這個是高度，對吧？

129
00:10:16,580 --> 00:10:21,510
所以輸出容積的高度是這樣，
您可以從

130
00:10:21,510 --> 00:10:24,680
右邊的公式計算得到，
寬度也是一樣

131
00:10:24,680 --> 00:10:26,670
所以您可以劃掉 h

132
00:10:26,670 --> 00:10:30,780
然後用 w , 同樣的公式
對於不管是高度或者

133
00:10:30,780 --> 00:10:34,775
寬度，代入計算可以得到
輸出值的高度或者寬度

134
00:10:36,570 --> 00:10:44,024
所以這是 nh[L-1] 跟 nh[L] 的關係
以及nw[L-1] 跟 nw[L] 的關係

135
00:10:44,024 --> 00:10:48,105
那麼，通道的數目呢？這些數字從何而來？

136
00:10:48,105 --> 00:10:52,784
我們來看看，如果這個輸出值有這個深度

137
00:10:52,784 --> 00:10:57,662
我們從前面的例子得知，這是等於

138
00:10:57,662 --> 00:11:02,167
在那一層過濾器的數目，對吧？

139
00:11:02,167 --> 00:11:07,017
我們如果有兩個過濾器，
輸出值會是 4乘4乘2, 是 2 維

140
00:11:07,017 --> 00:11:11,097
而如果您有 10 個過濾器，
您的輸出容積會是 4乘4乘10

141
00:11:11,097 --> 00:11:15,744
所以在輸出值的通道數目

142
00:11:15,744 --> 00:11:23,098
也就是我們在神經網路這一層
使用的過濾器數目

143
00:11:23,098 --> 00:11:26,849
接下來，這個過濾器的大小為何？

144
00:11:26,849 --> 00:11:33,057
每一個過濾器都是 f[L]乘f[L]乘 一個數字

145
00:11:33,057 --> 00:11:34,704
最後一個數字是多少？

146
00:11:34,704 --> 00:11:39,465
我們看到您需要卷積一個 6乘6乘3 影像

147
00:11:39,465 --> 00:11:41,580
用一個 3乘3乘3 過濾器

148
00:11:43,070 --> 00:11:48,150
所以過濾器的通道數目，必須等於

149
00:11:48,150 --> 00:11:54,360
您輸入的通道數目，
所以這個數字要等於那個數字，對吧？

150
00:11:54,360 --> 00:12:02,627
這也就是每個過濾器將會是 
f[L]乘f[L]乘nc[L-1]

151
00:12:02,627 --> 00:12:07,875
而這一層的輸出通常會
應用上偏差跟非線性

152
00:12:07,875 --> 00:12:11,745
這將會是這一層的啟動值 a[L]

153
00:12:11,745 --> 00:12:15,115
我們已經看過它的維度了，對吧？

154
00:12:15,115 --> 00:12:20,451
a[L] 會是一個 3D 的容積

155
00:12:20,451 --> 00:12:25,556
也就是  nh[L]乘nw[L]乘nc[L]

156
00:12:25,556 --> 00:12:31,082
而當您使用向量化建置，或者批次梯度下降

157
00:12:31,082 --> 00:12:36,891
或者小批次梯度下降，然後您真的輸出 a[L]

158
00:12:36,891 --> 00:12:41,387
也會是一組 m 啟動值，
如果您有 m 個例子

159
00:12:41,387 --> 00:12:48,275
所以會是  m乘nh[L]乘nw[L]乘nc[L], 對吧？

160
00:12:48,275 --> 00:12:51,375
假設您用的是批次梯度下降

161
00:12:51,375 --> 00:12:55,999
在程式練習中，這會是變數的次序

162
00:12:55,999 --> 00:12:59,962
我們會先放索引跟訓練例子

163
00:12:59,962 --> 00:13:02,384
然後這三個變數

164
00:13:02,384 --> 00:13:07,618
接下來權重或者參數呢？
或者像是  w 參數呢？

165
00:13:07,618 --> 00:13:10,264
我們在過濾器的維度時已經見過

166
00:13:10,264 --> 00:13:16,258
過濾器將會是 f[L]乘f[L]乘nc[L-1]

167
00:13:16,258 --> 00:13:20,230
但那是一個過濾器的維度

168
00:13:20,230 --> 00:13:22,247
我們有多少過濾器？

169
00:13:22,247 --> 00:13:25,097
這是過濾器的總數，

170
00:13:25,097 --> 00:13:30,029
所以權重其實是將所有的過濾器放在一起，
所以維度會是

171
00:13:30,029 --> 00:13:33,513
這個乘上過濾器的數目，對吧？

172
00:13:33,513 --> 00:13:38,625
因為這個，最後這個數是過濾器的數目

173
00:13:38,625 --> 00:13:43,750
在 L 層

174
00:13:45,680 --> 00:13:48,710
最後，您有這個偏差參數

175
00:13:48,710 --> 00:13:54,100
您有一個偏差參數，
一個實數對每一個過濾器

176
00:13:54,100 --> 00:13:57,970
所以您將會有這個偏差，會有這些變數

177
00:13:57,970 --> 00:14:00,810
這只是一個這個維度的向量

178
00:14:00,810 --> 00:14:05,052
雖然後面我們在程式中會

179
00:14:05,052 --> 00:14:09,813
比較方便的用 
1乘1乘1乘nc[L] 

180
00:14:09,813 --> 00:14:14,790
4 維度的矩陣，或者說 4 維度的張量

181
00:14:16,430 --> 00:14:19,408
我知道這裡有很多的符號

182
00:14:19,408 --> 00:14:23,303
這個是我大部分使用的慣例

183
00:14:23,303 --> 00:14:27,498
我只是想提醒一下，
假設您在線上搜尋或者看開源程式

184
00:14:27,498 --> 00:14:32,311
並沒有完全通用的標準公約有關於

185
00:14:32,311 --> 00:14:34,260
高度，寬度跟通道的次序

186
00:14:34,260 --> 00:14:39,142
所以如果您在 Github 看開源程式，
或者開源程式的建置

187
00:14:39,142 --> 00:14:43,873
您會發現有些作者用另外一種順序，您先放

188
00:14:43,873 --> 00:14:48,631
通道，有時候您會看到這種變數的次序

189
00:14:48,631 --> 00:14:52,154
事實上一些通用的框架，實際上多個框架

190
00:14:52,154 --> 00:14:54,155
實際在放變數或者說參數時

191
00:14:54,155 --> 00:14:57,652
您想將通道的數目先放，或者

192
00:14:57,652 --> 00:15:02,000
將通道的數目放在最後
當將索引放進這個容積時

193
00:15:02,000 --> 00:15:08,137
我想兩種公約都可以，只要您用的時候一致

194
00:15:08,137 --> 00:15:13,049
很不幸的，或許這個符號

195
00:15:13,049 --> 00:15:17,772
在深度學習文獻上並不一致

196
00:15:17,772 --> 00:15:21,752
但我會用這個公約在我們的影片中

197
00:15:24,681 --> 00:15:30,769
也就是高度跟寬度，
然後通道的數目放最後

198
00:15:30,769 --> 00:15:34,327
我知道突然有很多的符號您要用，

199
00:15:34,327 --> 00:15:38,027
您或許會想，哇，
這麼長的符號，我要怎樣記住這些

200
00:15:38,027 --> 00:15:41,994
不用擔心，您不需要去記這些符號

201
00:15:41,994 --> 00:15:46,036
透過這個禮拜的練習，屆時您會變得比較熟悉

202
00:15:46,036 --> 00:15:49,116
但這個影片的重點是

203
00:15:49,116 --> 00:15:52,694
一個一層的卷積神經網路如何作用

204
00:15:52,694 --> 00:15:57,006
跟相關的計算從拿一層的啟動值

205
00:15:57,006 --> 00:16:00,052
對應到下一層的啟動值

206
00:16:00,052 --> 00:16:04,063
接下來，您理解了一層的卷積神經網路如何作用

207
00:16:04,063 --> 00:16:07,740
我們來將一堆這樣的東西疊起來變成一個

208
00:16:07,740 --> 00:16:09,040
更深度的卷積神經網路

209
00:16:09,040 --> 00:16:10,200
我們下一段影片見