最後のビデオでは 単一層の構成要素を見た ConvNetにおける単一の畳み込み層のだ それじゃ ディープ畳み込みニューラルネットワークの具体例を見てみよう これは 最後のビデオで導入した表記法の 練習にもなる ここに画像があるとしよう そして 画像の分類 もしくは 画像の識別をしたいとする 入力 画像ｘを得たら それが猫かどうか 0 か 1 か 決めたい これは 分類問題だ このタスクのため ConvNetの例を構築しよう この例のために かなり小さな画像を使うことにする この画像は 39 x 39 x 3 だ この決定により いくつかの数を 少しだけ良く働かせる 層 0 の nH は nW と同じになり 高さと幅は 39 に等しい 層 0 のチャンネル数は ３に等しい 最初の層では 3 x 3 フィルターのセットを使おう 特徴を検出するためだ f = 3 実際は f[1] = 3 なぜなら 3 x 3 フィルターだから そして ストライドは１で パディング無しとする 同じ畳み込みを使う
そうだな 10 フィルターとしよう そうしたら このニューラルネットワークの 次の層の活性化は 37 x 37 x 10 となる この 10 は 10フィルターを使ったことから来ている そして 37 は この式から来た (n + 2p - f) / s + 1 よし 皆できたと思うが (39 + 0 - 3) / 1 + 1 = 37 だ だから 出力が 37 x 37 になった
これは Valid畳み込みだ そして これが出力サイズだ 我々の表記法では nH[1] = nW[1] = 37 で nC[1] = 10 nC[1] は 最初の層のフィルター数に等しい これが 最初の層の活性化の次元になる それじゃ 別の畳み込み層を入れよう 今度は 5 x 5 フィルターとする 我々の表記法では 次のニューラルネットワーク層の f[2] は 5 だ 今度は ストライドを２としよう そして パディング無し そして 20個の フィルター そうしたら この出力は 別のボリュームとなり 今度のそれは 17 x 17 x 20 となる ストライドを２にしたから 次元数が より速く縮んだ 37 x 37 は 因数の２よりも 少しだけ多く 17 x 17 まで小さくなった そして 20 個のフィルターがあるので チャンネル数は 20 だ この活性化 a[2] が そのような次元になる それで nH[2] = nW[2] = 17 で nC[2] = 20 よろしい 最初の畳み込み層を適用しよう 再び 5 x 5 フィルターを使おう そして また ストライドは２だ そうすれば 数式は飛ばすけど 最終的に 7 x 7 になる そして 40個のフィルターを使うとしよう パディング無しの40フィルターだ 最後に 7 x 7 x 40 を得る ここで やったことは 39 x 39 x 3 の入力画像を得て 7 x 7 x 40 の特徴量を持つ画像を算出した 最後に 行われ 得たものは もし 7 x 7 x 40 を得たのなら それは 7 x 7 x 40 で 1,960 だ そして このボリュームを 平坦化する つまり 展開すると 1,960 ユニットとなる 単に平たくしてベクトルにする それから これを ロジスティック回帰や ソフトマックスに喰わせる 何を認識しようとしているのかによって 猫かそうでないかによって 異なる種類の物体の中から何を認識しようとしているかによって これが ニューラルネットワークの 最終出力 最終的な予測結果を 与える 正確に言うと ここの最後のステップは この数全てを 1,960個の数を 展開して とても長い1つのベクトルにする そうしたら 1つの長いベクトルを得たので ソフトマックスや
ロジスティック回帰に 喰わせることができるようになり 最終出力に 予測を生成できる さて これが ConvNet の典型例だ 畳み込みニューラルネットワークの設計では 多くが こんな風にして ハイパーパラメータを選び 合計数をどうするか決めている ストライドはどうだろう？ パディングは？ フィルターの数は？ 今週の終わりと次週にも どのように この選択を行うのかについて 提案とガイドラインを与えるよ だけど 今は ここから得られることは ニューラルネットワークを深くしていけば
典型的には 39 x 39 よりも大きな画像から始まって 高さと幅は 暫くは 同じ値に留まるけど 徐々に ニューラルネットワークが深くなるにつれ 小さくなっていくということだ それは 39 から 37 17 14 へと変わった ごめん 39 から 37 17「7」へだ 一方 チャンネル数は 徐々に増加するだろう それは 3 から 10 20 40 になった これは 他の多くの畳み込みニューラルネットワークでも見られる 一般的な傾向だ 後のビデオでは どのように これらのパラメータを設計するのかについて
もっと多くのガイドラインを得られる でも あなたは 今 畳み込みニューラルネットワークの最初の例を見た 略して ConvNet だ おめでとう また 明らにすることがある 典型的なConvNetでは ３種類の層がある 1つは 畳み込み層だ しばしば Conv 層 と言う それは 先ほどのネットワークで使っていた そして あなたが まだ 見ていない
２つの 別の一般的な種類の層があることも 明かそう でも それは この先の２~３のビデオでね 1つは プーリング層だ 私は しばしば これをプールと呼ぶ そして 最後は 全結合層 だ  FCと呼ぶ とても良いニューラルネットワークをデザインするのは 畳み込み層だけでも可能だが
多くのニューラルネットワーク構造には プーリング層と 少しの全結合層がある 幸運にも プーリング層と 全結合層は 畳み込み層よりも 定義するのが少し簡単だ 次の２つのビデオで サッと それをやり 畳み込みニューラルネットワークで使われる 
一般的な種類の層への感覚を得よう そして 今見たものより もっと強力なネットワークを 組み立てよう もう一度 おめでとう
最初の 完全な畳み込みニューラルネットワークを理解した 今週の後の方で これらのネットワークを訓練する方法について話すけど まずは プーリングと全結合層について 手短に話そう それから これらを訓練する
誤差逆伝播を使ってね これは もう知っているよね でも 次のビデオでは プーリング層の実装方法を サッと調べよう あなたの ConvNet のために