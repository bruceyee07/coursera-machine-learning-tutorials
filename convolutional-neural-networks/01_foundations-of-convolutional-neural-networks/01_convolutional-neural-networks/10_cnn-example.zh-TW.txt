您現在已經幾乎知道了所有建立 完整的卷積神經網路的基石 我們來看一個例子 假設您輸入一個 32乘32乘3 的影像 所以是一個 RGB 影像，或許您試著做手寫數字辨識 所以您有一個數字像是 7 在 32乘32 RGB，您試著 辨識 10 個數字中從 0 到 9 
這個是哪一個 我們來建立一個神經網路來做這件事 而在這個投影片中
我所使用的靈感 實際上是來自於典型的神經網路稱為 LeNet-5 是由 Yann LeCun 許多年前建立的 這裡我要顯示的並不完全是 LeNet-5 但靈感來自於它，
很多的參數選擇的靈感來自於它 所以拿這個  32乘32乘3 輸入，假設第一層 使用了 5乘5 過濾器且跨步為 1, 不使用填充 所以這一層的輸出 如果您使用了 6 個過濾器，輸出會是 28乘28乘3 我們稱這一層為 conv 1 所以您用了 6 個過濾器，增加了偏差，使用了非線性 或許是 ReLU 非線性，
而這就是 conv 1 輸出 接下來，我們來應用池層， 我將使用最大池層，假設使用 f=2, s=2 當我沒寫填充時，填充預設為 0 接下來應用池層，我將使用 最大池層，使用  2乘2 過濾器，跨步為 2 這個應該會降低高度跟 寬度縮為一半 所以 28乘28 變為 14乘14 通道的數目維持一樣是 14乘14乘6 我們稱之為 Pool 1 輸出 實際上在 ConvNet 文獻中，有兩種 約定，當人們說一層時，會有點不一樣 一種約定是稱這個為一層 所以在神經網路中這是一層，而另一種約定 會稱這個 conv 層為一層，這個 pool 層為一層 當人們報告神經網路有幾層時，通常人們 只紀錄那些有權重，有參數的那些層 而因為池層沒有權重，沒有參數 只有一些超參數，我使用的約定會是 Conv1 跟 Pool1 共享 我會將他們視為第一層，雖然有時候您會見到 或許人們讀線上文章或者讀研究論文，您會聽到  conv 層跟 pooling 層, 似乎是他們有兩層 但或許這會有些符號專有名詞的不一致性 但是當我計算層數時，
我將只計算那些有權重的層 所以會將它們是為一層  而 Conv1 跟 Pool1 的名字裡的 1 指的也就是我將它們
視為神經網路的第一層 而 Pool1 被分到第一層
因為它沒有自己的權重 接著，對於這個 14乘14乘6 容積，我們應用另一個 卷積層，我們使用一個過濾器大小為 5乘5 我們使用跨步 1，這次使用 10 個過濾器 最後您會得到一個 10乘10乘10 容積，我稱之為 Conv2 然後在這個網路，我們做一個 最大池化，再一次使用 f=2, s=2 所以您大概可以預測這個輸出對於 f=2, s=2, 這會減低高度跟 寬度為一半，所以您留下來會是 5乘5乘10 所以我將其稱為 Pool2 而在我們的約定中，這個是神經網路的第二層 現在讓我們在用另一個卷積層到這上面 我將使用一個 5乘5 過濾器，所以 f=5, 我們用跨步 1, 我不寫填充，意思是不用填充 而這會給您 Conv2 輸出，而那是您的 16 個過濾器 所以這會是 10乘10乘16 維度的輸出 所以我們看這個，這是 Conv2 層 然後讓我們應用最大池化到這個
用 f=2, s=2 您大概可以猜測這個的輸出 我們在 10乘10乘16 上用 f=2, s=2, 最大池化 這會將高度跟寬度減半 您可以猜測這樣的結果，對吧？ 使用 f=2, s=2 最大池化 這應該將高度跟寬度減半，所以您最終會有 一個 5乘5乘16 容積，跟之前一樣的通道 我們將其稱為 Pool2 而在我們的約定中，這會是第二層，因為 這個有一組的權重在您的 Conv2 層 現在 5乘5乘16, 5乘5乘16 等於 400 所以現在攤平我們的 Pool2 成為一個 400乘1 維度向量 因此把這個想成是攤平這些變成一組神經元，像這樣 我們要拿這些 400  個單元 來建立下一層成為 120 個單元 所以這其實就是我們第一個全連結層 我將它稱為  FC3, 因為我們 用  400 個單元密集連結了 120 個單元 所以這個全連結單元，這個全連結層就像 是一個您在第一，二課程中的
所學的單一的神經網路層 這只是一個標準的神經網路，您用 權重矩陣，被稱為 W3 維度是  120乘400 而這個稱為全連結因為
400 個單元中每一個單元都連結到 這 120 個單元中每一單元，您也會有一個偏差參數 而那個的維度會是 120，這是 1`20 輸出 最後，我們拿這個 120個單元，加入另一層 這一次小一點，假設我們有 84個單元 我將它稱為全連結第四層 最後我們現在將這 84 個實數您可以代入 "softmax" 單元 而如果您試著用來辨別手寫數字 用來辨識從 0, 1, 2, 一直到  9 那這個會是一個 softmax 有 10 個輸出 所以這是一個典型的 卷積神經網路的例子的樣子 我知道這似乎有很多的超參數 我們後面會給您一些更具體的建議 對於如何選擇這些超參數 或許一個通用的原則
是不要試著發明您自己 的超參數設定， 而是看看文獻上哪些超參數在別的地方作用得很好 而直接選擇一個架構，別人已經作用得很好 有很大的機會這些也會在您的應用上可行 我們在下一週會見到更多有關於這類的事情 但現在，我要指出當您進入更深度的神經網路 通常 nh 跟 nw ，高度跟寬度會降低 早先已經提過，就像這個從  32乘32, 到 20乘20, 到 14乘14 到 10乘10, 到 5乘5 所以當您用越深的網路，通常高度跟寬度會降低 然而通道的數目卻會增加 它從 3 到 6  到 16 然後最後是您的全連結層 而另一個很常見的模式是
您見到在神經網路中有卷積層 或許一個或多個卷積層接著一個池層 然後一個或多個卷積層接著一個池層 最後您會有一些全連結層 然後接著一個 softmax 層 這是另一個相當常見的模式
您在神經網路中常會見到 所以讓我們來 看一下這個神經網路的細節，有關於啟動值的形狀 啟動值的大小，跟這個網路的參數數目 所以輸入是  32乘32乘3 如果您乘這些數字，您應當會得到 3,072 所以這個啟動值，a0 的維度為 3072 實際上為 32乘32乘3  而在輸入層並沒有參數 而當您看一個不同的層 您可以自己算算看 這些是啟動值的形狀 跟啟動值的大小在不同的層上 只是想要指出一些事情 首先，注意最大池層並沒有任何參數 第二，注意  Conv 層的參數往往 比之前影片中討論的參數要少 而實際上，很多的參數，往往是在 神經網路的全連接層上 然後您注意到，啟動值的大小 或許逐漸降低當您
進入更深層的神經網路時 如果它下降太快，通常對於性能不太有幫助 所以開始於 6,000 ，經過 1,600 慢慢降到 84, 直到最後您使用了 softmax 輸出 您會在很多的卷積網路中看到 這些屬性類似的模式 所以您現在己經見過基本的神經網路架構基石 有卷積神經網路， conv 層，池層 跟全連結層 很多的電腦研究部門試著找出如何 將這些基石放在一起來建立一個有效的神經網路 而將這些放在一起實際上需要一些洞見 我想最好的方式 對於您想要獲取直觀，
有關於如何將這些放在一起，最好是 看一些具體的例子看看別人怎麼做 所以在下一個星期，
我會展示您一些具體的例子 即使超過這第一個例子，
您將會見到人們如何成功地將 這些事放一起，建立一個很有效的神經網路 透過下一個星期的影片，我希望您能抓住一些自己的直觀 有關如何建立這些網路 所以當我們給予這些具體的例子，這些或許您可以 直接使用這些別人開發的架構在您的應用上 所以我們在下一星期會做這些 在總結這個星期的影片之前，我想 在下一個影片談一下，為什麼您想用卷積網路 一些好處 跟優點來使用卷積，跟如何將這些擺在一起 如何拿一個您剛看過的神經網路，實際上訓鍊它 在訓練集上來進行影像辨識的一些工作 所以我們來進入這個星期的最後一個影片