1
00:00:00,252 --> 00:00:04,029
지난 강좌에서는 단일 레이어 빌딩블록,

2
00:00:04,029 --> 00:00:06,721
컨볼루션 신경망의 단일 컨볼루션 레이어를 보았습니다.

3
00:00:06,721 --> 00:00:12,339
이제 심층 컨볼루션 신경망의 구체적인 예를 살펴 보겠습니다

4
00:00:12,339 --> 00:00:15,876
이것은 지난 비디오의 끝 부분에서 소개 한

5
00:00:15,876 --> 00:00:17,373
표기법을 연습하도록 해 줄 것입니다.

6
00:00:19,648 --> 00:00:22,203
이미지를 이용해서

7
00:00:22,203 --> 00:00:26,959
이미지 분류, 혹은 이미지 인식을 한다고 가정 해 보겠습니다

8
00:00:26,959 --> 00:00:31,745
인풋 이미지 x를 가지고, 이게 고양이인지 아닌지, 0인지 1인지를 결정해야 하므로

9
00:00:31,745 --> 00:00:34,254
이는 분류 문제입니다.

10
00:00:34,254 --> 00:00:38,626
이 작업을 위해 사용할 수 있는 컨볼루션 신경망의 예를 만들어 보겠습니다

11
00:00:38,626 --> 00:00:42,943
이 예제에서는 상당히 작은 이미지를 사용하려고 합니다.

12
00:00:42,943 --> 00:00:48,499
이 이미지가 39 x 39 x 3이라고 가정 해 보겠습니다.

13
00:00:48,499 --> 00:00:51,529
이렇게 하면 숫자들이 조금 더 잘 작동하도록 만들어줍니다.

14
00:00:51,529 --> 00:00:57,470
레이어 0에 있는 nH 가 nW 와 같아질 것입니다.

15
00:00:57,470 --> 00:01:00,581
즉, 높이와 넓이가 39가 됩니다.

16
00:01:00,581 --> 00:01:06,532
그리고 레이어 0에 있는 채널의 수는3이 됩니다

17
00:01:06,532 --> 00:01:11,906
첫 번째 레이어가 3 x 3 필터 세트를 사용하여 .

18
00:01:11,906 --> 00:01:16,924
feature를 감지한다고 가정 해 보겠습니다. 그러면 f=3 혹은 f1=3이 되죠.

19
00:01:16,924 --> 00:01:20,992
우리가 3x3 process를 사용하고 있기 때문입니다.

20
00:01:20,992 --> 00:01:26,871
스트라이드 1을 사용하고 패딩이 없다고 생각해 봅시다.

21
00:01:26,871 --> 00:01:32,641
그래서 같은 컨볼루션을 사용하고, 10 개의 필터가 있다고 가정 해 봅시다.

22
00:01:34,632 --> 00:01:38,779
그러면 신경망의 다음 레이어에서 activation은

23
00:01:38,779 --> 00:01:43,755
37 x 37 x 10 이 될 것입니다.

24
00:01:43,755 --> 00:01:49,335
또한 10은 10개의 필터를 사용하는 데에서 비롯된 것이고

25
00:01:49,335 --> 00:01:54,735
이 공식

26
00:01:54,735 --> 00:01:58,739
n + 2p- f 나누기 s 더하기 1 (n + 2p - f/ s + 1) 로 부터 37이 나오게 됩니다.

27
00:01:58,739 --> 00:02:03,919
그렇다면, 39 + 0 - 3 나누기 1+1 은

28
00:02:03,919 --> 00:02:10,401
37 이 되죠.

29
00:02:10,401 --> 00:02:15,006
따라서 아웃풋이 37 x 37 이고, 곧 Valid Convolution 입니다.

30
00:02:15,006 --> 00:02:17,590
이게 아웃풋 사이즈이죠.

31
00:02:17,590 --> 00:02:25,029
따라서 우리의 표기법에서는 nh[1] = nw[1] = 37이고

32
00:02:25,029 --> 00:02:30,126
nc[1] = 10 이므로 nc[1]도

33
00:02:30,126 --> 00:02:36,240
첫 번째 레이어의 필터 수와 같습니다.

34
00:02:36,240 --> 00:02:42,040
그리고 이것은 첫 번째 레이어의 activation 차수가 됩니다.

35
00:02:43,300 --> 00:02:45,980
여러분이 또 다른 컨볼루션 레이어를 가지고 있고,

36
00:02:45,980 --> 00:02:48,900
이번엔 5 x 5 x 5 필터를 사용한다고 가정해봅시다.

37
00:02:48,900 --> 00:02:54,996
우리 표기법대로 해서, 다음 신경망에 있는 f[2]는  5이고,

38
00:02:54,996 --> 00:02:59,231
스트라이드 2를 사용한다고 생각합시다.

39
00:02:59,231 --> 00:03:03,922
패딩이 없고

40
00:03:03,922 --> 00:03:07,060
필터는 20개,

41
00:03:09,370 --> 00:03:15,933
그럼 이 아웃풋은 또 다른 볼륨이 될 것이고,

42
00:03:15,933 --> 00:03:20,946
이것은 17 x 17 x 20 이 되겠죠.

43
00:03:20,946 --> 00:03:23,866
여러분이 스트라이드 2를 사용하고 있기 때문에

44
00:03:23,866 --> 00:03:25,926
차수는 훨씬 빠르게 줄어들었고

45
00:03:25,926 --> 00:03:32,800
37 x 37 은 2배보다 약간 더 많이 줄어들어서 17 x 17 이 되었습니다.

46
00:03:32,800 --> 00:03:37,554
필터를 20 개 사용하기 때문에 채널 수는 이제 20입니다.

47
00:03:37,554 --> 00:03:42,167
따라서 이 activation a2는

48
00:03:42,167 --> 00:03:46,971
이 차수가 될 것입니다.

49
00:03:46,971 --> 00:03:52,160
따라서 nh[2] = nw[2] = 17 가 되고,

50
00:03:52,160 --> 00:03:55,247
nc[2] = 20 이 됩니다.

51
00:03:55,247 --> 00:03:58,180
좋습니다, 마지막 컨볼루션 레이어 하나를 적용해 봅시다.

52
00:03:58,180 --> 00:04:04,071
다시 5 x 5 필터를 사용한다고 가정 해 봅시다.

53
00:04:04,071 --> 00:04:07,390
스트라이드 2 이고,

54
00:04:07,390 --> 00:04:13,681
만약 여러분이 이렇게 하면, 저는 계산은 건너 뛰겠습니다. 하지만  7 x 7 사이즈에

55
00:04:13,681 --> 00:04:19,251
필터 40개를 사용한다고 생각하면, 패딩 없음, 필터 40 개.

56
00:04:19,251 --> 00:04:22,760
결국은 7 x 7 x 40 이 나옵니다.

57
00:04:22,760 --> 00:04:27,860
이제 여러분이 한 것은 39 x 39 x 3 인풋 이미지를 취해서

58
00:04:29,380 --> 00:04:34,810
이 이미지의 7 x 7 x 40 feature를 계산한 것입니다.

59
00:04:34,810 --> 00:04:41,075
마지막으로, 일반적으로 수행되는 작업은, 여러분이 7 x 7 x 40을

60
00:04:41,075 --> 00:04:45,137
사실 7 x 7 x 40은 1,960 이죠.

61
00:04:45,137 --> 00:04:50,888
이 볼륨을 취해서 편평하게 하고

62
00:04:50,888 --> 00:04:55,901
1,960 조각으로 이걸 펼쳐보십시오.

63
00:04:55,901 --> 00:04:59,347
이걸 벡터로 전개 한 다음

64
00:04:59,347 --> 00:05:05,283
이것을 로지스틱 회귀 단위 또는 softmax 단위로 피드하십시오.

65
00:05:07,917 --> 00:05:11,682
여러분이 중요한 다른 물체들 중 어느 하나를

66
00:05:11,682 --> 00:05:15,150
인식하려고 시도하는지에 따라

67
00:05:15,150 --> 00:05:19,900
신경망에서 마지막으로 예측된 아웃풋을 제공하셔야 합니다.

68
00:05:20,925 --> 00:05:26,520
분명히 말하자면, 이 마지막 단계는 이 모든 1,960 숫자들을 취해서,

69
00:05:26,520 --> 00:05:32,222
아주 긴 벡터로 펼치는 것입니다.

70
00:05:32,222 --> 00:05:36,483
최종 아웃풋에 대한 예측을 하기 위해 회귀할 때까지

71
00:05:36,483 --> 00:05:39,770
softmax에 피드 할 수있는 긴 벡터 하나가 있습니다.

72
00:05:41,600 --> 00:05:46,125
이는 컨볼루션 신경망의 전형적인 예입니다.

73
00:05:47,380 --> 00:05:51,187
컨볼루션 신경망을 설계하는 많은 작업들은

74
00:05:51,187 --> 00:05:54,880
이것들과 같은 하이퍼 파라미터 변수를 선택합니다. 총 사이즈는 어떤지 결정하고

75
00:05:54,880 --> 00:05:55,840
스트라이드는 어떤지,

76
00:05:55,840 --> 00:05:58,860
패딩이란 무엇이며 얼마나 많은 필터가 사용되는지도 결정합니다.

77
00:06:00,190 --> 00:06:03,980
다음 주와 이번 주 후반에도 저희는

78
00:06:03,980 --> 00:06:07,440
이런 결정을 어떻게 내리는지에 대한 제안과 가이드라인을 드리겠습니다.

79
00:06:07,440 --> 00:06:12,510
그러나 지금 당장은 신경망에서 더 깊숙이 들어가면

80
00:06:12,510 --> 00:06:17,950
일반적으로 39 x 39 크기의 큰 이미지로 시작합니다.

81
00:06:17,950 --> 00:06:21,202
그리고 나서 높이와 너비가 잠시 동안 동일하게 유지됩니다.

82
00:06:21,202 --> 00:06:25,859
그리고 신경망가 깊어짐에 따라, 점차적으로 작아지는 경향을 보입니다.

83
00:06:25,859 --> 00:06:29,663
39가 37로 17로 14로 작아집니다.

84
00:06:29,663 --> 00:06:33,961
죄송합니다. 39가 37로 17로 7이군요

85
00:06:33,961 --> 00:06:36,753
반면 채널 수는 일반적으로 증가합니다.

86
00:06:36,753 --> 00:06:41,412
그것은 3에서 10에서 20에서 40으로 커졌으며

87
00:06:41,412 --> 00:06:45,930
많은 다른 컨볼루션 신경망 에서도 이와 같은 일반적인 경향을 볼 수 있습니다.

88
00:06:47,060 --> 00:06:52,576
따라서 후속 강의들에서 이러한 파라미터를 설계하는 방법에 대한 가이드라인을 보겠지만,

89
00:06:52,576 --> 00:06:57,196
컨볼루션 신경망 혹은 짧게 콘볼네트의

90
00:06:57,196 --> 00:06:59,210
첫 번째 예를 간단히 보셨습니다.

91
00:06:59,210 --> 00:07:00,770
이 점 드립니다.

92
00:07:02,050 --> 00:07:05,500
그리고 일반적으로 컨볼루션 신경망에서

93
00:07:05,500 --> 00:07:07,870
보통 세 개 유형의 레이어가 있습니다.

94
00:07:07,870 --> 00:07:13,615
하나는 콥볼루션 레이어이며, 종종 이것을 콘볼 레이어로 나타냅니다

95
00:07:13,615 --> 00:07:17,025
이것이 이전 네트워크에서 우리가 사용해온 것입니다.

96
00:07:17,025 --> 00:07:20,893
아직 보지는 못한 다른 두 가지 유형의 레이어가 있습니다만,

97
00:07:20,893 --> 00:07:23,945
하지만 다음 강의에서 이야기해보도록 하겠습니다.

98
00:07:23,945 --> 00:07:28,272
하나는 pooling 레이어인데, 저는 종종 pool이라고 부릅니다.

99
00:07:28,272 --> 00:07:32,241
마지막은 FC라고 하는 완전히 연결된 레이어입니다.

100
00:07:32,241 --> 00:07:36,466
컨볼루션 레이어를 사용하여 꽤 좋은 신경망을 설계 할 수도 있지만

101
00:07:36,466 --> 00:07:41,278
대부분의 neural network 설계에 있어서는

102
00:07:41,278 --> 00:07:43,569
몇 개의 pooling 레이어와 완전히 연결된 레이어가 필요할 것입니다.

103
00:07:46,398 --> 00:07:48,103
다행히, pooling layer와

104
00:07:48,103 --> 00:07:52,340
완전히 연결된 레이어는 콘볼루션 레이어를 정의 내리는 것보다는 좀 더 간단합니다

105
00:07:54,150 --> 00:07:58,472
따라서 우리는 다음 두 강좌에서 이를 신속하게 배워보고

106
00:07:58,472 --> 00:08:03,173
컨볼루션 신경망에서 가장 보편적인 유형의 모든 레이어에 대한 감각을 가지게 될 것입니다.

107
00:08:03,173 --> 00:08:06,725
그리고 나면 우리가 방금 본 것보다

108
00:08:06,725 --> 00:08:07,290
더 강력한 네트워크를 만들게 될 것입니다.

109
00:08:08,990 --> 00:08:14,110
첫 번째 컨볼루션 신경망 강의를 전부 다 보신 것을 다시 한번 축하 드립니다

110
00:08:14,110 --> 00:08:18,450
또한 이번 주에 이 네트워크를 훈련하는 법에 대해서도 이야기 할 것입니다.

111
00:08:18,450 --> 00:08:22,180
먼저 pooling과 완전히 연결된 레이어에 대해 간단히 이야기합시다

112
00:08:22,180 --> 00:08:24,659
이 훈련을 위해서

113
00:08:24,659 --> 00:08:26,241
여러분에게 이미 익숙한 역 전파를 사용할 것입니다.

114
00:08:26,241 --> 00:08:30,421
다음 비디오에서는 컨볼루션 신경망에 대한 pooling 레이어를

115
00:08:30,421 --> 00:08:31,230
구현하는 방법을 빠르게 알아 보겠습니다.