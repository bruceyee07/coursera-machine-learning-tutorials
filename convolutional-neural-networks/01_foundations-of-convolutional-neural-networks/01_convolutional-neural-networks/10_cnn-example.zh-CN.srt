1
00:00:00,021 --> 00:00:02,721
现在你已经了解了几乎所有的

2
00:00:02,721 --> 00:00:04,509
用于创建一个完整的卷积神经网络的构件

3
00:00:04,509 --> 00:00:07,336
我们来看个例子

4
00:00:07,336 --> 00:00:12,436
假设输入图像大小为32*32*3

5
00:00:12,436 --> 00:00:18,855
一个RGB图像，或许你试着做手写数字识别

6
00:00:18,855 --> 00:00:24,396
例如，你有一个32*32 RGB图像

7
00:00:24,396 --> 00:00:30,495
要尝试识别图像中是0-9中10个数字的哪一个是7

8
00:00:30,495 --> 00:00:32,791
让我们创建一个神经网络来做这项工作

9
00:00:32,791 --> 00:00:35,827
这里中我将要使用的算法 是受到了

10
00:00:35,827 --> 00:00:41,106
一种和它非常相似的经典的神经网络的启发 称作LeNet-5

11
00:00:41,106 --> 00:00:43,942
LeNet-5多年前由Yann LeCun创建

12
00:00:43,942 --> 00:00:47,680
这里我将要展示的和LeNet-5不完全一样，但

13
00:00:47,680 --> 00:00:53,024
其中许多参数的选择是受到它启发的

14
00:00:53,024 --> 00:00:58,524
这里有一个32*32*3的输入，让我们用一个

15
00:00:58,524 --> 00:01:04,770
5*5的过滤器，步长为1，无补丁生成第一层输出

16
00:01:04,770 --> 00:01:08,240
那么第一层输出

17
00:01:08,240 --> 00:01:13,732
是28*28*6，假设使用了6个过滤器

18
00:01:13,732 --> 00:01:18,803
我们称这一层为conv 1

19
00:01:18,803 --> 00:01:23,660
那么用了6个过滤器，加上偏差值，再应用非线性

20
00:01:23,660 --> 00:01:28,356
或许ReLU非线性，得到了卷积层一的输出

21
00:01:28,356 --> 00:01:32,678
接下来，让我们使用一个池化层

22
00:01:32,678 --> 00:01:40,280
这里我将使用最大池化，应用参数f=2，s=2

23
00:01:40,280 --> 00:01:44,101
如果我没有写补丁参数，就意味补丁为零

24
00:01:44,101 --> 00:01:48,895
接下来让我们使用一个池化层

25
00:01:48,895 --> 00:01:54,975
假设我们用一个2*2的过滤器，步长为2做最大池化

26
00:01:54,975 --> 00:01:57,064
那么原有的高度和宽度

27
00:01:57,064 --> 00:01:59,614
将会缩小一半

28
00:01:59,614 --> 00:02:04,138
因此28*28现在变成了14*14

29
00:02:04,138 --> 00:02:10,472
通道数量保持不变，所以大小为14*14*6

30
00:02:10,472 --> 00:02:15,536
我们把这层称之为池1输出

31
00:02:15,536 --> 00:02:20,111
事实上在卷积网文档中中有二种关于层的说法

32
00:02:20,111 --> 00:02:25,562
二者有细微的差别

33
00:02:25,562 --> 00:02:30,918
一种说法是这称为一层

34
00:02:30,918 --> 00:02:35,900
那么这二个单元一起成为神经网络的第一层

35
00:02:35,900 --> 00:02:40,980
另一种说法是卷积层称为一层，池化层单独称为一层

36
00:02:40,980 --> 00:02:45,223
神经网络中当人们说到网络层数的时候，通常指

37
00:02:45,223 --> 00:02:49,025
那些有权重，有参数的网络层数量

38
00:02:49,025 --> 00:02:53,043
因为池化层没有权重，没有参数

39
00:02:53,043 --> 00:02:57,418
只有一些超参数，我会使用

40
00:02:57,418 --> 00:02:59,015
卷积层1和池化层1为一体的说法

41
00:02:59,015 --> 00:03:03,551
把他们作为层一，尽管有时候

42
00:03:03,551 --> 00:03:08,447
当你看在线文章或读研究论文时，你会听到卷积层

43
00:03:08,447 --> 00:03:11,703
和池化层被称为两个独立层

44
00:03:11,703 --> 00:03:16,788
但这不过是二种细微不一致的表示术语

45
00:03:16,788 --> 00:03:22,053
这里当我计算层数，我只会算那些有权重的层

46
00:03:22,053 --> 00:03:25,614
所以我视这二个一起为层一

47
00:03:25,614 --> 00:03:30,822
并且Conv1和Pool1名字末尾是1

48
00:03:30,822 --> 00:03:37,961
也表明了我认为二者都是神经网络层一的组成部分

49
00:03:37,961 --> 00:03:42,665
因为池化层1没有自己的权重，所以被并入层一

50
00:03:42,665 --> 00:03:47,585
接下来，在14*14*6的基础上，让我们

51
00:03:47,585 --> 00:03:53,181
再做一层卷积， 这次使用大小为5*5的过滤器

52
00:03:53,181 --> 00:03:58,796
并且步长为一，共10个过滤器

53
00:03:58,796 --> 00:04:04,350
那现在你会得到一个10*10*10的单元

54
00:04:04,350 --> 00:04:09,786
称之为卷积层二

55
00:04:09,786 --> 00:04:14,467
然后我们继续做最大池化

56
00:04:14,467 --> 00:04:19,008
再次使用池化参数f=2，s=2

57
00:04:19,008 --> 00:04:23,456
或许你已经了解了在此参数下

58
00:04:23,456 --> 00:04:26,769
输出的高度和宽度会减半

59
00:04:26,769 --> 00:04:31,425
所以剩下的是5*5*10大小的

60
00:04:31,425 --> 00:04:34,773
这一步被称为池化层2

61
00:04:34,773 --> 00:04:39,652
按照我们的说法这二个合称为神经网络层二

62
00:04:39,652 --> 00:04:42,293
接下来我们再使用一次卷积层

63
00:04:42,293 --> 00:04:47,113
我将会使用5*5大小的过滤器，那么f等于5

64
00:04:47,113 --> 00:04:51,962
假设步长为1，且不做填充

65
00:04:51,962 --> 00:04:58,254
让我们使用16个过滤器，这样你会获得卷积层2的输出

66
00:04:58,254 --> 00:05:03,860
也就是10*10*16大小的输出

67
00:05:03,860 --> 00:05:10,380
如图所示，这是卷积层二

68
00:05:10,380 --> 00:05:17,356
接着我们来做最大池化，假设f等于2，s等于2

69
00:05:17,356 --> 00:05:20,227
你或许已经知道了结果

70
00:05:20,227 --> 00:05:24,555
在10*10*16基础上做f等于2，s等于2的最大池化

71
00:05:24,555 --> 00:05:26,667
将会使输入高度和宽度减半

72
00:05:26,667 --> 00:05:31,075
你应该已经知道结果了，对吧

73
00:05:31,075 --> 00:05:32,463
用f等于2，s等于2做池化

74
00:05:32,463 --> 00:05:37,663
高度和宽度被减半，那么最终获得

75
00:05:37,663 --> 00:05:43,214
一个5*5*16的单元，通道数保持不变

76
00:05:43,214 --> 00:05:47,166
我们称这块为池化层二

77
00:05:47,166 --> 00:05:52,429
按照我们的说法这块被称为层二

78
00:05:52,429 --> 00:05:57,203
由于只有卷积层二有权重

79
00:05:57,203 --> 00:06:03,306
这里5乘5乘16等于400

80
00:06:03,306 --> 00:06:10,895
现在让我们把池化层二展开成一个400*1的向量

81
00:06:10,895 --> 00:06:16,686
把它想象成如图所示的一组展开的神经元

82
00:06:16,686 --> 00:06:22,373
接下来我们要做的是用着400个单元做输入

83
00:06:22,373 --> 00:06:30,070
创建一个有120个单元的下一层

84
00:06:30,070 --> 00:06:33,243
这实际上是我们第一个全连接网络层

85
00:06:33,243 --> 00:06:38,392
我将称之为FC3

86
00:06:38,392 --> 00:06:44,410
因为这400个输入单元和120输出单元密集的相连

87
00:06:46,245 --> 00:06:51,628
因此这个全连接层

88
00:06:51,628 --> 00:06:56,660
和你在课程1和课程2所见到的单神经网络层一样

89
00:06:56,660 --> 00:07:01,710
这不过是一个标准神经网络，其中被称为w3的

90
00:07:01,710 --> 00:07:08,044
权重矩阵大小为120*400

91
00:07:08,044 --> 00:07:13,406
因为400个输入中的每一个单元都和120输出的每一个单元相连

92
00:07:13,406 --> 00:07:18,354
所以被称为全连接网，并且这里还有个偏差参数

93
00:07:18,354 --> 00:07:23,655
大小也将是120的向量，因为有120个输出

94
00:07:23,655 --> 00:07:28,715
最后一步让我们在120的单元基础上再加一层

95
00:07:28,715 --> 00:07:33,119
这次让它变得更小，假设我们有84个单元最终

96
00:07:33,119 --> 00:07:36,883
我将它称为全连接网络层4

97
00:07:36,883 --> 00:07:44,435
最终我们获得了可以用于Softmax层的84个实数

98
00:07:44,435 --> 00:07:48,215
如果你是试着做手写数字识别

99
00:07:48,215 --> 00:07:51,794
来确定是0，1，2，直到9中的数字

100
00:07:51,794 --> 00:07:56,680
那这将是一个有着10个输出的Softmax层

101
00:07:56,680 --> 00:08:00,969
这就是一个相对典型的例子

102
00:08:00,969 --> 00:08:05,482
展示了一个卷积神经网络的大致构成

103
00:08:05,482 --> 00:08:09,367
我理解这其中似乎有许多超参数

104
00:08:09,367 --> 00:08:12,919
稍后我们将会给出一些更具体的建议

105
00:08:12,919 --> 00:08:15,882
关于如何选择这类超参数

106
00:08:15,882 --> 00:08:20,388
也许一个常用的法则实际上是

107
00:08:20,388 --> 00:08:22,802
不要试着创造你自己的超参数组

108
00:08:22,802 --> 00:08:27,887
而是查看文献，看看其他人使用的超参数

109
00:08:27,887 --> 00:08:30,963
从中选一组适用于其他人的超参数

110
00:08:30,963 --> 00:08:35,316
很可能它也适用于你的应用

111
00:08:35,316 --> 00:08:38,321
下周我们会做更多介绍

112
00:08:38,321 --> 00:08:43,715
目前我只想指出，通常随着神经网络的深入

113
00:08:43,715 --> 00:08:47,493
高度nh和宽度nw会减小

114
00:08:47,493 --> 00:08:52,432
如之前所示，从32*32减到20*20，14*14

115
00:08:52,432 --> 00:08:53,934
再到10*10，最终减到5*5

116
00:08:53,934 --> 00:08:57,870
因此当你深入下去通常高度和宽度将会减小

117
00:08:57,870 --> 00:09:00,852
然而通道数量会增加

118
00:09:00,852 --> 00:09:07,277
这里从3到6再到16，最后是全连通网络层

119
00:09:07,277 --> 00:09:13,135
另一类常见的神经网络模型是

120
00:09:13,135 --> 00:09:17,426
一个或多个卷积层接着一层池化层

121
00:09:17,426 --> 00:09:21,329
再接着一个或多个卷积层叠加一层池化层

122
00:09:21,329 --> 00:09:24,731
然后叠加几层全连接层

123
00:09:24,731 --> 00:09:26,756
也许最后还叠加一个Softmax层

124
00:09:26,756 --> 00:09:32,378
如上所述是另一个常见的神经网络模型

125
00:09:32,378 --> 00:09:33,956
那么让我们再回顾一下

126
00:09:33,956 --> 00:09:37,968
神经网络的一些细节

127
00:09:37,968 --> 00:09:41,799
如激活输入的尺寸，大小和网络参数数量

128
00:09:41,799 --> 00:09:44,181
这里输入为32*32*3

129
00:09:44,181 --> 00:09:48,324
这些数字相乘应该等于3072

130
00:09:48,324 --> 00:09:54,313
所以激活输入a0的大小为3072

131
00:09:54,313 --> 00:09:58,005
实际尺寸是32*32*3

132
00:09:58,005 --> 00:10:02,562
我认为输入层是没有参数的

133
00:10:02,562 --> 00:10:05,672
再看看接下来的不同层数据

134
00:10:05,672 --> 00:10:09,068
试着自己来算一算

135
00:10:09,068 --> 00:10:10,975
如表所示，这是不同层的激活输入的尺寸

136
00:10:10,975 --> 00:10:13,743
和大小

137
00:10:15,422 --> 00:10:16,957
这里需要指出几点

138
00:10:16,957 --> 00:10:23,352
首先，注意最大池化没有任何参数

139
00:10:23,352 --> 00:10:28,202
其次，注意卷积层趋向于拥有越来越少的参数

140
00:10:28,202 --> 00:10:32,302
正如我们在早前的视频中所述

141
00:10:32,302 --> 00:10:36,414
实际上，多数参数

142
00:10:36,414 --> 00:10:39,426
在神经网络的全连接层上

143
00:10:39,426 --> 00:10:44,584
同时，随着神经网络的深入

144
00:10:44,584 --> 00:10:50,289
你会发现激活输入大小也逐渐变小

145
00:10:50,289 --> 00:10:55,198
如果减少的太快，通常也不利于网络性能

146
00:10:55,198 --> 00:11:00,349
这里首先大小从6000减到1600

147
00:11:00,349 --> 00:11:06,405
接下来慢慢减小到84，直到最终得到softmax层的输出

148
00:11:06,405 --> 00:11:10,683
你会发现许多卷积神经网络有着

149
00:11:10,683 --> 00:11:13,293
与此相似的特性和模式

150
00:11:13,293 --> 00:11:16,455
到这里你已经了解了神经网络的基本构件

151
00:11:16,455 --> 00:11:20,068
卷积神经网络，卷积层，池化层

152
00:11:20,068 --> 00:11:21,601
和全连接层

153
00:11:21,601 --> 00:11:25,693
如何利用这些基本构件来构造一个有效的神经网络

154
00:11:25,693 --> 00:11:29,078
已经有许多计算机领域专家在深入研究了

155
00:11:29,078 --> 00:11:33,379
把这些构件组合到一起实际上需要相当的洞察力

156
00:11:33,379 --> 00:11:35,213
我认为最好的方法之一

157
00:11:35,213 --> 00:11:39,323
就是去学习一定数量的实例，看看人家是如何做的

158
00:11:39,323 --> 00:11:41,804
从中来获得灵感如何把这些构件组合在一起

159
00:11:41,804 --> 00:11:46,268
所以除了你现在看到的第一个实例，下周我将给你展示

160
00:11:46,268 --> 00:11:50,183
其他一些实例，看看大家是如何成功的用

161
00:11:50,183 --> 00:11:53,637
这些构件来创建很高效的神经网络

162
00:11:53,637 --> 00:11:58,532
通过下周的视频希望能帮到你

163
00:11:58,532 --> 00:12:00,098
获得一些关于如何构建神经网络的自己的想法

164
00:12:00,098 --> 00:12:05,068
而且既然已经给出了一些实例也许你可以直接

165
00:12:05,068 --> 00:12:09,120
在自己的应用程序中使用他人已经开发的构架

166
00:12:09,120 --> 00:12:10,971
下周我们再讲

167
00:12:10,971 --> 00:12:15,499
在本周收尾前，最后一点我想提一下

168
00:12:15,499 --> 00:12:19,840
接下来的视频我将浅谈一下为什么要用卷积

169
00:12:19,840 --> 00:12:20,869
使用卷积的好处和优点

170
00:12:20,869 --> 00:12:25,133
以及如何把他们组合在一起

171
00:12:25,133 --> 00:12:29,021
如何通过训练集来训练一个神经网络，如之前所见

172
00:12:29,021 --> 00:12:32,735
来做图像识别或者其他一些任务

173
00:12:32,735 --> 00:12:35,700
那么接下来让我们看看本周的最后一个视频