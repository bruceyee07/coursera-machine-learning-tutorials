1
00:00:00,252 --> 00:00:04,029
在上一节课中,
你看到了一层神经网络的的构造

2
00:00:04,029 --> 00:00:06,721
ConvNet中的一个卷积层

3
00:00:06,721 --> 00:00:12,339
现在我们来研究一个深度卷积神经网络的实例。

4
00:00:12,339 --> 00:00:15,876
这将会让你再次熟悉一些我们在

5
00:00:15,876 --> 00:00:17,373
上一节课中靠近结尾的部分引入的一些概念和符号。

6
00:00:19,648 --> 00:00:22,203
假设我们有一张图像

7
00:00:22,203 --> 00:00:26,959
并且你想做图像分类，或图像识别

8
00:00:26,959 --> 00:00:31,745
输入为一个图像, x ,
然后判断这是不是一只猫，0或1

9
00:00:31,745 --> 00:00:34,254
所以这是一个分类问题。

10
00:00:34,254 --> 00:00:38,626
让我们构建一个可用于此任务的ConvNet范例。

11
00:00:38,626 --> 00:00:42,943
为举例起见,我将使用一个相当小的图像。

12
00:00:42,943 --> 00:00:48,499
假设这个图像是 39 x 39 x 3。

13
00:00:48,499 --> 00:00:51,529
这个假设只是为了使得有些计算更加简单

14
00:00:51,529 --> 00:00:57,470
因此，第0层中的nH（高度）和nW（宽度）都

15
00:00:57,470 --> 00:01:00,581
因此，第0层中的nH和nW都为39

16
00:01:00,581 --> 00:01:06,532
然后通道数为3

17
00:01:06,532 --> 00:01:11,906
假设第一层使用一组3乘3过滤器

18
00:01:11,906 --> 00:01:16,924
来检测特征,所以，f=3，或写成f1=3,

19
00:01:16,924 --> 00:01:20,992
因为我们使用的是一个3x3的过程

20
00:01:20,992 --> 00:01:26,871
假设我们使用为1的步长（stride）
不使用填充（padding）

21
00:01:26,871 --> 00:01:32,641
使用相同的卷积,假设你有10过滤器

22
00:01:34,632 --> 00:01:38,779
那神经网络下一层的激活数

23
00:01:38,779 --> 00:01:43,755
将会是37 x 37 x 10

24
00:01:43,755 --> 00:01:49,335
这里10因为你使用了10个过滤器。

25
00:01:49,335 --> 00:01:54,735
37来自以下公式

26
00:01:54,735 --> 00:01:58,739
n + 2p - f除以 s，再 + 1

27
00:01:58,739 --> 00:02:03,919
对吧，那你有 39 + 0 - 3

28
00:02:03,919 --> 00:02:10,401
除以 1，在 + 1，得到37

29
00:02:10,401 --> 00:02:15,006
所以这就是为什么输出是 37 x 37 ,
这是一个有效的卷积

30
00:02:15,006 --> 00:02:17,590
并且这就是输出的大小

31
00:02:17,590 --> 00:02:25,029
因此,用我们的符号，你有nh[1]=nw[1]=37

32
00:02:25,029 --> 00:02:30,126
以及nc[1]=10

33
00:02:30,126 --> 00:02:36,240
nc[1]同时也等于第一层的过滤器的数量

34
00:02:36,240 --> 00:02:42,040
因此,这就是第一层的激活的维度。

35
00:02:43,300 --> 00:02:45,980
假设你现在有另一个卷积层

36
00:02:45,980 --> 00:02:48,900
而且假设这次你使用 5 x 5 个过滤器

37
00:02:48,900 --> 00:02:54,996
所以,用我们的符号表示，
下一个层神经网络的 f[2] = 5,

38
00:02:54,996 --> 00:02:59,231
假设这次使用2作为步长

39
00:02:59,231 --> 00:03:03,922
没有填充（padding）

40
00:03:03,922 --> 00:03:07,060
和20个过滤器

41
00:03:09,370 --> 00:03:15,933
那这次的输出的纬度将会是

42
00:03:15,933 --> 00:03:20,946
17 x 17 x 20

43
00:03:20,946 --> 00:03:23,866
请注意,因为你现在使用的步长为2

44
00:03:23,866 --> 00:03:25,926
维度收缩得更快

45
00:03:25,926 --> 00:03:32,800
37 x 37下降因子稍微超过2， 到 17 x 17。

46
00:03:32,800 --> 00:03:37,554
因为使用20个过滤器,所以现在的通道数是20。

47
00:03:37,554 --> 00:03:42,167
所以这里的激活a2

48
00:03:42,167 --> 00:03:46,971
将是这个维度

49
00:03:46,971 --> 00:03:52,160
并且nh[2] = nw[2] = 17，

50
00:03:52,160 --> 00:03:55,247
nc [2] = 20

51
00:03:55,247 --> 00:03:58,180
好,让我们再加最后一个卷积层。

52
00:03:58,180 --> 00:04:04,071
假设你还是使用 5 x 5 过滤器

53
00:04:04,071 --> 00:04:07,390
同时步长为2

54
00:04:07,390 --> 00:04:13,681
如果你这样做, 通过计算，你将得到7 x 7,

55
00:04:13,681 --> 00:04:19,251
假设您使用40过滤器, 
没有填充, 40 过滤器

56
00:04:19,251 --> 00:04:22,760
你将会得到 7 x 7 x 40

57
00:04:22,760 --> 00:04:27,860
所以现在你所做的是，把39 x 39 x 3的输入图像

58
00:04:29,380 --> 00:04:34,810
计算出此图像的 7 x 7 x 40 特征。

59
00:04:34,810 --> 00:04:41,075
最后,通常会做的是, 如果你把这个 7 x 7 x 40,

60
00:04:41,075 --> 00:04:45,137
7乘以7乘以40实际上是1960

61
00:04:45,137 --> 00:04:50,888
现在，我们可以把这些特征

62
00:04:50,888 --> 00:04:55,901
展开为1960个单元，对吧

63
00:04:55,901 --> 00:04:59,347
扁平化成一个向量,

64
00:04:59,347 --> 00:05:05,283
然后将其输入到一个逻辑回归
或 softmax 单元。

65
00:05:07,917 --> 00:05:11,682
取决于你是在试图识别
“有猫“或者“无猫“（二元）

66
00:05:11,682 --> 00:05:15,150
或者试图识别任意的k种东西（多元）

67
00:05:15,150 --> 00:05:19,900
这就会给出神经网络的最终预测输出

68
00:05:20,925 --> 00:05:26,520
所以,要清楚,
这最后一步只是采取所有这些数字,

69
00:05:26,520 --> 00:05:32,222
所有1960数字,并展开他们成为一个非常长的矢量。

70
00:05:32,222 --> 00:05:36,483
所以,你只是有一个长的向量,
你可以输送到softmax, 直到它是

71
00:05:36,483 --> 00:05:39,770
为了做出最终预测输出的一个回归

72
00:05:41,600 --> 00:05:46,125
所以这将是一个非常典型的 ConvNet 的例子。

73
00:05:47,380 --> 00:05:51,187
卷积神经网络设计中的许多工作是选择

74
00:05:51,187 --> 00:05:54,880
像这样的超参数（hyperparameter）：
总单元数是多少？

75
00:05:54,880 --> 00:05:55,840
步长是什么？

76
00:05:55,840 --> 00:05:58,860
padding是多少和使用了多少过滤器？

77
00:06:00,190 --> 00:06:03,980
在本周晚些时候以及下周, 
我们将给一些

78
00:06:03,980 --> 00:06:07,440
关于如何做出这些选择的一些建议和指导方针。

79
00:06:07,440 --> 00:06:12,510
但是就目前而言，我们要记住的一件事是
随着你建的神经网络越来越深，

80
00:06:12,510 --> 00:06:17,950
通常你开始的时候，
图像都是比较大的，例如39乘39

81
00:06:17,950 --> 00:06:21,202
高度和宽度在保持不变一阵子后

82
00:06:21,202 --> 00:06:25,859
随着你在神经网络中的深入, 逐渐变小

83
00:06:25,859 --> 00:06:29,663
在我们的这个例子里，它从39到37到17到14。

84
00:06:29,663 --> 00:06:33,961
不好意思，是39到37到17到7

85
00:06:33,961 --> 00:06:36,753
而频道的数目一般会增加。

86
00:06:36,753 --> 00:06:41,412
它已经从3到10到20到40,

87
00:06:41,412 --> 00:06:45,930
而你会在许多其他卷积神经网络中
看到这样的一种常见的趋势

88
00:06:47,060 --> 00:06:52,576
因此, 我们将在以后的视频中给出
更多如何设计这些参数的指导。

89
00:06:52,576 --> 00:06:57,196
但你现在已经看到了一个卷积神经网络的第一个例子,

90
00:06:57,196 --> 00:06:59,210
或者简称为ConvNet

91
00:06:59,210 --> 00:07:00,770
恭喜你

92
00:07:02,050 --> 00:07:05,500
事实证明, 在一个典型的 ConvNet,

93
00:07:05,500 --> 00:07:07,870
通常有三种类型的层。

94
00:07:07,870 --> 00:07:13,615
一个是卷积层, 通常我们会将其表示为一个Conv层

95
00:07:13,615 --> 00:07:17,025
这就是我们在前面的网络中使用的。

96
00:07:17,025 --> 00:07:20,893
事实证明, 还有两种常见的层类型,

97
00:07:20,893 --> 00:07:23,945
你没有看到，但是我们会在后面的几个视频中讲到

98
00:07:23,945 --> 00:07:28,272
一个称为 "池" 层, 通常我会直接称为池。

99
00:07:28,272 --> 00:07:32,241
最后是一个完全连通的层称为 FC。

100
00:07:32,241 --> 00:07:36,466
虽然我们可能只是使用

101
00:07:36,466 --> 00:07:41,278
卷积层来设计一个相当好的神经网络,
大多数神经网络架构也将有几个池层

102
00:07:41,278 --> 00:07:43,569
和几个完全连接层。

103
00:07:46,398 --> 00:07:48,103
幸运的是池层和

104
00:07:48,103 --> 00:07:52,340
完全连接层比卷积层要简单一些。

105
00:07:54,150 --> 00:07:58,472
因此, 我们将在下两节课中做简单的介绍,

106
00:07:58,472 --> 00:08:03,173
你将对卷积神经网络中所有常见的层都有一个感觉

107
00:08:03,173 --> 00:08:06,725
你将会够构建比我们刚刚展示的

108
00:08:06,725 --> 00:08:07,290
更强大的神经网络

109
00:08:08,990 --> 00:08:14,110
再次恭喜你看到你的第一个完整的卷积神经网络。

110
00:08:14,110 --> 00:08:18,450
我们还将在本周晚些时候讨论
如何训练这些网络,

111
00:08:18,450 --> 00:08:22,180
但是在那之前，我们会先简单地
谈谈池层和完全连接层。

112
00:08:22,180 --> 00:08:24,659
我们将使用熟悉的反向传播

113
00:08:24,659 --> 00:08:26,241
来练这些网络

114
00:08:26,241 --> 00:08:30,421
但在下一节课中,让我们快速了解如何实现

115
00:08:30,421 --> 00:08:31,230
ConvNet中的一个池层
GTC字幕组翻译