1
00:00:03,945 --> 00:00:07,666
이제 컨볼루션 신경망의 한 레이어를 어떻게 만들 수 있을지 알아보도록 하겠습니다.

2
00:00:07,666 --> 00:00:09,223
예제를 살펴 보도록 하죠.

3
00:00:12,432 --> 00:00:17,146
이전 비디오에서 3D 볼륨을 가져 오는 방법과

4
00:00:17,146 --> 00:00:20,670
두 개의 서로 다른 필터를 사용하여 볼륨을 합성(convolve)하는 방법을 살펴 보았습니다.

5
00:00:21,810 --> 00:00:25,861
이 예제를 다른 4x4 아웃풋으로 가져 오기 위해.

6
00:00:30,891 --> 00:00:35,548
첫 번째 필터로 convolving하면

7
00:00:35,548 --> 00:00:40,769
첫 번째 4x4 아웃풋이 생성됩니다.

8
00:00:40,769 --> 00:00:49,108
두 번째 필터로 convolving하면 또 다른 4x4 아웃풋이 제공됩니다.

9
00:00:49,108 --> 00:00:55,574
이것을 컨볼루션 신경망 레이어로 바꿔야 할 마지막 작업은

10
00:00:55,574 --> 00:01:00,566
이들 각각에 바이어스를 추가하는 것입니다.

11
00:01:00,566 --> 00:01:03,980
그래서 이것은 실제 숫자가 될 것입니다.

12
00:01:03,980 --> 00:01:08,840
그리고 파이썬 브로드캐스팅이있는 곳에서는.

13
00:01:08,840 --> 00:01:11,750
이 16개 원소의 각각에 같은 숫자를 추가해야 합니다

14
00:01:11,750 --> 00:01:16,805
그리고 나서 이 그림에 비선형성을 적용하면, 상대적인 비선형성이죠,

15
00:01:16,805 --> 00:01:23,430
그러면 4x4 아웃풋을 산출하게 됩니다, 이해되시죠?

16
00:01:23,430 --> 00:01:27,295
바이어스와 비 선형성을 적용하고 나면

17
00:01:27,295 --> 00:01:31,894
바닥의 이 점에 대해서도 다른 바이어스를 더합니다.

18
00:01:31,894 --> 00:01:33,154
다시 말하지만, 이것은 실제 숫자입니다.

19
00:01:33,154 --> 00:01:36,473
따라서 모든 16 개의 숫자에 단일 숫자를 추가 한 다음

20
00:01:36,473 --> 00:01:40,934
비선형 성을 적용합니다, 실제 비선형성이라고 가정해보죠.

21
00:01:40,934 --> 00:01:47,369
그러면 또 다른 4x4 아웃풋이 만들어집니다.

22
00:01:47,369 --> 00:01:52,461
이전에 했던 것처럼 이것을 취해서 다음과 같이 쌓아두면

23
00:01:52,461 --> 00:01:59,698
4x4x2 아웃풋을 마침내 얻게 됩니다.

24
00:01:59,698 --> 00:02:06,326
그런 다음이 계산은 6x6x3x4x4x4인데요,

25
00:02:06,326 --> 00:02:11,303
이게 바로 컨볼루션 신경망의 하나의 레이어가 됩니다.

26
00:02:11,303 --> 00:02:15,634
표준 신경망, 즉 컨볼루션이 아닌 신경망에서

27
00:02:15,634 --> 00:02:18,832
4가지 역전파 중 하나의 레이어에 이걸 매핑하기 위해서는

28
00:02:18,832 --> 00:02:23,155
역전파 단계 바로 전에 이런 게 있다는 걸 기억하셔야 합니다.

29
00:02:23,155 --> 00:02:28,461
z[1]=w[1] a[0], 여기서 a0은 x 입니다.

30
00:02:28,461 --> 00:02:31,265
그리고 나서 b[1] 을 더합니다.

31
00:02:31,265 --> 00:02:38,121
그리고 나서 a[1]산출하기 위해서 비선형성을 적용하면 g(z[1])가 됩니다.

32
00:02:38,121 --> 00:02:43,400
그래서 유추해보자면 여기서 이 인풋은, a[0], x3입니다.

33
00:02:44,770 --> 00:02:48,076
그리고 이 필터들은

34
00:02:48,076 --> 00:02:52,488
w1과 비슷한 역할을 합니다.

35
00:02:52,488 --> 00:02:56,377
컨볼루션 연산 작업 중에 이 27 개의 숫자를 취하게 된다는 걸 명심하세요

36
00:02:56,377 --> 00:03:01,149
아니면 두 개의 필터가 있기 때문에 27의 2배에 해당하는 숫자를 가질 수도 있습니다.

37
00:03:01,149 --> 00:03:03,900
이 숫자들을 모두 가져 와서 그것을 곱하십시오.

38
00:03:03,900 --> 00:03:09,631
여러분은 이 4x4 행렬을 얻기 위한 선형 함수를 실제로 계산하고 있습니다.

39
00:03:09,631 --> 00:03:15,048
따라서 4x4 행렬, 즉 컨볼루션 연산의 아웃풋은

40
00:03:15,048 --> 00:03:19,245
W[1]와 a[0] 를 곱한 것과 비슷한 역할을 합니다.

41
00:03:19,245 --> 00:03:25,340
저쪽 4x4 뿐만 아니라 이쪽에 있는 4x4 아웃풋이 생겨나게 됩니다.

42
00:03:25,340 --> 00:03:29,599
해야 할 또 다른 건 바이어스를 더하는 일입니다.

43
00:03:29,599 --> 00:03:35,138
값을 더하기에 앞서서

44
00:03:35,138 --> 00:03:38,939
여기 있는 이것들은 z과 비슷한 역할을 합니다.

45
00:03:38,939 --> 00:03:43,848
제 생각엔 이것은 마침에 비선형성을 적용하는 것으로 만들어지는 거죠.

46
00:03:43,848 --> 00:03:49,740
이 아웃풋은 a[1]의 역할을 하고,

47
00:03:49,740 --> 00:03:53,390
이것은 곧 다음 레이어에서 여러분의 활동성이 됩니다.

48
00:03:53,390 --> 00:03:58,794
따라서 이것이 바로 선형의 연산과 컨볼루션이 곱해질 때

49
00:03:58,794 --> 00:04:02,192
a[0]에서 a[1] 진행하는 방법입니다.

50
00:04:02,192 --> 00:04:05,507
따라서 컨볼루션은 실제로 선형 연산을 적용하고 있으며,

51
00:04:05,507 --> 00:04:08,437
바이어스와 적용된 Value Operation 또한 가지고 있는 것입니다.

52
00:04:08,437 --> 00:04:14,107
6x6x3 차원의 a[0]로 부터 시작해서

53
00:04:14,107 --> 00:04:18,210
신경망의 레이어 하나를 거쳐서

54
00:04:18,210 --> 00:04:22,693
4x4x2 차원의 a[1]으로 바뀌게 되는 것입니다.

55
00:04:22,693 --> 00:04:27,144
따라서 6x6x3 은 4x4x2로 바뀌어지고,

56
00:04:27,144 --> 00:04:30,860
컨볼루션 망의 하나의 레이어가 된는 것이지요.

57
00:04:33,697 --> 00:04:40,504
이 예제에서 우리는 두 개의 필터를 가지고 있습니다, 즉, 두 가지의 feature이죠.

58
00:04:40,504 --> 00:04:45,270
이것으로 우린 4x4x2의 아웃풋을 생성할 수 있었습니다

59
00:04:45,270 --> 00:04:49,248
하지만 예를 들어 2개가 아니라 필터가 10개가 있다면,

60
00:04:49,248 --> 00:04:54,396
4x4x10 dimension의 아웃풋 볼륨을 도출하게 되겠지요.

61
00:04:54,396 --> 00:05:00,334
2개가 아니라 10개의 망을 취하게 될 것이고,

62
00:05:00,334 --> 00:05:05,598
4x4x10의 아웃풋 볼륨을 형성하기 위해 쌓아 올리게 될 것이기 때문입니다. 그게 바로 a[1]이 되는 것이죠.

63
00:05:05,598 --> 00:05:09,630
그럼, 이해가 잘 됐는지 확인하기 위해서, 예제를 같이 풀어봅시다.

64
00:05:09,630 --> 00:05:14,655
여러분에게 필터가 10개 있다고 가정해봅시다. 2개가 아니라 3x3x3의 볼륨이고,

65
00:05:14,655 --> 00:05:19,600
신경망의 하나의 레이어라고 생각해보세요. 이 레이어는 얼마나 많은 파라미터를 가지고 있을까요?

66
00:05:21,000 --> 00:05:22,984
함께 알아보시죠.

67
00:05:22,984 --> 00:05:29,439
각각의 필터가 3x3x3의 볼륨이므로,

68
00:05:29,439 --> 00:05:35,318
각각은 27개의 파라미터로 채워지겠죠, 그렇죠?

69
00:05:35,318 --> 00:05:39,800
들어갈 숫자가 27개가 될 것이고, 거기에 바이어스가 있습니다.

70
00:05:42,210 --> 00:05:46,818
그러면 b는 파라미터이므로, 그럼 28개의 파라미터가 되는 것이죠.

71
00:05:50,125 --> 00:05:54,456
그리고 이전 슬라이드에 있던 두 개의 필터를 그렸다고 상상해보십시오.

72
00:05:54,456 --> 00:05:58,329
그러나 지금 실제로 10 개가 있다고 상상한다면,

73
00:05:58,329 --> 00:06:01,747
1, 2… 10

74
00:06:01,747 --> 00:06:06,942
그리고 이 모두 합쳐서 28개에 10을 곱하면

75
00:06:06,942 --> 00:06:10,753
총 280개의 파라미터가 됩니다.

76
00:06:10,753 --> 00:06:16,316
이것에 관해 좋은 점 하나를 주목하십시오. 인풋 이미지의 크기가 아무리 크다 해도

77
00:06:16,316 --> 00:06:22,051
인풋 이미지는 1,000x1,000 또는 5,000x5,000가 될 것입니다.

78
00:06:22,051 --> 00:06:26,973
하지만 파라미터의 수는 여전히 280로 고정되어 있습니다.

79
00:06:26,973 --> 00:06:31,453
그리고 이 10 개의 필터를 사용하여 feature, 수직 모서리,

80
00:06:31,453 --> 00:06:35,485
수평 모서리를 감지할 수 있습니다.

81
00:06:35,485 --> 00:06:39,240
어쩌면 매우 큰 이미지 에서조차 도 다른 feature는 파라미터가 매우 작은 수 일 수도 있습니다.

82
00:06:40,920 --> 00:06:44,410
따라서, 이는 컨볼루션 신경망의 하나의 속성으로서

83
00:06:44,410 --> 00:06:48,000
여러분이 할 수만 있다면 overfitting 되지 않도록 만들 수 있습니다.

84
00:06:48,000 --> 00:06:51,450
그러므로 일단 작동하는 10 개의 feature detector를 배워두면

85
00:06:51,450 --> 00:06:54,770
이걸 큰 이미지들에도 적용해 볼 수 있습니다.

86
00:06:54,770 --> 00:06:58,300
그리고 파라미터의 수는 여전히 고정되어 있으며 상대적으로 작습니다

87
00:06:58,300 --> 00:07:00,494
이 예시의 280처럼 말이죠.

88
00:07:00,494 --> 00:07:05,130
좋습니다, 이 강의를 마무리 지으며,

89
00:07:05,130 --> 00:07:09,766
컨볼루션 신경망에 있는 컨벌루션 레이어를 설명해주는

90
00:07:09,766 --> 00:07:11,980
하나의 레이어를 설명하기 위해 사용하는 표기법을 요약해보도록 합시다.

91
00:07:11,980 --> 00:07:14,302
따라서 레이어 l 는 콘볼루션 레이어입니다.

92
00:07:14,302 --> 00:07:18,555
필터 크기를 나타내기 위해 위 첨자 f를 사용하려고 합니다.

93
00:07:18,555 --> 00:07:23,219
이전 강의에서 우리는 필터가 fxf 인 것을 보았습니다.

94
00:07:23,219 --> 00:07:28,163
이 위 첨자 대괄호 안에 있는 [l]는

95
00:07:28,163 --> 00:07:31,074
fxf 필터 레이어 l 의 필터 사이즈임을 나타냅니다.

96
00:07:31,074 --> 00:07:35,767
위 첨자 대괄호 안에 있는 [l]은

97
00:07:35,767 --> 00:07:37,611
특정 레이어 l 을 가리키기 위해 사용하는 표기법 입니다.

98
00:07:39,812 --> 00:07:42,809
저는 이제 padding의 양을 낱아 내기 위해 p [l]을 사용할 것입니다.

99
00:07:42,809 --> 00:07:47,363
다시 말하자면, 패딩의 양은 유효한 컨볼루션 즉 패딩 하지 않겠다고 말하는 것만으로도

100
00:07:47,363 --> 00:07:50,135
구체화될 수 있습니다. 아니면

101
00:07:50,135 --> 00:07:53,240
혹은 똑같은 패딩을 선택 함으로서도 가능한데요, 이는 여러분이 패딩을 선택한다는 의미가 되는 거죠.

102
00:07:53,240 --> 00:07:57,910
그래서 아웃풋 크기는 인풋 크기와 동일한 높이와 너비를 갖습니다.

103
00:07:59,000 --> 00:08:01,590
그리고 나서 스트라이드를 나타내기 위해 s [l]을 사용하게 될 것입니다.

104
00:08:03,250 --> 00:08:09,450
자, 이 레이어에 대한 인풋은 어떤 dimension 될 것입니다.

105
00:08:09,450 --> 00:08:18,590
이것은 nxn x 이전 레이어에 있는 채널의 수가 되겠죠.

106
00:08:18,590 --> 00:08:21,162
자, 저는 이 표기법을 조금 수정하려고 합니다.

107
00:08:21,162 --> 00:08:25,385
저는 위 첨자 l- 1 을 사용하도록 하겠습니다.

108
00:08:25,385 --> 00:08:29,902
왜냐하면 이것은 이전 레이어에서 활성화 되었기 때문인데요,

109
00:08:29,902 --> 00:08:35,594
위 첨자 ㅣ-1 하고, 또한 nC에도 위 첨자 ㅣ-1 하면 되겠죠.

110
00:08:35,594 --> 00:08:40,966
지금까지 예제에서 우리는 단지 같은 높이와 너비의 이미지를 사용했습니다.

111
00:08:40,966 --> 00:08:43,990
높이와 너비가 다를 경우,

112
00:08:43,990 --> 00:08:48,528
이전 레이어 인풋의 높이와 넓이를 표현하기 위해

113
00:08:48,528 --> 00:08:51,949
위 첨자 h와 위 첨자 w를 사용하려고 합니다, 아시겠죠?

114
00:08:51,949 --> 00:08:56,418
따라서 레이어 l에서 볼륨의 크기는 nh x nw x nc이고

115
00:08:56,418 --> 00:09:01,134
위 첨자 대괄호 l 를 가지고 있습니다.

116
00:09:01,134 --> 00:09:05,597
그것은 그냥 레이어 l 에 있는 것이죠, 이 레이어에 대한 인풋은

117
00:09:05,597 --> 00:09:09,451
이전 레이어에 있던 게 어떤 것이던, 여기에 ㅣ-1 이 있는 겁니다.

118
00:09:09,451 --> 00:09:16,730
the 신경망의 이 레이어는 그 자체로 그 값을 산출해낼 것입니다.

119
00:09:16,730 --> 00:09:23,065
그러면 nh 의 ㅣ, nw 의 ㅣ, nc의 ㅣ

120
00:09:23,065 --> 00:09:28,495
이것이 아웃풋 사이즈가 됩니다.

121
00:09:28,495 --> 00:09:34,941
아웃풋 볼륨 사이즈, 아니면 적어도 높이나 무게가

122
00:09:34,941 --> 00:09:40,657
이 n + 2p- f / s + 1 수식에 의해 산출된다는 것을 인정하긴 하지만,

123
00:09:40,657 --> 00:09:47,971
그 값을 온전히 취해서 그걸 내림 하십시오.

124
00:09:47,971 --> 00:09:55,605
이 새로운 표기법에서, 레이어 l의 아웃풋 값은

125
00:09:55,605 --> 00:10:00,891
이전 레이어의 dimension이 될 것입니다.

126
00:10:00,891 --> 00:10:05,471
그리고 레이어l에서 사용하고 있는 패딩을 더해준 후,

127
00:10:05,471 --> 00:10:11,760
이 레이어 l 에서 사용하고 있는 필터사이즈를 빼면 됩니다.

128
00:10:11,760 --> 00:10:16,580
기술적으로 높이는 이렇게 하면 맞겠죠?

129
00:10:16,580 --> 00:10:21,510
따라서 아웃풋 볼륨의 높이는 이렇게 하면 주어집니다, 그리고

130
00:10:21,510 --> 00:10:24,680
오른쪽에 있는 이 수식으로 계산하면 됩니다. 그리고 넓이 또한 같은 방식으로 하면 됩니다.

131
00:10:24,680 --> 00:10:26,670
따라서 h를 지우고

132
00:10:26,670 --> 00:10:30,780
w를 넣어주세요, 그리고 나서 아웃풋 값의 높이나 넓이를 계산하기 위해서

133
00:10:30,780 --> 00:10:34,775
연관된 높이나 넓이 중에 하나로 같은 수식을 계산하세요.

134
00:10:36,570 --> 00:10:44,024
이렇게 해서 nh[l-1] 은 nh[l]과 연관시키고 w[l-1]은 nwl과 연관시키는 겁니다.

135
00:10:44,024 --> 00:10:48,105
이제, 채널의 숫자는 어떤가요? 이 숫자들은 어디서 나온걸 까요?

136
00:10:48,105 --> 00:10:52,784
살펴보시죠, 아웃풋 볼륨이 이 정도 depth를 가지고 있다면

137
00:10:52,784 --> 00:10:57,662
이전 예제들에서 본 것처럼

138
00:10:57,662 --> 00:11:02,167
그것은 그 레이어에 있는 필터의 수와 동일합니다.

139
00:11:02,167 --> 00:11:07,017
리는 두 개의 필터를 가지고 있는 것이죠, 아웃풋 값은 4x4x2, 2차원이었습니다.

140
00:11:07,017 --> 00:11:11,097
만약 10개의 필터를 가지고 있고 upper volume이 4x4x10 이라면,

141
00:11:11,097 --> 00:11:15,744
아웃풋 값에 있는 채널의 수는

142
00:11:15,744 --> 00:11:23,098
신경망의 이 레이어에서 사용하고 있는 필터의 수와 같습니다.

143
00:11:23,098 --> 00:11:26,849
다음으로, 이 필터의 사이즈는 어떤가요?

144
00:11:26,849 --> 00:11:33,057
각 필터는 f[l] x f[l] x 100개의 수일 텐데요,

145
00:11:33,057 --> 00:11:34,704
그럼 그 마지막 숫자는 뭘까요?

146
00:11:34,704 --> 00:11:39,465
6x6x3 이미지를

147
00:11:39,465 --> 00:11:41,580
3x3x3필터와 합성해야 한다고 배웠으니까

148
00:11:43,070 --> 00:11:48,150
여러분의 필터에 있는 채널의 수를

149
00:11:48,150 --> 00:11:54,360
인풋에 있는 채널 수와 매치를 시켜야만 합니다. 그래야 이 숫자가 저 숫자와 일치하게 되겠죠.

150
00:11:54,360 --> 00:12:02,627
이런 이유로 각각의 필터는 f[l] x f[l] x nc[n-1]이 됩니다.

151
00:12:02,627 --> 00:12:07,875
이 레이어의 아웃풋은 때로는 비선형성 있는 기기들을 적용해서

152
00:12:07,875 --> 00:12:11,745
이 레이어 a[l] 의 activation 이 됩니다.

153
00:12:11,745 --> 00:12:15,115
그리고 이것은 우리가 살펴보았던 이 dimension이 되겠죠.

154
00:12:15,115 --> 00:12:20,451
a[l] 은 3D 볼륨이 되고

155
00:12:20,451 --> 00:12:25,556
즉, nH[l] x nW[l] x nC[l] 이 되겠습니다.

156
00:12:25,556 --> 00:12:31,082
그리고 여러분이 vectorized implementation 또는 batch 기울기 강하

157
00:12:31,082 --> 00:12:36,891
또는 mini batch 기울기 강하을 사용하는 경우,

158
00:12:36,891 --> 00:12:41,387
m 예시를 가지고 있다면, m activation인 A[l]을 산출하게 됩니다.

159
00:12:41,387 --> 00:12:48,275
그러면 그건 M x nH][l] x nW[l] x nC[l] 이 되겠죠?

160
00:12:48,275 --> 00:12:51,375
프로그래밍 사이즈로서 batch 기울기 강하을 사용하고 계시다면,

161
00:12:51,375 --> 00:12:55,999
변수들의 배치가 될 것입니다.

162
00:12:55,999 --> 00:12:59,962
그리고 인덱스와 트레일 링 예제를 우선 가지고 있고

163
00:12:59,962 --> 00:13:02,384
이 세 개의 변수들이 있습니다.

164
00:13:02,384 --> 00:13:07,618
다음으로, 가중치 또는 파라미터 또는 w 파라미터는 어떻습니까?

165
00:13:07,618 --> 00:13:10,264
우린 이미 필터 dimension이 무엇인지 알아보았는데요,

166
00:13:10,264 --> 00:13:16,258
필터는 f[l] x f[l] x nc [l- 1] 이죠.

167
00:13:16,258 --> 00:13:20,230
하지만 이건 필터 하나의 dimension입니다.

168
00:13:20,230 --> 00:13:22,247
그럼 우리는 얼마나 많은 필터를 가지고 있을까요?

169
00:13:22,247 --> 00:13:25,097
글쎄요, 이것은 총 필터 수입니다.

170
00:13:25,097 --> 00:13:30,029
모든 필터의 가중치는 이 수식에 의해 산출되는 만큼의 dimension을 가지게 됩니다.

171
00:13:30,029 --> 00:13:33,513
이렇게 총 필터수를 곱한 만큼이 되겠죠?

172
00:13:33,513 --> 00:13:38,625
이렇기 때문에, 마지막 숫자는 레이어 l안에 있는

173
00:13:38,625 --> 00:13:43,750
필터의 수가 됩니다.

174
00:13:45,680 --> 00:13:48,710
마지막으로, 바이어스 파라미터가 있습니다.

175
00:13:48,710 --> 00:13:54,100
하나의 바이어스 파라미터 가지고 있는데요, 각각의 필터에 하나의 숫자가 있습니다.

176
00:13:54,100 --> 00:13:57,970
바이어스는 이 많은 변수들을 가지고 있죠.

177
00:13:57,970 --> 00:14:00,810
이건 그냥 이 dimension의 벡터 값일 뿐입니다.

178
00:14:00,810 --> 00:14:05,052
나중에 알게 되겠지만,

179
00:14:05,052 --> 00:14:09,813
4 차 행렬 또는 4 차 텐서에서는  (1, 1, 1, nC[1]) 로 표현되는 것이

180
00:14:09,813 --> 00:14:14,790
더 편리합니다.

181
00:14:16,430 --> 00:14:19,408
자, 표기법이 좀 많았다는 걸 저도 알고 있습니다.

182
00:14:19,408 --> 00:14:23,303
하지만 이게 많은 부분에서 사용하게 될 관례적인 것입니다.

183
00:14:23,303 --> 00:14:27,498
여러분이 온라인으로 검색하고 오픈 소스 코드를 살펴볼 때를

184
00:14:27,498 --> 00:14:32,311
완전히 보편적인 표준 규칙은 없다고 말씀 드리고 싶습니다.

185
00:14:32,311 --> 00:14:34,260
높이, 넓이, 그리고 채널을 정리하는데 있어서 말이죠.

186
00:14:34,260 --> 00:14:39,142
GitHub에서 혹은 open source implementations에서 소스코드를 찾아보신다면,

187
00:14:39,142 --> 00:14:43,873
일부 작성자들이 이 용어 규칙을 사용하고 있는걸 보실 수 있을 겁니다,

188
00:14:43,873 --> 00:14:48,631
채널을 먼저 넣는 변수들의 순서도 보게 될 것입니다.

189
00:14:48,631 --> 00:14:52,154
사실 일반적인 framework에서 , 실제로 다수의 일반적인 framework에서,

190
00:14:52,154 --> 00:14:54,155
변수 혹은 파라미터가 있습니다.

191
00:14:54,155 --> 00:14:57,652
여러분은 왜 채널의 수를 제일 먼저 나열하려고 하십니까?

192
00:14:57,652 --> 00:15:02,000
혹은 왜 이러한 볼륨에 색인을 생성할 때 채널 수를 마지막에 나열하려고 하시나요?

193
00:15:02,000 --> 00:15:08,137
제 생각에 이 두 방법 모두 일관성이 있는 한 괜찮습니다

194
00:15:08,137 --> 00:15:13,049
그리고 안타깝게도, 이것은 딥러닝 문헌에서 동의되지 않은

195
00:15:13,049 --> 00:15:17,772
부연설명 같은 것일지도 모르겠습니다.

196
00:15:17,772 --> 00:15:21,752
하지만 저는 이 강의에서 이 형식 사용할 것입니다.

197
00:15:24,681 --> 00:15:30,769
높이와 너비를 나열한 다음 채널 수가 마지막으로 나열됩니다

198
00:15:30,769 --> 00:15:34,327
여러분이 사용할 수 있는 새로운 표기법이 분명 많이 존재한다는 것을 알고 있습니다.

199
00:15:34,327 --> 00:15:38,027
하지만 여러분이 놀라고 있는 건, 긴 표기법일 겁니다. 이 모든걸 어떻게 다 기억할 수 있을까? 라고 생각하겠죠.

200
00:15:38,027 --> 00:15:41,994
걱정 마십시오, 이 모든 표기법을 기억하고 있을 필요는 없으니까요.

201
00:15:41,994 --> 00:15:46,036
이번 주의 예제들을 통해 여러분은 더 익숙해질 것입니다.

202
00:15:46,036 --> 00:15:49,116
하지만, 이 강의에서 여러분이 꼭 알았으면 하는 키포인트는 바로

203
00:15:49,116 --> 00:15:52,694
컨볼루션 신경망가 작동하는 방법의 단면입니다.

204
00:15:52,694 --> 00:15:57,006
그리고 한 레이어의 활성화를 시킬 수 있는 계산법과

205
00:15:57,006 --> 00:16:00,052
다음 레이어의 활성화를 위해 매핑하는 것을 알고 계셨으면 좋겠습니다.

206
00:16:00,052 --> 00:16:04,063
그리고 이제 여러분은 컨볼루션 신경망의 한 레이어가 어떻게 작동하는지 알게 되었습니다.

207
00:16:04,063 --> 00:16:07,740
이것들을 모두 합쳐서 실제로 더 깊은 컨볼루션 신경망을 만들어봅시다.

208
00:16:07,740 --> 00:16:09,040
이것들을 모두 합쳐서 실제로 더 깊은 컨볼루션 신경망을 만들어봅시다.

209
00:16:09,040 --> 00:16:10,200
그럼, 다음 강좌를 보러 가시죠.