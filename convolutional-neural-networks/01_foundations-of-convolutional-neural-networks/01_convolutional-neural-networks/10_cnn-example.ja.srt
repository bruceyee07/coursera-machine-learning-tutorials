1
00:00:00,021 --> 00:00:02,721
完全な畳み込みニューラルネットワークを作るための

2
00:00:02,721 --> 00:00:04,509
全構成要素が良く分かったでしょう

3
00:00:04,509 --> 00:00:07,336
例を見てみましょう。

4
00:00:07,336 --> 00:00:12,436
32 x 32 x 3 の画像を入力とする

5
00:00:12,436 --> 00:00:18,855
これは RGB画像だ
手書き数字認識をしようとしている

6
00:00:18,855 --> 00:00:24,396
32 x 32 画像には ７のような数がある

7
00:00:24,396 --> 00:00:30,495
０~９の数字の中から1つを識別しようとしている

8
00:00:30,495 --> 00:00:32,791
ニューラルネットワークでこれを行おう

9
00:00:32,791 --> 00:00:35,827
このスライドで使うのは

10
00:00:35,827 --> 00:00:41,106
クラシック ニューラルネットワークの1つ LeNet-5 とよく似た それから触発されたものだ

11
00:00:41,106 --> 00:00:43,942
それは Yann LeCun が何年も前に作った

12
00:00:43,942 --> 00:00:47,680
ここの見せるのは LeNet-5 そのものでは無いが

13
00:00:47,680 --> 00:00:53,024
それから触発されたもので 多くのパラメータ選択が そこから来ている

14
00:00:53,024 --> 00:00:58,524
32 x 32 x 3 入力に対し 最初の層だが

15
00:00:58,524 --> 00:01:04,770
5 x 5 フィルターで ストライド１ そしてパディング無し

16
00:01:04,770 --> 00:01:08,240
よって この層の出力は

17
00:01:08,240 --> 00:01:13,732
もし ６個のフィルターを使うなら 28 x 28 x 6

18
00:01:13,732 --> 00:01:18,803
この層を "Conv 1" と呼ぼう

19
00:01:18,803 --> 00:01:23,660
６フィルターを適用し バイアスを加え 非線形化する

20
00:01:23,660 --> 00:01:28,356
ReLU 非線形だろう そして これが Conv1の出力だ

21
00:01:28,356 --> 00:01:32,678
次に プーリング層を適用しよう

22
00:01:32,678 --> 00:01:40,280
最大プーリングを適用しよう f = 2, s = 2 にする

23
00:01:40,280 --> 00:01:44,101
パディングを書かない場合は パディングは０だ

24
00:01:44,101 --> 00:01:48,895
次に プーリング層を適用しよう

25
00:01:48,895 --> 00:01:54,975
最大プーリングを適用しよう 2 x 2 フィルターでストライドは２

26
00:01:54,975 --> 00:01:57,064
これは 表現の高さと幅を

27
00:01:57,064 --> 00:01:59,614
因数２で減らす

28
00:01:59,614 --> 00:02:04,138
よって 28 x 28 は 14 x 14 となる

29
00:02:04,138 --> 00:02:10,472
チャンネル数はそのままだから 14 x 14 x 6

30
00:02:10,472 --> 00:02:15,536
これを "Pool 1" 出力と呼ぼう

31
00:02:15,536 --> 00:02:20,111
ConvNet の文献では

32
00:02:20,111 --> 00:02:25,562
２つの表現があって それは層と呼ぶものについてだが 少しだけ一致しない点がある

33
00:02:25,562 --> 00:02:30,918
1つの表現では これを層と呼ぶ

34
00:02:30,918 --> 00:02:35,900
よって これが ニューラルネットワークの "Layer 1" だ

35
00:02:35,900 --> 00:02:40,980
もう1つの表現では 畳み込み層で１層 プール層で１層と数える

36
00:02:40,980 --> 00:02:45,223
人々がニューラルネットワークの層数を報告する場合 通常は

37
00:02:45,223 --> 00:02:49,025
重みやパラメータを持っている層の数を上げる

38
00:02:49,025 --> 00:02:53,043
そして プーリング層は 重みもパラメータも持たず

39
00:02:53,043 --> 00:02:57,418
ただ若干のハイパーパラメータしか持たないので ここでは

40
00:02:57,418 --> 00:02:59,015
"Conv 1" と "Pool 1" を一緒にする表記法を使おう

41
00:02:59,015 --> 00:03:03,551
よって これらを "Layer 1" として扱う
ただし 時々

42
00:03:03,551 --> 00:03:08,447
オンラインで文献を読んだり 研究報告を読んだりした場合 

43
00:03:08,447 --> 00:03:11,703
畳み込み層とプーリング層を別々の層のように言っている場合もある

44
00:03:11,703 --> 00:03:16,788
これが ２つの表記用語における 若干の不一致点だ

45
00:03:16,788 --> 00:03:22,053
しかし 私は 層を数える時 重みを持つ層を数える

46
00:03:22,053 --> 00:03:25,614
よって この２つは一緒にして "Layer 1" となる

47
00:03:25,614 --> 00:03:30,822
そして 名前の "Conv 1" と "Pool 1" の最後にある１は

48
00:03:30,822 --> 00:03:37,961
両方とも ニューラルネットワークの "Layer 1" を参照している

49
00:03:37,961 --> 00:03:42,665
そして "Pool 1" は "Layer 1" に組み込まれる
それ自身は重みを持たないからだ

50
00:03:42,665 --> 00:03:47,585
次に 14 x 14 x 6 ボリュームを

51
00:03:47,585 --> 00:03:53,181
別の畳み込み層に適用させよう
フィルターサイズは 5 x 5

52
00:03:53,181 --> 00:03:58,796
ストライドは１ そして 今度は10個のフィルターにする

53
00:03:58,796 --> 00:04:04,350
それで 結局

54
00:04:04,350 --> 00:04:09,786
10 x 10 x 10 ボリュームとなる
これを "Conv 2"と呼ぼう

55
00:04:09,786 --> 00:04:14,467
それから 最大プーリングを 再び

56
00:04:14,467 --> 00:04:19,008
f = 2, s = 2 で行う

57
00:04:19,008 --> 00:04:23,456
多分 当たりを付けているだろうけど この出力は f = 2, s = 2 で

58
00:04:23,456 --> 00:04:26,769
これは 高さと幅を 因数２で減らし

59
00:04:26,769 --> 00:04:31,425
5 x 5 x 10 が残る

60
00:04:31,425 --> 00:04:34,773
それでは これを "Pool 2" と呼ぼう

61
00:04:34,773 --> 00:04:39,652
我々の表記規約では これは ニューラルネットワークの "Layer 2" だ

62
00:04:39,652 --> 00:04:42,293
では これを別の畳み込み層に適用させよう

63
00:04:42,293 --> 00:04:47,113
フィルターサイズは 5 x 5 つまり f = 5

64
00:04:47,113 --> 00:04:51,962
ストライドは１ パディングは書かない これはパディング無しのこと

65
00:04:51,962 --> 00:04:58,254
これで "Conv 2" 出力が得られる
そして 16個のフィルターを使おう

66
00:04:58,254 --> 00:05:03,860
そうすると 10 x 10 x 16 次元の出力

67
00:05:03,860 --> 00:05:10,380
こうなるね これが "Conv 2" 層だ

68
00:05:10,380 --> 00:05:17,356
それから 最大プーリングを適用しよう f = 2, s = 2 で

69
00:05:17,356 --> 00:05:20,227
多分 分かるだろうけど この出力は

70
00:05:20,227 --> 00:05:24,555
10 x 10 x 16
f = 2, s = 2 の最大プーリングだと

71
00:05:24,555 --> 00:05:26,667
これは 高さと幅を半分にする

72
00:05:26,667 --> 00:05:31,075
結果が分かるでしょ？

73
00:05:31,075 --> 00:05:32,463
f = 2, s = 2 で最大プーリングする

74
00:05:32,463 --> 00:05:37,663
これは 高さと幅を半分にする それで 結局

75
00:05:37,663 --> 00:05:43,214
5 x 5 x 16 ボリュームを得る チャンネル数は前と同じ

76
00:05:43,214 --> 00:05:47,166
これを "Pool 2" と呼ぼう

77
00:05:47,166 --> 00:05:52,429
我々の表記規約では "Layer 2" だ

78
00:05:52,429 --> 00:05:57,203
なぜなら これは "Conv 2" 層に重みセットを持つからだ

79
00:05:57,203 --> 00:06:03,306
さぁ 5 x 5 x 16 だ
5 x 5 x 16 = 400

80
00:06:03,306 --> 00:06:10,895
それじゃ "Pool 2" を 400 x 1 次元ベクトルに平坦化しよう

81
00:06:10,895 --> 00:06:16,686
これを平坦化すると こんな感じのニューロンのセットになる

82
00:06:16,686 --> 00:06:22,373
それから やることは この400ユニットを取ってきて

83
00:06:22,373 --> 00:06:30,070
次のの層を構築する 120ユニットを持っている

84
00:06:30,070 --> 00:06:33,243
そう これが最初の全結合層だ

85
00:06:33,243 --> 00:06:38,392
これを "FC3" と呼ぼう なぜなら

86
00:06:38,392 --> 00:06:44,410
400ユニットが 120ユニットと密に結合しているからだ

87
00:06:46,245 --> 00:06:51,628
この 全結合ユニット 全結合層は

88
00:06:51,628 --> 00:06:56,660
コース１と２で見た 単なるニューラルネットワーク層だ

89
00:06:56,660 --> 00:07:01,710
これは標準的なニューラルネットワークで そこには重み行列がある

90
00:07:01,710 --> 00:07:08,044
それを"W3"と呼ぼう その次元数は 120 x 400 だ

91
00:07:08,044 --> 00:07:13,406
これは全結合層と呼ばれる なぜなら ここの400ユニットのそれぞれが
ここの120ユニットのそれぞれに結合しているからだ

92
00:07:13,406 --> 00:07:18,354
そして バイアスパラメータもある

93
00:07:18,354 --> 00:07:23,655
そう ちょうど120次元のがあり 120個の出力となる

94
00:07:23,655 --> 00:07:28,715
それから最後に 120ユニットを取り 別の層を加える

95
00:07:28,715 --> 00:07:33,119
今度はすこ小さめで 84ユニットある

96
00:07:33,119 --> 00:07:36,883
これを 全結合層４と呼ぼう

97
00:07:36,883 --> 00:07:44,435
最終的に84個の実数を得た
これをソフトマックスユニットに喰わせることができる

98
00:07:44,435 --> 00:07:48,215
そして 手書き数字認識をしようとしているのなら

99
00:07:48,215 --> 00:07:51,794
これは 0 1 2 と続いて 9 までになる

100
00:07:51,794 --> 00:07:56,680
よって これは 10個の出力を行うソフトマックスになる

101
00:07:56,680 --> 00:08:00,969
そう これが 畳み込みニューラルネットワークが

102
00:08:00,969 --> 00:08:05,482
どのようになるかの 妥当で典型的な例だ

103
00:08:05,482 --> 00:08:09,367
非常に沢山のハイパーパラメータがあるように見えるかもしれない

104
00:08:09,367 --> 00:08:12,919
後で 具体的な提言をするよ

105
00:08:12,919 --> 00:08:15,882
どのように これらのハイパーパラメータを選ぶのかについて

106
00:08:15,882 --> 00:08:20,388
1つの一般的ガイドラインは

107
00:08:20,388 --> 00:08:22,802
自分独自のハイパーパラメータの設定をしようとせずに

108
00:08:22,802 --> 00:08:27,887
文献を見て どんなハイパーパラメータに自分が取り組んでいるのか見てみることだ

109
00:08:27,887 --> 00:08:30,963
そして 別の誰かが上手く行った構造を選べば

110
00:08:30,963 --> 00:08:35,316
それは あなたのアプリケーションでも上手く行く可能性がある

111
00:08:35,316 --> 00:08:38,321
次週 その事について もっと話すよ

112
00:08:38,321 --> 00:08:43,715
でも今は 単に次の点を指摘しておきたい
ニューラルネットワークが深くなるにつれ

113
00:08:43,715 --> 00:08:47,493
通常 nH nW そして 高さと幅は減っていく

114
00:08:47,493 --> 00:08:52,432
前に指摘したよね
それは 32 x 32 が 20 x 20, 14 x 14, 10 x 10,

115
00:08:52,432 --> 00:08:53,934
5 x 5 になる

116
00:08:53,934 --> 00:08:57,870
より深くなれば 通常 高さと幅は減っていく

117
00:08:57,870 --> 00:09:00,852
一方 チャンネル数は増えていく

118
00:09:00,852 --> 00:09:07,277
3, 6, 16 と増えていく
そして 最後に全結合層に行く

119
00:09:07,277 --> 00:09:13,135
ニューラルネットワークにおける 別の かなり一般的なパターンに
畳み込み層が

120
00:09:13,135 --> 00:09:17,426
１つ以上の畳み込み層があり 1つのプーリング層が続き そして

121
00:09:17,426 --> 00:09:21,329
１つ以上の畳み込み層があり 1つのプーリング層が続く

122
00:09:21,329 --> 00:09:24,731
そして 最後に いくつかの全結合層が来て

123
00:09:24,731 --> 00:09:26,756
ソフトマックスが続く というものがある

124
00:09:26,756 --> 00:09:32,378
これが ニューラルネットワークにとてもよく見られる別のパターンだ

125
00:09:32,378 --> 00:09:33,956
それでは このニューラルネットワークについて

126
00:09:33,956 --> 00:09:37,968
もう少し詳細を見てみよう
活性化の shape や

127
00:09:37,968 --> 00:09:41,799
活性化のサイズ そして ネットワークのパラメータ数がどうなるのか

128
00:09:41,799 --> 00:09:44,181
入力は 32 x 30 x 3

129
00:09:44,181 --> 00:09:48,324
これらをかけ合わせれば 3,072を得る

130
00:09:48,324 --> 00:09:54,313
よって 活性化 a[0] は 3,072次元だ

131
00:09:54,313 --> 00:09:58,005
本当は 32 x 32 x 3

132
00:09:58,005 --> 00:10:02,562
入力層には何のパラメータも無い

133
00:10:02,562 --> 00:10:05,672
そして 別の層を見れば

134
00:10:05,672 --> 00:10:09,068
詳細は自分で勝手に計算してね

135
00:10:09,068 --> 00:10:10,975
これらが活性化の shape で

136
00:10:10,975 --> 00:10:13,743
そして 活性化のサイズだ

137
00:10:15,422 --> 00:10:16,957
何点か指摘しておく

138
00:10:16,957 --> 00:10:23,352
最初に プーリング層 最大プーリング層は 何のパラメータも持たない

139
00:10:23,352 --> 00:10:28,202
２番目に 畳み込み層は 比較的 少ない数のパラメータを持つ傾向がある

140
00:10:28,202 --> 00:10:32,302
前のビデオで議論したようにね

141
00:10:32,302 --> 00:10:36,414
そして実際 パラメータの多くは ニューラルネットワークの

142
00:10:36,414 --> 00:10:39,426
全結合層にある傾向がある

143
00:10:39,426 --> 00:10:44,584
それから 活性化のサイズは ニューラルネットワークが深くなるにつれ

144
00:10:44,584 --> 00:10:50,289
だんだんと少なくなるのに気付いただろう

145
00:10:50,289 --> 00:10:55,198
もし これが急に下がる場合は 通常あまり良い性能にならない

146
00:10:55,198 --> 00:11:00,349
それは 最初 6,000 で 1,5 1,600 そして

147
00:11:00,349 --> 00:11:06,405
ゆっくり 84に落ちていき
最後にソフトマックス出力となる

148
00:11:06,405 --> 00:11:10,683
多くの ConvNet が傾向を持っている

149
00:11:10,683 --> 00:11:13,293
これと似たパターンを持っている

150
00:11:13,293 --> 00:11:16,455
さぁ ニューラルネットワーク 畳み込みニューラルネットワークの

151
00:11:16,455 --> 00:11:20,068
基本構成要素を見てきた
畳み込み層 プーリング層

152
00:11:20,068 --> 00:11:21,601
そして 全結合層

153
00:11:21,601 --> 00:11:25,693
多くのコンピュータ ビジョン研究が明らかにしてきた
どのように

154
00:11:25,693 --> 00:11:29,078
これら基本構成要素をまとめ 効力のあるニューラルネットワークを作るかを

155
00:11:29,078 --> 00:11:33,379
そして これらを纏めて 非常に多くの洞察を得てきた

156
00:11:33,379 --> 00:11:35,213
思うに

157
00:11:35,213 --> 00:11:39,323
どのようにこれらを纏めるのか洞察を得るのに一番良い方法は

158
00:11:39,323 --> 00:11:41,804
他の人が行った具体例を数多く見ることだ

159
00:11:41,804 --> 00:11:46,268
それで 次週行うことは この最初のを超える具体例をいくつか 見ることだ

160
00:11:46,268 --> 00:11:50,183
そこでは 人々はうまくこれらを纏めて

161
00:11:50,183 --> 00:11:53,637
効果的なニューラルネットワークを作成するのに成功している

162
00:11:53,637 --> 00:11:58,532
そして 次週のビデオでは あなたが これらをどうやって作るかについて

163
00:11:58,532 --> 00:12:00,098
自分自身の洞察を持つようになることを期待する

164
00:12:00,098 --> 00:12:05,068
具体的な例・構造があるれば 単にそれを利用できるし

165
00:12:05,068 --> 00:12:09,120
他の誰かが作ったものをそのまま自身のアプリケーションに使うこともできる

166
00:12:09,120 --> 00:12:10,971
それは 次週行おう

167
00:12:10,971 --> 00:12:15,499
今週のビデオを纏める前に
最後に1つ言っておくことが有る

168
00:12:15,499 --> 00:12:19,840
それは なぜ畳み込みを使いたいのだろうか ということ

169
00:12:19,840 --> 00:12:20,869
畳み込みを使うことの

170
00:12:20,869 --> 00:12:25,133
何らかの利点や強み
また どうやって全体を纏めるのか ということだ

171
00:12:25,133 --> 00:12:29,021
ここで見たようなニューラルネットワークを どのようにして得たり
学習データを使って どのように実際に学習するのか

172
00:12:29,021 --> 00:12:32,735
画像認識やその手のタスクを実現するのに

173
00:12:32,735 --> 00:12:35,700
それでは 今週の最後のビデオに進もう