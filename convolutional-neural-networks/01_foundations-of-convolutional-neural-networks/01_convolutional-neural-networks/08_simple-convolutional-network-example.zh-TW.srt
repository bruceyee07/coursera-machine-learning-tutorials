1
00:00:00,252 --> 00:00:04,029
前一段影片中， 您見過了一層的

2
00:00:04,029 --> 00:00:06,721
單一卷積層的卷積網路的建構基石

3
00:00:06,721 --> 00:00:12,339
現在我們來談一個
實際的深度卷積神經網路例子

4
00:00:12,339 --> 00:00:15,876
這會給您一些有關於我們在

5
00:00:15,876 --> 00:00:17,373
上一段影片最後介紹的記號的練習

6
00:00:19,648 --> 00:00:22,203
假設您有一個影像

7
00:00:22,203 --> 00:00:26,959
您想要做影像分類，或者影像識別

8
00:00:26,959 --> 00:00:31,745
也就是您想拿這個影像當輸入 Ｘ，決定它是不是貓

9
00:00:31,745 --> 00:00:34,254
0 或者 1, 所以這是一個分類問題

10
00:00:34,254 --> 00:00:38,626
讓我們來建立一個卷積網路的例子，
讓您可以用在這個任務上

11
00:00:38,626 --> 00:00:42,943
為了這個例子，
我將使用一個很小的影像

12
00:00:42,943 --> 00:00:48,499
假設這個影像是 39乘39乘3

13
00:00:48,499 --> 00:00:51,529
這些數字的選擇
只是要讓這個例子作用好一些

14
00:00:51,529 --> 00:00:57,470
所以，nh 在 0 層會是等於 nw 高度

15
00:00:57,470 --> 00:01:00,581
跟寬度都是 39

16
00:01:00,581 --> 00:01:06,532
而在第0層通道的數目會是 3

17
00:01:06,532 --> 00:01:11,906
假設在第一層使用一組 3乘3 過濾器

18
00:01:11,906 --> 00:01:16,924
來偵測特徵，所以 f = 3 或者實際上是 f1 = 3

19
00:01:16,924 --> 00:01:20,992
因為我們用了 3乘3 過濾器

20
00:01:20,992 --> 00:01:26,871
而且假設我們使用 1 的跨步，沒有填充

21
00:01:26,871 --> 00:01:32,641
也就是使用相同的卷積，
假設您用了 10 個過濾器

22
00:01:34,632 --> 00:01:38,779
那這個神經網路下一層的啟動值會是

23
00:01:38,779 --> 00:01:43,755
37乘37乘10

24
00:01:43,755 --> 00:01:49,335
而這個 10 是因為您用了 10 個過濾器

25
00:01:49,335 --> 00:01:54,735
而 37 來自於這個公式

26
00:01:54,735 --> 00:01:58,739
n + 2p - f 除以 s + 1

27
00:01:58,739 --> 00:02:03,919
我想您會有 39

28
00:02:03,919 --> 00:02:10,401
+ 0 - 3 除以 1 ＋ 1 所以是 37

29
00:02:10,401 --> 00:02:15,006
這是為什麼輸出是 37乘37, 它是有效卷積

30
00:02:15,006 --> 00:02:17,590
而這是輸出維度

31
00:02:17,590 --> 00:02:25,029
在我們的記號中，我們會是 nh[1] = nw[1] = 37 

32
00:02:25,029 --> 00:02:30,126
nc[1] = 10, 所以 nc[1] 會等於

33
00:02:30,126 --> 00:02:36,240
從第一層的過濾器數目

34
00:02:36,240 --> 00:02:42,040
所以這會變成是第一層啟動值的維度

35
00:02:43,300 --> 00:02:45,980
假設您有另一個卷積層

36
00:02:45,980 --> 00:02:48,900
假設這一層您使用 5乘5 過濾器

37
00:02:48,900 --> 00:02:54,996
所以在下一層神經網路使用我們的記號 f[2] = 5

38
00:02:54,996 --> 00:02:59,231
假設這次我們使用 2 的跨步

39
00:02:59,231 --> 00:03:03,922
或許您沒用填充

40
00:03:03,922 --> 00:03:07,060
假設用了 20 個過濾器

41
00:03:09,370 --> 00:03:15,933
這個輸出會是另一個容積

42
00:03:15,933 --> 00:03:20,946
這一次會是 17乘17乘20

43
00:03:20,946 --> 00:03:23,866
注意到，因為這次您使用跨步 2

44
00:03:23,866 --> 00:03:25,926
維度收縮得很快

45
00:03:25,926 --> 00:03:32,800
37乘37 縮小為比一半大一點的 17乘17

46
00:03:32,800 --> 00:03:37,554
而因為您使用了 20 個過濾器，通道的數目會是 20

47
00:03:37,554 --> 00:03:42,167
所以這是啟動值 a2

48
00:03:42,167 --> 00:03:46,971
會是這個維度，所以

49
00:03:46,971 --> 00:03:52,160
nh[2] = nw[2] = 17 而

50
00:03:52,160 --> 00:03:55,247
nc[2] = 20

51
00:03:55,247 --> 00:03:58,180
好的，我們用上最後一個卷積層

52
00:03:58,180 --> 00:04:04,071
假設您再次使用 5乘5 過濾器

53
00:04:04,071 --> 00:04:07,390
再次使用跨步 2

54
00:04:07,390 --> 00:04:13,681
如果這樣做，我省略掉計算的部分，您最後會得到  7乘7

55
00:04:13,681 --> 00:04:19,251
假設您使用 40 個過濾器，不使用填充，40 個過濾器

56
00:04:19,251 --> 00:04:22,760
最後您會得到  7乘7乘40

57
00:04:22,760 --> 00:04:27,860
所以現在您做到的是用您的 39乘39乘3 輸入影像

58
00:04:29,380 --> 00:04:34,810
計算為 7乘7乘40 特徵的影像

59
00:04:34,810 --> 00:04:41,075
然後最後通常會作的是如果您拿這個 7乘7乘40

60
00:04:41,075 --> 00:04:45,137
7乘7乘40 實際上是 1,960

61
00:04:45,137 --> 00:04:50,888
我們要做的是我們可以拿這個容積，鋪平它

62
00:04:50,888 --> 00:04:55,901
或者攤開它變成是 1,960 個單元，是吧

63
00:04:55,901 --> 00:04:59,347
把它攤平成為一個向量

64
00:04:59,347 --> 00:05:05,283
然後餵進一個羅吉斯迴歸單元，
或者一個 softmax 單元

65
00:05:07,917 --> 00:05:11,682
根據您是否是試著去判別貓或者不是貓

66
00:05:11,682 --> 00:05:15,150
或者判別不同物件之一然後

67
00:05:15,150 --> 00:05:19,900
只給神經網路最後預測輸出

68
00:05:20,925 --> 00:05:26,520
在講清楚一點，最後的這個步驟是拿所以這些數字

69
00:05:26,520 --> 00:05:32,222
所有 1,960 個數字，攤平他們成一個很長的向量

70
00:05:32,222 --> 00:05:36,483
然後您只是拿這個長長的向量，
您可以餵進到 softmax 單元直到

71
00:05:36,483 --> 00:05:39,770
迴歸到最終輸出的預測

72
00:05:41,600 --> 00:05:46,125
所以這會是一個相當典型的 ConvNet 例子

73
00:05:47,380 --> 00:05:51,187
很多設計卷積神經網路的工作在於

74
00:05:51,187 --> 00:05:54,880
選擇超參數像這樣，決定總共大小多少？

75
00:05:54,880 --> 00:05:55,840
跨步是多少？

76
00:05:55,840 --> 00:05:58,860
填充是多少？用多少個過濾器？

77
00:06:00,190 --> 00:06:03,980
而這個禮拜後半段跟下個星期，我們會給一些建議

78
00:06:03,980 --> 00:06:07,440
跟原則來看如何做這些選擇

79
00:06:07,440 --> 00:06:12,510
但現在，這裡或許最重要的是當您

80
00:06:12,510 --> 00:06:17,950
進入到深度神經網路，
典型的是您開始用的影像大於 39乘39

81
00:06:17,950 --> 00:06:21,202
然後保持高度跟寬度都一樣

82
00:06:21,202 --> 00:06:25,859
逐漸減少，當您進入更深度的神經網路

83
00:06:25,859 --> 00:06:29,663
像是從 39 到 37 到 17 到 14

84
00:06:29,663 --> 00:06:33,961
不好意思，是從 39 到 37 到 17 到 7

85
00:06:33,961 --> 00:06:36,753
而通道的數目一般而言是增加的

86
00:06:36,753 --> 00:06:41,412
它是從 3 到 10 到 20 到 40，您會看到這種

87
00:06:41,412 --> 00:06:45,930
趨勢在很多其他的卷積神經網路上

88
00:06:47,060 --> 00:06:52,576
我們將會得到更多的原則有關於
如何去設計這些參數在未來的影片中

89
00:06:52,576 --> 00:06:57,196
但您現在已經看到您第一個卷積神經網路的例子

90
00:06:57,196 --> 00:06:59,210
簡稱為 ConvNet

91
00:06:59,210 --> 00:07:00,770
恭喜您

92
00:07:02,050 --> 00:07:05,500
實際上在典型的 ConvNet （卷積神經網路）中

93
00:07:05,500 --> 00:07:07,870
通常有三種層的類型

94
00:07:07,870 --> 00:07:13,615
一種是卷積層，通常記為一個 Conv 層

95
00:07:13,615 --> 00:07:17,025
也就是我們之前用到的網路

96
00:07:17,025 --> 00:07:20,893
實際上還有兩種類型
常見類別的層您還未見過

97
00:07:20,893 --> 00:07:23,945
但我們會在下幾個影片中談到

98
00:07:23,945 --> 00:07:28,272
一種是稱為池層，通常我稱之為 池 (pool)

99
00:07:28,272 --> 00:07:32,241
最後一種是全連結層，稱為 FC 

100
00:07:32,241 --> 00:07:36,466
雖然設計一個很棒的神經網路只用到

101
00:07:36,466 --> 00:07:41,278
卷積層是可行的，但大部分神經網路架構
還是會有一些池層

102
00:07:41,278 --> 00:07:43,569
跟一些全連結層

103
00:07:46,398 --> 00:07:48,103
好在池層跟

104
00:07:48,103 --> 00:07:52,340
全連結層比卷積層的定義簡單一點

105
00:07:54,150 --> 00:07:58,472
所以我們會很快的在下兩段影片中談到，
您到時會有一些感覺

106
00:07:58,472 --> 00:08:03,173
對於所有這些常用在卷積神經網路類型的層

107
00:08:03,173 --> 00:08:06,725
而您會將這些放在一起變成更強大的神經網路

108
00:08:06,725 --> 00:08:07,290
比起我們剛剛看到的

109
00:08:08,990 --> 00:08:14,110
再次恭喜您見到了第一個完全的卷積神經網路

110
00:08:14,110 --> 00:08:18,450
我們在這個禮拜也會談到如何訓練這些網路，但

111
00:08:18,450 --> 00:08:22,180
首先讓我們先簡單談一下池層跟全連結層

112
00:08:22,180 --> 00:08:24,659
然後訓練這些，我們會用到反向傳播

113
00:08:24,659 --> 00:08:26,241
您應該已經很熟悉

114
00:08:26,241 --> 00:08:30,421
但在下一段影片，讓我們很快的談一下如何建置池層

115
00:08:30,421 --> 00:08:31,230
在您的 ConvNet