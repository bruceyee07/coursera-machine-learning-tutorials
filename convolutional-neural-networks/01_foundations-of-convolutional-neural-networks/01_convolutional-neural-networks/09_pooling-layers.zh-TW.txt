除了卷積層外 ConvNets 通常也使用池層來減少大小 加快計算的速度 同時也產生一些特徵讓偵測更加堅固 我們來看看，我們用一個例子來看池層 然後我們再來談為什麼要這樣做 假設您有一個 4乘4  輸入 而您想要應用一種稱為最大池化的池層類型 而輸出 在這特別的最大池化建置中會是 2乘2 輸出 而您要做的其實很簡單 拿您的 4乘4 輸入將其分解為 不同的區域，我將用不同的顏色來表示這四個區域 而在輸出 也就是 2乘2 每一個輸出就只是相對應區域的最大值 所以在左上角 這四個數字的最大值是  9 在右上角，藍色部分最大值是 2 左下角，最大的數字是 6 右下角，最大的數字是   3 所以要計算在右邊的每個數字 我們取 2乘2 區域的最大值 這個就像是您用了一個大小為 2 的過濾器 因為您用了一個 2乘2 區域，然後您跨步為 2 所以，實際上這些就是 最大池化的超參數，
因為我們開始用這的過濾器大小 像這個 2乘2 的區域，給您  9 然後您跨過兩步，到這個區域，這給您 2 接著下一行 您往下走兩步，給您  6 然後您往右走兩步，給您 3 因為方塊是  2乘2, f 等於 2 因為您每次跨兩步 s 等於  2 最大池化背後的一些直觀是 如果您想像這個 4乘4 區域是一組特徵 在神經網路的一些層的啟動值 那一個大的數字 它意思是或許偵測一種特別的特徵 所以在左上角的這個象限有這個特別的特徵 它或許是一個垂直邊緣，或許是眼睛 ,
也許是鬍鬚，如果您試著偵測貓 很明顯的，這個特徵出現在左上角象限 而這個特徵，或許是貓眼的偵測 而這個特徵，並不真的存在於右上角象限 所以最大化運算做的是
偵測任何地方很多的特徵 而任何一個象限，它保有了最大池的輸出 所以最大化運算真正說的是 如果這個特徵在這個過濾器中任何地方偵測到 那就保持最高的數值 但如果這個特徵並沒被偵測到 或許這個特徵並不存在於右上角象限 那即使是所有這些數字的最大值，還是很小 或許這就是最大池化背後的直觀 但我必須承認 我想人們使用最大池化的最主要原因是 人們從很多的實驗中發現到它作用得很好 而我剛剛所說的直觀 除了經常這樣被引用 我不認為任何人完全了解背後真正的道理 我不認識任何人知道是否那就是 最大池化在 ConvNets  中
作用的很好的背後原因 最大池化終有一個有趣的特性也就是 它有一組超參數，但沒有參數需要學習 實際上梯度下降並沒有東西可以學習 一旦您固定 f 跟 s 它就是一個固定的運算，
而梯度下降不用改變任何東西 我們來進行一個使用不同超參數的例子 這裡我將使用一個 5乘5 的輸入 而我們將應用最大池化使用一個 3乘3 的過濾器 所以 f 等於 3, 而跨步則為 1 這種設定下的輸出會是 3乘3 而我們在之前投影片中使用的公式 來計算卷積層的輸出大小 這個公式在最大池化一樣適用 所以，那是 n 加 2p 減去 f 除以 s 加 1 這個公式也適用於計算最大池化的輸出大小 在這個例子中，我們來計算這個 3乘3 輸出的每個元素 在最左上角 我們將看這個區域 注意到這是一個 3乘3 區域 因為過濾器的大小是 3, 而最大值在這裡 所以會是 9 然後我們向左跨一步，因為我們的跨步是 1 所以在這個藍色框框中最大值是 9 我們再次向左移一步 這個藍色框框最大值是 5 然後我們往下一行走，跨步是 1 所以我們只往下跨一步 所以這個區域的最大值是 9, 這個區域的最大值是 9, 這個區域的最大值是 現在有兩個 5, 最大值是 5 而最後，這個區域的最大值是 8 這個區域的最大值是 6 這個區域的最大值是 9 好的，所以使用這組超參數 f 等於 3 s 等於 1 給我們這個輸出 到目前為止，我顯示的最大池化都是在 2D 輸入 如果您有一個 3D 輸入 那輸出會是一樣的維度 所以舉個例子，如果您有一個 5乘5乘2 而輸出會是 3乘3乘2, 而計算 最大池化就是您計算 我們剛剛描述的方式單獨在每一個通道上 所以顯示在最上面的第一個通道還是一樣 而第二個通道，我想 我畫的是在底部 您會用同樣的運算在這一片上 而那個值會給您第二片 而一般而言，如果這是一個 5乘5乘 一個數字的通道 輸出會是 3乘3乘 同樣那個數字的通道數 而最大池化計算是在每一個通道
 總共Nc通道 單獨計算 所以，這就是最大池化 還有一種型態的池化不太常使用 但我大略提一下，也就是平均池化 它大概是您所期望的 與其使用在每個過濾器中取最大值 您取平均值 在這個例子中 在紫色區域的平均是 3.75 然後這個是 1.25 然後是 4, 跟 2  所以這是平均池化使用超參數為 f 等於 2 s 等於 2, 我們也可以選擇其他超參數 時至今日，最大池化使用上遠大於 平均池化，但有一個例外 就是有時候在很深的神經網路上 您或許使用平均池化來壓縮，舉例  7乘7乘1000 平均所有的值 您得到 1乘1乘1000 以後我們會見到這個例子 但，最大池化在神經網路中使用遠大於平均池化 總結一下 池化的超參數是 f 過濾器的大小， s 跨步的大小 或許最常見的參數選擇或許是 f 等於 2, s 等於 2 這個相當常用，而這個個效用是 大約將常跟寬縮成一半 最常見的超參數選擇是，f 等於 2, s 等於 2 這個的效果會是 將長跟寬縮為一半 我也看過有人用 f  等於 3, s 等於 2 而其他超參數的選擇會像是二進位 您要使用最大池化或是平均池化 您要的話，您可以加一個額外的超參數 用來填充，雖然這種做法非常非常少 當您使用最大池化時，通常 您不使用填充 雖然我們在這個星期會看到一個例外 但對於大部分的最大池化 通常不使用填充 最常用的 p 值目前都是 0 而最大池化的輸入容積大小是 nh乘nw乘nc 而這個會輸出的大小是這個 假設不用填充，乘 nw 減 f 除以 s 加 1 乘 nc 輸入通道的數目等於輸出通道的數目 因為池化單獨應用到每個通道上 有關池化的一個重點是，沒有要學習的參數 所以當我們實行反向傳播時 在最大池化時，您會發現沒有參數在反向傳播中需要導入 相反地，這些超參數您只要設定一次 或許手工設定一次，或者使用交叉驗證設定 設定完成就完成 那只是一個固定函數，神經網路在一層中計算 實際上並沒有要學習的東西 它就只是一個固定函數 所以這就是池化 您現在知道如何建置卷積層和池層 在下一個影片中 我們來看一個複雜的 ConvNet 例子 一個讓我們能夠介紹全連結層