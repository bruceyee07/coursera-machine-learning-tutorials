이제 컨볼루션 신경망의 한 레이어를 어떻게 만들 수 있을지 알아보도록 하겠습니다. 예제를 살펴 보도록 하죠. 이전 비디오에서 3D 볼륨을 가져 오는 방법과 두 개의 서로 다른 필터를 사용하여 볼륨을 합성(convolve)하는 방법을 살펴 보았습니다. 이 예제를 다른 4x4 아웃풋으로 가져 오기 위해. 첫 번째 필터로 convolving하면 첫 번째 4x4 아웃풋이 생성됩니다. 두 번째 필터로 convolving하면 또 다른 4x4 아웃풋이 제공됩니다. 이것을 컨볼루션 신경망 레이어로 바꿔야 할 마지막 작업은 이들 각각에 바이어스를 추가하는 것입니다. 그래서 이것은 실제 숫자가 될 것입니다. 그리고 파이썬 브로드캐스팅이있는 곳에서는. 이 16개 원소의 각각에 같은 숫자를 추가해야 합니다 그리고 나서 이 그림에 비선형성을 적용하면, 상대적인 비선형성이죠, 그러면 4x4 아웃풋을 산출하게 됩니다, 이해되시죠? 바이어스와 비 선형성을 적용하고 나면 바닥의 이 점에 대해서도 다른 바이어스를 더합니다. 다시 말하지만, 이것은 실제 숫자입니다. 따라서 모든 16 개의 숫자에 단일 숫자를 추가 한 다음 비선형 성을 적용합니다, 실제 비선형성이라고 가정해보죠. 그러면 또 다른 4x4 아웃풋이 만들어집니다. 이전에 했던 것처럼 이것을 취해서 다음과 같이 쌓아두면 4x4x2 아웃풋을 마침내 얻게 됩니다. 그런 다음이 계산은 6x6x3x4x4x4인데요, 이게 바로 컨볼루션 신경망의 하나의 레이어가 됩니다. 표준 신경망, 즉 컨볼루션이 아닌 신경망에서 4가지 역전파 중 하나의 레이어에 이걸 매핑하기 위해서는 역전파 단계 바로 전에 이런 게 있다는 걸 기억하셔야 합니다. z[1]=w[1] a[0], 여기서 a0은 x 입니다. 그리고 나서 b[1] 을 더합니다. 그리고 나서 a[1]산출하기 위해서 비선형성을 적용하면 g(z[1])가 됩니다. 그래서 유추해보자면 여기서 이 인풋은, a[0], x3입니다. 그리고 이 필터들은 w1과 비슷한 역할을 합니다. 컨볼루션 연산 작업 중에 이 27 개의 숫자를 취하게 된다는 걸 명심하세요 아니면 두 개의 필터가 있기 때문에 27의 2배에 해당하는 숫자를 가질 수도 있습니다. 이 숫자들을 모두 가져 와서 그것을 곱하십시오. 여러분은 이 4x4 행렬을 얻기 위한 선형 함수를 실제로 계산하고 있습니다. 따라서 4x4 행렬, 즉 컨볼루션 연산의 아웃풋은 W[1]와 a[0] 를 곱한 것과 비슷한 역할을 합니다. 저쪽 4x4 뿐만 아니라 이쪽에 있는 4x4 아웃풋이 생겨나게 됩니다. 해야 할 또 다른 건 바이어스를 더하는 일입니다. 값을 더하기에 앞서서 여기 있는 이것들은 z과 비슷한 역할을 합니다. 제 생각엔 이것은 마침에 비선형성을 적용하는 것으로 만들어지는 거죠. 이 아웃풋은 a[1]의 역할을 하고, 이것은 곧 다음 레이어에서 여러분의 활동성이 됩니다. 따라서 이것이 바로 선형의 연산과 컨볼루션이 곱해질 때 a[0]에서 a[1] 진행하는 방법입니다. 따라서 컨볼루션은 실제로 선형 연산을 적용하고 있으며, 바이어스와 적용된 Value Operation 또한 가지고 있는 것입니다. 6x6x3 차원의 a[0]로 부터 시작해서 신경망의 레이어 하나를 거쳐서 4x4x2 차원의 a[1]으로 바뀌게 되는 것입니다. 따라서 6x6x3 은 4x4x2로 바뀌어지고, 컨볼루션 망의 하나의 레이어가 된는 것이지요. 이 예제에서 우리는 두 개의 필터를 가지고 있습니다, 즉, 두 가지의 feature이죠. 이것으로 우린 4x4x2의 아웃풋을 생성할 수 있었습니다 하지만 예를 들어 2개가 아니라 필터가 10개가 있다면, 4x4x10 dimension의 아웃풋 볼륨을 도출하게 되겠지요. 2개가 아니라 10개의 망을 취하게 될 것이고, 4x4x10의 아웃풋 볼륨을 형성하기 위해 쌓아 올리게 될 것이기 때문입니다. 그게 바로 a[1]이 되는 것이죠. 그럼, 이해가 잘 됐는지 확인하기 위해서, 예제를 같이 풀어봅시다. 여러분에게 필터가 10개 있다고 가정해봅시다. 2개가 아니라 3x3x3의 볼륨이고, 신경망의 하나의 레이어라고 생각해보세요. 이 레이어는 얼마나 많은 파라미터를 가지고 있을까요? 함께 알아보시죠. 각각의 필터가 3x3x3의 볼륨이므로, 각각은 27개의 파라미터로 채워지겠죠, 그렇죠? 들어갈 숫자가 27개가 될 것이고, 거기에 바이어스가 있습니다. 그러면 b는 파라미터이므로, 그럼 28개의 파라미터가 되는 것이죠. 그리고 이전 슬라이드에 있던 두 개의 필터를 그렸다고 상상해보십시오. 그러나 지금 실제로 10 개가 있다고 상상한다면, 1, 2… 10 그리고 이 모두 합쳐서 28개에 10을 곱하면 총 280개의 파라미터가 됩니다. 이것에 관해 좋은 점 하나를 주목하십시오. 인풋 이미지의 크기가 아무리 크다 해도 인풋 이미지는 1,000x1,000 또는 5,000x5,000가 될 것입니다. 하지만 파라미터의 수는 여전히 280로 고정되어 있습니다. 그리고 이 10 개의 필터를 사용하여 feature, 수직 모서리, 수평 모서리를 감지할 수 있습니다. 어쩌면 매우 큰 이미지 에서조차 도 다른 feature는 파라미터가 매우 작은 수 일 수도 있습니다. 따라서, 이는 컨볼루션 신경망의 하나의 속성으로서 여러분이 할 수만 있다면 overfitting 되지 않도록 만들 수 있습니다. 그러므로 일단 작동하는 10 개의 feature detector를 배워두면 이걸 큰 이미지들에도 적용해 볼 수 있습니다. 그리고 파라미터의 수는 여전히 고정되어 있으며 상대적으로 작습니다 이 예시의 280처럼 말이죠. 좋습니다, 이 강의를 마무리 지으며, 컨볼루션 신경망에 있는 컨벌루션 레이어를 설명해주는 하나의 레이어를 설명하기 위해 사용하는 표기법을 요약해보도록 합시다. 따라서 레이어 l 는 콘볼루션 레이어입니다. 필터 크기를 나타내기 위해 위 첨자 f를 사용하려고 합니다. 이전 강의에서 우리는 필터가 fxf 인 것을 보았습니다. 이 위 첨자 대괄호 안에 있는 [l]는 fxf 필터 레이어 l 의 필터 사이즈임을 나타냅니다. 위 첨자 대괄호 안에 있는 [l]은 특정 레이어 l 을 가리키기 위해 사용하는 표기법 입니다. 저는 이제 padding의 양을 낱아 내기 위해 p [l]을 사용할 것입니다. 다시 말하자면, 패딩의 양은 유효한 컨볼루션 즉 패딩 하지 않겠다고 말하는 것만으로도 구체화될 수 있습니다. 아니면 혹은 똑같은 패딩을 선택 함으로서도 가능한데요, 이는 여러분이 패딩을 선택한다는 의미가 되는 거죠. 그래서 아웃풋 크기는 인풋 크기와 동일한 높이와 너비를 갖습니다. 그리고 나서 스트라이드를 나타내기 위해 s [l]을 사용하게 될 것입니다. 자, 이 레이어에 대한 인풋은 어떤 dimension 될 것입니다. 이것은 nxn x 이전 레이어에 있는 채널의 수가 되겠죠. 자, 저는 이 표기법을 조금 수정하려고 합니다. 저는 위 첨자 l- 1 을 사용하도록 하겠습니다. 왜냐하면 이것은 이전 레이어에서 활성화 되었기 때문인데요, 위 첨자 ㅣ-1 하고, 또한 nC에도 위 첨자 ㅣ-1 하면 되겠죠. 지금까지 예제에서 우리는 단지 같은 높이와 너비의 이미지를 사용했습니다. 높이와 너비가 다를 경우, 이전 레이어 인풋의 높이와 넓이를 표현하기 위해 위 첨자 h와 위 첨자 w를 사용하려고 합니다, 아시겠죠? 따라서 레이어 l에서 볼륨의 크기는 nh x nw x nc이고 위 첨자 대괄호 l 를 가지고 있습니다. 그것은 그냥 레이어 l 에 있는 것이죠, 이 레이어에 대한 인풋은 이전 레이어에 있던 게 어떤 것이던, 여기에 ㅣ-1 이 있는 겁니다. the 신경망의 이 레이어는 그 자체로 그 값을 산출해낼 것입니다. 그러면 nh 의 ㅣ, nw 의 ㅣ, nc의 ㅣ 이것이 아웃풋 사이즈가 됩니다. 아웃풋 볼륨 사이즈, 아니면 적어도 높이나 무게가 이 n + 2p- f / s + 1 수식에 의해 산출된다는 것을 인정하긴 하지만, 그 값을 온전히 취해서 그걸 내림 하십시오. 이 새로운 표기법에서, 레이어 l의 아웃풋 값은 이전 레이어의 dimension이 될 것입니다. 그리고 레이어l에서 사용하고 있는 패딩을 더해준 후, 이 레이어 l 에서 사용하고 있는 필터사이즈를 빼면 됩니다. 기술적으로 높이는 이렇게 하면 맞겠죠? 따라서 아웃풋 볼륨의 높이는 이렇게 하면 주어집니다, 그리고 오른쪽에 있는 이 수식으로 계산하면 됩니다. 그리고 넓이 또한 같은 방식으로 하면 됩니다. 따라서 h를 지우고 w를 넣어주세요, 그리고 나서 아웃풋 값의 높이나 넓이를 계산하기 위해서 연관된 높이나 넓이 중에 하나로 같은 수식을 계산하세요. 이렇게 해서 nh[l-1] 은 nh[l]과 연관시키고 w[l-1]은 nwl과 연관시키는 겁니다. 이제, 채널의 숫자는 어떤가요? 이 숫자들은 어디서 나온걸 까요? 살펴보시죠, 아웃풋 볼륨이 이 정도 depth를 가지고 있다면 이전 예제들에서 본 것처럼 그것은 그 레이어에 있는 필터의 수와 동일합니다. 리는 두 개의 필터를 가지고 있는 것이죠, 아웃풋 값은 4x4x2, 2차원이었습니다. 만약 10개의 필터를 가지고 있고 upper volume이 4x4x10 이라면, 아웃풋 값에 있는 채널의 수는 신경망의 이 레이어에서 사용하고 있는 필터의 수와 같습니다. 다음으로, 이 필터의 사이즈는 어떤가요? 각 필터는 f[l] x f[l] x 100개의 수일 텐데요, 그럼 그 마지막 숫자는 뭘까요? 6x6x3 이미지를 3x3x3필터와 합성해야 한다고 배웠으니까 여러분의 필터에 있는 채널의 수를 인풋에 있는 채널 수와 매치를 시켜야만 합니다. 그래야 이 숫자가 저 숫자와 일치하게 되겠죠. 이런 이유로 각각의 필터는 f[l] x f[l] x nc[n-1]이 됩니다. 이 레이어의 아웃풋은 때로는 비선형성 있는 기기들을 적용해서 이 레이어 a[l] 의 activation 이 됩니다. 그리고 이것은 우리가 살펴보았던 이 dimension이 되겠죠. a[l] 은 3D 볼륨이 되고 즉, nH[l] x nW[l] x nC[l] 이 되겠습니다. 그리고 여러분이 vectorized implementation 또는 batch 기울기 강하 또는 mini batch 기울기 강하을 사용하는 경우, m 예시를 가지고 있다면, m activation인 A[l]을 산출하게 됩니다. 그러면 그건 M x nH][l] x nW[l] x nC[l] 이 되겠죠? 프로그래밍 사이즈로서 batch 기울기 강하을 사용하고 계시다면, 변수들의 배치가 될 것입니다. 그리고 인덱스와 트레일 링 예제를 우선 가지고 있고 이 세 개의 변수들이 있습니다. 다음으로, 가중치 또는 파라미터 또는 w 파라미터는 어떻습니까? 우린 이미 필터 dimension이 무엇인지 알아보았는데요, 필터는 f[l] x f[l] x nc [l- 1] 이죠. 하지만 이건 필터 하나의 dimension입니다. 그럼 우리는 얼마나 많은 필터를 가지고 있을까요? 글쎄요, 이것은 총 필터 수입니다. 모든 필터의 가중치는 이 수식에 의해 산출되는 만큼의 dimension을 가지게 됩니다. 이렇게 총 필터수를 곱한 만큼이 되겠죠? 이렇기 때문에, 마지막 숫자는 레이어 l안에 있는 필터의 수가 됩니다. 마지막으로, 바이어스 파라미터가 있습니다. 하나의 바이어스 파라미터 가지고 있는데요, 각각의 필터에 하나의 숫자가 있습니다. 바이어스는 이 많은 변수들을 가지고 있죠. 이건 그냥 이 dimension의 벡터 값일 뿐입니다. 나중에 알게 되겠지만, 4 차 행렬 또는 4 차 텐서에서는  (1, 1, 1, nC[1]) 로 표현되는 것이 더 편리합니다. 자, 표기법이 좀 많았다는 걸 저도 알고 있습니다. 하지만 이게 많은 부분에서 사용하게 될 관례적인 것입니다. 여러분이 온라인으로 검색하고 오픈 소스 코드를 살펴볼 때를 완전히 보편적인 표준 규칙은 없다고 말씀 드리고 싶습니다. 높이, 넓이, 그리고 채널을 정리하는데 있어서 말이죠. GitHub에서 혹은 open source implementations에서 소스코드를 찾아보신다면, 일부 작성자들이 이 용어 규칙을 사용하고 있는걸 보실 수 있을 겁니다, 채널을 먼저 넣는 변수들의 순서도 보게 될 것입니다. 사실 일반적인 framework에서 , 실제로 다수의 일반적인 framework에서, 변수 혹은 파라미터가 있습니다. 여러분은 왜 채널의 수를 제일 먼저 나열하려고 하십니까? 혹은 왜 이러한 볼륨에 색인을 생성할 때 채널 수를 마지막에 나열하려고 하시나요? 제 생각에 이 두 방법 모두 일관성이 있는 한 괜찮습니다 그리고 안타깝게도, 이것은 딥러닝 문헌에서 동의되지 않은 부연설명 같은 것일지도 모르겠습니다. 하지만 저는 이 강의에서 이 형식 사용할 것입니다. 높이와 너비를 나열한 다음 채널 수가 마지막으로 나열됩니다 여러분이 사용할 수 있는 새로운 표기법이 분명 많이 존재한다는 것을 알고 있습니다. 하지만 여러분이 놀라고 있는 건, 긴 표기법일 겁니다. 이 모든걸 어떻게 다 기억할 수 있을까? 라고 생각하겠죠. 걱정 마십시오, 이 모든 표기법을 기억하고 있을 필요는 없으니까요. 이번 주의 예제들을 통해 여러분은 더 익숙해질 것입니다. 하지만, 이 강의에서 여러분이 꼭 알았으면 하는 키포인트는 바로 컨볼루션 신경망가 작동하는 방법의 단면입니다. 그리고 한 레이어의 활성화를 시킬 수 있는 계산법과 다음 레이어의 활성화를 위해 매핑하는 것을 알고 계셨으면 좋겠습니다. 그리고 이제 여러분은 컨볼루션 신경망의 한 레이어가 어떻게 작동하는지 알게 되었습니다. 이것들을 모두 합쳐서 실제로 더 깊은 컨볼루션 신경망을 만들어봅시다. 이것들을 모두 합쳐서 실제로 더 깊은 컨볼루션 신경망을 만들어봅시다. 그럼, 다음 강좌를 보러 가시죠.