Şimdi evrişimsel bir sinir ağının
 nasıl oluşturulacağını göreceğiz, bir örnek üzerinden gidelim. Bir önceki videoda,
 nasıl 3 boyutlu bir hacmi alıp iki farklı filtre ile evriştirileceğini gördünüz. Ve sonuçta iki farklı 4e 4lük çıktı aldık. Ilk fıltreyle evriştirdiğimizde bu 4'e 4'lük çıktıyı elde ediyoruz, ikinci filtre ile evriştirdiğimizde ise
 4'e 4'lük farklı bir çıktı veriyor. Bunu evrişimsel bir sinir ağına çevirmek
için yapmamız gereken son şey ise her ikisine de reel bir yanlılık ekleyeceğiz. Python'da olduğu gibi,
 aynı sayıyı buradaki 16 elemanın hepsine ekliyoruz. Ardından doğrusal olmayan bir
 fonksiyona uyguluyoruz ki bu örnekte ReLu fonksiyonuna karşılık geliyor
ve bu size 4'e 4'lük bir çıktı verıyor. Yanlılığı ekledikten ve doğrusal olmayan fonksiyona soktuktan sonra alttaki için de aynı şeyleri yapıyoruz, 
 farklı bir yanlılık değeri ekliyoruz b2 de reel bir sayı. Bunu 16 sayının hepsine ekliyoruz, ve ardından doğrusal olmayan fonksiyona sokuyoruz
ReLu fonksiyonu olsun. Ve bu da 4'e 4'lük farklı bir çıktı veriyor. Daha önce de yaptığımız gibi
bunu alır ve şu şekilde birleştirirsek
4x4x2'lik çıktıyı elde ediyoruz. 6x6x3'lük hacimden 4x4x2'lik hacime ulaştğımız
 tüm bu hesaplamalar neticesinde evrişimsel sinir ağımızın
bir katmanını elde etmiş oluyoruz. Yani bunu eslemek icin standart sinir ağındaki ileri yayılımın bir katmanına eşlemek için, Hatırlarsanız yayılımdan öncekı adımlardan biri şu şekilde idi, değil mi? z1=w1 çarpı a[0], a[0] da x'e eşitti, artı b[1]. Daha sonra, a[1]'i elde etmek için doğrusal olmayan fonksiyon uyguluyoruz ve bu da g(z[1])'e eşitç Buradaki girdi(input), bu benzetmede bu a[0]. Buradaki filtreler de w[1]'e benzeyen bir rol üstlenir. Hatırlarsanız evriştirme işlemi sırasında, bu 27 sayıyı alıp, aslında 27 çarpı 2 çünkü iki filtre var burada. Tüm bu sayıları alıp çarpıyorsunuz. Bu 4x4 matrisi elde edebilmek için doğrusal bir fonksiyon hesaplıyorsunuz. Bu 4x4 matris, evriştirme işleminin çıktısı, bu w[1] çarpı a[0] gibi bir rol üstlenir. Bu gerçekten de bu 4x4 ün aynı zamanda bu 4x4'ün çıktısıdır. Diğer yaptığınız şey ise yanlılğı eklemek. ReLU fonksıyonunu uygulamadan önceki buradaki ifade z'ye benzeyen bır rol üstlenir. En sonunda doğrusal olmayan fonksiyonu uyguuyoruz, bu da bir nevi buna eşit. Bu da sizin bir sonraki katman için aktivasyonunuz olur. a[0]'dan başlayarak a[1]'e işte bu şekilde ulaşıyoruz, ilk başta doğrusal işlemler evriştirme işlemi çarpımlardan oluşuyor. Bu yüzden evriştirmeyi doğrusal bir işleme tabi tutmak olarak düşünebiliriz ve yanlılığı ekliyoruz ve ReLU fonksiyonu uyguluyoruz. 6x6x3 boyutundaki a[0]'dan başlayarak sinir ağının bir katmanı aracılığı ile, 4x4x2'lik a[1]'i elde ettik. 6x6x3'ten 4x4x2'e gittik. Bu evrişimsel sinir ağının bir katmanı. Bu örnekte 2 filtremiz var, iki öznitelik, 4x4x2 boyutlu çıktı elde etmemizin sebebi de bu, Fakat eğer 2 filtre yerine 10 filtre olsaydı, çıktının hacmi 4x4x10 olurdu. çünkü bunlardan 10 tane olacaktı sadece 2 değil ve bunları yığdığımızda 4x4x10'luk çıktı hacmi elde edecektik ve a[1] bu olacaktı. Peki, bunu anladığınızdan emin olmak için bir örnek üzerinden gidelim. Sinir ağının bır katmanında sadece 2 filtre değil de boyutları 3x3x3 olan 10 filtre olduğunu varsayalım, Bu katmanda kaç tane parametre vardır? Hadi bunu bulalım. Her filtrenin, hacmi 3x3x3 her filtre 27 parametreye sahip, değil mi? Öğrenilmesi gereken 27 sayı var ve bir de yanlılık. Bu b parametresiydi, toplamda 28 yaptı. Düşünün önceki slaytda 2 filtre vardı şimdi ise 10 tane fıltre var. 1, 2, ...., 10 tane hepsi birlikte 28 çarpı 10 ve sonuçta 280 parametre ediyor. Bunun hakkında dikkat edilmesi gereken önemli bir husus da
giriş resminiz ne kadar büyük olursa olsun, giriş resminiz 1000'e 1000 ya da 
5,000'e 5,000 de olabilir, ama parametre sayısı hala 280 olarak kalır. Ve bu 10 filtreyi kullanarak birtakım özellikleri tespit edebilirsiniz, yatay kenar çizgilerini, dikey kenar çizgilerini belki diğer
başka özellikleri, çok çok büyük bir resmin herhangi bir yerindeki bile sadece az sayıda parametre ile tespit edebilirsiniz. Bu sinirsel ağları aşırı uyumluluğa(overfitting) daha az yatkın hale getiren bir özellik. Yani 10 tane çalışan öznitelik saptayıcısı öğrendiğinizde, bunu çok büyük bir resme bile uygulayabilirsiniz. Ve parametre sayısı hala sabit ve nispeten az, mesela bu örnekte 280. Pekala, bu videoyu bir özetlersek hadi evrişimli sinir ağlarını özetlemek için evrişimli katmanı tanımlayalım temel yapı taşlarından biridir Yani l katmanı bir evrişimli katmansa filtre boyutunu ifade etmek için f üzeri [l] kullanacağım. Önceden filtrelerimizin boyutlarını
ifade etmek için f x f kullanıyorduk Şimdiyse [l] ile sadece l katmanının filtre boyutlarının
 f x f olduğunu ifade ediyoruz. Önceden de olduğu gibi, üzeri [l] ifadesini,
l katmanından bahsettiğimizi belirtmek için kullanıyoruz. Doldurma miktarını belirtmek için p[l] ifadesini
kullanacağız. Ve yine, doldurma miktarını, yalnızca
'valid'(geçerli) evrişim istediğimizi belirterek, sıfır olarak ayarlayabiliriz. Ya da 'same'(aynı) evrişim istediğimizi belirterek
dolgunun, çıktıyla girdinin yükseklik ve genişliğinin aynı olacağı şekilde ayarlanmasını
sağlayabiliriz. Kaydırma adımını belirtmek için s[l] kullanacağız. Bu katmanın girdisinin belli bir boyu olacak. O da n x n x bir önceki katmanın kanal sayısı
olacak. Şimdi bu ifade şeklini biraz değiştireceğim. n üzeri [l-1] kullanacağım çünkü bu bir önceki katmanın aktivasyonu. n[l-1] x n[l-1] x nc[l-1] Şimdiye kadarki örneklerimizde aynı yükseklik
ve genişliğe sahip resimleri kullanıyorduk Ama yükseklik ve genişlik farklı olabileceği için Bir önceki katmanın girdisinin genişliğini ve yüksekliğini
ifade etmek için _h (height/yükseklik) ve _w (width/genişlik) kullanacağım. Yani l katmanındaki hacmin büyüklüğü n_h x n_w x n_c üzeri [l] olacak l katmanının girdisi, bir önceki katmandan geldiği için l-1 kullanıyoruz. Sonra yapay sinir ağının bu katmanının bir de 
kendi çıktısı olacak. O da n_h[l] x n_w[l] x n_c boyutunda olacak. Böylece önceden çıktının büyüklüğü için,
en azından yüksekliği ve genişliği için şu formülü kullanıp aşağı yuvarlarken: (n + 2p - f/s)+1 Bu yeni gösterimde, l katmanının çıktısı önceki katmandaki boyutlar n[l-1] artı şu anki l katmanında kullandığımız
doldurma (padding) p[l] eksi l katmanının filtre büyüklüğü f[l], ve bunun gibi... Ve tekik olarak bu yükseklik için geçerli,
değil mi? Yani çıktı hacminin yüksekliği bu formülle veriliyor, 
sağdaki formülle bunu hesaplayabilirsiniz. Aynısı genişlik için de geçerli h'nin üstünü çizip w yazarsak aynı formülle genişliği elde edebiliriz. Yani bu aynı formülü kullanarak çıktının
yüksekliğini veya genişliğini hesaplayabiliriz. n_h[l-1]'la n_h[l] ve n_w[l-1] ve n_w[l] bu şekilde
ilişkilendirilebilirler. Peki ya kanalların sayısı? O sayı nereden
geliyor ona bakalım. Eğer çıktı hacmi bu derinliğe sahipse, bunun bu katmandaki filtrelerin sayısına eşit
olduğunu önceki örneklerden biliyoruz değil mi? Mesela 2 filtremiz vardıysa, çıktımız 
4x4x2, 2 boyutlu oluyordu. Ya da 10 filtremiz olduğunda,
çıktı hacmi 4x4x10 oluyordu. Yani bu, çıktı hacmindeki kanal sayısı, yalnızca sinir ağının bu katmanında kullandığımız
filtre sayısını gösteriyor. Sırada, bu filtrenin büyüklüğü ne olmalı? Her bir filtre f[l] x f[l] x bir sayı olacak
değil mi? Peki bu son sayı nedir? 6x6x3 bir resmi 3x3x3 bir filtreyle evriştirmemiz gerektiğini görmüştük. Yani filtrenizdeki kanalların sayısı
girdinizdeki kanalların sayısına eşit olmalı. Bu sayı, bu sayıyla aynı olmalı. Bu nedenle her filtrenin büyüklüğü
f[l] x f[l] x n_c[l-1] olacak. Bu katmanın çıktısı, yanlılığı ekleyip
doğrusal olmayan fonksiyona soktuktan sonra bu a[l] katmanının aktivasyonları olacak Ve de bunun, bu boyutlarda olacağını
gördük değil mi? a[l], n_h[l] x n_w[l] x n_c[l] boyutlarında 3 boyutlu bir hacim olacak. Ve de vektörize bir uyarlama yapıyorsanız, 
ya da Toplu Gradyan İniş (Batch Gradient Descent) ya da Mini Toplu Gradyan (mini batch GD) İniş 
kullanıyorsanız ve m örneğiniz varsa, m sayıda aktivasyona karşılık gelen 
A[l] çıktınız olacak demek. Yani m x n_h[l] x n_w[l] x n_c[l] Diyelim ki, eğer sert bir gradyan inişi kullanıyorsanız ve programlama boyutlarında bu değişkenlerin sıralanması olacaktır. Ve bizim önce dizin ve izleyen örneklerimiz var, Ve sonra da bu üç değişken. Peki sonra ağırlılar veya parametreler veya bir çeşit w parametresi ne olacak? Pekala, daha önce filtre boyutunun ne olduğunu görmüştük. Evet, filtreler f[l] ye f[l] ye nc[l-1] olacaklardır, ama bu bir filtrening boyutu. Kaç filtremiz var? Bu kullandığımız filtrelerin sayısı, yani tüm filtreler bir araya gelince oluşan ağırlığın boyutu bununla ifade edilir, çarpı filtrelerin toplam sayısı, değil mi? Çünkü bu, l katmanındaki son nicelik filtrelerin sayısıdır. Ve son olarak, yanlılık parametreleriniz var, ve her bir filtre için bir gerçek sayı olarak bir yanlılık parametreniz var. Yani, yanlılık parametresinin bu kadar değişkeni olacak ve bu sadece bu boyun bir vektörü olacak. Her ne kadar kodun daha sonra 1 e 1 e 1 e nc[l] olarak dört boyutlu bir dizeyde veya dört boyutlu bir tensörde ifade edilmesinin daha uygun olacağını görecek olsak da. Evet, biliyorum bu oldukça fazla formüldü, ve ve bu çoğu kısım için kullanacağım düzen. Sadece belirtmek isterim ki, eğer çevrimiçi olarak araştıracak olursanız ve açık kaynak koduna bakarsanız yükseklik, genişlik ve kanal'ın sıralaması hakkında evresel olarak geçerli standart bir uygulama yok. Yani eğer GitHub'ta kaynak koduna bakarsanız veya bu açık kaynak uygulamalarda, göreceksiniz ki bazı yazarlar bunun yerine, önceki kanalı koyduğunuz bu sıralamayı kullanıyorlar, ve bazen değişkenlerin bu sıralamasını görürsünüz. Ve aslında bazı genel çatılarda, aslında çoklu genel çatılarda, aslında bir değişken veya bir parametre var. Bu hacimleri dizinlerken neden kanalların sayısını ilk listelemek istiyorsunuz, veya kanalların sayısını son listelemek istiyorsunuz. Bence tutarlı olduğunuz sürece bu uygulamaların her ikisi de doğru çalışıyor. Ve maalesef belki bu bir çeşit ek açıklama olacak, derin öğrenme literatüründe bir fikir birliği yok, ama bu videolar için boy ve en ve sonra kanal sayısını son olarak listelediğimiz bu formülü kullanacağım. Evet, biliyorum ki kesinlikle kullanabileceğiniz oldukça fazla formül vardı, ama vay diye düşünüyorsunuz, bu oldukça uzun bir formül ve bunların hepsini nasıl hatırlaycağım? Bunu endişe etmeyin, bu formüllerin hepsini hatırlamanız gerekmiyor, ve bu haftanın egzersizleri süresince daha aşina olacaksınız. Ancak bu videodan alacağınızı umduğum kilit nokta, konvansiyonel sinir ağlarının nasıl çalıştığına dair sadece bir katman. Ve bir katmanın aktivasyonlarını almak ile ilgili olan hesaplamalar ve bunları bir sonraki katmanın aktivasyonlarına eşlemek. Ve daha sonra, şimdi bileşimsel sinir ağının bir katmanının nasıl çalıştığını bildiğinize göre, bunlardan bir demeti bir araya getirip daha derin bileşimsel bir sinir ağı oluşturalım. Bir sonraki videoya gidip görelim.