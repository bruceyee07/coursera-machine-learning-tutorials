现在让我们做好准备<br />来构建单层卷积神经网络 我们先来看一个例子 之前我们已经提过如何应用两个不同的过滤器 对三维输入进行卷积计算 比如现在，对图中的输入，我们想要得到不同的4x4输出 应用第一个卷积过滤器 我们得到了第一个4x4的输出 应用第二个卷积过滤器我们得到了另一个4x4输出 最终我们需要把这些输出变成单层卷积神经网络 还需要对每一个输出添加一个偏差（bias） 这里偏差是一个实数 这里使用的是广播机制，我们对这 16个元素添加同样的偏差 然后我们可以继续添加一些非线性转换ReLU 最终，通过添加偏差和非线性转换 我们得到一个4x4矩阵输出 对于下面的矩阵也是一样，添加一个不同的偏差值 这里的偏差也是一个实数 这个数字也像上面一样，通过广播添加到其他16个元素中 最后应用一些非线性处理 比方线性整流函数(ReLU) 这样我们得到了另一个4x4的矩阵输出 像之前一样 我们最后把这两个矩阵放在一起 得到一个4x4x2的输出 这个例子中对于6x6x3的输入<br />我们通过计算得到一个4x4x2的输出(Andrew原话是4x4x4，应该是口误） 这就是卷积神经网络的一层 现在我们把这个例子和普通的 非卷积单层前向传播神经网络对应起来 在（神经网络）传播之前我们需要做这些 z[1] = w[1] x a[0], a[0] = x 再加上b[1] 然后再应用非线性函数g 也就是g(z[1]) 得到a[1] 在图中这个例子中，这个输入就是a[0]，也就是x 这里的过滤器 作用和w[1]类似 之前我们在卷积计算中 我们有27个输入 或者确切的说<br />是两组27个输入 因为我们有两个过滤器 我们需要把上面这些数字相乘 这其实就是通过一个线性方程计算得到一个4x4的矩阵 这里通过卷积计算得出4x4矩阵 这个过程和 w[1] x a[0] 类似 输出也是一个4x4的矩阵 另外就是添加偏差值 因此方框里的这些 作用和z类似 最后 应用非线性方程 因此这里的输出 其实成为了下一层(神经网络)的激活函数 这就是从a[0]到a[1]的步骤：首先是线性计算 然后再进行卷积，对这些相乘 因此，卷积计算其实就是应用线性操作计算 再添加偏差 然后通过ReLU操作 我们从6x6x3的输入 a[0] 经过一层神经网络的传播 得到了4x4x2的输出 也就是a[1] 从6x6x3到4x4x2 这就是一层卷积(神经)网络 在这个例子中 我们有2个过滤器 也就是两个特征 因此我们得到输出是4x4x2 如果我们有10个过滤器而不是2个 那么我们得到的输出就是4x4x10 因为这里需要有10个这样的操作(而不是2个).<br />然后把结果放到一起 得到一个4x4x10的输出 也就是a[1] 为了进一步理解这里所说的 我们来做一个练习 假定你的单层神经网络中有10个<br />而不是2个 3x3x3的过滤器， 这层网络中有多少参数？ 我们来计算一下 每个过滤器是一个3x3x3的三维矩阵 因此每个过滤器有27个参数 也就是，有27个数字需要进行训练学习得到 还有一个参数b，也就是总共28个参数 现在想象一下 前面我们的图中是两个过滤器 现在这里我们有10个过滤器 1, 2, ... 10个 加起来是28x10 也就是280个参数 注意这里. 一个很好的特性是. 不管输入的图像有多大 比方1000 x 1000 或者5000 x 5000， 这里的参数个数不变. 依然是280个 因此 用这10个过滤器来检测一个图片的不同的特征，比方垂直边缘线， 水平边缘线 或者其他不同的特征 不管图片多大 所用的参数个数都是一样的 这个特征使得卷积神经网络 不太容易过拟合（overfitting） 因此，比方你训练学习得到10个特征检测器(函数) 你可以把它们应用到非常大的图像(特征检测)中 所用的参数数目还是不变的，比方这个例子中的280 相对(图片大小)来说非常小 现在我们来总结回归一下 用来在卷积神经网络中 用来描述一层网络的(形式化表示的)符号标记 我们说，l层是一个卷积层 用f加上标l来表示过滤器的(矩阵维度)大小 之前我们用 f x f 来表示 现在我们加上上标[l]来表示 这是一个l层大小为 f x f 的过滤器 按照惯例 这里的上标[l] 表示当前层l p加上上标[l] 表示填充（padding）的大小 填充的大小也可以通过不同的卷积名称来定义 比方valid填充， 就是没有填充 Same填充，表示应用的填充大小会 使得输出的结果大小和输入拥有相同的维度大小 s加上上标[l]表示步长大小 这一层的输入 是一个多维矩阵 也就是一个n x n x n c (上一层的通道数目） 我们来变一下这个表示方法 我们用上标l-1 因为这个来自上一层的激活函数 n[l-1] 乘以 nc [l-1] 迄今为止我们所用的例子中图像的长宽都是一样的 有些情况下图像长宽可能不同 因此我们用下标 h 和 w 来表示 来自上一层的输入的长和宽 因此l层的矩阵大小是 n_h * n_l * n_c, 所有都加上上标[l] 这个是l层的情况. 这一层的输入是 上一层的输出. 因此这里是l-1 然后这一层神经网络的输出就是 n_h[l] x n_w[l] x n_c[l] 这就是输出的(维度)大小 和之前所说的一样，这里输出的大小 起码长和宽是由这个公式 (n+2p-f)/2 决定的 结果如果不是整数的话 取下界 在我们现在的表示方法中 l层的输出 是前一层的维度 加上当前l层的padding 再减去当前l层所用的过滤器大小 我们现在计算的是输出矩阵的长 同样 这个公式也可以计算矩阵的宽 只要把h换成 w就可以 这两个的计算公式都是一样的 这就如何从n_h[l-1]到n_h[l]，和从n_w[l-1]到n_w[l] 那么通道数目呢，我们从哪里得到这个数字？ 我们来看一下输出的(维度的)深度 从之前的例子中我们知道 通道数目的大小和过滤器数目是一样的 比方 如果我们有2个过滤器 那么我们的输出就是4x4x2. 是2维的 如果过滤器数目是10个 那么大小就是4x4x10 因此这个输出中的通道数目 就是我们这一层神经网络中所用的过滤器的数目 下面 我们来看一下过滤器的大小 每个过滤器的大小是f[l] x f[l] 再乘一个数 那么这个数是什么呢？ 我们之前的例子中，如果对一个6x6x3的图像进行卷积 所用的filter是3x3x3 因此所用的过滤器的通道的大小 应该和输入的通道数目相同 这两个数字一样大小 因此过滤器就是f[l] x f[l] x n_c[l-1] 最后通过非线性计算. 这一层的输出 就是这一层的激活函数a[l] 这个维度我们已经在这里可以看到了 a[l] 是一个三维矩阵 n_h[l] x n_w[l] x n_c[l] 如果你应用的是向量化实现 或者批量梯度下降 或者是小批量梯度下降 那么你的输出A[l]，<br />加入你有m个输入，就是一组m个激活函数 也就是 m x n_h[l] x n_w[l] x n_c[l] 如果你是用的是批量梯度下降 在代码中这就是你需要使用的变量的(梯度下降)顺序 首先我们有这些训练数据的大小 然后，这三个变量 那么权重矩阵w呢？ 我们已经知道了过滤器的维度 过滤器是f[l] x f[l] x n_c[l-1] 但是这只是一个过滤器的维度 我们需要多少个过滤器呢？ 这个是我们需要的过滤器的数目 所有过滤器的权重的维度就是所有的过滤器的大小总和 由这个给定 对吧？ 因为这个，最后的数字大小是l层神经网络中 过滤器的数目大小 最后 每一个过滤器 加上一个偏差 也就是每个过滤器加上一个实数 因此(我们有) 偏差参数有这么多变量 这只是一个大小为这个的向量 虽然后面我们会看到 更加方便的偏差系数编码 (1, 1, 1, n_c[l]) 这样的四维矩阵，或称为4维tensor 到现在为止 我们提到了很多的参数和符号 这些都我们在本课程会用到的 另外我想提一下 如果你上网搜索 或者查看源代码的话 你会发现并没有一个标准的惯例来规定(公式中的） 长 宽 和通道的顺序 因此你如果你读到GitHub 或者其他开源的实现源代码 你可能会发现有些作者会把通道数放到最前面 另外一些时候你看到(我们之前用的)那个顺序 其实在一些常见的框架中 会有一个参数来设置在(矩阵的) 索引中 是要把通道数目放在最开始 还是最后面 我觉得这些情况都合理 只要你使用的时候保持前后一致。 不幸的是 这个表示方式并没有在 深度学习的各种文章中广泛使用 但是在这门课中我会采用这种表示方式 (也就是)高和宽这两个维度放在前面 通道维度放在最后 我知道这里突然出现了很多的注释符号 你可能会觉得太多了记不住 不要担心 你不需要记住这些注释和符号 通过本周的练习 你会更加熟悉这些表示方式和注释的 这里我希望你把这个视频的重点放在 卷积神经网络的单层卷积神经网络工作原理 以及在这层中激活函数的计算 还有怎么把激活函数对应到下一层 现在你知道了单层卷积神经网络怎么工作 下一步 我们联合几层(神经网络) 来讨论深度卷积 神经网络 我们来看下一个视频
GTC字幕组翻译