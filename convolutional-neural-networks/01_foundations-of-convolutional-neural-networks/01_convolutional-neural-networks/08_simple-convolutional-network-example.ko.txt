지난 강좌에서는 단일 레이어 빌딩블록, 컨볼루션 신경망의 단일 컨볼루션 레이어를 보았습니다. 이제 심층 컨볼루션 신경망의 구체적인 예를 살펴 보겠습니다 이것은 지난 비디오의 끝 부분에서 소개 한 표기법을 연습하도록 해 줄 것입니다. 이미지를 이용해서 이미지 분류, 혹은 이미지 인식을 한다고 가정 해 보겠습니다 인풋 이미지 x를 가지고, 이게 고양이인지 아닌지, 0인지 1인지를 결정해야 하므로 이는 분류 문제입니다. 이 작업을 위해 사용할 수 있는 컨볼루션 신경망의 예를 만들어 보겠습니다 이 예제에서는 상당히 작은 이미지를 사용하려고 합니다. 이 이미지가 39 x 39 x 3이라고 가정 해 보겠습니다. 이렇게 하면 숫자들이 조금 더 잘 작동하도록 만들어줍니다. 레이어 0에 있는 nH 가 nW 와 같아질 것입니다. 즉, 높이와 넓이가 39가 됩니다. 그리고 레이어 0에 있는 채널의 수는3이 됩니다 첫 번째 레이어가 3 x 3 필터 세트를 사용하여 . feature를 감지한다고 가정 해 보겠습니다. 그러면 f=3 혹은 f1=3이 되죠. 우리가 3x3 process를 사용하고 있기 때문입니다. 스트라이드 1을 사용하고 패딩이 없다고 생각해 봅시다. 그래서 같은 컨볼루션을 사용하고, 10 개의 필터가 있다고 가정 해 봅시다. 그러면 신경망의 다음 레이어에서 activation은 37 x 37 x 10 이 될 것입니다. 또한 10은 10개의 필터를 사용하는 데에서 비롯된 것이고 이 공식 n + 2p- f 나누기 s 더하기 1 (n + 2p - f/ s + 1) 로 부터 37이 나오게 됩니다. 그렇다면, 39 + 0 - 3 나누기 1+1 은 37 이 되죠. 따라서 아웃풋이 37 x 37 이고, 곧 Valid Convolution 입니다. 이게 아웃풋 사이즈이죠. 따라서 우리의 표기법에서는 nh[1] = nw[1] = 37이고 nc[1] = 10 이므로 nc[1]도 첫 번째 레이어의 필터 수와 같습니다. 그리고 이것은 첫 번째 레이어의 activation 차수가 됩니다. 여러분이 또 다른 컨볼루션 레이어를 가지고 있고, 이번엔 5 x 5 x 5 필터를 사용한다고 가정해봅시다. 우리 표기법대로 해서, 다음 신경망에 있는 f[2]는  5이고, 스트라이드 2를 사용한다고 생각합시다. 패딩이 없고 필터는 20개, 그럼 이 아웃풋은 또 다른 볼륨이 될 것이고, 이것은 17 x 17 x 20 이 되겠죠. 여러분이 스트라이드 2를 사용하고 있기 때문에 차수는 훨씬 빠르게 줄어들었고 37 x 37 은 2배보다 약간 더 많이 줄어들어서 17 x 17 이 되었습니다. 필터를 20 개 사용하기 때문에 채널 수는 이제 20입니다. 따라서 이 activation a2는 이 차수가 될 것입니다. 따라서 nh[2] = nw[2] = 17 가 되고, nc[2] = 20 이 됩니다. 좋습니다, 마지막 컨볼루션 레이어 하나를 적용해 봅시다. 다시 5 x 5 필터를 사용한다고 가정 해 봅시다. 스트라이드 2 이고, 만약 여러분이 이렇게 하면, 저는 계산은 건너 뛰겠습니다. 하지만  7 x 7 사이즈에 필터 40개를 사용한다고 생각하면, 패딩 없음, 필터 40 개. 결국은 7 x 7 x 40 이 나옵니다. 이제 여러분이 한 것은 39 x 39 x 3 인풋 이미지를 취해서 이 이미지의 7 x 7 x 40 feature를 계산한 것입니다. 마지막으로, 일반적으로 수행되는 작업은, 여러분이 7 x 7 x 40을 사실 7 x 7 x 40은 1,960 이죠. 이 볼륨을 취해서 편평하게 하고 1,960 조각으로 이걸 펼쳐보십시오. 이걸 벡터로 전개 한 다음 이것을 로지스틱 회귀 단위 또는 softmax 단위로 피드하십시오. 여러분이 중요한 다른 물체들 중 어느 하나를 인식하려고 시도하는지에 따라 신경망에서 마지막으로 예측된 아웃풋을 제공하셔야 합니다. 분명히 말하자면, 이 마지막 단계는 이 모든 1,960 숫자들을 취해서, 아주 긴 벡터로 펼치는 것입니다. 최종 아웃풋에 대한 예측을 하기 위해 회귀할 때까지 softmax에 피드 할 수있는 긴 벡터 하나가 있습니다. 이는 컨볼루션 신경망의 전형적인 예입니다. 컨볼루션 신경망을 설계하는 많은 작업들은 이것들과 같은 하이퍼 파라미터 변수를 선택합니다. 총 사이즈는 어떤지 결정하고 스트라이드는 어떤지, 패딩이란 무엇이며 얼마나 많은 필터가 사용되는지도 결정합니다. 다음 주와 이번 주 후반에도 저희는 이런 결정을 어떻게 내리는지에 대한 제안과 가이드라인을 드리겠습니다. 그러나 지금 당장은 신경망에서 더 깊숙이 들어가면 일반적으로 39 x 39 크기의 큰 이미지로 시작합니다. 그리고 나서 높이와 너비가 잠시 동안 동일하게 유지됩니다. 그리고 신경망가 깊어짐에 따라, 점차적으로 작아지는 경향을 보입니다. 39가 37로 17로 14로 작아집니다. 죄송합니다. 39가 37로 17로 7이군요 반면 채널 수는 일반적으로 증가합니다. 그것은 3에서 10에서 20에서 40으로 커졌으며 많은 다른 컨볼루션 신경망 에서도 이와 같은 일반적인 경향을 볼 수 있습니다. 따라서 후속 강의들에서 이러한 파라미터를 설계하는 방법에 대한 가이드라인을 보겠지만, 컨볼루션 신경망 혹은 짧게 콘볼네트의 첫 번째 예를 간단히 보셨습니다. 이 점 드립니다. 그리고 일반적으로 컨볼루션 신경망에서 보통 세 개 유형의 레이어가 있습니다. 하나는 콥볼루션 레이어이며, 종종 이것을 콘볼 레이어로 나타냅니다 이것이 이전 네트워크에서 우리가 사용해온 것입니다. 아직 보지는 못한 다른 두 가지 유형의 레이어가 있습니다만, 하지만 다음 강의에서 이야기해보도록 하겠습니다. 하나는 pooling 레이어인데, 저는 종종 pool이라고 부릅니다. 마지막은 FC라고 하는 완전히 연결된 레이어입니다. 컨볼루션 레이어를 사용하여 꽤 좋은 신경망을 설계 할 수도 있지만 대부분의 neural network 설계에 있어서는 몇 개의 pooling 레이어와 완전히 연결된 레이어가 필요할 것입니다. 다행히, pooling layer와 완전히 연결된 레이어는 콘볼루션 레이어를 정의 내리는 것보다는 좀 더 간단합니다 따라서 우리는 다음 두 강좌에서 이를 신속하게 배워보고 컨볼루션 신경망에서 가장 보편적인 유형의 모든 레이어에 대한 감각을 가지게 될 것입니다. 그리고 나면 우리가 방금 본 것보다 더 강력한 네트워크를 만들게 될 것입니다. 첫 번째 컨볼루션 신경망 강의를 전부 다 보신 것을 다시 한번 축하 드립니다 또한 이번 주에 이 네트워크를 훈련하는 법에 대해서도 이야기 할 것입니다. 먼저 pooling과 완전히 연결된 레이어에 대해 간단히 이야기합시다 이 훈련을 위해서 여러분에게 이미 익숙한 역 전파를 사용할 것입니다. 다음 비디오에서는 컨볼루션 신경망에 대한 pooling 레이어를 구현하는 방법을 빠르게 알아 보겠습니다.