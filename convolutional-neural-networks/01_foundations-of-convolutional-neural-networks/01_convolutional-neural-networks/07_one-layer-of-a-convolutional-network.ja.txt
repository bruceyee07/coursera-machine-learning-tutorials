畳み込みニューラルネットワークの層をどうやって作るか 見る準備ができただろう 例から始めよう 前のビデオでは ３Dボリュームを畳み込む方法を見てきた 例えば ２つの異なるフィルターを使ってね この例では 異なる 4 x 4 出力を得るためにね じゃぁ 最初のフィルターで畳み込むと 4 x 4 出力を得る ２番目のフィルターで畳み込むと 異なる 4 x 4 出力を得る これを 畳み込みニューラルネットワーク層に組み込むために 最後にするのは それぞれに バイアスを加えることだ これは 実数になる Pythonブロードキャスティングを使うと 同じ数を これら16個の要素それぞれに 足すことができる そして 非線形性を加えるために この括弧()に ReLU非線形を行い 4 x 4 出力を得る いいね？ バイアスを加えた後に 非線形化する そして これを 下にも行う 異なるバイアスを加え これは実数だ 同じ実数を 全ての16個の数に加える そして 何らかの非線形化を行う 例えばReLU非線形化だ こうして 異なる 4 x 4 出力を得る そして 前と同様 これを このように重ねて 最後に 4 x 4 x 2 出力を得る この 6 x 6 x 3 から 4 x 4 x 4(訳注:2の言い間違い) を得る計算 これが 畳み込みニューラルネットワークの1つの層だ それでは 通常の 畳み込みでない ニューラルネットワークの順伝播に この１層をマップしてみよう 思い出して 伝播の前に行うステップは こんなだったよね？ z[1] = w[1] * a[0] で  a[0] はまた x と同じでもある そして + b[1] そして 非線形化した a[1] を得るのに g(z[1]) とする ここの入力は この類似では a[0] となり これらのフィルターは w[1] に似た役割となる 覚えているだろうけど 畳み込み処理では 27個の数を使った 実際には 27 x 2 だった 2つフィルターがあったから これらの数全てを取り 掛け算を行う つまり この 4 x 4 行列を得るのに使ったのは 線形関数の計算だ よって 畳み込み処理の結果 4 x 4 行列 は w[1] * a[0] と似た役割を行う これは この 4 x 4 も この 4 x 4 も同様だ そして 他に行う処理 バイアスを加える ここ ReLU を掛ける前のものは z と似た役割を行う そして 最後に非線形化する こんな感じだ よって この出力は 次の層への活性化の役割を行う これが どのように a[0] から a[1] を得るかだ 線形処理と 畳み込みが これら全てを行う 畳み込みは 実際には 線形処理を行う そして バイアスを足して ReLU処理を行う 6 x 6 x 3 次元の a[0] から ニューラルネットワークの１層を通して 4 x 4 x 2 次元の a[1] を得た 6 x 6 x 3 が 4 x 4 x 2 になった これが 畳み込みニューラルネットワークの1つの層だ この例には ２つのフィルターがある つまり ２つの特徴がある だから 出力が 4 x 4 x 2 となった しかし もし例えば ２つではなく10個のフィルターがあったなら 4 x 4 x 10 次元の出力ボリュームとなるだろう なぜなら これらの行列が10個あることになり それらを積み重ねて 4 x 4 x 10 出力ボリュームになるからね
それがa[1]だ それでは 理解したか確認するため 演習を行おう 10個のフィルターがあるとする ２つではなくね
それは 3 x 3 x 3 で １層のニューラルネットワークだ この層には いくつのパラメータがあるだろうか？ さぁ 当ててみて 各フィルターは 3 x 3 x 3 ボリュームだ そう 3 x 3 x 3 各フィルターは 27個のパラメータを持つ いいね？ 実装すべき27個の数がある
そして バイアスが足される それは b パラメータ そう これは28個のパラメータだ 前のスライドでのことを想像してみて
２つのフィルターを描いたよね でも 今度は 10個あると想像してね いい？ 1, 2, ..., 10 よって 28 x 10 で 280パラメータだ 今やったことについて 1つ良いことに気付いたかな
どんなに入力画像が大きくても 入力画像は 1000 x 1000 や 5000 x 5000 になり得るけど パラメータの数は 以前として280のままだ そして この10個のフィルターを使って 特徴を検出できる 垂直エッジや 水平エッジや その他の特徴をどこにでも とてもとても大きい画像であっても とても少ないパラメータでね そう 畳み込みニューラルネットワークの1つの特徴に 過学習し難いというのがある つまり 一度10個の特徴検出器が働くように学習できたなら それを 大きな画像にも適用できる そして パラメータ数は固定で 比較的小さい この例では280個だ よろしい このビデオを要約すると
記法をまとめよう 畳み込みニューラルネットワークの畳み込み層を記述する 1層を記述するための記法を 基本構成要素だ 層 l(エル) は 畳み込み層だ f 上付き文字 [l] はフィルターサイズを表す これまでは フィルターは f x f だった この 上付き角括弧は これが f x f サイズの 層ｌ(エル)のフィルターであることを示す これまでの同じく 上付き角括弧は 特定の層を意味する p[l] は パディング量を表す もう一度言うけど パディング量は Valid畳み込み これはパディング無しのこと と指定することもできるし Same畳み込み これは出力サイズが 入力サイズの高さと幅が同じになるように パディングする と指定することもできる そして s[l] はストライドを表す この層への入力は 何次元かになる それは n x n x 前の層のチャンネル数 だ ここで この記法を少しだけ修正したい 上付き文字 l - 1 を使う なぜなら これは 前の層の活性化の結果だから l - 1, 掛ける nC[l - 1] この例では ここまで 同じ高さと幅を持つ画像を使ってきた でも 高さと幅は 異なる場合もある 下付き文字 H と下付き文字 Wを 前層の高さと幅を
(訳注:実際には上付き文字と言っているが 下付き文字とした) 表すのに使う いいね？ よって 層ｌでは ボリュームサイズは nH x nW x nC の上付き角括弧ｌとなる これが 層ｌだ この層への入力は 何であれ 前層のであり だから ここに l - 1 を付けた それから ニューラルネットワークの層の出力は nH[l] x nW[l] x nC[l] これが 出力サイズだ それから 前に言ったように 出力ボリュームサイズは 高さと幅を使った この等式で与えられる n + 2p -f / s +1 そして全体を切り捨て丸める この新しい表記では 層ｌの出力値は 前層の次元に この層ｌのパディングを足して この層ｌのフィルターサイズを引き ここも同じ これは高さについてだ いい？ 出力ボリュームの高さは これで与えられ 計算できる この右の式で
そして 同じことが幅についても言える Hを消して Wを置けば 同じ式が 高さにも幅にも使え 出力値の高さと幅を計算できる つまり nH[l] は nH[l-1] と関係し nW[l] は nW[l-1] と関係する では チャンネル数については どうだろう
これらの数はどこからくるか？ 見てみよう もし出力ボリュームが この深さを持つなら 前の例から分かるように これは この層で持つフィルター数と等しい ね？ ２つのフィルターがあったから 出力値は 4 x 4 x 2 次元になった もし10個のフィルターがあれば 出力ボリュームは 4 x 4 x 10 だ だから このチャンネル数が 出力値では ニューラルネットワークのこの層で使うフィルター数となる 次に フィルターのサイズはどうだろうか？ 各フィルターは f[l] x f[l] x ある数 だけど 最後の数は何でしょう？ 6 x 6 x 3 画像を畳み込むには 3 x 3 x 3 フィルターが必要だった つまり フィルターのチャンネル数は 入力チャンネル数と一致しなければならない だから この数は この数と一致しなくてはならない だから 各フィルターは f[l] x f[l] x nC[l-1] となる また この層の出力は バイアスと非線形化が適用され この層の活性化値 a[l] になる これが 既に見てきたこの次元でしょ？ a[l] は ３Dボリュームで nH[l] x nW[l] x nC[l] そして ベクトル化した実装や バッチ勾配降下や ミニパッチ勾配降下を使う場合 実際の出力 a[l] は m 個のサンプルがあるなら m 個の活性化のセットになる つまり m x nH[l] x nW[l] x nC[l] だ バッチ勾配降下法ではそうなる そして プログラミング演習では 次元..変数の順序についてだけど 学習サンプルのインデックスを最初にして それから ３つの変数を置く 次は 重み もしくは パラメータについてだ
wパラメータのようなものはどうなる？ 既に フィルター次元が何かは見た つまり フィルターは f[l] x f[l] x nC[l-1] ただし これは１つのフィルターのだ どのくらいの数のフィルターがある？ これがフィルターの総数だ 重みは 全てのフィルターを集め これで与えられる次元を持つ 掛ける 総フィルター数 なぜなら 最後の数は 層ｌのフィルター数だから そして 最後に バイアスパラメータがある 各フィルターに 1つのバイアスパラメータ 1つの実数 を持つ よって バイアスは この数の変数となる この次数の 単なるベクトルだ ただし 後で見るように コードを もっと便利にするため 1 x 1 x 1 x nC[l] の 4次元行列 もしくは 4次元テンソルとして表現する 沢山の表記が出てきた これが ほとんどのパートで使う表記だ 言っておきたいけど もし オンライン検索で オープンソースコードを探す場合 完全などこでも通じる標準の順序規約は存在しない 高さ 幅 チャンネルについてのね だから もし GitHubのソースコードや これらのオープンソース実装を見れば ある人は あなたが チャンネルを最初にする所で この順序を使っているのを発見するだろう 時々そんな変数の順番を見るだろう 実際に いつくかの標準的なフレームワークでは 実際に複数の標準的フレームワークでは 変数やパラメータは チャンネル数を最初に持ってきたり チャンネル数を最後に持ってきたりする これらのボリュームを表記するのに どの規約を使っても 一貫性を保っていれば ちゃんと機能する 残念なことに これは ディープラーニング文献で意見が一致していない表記法の1つだ しかし このビデオでは この規約を使う 高さ そして 幅 そして 最後に チャンネル数 だ 導入すべき新しい表記が突然出てきて わぁお 表記がたくさんある どうやって全てを覚えるの とか思うでしょう 心配しないで この表記全てを覚える必要は無い 今週の演習を通して その時には もっとそれらに詳しくなるから でも このビデオから得て欲しい鍵は 畳み込みニューラルネットワークの層がどのように機能するのかということだ ある層で活性化を行うための計算と それを次の層の活性化にマップするということだ そして次は 畳み込みニューラルネットワークの１つの層がどのように機能するか分かったところで これらを複数まとめて 実際に深い 畳み込みニューラルネットワークを作る 次のビデオに進もう