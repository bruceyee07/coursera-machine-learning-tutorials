1
00:00:03,945 --> 00:00:07,666
畳み込みニューラルネットワークの層をどうやって作るか 見る準備ができただろう

2
00:00:07,666 --> 00:00:09,223
例から始めよう

3
00:00:12,432 --> 00:00:17,146
前のビデオでは ３Dボリュームを畳み込む方法を見てきた

4
00:00:17,146 --> 00:00:20,670
例えば ２つの異なるフィルターを使ってね

5
00:00:21,810 --> 00:00:25,861
この例では 異なる 4 x 4 出力を得るためにね

6
00:00:30,891 --> 00:00:35,548
じゃぁ 最初のフィルターで畳み込むと

7
00:00:35,548 --> 00:00:40,769
4 x 4 出力を得る

8
00:00:40,769 --> 00:00:49,108
２番目のフィルターで畳み込むと 異なる 4 x 4 出力を得る

9
00:00:49,108 --> 00:00:55,574
これを 畳み込みニューラルネットワーク層に組み込むために 最後にするのは

10
00:00:55,574 --> 00:01:00,566
それぞれに バイアスを加えることだ

11
00:01:00,566 --> 00:01:03,980
これは 実数になる

12
00:01:03,980 --> 00:01:08,840
Pythonブロードキャスティングを使うと 同じ数を

13
00:01:08,840 --> 00:01:11,750
これら16個の要素それぞれに 足すことができる

14
00:01:11,750 --> 00:01:16,805
そして 非線形性を加えるために この括弧()に

15
00:01:16,805 --> 00:01:23,430
ReLU非線形を行い 4 x 4 出力を得る いいね？

16
00:01:23,430 --> 00:01:27,295
バイアスを加えた後に 非線形化する

17
00:01:27,295 --> 00:01:31,894
そして これを 下にも行う 異なるバイアスを加え

18
00:01:31,894 --> 00:01:33,154
これは実数だ

19
00:01:33,154 --> 00:01:36,473
同じ実数を 全ての16個の数に加える

20
00:01:36,473 --> 00:01:40,934
そして 何らかの非線形化を行う 例えばReLU非線形化だ

21
00:01:40,934 --> 00:01:47,369
こうして 異なる 4 x 4 出力を得る

22
00:01:47,369 --> 00:01:52,461
そして 前と同様 これを このように重ねて

23
00:01:52,461 --> 00:01:59,698
最後に 4 x 4 x 2 出力を得る

24
00:01:59,698 --> 00:02:06,326
この 6 x 6 x 3 から 4 x 4 x 4(訳注:2の言い間違い) を得る計算

25
00:02:06,326 --> 00:02:11,303
これが 畳み込みニューラルネットワークの1つの層だ

26
00:02:11,303 --> 00:02:15,634
それでは 通常の 畳み込みでない ニューラルネットワークの順伝播に

27
00:02:15,634 --> 00:02:18,832
この１層をマップしてみよう

28
00:02:18,832 --> 00:02:23,155
思い出して 伝播の前に行うステップは こんなだったよね？

29
00:02:23,155 --> 00:02:28,461
z[1] = w[1] * a[0] で  a[0] はまた x と同じでもある

30
00:02:28,461 --> 00:02:31,265
そして + b[1]

31
00:02:31,265 --> 00:02:38,121
そして 非線形化した a[1] を得るのに g(z[1]) とする

32
00:02:38,121 --> 00:02:43,400
ここの入力は この類似では a[0] となり

33
00:02:44,770 --> 00:02:48,076
これらのフィルターは

34
00:02:48,076 --> 00:02:52,488
w[1] に似た役割となる

35
00:02:52,488 --> 00:02:56,377
覚えているだろうけど 畳み込み処理では

36
00:02:56,377 --> 00:03:01,149
27個の数を使った 実際には 27 x 2 だった 2つフィルターがあったから

37
00:03:01,149 --> 00:03:03,900
これらの数全てを取り 掛け算を行う

38
00:03:03,900 --> 00:03:09,631
つまり この 4 x 4 行列を得るのに使ったのは 線形関数の計算だ

39
00:03:09,631 --> 00:03:15,048
よって 畳み込み処理の結果 4 x 4 行列 は

40
00:03:15,048 --> 00:03:19,245
w[1] * a[0] と似た役割を行う

41
00:03:19,245 --> 00:03:25,340
これは この 4 x 4 も この 4 x 4 も同様だ

42
00:03:25,340 --> 00:03:29,599
そして 他に行う処理 バイアスを加える

43
00:03:29,599 --> 00:03:35,138
ここ ReLU を掛ける前のものは

44
00:03:35,138 --> 00:03:38,939
z と似た役割を行う

45
00:03:38,939 --> 00:03:43,848
そして 最後に非線形化する こんな感じだ

46
00:03:43,848 --> 00:03:49,740
よって この出力は

47
00:03:49,740 --> 00:03:53,390
次の層への活性化の役割を行う

48
00:03:53,390 --> 00:03:58,794
これが どのように a[0] から a[1] を得るかだ

49
00:03:58,794 --> 00:04:02,192
線形処理と 畳み込みが これら全てを行う

50
00:04:02,192 --> 00:04:05,507
畳み込みは 実際には 線形処理を行う

51
00:04:05,507 --> 00:04:08,437
そして バイアスを足して ReLU処理を行う

52
00:04:08,437 --> 00:04:14,107
6 x 6 x 3 次元の a[0] から

53
00:04:14,107 --> 00:04:18,210
ニューラルネットワークの１層を通して

54
00:04:18,210 --> 00:04:22,693
4 x 4 x 2 次元の a[1] を得た

55
00:04:22,693 --> 00:04:27,144
6 x 6 x 3 が 4 x 4 x 2 になった

56
00:04:27,144 --> 00:04:30,860
これが 畳み込みニューラルネットワークの1つの層だ

57
00:04:33,697 --> 00:04:40,504
この例には ２つのフィルターがある つまり ２つの特徴がある

58
00:04:40,504 --> 00:04:45,270
だから 出力が 4 x 4 x 2 となった

59
00:04:45,270 --> 00:04:49,248
しかし もし例えば ２つではなく10個のフィルターがあったなら

60
00:04:49,248 --> 00:04:54,396
4 x 4 x 10 次元の出力ボリュームとなるだろう

61
00:04:54,396 --> 00:05:00,334
なぜなら これらの行列が10個あることになり それらを積み重ねて

62
00:05:00,334 --> 00:05:05,598
4 x 4 x 10 出力ボリュームになるからね
それがa[1]だ

63
00:05:05,598 --> 00:05:09,630
それでは 理解したか確認するため 演習を行おう

64
00:05:09,630 --> 00:05:14,655
10個のフィルターがあるとする ２つではなくね
それは 3 x 3 x 3 で

65
00:05:14,655 --> 00:05:19,600
１層のニューラルネットワークだ この層には いくつのパラメータがあるだろうか？

66
00:05:21,000 --> 00:05:22,984
さぁ 当ててみて

67
00:05:22,984 --> 00:05:29,439
各フィルターは 3 x 3 x 3 ボリュームだ そう 3 x 3 x 3

68
00:05:29,439 --> 00:05:35,318
各フィルターは 27個のパラメータを持つ いいね？

69
00:05:35,318 --> 00:05:39,800
実装すべき27個の数がある
そして バイアスが足される

70
00:05:42,210 --> 00:05:46,818
それは b パラメータ そう これは28個のパラメータだ

71
00:05:50,125 --> 00:05:54,456
前のスライドでのことを想像してみて
２つのフィルターを描いたよね

72
00:05:54,456 --> 00:05:58,329
でも 今度は 10個あると想像してね いい？

73
00:05:58,329 --> 00:06:01,747
1, 2, ..., 10

74
00:06:01,747 --> 00:06:06,942
よって 28 x 10 で

75
00:06:06,942 --> 00:06:10,753
280パラメータだ

76
00:06:10,753 --> 00:06:16,316
今やったことについて 1つ良いことに気付いたかな
どんなに入力画像が大きくても

77
00:06:16,316 --> 00:06:22,051
入力画像は 1000 x 1000 や 5000 x 5000 になり得るけど

78
00:06:22,051 --> 00:06:26,973
パラメータの数は 以前として280のままだ

79
00:06:26,973 --> 00:06:31,453
そして この10個のフィルターを使って 特徴を検出できる 垂直エッジや

80
00:06:31,453 --> 00:06:35,485
水平エッジや その他の特徴をどこにでも

81
00:06:35,485 --> 00:06:39,240
とてもとても大きい画像であっても とても少ないパラメータでね

82
00:06:40,920 --> 00:06:44,410
そう 畳み込みニューラルネットワークの1つの特徴に

83
00:06:44,410 --> 00:06:48,000
過学習し難いというのがある

84
00:06:48,000 --> 00:06:51,450
つまり 一度10個の特徴検出器が働くように学習できたなら

85
00:06:51,450 --> 00:06:54,770
それを 大きな画像にも適用できる

86
00:06:54,770 --> 00:06:58,300
そして パラメータ数は固定で 比較的小さい

87
00:06:58,300 --> 00:07:00,494
この例では280個だ

88
00:07:00,494 --> 00:07:05,130
よろしい このビデオを要約すると
記法をまとめよう

89
00:07:05,130 --> 00:07:09,766
畳み込みニューラルネットワークの畳み込み層を記述する 1層を記述するための記法を

90
00:07:09,766 --> 00:07:11,980
基本構成要素だ

91
00:07:11,980 --> 00:07:14,302
層 l(エル) は 畳み込み層だ

92
00:07:14,302 --> 00:07:18,555
f 上付き文字 [l] はフィルターサイズを表す

93
00:07:18,555 --> 00:07:23,219
これまでは フィルターは f x f だった

94
00:07:23,219 --> 00:07:28,163
この 上付き角括弧は

95
00:07:28,163 --> 00:07:31,074
これが f x f サイズの 層ｌ(エル)のフィルターであることを示す

96
00:07:31,074 --> 00:07:35,767
これまでの同じく 上付き角括弧は

97
00:07:35,767 --> 00:07:37,611
特定の層を意味する

98
00:07:39,812 --> 00:07:42,809
p[l] は パディング量を表す

99
00:07:42,809 --> 00:07:47,363
もう一度言うけど パディング量は

100
00:07:47,363 --> 00:07:50,135
Valid畳み込み これはパディング無しのこと と指定することもできるし

101
00:07:50,135 --> 00:07:53,240
Same畳み込み これは出力サイズが 入力サイズの高さと幅が同じになるように

102
00:07:53,240 --> 00:07:57,910
パディングする と指定することもできる

103
00:07:59,000 --> 00:08:01,590
そして s[l] はストライドを表す

104
00:08:03,250 --> 00:08:09,450
この層への入力は 何次元かになる

105
00:08:09,450 --> 00:08:18,590
それは n x n x 前の層のチャンネル数 だ

106
00:08:18,590 --> 00:08:21,162
ここで この記法を少しだけ修正したい

107
00:08:21,162 --> 00:08:25,385
上付き文字 l - 1 を使う

108
00:08:25,385 --> 00:08:29,902
なぜなら これは 前の層の活性化の結果だから

109
00:08:29,902 --> 00:08:35,594
l - 1, 掛ける nC[l - 1]

110
00:08:35,594 --> 00:08:40,966
この例では ここまで 同じ高さと幅を持つ画像を使ってきた

111
00:08:40,966 --> 00:08:43,990
でも 高さと幅は 異なる場合もある

112
00:08:43,990 --> 00:08:48,528
下付き文字 H と下付き文字 Wを 前層の高さと幅を
(訳注:実際には上付き文字と言っているが 下付き文字とした)

113
00:08:48,528 --> 00:08:51,949
表すのに使う いいね？

114
00:08:51,949 --> 00:08:56,418
よって 層ｌでは ボリュームサイズは

115
00:08:56,418 --> 00:09:01,134
nH x nW x nC の上付き角括弧ｌとなる

116
00:09:01,134 --> 00:09:05,597
これが 層ｌだ この層への入力は 何であれ

117
00:09:05,597 --> 00:09:09,451
前層のであり だから ここに l - 1 を付けた

118
00:09:09,451 --> 00:09:16,730
それから ニューラルネットワークの層の出力は

119
00:09:16,730 --> 00:09:23,065
nH[l] x nW[l] x nC[l]

120
00:09:23,065 --> 00:09:28,495
これが 出力サイズだ

121
00:09:28,495 --> 00:09:34,941
それから 前に言ったように 出力ボリュームサイズは

122
00:09:34,941 --> 00:09:40,657
高さと幅を使った この等式で与えられる

123
00:09:40,657 --> 00:09:47,971
n + 2p -f / s +1 そして全体を切り捨て丸める

124
00:09:47,971 --> 00:09:55,605
この新しい表記では 層ｌの出力値は

125
00:09:55,605 --> 00:10:00,891
前層の次元に

126
00:10:00,891 --> 00:10:05,471
この層ｌのパディングを足して

127
00:10:05,471 --> 00:10:11,760
この層ｌのフィルターサイズを引き ここも同じ

128
00:10:11,760 --> 00:10:16,580
これは高さについてだ いい？

129
00:10:16,580 --> 00:10:21,510
出力ボリュームの高さは これで与えられ 計算できる

130
00:10:21,510 --> 00:10:24,680
この右の式で
そして 同じことが幅についても言える

131
00:10:24,680 --> 00:10:26,670
Hを消して

132
00:10:26,670 --> 00:10:30,780
Wを置けば 同じ式が

133
00:10:30,780 --> 00:10:34,775
高さにも幅にも使え 出力値の高さと幅を計算できる

134
00:10:36,570 --> 00:10:44,024
つまり nH[l] は nH[l-1] と関係し nW[l] は nW[l-1] と関係する

135
00:10:44,024 --> 00:10:48,105
では チャンネル数については どうだろう
これらの数はどこからくるか？

136
00:10:48,105 --> 00:10:52,784
見てみよう もし出力ボリュームが

137
00:10:52,784 --> 00:10:57,662
この深さを持つなら 前の例から分かるように これは

138
00:10:57,662 --> 00:11:02,167
この層で持つフィルター数と等しい ね？

139
00:11:02,167 --> 00:11:07,017
２つのフィルターがあったから 出力値は 4 x 4 x 2 次元になった

140
00:11:07,017 --> 00:11:11,097
もし10個のフィルターがあれば 出力ボリュームは 4 x 4 x 10 だ

141
00:11:11,097 --> 00:11:15,744
だから このチャンネル数が

142
00:11:15,744 --> 00:11:23,098
出力値では ニューラルネットワークのこの層で使うフィルター数となる

143
00:11:23,098 --> 00:11:26,849
次に フィルターのサイズはどうだろうか？

144
00:11:26,849 --> 00:11:33,057
各フィルターは f[l] x f[l] x ある数 だけど

145
00:11:33,057 --> 00:11:34,704
最後の数は何でしょう？

146
00:11:34,704 --> 00:11:39,465
6 x 6 x 3 画像を畳み込むには

147
00:11:39,465 --> 00:11:41,580
3 x 3 x 3 フィルターが必要だった

148
00:11:43,070 --> 00:11:48,150
つまり フィルターのチャンネル数は 入力チャンネル数と一致しなければならない

149
00:11:48,150 --> 00:11:54,360
だから この数は この数と一致しなくてはならない

150
00:11:54,360 --> 00:12:02,627
だから 各フィルターは f[l] x f[l] x nC[l-1] となる

151
00:12:02,627 --> 00:12:07,875
また この層の出力は バイアスと非線形化が適用され

152
00:12:07,875 --> 00:12:11,745
この層の活性化値 a[l] になる

153
00:12:11,745 --> 00:12:15,115
これが 既に見てきたこの次元でしょ？

154
00:12:15,115 --> 00:12:20,451
a[l] は ３Dボリュームで

155
00:12:20,451 --> 00:12:25,556
nH[l] x nW[l] x nC[l]

156
00:12:25,556 --> 00:12:31,082
そして ベクトル化した実装や バッチ勾配降下や

157
00:12:31,082 --> 00:12:36,891
ミニパッチ勾配降下を使う場合 実際の出力 a[l] は

158
00:12:36,891 --> 00:12:41,387
m 個のサンプルがあるなら m 個の活性化のセットになる

159
00:12:41,387 --> 00:12:48,275
つまり m x nH[l] x nW[l] x nC[l] だ

160
00:12:48,275 --> 00:12:51,375
バッチ勾配降下法ではそうなる

161
00:12:51,375 --> 00:12:55,999
そして プログラミング演習では 次元..変数の順序についてだけど

162
00:12:55,999 --> 00:12:59,962
学習サンプルのインデックスを最初にして

163
00:12:59,962 --> 00:13:02,384
それから ３つの変数を置く

164
00:13:02,384 --> 00:13:07,618
次は 重み もしくは パラメータについてだ
wパラメータのようなものはどうなる？

165
00:13:07,618 --> 00:13:10,264
既に フィルター次元が何かは見た

166
00:13:10,264 --> 00:13:16,258
つまり フィルターは f[l] x f[l] x nC[l-1]

167
00:13:16,258 --> 00:13:20,230
ただし これは１つのフィルターのだ

168
00:13:20,230 --> 00:13:22,247
どのくらいの数のフィルターがある？

169
00:13:22,247 --> 00:13:25,097
これがフィルターの総数だ

170
00:13:25,097 --> 00:13:30,029
重みは 全てのフィルターを集め これで与えられる次元を持つ

171
00:13:30,029 --> 00:13:33,513
掛ける 総フィルター数

172
00:13:33,513 --> 00:13:38,625
なぜなら 最後の数は

173
00:13:38,625 --> 00:13:43,750
層ｌのフィルター数だから

174
00:13:45,680 --> 00:13:48,710
そして 最後に バイアスパラメータがある

175
00:13:48,710 --> 00:13:54,100
各フィルターに 1つのバイアスパラメータ 1つの実数 を持つ

176
00:13:54,100 --> 00:13:57,970
よって バイアスは この数の変数となる

177
00:13:57,970 --> 00:14:00,810
この次数の 単なるベクトルだ

178
00:14:00,810 --> 00:14:05,052
ただし 後で見るように コードを もっと便利にするため

179
00:14:05,052 --> 00:14:09,813
1 x 1 x 1 x nC[l] の

180
00:14:09,813 --> 00:14:14,790
4次元行列 もしくは 4次元テンソルとして表現する

181
00:14:16,430 --> 00:14:19,408
沢山の表記が出てきた

182
00:14:19,408 --> 00:14:23,303
これが ほとんどのパートで使う表記だ

183
00:14:23,303 --> 00:14:27,498
言っておきたいけど もし オンライン検索で オープンソースコードを探す場合

184
00:14:27,498 --> 00:14:32,311
完全などこでも通じる標準の順序規約は存在しない

185
00:14:32,311 --> 00:14:34,260
高さ 幅 チャンネルについてのね

186
00:14:34,260 --> 00:14:39,142
だから もし GitHubのソースコードや これらのオープンソース実装を見れば

187
00:14:39,142 --> 00:14:43,873
ある人は あなたが チャンネルを最初にする所で 

188
00:14:43,873 --> 00:14:48,631
この順序を使っているのを発見するだろう 時々そんな変数の順番を見るだろう

189
00:14:48,631 --> 00:14:52,154
実際に いつくかの標準的なフレームワークでは 実際に複数の標準的フレームワークでは

190
00:14:52,154 --> 00:14:54,155
変数やパラメータは

191
00:14:54,155 --> 00:14:57,652
チャンネル数を最初に持ってきたり

192
00:14:57,652 --> 00:15:02,000
チャンネル数を最後に持ってきたりする これらのボリュームを表記するのに

193
00:15:02,000 --> 00:15:08,137
どの規約を使っても 一貫性を保っていれば ちゃんと機能する

194
00:15:08,137 --> 00:15:13,049
残念なことに

195
00:15:13,049 --> 00:15:17,772
これは ディープラーニング文献で意見が一致していない表記法の1つだ

196
00:15:17,772 --> 00:15:21,752
しかし このビデオでは この規約を使う

197
00:15:24,681 --> 00:15:30,769
高さ そして 幅 そして 最後に チャンネル数 だ

198
00:15:30,769 --> 00:15:34,327
導入すべき新しい表記が突然出てきて

199
00:15:34,327 --> 00:15:38,027
わぁお 表記がたくさんある どうやって全てを覚えるの とか思うでしょう

200
00:15:38,027 --> 00:15:41,994
心配しないで この表記全てを覚える必要は無い

201
00:15:41,994 --> 00:15:46,036
今週の演習を通して その時には もっとそれらに詳しくなるから

202
00:15:46,036 --> 00:15:49,116
でも このビデオから得て欲しい鍵は

203
00:15:49,116 --> 00:15:52,694
畳み込みニューラルネットワークの層がどのように機能するのかということだ

204
00:15:52,694 --> 00:15:57,006
ある層で活性化を行うための計算と

205
00:15:57,006 --> 00:16:00,052
それを次の層の活性化にマップするということだ

206
00:16:00,052 --> 00:16:04,063
そして次は 畳み込みニューラルネットワークの１つの層がどのように機能するか分かったところで

207
00:16:04,063 --> 00:16:07,740
これらを複数まとめて 実際に深い

208
00:16:07,740 --> 00:16:09,040
畳み込みニューラルネットワークを作る

209
00:16:09,040 --> 00:16:10,200
次のビデオに進もう