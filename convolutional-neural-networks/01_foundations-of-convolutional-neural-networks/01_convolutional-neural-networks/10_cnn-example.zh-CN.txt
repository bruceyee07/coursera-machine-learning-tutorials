现在你已经了解了几乎所有的 用于创建一个完整的卷积神经网络的构件 我们来看个例子 假设输入图像大小为32*32*3 一个RGB图像，或许你试着做手写数字识别 例如，你有一个32*32 RGB图像 要尝试识别图像中是0-9中10个数字的哪一个是7 让我们创建一个神经网络来做这项工作 这里中我将要使用的算法 是受到了 一种和它非常相似的经典的神经网络的启发 称作LeNet-5 LeNet-5多年前由Yann LeCun创建 这里我将要展示的和LeNet-5不完全一样，但 其中许多参数的选择是受到它启发的 这里有一个32*32*3的输入，让我们用一个 5*5的过滤器，步长为1，无补丁生成第一层输出 那么第一层输出 是28*28*6，假设使用了6个过滤器 我们称这一层为conv 1 那么用了6个过滤器，加上偏差值，再应用非线性 或许ReLU非线性，得到了卷积层一的输出 接下来，让我们使用一个池化层 这里我将使用最大池化，应用参数f=2，s=2 如果我没有写补丁参数，就意味补丁为零 接下来让我们使用一个池化层 假设我们用一个2*2的过滤器，步长为2做最大池化 那么原有的高度和宽度 将会缩小一半 因此28*28现在变成了14*14 通道数量保持不变，所以大小为14*14*6 我们把这层称之为池1输出 事实上在卷积网文档中中有二种关于层的说法 二者有细微的差别 一种说法是这称为一层 那么这二个单元一起成为神经网络的第一层 另一种说法是卷积层称为一层，池化层单独称为一层 神经网络中当人们说到网络层数的时候，通常指 那些有权重，有参数的网络层数量 因为池化层没有权重，没有参数 只有一些超参数，我会使用 卷积层1和池化层1为一体的说法 把他们作为层一，尽管有时候 当你看在线文章或读研究论文时，你会听到卷积层 和池化层被称为两个独立层 但这不过是二种细微不一致的表示术语 这里当我计算层数，我只会算那些有权重的层 所以我视这二个一起为层一 并且Conv1和Pool1名字末尾是1 也表明了我认为二者都是神经网络层一的组成部分 因为池化层1没有自己的权重，所以被并入层一 接下来，在14*14*6的基础上，让我们 再做一层卷积， 这次使用大小为5*5的过滤器 并且步长为一，共10个过滤器 那现在你会得到一个10*10*10的单元 称之为卷积层二 然后我们继续做最大池化 再次使用池化参数f=2，s=2 或许你已经了解了在此参数下 输出的高度和宽度会减半 所以剩下的是5*5*10大小的 这一步被称为池化层2 按照我们的说法这二个合称为神经网络层二 接下来我们再使用一次卷积层 我将会使用5*5大小的过滤器，那么f等于5 假设步长为1，且不做填充 让我们使用16个过滤器，这样你会获得卷积层2的输出 也就是10*10*16大小的输出 如图所示，这是卷积层二 接着我们来做最大池化，假设f等于2，s等于2 你或许已经知道了结果 在10*10*16基础上做f等于2，s等于2的最大池化 将会使输入高度和宽度减半 你应该已经知道结果了，对吧 用f等于2，s等于2做池化 高度和宽度被减半，那么最终获得 一个5*5*16的单元，通道数保持不变 我们称这块为池化层二 按照我们的说法这块被称为层二 由于只有卷积层二有权重 这里5乘5乘16等于400 现在让我们把池化层二展开成一个400*1的向量 把它想象成如图所示的一组展开的神经元 接下来我们要做的是用着400个单元做输入 创建一个有120个单元的下一层 这实际上是我们第一个全连接网络层 我将称之为FC3 因为这400个输入单元和120输出单元密集的相连 因此这个全连接层 和你在课程1和课程2所见到的单神经网络层一样 这不过是一个标准神经网络，其中被称为w3的 权重矩阵大小为120*400 因为400个输入中的每一个单元都和120输出的每一个单元相连 所以被称为全连接网，并且这里还有个偏差参数 大小也将是120的向量，因为有120个输出 最后一步让我们在120的单元基础上再加一层 这次让它变得更小，假设我们有84个单元最终 我将它称为全连接网络层4 最终我们获得了可以用于Softmax层的84个实数 如果你是试着做手写数字识别 来确定是0，1，2，直到9中的数字 那这将是一个有着10个输出的Softmax层 这就是一个相对典型的例子 展示了一个卷积神经网络的大致构成 我理解这其中似乎有许多超参数 稍后我们将会给出一些更具体的建议 关于如何选择这类超参数 也许一个常用的法则实际上是 不要试着创造你自己的超参数组 而是查看文献，看看其他人使用的超参数 从中选一组适用于其他人的超参数 很可能它也适用于你的应用 下周我们会做更多介绍 目前我只想指出，通常随着神经网络的深入 高度nh和宽度nw会减小 如之前所示，从32*32减到20*20，14*14 再到10*10，最终减到5*5 因此当你深入下去通常高度和宽度将会减小 然而通道数量会增加 这里从3到6再到16，最后是全连通网络层 另一类常见的神经网络模型是 一个或多个卷积层接着一层池化层 再接着一个或多个卷积层叠加一层池化层 然后叠加几层全连接层 也许最后还叠加一个Softmax层 如上所述是另一个常见的神经网络模型 那么让我们再回顾一下 神经网络的一些细节 如激活输入的尺寸，大小和网络参数数量 这里输入为32*32*3 这些数字相乘应该等于3072 所以激活输入a0的大小为3072 实际尺寸是32*32*3 我认为输入层是没有参数的 再看看接下来的不同层数据 试着自己来算一算 如表所示，这是不同层的激活输入的尺寸 和大小 这里需要指出几点 首先，注意最大池化没有任何参数 其次，注意卷积层趋向于拥有越来越少的参数 正如我们在早前的视频中所述 实际上，多数参数 在神经网络的全连接层上 同时，随着神经网络的深入 你会发现激活输入大小也逐渐变小 如果减少的太快，通常也不利于网络性能 这里首先大小从6000减到1600 接下来慢慢减小到84，直到最终得到softmax层的输出 你会发现许多卷积神经网络有着 与此相似的特性和模式 到这里你已经了解了神经网络的基本构件 卷积神经网络，卷积层，池化层 和全连接层 如何利用这些基本构件来构造一个有效的神经网络 已经有许多计算机领域专家在深入研究了 把这些构件组合到一起实际上需要相当的洞察力 我认为最好的方法之一 就是去学习一定数量的实例，看看人家是如何做的 从中来获得灵感如何把这些构件组合在一起 所以除了你现在看到的第一个实例，下周我将给你展示 其他一些实例，看看大家是如何成功的用 这些构件来创建很高效的神经网络 通过下周的视频希望能帮到你 获得一些关于如何构建神经网络的自己的想法 而且既然已经给出了一些实例也许你可以直接 在自己的应用程序中使用他人已经开发的构架 下周我们再讲 在本周收尾前，最后一点我想提一下 接下来的视频我将浅谈一下为什么要用卷积 使用卷积的好处和优点 以及如何把他们组合在一起 如何通过训练集来训练一个神经网络，如之前所见 来做图像识别或者其他一些任务 那么接下来让我们看看本周的最后一个视频