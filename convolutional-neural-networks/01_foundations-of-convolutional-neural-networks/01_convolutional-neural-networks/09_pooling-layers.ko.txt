컨볼네트는 컨볼루션 레이어외에도 표현되는 크기를 줄이고, 계산 속도를 높이고 조금 더 견고하게 감지하는 기능을 만들기 위해 pooling 레이어를 사용합니다. 보시죠. pooling 의 예를 살펴본 다음, 이렇게 하면 왜 좋은지에 대해 이야기하겠습니다. 4 x 4 인풋이 있다고 가정해봅시다 max pooling이라고 하는 pooling 유형을 적용하려고 합니다. 이러한 특정한 max pooing의 아웃풋은 2 x 2 아웃풋이 될 것입니다. 이걸 하는 방법은 꽤 간단합니다. 4 x 4 입력을 받아 4 개의 다른 영역으로 나누십시오. 네 군데를 이렇게 색칠해보겠습니다. 그리고 나서, 아웃풋에서 이건 2 x 2 인데요, 각각의 아웃풋은 대응하는 다시 색칠된 영역으로부터 최대 값이 됩니다. 그래서 왼쪽 상단에, 이 네 숫자의 최대 값은 9입니다. 오른쪽 상단의 파란색 숫자의 최대 값은 2입니다 왼쪽 아래, 가장 큰 숫자는 6이고, 오른쪽 아래, 가장 큰 숫자는 3입니다. 따라서 오른쪽의 각 숫자를 계산할 때 이 2 x 2 영역에 대해 최대 값을 취했습니다. 이는 마치 두 개의 필터사이즈를 적용하는 것과 같은 것입니다. 2 x 2 영역을 취해서 스트라이드 2로  처리하고 있기 때문입니다. 이것은 최대 pooling의 하이퍼 파라미터입니다 왜냐하면 값이 9가 되는 2 x 2 영역의 이 필터사이즈로 시작하기 때문이죠. 그리고 나서, 이 영역을 위해 두 칸을 옮겨가면, 값은 2가 됩니다. 다음 행으로 옮겨 두 칸을 아래로 옮겨가면 값은 6이 됩니다. 그리고 다시 오른쪽으로 2칸 옮기면 3을 얻을 수 있죠. 정사각형이 2 x 2 이므로, f=2 스트라이드 = 2 이므로 s=2 입니다. 그래서 최대 pooling이하는 일에 대해 생각해봅시다. 이 4 x 4영역을 feature 세트로 간주해보면 신경망의 레이어의 activation은 큰 숫자가 될 것입니다. 이는 특정한 feature가 감지된다는 것을 의미합니다. 따라서 왼쪽 상단 사분 면에는 이 특별한 feature가 있습니다 이는 수직 모서리거나, 고양이를 감지하려는 거면 고양이의 눈 아니면 수염이 될 수 있습니다. 분명히 그 feature는 왼쪽 상단 사분 면에 존재합니다. 이 feature 자리에 있는 이것은 cat eye 감지기가 아닙니다. 이 feature 자리에 이것은 오른쪽 상단 사분 면에는 실제로 존재하지 않습니다. 따라서 max operation이 하는 일은 어디에서나 feature가 많이 감지되도록 하는 것이고, 이 사분면 중에 하나는 최대 pooling 아웃풋으로 그대로 남게 됩니다. 따라서 max가하는 일은 말하자면, 이 feature들이 이 필터 어디에서든 감지되면, 최대 값을 잡아두는 것입니다. 하지만 이 feature가 감지되지 않으면, 아마도 이 feature는 오른쪽 상단 사분 면에 존재하지 않을 겁니다. 그렇다면 그 숫자들의 최대 값은 여전히 아주 작습니다. 어쩌면 이것이 바로 max pooling의 직관력입니다. 하지만 사람들이 max pooling을 사용하는 주요 이유는 잘 작동하는 많은 실험에서 발견되기 때문이라는 점은 인정해야 합니다. 그리고 방금 설명했던 직관력 때문 일 수도 있습니다. 종종 거론됨에도 불구하고 이게 근본원인인지 아닌지 확실히 아는 사람은 본 적이 없습니다. max pooling이 컨볼네트에서 잘 작동되는 진짜 근본적인 이유를 아는 사람은 없습니다. max pooling의 흥미로운 속성 중 하나는 하이퍼 파라미터 집합은 가지고 있지만, 배울 파라미터는 없다는 것입니다. 기울기 강하를 사실 배울 것은 없습니다. 일단 f와 s를 고정시키면, 그것은 이미 고정 된 계산이고, 기울기 강하는 아무 것도 변화시키지 않습니다. 다른 하이퍼 파라미터로 예제를 살펴 보겠습니다. 여기서는 5 x 5 인풋을 사용하고 필터 크기가 3 x 3 인 최대 pooling을 적용 할 것입니다 그래서 f=3 이고, 스트라이드=1 을 사용합시다 이 경우에 아웃풋 크기는 3 x 3이 될 것입니다. 컨볼 레이어의 아웃풋 사이즈를 파악하기 위해 이전 동영상에서 발전시켰던 수식은 최대 pooling에서도 작동합니다. 그 수식은 n + 2p - f 나누기 s 를 하고 여기에 1을 더하는 것입니다 이 수식은 또한 max pooling의 아웃풋 사이 지를 파악하는 데에도 사용할 수 있습니다. 그러나 이 예제에서, 이 3 x 3 아웃풋의 각 원소를 계산해 봅시다. 왼쪽 상단 요소, 80 우리는 그 영역을 살펴볼 것입니다. 필터 크기가 3이고 최대 값이기 때문에 3 x 3 영역이라는 걸 기억하세요. 그럼, 이것은 9가 되겠죠. 그리고 나서 스트라이드=1 이니까 한 칸 옮기십시오, 그러면 파란 상자의 최대값은 9가 됩니다. 한 번 더 옮깁시다. 파란 상자의 최대 값은 5입니다. 그리고 다음 행으로 넘어 가서, 스트라이드=1 이렇게 한 칸씩 아래로 내려가면 됩니다. 이 영역의 최대 값은 9입니다, 이 영역의 최대 값은 9입니다 이 영역의 최대값은, 5가 두 개가 있군요, 최대값 5가 있습니다. 마지막으로, 여기서 최대값은 8 입니다. 여기서 최대값은 6이고, 여기서 최대값은 9입니다. 좋아요, 그래서, 이 하이퍼 파라미터 집합 f=3 인 경우 여기 보이는 아웃풋대로 s=1입니다. 이제까지는 2D 인풋상의 최대 pooling을 보여드렸습니다. 3D 인풋이 있으면 아웃풋 또한 동일한 차수를 가지게 될 것입니다. 예를 들어, 5 x 5 x 2 이미지를 가지고 있고 , 아웃풋이 3 x 3 x 2 이면, 최대 pooling을 계산하는 방법은 방금 설명한 계산을 각 채널에서 독립적으로 수행하는 것입니다. 따라서 위에 표시된 첫 번째 채널은 여전히 동일합니다. 두 번째 채널의 경우, 방금 그 아래에 그린 이것은 이 슬라이스에 있는 값들에 대해서도 동일한 계산을 하면 됩니다. 그러면 이 두 번째 슬라이스를 완성할 수 있습니다. 그리고 좀 더 일반적으로, 만약 이것이 5 x 5 x 채널 개수 라면 아웃풋은 3 x 3 x 동일한 채널 수 가 될 것입니다. max pooling computation은 이 각각의 nC 채널상에서 독립적으로 행해집니다. 자, 이게 max pooling입니다. 이것은 자주 사용되지 않는 유형의 pooling이긴 하나, 평균 pooling이라는 것에 대해 간단히 언급 할 것입니다. 따라서 이것은 여러분이 많이 기대하는 부분일 텐데요, 각 필터 내에서 최대 값을 취하는 대신 평균값을 취하는 것입니다. 따라서 이 예에서 보라색 숫자의 평균은 3.75이고, 그 다음에는 1.25가 있죠. 그리고 4와 2가 있습니다 그래서 이것은 하이퍼 파라미터 f=2, s=2 인 평균 pooling입니다. 다른 하이퍼 파라미터를 선택해도 괜찮습니다. 그래서 요즘에는 최대 pooling이 평균 pooling보다 훨씬 더 자주 사용되는데, 때때로 신경망에 깊이 들어가면 예외적 인 경우도 있습니다. 평균 pooling을 사용하여 표현을 7 x 7 x 1000 로 축소 할 수 있습니다. 모든 공간에 있는 것을 평균을 내면 1 x 1 x 1000이 됩니다. 이 예시를 나중에 보도록 하겠습니다. 하지만 보시다시피, max pooling은 average pooling 보다 신경망에서 훨씬 더 많이 사용됩니다 요약해보자면, pooling에 대한 하이퍼 파라미터는 필터 크기는 f, 스트라이드는 s, 아마도 파라미터의 일반적인 선택은 f=2, s=2 가 되겠죠 이것은 아주 자주 사용되고 2배 이상으로 높이와 넓이를 대략적으로 줄이는 효과를 가지고 있습니다. 자주 선택되는 하이퍼 파라미터는 f=2, s=2이고 이것 역시 표현되는 높이와 넓이를 2배 이상으로 줄여주는 효과를 가지고 있습니다. f=3, s=2 가 사용되는 것도 보셨습니다. 또 다른 하이퍼 파라미터는 약간 이진법 같은 것인데요, 최대 pooling을 사용하거나 평균 pooling을 사용하는 것입니다. 원하시면, 패딩하면서 또 하나의 하이퍼 파라미터를 추가하실 수 있습니다 아주 아주 뜸하게 사용되긴 하지만 말이죠. 보통 최대 pooling 하실 때, 패딩은 사용하지 않습니다. 한 가지 예외 경우는 다음 주에 살펴보도록 하겠습니다. 그러나 max pooling의 대부분은 대개 어떤 패딩도 사용하지 않습니다. 따라서 p의 가장 일반적인 값은 p=0 입니다. 그리고 최대 pooling의 인풋은 nC x nH x nH 크기의 볼륨을 입력하는 것입니다. 그러면 이렇게 주어진 크기의 볼륨으로 결과물이 나오겠죠. 그래서 패딩이 없다고 가정하고, 거기에 x nW-f / s 를 합니다. 그리고 x nC 하면 되겠죠. 따라서 인풋 채널의 수는 아웃풋 채널 수와 동일합니다. 왜냐하면 pooling은 각 채널에 독립적으로 적용되기 때문입니다. pooling에 대해 주의해야 할 점은 배울 파라미터가 없다는 것입니다. 그래서 우리가 잘라내기를 실행할 때, 배경이 최대 pooling을 통해 조정 되 야할 파라미터가 없다는 걸 알게 되죠. 대신에, 여러분이 일단 설정한 이러한 하이퍼 파라미터 변수가 있습니다. 수동으로 혹은 교차 검증을 이용해서 설정된 것이겠죠. 그렇게 하고 나면, 끝난 것입니다. 이것이 신경망에 레이어 에서 계산을 수행하는 정해 된 함수입니다. 사실 배울 것도 없죠. 이건 그냥 정해진 함수니까요. 이게 바로 pooling입니다. 이제 여러분은 컨볼루션 레이어와 pooling 레이어를 만드는 방법을 알게 되었습니다. 다음 비디오에서는 컨볼네트보다 복잡한 예를 살펴 보겠습니다. 이것이 완전히 연결된 레이어를 소개해 줄 것입니다.