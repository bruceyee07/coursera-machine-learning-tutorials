1
00:00:00,000 --> 00:00:02,335
除了卷積層外

2
00:00:02,335 --> 00:00:07,130
ConvNets 通常也使用池層來減少大小

3
00:00:07,130 --> 00:00:08,510
加快計算的速度

4
00:00:08,510 --> 00:00:12,020
同時也產生一些特徵讓偵測更加堅固

5
00:00:12,020 --> 00:00:16,390
我們來看看，我們用一個例子來看池層

6
00:00:16,390 --> 00:00:20,205
然後我們再來談為什麼要這樣做

7
00:00:20,205 --> 00:00:24,300
假設您有一個 4乘4  輸入

8
00:00:24,300 --> 00:00:28,675
而您想要應用一種稱為最大池化的池層類型

9
00:00:28,675 --> 00:00:30,320
而輸出

10
00:00:30,320 --> 00:00:34,375
在這特別的最大池化建置中會是 2乘2 輸出

11
00:00:34,375 --> 00:00:37,270
而您要做的其實很簡單

12
00:00:37,270 --> 00:00:40,310
拿您的 4乘4 輸入將其分解為

13
00:00:40,310 --> 00:00:44,280
不同的區域，我將用不同的顏色來表示這四個區域

14
00:00:44,280 --> 00:00:46,130
而在輸出

15
00:00:46,130 --> 00:00:47,480
也就是 2乘2

16
00:00:47,480 --> 00:00:53,240
每一個輸出就只是相對應區域的最大值

17
00:00:53,240 --> 00:00:54,680
所以在左上角

18
00:00:54,680 --> 00:00:57,900
這四個數字的最大值是  9

19
00:00:57,900 --> 00:01:01,760
在右上角，藍色部分最大值是 2

20
00:01:01,760 --> 00:01:04,273
左下角，最大的數字是 6

21
00:01:04,273 --> 00:01:08,050
右下角，最大的數字是   3

22
00:01:08,050 --> 00:01:10,737
所以要計算在右邊的每個數字

23
00:01:10,737 --> 00:01:13,400
我們取 2乘2 區域的最大值

24
00:01:13,400 --> 00:01:18,740
這個就像是您用了一個大小為 2 的過濾器

25
00:01:18,740 --> 00:01:25,290
因為您用了一個 2乘2 區域，然後您跨步為 2

26
00:01:25,290 --> 00:01:30,825
所以，實際上這些就是

27
00:01:30,825 --> 00:01:36,540
最大池化的超參數，
因為我們開始用這的過濾器大小

28
00:01:36,540 --> 00:01:39,650
像這個 2乘2 的區域，給您  9

29
00:01:39,650 --> 00:01:45,580
然後您跨過兩步，到這個區域，這給您 2

30
00:01:45,580 --> 00:01:46,880
接著下一行

31
00:01:46,880 --> 00:01:49,580
您往下走兩步，給您  6

32
00:01:49,580 --> 00:01:52,570
然後您往右走兩步，給您 3

33
00:01:52,570 --> 00:01:54,620
因為方塊是  2乘2, f 等於 2

34
00:01:54,620 --> 00:01:58,070
因為您每次跨兩步

35
00:01:58,070 --> 00:02:00,210
s 等於  2

36
00:02:00,210 --> 00:02:09,526
最大池化背後的一些直觀是

37
00:02:09,526 --> 00:02:15,050
如果您想像這個 4乘4 區域是一組特徵

38
00:02:15,050 --> 00:02:19,204
在神經網路的一些層的啟動值

39
00:02:19,204 --> 00:02:20,490
那一個大的數字

40
00:02:20,490 --> 00:02:23,670
它意思是或許偵測一種特別的特徵

41
00:02:23,670 --> 00:02:26,495
所以在左上角的這個象限有這個特別的特徵

42
00:02:26,495 --> 00:02:32,470
它或許是一個垂直邊緣，或許是眼睛 ,
也許是鬍鬚，如果您試著偵測貓

43
00:02:32,470 --> 00:02:34,820
很明顯的，這個特徵出現在左上角象限

44
00:02:34,820 --> 00:02:40,055
而這個特徵，或許是貓眼的偵測

45
00:02:40,055 --> 00:02:43,975
而這個特徵，並不真的存在於右上角象限

46
00:02:43,975 --> 00:02:47,764
所以最大化運算做的是
偵測任何地方很多的特徵

47
00:02:47,764 --> 00:02:53,504
而任何一個象限，它保有了最大池的輸出

48
00:02:53,504 --> 00:02:56,265
所以最大化運算真正說的是

49
00:02:56,265 --> 00:02:59,780
如果這個特徵在這個過濾器中任何地方偵測到

50
00:02:59,780 --> 00:03:01,348
那就保持最高的數值

51
00:03:01,348 --> 00:03:03,510
但如果這個特徵並沒被偵測到

52
00:03:03,510 --> 00:03:07,690
或許這個特徵並不存在於右上角象限

53
00:03:07,690 --> 00:03:11,090
那即使是所有這些數字的最大值，還是很小

54
00:03:11,090 --> 00:03:15,252
或許這就是最大池化背後的直觀

55
00:03:15,252 --> 00:03:16,535
但我必須承認

56
00:03:16,535 --> 00:03:19,550
我想人們使用最大池化的最主要原因是

57
00:03:19,550 --> 00:03:23,627
人們從很多的實驗中發現到它作用得很好

58
00:03:23,627 --> 00:03:25,646
而我剛剛所說的直觀

59
00:03:25,646 --> 00:03:27,375
除了經常這樣被引用

60
00:03:27,375 --> 00:03:33,020
我不認為任何人完全了解背後真正的道理

61
00:03:33,020 --> 00:03:34,655
我不認識任何人知道是否那就是

62
00:03:34,655 --> 00:03:39,930
最大池化在 ConvNets  中
作用的很好的背後原因

63
00:03:39,930 --> 00:03:43,490
最大池化終有一個有趣的特性也就是

64
00:03:43,490 --> 00:03:47,770
它有一組超參數，但沒有參數需要學習

65
00:03:47,770 --> 00:03:50,293
實際上梯度下降並沒有東西可以學習

66
00:03:50,293 --> 00:03:51,780
一旦您固定 f 跟 s

67
00:03:51,780 --> 00:03:56,876
它就是一個固定的運算，
而梯度下降不用改變任何東西

68
00:03:56,876 --> 00:04:00,810
我們來進行一個使用不同超參數的例子

69
00:04:00,810 --> 00:04:04,675
這裡我將使用一個 5乘5 的輸入

70
00:04:04,675 --> 00:04:10,290
而我們將應用最大池化使用一個 3乘3 的過濾器

71
00:04:10,290 --> 00:04:13,815
所以 f 等於 3, 而跨步則為 1

72
00:04:13,815 --> 00:04:18,190
這種設定下的輸出會是 3乘3

73
00:04:18,190 --> 00:04:20,570
而我們在之前投影片中使用的公式

74
00:04:20,570 --> 00:04:23,945
來計算卷積層的輸出大小

75
00:04:23,945 --> 00:04:27,345
這個公式在最大池化一樣適用

76
00:04:27,345 --> 00:04:34,345
所以，那是 n 加 2p 減去 f 除以 s 加 1

77
00:04:34,345 --> 00:04:38,458
這個公式也適用於計算最大池化的輸出大小

78
00:04:38,458 --> 00:04:41,820
在這個例子中，我們來計算這個 3乘3 輸出的每個元素

79
00:04:41,820 --> 00:04:45,080
在最左上角

80
00:04:45,080 --> 00:04:46,670
我們將看這個區域

81
00:04:46,670 --> 00:04:48,735
注意到這是一個 3乘3 區域

82
00:04:48,735 --> 00:04:51,695
因為過濾器的大小是 3, 而最大值在這裡

83
00:04:51,695 --> 00:04:53,715
所以會是 9

84
00:04:53,715 --> 00:04:57,920
然後我們向左跨一步，因為我們的跨步是 1

85
00:04:57,920 --> 00:05:00,960
所以在這個藍色框框中最大值是 9

86
00:05:00,960 --> 00:05:03,695
我們再次向左移一步

87
00:05:03,695 --> 00:05:06,235
這個藍色框框最大值是 5

88
00:05:06,235 --> 00:05:09,710
然後我們往下一行走，跨步是 1

89
00:05:09,710 --> 00:05:12,465
所以我們只往下跨一步

90
00:05:12,465 --> 00:05:16,520
所以這個區域的最大值是 9, 這個區域的最大值是 9,

91
00:05:16,520 --> 00:05:19,970
這個區域的最大值是

92
00:05:19,970 --> 00:05:22,516
現在有兩個 5, 最大值是 5

93
00:05:22,516 --> 00:05:26,130
而最後，這個區域的最大值是 8

94
00:05:26,130 --> 00:05:28,965
這個區域的最大值是 6

95
00:05:28,965 --> 00:05:31,350
這個區域的最大值是 9

96
00:05:31,350 --> 00:05:35,810
好的，所以使用這組超參數 f 等於 3

97
00:05:35,810 --> 00:05:40,007
s 等於 1 給我們這個輸出

98
00:05:40,007 --> 00:05:44,975
到目前為止，我顯示的最大池化都是在 2D 輸入

99
00:05:44,975 --> 00:05:47,370
如果您有一個 3D 輸入

100
00:05:47,370 --> 00:05:53,245
那輸出會是一樣的維度

101
00:05:53,245 --> 00:05:56,765
所以舉個例子，如果您有一個 5乘5乘2

102
00:05:56,765 --> 00:06:02,360
而輸出會是 3乘3乘2, 而計算

103
00:06:02,360 --> 00:06:05,045
最大池化就是您計算

104
00:06:05,045 --> 00:06:08,368
我們剛剛描述的方式單獨在每一個通道上

105
00:06:08,368 --> 00:06:11,960
所以顯示在最上面的第一個通道還是一樣

106
00:06:11,960 --> 00:06:13,790
而第二個通道，我想

107
00:06:13,790 --> 00:06:15,790
我畫的是在底部

108
00:06:15,790 --> 00:06:19,250
您會用同樣的運算在這一片上

109
00:06:19,250 --> 00:06:24,365
而那個值會給您第二片

110
00:06:24,365 --> 00:06:29,300
而一般而言，如果這是一個 5乘5乘 一個數字的通道

111
00:06:29,300 --> 00:06:34,395
輸出會是 3乘3乘 同樣那個數字的通道數

112
00:06:34,395 --> 00:06:44,541
而最大池化計算是在每一個通道
 總共Nc通道 單獨計算

113
00:06:44,541 --> 00:06:46,520
所以，這就是最大池化

114
00:06:46,520 --> 00:06:49,815
還有一種型態的池化不太常使用

115
00:06:49,815 --> 00:06:52,870
但我大略提一下，也就是平均池化

116
00:06:52,870 --> 00:06:56,395
它大概是您所期望的

117
00:06:56,395 --> 00:06:59,080
與其使用在每個過濾器中取最大值

118
00:06:59,080 --> 00:07:02,040
您取平均值

119
00:07:02,040 --> 00:07:03,250
在這個例子中

120
00:07:03,250 --> 00:07:07,540
在紫色區域的平均是 3.75

121
00:07:07,540 --> 00:07:09,940
然後這個是 1.25

122
00:07:09,940 --> 00:07:12,930
然後是 4, 跟 2

123
00:07:12,930 --> 00:07:17,020
 所以這是平均池化使用超參數為 f 等於 2

124
00:07:17,020 --> 00:07:21,795
s 等於 2, 我們也可以選擇其他超參數

125
00:07:21,795 --> 00:07:24,640
時至今日，最大池化使用上遠大於

126
00:07:24,640 --> 00:07:28,340
平均池化，但有一個例外

127
00:07:28,340 --> 00:07:32,125
就是有時候在很深的神經網路上

128
00:07:32,125 --> 00:07:36,670
您或許使用平均池化來壓縮，舉例

129
00:07:36,670 --> 00:07:40,290
 7乘7乘1000

130
00:07:40,290 --> 00:07:42,755
平均所有的值

131
00:07:42,755 --> 00:07:45,625
您得到 1乘1乘1000

132
00:07:45,625 --> 00:07:47,475
以後我們會見到這個例子

133
00:07:47,475 --> 00:07:54,085
但，最大池化在神經網路中使用遠大於平均池化

134
00:07:54,085 --> 00:07:56,305
總結一下

135
00:07:56,305 --> 00:08:00,100
池化的超參數是 f

136
00:08:00,100 --> 00:08:02,840
過濾器的大小， s 跨步的大小

137
00:08:02,840 --> 00:08:07,360
或許最常見的參數選擇或許是 f 等於 2, s 等於 2

138
00:08:07,360 --> 00:08:11,045
這個相當常用，而這個個效用是

139
00:08:11,045 --> 00:08:15,925
大約將常跟寬縮成一半

140
00:08:15,925 --> 00:08:21,150
最常見的超參數選擇是，f 等於 2, s 等於 2

141
00:08:21,150 --> 00:08:23,530
這個的效果會是

142
00:08:23,530 --> 00:08:28,440
將長跟寬縮為一半

143
00:08:28,440 --> 00:08:32,094
我也看過有人用 f  等於 3, s 等於 2

144
00:08:32,094 --> 00:08:37,150
而其他超參數的選擇會像是二進位

145
00:08:37,150 --> 00:08:40,120
您要使用最大池化或是平均池化

146
00:08:40,120 --> 00:08:43,380
您要的話，您可以加一個額外的超參數

147
00:08:43,380 --> 00:08:48,140
用來填充，雖然這種做法非常非常少

148
00:08:48,140 --> 00:08:50,080
當您使用最大池化時，通常

149
00:08:50,080 --> 00:08:51,685
您不使用填充

150
00:08:51,685 --> 00:08:55,025
雖然我們在這個星期會看到一個例外

151
00:08:55,025 --> 00:08:57,160
但對於大部分的最大池化

152
00:08:57,160 --> 00:08:59,710
通常不使用填充

153
00:08:59,710 --> 00:09:05,345
最常用的 p 值目前都是 0

154
00:09:05,345 --> 00:09:13,215
而最大池化的輸入容積大小是

155
00:09:13,215 --> 00:09:14,945
nh乘nw乘nc

156
00:09:14,945 --> 00:09:21,265
而這個會輸出的大小是這個

157
00:09:21,265 --> 00:09:29,465
假設不用填充，乘 nw 減 f 除以 s

158
00:09:29,465 --> 00:09:32,015
加 1 乘 nc

159
00:09:32,015 --> 00:09:35,295
輸入通道的數目等於輸出通道的數目

160
00:09:35,295 --> 00:09:40,555
因為池化單獨應用到每個通道上

161
00:09:40,555 --> 00:09:47,205
有關池化的一個重點是，沒有要學習的參數

162
00:09:47,205 --> 00:09:50,470
所以當我們實行反向傳播時

163
00:09:50,470 --> 00:09:55,645
在最大池化時，您會發現沒有參數在反向傳播中需要導入

164
00:09:55,645 --> 00:09:58,400
相反地，這些超參數您只要設定一次

165
00:09:58,400 --> 00:10:01,485
或許手工設定一次，或者使用交叉驗證設定

166
00:10:01,485 --> 00:10:03,710
設定完成就完成

167
00:10:03,710 --> 00:10:07,140
那只是一個固定函數，神經網路在一層中計算

168
00:10:07,140 --> 00:10:09,829
實際上並沒有要學習的東西

169
00:10:09,829 --> 00:10:11,999
它就只是一個固定函數

170
00:10:11,999 --> 00:10:13,350
所以這就是池化

171
00:10:13,350 --> 00:10:15,460
您現在知道如何建置卷積層和池層

172
00:10:15,460 --> 00:10:18,095
在下一個影片中

173
00:10:18,095 --> 00:10:20,830
我們來看一個複雜的 ConvNet 例子

174
00:10:20,830 --> 00:10:25,000
一個讓我們能夠介紹全連結層