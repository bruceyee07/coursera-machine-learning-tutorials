1
00:00:03,945 --> 00:00:07,666
现在让我们做好准备<br />来构建单层卷积神经网络

2
00:00:07,666 --> 00:00:09,223
我们先来看一个例子

3
00:00:12,432 --> 00:00:17,146
之前我们已经提过如何应用两个不同的过滤器

4
00:00:17,146 --> 00:00:20,670
对三维输入进行卷积计算

5
00:00:21,810 --> 00:00:25,861
比如现在，对图中的输入，我们想要得到不同的4x4输出

6
00:00:30,891 --> 00:00:35,548
应用第一个卷积过滤器

7
00:00:35,548 --> 00:00:40,769
我们得到了第一个4x4的输出

8
00:00:40,769 --> 00:00:49,108
应用第二个卷积过滤器我们得到了另一个4x4输出

9
00:00:49,108 --> 00:00:55,574
最终我们需要把这些输出变成单层卷积神经网络

10
00:00:55,574 --> 00:01:00,566
还需要对每一个输出添加一个偏差（bias）

11
00:01:00,566 --> 00:01:03,980
这里偏差是一个实数

12
00:01:03,980 --> 00:01:08,840
这里使用的是广播机制，我们对这

13
00:01:08,840 --> 00:01:11,750
16个元素添加同样的偏差

14
00:01:11,750 --> 00:01:16,805
然后我们可以继续添加一些非线性转换ReLU

15
00:01:16,805 --> 00:01:23,430
最终，通过添加偏差和非线性转换

16
00:01:23,430 --> 00:01:27,295
我们得到一个4x4矩阵输出

17
00:01:27,295 --> 00:01:31,894
对于下面的矩阵也是一样，添加一个不同的偏差值

18
00:01:31,894 --> 00:01:33,154
这里的偏差也是一个实数

19
00:01:33,154 --> 00:01:36,473
这个数字也像上面一样，通过广播添加到其他16个元素中

20
00:01:36,473 --> 00:01:40,934
最后应用一些非线性处理 比方线性整流函数(ReLU)

21
00:01:40,934 --> 00:01:47,369
这样我们得到了另一个4x4的矩阵输出

22
00:01:47,369 --> 00:01:52,461
像之前一样 我们最后把这两个矩阵放在一起

23
00:01:52,461 --> 00:01:59,698
得到一个4x4x2的输出

24
00:01:59,698 --> 00:02:06,326
这个例子中对于6x6x3的输入<br />我们通过计算得到一个4x4x2的输出(Andrew原话是4x4x4，应该是口误）

25
00:02:06,326 --> 00:02:11,303
这就是卷积神经网络的一层

26
00:02:11,303 --> 00:02:15,634
现在我们把这个例子和普通的

27
00:02:15,634 --> 00:02:18,832
非卷积单层前向传播神经网络对应起来

28
00:02:18,832 --> 00:02:23,155
在（神经网络）传播之前我们需要做这些

29
00:02:23,155 --> 00:02:28,461
z[1] = w[1] x a[0], a[0] = x

30
00:02:28,461 --> 00:02:31,265
再加上b[1]

31
00:02:31,265 --> 00:02:38,121
然后再应用非线性函数g 也就是g(z[1]) 得到a[1]

32
00:02:38,121 --> 00:02:43,400
在图中这个例子中，这个输入就是a[0]，也就是x

33
00:02:44,770 --> 00:02:48,076
这里的过滤器

34
00:02:48,076 --> 00:02:52,488
作用和w[1]类似

35
00:02:52,488 --> 00:02:56,377
之前我们在卷积计算中

36
00:02:56,377 --> 00:03:01,149
我们有27个输入 或者确切的说<br />是两组27个输入 因为我们有两个过滤器

37
00:03:01,149 --> 00:03:03,900
我们需要把上面这些数字相乘

38
00:03:03,900 --> 00:03:09,631
这其实就是通过一个线性方程计算得到一个4x4的矩阵

39
00:03:09,631 --> 00:03:15,048
这里通过卷积计算得出4x4矩阵

40
00:03:15,048 --> 00:03:19,245
这个过程和 w[1] x a[0] 类似

41
00:03:19,245 --> 00:03:25,340
输出也是一个4x4的矩阵

42
00:03:25,340 --> 00:03:29,599
另外就是添加偏差值

43
00:03:29,599 --> 00:03:35,138
因此方框里的这些

44
00:03:35,138 --> 00:03:38,939
作用和z类似

45
00:03:38,939 --> 00:03:43,848
最后 应用非线性方程

46
00:03:43,848 --> 00:03:49,740
因此这里的输出

47
00:03:49,740 --> 00:03:53,390
其实成为了下一层(神经网络)的激活函数

48
00:03:53,390 --> 00:03:58,794
这就是从a[0]到a[1]的步骤：首先是线性计算

49
00:03:58,794 --> 00:04:02,192
然后再进行卷积，对这些相乘

50
00:04:02,192 --> 00:04:05,507
因此，卷积计算其实就是应用线性操作计算

51
00:04:05,507 --> 00:04:08,437
再添加偏差 然后通过ReLU操作

52
00:04:08,437 --> 00:04:14,107
我们从6x6x3的输入 a[0]

53
00:04:14,107 --> 00:04:18,210
经过一层神经网络的传播

54
00:04:18,210 --> 00:04:22,693
得到了4x4x2的输出 也就是a[1]

55
00:04:22,693 --> 00:04:27,144
从6x6x3到4x4x2

56
00:04:27,144 --> 00:04:30,860
这就是一层卷积(神经)网络

57
00:04:33,697 --> 00:04:40,504
在这个例子中 我们有2个过滤器 也就是两个特征

58
00:04:40,504 --> 00:04:45,270
因此我们得到输出是4x4x2

59
00:04:45,270 --> 00:04:49,248
如果我们有10个过滤器而不是2个

60
00:04:49,248 --> 00:04:54,396
那么我们得到的输出就是4x4x10

61
00:04:54,396 --> 00:05:00,334
因为这里需要有10个这样的操作(而不是2个).<br />然后把结果放到一起

62
00:05:00,334 --> 00:05:05,598
得到一个4x4x10的输出 也就是a[1]

63
00:05:05,598 --> 00:05:09,630
为了进一步理解这里所说的 我们来做一个练习

64
00:05:09,630 --> 00:05:14,655
假定你的单层神经网络中有10个<br />而不是2个 3x3x3的过滤器，

65
00:05:14,655 --> 00:05:19,600
这层网络中有多少参数？

66
00:05:21,000 --> 00:05:22,984
我们来计算一下

67
00:05:22,984 --> 00:05:29,439
每个过滤器是一个3x3x3的三维矩阵

68
00:05:29,439 --> 00:05:35,318
因此每个过滤器有27个参数

69
00:05:35,318 --> 00:05:39,800
也就是，有27个数字需要进行训练学习得到

70
00:05:42,210 --> 00:05:46,818
还有一个参数b，也就是总共28个参数

71
00:05:50,125 --> 00:05:54,456
现在想象一下 前面我们的图中是两个过滤器

72
00:05:54,456 --> 00:05:58,329
现在这里我们有10个过滤器

73
00:05:58,329 --> 00:06:01,747
1, 2, ... 10个

74
00:06:01,747 --> 00:06:06,942
加起来是28x10

75
00:06:06,942 --> 00:06:10,753
也就是280个参数

76
00:06:10,753 --> 00:06:16,316
注意这里. 一个很好的特性是. 不管输入的图像有多大

77
00:06:16,316 --> 00:06:22,051
比方1000 x 1000 或者5000 x 5000，

78
00:06:22,051 --> 00:06:26,973
这里的参数个数不变. 依然是280个

79
00:06:26,973 --> 00:06:31,453
因此 用这10个过滤器来检测一个图片的不同的特征，比方垂直边缘线，

80
00:06:31,453 --> 00:06:35,485
水平边缘线 或者其他不同的特征 

81
00:06:35,485 --> 00:06:39,240
不管图片多大 所用的参数个数都是一样的

82
00:06:40,920 --> 00:06:44,410
这个特征使得卷积神经网络

83
00:06:44,410 --> 00:06:48,000
不太容易过拟合（overfitting）

84
00:06:48,000 --> 00:06:51,450
因此，比方你训练学习得到10个特征检测器(函数)

85
00:06:51,450 --> 00:06:54,770
你可以把它们应用到非常大的图像(特征检测)中

86
00:06:54,770 --> 00:06:58,300
所用的参数数目还是不变的，比方这个例子中的280

87
00:06:58,300 --> 00:07:00,494
相对(图片大小)来说非常小

88
00:07:00,494 --> 00:07:05,130
现在我们来总结回归一下

89
00:07:05,130 --> 00:07:09,766
用来在卷积神经网络中

90
00:07:09,766 --> 00:07:11,980
用来描述一层网络的(形式化表示的)符号标记

91
00:07:11,980 --> 00:07:14,302
我们说，l层是一个卷积层

92
00:07:14,302 --> 00:07:18,555
用f加上标l来表示过滤器的(矩阵维度)大小

93
00:07:18,555 --> 00:07:23,219
之前我们用 f x f 来表示

94
00:07:23,219 --> 00:07:28,163
现在我们加上上标[l]来表示

95
00:07:28,163 --> 00:07:31,074
这是一个l层大小为 f x f 的过滤器

96
00:07:31,074 --> 00:07:35,767
按照惯例 这里的上标[l]

97
00:07:35,767 --> 00:07:37,611
表示当前层l

98
00:07:39,812 --> 00:07:42,809
p加上上标[l] 表示填充（padding）的大小

99
00:07:42,809 --> 00:07:47,363
填充的大小也可以通过不同的卷积名称来定义 比方valid填充，

100
00:07:47,363 --> 00:07:50,135
就是没有填充

101
00:07:50,135 --> 00:07:53,240
Same填充，表示应用的填充大小会

102
00:07:53,240 --> 00:07:57,910
使得输出的结果大小和输入拥有相同的维度大小

103
00:07:59,000 --> 00:08:01,590
s加上上标[l]表示步长大小

104
00:08:03,250 --> 00:08:09,450
这一层的输入 是一个多维矩阵

105
00:08:09,450 --> 00:08:18,590
也就是一个n x n x n c (上一层的通道数目）

106
00:08:18,590 --> 00:08:21,162
我们来变一下这个表示方法

107
00:08:21,162 --> 00:08:25,385
我们用上标l-1

108
00:08:25,385 --> 00:08:29,902
因为这个来自上一层的激活函数

109
00:08:29,902 --> 00:08:35,594
n[l-1] 乘以 nc [l-1]

110
00:08:35,594 --> 00:08:40,966
迄今为止我们所用的例子中图像的长宽都是一样的

111
00:08:40,966 --> 00:08:43,990
有些情况下图像长宽可能不同

112
00:08:43,990 --> 00:08:48,528
因此我们用下标 h 和 w 来表示

113
00:08:48,528 --> 00:08:51,949
来自上一层的输入的长和宽

114
00:08:51,949 --> 00:08:56,418
因此l层的矩阵大小是

115
00:08:56,418 --> 00:09:01,134
n_h * n_l * n_c, 所有都加上上标[l]

116
00:09:01,134 --> 00:09:05,597
这个是l层的情况. 这一层的输入是

117
00:09:05,597 --> 00:09:09,451
上一层的输出. 因此这里是l-1

118
00:09:09,451 --> 00:09:16,730
然后这一层神经网络的输出就是

119
00:09:16,730 --> 00:09:23,065
n_h[l] x n_w[l] x n_c[l]

120
00:09:23,065 --> 00:09:28,495
这就是输出的(维度)大小

121
00:09:28,495 --> 00:09:34,941
和之前所说的一样，这里输出的大小

122
00:09:34,941 --> 00:09:40,657
起码长和宽是由这个公式 (n+2p-f)/2 决定的

123
00:09:40,657 --> 00:09:47,971
结果如果不是整数的话 取下界

124
00:09:47,971 --> 00:09:55,605
在我们现在的表示方法中 l层的输出

125
00:09:55,605 --> 00:10:00,891
是前一层的维度

126
00:10:00,891 --> 00:10:05,471
加上当前l层的padding

127
00:10:05,471 --> 00:10:11,760
再减去当前l层所用的过滤器大小

128
00:10:11,760 --> 00:10:16,580
我们现在计算的是输出矩阵的长

129
00:10:16,580 --> 00:10:21,510
同样

130
00:10:21,510 --> 00:10:24,680
这个公式也可以计算矩阵的宽

131
00:10:24,680 --> 00:10:26,670
只要把h换成

132
00:10:26,670 --> 00:10:30,780
w就可以

133
00:10:30,780 --> 00:10:34,775
这两个的计算公式都是一样的

134
00:10:36,570 --> 00:10:44,024
这就如何从n_h[l-1]到n_h[l]，和从n_w[l-1]到n_w[l]

135
00:10:44,024 --> 00:10:48,105
那么通道数目呢，我们从哪里得到这个数字？

136
00:10:48,105 --> 00:10:52,784
我们来看一下输出的(维度的)深度

137
00:10:52,784 --> 00:10:57,662
从之前的例子中我们知道

138
00:10:57,662 --> 00:11:02,167
通道数目的大小和过滤器数目是一样的

139
00:11:02,167 --> 00:11:07,017
比方 如果我们有2个过滤器 那么我们的输出就是4x4x2. 是2维的

140
00:11:07,017 --> 00:11:11,097
如果过滤器数目是10个 那么大小就是4x4x10

141
00:11:11,097 --> 00:11:15,744
因此这个输出中的通道数目

142
00:11:15,744 --> 00:11:23,098
就是我们这一层神经网络中所用的过滤器的数目

143
00:11:23,098 --> 00:11:26,849
下面 我们来看一下过滤器的大小

144
00:11:26,849 --> 00:11:33,057
每个过滤器的大小是f[l] x f[l] 再乘一个数

145
00:11:33,057 --> 00:11:34,704
那么这个数是什么呢？

146
00:11:34,704 --> 00:11:39,465
我们之前的例子中，如果对一个6x6x3的图像进行卷积

147
00:11:39,465 --> 00:11:41,580
所用的filter是3x3x3

148
00:11:43,070 --> 00:11:48,150
因此所用的过滤器的通道的大小

149
00:11:48,150 --> 00:11:54,360
应该和输入的通道数目相同 这两个数字一样大小

150
00:11:54,360 --> 00:12:02,627
因此过滤器就是f[l] x f[l] x n_c[l-1]

151
00:12:02,627 --> 00:12:07,875
最后通过非线性计算. 这一层的输出

152
00:12:07,875 --> 00:12:11,745
就是这一层的激活函数a[l]

153
00:12:11,745 --> 00:12:15,115
这个维度我们已经在这里可以看到了

154
00:12:15,115 --> 00:12:20,451
a[l] 是一个三维矩阵

155
00:12:20,451 --> 00:12:25,556
n_h[l] x n_w[l] x n_c[l]

156
00:12:25,556 --> 00:12:31,082
如果你应用的是向量化实现 或者批量梯度下降

157
00:12:31,082 --> 00:12:36,891
或者是小批量梯度下降 

158
00:12:36,891 --> 00:12:41,387
那么你的输出A[l]，<br />加入你有m个输入，就是一组m个激活函数

159
00:12:41,387 --> 00:12:48,275
也就是 m x n_h[l] x n_w[l] x n_c[l]

160
00:12:48,275 --> 00:12:51,375
如果你是用的是批量梯度下降

161
00:12:51,375 --> 00:12:55,999
在代码中这就是你需要使用的变量的(梯度下降)顺序

162
00:12:55,999 --> 00:12:59,962
首先我们有这些训练数据的大小

163
00:12:59,962 --> 00:13:02,384
然后，这三个变量

164
00:13:02,384 --> 00:13:07,618
那么权重矩阵w呢？

165
00:13:07,618 --> 00:13:10,264
我们已经知道了过滤器的维度

166
00:13:10,264 --> 00:13:16,258
过滤器是f[l] x f[l] x n_c[l-1]

167
00:13:16,258 --> 00:13:20,230
但是这只是一个过滤器的维度

168
00:13:20,230 --> 00:13:22,247
我们需要多少个过滤器呢？

169
00:13:22,247 --> 00:13:25,097
这个是我们需要的过滤器的数目

170
00:13:25,097 --> 00:13:30,029
所有过滤器的权重的维度就是所有的过滤器的大小总和

171
00:13:30,029 --> 00:13:33,513
由这个给定 对吧？

172
00:13:33,513 --> 00:13:38,625
因为这个，最后的数字大小是l层神经网络中

173
00:13:38,625 --> 00:13:43,750
过滤器的数目大小

174
00:13:45,680 --> 00:13:48,710
最后 每一个过滤器

175
00:13:48,710 --> 00:13:54,100
加上一个偏差 也就是每个过滤器加上一个实数

176
00:13:54,100 --> 00:13:57,970
因此(我们有) 偏差参数有这么多变量

177
00:13:57,970 --> 00:14:00,810
这只是一个大小为这个的向量

178
00:14:00,810 --> 00:14:05,052
虽然后面我们会看到

179
00:14:05,052 --> 00:14:09,813
更加方便的偏差系数编码 (1, 1, 1, n_c[l])

180
00:14:09,813 --> 00:14:14,790
这样的四维矩阵，或称为4维tensor

181
00:14:16,430 --> 00:14:19,408
到现在为止 我们提到了很多的参数和符号

182
00:14:19,408 --> 00:14:23,303
这些都我们在本课程会用到的 

183
00:14:23,303 --> 00:14:27,498
另外我想提一下 如果你上网搜索 或者查看源代码的话

184
00:14:27,498 --> 00:14:32,311
你会发现并没有一个标准的惯例来规定(公式中的）

185
00:14:32,311 --> 00:14:34,260
长 宽 和通道的顺序

186
00:14:34,260 --> 00:14:39,142
因此你如果你读到GitHub 或者其他开源的实现源代码

187
00:14:39,142 --> 00:14:43,873
你可能会发现有些作者会把通道数放到最前面

188
00:14:43,873 --> 00:14:48,631
另外一些时候你看到(我们之前用的)那个顺序

189
00:14:48,631 --> 00:14:52,154
其实在一些常见的框架中

190
00:14:52,154 --> 00:14:54,155
会有一个参数来设置在(矩阵的) 索引中

191
00:14:54,155 --> 00:14:57,652
是要把通道数目放在最开始

192
00:14:57,652 --> 00:15:02,000
还是最后面

193
00:15:02,000 --> 00:15:08,137
我觉得这些情况都合理 只要你使用的时候保持前后一致。

194
00:15:08,137 --> 00:15:13,049
不幸的是 这个表示方式并没有在

195
00:15:13,049 --> 00:15:17,772
深度学习的各种文章中广泛使用

196
00:15:17,772 --> 00:15:21,752
但是在这门课中我会采用这种表示方式

197
00:15:24,681 --> 00:15:30,769
(也就是)高和宽这两个维度放在前面 通道维度放在最后

198
00:15:30,769 --> 00:15:34,327
我知道这里突然出现了很多的注释符号

199
00:15:34,327 --> 00:15:38,027
你可能会觉得太多了记不住

200
00:15:38,027 --> 00:15:41,994
不要担心 你不需要记住这些注释和符号

201
00:15:41,994 --> 00:15:46,036
通过本周的练习 你会更加熟悉这些表示方式和注释的

202
00:15:46,036 --> 00:15:49,116
这里我希望你把这个视频的重点放在

203
00:15:49,116 --> 00:15:52,694
卷积神经网络的单层卷积神经网络工作原理

204
00:15:52,694 --> 00:15:57,006
以及在这层中激活函数的计算

205
00:15:57,006 --> 00:16:00,052
还有怎么把激活函数对应到下一层

206
00:16:00,052 --> 00:16:04,063
现在你知道了单层卷积神经网络怎么工作 下一步

207
00:16:04,063 --> 00:16:07,740
我们联合几层(神经网络) 来讨论深度卷积

208
00:16:07,740 --> 00:16:09,040
神经网络

209
00:16:09,040 --> 00:16:10,200
我们来看下一个视频
GTC字幕组翻译