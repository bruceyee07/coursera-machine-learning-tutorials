1
00:00:00,252 --> 00:00:04,029
最後のビデオでは 単一層の構成要素を見た

2
00:00:04,029 --> 00:00:06,721
ConvNetにおける単一の畳み込み層のだ

3
00:00:06,721 --> 00:00:12,339
それじゃ ディープ畳み込みニューラルネットワークの具体例を見てみよう

4
00:00:12,339 --> 00:00:15,876
これは 最後のビデオで導入した表記法の

5
00:00:15,876 --> 00:00:17,373
練習にもなる

6
00:00:19,648 --> 00:00:22,203
ここに画像があるとしよう

7
00:00:22,203 --> 00:00:26,959
そして 画像の分類 もしくは 画像の識別をしたいとする

8
00:00:26,959 --> 00:00:31,745
入力 画像ｘを得たら それが猫かどうか 0 か 1 か 決めたい

9
00:00:31,745 --> 00:00:34,254
これは 分類問題だ

10
00:00:34,254 --> 00:00:38,626
このタスクのため ConvNetの例を構築しよう

11
00:00:38,626 --> 00:00:42,943
この例のために かなり小さな画像を使うことにする

12
00:00:42,943 --> 00:00:48,499
この画像は 39 x 39 x 3 だ

13
00:00:48,499 --> 00:00:51,529
この決定により いくつかの数を 少しだけ良く働かせる

14
00:00:51,529 --> 00:00:57,470
層 0 の nH は nW と同じになり

15
00:00:57,470 --> 00:01:00,581
高さと幅は 39 に等しい

16
00:01:00,581 --> 00:01:06,532
層 0 のチャンネル数は ３に等しい

17
00:01:06,532 --> 00:01:11,906
最初の層では 3 x 3 フィルターのセットを使おう

18
00:01:11,906 --> 00:01:16,924
特徴を検出するためだ f = 3 実際は f[1] = 3

19
00:01:16,924 --> 00:01:20,992
なぜなら 3 x 3 フィルターだから

20
00:01:20,992 --> 00:01:26,871
そして ストライドは１で パディング無しとする

21
00:01:26,871 --> 00:01:32,641
同じ畳み込みを使う
そうだな 10 フィルターとしよう

22
00:01:34,632 --> 00:01:38,779
そうしたら このニューラルネットワークの

23
00:01:38,779 --> 00:01:43,755
次の層の活性化は 37 x 37 x 10 となる

24
00:01:43,755 --> 00:01:49,335
この 10 は 10フィルターを使ったことから来ている

25
00:01:49,335 --> 00:01:54,735
そして 37 は この式から来た

26
00:01:54,735 --> 00:01:58,739
(n + 2p - f) / s + 1

27
00:01:58,739 --> 00:02:03,919
よし 皆できたと思うが

28
00:02:03,919 --> 00:02:10,401
(39 + 0 - 3) / 1 + 1 = 37 だ

29
00:02:10,401 --> 00:02:15,006
だから 出力が 37 x 37 になった
これは Valid畳み込みだ

30
00:02:15,006 --> 00:02:17,590
そして これが出力サイズだ

31
00:02:17,590 --> 00:02:25,029
我々の表記法では nH[1] = nW[1] = 37 で

32
00:02:25,029 --> 00:02:30,126
nC[1] = 10

33
00:02:30,126 --> 00:02:36,240
nC[1] は 最初の層のフィルター数に等しい

34
00:02:36,240 --> 00:02:42,040
これが 最初の層の活性化の次元になる

35
00:02:43,300 --> 00:02:45,980
それじゃ 別の畳み込み層を入れよう

36
00:02:45,980 --> 00:02:48,900
今度は 5 x 5 フィルターとする

37
00:02:48,900 --> 00:02:54,996
我々の表記法では 次のニューラルネットワーク層の f[2] は 5 だ

38
00:02:54,996 --> 00:02:59,231
今度は ストライドを２としよう

39
00:02:59,231 --> 00:03:03,922
そして パディング無し

40
00:03:03,922 --> 00:03:07,060
そして 20個の フィルター

41
00:03:09,370 --> 00:03:15,933
そうしたら この出力は 別のボリュームとなり

42
00:03:15,933 --> 00:03:20,946
今度のそれは 17 x 17 x 20 となる

43
00:03:20,946 --> 00:03:23,866
ストライドを２にしたから

44
00:03:23,866 --> 00:03:25,926
次元数が より速く縮んだ

45
00:03:25,926 --> 00:03:32,800
37 x 37 は 因数の２よりも 少しだけ多く 17 x 17 まで小さくなった

46
00:03:32,800 --> 00:03:37,554
そして 20 個のフィルターがあるので チャンネル数は 20 だ

47
00:03:37,554 --> 00:03:42,167
この活性化 a[2] が そのような次元になる

48
00:03:42,167 --> 00:03:46,971
それで

49
00:03:46,971 --> 00:03:52,160
nH[2] = nW[2] = 17 で

50
00:03:52,160 --> 00:03:55,247
nC[2] = 20

51
00:03:55,247 --> 00:03:58,180
よろしい 最初の畳み込み層を適用しよう

52
00:03:58,180 --> 00:04:04,071
再び 5 x 5 フィルターを使おう

53
00:04:04,071 --> 00:04:07,390
そして また ストライドは２だ

54
00:04:07,390 --> 00:04:13,681
そうすれば 数式は飛ばすけど 最終的に 7 x 7 になる

55
00:04:13,681 --> 00:04:19,251
そして 40個のフィルターを使うとしよう パディング無しの40フィルターだ

56
00:04:19,251 --> 00:04:22,760
最後に 7 x 7 x 40 を得る

57
00:04:22,760 --> 00:04:27,860
ここで やったことは 39 x 39 x 3 の入力画像を得て

58
00:04:29,380 --> 00:04:34,810
7 x 7 x 40 の特徴量を持つ画像を算出した

59
00:04:34,810 --> 00:04:41,075
最後に 行われ 得たものは もし 7 x 7 x 40 を得たのなら

60
00:04:41,075 --> 00:04:45,137
それは 7 x 7 x 40 で 1,960 だ

61
00:04:45,137 --> 00:04:50,888
そして このボリュームを 平坦化する

62
00:04:50,888 --> 00:04:55,901
つまり 展開すると 1,960 ユニットとなる

63
00:04:55,901 --> 00:04:59,347
単に平たくしてベクトルにする

64
00:04:59,347 --> 00:05:05,283
それから これを ロジスティック回帰や ソフトマックスに喰わせる

65
00:05:07,917 --> 00:05:11,682
何を認識しようとしているのかによって 猫かそうでないかによって

66
00:05:11,682 --> 00:05:15,150
異なる種類の物体の中から何を認識しようとしているかによって

67
00:05:15,150 --> 00:05:19,900
これが ニューラルネットワークの 最終出力 最終的な予測結果を 与える

68
00:05:20,925 --> 00:05:26,520
正確に言うと ここの最後のステップは この数全てを

69
00:05:26,520 --> 00:05:32,222
1,960個の数を 展開して とても長い1つのベクトルにする

70
00:05:32,222 --> 00:05:36,483
そうしたら 1つの長いベクトルを得たので ソフトマックスや
ロジスティック回帰に 喰わせることができるようになり

71
00:05:36,483 --> 00:05:39,770
最終出力に 予測を生成できる

72
00:05:41,600 --> 00:05:46,125
さて これが ConvNet の典型例だ

73
00:05:47,380 --> 00:05:51,187
畳み込みニューラルネットワークの設計では 多くが

74
00:05:51,187 --> 00:05:54,880
こんな風にして ハイパーパラメータを選び 合計数をどうするか決めている

75
00:05:54,880 --> 00:05:55,840
ストライドはどうだろう？

76
00:05:55,840 --> 00:05:58,860
パディングは？ フィルターの数は？

77
00:06:00,190 --> 00:06:03,980
今週の終わりと次週にも どのように この選択を行うのかについて

78
00:06:03,980 --> 00:06:07,440
提案とガイドラインを与えるよ

79
00:06:07,440 --> 00:06:12,510
だけど 今は ここから得られることは

80
00:06:12,510 --> 00:06:17,950
ニューラルネットワークを深くしていけば
典型的には 39 x 39 よりも大きな画像から始まって

81
00:06:17,950 --> 00:06:21,202
高さと幅は 暫くは 同じ値に留まるけど

82
00:06:21,202 --> 00:06:25,859
徐々に ニューラルネットワークが深くなるにつれ 小さくなっていくということだ

83
00:06:25,859 --> 00:06:29,663
それは 39 から 37 17 14 へと変わった

84
00:06:29,663 --> 00:06:33,961
ごめん 39 から 37 17「7」へだ

85
00:06:33,961 --> 00:06:36,753
一方 チャンネル数は 徐々に増加するだろう

86
00:06:36,753 --> 00:06:41,412
それは 3 から 10 20 40 になった

87
00:06:41,412 --> 00:06:45,930
これは 他の多くの畳み込みニューラルネットワークでも見られる 一般的な傾向だ

88
00:06:47,060 --> 00:06:52,576
後のビデオでは どのように これらのパラメータを設計するのかについて
もっと多くのガイドラインを得られる

89
00:06:52,576 --> 00:06:57,196
でも あなたは 今 畳み込みニューラルネットワークの最初の例を見た

90
00:06:57,196 --> 00:06:59,210
略して ConvNet だ

91
00:06:59,210 --> 00:07:00,770
おめでとう

92
00:07:02,050 --> 00:07:05,500
また 明らにすることがある 典型的なConvNetでは

93
00:07:05,500 --> 00:07:07,870
３種類の層がある

94
00:07:07,870 --> 00:07:13,615
1つは 畳み込み層だ しばしば Conv 層 と言う

95
00:07:13,615 --> 00:07:17,025
それは 先ほどのネットワークで使っていた

96
00:07:17,025 --> 00:07:20,893
そして あなたが まだ 見ていない
２つの 別の一般的な種類の層があることも 明かそう

97
00:07:20,893 --> 00:07:23,945
でも それは この先の２~３のビデオでね

98
00:07:23,945 --> 00:07:28,272
1つは プーリング層だ 私は しばしば これをプールと呼ぶ

99
00:07:28,272 --> 00:07:32,241
そして 最後は 全結合層 だ  FCと呼ぶ

100
00:07:32,241 --> 00:07:36,466
とても良いニューラルネットワークをデザインするのは

101
00:07:36,466 --> 00:07:41,278
畳み込み層だけでも可能だが
多くのニューラルネットワーク構造には

102
00:07:41,278 --> 00:07:43,569
プーリング層と 少しの全結合層がある

103
00:07:46,398 --> 00:07:48,103
幸運にも プーリング層と

104
00:07:48,103 --> 00:07:52,340
全結合層は 畳み込み層よりも 定義するのが少し簡単だ

105
00:07:54,150 --> 00:07:58,472
次の２つのビデオで サッと それをやり

106
00:07:58,472 --> 00:08:03,173
畳み込みニューラルネットワークで使われる 
一般的な種類の層への感覚を得よう

107
00:08:03,173 --> 00:08:06,725
そして 今見たものより もっと強力なネットワークを

108
00:08:06,725 --> 00:08:07,290
組み立てよう

109
00:08:08,990 --> 00:08:14,110
もう一度 おめでとう
最初の 完全な畳み込みニューラルネットワークを理解した

110
00:08:14,110 --> 00:08:18,450
今週の後の方で これらのネットワークを訓練する方法について話すけど

111
00:08:18,450 --> 00:08:22,180
まずは プーリングと全結合層について 手短に話そう

112
00:08:22,180 --> 00:08:24,659
それから これらを訓練する
誤差逆伝播を使ってね

113
00:08:24,659 --> 00:08:26,241
これは もう知っているよね

114
00:08:26,241 --> 00:08:30,421
でも 次のビデオでは プーリング層の実装方法を サッと調べよう

115
00:08:30,421 --> 00:08:31,230
あなたの ConvNet のために