畳み込み層以外に ConvNet は しばしば プーリング層を使い 表現のサイズを減らす 計算を速くするために また 検出される特徴量を 少しだけ堅牢にするために じゃ見てみよう プーリングの例を見よう そして なぜ これをしたいのかについて 話そう 4 x 4 の入力があったとする そして プーリングの一種 最大プーリングを適用したいとする この 最大プーリングの特定の実装からは 2 x 2 の出力が得られる それを行う方法は 全く単純だ 4 x 4 入力を 異なる領域に分ける このように４つの領域に色を付けよう そして 出力では それは 2 x 2 だ 出力のそれぞれは 対応する色付けしたの領域からの最大値だ 例えば 左上なら この４つの数の最大値 ９ だ 右上では 青い数の最大値は ２ 左下では 一番大きな数は ６ 右下では 一番大きな数は ３ 右にある各数を計算するのに 2 x 2 領域の最大値を取った そう これは サイズ２のフィルターを適用したようなものだ なぜなら 2 x 2 領域を取って ２ストライドしているから これらは 実際には 最大プーリングの ハイパーパラメータだ
このフィルターサイズから初めたので それは 2 x 2 領域で ９を与える それから ２ステップ動かし この領域に来て ２が得られる それから 次の行では ２ステップ下に行き ６を得る それから ２ステップ右に行き ３を得る 四角形が 2 x 2 なので f = 2
そして ２つストライドするので s = 2 ここに 最大プーリングが行っていることの背後にある考え方を述べておこう もし この 4 x 4 領域を 何かの特徴量のセットと捉えるなら ニューラルネットワークの何かの層における活性化だ それから 大きな数 それは 特定の特徴を検出するであろうことを意味している 左上1/4は この特定の特徴を持ち それは 垂直エッジかもしれないし 猫を検出するための目やほお髭かもしれない 明らかに その特徴は 左上1/4に存在する 一方 この特徴は 猫の目を検出しないかもしれない しかし この特徴は 右上1/4には存在しない つまり 最大値処理が行っているのは 1/4領域内のどこかにある特徴を検出することで それは 最大プーリングの出力の中に保持される そう 最大値処理がしていることは 言うならば 特徴が このフィルター内に見つかった場合 高い数を取っておくということだ しかし もし その特徴が見つからなければ 例えば その特徴が右上の1/4に存在しないかもしれない その場合は それらの数の最大値は とても小さい 多分 これが 最大プーリングの背後にある洞察だ しかし 認めなくてはならないことがある 人々が 最大プーリングを使う主な理由は 多くの試行で上手くいくことが分かったからだ 私が述べた洞察は しばしば 言及されはするが 真の隠れた理由を完全に分かっている人を 私は知らない 私は そんな人を知らない ConvNet で 最大プーリングが上手くいくことの真の隠れた理由を知る人を 最大プーリングの興味深い特徴の1つが ハイパーパラメータは持つが 学習するパラメータを持たない というのがある 実際 勾配降下法で学習させるものは 何もない 一度 f と s を決めたら それは 固定の計算機となり 勾配降下法は何も変えない いつくか別のハイパーパラメータを持つ例を試してみよう ここに 5 x 5 の入力がある そして 3 x 3 のフィルターサイズを持つ 最大プーリングを適用する f = 3 で ストライドは 1 にする よって この場合は 出力サイズは 3 x 3 になる そして 前のビデオで開発した 畳み込み層の出力サイズを決める公式は それらの公式は 最大プーリングでも機能する (n + 2p - f) / 2 + 1 この公式は 最大プーリングの出力サイズを決めるのにも使える でも この例では 3 x 3 出力の個々の要素を計算してみよう 左上の要素は この領域を見ることになり お分かりの通り これは 3 x 3 領域だ なぜなら フィルターサイズが３だから
それで 最大値はこう それは ９ それから １つだけずらす なぜなら １つだけストライドできるから それで 青い箱の中の最大値は ９ また ずらして 青い箱の中の最大値は ５ それから 次の行に行く １ストライドで つまり １ステップだけ下に行く この領域の最大値は ９ この領域の最大値は ９ この領域の最大値は 今度は ２つの５があり ２つの５の最大値がある 最後は ここの最大値は ８ ここの最大値は ６ ここの最大値は 角にある９ いい？ これが ハイパーパラメータ f = 3, s = 1 のセットが ここに表示された出力を与えた ここまでは ２D入力に対しての 最大プーリングを見てきた ３D入力の場合は 出力は 同じ次元数を持つ 例えば 5 x 5 x 2 なら 出力は 3 x 3 x 2 になり 最大プーリングを計算する方法は 各チャンネル毎に個別に これまでに説明した計算を行うことだ よって ここに示した 上にある最初のチャンネルは 前と同じで ２番目のチャンネルに対しては この 下に描いたものだが この面の値に対して 同じ計算を行い この２番目の面を得る より一般的には これが 5 x 5 x チャンネル数 ならば 出力は 3 x 3 x 同じチャンネル数 になる 最大プーリングの計算は これら nC 個のチャンネル個々に対して 独立して行われる これが 最大プーリングだ これは そんなに多くは使われない種類のプーリングだ でも 平均プーリングに簡単に触れておこう 予想しているだろうけど 各フィルターで最大値を取る代わりに 平均値を取るんだ この例において 紫の平均は 3.75 それから 1.25 そして 4 と 2 これが ハイパーパラメータ f = 2, s = 2 の平均プーリングだ 他のハイパーパラメータを選ぶこともできる 最近は 最大プーリングが大変多く使われてる 平均プーリングよりも頻繁に 1つの例外はあるが それは 時々 非常に深いニューラルネットワークで 表現を崩すために 平均プーリングを使うかもしれない つまり 7 x 7 x 1000 この全ての範囲に渡って 平均を取ると 1 x 1 x 1000 となる この例は 後でみる ただし 最大プーリングは 平均プーリングよりも 大変多く使われている まとめると プーリングのハイパーパラメータは フィルターサイズ f と ストライド s 良く使われるパラメータ値は f = 2, s = 2 これは 本当に良く使われていて 高さと幅を 因数となる上の２で縮める効果がある 良く使われるハイパーパラメータは f = 2, s = 2 で 表現の高さと幅を 因数の２で縮める効果がある f = 3, s = 2 というのも見たことが有る 別のハイパーパラメータは 2進ビットだ つまり 最大プーリングか平均プーリングか だ もし望むなら 追加のハイパーパラメータも追加できる パディングだ ただし これは滅多に使われない 最大プーリングをする場合 通常 パディングは使わない しかし 次週見るように 1つの例外がある しかし 大抵の最大プーリングでは 通常 いかなるパディングも行わない よって 最も一般的な p の値は これまでのところ p = 0 最大プーリングの入力が 入力ボリュームのサイズが nH x nW x nC だと 出力ボリュームサイズは こうなる パディング無しと仮定すると (nW - f) / s + 1 x nC 入力チャンネル数と出力チャンネル数は等しい なぜなら プーリングは チャンネル毎に独立して適用されるから プーリングの特筆すべき点は 学習するパラメータが無いということだ よって 逆伝播を実装する時に 最大プーリングには 逆伝播を適用する何のパラメータも無いことに気付くだろう 代わりに 一度設定しておくハイパーパラメータだけがある 手でそれらを設定したり 公差検証を使い設定したりするだろう それが終われば 完了だ これは ニューラルネットワークが 層の1つで 計算するただの固定の関数だ そこには 何も学習するものがない これは ただの固定関数だ そう これがプーリングだ どのように畳み込み層とプーリング層を作るか 分かったね 見ての通り、ここある。 ConvNet のもっと複雑な例を見よう それは 全結合層の導入に繋がるものだ