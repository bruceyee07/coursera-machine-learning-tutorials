完全な畳み込みニューラルネットワークを作るための 全構成要素が良く分かったでしょう 例を見てみましょう。 32 x 32 x 3 の画像を入力とする これは RGB画像だ
手書き数字認識をしようとしている 32 x 32 画像には ７のような数がある ０~９の数字の中から1つを識別しようとしている ニューラルネットワークでこれを行おう このスライドで使うのは クラシック ニューラルネットワークの1つ LeNet-5 とよく似た それから触発されたものだ それは Yann LeCun が何年も前に作った ここの見せるのは LeNet-5 そのものでは無いが それから触発されたもので 多くのパラメータ選択が そこから来ている 32 x 32 x 3 入力に対し 最初の層だが 5 x 5 フィルターで ストライド１ そしてパディング無し よって この層の出力は もし ６個のフィルターを使うなら 28 x 28 x 6 この層を "Conv 1" と呼ぼう ６フィルターを適用し バイアスを加え 非線形化する ReLU 非線形だろう そして これが Conv1の出力だ 次に プーリング層を適用しよう 最大プーリングを適用しよう f = 2, s = 2 にする パディングを書かない場合は パディングは０だ 次に プーリング層を適用しよう 最大プーリングを適用しよう 2 x 2 フィルターでストライドは２ これは 表現の高さと幅を 因数２で減らす よって 28 x 28 は 14 x 14 となる チャンネル数はそのままだから 14 x 14 x 6 これを "Pool 1" 出力と呼ぼう ConvNet の文献では ２つの表現があって それは層と呼ぶものについてだが 少しだけ一致しない点がある 1つの表現では これを層と呼ぶ よって これが ニューラルネットワークの "Layer 1" だ もう1つの表現では 畳み込み層で１層 プール層で１層と数える 人々がニューラルネットワークの層数を報告する場合 通常は 重みやパラメータを持っている層の数を上げる そして プーリング層は 重みもパラメータも持たず ただ若干のハイパーパラメータしか持たないので ここでは "Conv 1" と "Pool 1" を一緒にする表記法を使おう よって これらを "Layer 1" として扱う
ただし 時々 オンラインで文献を読んだり 研究報告を読んだりした場合 畳み込み層とプーリング層を別々の層のように言っている場合もある これが ２つの表記用語における 若干の不一致点だ しかし 私は 層を数える時 重みを持つ層を数える よって この２つは一緒にして "Layer 1" となる そして 名前の "Conv 1" と "Pool 1" の最後にある１は 両方とも ニューラルネットワークの "Layer 1" を参照している そして "Pool 1" は "Layer 1" に組み込まれる
それ自身は重みを持たないからだ 次に 14 x 14 x 6 ボリュームを 別の畳み込み層に適用させよう
フィルターサイズは 5 x 5 ストライドは１ そして 今度は10個のフィルターにする それで 結局 10 x 10 x 10 ボリュームとなる
これを "Conv 2"と呼ぼう それから 最大プーリングを 再び f = 2, s = 2 で行う 多分 当たりを付けているだろうけど この出力は f = 2, s = 2 で これは 高さと幅を 因数２で減らし 5 x 5 x 10 が残る それでは これを "Pool 2" と呼ぼう 我々の表記規約では これは ニューラルネットワークの "Layer 2" だ では これを別の畳み込み層に適用させよう フィルターサイズは 5 x 5 つまり f = 5 ストライドは１ パディングは書かない これはパディング無しのこと これで "Conv 2" 出力が得られる
そして 16個のフィルターを使おう そうすると 10 x 10 x 16 次元の出力 こうなるね これが "Conv 2" 層だ それから 最大プーリングを適用しよう f = 2, s = 2 で 多分 分かるだろうけど この出力は 10 x 10 x 16
f = 2, s = 2 の最大プーリングだと これは 高さと幅を半分にする 結果が分かるでしょ？ f = 2, s = 2 で最大プーリングする これは 高さと幅を半分にする それで 結局 5 x 5 x 16 ボリュームを得る チャンネル数は前と同じ これを "Pool 2" と呼ぼう 我々の表記規約では "Layer 2" だ なぜなら これは "Conv 2" 層に重みセットを持つからだ さぁ 5 x 5 x 16 だ
5 x 5 x 16 = 400 それじゃ "Pool 2" を 400 x 1 次元ベクトルに平坦化しよう これを平坦化すると こんな感じのニューロンのセットになる それから やることは この400ユニットを取ってきて 次のの層を構築する 120ユニットを持っている そう これが最初の全結合層だ これを "FC3" と呼ぼう なぜなら 400ユニットが 120ユニットと密に結合しているからだ この 全結合ユニット 全結合層は コース１と２で見た 単なるニューラルネットワーク層だ これは標準的なニューラルネットワークで そこには重み行列がある それを"W3"と呼ぼう その次元数は 120 x 400 だ これは全結合層と呼ばれる なぜなら ここの400ユニットのそれぞれが
ここの120ユニットのそれぞれに結合しているからだ そして バイアスパラメータもある そう ちょうど120次元のがあり 120個の出力となる それから最後に 120ユニットを取り 別の層を加える 今度はすこ小さめで 84ユニットある これを 全結合層４と呼ぼう 最終的に84個の実数を得た
これをソフトマックスユニットに喰わせることができる そして 手書き数字認識をしようとしているのなら これは 0 1 2 と続いて 9 までになる よって これは 10個の出力を行うソフトマックスになる そう これが 畳み込みニューラルネットワークが どのようになるかの 妥当で典型的な例だ 非常に沢山のハイパーパラメータがあるように見えるかもしれない 後で 具体的な提言をするよ どのように これらのハイパーパラメータを選ぶのかについて 1つの一般的ガイドラインは 自分独自のハイパーパラメータの設定をしようとせずに 文献を見て どんなハイパーパラメータに自分が取り組んでいるのか見てみることだ そして 別の誰かが上手く行った構造を選べば それは あなたのアプリケーションでも上手く行く可能性がある 次週 その事について もっと話すよ でも今は 単に次の点を指摘しておきたい
ニューラルネットワークが深くなるにつれ 通常 nH nW そして 高さと幅は減っていく 前に指摘したよね
それは 32 x 32 が 20 x 20, 14 x 14, 10 x 10, 5 x 5 になる より深くなれば 通常 高さと幅は減っていく 一方 チャンネル数は増えていく 3, 6, 16 と増えていく
そして 最後に全結合層に行く ニューラルネットワークにおける 別の かなり一般的なパターンに
畳み込み層が １つ以上の畳み込み層があり 1つのプーリング層が続き そして １つ以上の畳み込み層があり 1つのプーリング層が続く そして 最後に いくつかの全結合層が来て ソフトマックスが続く というものがある これが ニューラルネットワークにとてもよく見られる別のパターンだ それでは このニューラルネットワークについて もう少し詳細を見てみよう
活性化の shape や 活性化のサイズ そして ネットワークのパラメータ数がどうなるのか 入力は 32 x 30 x 3 これらをかけ合わせれば 3,072を得る よって 活性化 a[0] は 3,072次元だ 本当は 32 x 32 x 3 入力層には何のパラメータも無い そして 別の層を見れば 詳細は自分で勝手に計算してね これらが活性化の shape で そして 活性化のサイズだ 何点か指摘しておく 最初に プーリング層 最大プーリング層は 何のパラメータも持たない ２番目に 畳み込み層は 比較的 少ない数のパラメータを持つ傾向がある 前のビデオで議論したようにね そして実際 パラメータの多くは ニューラルネットワークの 全結合層にある傾向がある それから 活性化のサイズは ニューラルネットワークが深くなるにつれ だんだんと少なくなるのに気付いただろう もし これが急に下がる場合は 通常あまり良い性能にならない それは 最初 6,000 で 1,5 1,600 そして ゆっくり 84に落ちていき
最後にソフトマックス出力となる 多くの ConvNet が傾向を持っている これと似たパターンを持っている さぁ ニューラルネットワーク 畳み込みニューラルネットワークの 基本構成要素を見てきた
畳み込み層 プーリング層 そして 全結合層 多くのコンピュータ ビジョン研究が明らかにしてきた
どのように これら基本構成要素をまとめ 効力のあるニューラルネットワークを作るかを そして これらを纏めて 非常に多くの洞察を得てきた 思うに どのようにこれらを纏めるのか洞察を得るのに一番良い方法は 他の人が行った具体例を数多く見ることだ それで 次週行うことは この最初のを超える具体例をいくつか 見ることだ そこでは 人々はうまくこれらを纏めて 効果的なニューラルネットワークを作成するのに成功している そして 次週のビデオでは あなたが これらをどうやって作るかについて 自分自身の洞察を持つようになることを期待する 具体的な例・構造があるれば 単にそれを利用できるし 他の誰かが作ったものをそのまま自身のアプリケーションに使うこともできる それは 次週行おう 今週のビデオを纏める前に
最後に1つ言っておくことが有る それは なぜ畳み込みを使いたいのだろうか ということ 畳み込みを使うことの 何らかの利点や強み
また どうやって全体を纏めるのか ということだ ここで見たようなニューラルネットワークを どのようにして得たり
学習データを使って どのように実際に学習するのか 画像認識やその手のタスクを実現するのに それでは 今週の最後のビデオに進もう