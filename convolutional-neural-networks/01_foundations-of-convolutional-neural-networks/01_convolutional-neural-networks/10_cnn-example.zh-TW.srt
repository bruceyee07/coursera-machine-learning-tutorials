1
00:00:00,021 --> 00:00:02,721
您現在已經幾乎知道了所有建立

2
00:00:02,721 --> 00:00:04,509
完整的卷積神經網路的基石

3
00:00:04,509 --> 00:00:07,336
我們來看一個例子

4
00:00:07,336 --> 00:00:12,436
假設您輸入一個 32乘32乘3 的影像

5
00:00:12,436 --> 00:00:18,855
所以是一個 RGB 影像，或許您試著做手寫數字辨識

6
00:00:18,855 --> 00:00:24,396
所以您有一個數字像是 7 在 32乘32 RGB，您試著

7
00:00:24,396 --> 00:00:30,495
辨識 10 個數字中從 0 到 9 
這個是哪一個

8
00:00:30,495 --> 00:00:32,791
我們來建立一個神經網路來做這件事

9
00:00:32,791 --> 00:00:35,827
而在這個投影片中
我所使用的靈感

10
00:00:35,827 --> 00:00:41,106
實際上是來自於典型的神經網路稱為 LeNet-5

11
00:00:41,106 --> 00:00:43,942
是由 Yann LeCun 許多年前建立的

12
00:00:43,942 --> 00:00:47,680
這裡我要顯示的並不完全是 LeNet-5

13
00:00:47,680 --> 00:00:53,024
但靈感來自於它，
很多的參數選擇的靈感來自於它

14
00:00:53,024 --> 00:00:58,524
所以拿這個  32乘32乘3 輸入，假設第一層

15
00:00:58,524 --> 00:01:04,770
使用了 5乘5 過濾器且跨步為 1, 不使用填充

16
00:01:04,770 --> 00:01:08,240
所以這一層的輸出

17
00:01:08,240 --> 00:01:13,732
如果您使用了 6 個過濾器，輸出會是 28乘28乘3

18
00:01:13,732 --> 00:01:18,803
我們稱這一層為 conv 1

19
00:01:18,803 --> 00:01:23,660
所以您用了 6 個過濾器，增加了偏差，使用了非線性

20
00:01:23,660 --> 00:01:28,356
或許是 ReLU 非線性，
而這就是 conv 1 輸出

21
00:01:28,356 --> 00:01:32,678
接下來，我們來應用池層，

22
00:01:32,678 --> 00:01:40,280
我將使用最大池層，假設使用 f=2, s=2

23
00:01:40,280 --> 00:01:44,101
當我沒寫填充時，填充預設為 0

24
00:01:44,101 --> 00:01:48,895
接下來應用池層，我將使用

25
00:01:48,895 --> 00:01:54,975
最大池層，使用  2乘2 過濾器，跨步為 2

26
00:01:54,975 --> 00:01:57,064
這個應該會降低高度跟

27
00:01:57,064 --> 00:01:59,614
寬度縮為一半

28
00:01:59,614 --> 00:02:04,138
所以 28乘28 變為 14乘14

29
00:02:04,138 --> 00:02:10,472
通道的數目維持一樣是 14乘14乘6

30
00:02:10,472 --> 00:02:15,536
我們稱之為 Pool 1 輸出

31
00:02:15,536 --> 00:02:20,111
實際上在 ConvNet 文獻中，有兩種

32
00:02:20,111 --> 00:02:25,562
約定，當人們說一層時，會有點不一樣

33
00:02:25,562 --> 00:02:30,918
一種約定是稱這個為一層

34
00:02:30,918 --> 00:02:35,900
所以在神經網路中這是一層，而另一種約定

35
00:02:35,900 --> 00:02:40,980
會稱這個 conv 層為一層，這個 pool 層為一層

36
00:02:40,980 --> 00:02:45,223
當人們報告神經網路有幾層時，通常人們

37
00:02:45,223 --> 00:02:49,025
只紀錄那些有權重，有參數的那些層

38
00:02:49,025 --> 00:02:53,043
而因為池層沒有權重，沒有參數

39
00:02:53,043 --> 00:02:57,418
只有一些超參數，我使用的約定會是

40
00:02:57,418 --> 00:02:59,015
Conv1 跟 Pool1 共享

41
00:02:59,015 --> 00:03:03,551
我會將他們視為第一層，雖然有時候您會見到

42
00:03:03,551 --> 00:03:08,447
或許人們讀線上文章或者讀研究論文，您會聽到  conv 層跟

43
00:03:08,447 --> 00:03:11,703
pooling 層, 似乎是他們有兩層

44
00:03:11,703 --> 00:03:16,788
但或許這會有些符號專有名詞的不一致性

45
00:03:16,788 --> 00:03:22,053
但是當我計算層數時，
我將只計算那些有權重的層

46
00:03:22,053 --> 00:03:25,614
所以會將它們是為一層 

47
00:03:25,614 --> 00:03:30,822
而 Conv1 跟 Pool1 的名字裡的 1

48
00:03:30,822 --> 00:03:37,961
指的也就是我將它們
視為神經網路的第一層

49
00:03:37,961 --> 00:03:42,665
而 Pool1 被分到第一層
因為它沒有自己的權重

50
00:03:42,665 --> 00:03:47,585
接著，對於這個 14乘14乘6 容積，我們應用另一個

51
00:03:47,585 --> 00:03:53,181
卷積層，我們使用一個過濾器大小為 5乘5

52
00:03:53,181 --> 00:03:58,796
我們使用跨步 1，這次使用 10 個過濾器

53
00:03:58,796 --> 00:04:04,350
最後您會得到一個 10乘10乘10

54
00:04:04,350 --> 00:04:09,786
容積，我稱之為 Conv2

55
00:04:09,786 --> 00:04:14,467
然後在這個網路，我們做一個

56
00:04:14,467 --> 00:04:19,008
最大池化，再一次使用 f=2, s=2

57
00:04:19,008 --> 00:04:23,456
所以您大概可以預測這個輸出對於 f=2,

58
00:04:23,456 --> 00:04:26,769
s=2, 這會減低高度跟

59
00:04:26,769 --> 00:04:31,425
寬度為一半，所以您留下來會是 5乘5乘10

60
00:04:31,425 --> 00:04:34,773
所以我將其稱為 Pool2

61
00:04:34,773 --> 00:04:39,652
而在我們的約定中，這個是神經網路的第二層

62
00:04:39,652 --> 00:04:42,293
現在讓我們在用另一個卷積層到這上面

63
00:04:42,293 --> 00:04:47,113
我將使用一個 5乘5 過濾器，所以 f=5, 我們用跨步

64
00:04:47,113 --> 00:04:51,962
1, 我不寫填充，意思是不用填充

65
00:04:51,962 --> 00:04:58,254
而這會給您 Conv2 輸出，而那是您的 16 個過濾器

66
00:04:58,254 --> 00:05:03,860
所以這會是 10乘10乘16 維度的輸出

67
00:05:03,860 --> 00:05:10,380
所以我們看這個，這是 Conv2 層

68
00:05:10,380 --> 00:05:17,356
然後讓我們應用最大池化到這個
用 f=2, s=2

69
00:05:17,356 --> 00:05:20,227
您大概可以猜測這個的輸出

70
00:05:20,227 --> 00:05:24,555
我們在 10乘10乘16 上用 f=2, s=2, 最大池化

71
00:05:24,555 --> 00:05:26,667
這會將高度跟寬度減半

72
00:05:26,667 --> 00:05:31,075
您可以猜測這樣的結果，對吧？

73
00:05:31,075 --> 00:05:32,463
使用 f=2, s=2 最大池化

74
00:05:32,463 --> 00:05:37,663
這應該將高度跟寬度減半，所以您最終會有

75
00:05:37,663 --> 00:05:43,214
一個 5乘5乘16 容積，跟之前一樣的通道

76
00:05:43,214 --> 00:05:47,166
我們將其稱為 Pool2

77
00:05:47,166 --> 00:05:52,429
而在我們的約定中，這會是第二層，因為

78
00:05:52,429 --> 00:05:57,203
這個有一組的權重在您的 Conv2 層

79
00:05:57,203 --> 00:06:03,306
現在 5乘5乘16, 5乘5乘16 等於 400

80
00:06:03,306 --> 00:06:10,895
所以現在攤平我們的 Pool2 成為一個 400乘1 維度向量

81
00:06:10,895 --> 00:06:16,686
因此把這個想成是攤平這些變成一組神經元，像這樣

82
00:06:16,686 --> 00:06:22,373
我們要拿這些 400  個單元

83
00:06:22,373 --> 00:06:30,070
來建立下一層成為 120 個單元

84
00:06:30,070 --> 00:06:33,243
所以這其實就是我們第一個全連結層

85
00:06:33,243 --> 00:06:38,392
我將它稱為  FC3, 因為我們

86
00:06:38,392 --> 00:06:44,410
用  400 個單元密集連結了 120 個單元

87
00:06:46,245 --> 00:06:51,628
所以這個全連結單元，這個全連結層就像

88
00:06:51,628 --> 00:06:56,660
是一個您在第一，二課程中的
所學的單一的神經網路層

89
00:06:56,660 --> 00:07:01,710
這只是一個標準的神經網路，您用

90
00:07:01,710 --> 00:07:08,044
權重矩陣，被稱為 W3 維度是  120乘400

91
00:07:08,044 --> 00:07:13,406
而這個稱為全連結因為
400 個單元中每一個單元都連結到

92
00:07:13,406 --> 00:07:18,354
這 120 個單元中每一單元，您也會有一個偏差參數

93
00:07:18,354 --> 00:07:23,655
而那個的維度會是 120，這是 1`20 輸出

94
00:07:23,655 --> 00:07:28,715
最後，我們拿這個 120個單元，加入另一層

95
00:07:28,715 --> 00:07:33,119
這一次小一點，假設我們有 84個單元

96
00:07:33,119 --> 00:07:36,883
我將它稱為全連結第四層

97
00:07:36,883 --> 00:07:44,435
最後我們現在將這 84 個實數您可以代入 "softmax" 單元

98
00:07:44,435 --> 00:07:48,215
而如果您試著用來辨別手寫數字

99
00:07:48,215 --> 00:07:51,794
用來辨識從 0, 1, 2, 一直到  9

100
00:07:51,794 --> 00:07:56,680
那這個會是一個 softmax 有 10 個輸出

101
00:07:56,680 --> 00:08:00,969
所以這是一個典型的

102
00:08:00,969 --> 00:08:05,482
卷積神經網路的例子的樣子

103
00:08:05,482 --> 00:08:09,367
我知道這似乎有很多的超參數

104
00:08:09,367 --> 00:08:12,919
我們後面會給您一些更具體的建議

105
00:08:12,919 --> 00:08:15,882
對於如何選擇這些超參數

106
00:08:15,882 --> 00:08:20,388
或許一個通用的原則
是不要試著發明您自己

107
00:08:20,388 --> 00:08:22,802
的超參數設定，

108
00:08:22,802 --> 00:08:27,887
而是看看文獻上哪些超參數在別的地方作用得很好

109
00:08:27,887 --> 00:08:30,963
而直接選擇一個架構，別人已經作用得很好

110
00:08:30,963 --> 00:08:35,316
有很大的機會這些也會在您的應用上可行

111
00:08:35,316 --> 00:08:38,321
我們在下一週會見到更多有關於這類的事情

112
00:08:38,321 --> 00:08:43,715
但現在，我要指出當您進入更深度的神經網路

113
00:08:43,715 --> 00:08:47,493
通常 nh 跟 nw ，高度跟寬度會降低

114
00:08:47,493 --> 00:08:52,432
早先已經提過，就像這個從  32乘32, 到 20乘20, 到 14乘14

115
00:08:52,432 --> 00:08:53,934
到 10乘10, 到 5乘5

116
00:08:53,934 --> 00:08:57,870
所以當您用越深的網路，通常高度跟寬度會降低

117
00:08:57,870 --> 00:09:00,852
然而通道的數目卻會增加

118
00:09:00,852 --> 00:09:07,277
它從 3 到 6  到 16 然後最後是您的全連結層

119
00:09:07,277 --> 00:09:13,135
而另一個很常見的模式是
您見到在神經網路中有卷積層

120
00:09:13,135 --> 00:09:17,426
或許一個或多個卷積層接著一個池層

121
00:09:17,426 --> 00:09:21,329
然後一個或多個卷積層接著一個池層

122
00:09:21,329 --> 00:09:24,731
最後您會有一些全連結層

123
00:09:24,731 --> 00:09:26,756
然後接著一個 softmax 層

124
00:09:26,756 --> 00:09:32,378
這是另一個相當常見的模式
您在神經網路中常會見到

125
00:09:32,378 --> 00:09:33,956
所以讓我們來

126
00:09:33,956 --> 00:09:37,968
看一下這個神經網路的細節，有關於啟動值的形狀

127
00:09:37,968 --> 00:09:41,799
啟動值的大小，跟這個網路的參數數目

128
00:09:41,799 --> 00:09:44,181
所以輸入是  32乘32乘3

129
00:09:44,181 --> 00:09:48,324
如果您乘這些數字，您應當會得到 3,072

130
00:09:48,324 --> 00:09:54,313
所以這個啟動值，a0 的維度為 3072

131
00:09:54,313 --> 00:09:58,005
實際上為 32乘32乘3

132
00:09:58,005 --> 00:10:02,562
 而在輸入層並沒有參數

133
00:10:02,562 --> 00:10:05,672
而當您看一個不同的層

134
00:10:05,672 --> 00:10:09,068
您可以自己算算看

135
00:10:09,068 --> 00:10:10,975
這些是啟動值的形狀

136
00:10:10,975 --> 00:10:13,743
跟啟動值的大小在不同的層上

137
00:10:15,422 --> 00:10:16,957
只是想要指出一些事情

138
00:10:16,957 --> 00:10:23,352
首先，注意最大池層並沒有任何參數

139
00:10:23,352 --> 00:10:28,202
第二，注意  Conv 層的參數往往

140
00:10:28,202 --> 00:10:32,302
比之前影片中討論的參數要少

141
00:10:32,302 --> 00:10:36,414
而實際上，很多的參數，往往是在

142
00:10:36,414 --> 00:10:39,426
神經網路的全連接層上

143
00:10:39,426 --> 00:10:44,584
然後您注意到，啟動值的大小

144
00:10:44,584 --> 00:10:50,289
或許逐漸降低當您
進入更深層的神經網路時

145
00:10:50,289 --> 00:10:55,198
如果它下降太快，通常對於性能不太有幫助

146
00:10:55,198 --> 00:11:00,349
所以開始於 6,000 ，經過 1,600

147
00:11:00,349 --> 00:11:06,405
慢慢降到 84, 直到最後您使用了 softmax 輸出

148
00:11:06,405 --> 00:11:10,683
您會在很多的卷積網路中看到

149
00:11:10,683 --> 00:11:13,293
這些屬性類似的模式

150
00:11:13,293 --> 00:11:16,455
所以您現在己經見過基本的神經網路架構基石

151
00:11:16,455 --> 00:11:20,068
有卷積神經網路， conv 層，池層

152
00:11:20,068 --> 00:11:21,601
跟全連結層

153
00:11:21,601 --> 00:11:25,693
很多的電腦研究部門試著找出如何

154
00:11:25,693 --> 00:11:29,078
將這些基石放在一起來建立一個有效的神經網路

155
00:11:29,078 --> 00:11:33,379
而將這些放在一起實際上需要一些洞見

156
00:11:33,379 --> 00:11:35,213
我想最好的方式

157
00:11:35,213 --> 00:11:39,323
對於您想要獲取直觀，
有關於如何將這些放在一起，最好是

158
00:11:39,323 --> 00:11:41,804
看一些具體的例子看看別人怎麼做

159
00:11:41,804 --> 00:11:46,268
所以在下一個星期，
我會展示您一些具體的例子

160
00:11:46,268 --> 00:11:50,183
即使超過這第一個例子，
您將會見到人們如何成功地將

161
00:11:50,183 --> 00:11:53,637
這些事放一起，建立一個很有效的神經網路

162
00:11:53,637 --> 00:11:58,532
透過下一個星期的影片，我希望您能抓住一些自己的直觀

163
00:11:58,532 --> 00:12:00,098
有關如何建立這些網路

164
00:12:00,098 --> 00:12:05,068
所以當我們給予這些具體的例子，這些或許您可以

165
00:12:05,068 --> 00:12:09,120
直接使用這些別人開發的架構在您的應用上

166
00:12:09,120 --> 00:12:10,971
所以我們在下一星期會做這些

167
00:12:10,971 --> 00:12:15,499
在總結這個星期的影片之前，我想

168
00:12:15,499 --> 00:12:19,840
在下一個影片談一下，為什麼您想用卷積網路

169
00:12:19,840 --> 00:12:20,869
一些好處

170
00:12:20,869 --> 00:12:25,133
跟優點來使用卷積，跟如何將這些擺在一起

171
00:12:25,133 --> 00:12:29,021
如何拿一個您剛看過的神經網路，實際上訓鍊它

172
00:12:29,021 --> 00:12:32,735
在訓練集上來進行影像辨識的一些工作

173
00:12:32,735 --> 00:12:35,700
所以我們來進入這個星期的最後一個影片