ストライドした畳み込みは 畳み込みニューラルネットワークで使われる
畳み込みの構成要素のもう一つのピースだ 例を見てみよう この 7 x 7 画像を この 3 x 3 フィルターで畳み込むとしよう ただし 通常のやり方ではなく それを２ストライドで行う それが意味するのは 要素毎の掛け算は普通に行い この左上の 3 x 3 領域を掛け算と足し算を行い 91 を得る しかし この青い箱を１ステップ動かす代わりに ２ステップ動かす つまり こんな風に ２ステップ跳ばす 左上角は この点から この点へ 1つ飛ばしでジャンプする そして 通常の要素毎の掛け算と合計を行い 100 を得る そして また それを行っていく 青い箱を２ステップジャンプさせる この位置に来ることになり 83 を得る では 次の行に行こう 再び １ステップの代わりに ２ステップ取り 青い箱をここに動かす 位置を1つ跨いで 今度は 69 を得る そして また ２ステップ動かす 今度は 91 を得て 続けて 127 そして 最終行では 44 72 74 この例では 7 x 7 行列を畳み込んだ この 3 x 3 行列で そして 3 x 3 出力を得た 入力と出力の次元は 次の式に支配される もし n x n 画像があり f x f フィルターで畳み込むなら また パディング p と ストライド s を使うなら この例では s = 2 なので 出力は n + 2p - f そして 一度に s ステップ動かす これは １ステップ動かした場合ので 今度は s で割り そして 1 を足す
そして 同じことを行う 我々の例では 7 + 0 - 3 割る 2 ｓストライド + 1 は ええっと これは 4 ÷ 2 + 1 = 3 だから 3 x 3 出力を得ることになった では 最後の詳細を1つ この分数が整数にならなかったら？ その場合は それを丸める この記法は 切り捨てを意味する これは floor(Z) と呼ばれる これは Zを丸めて 最も近い整数に切り捨てする これを実装する方法は この種の青い箱の掛け算を 青い箱全てが 画像内に収まるか 画像＋パディング内に収まる限りにおいて 行う そして もし この青い箱の一部でも 外にはみ出すようなら 計算をしない よって 3 x 3 フィルターが 画像内 もしくは 画像＋パディング内に 完全に収まって初めて 対応する出力が生成される
これが規約となっている よって 出力次元を計算する正しい方法は この (n + 2p - f) / s が整数で無い場合は切り捨てる 次元数についてまとめよう n x n 行列 もしくは n x n 画像があり f x f 行列 もしくは f x f フィルターと
パティング p , ストライド s で畳み込むならば 出力サイズはこの次元になる 整数になるように これらの数を選ぶこともできる 最も 時にはそうする必要は無いし 切り捨てしまってもよい しかし n f の値の例を処理するのは気軽に考えて欲しい p と s については あなたが欲しい 出力サイズに対して正しい式となるようにする では 続ける前に 技術的な注意事項を挙げておきたい それは 相互相関と畳み込みに関するもので 畳み込みニューラルネットワーを実装するためのものだ もし 別の 数学や信号処理の教科書を読むと 表記が同じでないことに気付くかもしれない 典型的な数学の教科書では 畳み込みが 要素毎の掛け算と合計を行う前に 定義されている そこには 初めに行う もう一つの別のステップがある それは この 6 x 6 行列を 3 x 3 フィルターで畳み込むことなんだけど 最初に この 3 x 3 フィルターをひっくり返して 水平と垂直の軸を入れ替えることだ
そうすると この 3 4 5 1 0 2 -1 9 7 は こうなる
3 はここに 4 はここに 5 はここに
そして2番目の行は ここで 1 0 2, -1 9 7 これは 3 x 3 フィルターを取り出し それを ナローイングしている 垂直軸と水平軸 両方について じゃぁ このひっくり返した行列を コピーしてここに置く 出力を完了させるのに 2 x 7 して + 3 x 2 + 7 x 5 等々としていく このひっくり返した行列を掛け合わせなくはならない このような 4 x 4 出力の左上の要素を計算するのに 次に これら９つの数を １つずつ ずらしていく 我々が このビデオで 畳み込み処理として定義したやり方では この ナローイング処理を省略した 技術的には 我々が 実際にしていること これまでのビデオで使ってきた処理は 時には 畳み込みではなく 相互相関 と見なされる しかし ディープラーニング文献の慣例では これを 単に 畳み込み処理と呼ぶ まとめると 機械学習の慣例では このひっくり返す処理には関わらない 技術的には この処理は 相互相関と呼ぶべきだが ディープラーニング文献では 単に畳み込み処理と呼ぶ このコースのビデオでは 同じく この慣例を使う そして もし たくさんの機械学習の文献を読めば 多くの人々が このひっくり返しを使うことを気にせずに 単に これを 畳み込み処理と呼んでいる のに気付くだろう 信号処理やある種の数学の分野における 畳み込みの定義では このひっくり返しを行う なぜなら 畳み込み演算子に この有益な性質を持たせたいから (A * B) C = A * (B * C) これは 数学では 結合律 と呼ばれる これは 信号処理を行うアプリケーションでは 有益なことがある しかし ディープニューラルネットワークにとっては 意味が無い よって この２重の反転を省略し コードを簡単にした方が ニューラルネットワークには 却って良い そして 慣例では 我々の殆どが これを 畳み込みと呼ぶ また 数学者でさえも この相互相関を そう呼ぶのを好むことがある また あなたが 演習問題を実装するのに 何の影響も無いし ディープラーニングの文献を読んで理解する能力にも影響しない さぁ これで 畳み込みを行う方法が 理解できたでしょう そして パディングやストライドを 畳み込みに使う方法も分かったでしょう ただし ここまでは 畳み込みを行列に使ってきた 6 x 6 行列とかに 次のビデオでは 畳み込みをボリュームに対して行う方法を見る
(訳注:ボリュームは ２次元配列の行列に対し ３次元配列のことを指している) これにより 畳み込みが 本当に非常に強力だ と感じられることでしょう 次のビデオに進みましょう