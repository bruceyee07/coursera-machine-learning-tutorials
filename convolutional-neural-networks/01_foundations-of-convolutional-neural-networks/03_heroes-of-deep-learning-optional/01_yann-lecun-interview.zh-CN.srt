1
00:00:03,576 --> 00:00:06,447
嗨 Yann，长久以来您一直是深度学习的领导者

2
00:00:06,447 --> 00:00:08,730
谢谢你能过来和我一起来聊聊

3
00:00:08,730 --> 00:00:09,852
谢谢你能邀请我过来

4
00:00:09,852 --> 00:00:12,820
你已经在神经网络领域工作了很长时间了

5
00:00:12,820 --> 00:00:17,260
我特别想听听你的故事， 关于你怎么开始从事人工智能

6
00:00:17,260 --> 00:00:22,097
怎么开始研究神经网络？— 我一直对于

7
00:00:22,097 --> 00:00:27,957
“智慧”，或者说哪里产生了人类的智慧很感兴趣

8
00:00:27,957 --> 00:00:32,127
从小就对人类的演化很感兴趣

9
00:00:32,127 --> 00:00:33,060
那时你应该在法国吧？

10
00:00:33,060 --> 00:00:33,862
对， 是的， 当时在法国

11
00:00:33,862 --> 00:00:37,690
当时我在上中学

12
00:00:37,690 --> 00:00:42,484
我对科技，太空特别感兴趣

13
00:00:42,484 --> 00:00:44,840
我最喜欢的电影是：
 2001太空漫游 (2001 Space Odyssey)

14
00:00:44,840 --> 00:00:48,438
在剧里面， 你可以看到智能机器， 空间旅行， 还有

15
00:00:48,438 --> 00:00:53,495
人类演化等等这类东西， 这些都让我特别着迷

16
00:00:53,495 --> 00:00:57,160
包括智能机器的概念， 我觉得真的特别吸引我

17
00:00:57,160 --> 00:01:00,820
在那之后我开始学习电子工程

18
00:01:00,820 --> 00:01:05,086
当我在上学的时候，在我读工程学院二年级的时候

19
00:01:05,086 --> 00:01:08,554
我无意间看到了一本书， 其实是一本哲学方面的书

20
00:01:08,554 --> 00:01:14,181
是关于MIT计算语言学者Noam Chomsky

21
00:01:14,181 --> 00:01:18,112
和一位心理认知科学家 Jean Piaget，

22
00:01:18,112 --> 00:01:22,210
来自瑞士儿童发展心理学家的辩论

23
00:01:22,210 --> 00:01:25,934
这是一场关于先天和后天的争论

24
00:01:25,934 --> 00:01:31,026
Chomsky 认为语言有很多先天的结构

25
00:01:31,026 --> 00:01:34,220
但是Piaget认为这些很多事后天学习的

26
00:01:34,220 --> 00:01:40,218
在Piaget这边他援引了一个人的说法

27
00:01:40,218 --> 00:01:48,670
你知道，他们这两边的人都找了一堆人来为自己这边辩护

28
00:01:48,670 --> 00:01:53,280
在Piaget这边的是来自MIT的Seymour Papert

29
00:01:53,280 --> 00:01:57,382
他一直在研究第一个可以运行的感知模型

30
00:01:57,382 --> 00:02:00,273
那是我还没有听过感知器， 然后我就读了那篇文章

31
00:02:00,273 --> 00:02:02,535
文章中说这种感知模型是可以运转的， 这让我感觉太神奇了

32
00:02:02,535 --> 00:02:07,011
所以之后我开始去几个大学的图书馆然后搜索

33
00:02:07,011 --> 00:02:11,926
任何我能找到的关于感知器的文章，然后发现

34
00:02:11,926 --> 00:02:16,615
大部分的文章是50年代的， 而且到60年代就中断了

35
00:02:16,615 --> 00:02:20,480
找到一本由Seymour Papert一起合著的书

36
00:02:20,480 --> 00:02:21,490
— 那是哪一年？

37
00:02:21,490 --> 00:02:22,792
— 那一年是1980年

38
00:02:22,792 --> 00:02:23,920
差不多。 — 嗯， 对

39
00:02:23,920 --> 00:02:28,470
— 所以我和我的数学老师做了几个项目

40
00:02:28,470 --> 00:02:32,060
关于神经网络的

41
00:02:32,060 --> 00:02:35,118
但是我却找不到在这个领域工作可以聊天的人

42
00:02:35,118 --> 00:02:38,596
因为这个领域在那段时间已经消失了

43
00:02:38,596 --> 00:02:40,616
从1980年开始， 没有人在这个领域工作

44
00:02:42,791 --> 00:02:45,363
然后我做了一点探索

45
00:02:45,363 --> 00:02:50,740
写了一些不同种类的模拟软件， 看了关于神经科学的书

46
00:02:52,090 --> 00:02:58,120
当我完成工程学院的学习时， 我学的是芯片设计

47
00:02:58,120 --> 00:03:01,710
我那时非常善于芯片设计， 所以这对我来说是很不同的体验

48
00:03:01,710 --> 00:03:05,340
当我完成时，我真的想要从事这方面的研究

49
00:03:05,340 --> 00:03:09,442
在那时我已经知道了最重要的问题是你怎样训练

50
00:03:09,442 --> 00:03:10,630
多层的神经网络

51
00:03:10,630 --> 00:03:15,152
在60年代的论文中很清楚的表明那些重要的

52
00:03:15,152 --> 00:03:20,040
问题还没有被解决，也表明了他们的思想层次

53
00:03:20,040 --> 00:03:24,152
我已经读多了Fukushima的neocognitron论文

54
00:03:24,152 --> 00:03:29,951
是一种多层架构，与现在的很类似

55
00:03:29,951 --> 00:03:36,765
我们称之为卷积神经网络，但没有真正的反向传播学习算法

56
00:03:36,765 --> 00:03:43,822
在法国，我认识了一些人，他们来自一个小的独立俱乐部

57
00:03:43,822 --> 00:03:47,796
他们对于他们当时称之为自动机网络的很感兴趣

58
00:03:47,796 --> 00:03:50,040
他们给了我一些论文

59
00:03:50,040 --> 00:03:54,782
一些功能网络的论文，现在已经不流行了

60
00:03:54,782 --> 00:03:59,888
但是这是第一个有关于神经网络的记忆，而这份论文激起了

61
00:03:59,888 --> 00:04:04,715
一些研究团体重新进入神经网络的兴趣在80年代早期

62
00:04:04,715 --> 00:04:09,732
有大部分是物理学家和凝聚态物理学家

63
00:04:09,732 --> 00:04:14,558
还有一些心理学家，但对于工程师

64
00:04:14,558 --> 00:04:18,464
和电脑科学家还无法谈论神经网络

65
00:04:18,464 --> 00:04:23,566
他们也让我看另一个刚刚传播出来的论文

66
00:04:23,566 --> 00:04:28,590
作为预印本，标题是最佳知觉推论

67
00:04:28,590 --> 00:04:32,675
这是第一份玻尔兹曼机的论文，作者是Geoff Hinton

68
00:04:32,675 --> 00:04:33,811
和Terry Sejnowski

69
00:04:33,811 --> 00:04:34,801
它是讨论隐藏单元的

70
00:04:34,801 --> 00:04:40,401
也就是学习的一部分

71
00:04:40,401 --> 00:04:46,702
多层神经网络比仅有的分类器更有效

72
00:04:46,702 --> 00:04:47,697
所以我说

73
00:04:47,697 --> 00:04:51,350
我必须见到这些人[笑]

74
00:04:51,350 --> 00:04:52,093
因为他们只对

75
00:04:52,093 --> 00:04:53,230
正确的问题感兴趣

76
00:04:54,756 --> 00:05:00,330
在几年以后，在开始我的phd学习之后，我参加了

77
00:05:00,330 --> 00:05:06,020
在Le Juch的一个研讨会

78
00:05:06,020 --> 00:05:10,630
Terry是这个研讨会中的一位演讲者

79
00:05:10,630 --> 00:05:13,216
我在那时遇见了他 — 那是80年代早期

80
00:05:13,216 --> 00:05:15,862
— 是1985，1985年年初

81
00:05:15,862 --> 00:05:19,777
所以我在1985年，在法国Le Juch的研讨会遇到了 Terry Sejnowski

82
00:05:19,777 --> 00:05:23,881
很在那里的很多人，有早期神经网络的创始人，跳槽来的

83
00:05:23,881 --> 00:05:27,820
和很多从事理论神经网络的人和相关领域的工作人员

84
00:05:27,820 --> 00:05:29,930
那是个很棒的研讨会

85
00:05:29,930 --> 00:05:36,260
我还遇到了一些来自贝尔实验室的人，他们最终雇佣了我

86
00:05:36,260 --> 00:05:38,590
但这是我完成phD之前几年的时候

87
00:05:38,590 --> 00:05:42,821
所以我告诉Terry Sejnowski 有关我从事的工作

88
00:05:42,821 --> 00:05:45,479
有关一些反向传播的版本

89
00:05:45,479 --> 00:05:49,559
这是正式反向传播论文提出之前

90
00:05:49,559 --> 00:05:54,030
Terry那时正在做关于Net Talk的工作

91
00:05:55,430 --> 00:05:57,212
这是在Rumelhart

92
00:05:57,212 --> 00:06:00,453
Hinton和Williams的关于反向传播论文发表之前

93
00:06:00,453 --> 00:06:04,158
但它是Geoff的朋友，这样的信息传播着

94
00:06:04,158 --> 00:06:07,733
所以他当时已经再尝试将这项工作用于Net Talk

95
00:06:07,733 --> 00:06:10,804
但是他没有告诉我

96
00:06:10,804 --> 00:06:11,848
然后他回到了US

97
00:06:11,848 --> 00:06:14,882
并告诉了Geoff在法国有一些孩子也正在做着相同的事情

98
00:06:14,882 --> 00:06:16,672
跟我们正在做的事情相同

99
00:06:16,672 --> 00:06:19,117
[笑]几个月以后

100
00:06:19,117 --> 00:06:25,120
六月，在法国有另一个会议，Geoff是主演讲者

101
00:06:26,160 --> 00:06:28,250
他讲的主题是玻尔兹曼机模型

102
00:06:28,250 --> 00:06:30,920
当然，他也正在进行反向传播论文

103
00:06:32,060 --> 00:06:34,230
他讲完了主题

104
00:06:34,230 --> 00:06:37,520
之后在他周围有50个人想与他讲话

105
00:06:37,520 --> 00:06:40,210
而他对组织者讲的第一件事情是

106
00:06:40,210 --> 00:06:41,380
你知道这个年轻人Yann LeCun吗？

107
00:06:41,380 --> 00:06:45,025
因为他读过我正在进行的论文

108
00:06:45,025 --> 00:06:45,900
用法语写的

109
00:06:45,900 --> 00:06:47,890
它可以读懂法语，他也可以看那些数学

110
00:06:47,890 --> 00:06:51,600
他知道什么是反向传播，所以我们一起吃了午饭

111
00:06:51,600 --> 00:06:53,496
这就是我们怎么成为朋友的 —了解

112
00:06:53,496 --> 00:06:54,319
[笑]

113
00:06:54,319 --> 00:06:56,583
那是因为很多团体

114
00:06:56,583 --> 00:07:00,700
正在独立改造或发明类似的反向传播

115
00:07:00,700 --> 00:07:02,906
对的，我们意识到

116
00:07:02,906 --> 00:07:06,878
整个链式法则的想法或者最佳化控制，人们称之为连接状态

117
00:07:06,878 --> 00:07:10,870
实际是反向传播发明的真正主旨

118
00:07:10,870 --> 00:07:14,105
这种最佳化控制的主旨要回到60年代早期

119
00:07:14,105 --> 00:07:19,649
这种观念，使用梯度下降，并且基本上使用在多个层次

120
00:07:19,649 --> 00:07:26,160
是反向传播真正的特点，它出现在了不同的时间不同的场合

121
00:07:26,160 --> 00:07:30,849
但是我认为 Rumelhart, Hinton, Williams的论文才是

122
00:07:30,849 --> 00:07:35,560
让它普及的 —是的，我知道了

123
00:07:35,560 --> 00:07:39,800
然后几年之后，当您在贝尔实验室时

124
00:07:39,800 --> 00:07:45,865
在那里您发明了众多项目之一的LeNet，也是我们在课程中讨论的

125
00:07:45,865 --> 00:07:49,301
我记得之前，当我在贝尔实验室暑期实习的时候

126
00:07:49,301 --> 00:07:51,981
在那里我跟 Michael Kerns和其他一些人一起工作

127
00:07:51,981 --> 00:07:54,093
当时听说了您的有关工作

128
00:07:54,093 --> 00:07:57,509
所以请您告诉我一些关于你AT&T的LeNet的工作

129
00:07:57,509 --> 00:07:58,810
好的，实际上

130
00:07:58,810 --> 00:08:04,400
我实际做的是关于卷积网络的工作，当我在博士后时，

131
00:08:04,400 --> 00:08:06,090
在多伦多大学，跟着Geoffery Hinton

132
00:08:07,180 --> 00:08:09,075
我开始了我的第一项实验，我在那里写代码

133
00:08:09,075 --> 00:08:10,849
我做的第一项实验表明

134
00:08:10,849 --> 00:08:11,953
如果你有一个非常小的数据集

135
00:08:11,953 --> 00:08:16,995
我训练的数据集，几乎没有或者在当时有类似的东西

136
00:08:16,995 --> 00:08:20,195
所以我用鼠标画了一些字

137
00:08:20,195 --> 00:08:23,285
我有一台Amiga，一台私人电脑，那时当时最棒的电脑

138
00:08:23,285 --> 00:08:27,425
我画了一些字，然后使用它们

139
00:08:27,425 --> 00:08:30,345
我做了一些增强来扩充他们

140
00:08:30,345 --> 00:08:32,635
后来用它们来做性能测试

141
00:08:32,635 --> 00:08:35,125
我比较了全连接网络

142
00:08:35,125 --> 00:08:36,935
没有共用权重的局部连接网络

143
00:08:36,935 --> 00:08:38,190
然后是共享权重的网络

144
00:08:38,190 --> 00:08:40,000
那基本上是ConvNet的雏形

145
00:08:40,000 --> 00:08:46,090
而这对相对小的数据效果很好，可以显示你得到了

146
00:08:46,090 --> 00:08:50,320
最佳的效果，没有在传统架构下有过度训练

147
00:08:50,320 --> 00:08:53,481
当我在1988年10月去贝尔实验室时

148
00:08:53,481 --> 00:08:57,212
我做的第一件事是放大网络

149
00:08:57,212 --> 00:09:01,935
因为在我去贝尔实验室之前几个月，我们有了更快的电脑

150
00:09:01,935 --> 00:09:06,501
那时我的老板 Larry Jackal，成为了部门领导

151
00:09:06,501 --> 00:09:09,920
他说我们应该在你来之前，先订一台电脑

152
00:09:09,920 --> 00:09:10,450
你想要什么样的电脑

153
00:09:10,450 --> 00:09:15,860
我说在多伦多，这里有Sun 4

154
00:09:15,860 --> 00:09:17,432
如果我能有一台，是最好的了

155
00:09:17,432 --> 00:09:21,450
于是他们为我个人订了一台，

156
00:09:21,450 --> 00:09:24,660
在多伦多大学整个系只有一台，对吧？

157
00:09:24,660 --> 00:09:25,840
这里我可以自己用一台，对吗？

158
00:09:25,840 --> 00:09:30,030
所以Larry告诉我，贝尔实验室不是以省钱出名的

159
00:09:30,030 --> 00:09:30,866
[笑]

160
00:09:30,866 --> 00:09:34,483
那真的很棒

161
00:09:34,483 --> 00:09:39,810
他们已经在字元识别工作了一段时间了

162
00:09:39,810 --> 00:09:44,883
他们有巨大的数据集叫做USDS，包含了5000个训练样本

163
00:09:44,883 --> 00:09:51,700
[笑]我马上设计了一个卷积网络

164
00:09:51,700 --> 00:09:53,645
然后在这个数据集上训练

165
00:09:53,645 --> 00:09:58,320
得到了非常好的结果，比其他方法结果都好

166
00:09:58,320 --> 00:10:04,150
他们曾经尝试过，其他人以前也尝试过

167
00:10:04,150 --> 00:10:07,430
我们知道我们已经有了非常棒的东西

168
00:10:07,430 --> 00:10:11,420
这是在我加入贝尔实验室三个月内发生的

169
00:10:11,420 --> 00:10:14,690
所以这是第一版本的卷积网络

170
00:10:14,690 --> 00:10:19,580
我们的卷积网络有跨步，但我们没有单独的降采样

171
00:10:19,580 --> 00:10:21,240
和池化层

172
00:10:21,240 --> 00:10:23,690
所以每个卷积实际上

173
00:10:23,690 --> 00:10:24,390
很直接

174
00:10:24,390 --> 00:10:25,050
这个的原因是

175
00:10:25,050 --> 00:10:30,130
我们负担不起在每一个区域都做卷积

176
00:10:30,130 --> 00:10:32,299
这包含了太多的计算

177
00:10:32,299 --> 00:10:35,839
所以第二个版本是

178
00:10:35,839 --> 00:10:42,040
有单独的卷积，池化层和降采样

179
00:10:43,720 --> 00:10:47,070
我觉得这个才应该称为LeNet-1

180
00:10:47,070 --> 00:10:53,380
所以我们在NIPS上发表了几篇论文

181
00:10:53,380 --> 00:10:57,270
很有意思的事情是， 当我在NIPS上谈论这个论文时

182
00:10:58,460 --> 00:11:01,580
当时Geoffrey Hinton就是听众之一，当我讲完回到座位上时

183
00:11:01,580 --> 00:11:05,920
我就坐在他旁边， 他说：你的演讲说明了一点

184
00:11:05,920 --> 00:11:08,570
那就是， 如果你做了所有明智的事情

185
00:11:08,570 --> 00:11:10,503
事情就会成功

186
00:11:10,503 --> 00:11:12,871
在那天之后不久

187
00:11:12,871 --> 00:11:16,820
这篇论文改写了历史， 因为他被大量采纳

188
00:11:16,820 --> 00:11:18,540
这个想法被广泛使用在

189
00:11:18,540 --> 00:11:20,570
支票辨识上， —是的

190
00:11:20,570 --> 00:11:26,200
在AT&T内部有很大使用价值， 当时
对其他外面的地方影响还没有那么大

191
00:11:26,200 --> 00:11:29,368
我觉得理解这件事情的原因

192
00:11:29,368 --> 00:11:34,560
对我来说有一点困难，简单的原因可能是

193
00:11:34,560 --> 00:11:40,360
那时是在80年代晚期， 当时还没有互联网

194
00:11:40,360 --> 00:11:45,110
我们有电邮， 有FTP， 但是却没有网络

195
00:11:45,110 --> 00:11:48,450
没有两个实验室用相同的软件和硬件平台

196
00:11:48,450 --> 00:11:51,270
有些人用Sun的工作站， 有些人用其他机器

197
00:11:51,270 --> 00:11:52,980
有些人用个人电脑或者其他什么的

198
00:11:52,980 --> 00:11:56,360
那时还没有Python， Matlab

199
00:11:56,360 --> 00:11:58,430
人们写自己的代码

200
00:11:58,430 --> 00:12:01,510
我花了一年半时间

201
00:12:01,510 --> 00:12:04,442
我和Leon Bottou， 当他还是一个学生的时候

202
00:12:04,442 --> 00:12:07,330
我们一起合作， 花了一年半时间

203
00:12:07,330 --> 00:12:10,580
基本上做了一个简单的神经网络模拟器

204
00:12:12,030 --> 00:12:14,360
在那个时间因为没有用Python

205
00:12:14,360 --> 00:12:16,040
你需要写你自己的解释器

206
00:12:16,040 --> 00:12:16,920
去控制他

207
00:12:16,920 --> 00:12:19,070
我们想要我们自己的Lisp解释器

208
00:12:19,070 --> 00:12:24,160
所以所有的网络模型都是用Lisp写的
 用数值电脑做后端

209
00:12:24,160 --> 00:12:27,830
和我们现在的很像， 可以有很多链接的单元

210
00:12:27,830 --> 00:12:31,380
但是和现在我们熟悉的

211
00:12:31,380 --> 00:12:36,250
像Torch， PyTorch， TensorFlow这些东西不同

212
00:12:37,400 --> 00:12:41,160
我们开发了一些应用

213
00:12:41,160 --> 00:12:45,110
我们和一群工程师合作

214
00:12:46,460 --> 00:12:47,440
都是很聪明的人

215
00:12:48,880 --> 00:12:52,230
其中有些人是理论

216
00:12:52,230 --> 00:12:56,186
物理学家， 他们在贝尔实验室变成工程师

217
00:12:57,280 --> 00:13:00,070
Chris Dodgers是其中一位， 他后来

218
00:13:01,900 --> 00:13:04,095
在微软研究院做出了很棒的成果

219
00:13:04,095 --> 00:13:04,766
还有Krieg Nolan

220
00:13:04,766 --> 00:13:09,447
还有一些人， 我们共同合作

221
00:13:09,447 --> 00:13:12,300
让这个技术变得实用。 —嗯

222
00:13:12,300 --> 00:13:12,830
所以

223
00:13:12,830 --> 00:13:17,840
我们一起开发了这个字母识别系统

224
00:13:17,840 --> 00:13:22,614
这个系统整合了卷积网络和

225
00:13:22,614 --> 00:13:27,471
类似我们现在说的CRF(Conditional Random Field)的技术

226
00:13:27,471 --> 00:13:30,830
用来解释一串字母而不是单个 —是的

227
00:13:30,830 --> 00:13:33,710
那篇文章一部分在讲神经网络

228
00:13:33,710 --> 00:13:37,630
一部分在讲原子机械 —是的，把他们合在一起

229
00:13:37,630 --> 00:13:38,230
是的，没错

230
00:13:38,230 --> 00:13:41,310
所以论文的前半部分是关于卷积神经网络的

231
00:13:41,310 --> 00:13:43,080
也是这个论文被最多引用的原因

232
00:13:43,080 --> 00:13:45,205
但是论文的后半部分， 几乎没有人读

233
00:13:45,205 --> 00:13:49,530
那部分有关于序列，判别运算

234
00:13:49,530 --> 00:13:53,900
基本上是不使用正则化的结构预测

235
00:13:53,900 --> 00:13:57,060
所以事实上和CRF真的很像

236
00:13:57,060 --> 00:14:00,790
你知道， 就是PTCRFS的那些年

237
00:14:00,790 --> 00:14:06,770
这是很成功的， 除了

238
00:14:08,290 --> 00:14:12,300
我们庆祝这套系统在主流银行部署的那一天

239
00:14:13,450 --> 00:14:17,708
我们和我刚才提到的那个组

240
00:14:17,708 --> 00:14:19,640
一起做整个系统的工程部分

241
00:14:19,640 --> 00:14:23,650
另外一部分产品组， 在另一个国家

242
00:14:23,650 --> 00:14:25,940
隶属于AT&T的子部门， 叫NCR

243
00:14:25,940 --> 00:14:26,480
所以

244
00:14:26,480 --> 00:14:29,280
NCR就是National Cash Register公司

245
00:14:29,280 --> 00:14:32,610
他们建立大型的ATM机器

246
00:14:32,610 --> 00:14:36,250
大型的给银行读支票的系统

247
00:14:36,250 --> 00:14:38,220
所以他们就是我们的客户

248
00:14:38,220 --> 00:14:41,100
他们在使用我们的支票付款系统

249
00:14:41,100 --> 00:14:43,570
然后他们把他部署到了一个银行

250
00:14:43,570 --> 00:14:45,300
但是我不记得到底是哪家银行了

251
00:14:45,300 --> 00:14:47,490
他们也部署了ATM机器到法国银行

252
00:14:47,490 --> 00:14:51,979
这样机器就可以读你存款时的支票，

253
00:14:51,979 --> 00:14:56,804
我们就在一个豪华的餐厅庆祝部署成功这件事情

254
00:14:56,804 --> 00:15:00,982
就在当时公司宣布将AT&T拆散

255
00:15:00,982 --> 00:15:02,311
这是在1995年发生的

256
00:15:02,311 --> 00:15:06,970
AT&T宣布他会分拆成三家公司

257
00:15:06,970 --> 00:15:11,370
AT&T，Lucent Technologies，和NCR

258
00:15:11,370 --> 00:15:14,670
所以NCR被分拆出去，Lucent Technologies也被分出去

259
00:15:14,670 --> 00:15:17,090
原来的工程团队在Lucent Technologies, 但是产品团队

260
00:15:17,090 --> 00:15:18,210
当然跟着NCR

261
00:15:19,770 --> 00:15:24,620
令人悲伤的事情是AT&T的律师们用他们无限的智慧

262
00:15:24,620 --> 00:15:29,090
获得了专利，当时的卷积网络是有专利的

263
00:15:29,090 --> 00:15:31,991
但是庆幸的是现在已经取消了

264
00:15:31,991 --> 00:15:33,650
在2007年取消 [笑]

265
00:15:33,650 --> 00:15:36,370
大概十年以前

266
00:15:36,370 --> 00:15:41,100
他们将专利给了NCR，但是在NCR没有人真的懂得

267
00:15:41,100 --> 00:15:44,710
什么是卷积网络

268
00:15:44,710 --> 00:15:48,470
所以这个专利到了一群不知道他的价值的人手上

269
00:15:48,470 --> 00:15:51,254
我们在不同的公司，所以我们不能开发这个技术

270
00:15:51,254 --> 00:15:54,187
因为我们和工程团队在不同的公司

271
00:15:54,187 --> 00:15:56,724
因为我们去了AT&T，而工程团队去了Lucent

272
00:15:56,724 --> 00:15:58,140
产品团队去了NCR

273
00:15:58,140 --> 00:16:04,190
所以有点令人悲伤 ，—所以除了你的早期工作

274
00:16:04,190 --> 00:16:08,980
现在的神经网络非常热门，
但是您一直持续坚持在神经网络领域

275
00:16:08,980 --> 00:16:12,020
即使在神经网络很低谷的时候

276
00:16:12,020 --> 00:16:15,126
那是一种什么样的感觉？ —嗯，是的

277
00:16:15,126 --> 00:16:16,884
我坚持了一些，但在某些方面也没有坚持

278
00:16:16,884 --> 00:16:21,555
我总是相信最终那些技术会回归到人们的视线之内

279
00:16:21,555 --> 00:16:26,369
人们也会知道如何在实际中使用他们

280
00:16:26,369 --> 00:16:27,650
它会是很有用的

281
00:16:27,650 --> 00:16:30,901
我一直在脑子里这么想

282
00:16:30,901 --> 00:16:33,661
但是在1996年，当AT&T分拆的时候

283
00:16:33,661 --> 00:16:36,750
所有的在字母识别方面的工作成果

284
00:16:36,750 --> 00:16:40,627
基本上也拆散了，因为部分的群组被拆开了

285
00:16:40,627 --> 00:16:45,490
我被晋升为部门主管，我需要搞明白要做什么

286
00:16:45,490 --> 00:16:49,597
那是因特网刚刚开始的时候，也就是1995年

287
00:16:49,597 --> 00:16:53,836
我有一个想法，因特网兴起的一个大的结果是

288
00:16:53,836 --> 00:16:58,175
会把我们纸质材料上的知识

289
00:16:58,175 --> 00:17:03,120
带到数字世界中去

290
00:17:03,120 --> 00:17:07,193
所以我开始了一个项目，叫DjVu

291
00:17:07,193 --> 00:17:10,694
这个项目主要是想要压缩扫描的文档

292
00:17:10,694 --> 00:17:13,635
这样一来他们就可以传播到全世界的各个地点

293
00:17:13,635 --> 00:17:17,528
这个项目在一段时间都很有趣，也有了一点成功

294
00:17:17,528 --> 00:17:21,443
尽管AT&T不知道这个可以用来做什么 —是的，我记得这个

295
00:17:21,443 --> 00:17:24,790
这帮助了线上论文的传播 —是的，确实是这样

296
00:17:24,790 --> 00:17:28,830
我们扫描了整个NIPS的文章，并把它们发布到网上

297
00:17:28,830 --> 00:17:30,110
—我记得这件事

298
00:17:30,110 --> 00:17:31,590
—为了展示这个技术如何使用

299
00:17:31,590 --> 00:17:35,736
我们可以压缩高清图片到几kB的大小

300
00:17:35,736 --> 00:17:36,502
所以，卷积网络

301
00:17:36,502 --> 00:17:39,988
从您早期的工作开始， 到现在

302
00:17:39,988 --> 00:17:43,336
几乎覆盖了计算机视觉的所有领域

303
00:17:43,336 --> 00:17:46,980
并且甚至开始去进入其他领域

304
00:17:46,980 --> 00:17:50,407
所以能告诉我你是如何看待整个过程的吗

305
00:17:50,407 --> 00:17:51,446
— [笑]

306
00:17:51,446 --> 00:17:55,150
我可以告诉你为什么我早先就觉得这些会在未来发生

307
00:17:55,150 --> 00:17:59,178
首先， 我一直相信这是会成功

308
00:17:59,178 --> 00:18:04,074
深度学习这个方向需要快速的计算机
和大量的数据， 并且我一直相信

309
00:18:04,074 --> 00:18:07,160
这会是以后会发生的事情

310
00:18:07,160 --> 00:18:11,695
当我在贝尔实验室的时候， 我想这会是一个

311
00:18:11,695 --> 00:18:16,392
持续进展的过程， 随着计算机越来越强大。

312
00:18:16,392 --> 00:18:20,874
我们在贝尔实验室的时候，甚至自己设计芯片运行卷积神经网络

313
00:18:20,874 --> 00:18:25,566
那时候实际上是在两个不同芯片上运行整个计算图

314
00:18:25,566 --> 00:18:28,593
使得卷积网络更加高效

315
00:18:28,593 --> 00:18:33,186
我当时想这应该会开始流行

316
00:18:33,186 --> 00:18:37,882
而且会逐渐被重视， 然后能持续的发展下去

317
00:18:37,882 --> 00:18:41,860
但是实际上， 因为大家对神经网络的兴趣

318
00:18:41,860 --> 00:18:45,470
在90年代中期几乎中断， 这些并没有发生

319
00:18:45,470 --> 00:18:51,444
这段6，7年的时间， 从大概1995到2002， 
是神经网络的低潮期

320
00:18:51,444 --> 00:18:55,351
当时，几乎没人对这个领域进行研究

321
00:18:55,351 --> 00:18:57,192
事实上， 还是有一丢丢进展的

322
00:18:57,192 --> 00:19:01,971
在2000年初， 有些微软的研究人员

323
00:19:01,971 --> 00:19:06,401
用卷积神经网络去做汉字的识别

324
00:19:08,676 --> 00:19:11,676
嗯， 就是这样的

325
00:19:11,676 --> 00:19:14,844
还有些其他的少量工作， 像在法国有用这个技术做人脸识别

326
00:19:14,844 --> 00:19:19,780
还有一些其他地方的，但都是很小的工作

327
00:19:19,780 --> 00:19:24,400
我发现最近有一些团队

328
00:19:24,400 --> 00:19:27,320
提出一些和卷积神经网络很像的想法

329
00:19:27,320 --> 00:19:31,370
但是却没有发表出来， 像一些对医学图像的分析

330
00:19:31,370 --> 00:19:33,880
那些人更多是在商业系统下进行

331
00:19:33,880 --> 00:19:37,310
所以那些人没有把成果公之于众

332
00:19:37,310 --> 00:19:42,343
我的意思是当我们有了第一个卷积神经网络的成果的时候

333
00:19:42,343 --> 00:19:47,475
他们并没有意识到， 所以有一点大家在并行开发

334
00:19:47,475 --> 00:19:52,764
所以这些人在这段时间内都有差不多的想法

335
00:19:52,764 --> 00:19:56,950
但是我非常惊讶于从ImageNet开始

336
00:19:56,950 --> 00:20:01,250
大家兴趣转变的速度之快

337
00:20:01,250 --> 00:20:03,646
那是在2012年， 应该说是2012年底

338
00:20:03,646 --> 00:20:07,707
在ECCV有一个很有趣的事件

339
00:20:07,707 --> 00:20:12,389
在佛罗伦萨，有一个ImageNet的研讨会

340
00:20:12,389 --> 00:20:19,552
大家都知道Geoffrey Hinton,
 Alex Krizhevsky和Ilya Sutskever大幅度领先

341
00:20:19,552 --> 00:20:21,004
所以大家都在等着这个演讲

342
00:20:21,004 --> 00:20:25,717
计算机视觉领域的绝大部分人完全不知道

343
00:20:25,717 --> 00:20:26,281
卷积神经网络是什么东西

344
00:20:26,281 --> 00:20:27,210
事实上他们听我谈过这个东西

345
00:20:27,210 --> 00:20:32,181
我在2000年的CVPR上被邀请去做一个关于这个的演讲

346
00:20:32,181 --> 00:20:35,560
但是大部分人没有给予很大的关注

347
00:20:35,560 --> 00:20:37,822
资深的人员知道， 但是

348
00:20:37,822 --> 00:20:41,607
这个领域的年轻人就不太知道这是什么了

349
00:20:41,607 --> 00:20:45,654
所以当Alex Krizhevsky做演讲的时候，
他并没有解释什么是卷积网络

350
00:20:45,654 --> 00:20:47,824
因为他认为每个人都知道

351
00:20:47,824 --> 00:20:53,093
因为他从机器学习领域来， 所以当他说这些东西是如何连接

352
00:20:53,093 --> 00:20:56,753
它是如何转换数据， 并且得到了什么结果的时候

353
00:20:56,753 --> 00:20:59,450
它还是觉得每个人都知道这是什么

354
00:20:59,450 --> 00:21:02,198
大量的人都觉得很震惊

355
00:21:02,198 --> 00:21:07,112
并且你能看到当他在做演讲的时候，台下的人观念的转变

356
00:21:07,112 --> 00:21:11,946
特别是资深的研究人员。 —所以你觉得那场研讨会

357
00:21:11,946 --> 00:21:16,058
对于计算机视觉领域是一个决定的时刻

358
00:21:16,058 --> 00:21:16,724
是的 当然

359
00:21:16,724 --> 00:21:17,572
是的

360
00:21:17,572 --> 00:21:18,874
这就是它如何发生的

361
00:21:18,874 --> 00:21:23,370
现在，你依然在纽约大学做教授

362
00:21:23,370 --> 00:21:27,998
并且带领Facebook的人工智能研究院(FAIR)

363
00:21:27,998 --> 00:21:32,241
我知道你一定对如何让学术研究和工业界合作

364
00:21:32,241 --> 00:21:33,230
有独特的方法

365
00:21:33,230 --> 00:21:34,530
你能跟我们分享下这方面的想法吗

366
00:21:34,530 --> 00:21:37,688
是的， 这其中最美妙的事情在于

367
00:21:37,688 --> 00:21:44,105
在我过去4年中领导Facebook人工智能研究院的过程中

368
00:21:44,105 --> 00:21:50,128
我有很大的自由度，去把他建构成我觉得合适的样子

369
00:21:50,128 --> 00:21:56,010
因为这是在Facebook内部第一个研究机构

370
00:21:56,010 --> 00:21:58,910
Facebook是一个以工程为导向的公司

371
00:21:58,910 --> 00:22:03,007
到现在为止， 他都在专注于生存和短期的事情

372
00:22:03,007 --> 00:22:10,714
Facebook已经10岁了， 也有了成功的上市

373
00:22:10,714 --> 00:22:14,220
并且正在思考未来的10年

374
00:22:14,220 --> 00:22:18,188
我的意思是Mark在思考未来10年中

375
00:22:18,188 --> 00:22:19,341
什么是最重要的事情

376
00:22:19,341 --> 00:22:21,917
公司的生死已经不是一个问题了

377
00:22:21,917 --> 00:22:26,343
所以这是一个转变发生的时候，一家大公司开始思考

378
00:22:26,343 --> 00:22:28,846
应该说当时也不是特别大

379
00:22:28,846 --> 00:22:34,003
Facebook那时候有5000人， 但是他们有这个资格去

380
00:22:34,003 --> 00:22:39,837
思考未来10年， 思考什么对科技发展更加重要

381
00:22:39,837 --> 00:22:45,069
Mark和他的团队觉得人工智能是

382
00:22:45,069 --> 00:22:52,372
很重要的一环，对于Facebook要
“链接所有人”的这个使命来说

383
00:22:52,372 --> 00:22:55,303
所有他们探索了很多方式， 去赋能人工智能

384
00:22:55,303 --> 00:22:57,808
他们有一个小的内部工程团队

385
00:22:57,808 --> 00:23:01,459
对卷积网络很有经验

386
00:23:01,459 --> 00:23:05,450
也在人脸识别和其他方向得到了很好的结果，
这激起了他们的兴趣

387
00:23:05,450 --> 00:23:08,724
所以他们探索着雇佣了一批年轻的研究人员，

388
00:23:08,724 --> 00:23:10,820
也收购了公司，或者类似的事情

389
00:23:10,820 --> 00:23:14,200
最终他们定下来要去雇佣在这个领域有资深经验的人

390
00:23:14,200 --> 00:23:18,097
并且建立一个研究机构

391
00:23:20,210 --> 00:23:23,340
刚开始确实有一点文化冲击

392
00:23:23,340 --> 00:23:26,750
因为做研究的方法是与在公司中做工程完全不同的

393
00:23:26,750 --> 00:23:29,250
你会思考更长的时间和更广的空间

394
00:23:29,250 --> 00:23:32,672
研究人员倾向于

395
00:23:32,672 --> 00:23:33,821
保守地选择所要研究的方向

396
00:23:33,821 --> 00:23:38,552
我在开始就很明确的一点是，研究人员应该保持开放的态度

397
00:23:38,552 --> 00:23:43,034
研究人员不仅需要鼓励将成果发表

398
00:23:43,034 --> 00:23:45,110
更需要将发表作为一种要求

399
00:23:45,110 --> 00:23:50,970
并且需要能被类似我们衡量学术研究的评估方式

400
00:23:50,970 --> 00:23:56,440
去衡量研究成果

401
00:23:56,440 --> 00:24:01,644
所以Mark和公司的CTO，
Mike Schiroepfer, 也是我现在的上司

402
00:24:01,644 --> 00:24:07,140
他们说，Facebook是一个开放的公司

403
00:24:07,140 --> 00:24:09,890
我们贡献了很多的开源产品。

404
00:24:13,188 --> 00:24:14,799
你知道，Mike Schiroepfer，我们的CTO

405
00:24:14,799 --> 00:24:17,910
就是来自开源社区

406
00:24:17,910 --> 00:24:19,890
他之前在Mozilla工作，

407
00:24:19,890 --> 00:24:21,260
也有很多人都从那边过来。

408
00:24:21,260 --> 00:24:24,440
所以这种开放是根植在公司的DNA中的，这也使得我

409
00:24:24,440 --> 00:24:28,390
对于建立这个开放的研究院感到自信

410
00:24:28,390 --> 00:24:34,941
另外事实上Facebook也不会
像其他公司一样对专利太过痴迷和强迫

411
00:24:34,941 --> 00:24:41,397
这也使公司更利于和大学间进行合作

412
00:24:41,397 --> 00:24:46,774
可以安排一些人一边身在工业界工作，

413
00:24:46,774 --> 00:24:49,555
另一边也和学术界保持联系。 —你觉得这是很有价值的？

414
00:24:49,555 --> 00:24:52,630
是的，当然

415
00:24:52,630 --> 00:24:56,261
当你看我这4年的学术成果时

416
00:24:56,261 --> 00:24:59,696
大部分的是我在纽约大学的学生的成果

417
00:24:59,696 --> 00:25:01,170
嗯

418
00:25:01,170 --> 00:25:03,190
因为在Facebook，

419
00:25:03,190 --> 00:25:07,016
我要做很多实验室的管理，招聘，

420
00:25:07,016 --> 00:25:12,029
确定研究方向，指导他们，或者其他的一些事情

421
00:25:12,029 --> 00:25:16,345
但是我没有参与他们个人的研究项目，

422
00:25:16,345 --> 00:25:16,910
文章上也不会有我的名字

423
00:25:16,910 --> 00:25:20,478
你知道，我已经不在意文章是不是署我的名字。

424
00:25:20,478 --> 00:25:21,666
所以你没有把这些琐事交给别人来干，

425
00:25:21,666 --> 00:25:23,580
而是自己亲自把这些活都干了。

426
00:25:23,580 --> 00:25:24,590
的确，你不再想把你自己放在前台

427
00:25:24,590 --> 00:25:27,390
而是让自己隐藏在幕后

428
00:25:27,390 --> 00:25:30,539
你不想让你自己和实验室的人产生竞争

429
00:25:30,539 --> 00:25:32,721
我想您可能被问过很多次

430
00:25:32,721 --> 00:25:35,760
但我还是希望可以让所有看到这段采访的人得到答案

431
00:25:36,830 --> 00:25:40,719
你对那些想要进入人工智能这个领域的人有什么建议？

432
00:25:40,719 --> 00:25:42,459
哈哈

433
00:25:42,459 --> 00:25:46,470
现在和我当时刚开始的时候已经大不相同了

434
00:25:46,470 --> 00:25:51,820
但是我觉得现在非常棒的事情是：
人们一定程度上可以更容易的参与进来

435
00:25:51,820 --> 00:25:57,030
工具现在已经变得很容易使用，像TensorFlow，PyTorch

436
00:25:57,030 --> 00:26:01,928
你可以用卧室的廉价电脑运行这些软件

437
00:26:01,928 --> 00:26:06,905
并且可以训练你的卷积神经网络，循环神经网络等等

438
00:26:06,905 --> 00:26:09,140
也有很多工具

439
00:26:09,140 --> 00:26:16,190
你可以通过线上材料学到很多，这不会太繁重

440
00:26:16,190 --> 00:26:19,860
所以你会看到高中学生现在开始学习这个

441
00:26:19,860 --> 00:26:24,930
这真的很棒，我觉得这确实在

442
00:26:24,930 --> 00:26:29,730
学生群体中引起了学习机器学习和人工智能的兴趣

443
00:26:29,730 --> 00:26:36,820
对年轻人来说这很令人兴奋，我觉得很棒

444
00:26:36,820 --> 00:26:42,430
所以我的建议是，如果你想进入这个领域，
就要让自己变得有用

445
00:26:42,430 --> 00:26:45,260
比如，贡献自己的力量给开源社区

446
00:26:45,260 --> 00:26:49,810
或者去实现一些网上找不到的标准算法

447
00:26:49,810 --> 00:26:54,600
并把他们贡献出来让别人去使用

448
00:26:54,600 --> 00:26:56,610
拿一篇你觉得很重要的文章

449
00:26:56,610 --> 00:27:01,080
并去重新实现里面的算法，把他放到开源社区中去

450
00:27:01,080 --> 00:27:04,260
或者去贡献某些开源项目

451
00:27:04,260 --> 00:27:09,132
如果你写的东西很有趣，也有用，你就会被关注到

452
00:27:09,132 --> 00:27:14,030
也许你会在一个你心仪的公司有一个好的工作

453
00:27:14,030 --> 00:27:18,580
或者你会被你心水的PhD项目录取

454
00:27:18,580 --> 00:27:19,950
我觉得这是一个好的开始

455
00:27:19,950 --> 00:27:20,962
嗯

456
00:27:20,962 --> 00:27:24,973
给开源社区做贡献是一个进入社区的好的方式，
把学到的知识回馈给别人

457
00:27:24,973 --> 00:27:26,368
嗯，是这样的

458
00:27:26,368 --> 00:27:29,651
多谢，Yann，这真的很棒

459
00:27:29,651 --> 00:27:32,520
我已经认识你很多年了，但是当我听到你讲这些故事的细节时

460
00:27:32,520 --> 00:27:34,813
我依然觉得非常吸引人

461
00:27:34,813 --> 00:27:37,248
是的，有很多像这样的故事，

462
00:27:37,248 --> 00:27:41,895
但当你回想时，你在当时不会意识到那一刻是多么的重要

463
00:27:41,895 --> 00:27:45,380
只有当经过10年，20年后，
 你才会意识到这些时刻有多么重要

464
00:27:45,380 --> 00:27:47,113
嗯，谢谢

465
00:27:47,113 --> 00:27:48,678
—谢谢