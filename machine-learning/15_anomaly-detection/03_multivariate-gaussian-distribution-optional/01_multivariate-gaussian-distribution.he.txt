בסרטון זה ובווידאו הבא, ברצוני לספר לכם על הרחבה אפשרית אחת לאלגוריתם זיהוי החריגים שפיתחנו עד כה. ההרחבה הזו משתמשת במשהו שנקרא ההתפלגות הגאוסיאנית הרב-משתנית, ויש לה כמה יתרונות וכמה חסרונות, והיא יכולה לפעמים לתפוס כמה חריגים שהאלגוריתם הקודם לא תופס. כדי להתניע את זה, נתחיל בדוגמה. נניח שככה נראים הנתונים הלא מתויגים, כמו מה שציירתי כאן. ואני אשתמש בדוגמה של ניטור מכונות במרכז נתונים, ניטור של מחשבים במרכז הנתונים. שתי התכונות שלי הן x1 שהוא העומס על ה-CPU ו-x2 שהוא כמות הזיכרון בשימוש. אז אם אני לוקח את שתי התכונות שלי, x1 ו-x2, ואני ממדל אותן כגאוסיאנים אז הנה גרף של תכונה x1 והנה הגרף של תכונה x2, ואם אני מתאים גאוסיאן לזה, אני אקבל אולי גאוסיאן כזה, אז הנה p של x1, שתלוי בפרמטרים μ1 ו-σ²1, והנה כמות הזיכרון בשימוש, ואולי אני אקבל גאוסיאן שנראה ככה, וזהו p של x2, שתלוי ב-μ2 ו-σ²2. אז כך ממדל אלגוריתם זיהוי החריגים את x1 ו-x2. עכשיו נניח שבסדרת הבדיקה יש לי דוגמה שנראית כך. במיקומו של הצלב הירוק כאן, הערך של x1 הוא בערך 0.4, והערך של x2 הוא בערך 1.5. אם נסתכל על הנתונים, זה נראה כאילו רוב הנתונים נמצאים באזור הזה, והצלב הירוק הוא די רחוק מכל הנתונים שראינו. נראה שיש לסמן אותו כאנומליה. אז בנתונים של הדוגמאות הטובות שלנו, זה נראה כאילו העומס על המעבד והשימוש בזיכרון גדלים בערך באופן ליניארי זה עם זה. אז אם יש לנו מחשב עם עומס CPU גבוה, אנחנו מצפים שגם השימוש בזיכרון יהיה גבוה, ואילו בדוגמה הזו, הדוגמה הירוקה זה נראה שכאן העומס על ה-CPU הוא נמוך מאוד, אבל השימוש בזיכרון הוא גבוה מאוד, ואנחנו פשוט טרם ראינו כזה מקרה בנתוני האימון שלנו. נראה כאילו שזה צריך להיות חריג. אבל בואו נראה מה יעשה אלגוריתם זיהוי החריגים. אז לגבי העומס על המעבד, הוא שם אותו בסביבות 0.5 וההסתברות הגבוהה הזו אינה רחוקה מדוגמאות אחרות שאולי כבר ראינו, בעוד שלגבי השימוש בזיכרון, ... הוא נמצא בערך ב-1.5, כאן מימין. אז זה בתוך הגאוסיאן, הערך כאן והערך כאן אינם שונים מהרבה דוגמאות אחרות שראינו, אז p של x1 יהיה גבוה למדי, גבוה בצורה סבירה. וגם p של x2 גבוה באופן סביר. זאת אומרת, אם תסתכלו על התרשים הזה, הנקודה הזאת כאן לא נראית כל כך רע, ואם תסתכלו על התרשים הזה, אז גם זה לא נראה כל כך רע. הרי היו לנו דוגמאות עם צריכת זיכרון אפילו יותר גבוהה, והיו דוגמאות אפילו עם פחות צריכת מעבד, ולכן זאת לא נראית דוגמה חריגה. ולכן אלגוריתם זיהוי חריגים לא יצליח לסמן את הנקודה הזו כאנומליה. ומתברר שהאלגוריתם שלנו לאיתור חריגים לא קולט שהאליפסה הכחולה הזאת מראה את האזור בעל ההסתברות הגבוהה, הדברים האלה, הדוגמאות כאן, יש להן הסתברות גבוהה והדוגמאות במעגל השני בהסתברות נמוכה יותר, והדוגמאות כאן הם אפילו עם הסתברות עוד יותר נמוכה, ואיכשהו, לדברים כאן, הצלב הירוק כאן, יש סבירות גבוהה למדי. ובפרט, הוא חושב שלכל דבר באזור הזה, לכל מה שבתוך העיגול הזה, יש הסתברות שווה, והוא לא מבין שלמשהו שנמצא כאן יש למעשה הסתברות נמוכה בהרבה מאשר משהו שנמצא שם. אז כדי לתקן את זה, אנחנו נפתח גרסה שונה של אלגוריתם לזיהוי חריגים, באמצעות משהו שנקרא התפלגות גאוסיאנית מרובת-משתנים או התפלגות נורמלית מרובת-משתנים. הנה מה שנעשה. יש לנו תכונות x ב-Rn ובמקום לחשב בנפרד את p של x1, ואת p של x2, אנחנו נמדל את p של x בבת אחת, ניצור מודל של p של x, הכל בבת אחת. אז הפרמטרים של ההתפלגות הגאוסיאנית הרב-משתנית הם μ שהוא וקטור ו-Σ (סיגמא, האות הגדולה) שהיא מטריצה n על n, הנקראת מטריצת שונות משותפת, והיא דומה למטריצת השונות המשותפת שראינו כשעבדנו עם אלגוריתם PCA - ניתוח מרכיבים עיקריים. למען השלמות, תנו לי רק לכתוב את הנוסחה עבור התפלגות גאוסיאנית מרובת משתנים. אז אנחנו אומרים שההסתברות של x, שהפרמטרים שלו הם μ ו-Σ, שההסתברות של x שווה, ואני מדגיש, אין שום צורך לזכור את הנוסחה הזו. אפשר למצוא אותה באינטרנט כל פעם שצריך להשתמש בה, אבל כך נראית ההסתברות של x. e בחזקת מינוס חצי x מינוס μᵀ כפול ההופכית של מטריצת השונות כפול x מינוס μ. והדבר הזה כאן, הערך המוחלט של Σ, הדבר הזה כאן כשכותבים את הסמל הזה על מטריצות, זה נקרא הדטרמיננטה של המטריצה Σ וזו פונקציה מתמטית על מטריצות שבאמת אין שום צורך לדעת מה זה הדטרמיננטה של מטריצה, ובעצם כל מה שצריך לדעת הוא שאפשר לחשב את זה באוקטבה באמצעות הפקודה (det(sigma. ... אוקיי, ושוב רק לשם הבהירות, בביטוי הזה, הסיגמות (Σ) האלה כאן, הן פשוט מטריצות n על n. זה לא הסימן של סיכום, הן פשוט מטריצות n על n. אז זו הנוסחה של p של x, אבל מה שיותר מעניין או יותר חשוב, איך נראית בעצם p של x? בואו נסתכל על כמה דוגמאות של התפלגויות גאוסיאניות מרובות משתנים. בואו ניקח דוגמא דו-מימדית, נניח אם n שווה 2, יש לנו שתי תכונות, x1 ו-x2. נניח שאני מגדיר את μ להיות שווה ל-0 ואת Σ להיות המטריצה הזו כאן. עם 1-ים על האלכסונים ואפסים בשאר המקומות, המטריצה הזו מכונה לעתים גם מטריצת הזהות. במקרה הזה, p של x ייראה כך, ומה שאני מראה בתרשים הזה הוא עבור ערכים ספציפיים של x1 ו-x2, גובה המשטח הוא הערך של p של x. אז במצב הזה הערך של p של x הוא גבוה ביותר כאשר x1 ו-x2 שניהם שווים אפס, זה השיא של ההסתברות בהתפלגות הזו, וההסתברות יורדת עם הפעמון או הגאוסיאן הדו מימדי הזה. הגרף התחתון מראה אותה תמונה אבל עם גרף קווי מתאר, או באמצעות צבעים שונים, אז האדום החזק והכבד באמצע מתאים לערכים הגבוהים ביותר, וערכים נמוכים יותר בצהוב, עוד יותר נמוכים בתכול, והערכים הנמוכים ביותר בכחול עמוק, ולכן זה באמת אותו גרף אבל במבט על בעזרת צבעים במקום זה. אז עם ההתפלגות הזו, אפשר לראות שרוב ההסתברות נמצאת באזור 0,0 וכשיוצאים החוצה מ-0,0 ההסתברות של x1 ו-x2 יורדת. עכשיו בואו וננסה לשנות כמה פרמטרים ולראות מה קורה. אז בואו ניקח את Σ ונשנה אותה, נניח נקטין אותה קצת. Σ היא מטריצת שונות ולכן היא מודדת את השונות של התכונות x1 ו-x2. אז אם נכווץ את Σ מה שיקרה הוא שהרוחב של הבליטה הזאת פוחת והגובה יגדל קצת, כי הנפח מתחת לעקומה שווה 1. האינטגרל של הנפח מתחת למשטח שווה 1, כי סכום ההסתברויות חייב להסתכם ב-1. אבל כשמכווצים את השונות, זה קצת כמו לכווץ את σ², ומקבלים התפלגות קצת יותר צרה וגבוהה. וכפי שאפשר לראות כאן גם האליפסות הקונצנטריות התכווצו במקצת. ולעומת זאת לו הגדלנו את Σ ל-2 על האלכסונים, דהיינו מטריצת הזהות כפול שתיים, היינו מקבלים גאוסיאן הרבה יותר רחב ושטוח. ואתם רואים שזה אכן הרבה יותר רחב. קשה לראות אבל זה עדיין בצורת פעמון, פשוט זה נהיה הרבה יותר שטוח והרבה יותר רחב ולכן השונות של x1 ו-x2 פשוט נהיית רחבה יותר. הנה כמה דוגמאות נוספות. בואו עכשיו ננסה לשנות את האלמנטים של Σ אחד אחד. נניח ש-Σ הוא עכשיו 0.6 כאן, ו-1 שם. מה שזה עושה הוא להפחית את השונות של התכונה הראשונה, x1, תוך שמירה על השונות של התכונה השנייה x2. אז עם הגדרה כזו של פרמטרים, אפשר למדל דברים כאלה. ל-x1 יש שונות קטנה יותר, ול-x2 יש שונות גדולה יותר. ואם אני עושה ההפך, אני קובע את המטריצה להיות 1 ,2 אז אפשר גם למדל דוגמאות שבהם x1 מקבל מגוון רחב של ערכים ואילו x2 מקבל טווח יחסית צר של ערכים. וזה בא לידי ביטוי גם בגרף הזה, שבו ההתפלגות יורדת לאט יותר כש-x1 מתרחק מ-0, ונופל במהירות רבה כש-x2 מתרחק מ-0. ואם נשנה את האלמנט הזה של המטריצה במקום זה, אז בדומה לשקופית הקודמת, אלא שכאן כשאנחנו משחקים בערכים נראה ש-x2 יכול לקבל מגוון קטן מאוד של ערכים כאן כשהערך הוא 0.6, אנו רואים עכשיו ש-x2 נוטה לקבל מגוון קטן בהרבה של ערכים מאשר בדוגמה המקורית, ואילו אם נגדיר Σ להיות שווה ל-2 אז זה כמו לומר ש-x2 יכול לקבל מגוון רחב בהרבה של ערכים. עכשיו, אחד הדברים המגניבים בקשר להתפלגות מרובת-משתנים הוא שאפשר להשתמש בה גם כדי למדל קשר בין הנתונים. אנחנו יכולים להשתמש בה כדי להדגים את העובדה ש-x1 ו-x2 נוטים להיות מתואמים מאוד זה עם זה למשל. בפרט אם אנחנו מתחילים לשנות את ערכי המטריצה שמחוץ לאלכסונים נקבל סוג אחר של התפלגות גאוסיאנית. אז כשאני מגדיל את הערכים מחוץ לאלכסון מ-0.5 ל-0.8, אנחנו מקבלים התפלגות שהיא יותר ויותר מחודדת ובעלת פסגה לאורך הקו הזה של x שווה y. רואים במפת המתאר - קווי הגובה - ש-x ו-y נוטים לגדול יחד והדברים שיש להם הסתברות גדולה הם אם גם x1 הוא גדול וגם x2 הוא גדול או גם x1 הוא קטן וגם x2 הוא קטן. או איפשהו בין לבין. וכשהערך הזה, 0.8, גדל, מקבלים התפלגות גאוסיאנית שבה כל ההסתברות נמצאת על האזור הצר, שבו x1 שווה בערך ל-x2. זו התפלגות מאוד גבוהה ורזה בעיקר לאורך הקו הזה באזור המרכזי שבו x1 קרוב ל-x2. זה מה שקורה אם אנחנו מגדירים את הערכים האלה להיות ערכים חיוביים. לעומת זאת, אם נגדיר אותם להיות ערכים שליליים, כשאני מקטין את הערך כאן מ-0.5- ל-0.8-, מה שאנחנו מקבלים הוא מודל שבו רוב ההסתברות על מין אזור של מתאם שלילי בין x1 לבין x2. אז רוב ההסתברות נמצאת כעת באזור הזה, כאשר x1 שווה בערך למינוס x2, ולא באזור של x1 שווה ל-x2. אז זה לוכד מעין מתאם שלילי בין x1 ו-x2. אז אני מקווה שזה נותן לכם תחושה של ההתפלגויות השונות שהתפלגות גאוסיאנית מרובת-משתנים יכולה ללכוד. עד עכשיו שינינו את מטריצת השונות Σ, הדבר האחר שאפשר לעשות הוא לשנות את פרמטר הממוצע μ, אז במקור, היה לנו μ שווה 0, 0, ולכן ההתפלגות היתה מרוכזת סביב x1 שווה 0, x2 שווה ל-0, והשיא של ההתפלגות נמצא כאן בראשית הצירים, ואילו אם נשנה את הערכים של μ, זה ישנה את שיא ההתפלגות ולכן, אם μ שווה 0.5, 0 השיא יהיה ב-x1 שווה אפס ו-x2 שווה ל -0.5, השיא או מרכז ההתפלגות השתנה, ואם μ היה 1.5, מינוס 0.5 אז בדומה לכך השיא או הפסגה עובר עכשיו למקום אחר, המקביל למקום שבו x1 הוא 1.5 ו-x2 הוא 0.5-, אז פרמטר הממוצע μ פשוט מזיז את מרכז ההתפלגות. אז אני מקווה שכשאתם רואים את כל התמונות השונות האלה אתם מקבלים תחושה של סוגי התפלגויות ההסתברות שהתפלגות גאוס מרובת-משתנים מאפשרת לנו ללכוד. והיתרון העיקרי של זה הוא שהיא מאפשרת לנו ללכוד את המצב כאשר אנחנו חושדים ששתי תכונות שונות מתואמות חיובית או אולי שלילית. בסרטון הבא ניקח את ההתפלגות הגאוסיאנית מרובת-המשתנים הזו ונפעיל אותה בזיהוי חריגים.