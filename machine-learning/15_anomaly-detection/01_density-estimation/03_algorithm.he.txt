בהרצאה הקודמת דיברנו על התפלגות גאוסיאנית. בסרטון הזה נשתמש בזה כדי לפתח אלגוריתם זיהוי חריגים או אנומליות. נניח שיש לנו סדרת אימון לא מתויגת של m דוגמאות, והדוגמאות הן וקטורי תכונות ב-Rⁿ, לדוגמא סדרת האימון יכולה להיות, וקטורים של תכונות של m המטוסים האחרונים שיוצרו, או m תכונות של משתמשים או כל דבר אחר. הדרך בה אנחנו נטפל בזיהוי אנומליה היא לבנות מודל p של x מתוך הנתונים. אנחנו ננסה להבין לאילו תכונות יש הסתברות גבוהה, ולאילו סוגי תכונות יש הסתברות נמוכה יותר. אז x הוא וקטור ואנחנו נבנה מודל p של x, כמכפלה של ההסתברות של x1, כלומר של המרכיב הראשון של x, כפול ההסתברות של x2, כלומר ההסתברות של התכונה השנייה, כפול ההסתברות של התכונה השלישית, וכן הלאה עד ההסתברות של התכונה האחרונה xn. אני משאיר כאן מקום כי אני תיכף אמלא פה משהו. אבל איך נבנה מודל של ההסתברות של כל תכונה, p של x1, ו-p של x2, וכן הלאה? מה שנעשה הוא להניח שהתכונה x1 מתפלגת גאוסיאנית עם איזה שהוא פרמטר ממוצע שנקרא לו μ₁ ואיזו שונות, שנקרא לה σ²₁, אז p של x1 הוא התפלגות גאוסיאנית עם ממוצע μ₁ ושונות σ²₁. ונניח שוב שגם x2 מתפלג גאוסיאנית, זה מה שמציינת הטילדה הקטנה כאן, כלומר התפלגות גאוסיאנית עם ממוצע μ₂ ושונות σ²₂ ההתפלגות הגאוסיאנית כאן שונה, כי יש לה פרמטרים שונים, μ₂ ו-σ²₂. ובדומה לכך, אתם יודעים, x3 גם הוא גאוסיאני עם הממוצע שלו וסטיית התקן שלו, וכן הלאה, עד xn. אז זה המודל שלי. כהערת אגב לאלה מכם המומחים בסטטיסטיקה, מתברר שהמשוואה הזו שכתבתי בעצם מתאימה להנחה של אי-תלות בין ערכי התכונות x1 עד xn. אבל בפועל מתברר שהקטע הזה באלגוריתם עובד בסדר גמור, בין אם התכונות אינן תלויות ובין אם הן רק קרובות להיות בלתי-תלויות, ואפילו אם הנחת האי-תלות אינה נכונה, האלגוריתם הזה עובד בסדר גמור. אבל למי שלא מכיר את המונחים ולא הבין מה שאמרתי, אל תדאגו, תוכלו להבין ולבנות את האלגוריתם הזה בלי בעיות, ההערה באמת נועדה רק למומחים בסטטיסטיקה. לבסוף, לסיכום, תנו לי לקחת את הביטוי הזה ולכתוב אותו בצורה קצת יותר קומפקטית. אנחנו כותבים את זה כך: המכפלה מ-j שווה 1 עד n, של p של xj עם הפרמטרים μj ו-σ²j. הסימן הזה Π, האות היוונית הגדולה פיי (Π), הוא סימון שמשתמשים בו כדי לציין מכפלה של קבוצת ערכים. כמו שאתם מכירים את הסימן סיגמא - Σ, שמסמן סיכום, כמו הסכום של i שווה 1 עד n של i. דהיינו 1 + 2 + 3 ועוד וכו' וכו' עד n. אז כאן האות הזו היא סימן של מכפלה, לדוגמא מכפלה של i שווה 1 עד n של i. שזה בדיוק כמו סיכום, אבל עכשיו אנחנו מכפילים. ומקבלים 1 כפול 2 כפול 3 כפול וכו' עד n. אז כשמשתמשים בסימון הזה של כפל, כפל מ-j שווה 1 עד n של הביטוי הזה. זה קומפקטי יותר, זו דרך יותר קצרה לכתוב את המכפלה הזו של כל הביטויים האלה, כי אנחנו לוקחים את כל הביטויים האלה p של x j בהנחת μj ו-σ²j ומכפילים אותם זה בזה. ודרך אגב, הבעיה הזו של אומדן של ההתפלגות p של x נקראת לפעמים בעיית אומדן הצפיפות. ומכאן הכותרת של השקופית. אז בואו ונחבר את כל החלקים, הנה האלגוריתם שלנו לזיהוי חריגים. הצעד הראשון הוא לבחור תכונות, או למצוא תכונות xi שאנחנו חושבים שיכולות להצביע על דוגמאות חריגות. מה שאני מתכוון הוא לנסות למצוא את התכונות כך שכאשר יש משתמש יוצא דופן במערכת שעשוי לנסות הונאה, או כאשר המנוע מתנהג כך אפשר להבין שמשהו לא בסדר, שמשהו מוזר באחד ממנועי המטוסים. לבחור את התכונות xi, שלדעתך עשויות לקבל ערכים גדולים במיוחד, או קטנים במיוחד, בדוגמאות חריגות. אבל באופן כללי יותר, פשוט לנסות לבחור תכונות המתארות תכונות כלליות של הדברים שעליהם אנחנו אוספים נתונים. אחר כך, לגבי מערך האימון של m דוגמאות לא מתויגות, x1 עד xm, אנו מחשבים את הפרמטרים μ1 עד μn, ו-σ²1 עד σ²n שהן נוסחאות כמו הנוסחאות שבנינו בסרטון הקודם, אנחנו משתמשים באומדן של כל אחד מהפרמטרים האלה, ופשוט כדי להבין את זה, μj הוא הערך הממוצע של התכונה ה-j, μj הוא מה שנמצא כאן בנוסחה p של xj, שהפרמטרים שלה הם μj ו-σ²j. מה שזה אומר הוא ש-μj הוא פשוט הממוצע על סדרת האימון של הערכים של תכונה j. ורק להזכיר, עושים את זה, מחשבים את הנוסחאות האלה עבור j שווה 1 עד n. אז משתמשים בנוסחאות האלה כדי להעריך את μ1, ואת μ2, וכן הלאה עד μn, ובאופן דומה עבור σ², ואפשר גם לבנות גירסאות וקטוריות של הנוסחאות האלה. אם נחשוב על μ כעל וקטור, אז יש לו אברים μ1, μ2 וכו' עד μn, אז גירסה וקטורית של קבוצת הפרמטרים הזו ניתנת לכתיבה כסכום של i מ-1 עד n של xi. אז הנוסחה הזו שכתבתי מעריכה את μ על פי וקטורי התכונות xi עבור כל n הערכים של μ בו-זמנית. ואפשר גם לבנות נוסחה וקטורית להערכת σ²j. לבסוף, כאשר אנחנו מקבלים דוגמה חדשה, לדוגמא מקבלים מנוע חדש של מטוס ואנחנו רוצים לדעת אם המנוע הזה חריג, מה שאנחנו צריכים לעשות הוא לחשב את p של x, מה ההסתברות של הדוגמה החדשה זו? אז p של x שווה למכפלה הזו, ומה שאתם מיישמים, מה שאתם מחשבים, הוא הנוסחה הזו, וכאן, הדבר הזה הוא פשוט הנוסחה של ההסתברות הגאוסיאנית, אז אתם מחשבים את הדבר הזה, ואם ההסתברות הזאת קטנה מאוד, אז אתם מסמנים את הדוגמה החדשה הזו כחריגה. הנה דוגמה ליישום של שיטה זו. נניח שיש לנו קבוצת נתונים שמוצגת בגרף בפינה השמאלית העליונה של השקופית הזו. אם נסתכל על זה, בואו נסתכל על התכונה x1. אם נסתכל על סדרת הנתונים, נראה כאילו בממוצע, לתכונה x1 יש ממוצע של כ-5 וסטיית תקן, אם מסתכלים רק על ערכי x1 של הנתונים, יש להם סטיית תקן של אולי 2. אז זה σ1, ונראה כאילו שלגבי x2, הערכים של התכונות כפי שאנו מודדים על הציר האנכי, נראה שהממוצע הוא כ-3, וסטיית התקן שלו היא בערך 1. אז אם ניקח את סדרת הנתונים ונעריך את μ1, μ2, σ1 ו-σ2, זה מה שנקבל. ושוב, אני כותב כאן σ, ואני חושב על סטיות תקן, אבל הנוסחה בשקופית הקודמת בעצם נתנה את האומדנים של הריבועים של הדברים האלה, σ²1 ו-σ²2. צריך להיות זהירים בשימוש ב-σ1 ו-σ2 או ב-σ²1 או σ²2. σ²1 בריבוע כמובן שווה ל-4, הריבוע של 2. ובתמונות אנחנו רואים את p של x1 עם הפרמטרים μ1 ו-σ²1 בריבוע ואת p של x2 עם הפרמטרים μ2 ו-σ²2 בריבוע, זה נראה כמו שתי ההתפלגויות כאן. ומסתבר שאם נעשה גרף של p של x, שהוא המכפלה של שני הדברים האלה, נקבל למעשה גרף תלת-ממדי שנראה ככה. זה הגרף של p של x, שבו הגובה מעל המישור, הגובה של המשטח הזה בנקודה מסוימת, אם נתונים x1 ו-x2, ערכים מסוימים שלהם, אם נניח x1 שווה 2 ו-x2 שווה 2, בנקודה הזאת. אז הגובה התלת-ממדי כאן, זה p של x. אז p של x, זה גובה הגרף בנקודה הזו, הוא פשוט p של x1 עם פרמטרים μ1 ו-σ²₁, כפול p של x2 עם פרמטרים μ2 ו-σ²2. אז כך אנחנו מחשבים או מתאימים את הפרמטרים לנתונים האלה. בואו נראה מה קורה כשיש לנו דוגמאות חדשות. אולי יש לנו דוגמא חדשה כאן. האם היא אנומליה או לא? או אולי יש לנו דוגמה אחרת, אולי דוגמה אחרת שם. האם היא אנומליה או לא? אנחנו עושים את זה על ידי הגדרת ערך כלשהו עבור ε (אפסילון), נניח שנבחר ε שווה 0.02. אני אדבר יותר מאוחר על איך בוחרים ε. אבל בואו ניקח את הדוגמה הראשונה, תנו לי לקרוא לה דוגמת מבחן x1. ותנו לי לקרוא לדוגמה השנייה דוגמת מבחן x2. מה שאנחנו עושים הוא לחשב את p של דוגמת מבחן x1, אנחנו משתמשים בנוסחה הזו כדי לחשב את זה וזה נראה כמו ערך גדול למדי. בפרט, זה גדול, או גדול שווה ל-ε. ולכן יש לדוגמה הסתברות גבוהה למדי, לפחות גדולה מ-ε, ולכן אנו מחליטים שדוגמת מבחן x1 איננה חריגה. ולעומת זאת, אם נחשב את p של דוגמת מבחן x2, נקבל ערך הרבה יותר קטן. ערך קטן מ-ε ולכן נאמר שהיא אכן אנומליה, כי קיבלנו מספר הרבה יותר קטן מ-ε עליו החלטנו. ולמעשה, לא הוכחתי את זה, אבל מה שזה באמת אומר היא שאם נסתכל על הגרף התלת-ממדי הזה, זה אומר שכל הערכים של x1 ו-x2 שנמצאים בגובה גבוה מעל פני השטח, מתאימים לדוגמה לא חריגה של דגימה טובה ורגילה. בעוד שכל הנקודות הרחוקות כאן, כל הנקודות כאן, לכל הנקודות האלה יש סבירות נמוכה מאוד, אז אנחנו נסמן את הנקודות האלה כחריגות, וזה מגדיר אזור מסוים, שאולי נראה כך, כך שכל דבר מחוץ לו אנחנו מסמנים כחריג, ואילו כל הדברים בתוך האליפסה הזאת שציירתי, נראים בסדר, או לא חריגים, דוגמאות לא חריגות. אז דוגמת מבחן x2 נמצאת מחוץ לאזור הזה, ולכן יש לה הסתברות קטנה מאוד, ולכן אנו רואים בה דוגמה חריגה. בסרטון הזה דיברנו על איך אומדים את p של x, ההסתברות של x, לצורך פיתוח אלגוריתם זיהוי חריגים. ועוד בסרטון הזה עברנו על תהליך שבו כשאנחנו מקבלים ערכת נתונים, בנינו חישובים והערכות של הפרמטרים, חישבנו את הפרמטרים μ ו-σ, ואז קיבלנו דוגמאות חדשות והחלטנו אם הדוגמאות החדשות הן חריגות או לא. בכמה קטעי הוידאו הבאים אנו נצלול עמוק לתוך האלגוריתם הזה, ונדבר קצת יותר על איך באמת לגרום לו לעבוד היטב.