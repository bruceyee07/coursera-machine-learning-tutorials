בסרטון זה אני רוצה לדבר על התפלגות גאוסיאנית שנקראת גם התפלגות נורמלית. אם אתם כבר מכירים היטב את ההתפלגות הגאוסיאנית, זה בסדר גמור לדלג על הסרטון הזה, אבל אם אתם לא בטוחים או אם עבר זמן מה מאז שעבדתם עם התפלגות גאוסיאנית או התפלגות נורמלית אז בבקשה צפו בסרטון הזה עד הסוף. ובווידאו שאחרי זה נתחיל ליישם את ההתפלגות הגאוסיאנית כדי לפתח אלגוריתם זיהוי חריגים או זיהוי אנומליה. נניח ש-x הוא משתנה אקראי עם ערך ממשי, אז x הוא מספר ממשי. אם הסתברות ההתפלגות של x היא גאוסיאנית עם ממוצע μ (מיו) ושונות σ² (סיגמא בריבוע), אז נכתוב זאת כ- x, המשתנה האקראי. טילדה, ~ , הטילדה הקטנה הזו, המשמעות של זה היא "מתפלג כ". ואז כדי לציין התפלגות גאוסיאנית, לפעמים אני אכתוב N מסוגננת, ובסוגריים - (μ,σ²) - מיו פסיק סיגמא בריבוע. ה-N המסולסלת מציינת "נורמלי" מכיוון שגאוסיאני ונורמלי הן מילים בעלות משמעות זהה, מילים נרדפות. ולהתפלגות הגאוסיאנית יש שני פרמטרים, פרמטר הממוצע שאותו אנו מציינים ב-μ, ופרמטר השונות שאותו אנו מציינים ב-σ². אם נשרטט התפלגות גאוסיאנית או את צפיפות ההסתברות הגאוסיאנית, זה ייראה כמו פעמון, צורת פעמון שייתכן שראיתם בעבר. וכך עקומת הפעמון הזו מקבלת שני פרמטרים, μ ו-σ. המיקום של מרכז הפעמון הוא הממוצע, μ. ורוחב עקומת הפעמון, בערך בגובה הזה, זה הפרמטר σ, והוא נקרא גם סטיית תקן אחת, והוא מציין את ההסתברות ש-x מקבל ערכים שונים. x מקבל ערכים שנמצאים כאן באמצע העקומה בהסתברות גבוהה, כי הצפיפות הגאוסיאנית כאן היא די גבוהה, ואילו ההסתברות ש-x מקבל ערכים רחוק יותר מהאמצע יורדת והולכת. רק למען הַשְלֵמוּת תנו לי לכתוב את הנוסחה של התפלגות גאוסיאנית. ההסתברות של x, ולפעמים במקום לכתוב את זה כ-(p(x אני אכתוב את זה כ-(P(x; μ, σ², מה שמציין שההסתברות של x יש לה שני הפרמטרים μ וσ². אז הנוסחה של צפיפות גאוסיאנית היא 1 חלקי שורש 2π כפול σ כפול e בחזקת מינוס (x-μ) בריבוע חלקי 2σ². אין צורך לזכור את הנוסחה הזו. זוהי הנוסחה של עקומת הפעמון כאן משמאל. אין צורך לזכור אותה, וכשתצטרכו להשתמש בה, תמיד תוכלו למצוא אותה. אז הגרף משמאל, זה מה שתקבל אם תיקח ערך קבוע של μ וערך קבוע של σ, ותשרטט את הגרף של (p(x, אז תקבל את העקומה הזו. זה בעצם (p(x משורטט כפונקציה של x עבור ערכים קבועים של μ ו-σ². ודרך אגב, לפעמים קל יותר לחשוב במונחים של σ², מה שנקרא השונות. ולפעמים קל יותר לחשוב במונחים של σ. σ נקראת סטיית תקן, והיא מציינת את רוחב צפיפות ההתפלגות הגאוסיאנית הזו, וכאמור σ² נקראת שונות. בואו נראה כמה דוגמאות של איך נראית התפלגות גאוסיאנית. אם μ שווה לאפס ו-σ שווה לאחת, אז יש לנו התפלגות גאוסיאנית שממורכזת סביב האפס, כי μ הוא אפס, והרוחב של הגאוסיאן הזה, דהיינו סטיית תקן אחת היא σ כאן. בואו נראה כמה דוגמאות של גאוסיאנים. אם μ שווה לאפס ו-σ שווה לאחת, זה מתאים להתפלגות גאוסיאנית הממוקדת באפס, שכן μ הוא אפס, ורוחב הגאוסיאן הזה נשלט על ידי הפרמטר σ. הנה עוד דוגמה. אותו μ עדיין שווה ל-0 ו-σ שווה ל-0.5, אז סטיית התקן היא 0.5 והשונות, σ², היא הריבוע של 0.5 דהיינו 0.25 ובמקרה הזה ההתפלגות הגאוסיאנית, צפיפות ההסתברות הגאוסיאנית דומה לזֶה. וגם היא ממורכזת באפס. אבל עכשיו הרוחב הזה הוא הרבה יותר קטן, כי השונות קטנה יותר, הרוחב של הצפיפות הגאוסיאנית היא בערך חצי מהרוחב. אבל מכיוון שזו היא התפלגות של הסתברות, לכן השטח מתחת לעקומה, האזור המוצלל כאן, לאזור הזה יש שטח שווה לאחת, זה מאפיין של כל התפלגויות ההסתברויות. ולכן הצפיפות כאן חייבת להיות הרבה יותר גבוהה, כי הרוחב שלה הוא חצי, חצי רוחב של סטיית תקן ולכן הוא גבוה כפליים. עוד דוגמה היא עם σ שווה 2 אז מקבלים צפיפות גאוסיאנית הרבה יותר שמנה ורחבה, הפרמטר σ שולט בכך שלהתפלגות הגאוסיאנית יש רוחב גדול יותר. ושוב, האזור מתחת לעקומה, האזור המקווקו, תמיד יהיה בעל שטח 1, זהו מאפיין של התפלגויות הסתברותיות, וומכיוון שהוא רחב יותר, הוא גם יותר נמוך כך שהאינטגרל הוא עדיין אותו דבר. ולבסוף, עוד דוגמה אחת, נניח שאנחנו עכשיו משנים גם את הפרמטר μ. אז במקום להיות מרוכזים מסביב ל-0 עכשיו תהיה לנו התפלגות גאוסיאנית מסביב ל-3 כי זה מזיז את כל פעמון ההתפלגות הגאוסיאנית. עכשיו, בואו נדבר על בעיית אומדן הפרמטרים. מהי בעיית אומדן הפרמטרים? נניח שיש לנו מערך נתונים של m דוגמאות x1 עד xm ונניח שכל דוגמא היא מספר ממשי. כאן בתרשים התוויתי דוגמה של נתונים כאלה, הציר האופקי הוא ציר x ויש לנו מגוון של דוגמאות x, ואני פשוט שרטטתי אותם על הגרף הזה כאן. בעיית אומדן הפרמטרים היא זו, נניח שאני חושד שהדוגמאות האלה מתפלגות גאוסיאנית. אז נניח שאני חושד שכל אחת מהדוגמאות xi, מתפלגת, זה מה שמסמן הטילדה הזה, אני חושד שכל הדוגמאות הללו מתפלגות בהתפלגות נורמלית, או התפלגות גאוסיאנית, עם איזשהו פרמטר μ ואיזשהו פרמטר σ². אבל אני לא יודע מה הערכים של הפרמטרים האלה. הבעיה של אומדן או הערכת הפרמטרים היא,
כאשר אנחנו מקבלים ערכת נתונים כזו, אנחנו רוצים לנסות להבין, אנחנו רוצים להעריך מה הם הערכים של μ ו-σ². אז אם אנחנו מקבלים ערכת נתונים שנראית כך, זה נראה כאילו שאם נעריך את ההתפלגות הגאוסיאנית ממנה הגיעו הנתונים, אולי היא תיראה בערך כך. עם μ באמצע הפיזור, וסיגמה סטיית התקן מגדירה את הרוחב של ההתפלגות הגאוסיאנית הזו. זה נראה כמו התאמה סבירה לנתונים. כי נראה שלנתונים יש סבירות גבוהה מאוד להיות באזור המרכזי, והסתברות יותר נמוכה להיות רחוקים יותר, ועוד יותר נמוכה לעוד יותר רחוק וכן הלאה. אז אולי זה אומדן סביר של μ ו-σ². כלומר, אם ההתפלגות היא אכן נורמלית, אז פונקצית ההתפלגות נראית ככה. אז אני פשוט אכתוב את הנוסחאות הסטנדרטיות להערכת הפרמטרים μ ו-σ². ההערכה שלנו לגבי μ תהיה פשוט הממוצע של כל הדוגמאות. μ הוא הפרמטר של ממוצע ההתפלגות. פשוט ניקח את ערכת האימון, ניקח את m הדוגמאות שלנו ונמצא את הממוצע שלהם. זה פשוט נותן את מרכז ההתפלגות הזאת. מה בקשר ל-σ²? ובכן, השונות, אני פשוט שוב אכתוב את הנוסחה הרגילה, אני מעריך את השונות כסכום מ-1 עד m של xi פחות μ בריבוע. ו-μ כאן הוא למעשה אותו μ שחישבנו לפני רגע באמצעות הנוסחה הזו. והמשמעות של השונות, או יותר נכון פרשנות אחת למשמעות של השונות היא שכשמסתכלים על המונח הזה, הוא ריבוע ההפרש בין ערך הדוגמא והממוצע, דהיינו פחות האמצע, פחות אמצע ההתפלגות, אז אנחנו מעריכים את השונות כממוצע ריבועי ההפרשים בין הדוגמאות ובין הממוצע. והערת אגב, רק לאלו מכם המומחים בסטטיסטיקה. אם אתה מומחה לסטטיסטיקה, ואם שמעת על אומדן סבירות מקסימלית, אז ההערכות האלה הן למעשה אומדני הסבירות המקסימלית של הפרמטרים μ ו-σ², אבל מי שלא מכיר את הנושא, אין לכם מה לדאוג, כל מה שאתם צריכים לדעת הוא שאלה הן שתי הנוסחאות הסטנדרטיות כדי להעריך את μ ו-σ² לגבי ערכת נתונים נתונה. לבסוף עוד הערה צדדית לאלה מכם שלמדו סטטיסטיקה. חלק מכם אולי ראו את הנוסחה כאן אבל במקום m היה כאן m-1 אז הכופל הראשון כאן הוא 1 חלקי m-1 במקום 1 חלקי m. בלמידת מכונה אנחנו נוטים להשתמש בנוסחה 1 חלקי m, אבל למעשה אם זה 1 חלקי m או 1 חלקי m-1 זה לא ממש משנה בהנחה ש-m הוא לא ממש קטן. אם יש סדרת אימון גדולה במידה סבירה. אז אם במקרה ראיתם את הגרסה השונה מקודם, בשתי הגרסאות זה עובד כמעט במידה שווה, אבל בלמידת מכונה רובנו נוטים להשתמש ב-1 חלקי m בנוסחה הזו. ולשתי הגירסאות יש מאפיינים תיאורטיים מעט שונים זה מזה, תכונות מתמטיות שונות במעט. אבל למעשה באמת יש הבדל קטן מאוד, אם בכלל. אז אני מקווה שעכשיו יש לכם תחושה טובה של איך נראית התפלגות גאוסיאנית, כמו גם כיצד להעריך את הפרמטרים μ ו-σ² של התפלגות גאוסיאנית אם אתם מקבלים ערכת אימון, כלומר אם אתם מקבלים קבוצת נתונים שאתה חושב שהיא מתפלגת גאוסיאנית עם פרמטרים לא ידועים, μ ו-σ². בסרטון הבא, נתחיל לקחת את הדברים האלה ולהשתמש בה כדי לפתח אלגוריתם לאיתור אנומליה.