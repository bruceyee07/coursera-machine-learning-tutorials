이번 시간을 포함해 앞으로 몇 개의 동영상에서는 
분류(classification) 문제에 대해 이야기할 건데요, 예측하고자 하는 y 변수가 이산형 값을 가지는
(discrete valued) 경우에 대해 알아보겠습니다. 또한, 가장 널리 쓰이는 학습 알고리즘 중 하나인 로지스틱 회귀(logistic algorithm)에 대해 배워보도록 하죠. 우선 분류 문제에 대한 예를 몇 가지 봅시다. 앞서 스팸 이메일 검출이 분류 문제에 포함된다고 이야기한 바 있습니다. 온라인 거래도 한 가지 예가 될 수 있겠네요. 물건을 파는 쇼핑 웹사이트에서 특정 거래가 사기인지 아닌지, 즉 누군가가 도용된 신용카드나 다른 유저의 비밀번호를 
사용하고 있지는 않은지 분류할 수 있습니다. 다른 예로는 무엇이 있을까요? 악성 종양과 양성 종양을 구분하는 
문제도 분류 문제에 해당합니다. 이 또한 앞에서 말한 적이 있죠. 지금까지 논의한 문제들은 모두 0 또는 1 둘 중의 하나의 값을 가지는 
변수 y를 예측하는 문제입니다. 스팸인지 아닌지, 사기인지 아닌지, 
악성 종양인지 양성 종양인지처럼 말이죠. 0으로 표현하는 분류를 보통 음성 분류라고도 부르고, 1로 표현하는 분류를 양성 분류라고 합니다. 따라서, 양성 종양은 0으로, 악성 종양은 1로 표기할 수 있겠죠. 스팸인지 아닌지 등의 두 가지 분류 중 어떤 것을 음성 또는 양성, 0 또는 1로 표현할지는 다소 임의적인 측면이 있습니다. 항상 그런 것은 아니지만, 보통은 
우리가 찾고자 하는 무언가가 없는 경우 예컨데, 악성 종양이 없는 경우를 음성으로 분류하고, 해당 현상이 있는 경우, 즉, 악성 종양이 발견된 경우를 양성으로 분류하는 것이 직관적입니다. 그러나 반드시 이를 따를 필요는 없습니다. 우선은 0과 1 두 가지의 값만을 갖는 분류 문제를 먼저 다루도록 하고 이후에 0, 1, 2, 3 등 여러가지 y 값을 찾는 분류 문제도 
다룰 수 있도록 하겠습니다. 이와 같은 문제를 다중분류
(multiclass classification) 문제라고 하죠. 그러나 당분간은 2진분류
(binary-class) 문제만을 다루고 다중분류 상황은 이후에 고려하겠습니다. 그럼 분류 알고리즘을 어떻게 개발할까요? 여기 악성종양 / 양성종양 분류 작업을 위한 훈련 집합(training set)의 예시가 있습니다. 보시는 것처럼 악성 여부는 두 가지 값만을 가집니다: 
양성을 의미하는 0과 악성을 의미하는 1이죠. 따라서 이 학습집합에 대해 
우리가 할 수 있는 것 중 하나는 이미 알고 있는 선형회귀 알고리즘을 데이터에 대해 적용해 보는 일이에요. 즉, 직선 그래프에 데이터를 맞춰 보는 것이죠. 주어진 데이터에 대해 직선을 그어 보면 다음과 같은 가설을 얻을 수 있습니다. 이것이 제 가설이고, hθ(x)는 세타의 전치행렬(θT) X 와 같습니다. 예측을 하고 싶은 상황에서 
여러분이 할 수 있는 건 hθ(x)의 결과값이 0.5가 되는 곳에
수직 임계값 경계선을 긋는 것입니다. 그런 다음, 가설이 0.5와 동일하거나 더 큰 결과값을 
낼 경우 y는 1로 볼 수 있습니다. 0.5보다 작을 경우, y는 0으로 보면 되겠죠. 그렇게 바꾸면 어떻게 되나 봅시다. 0.5에 대해 임계값을 설정하는 것이 바로 선형회귀를 사용하는 방법입니다. 이 지점으로부터 오른쪽에 있는 모든 데이터는 양성(y=1)의 값을 가진다고 
예상할 수 있습니다. 왜냐하면 결과값이 수직축의 
0.5보다 크기 때문입니다. 또 이 지점으로부터 왼쪽의 모든 것은 
음성(y=0)임을 예측할 수 있습니다. 이 예시에서 선형회귀가 마치 합리적으로 
작동하는 것처럼 보일 수 있습니다. 비록 우리가 지금 하려는 것이 
분류 문제일지라도 말이죠. 이제 문제를 조금 바꿔봅시다. 이제 수평축을 조금 더 확장한 다음, 오른쪽 끝에 훈련 데이터를 
하나 더 추가해 보죠. 이 추가된 부분을 주목해 보면 여기에 하나가 생겼을 뿐 
다른 것은 전혀 바뀌지 않았습니다. 이제 이렇게 생긴 훈련 집합을 보면
 어떤게 좋은 가설인지 분명히 알 수 있습니다. 처음에 있었던 훈련 집합에서 
이 지점을 기준으로 하면 오른쪽에 존재할 경우 양성이라고 예측하고, 왼쪽은 음성일 거라고 예측할 수 있었죠. 그리고 훈련 집합 중에서 
이 특정 값보다 큰 모든 종양들은 악성으로 보이고, 그것보다 작은 종양들은 
악성이 아닌 것으로 보입니다. 처음에 있던 훈련 집합에서는 말이죠. 하지만 여기에 데이터를 추가하고 
선형 회귀를 진행한다고 생각해 보세요. 그러면 추가된 데이터에 맞는 
새로운 직선을 얻을 것입니다. 아마도 이렇게 생겼을 거예요. 그리고 y=0.5가 되는 가설의 임계값을, 이쯤에 설정하겠네요. 그래서 이 지점보다 오른쪽에 있는 
데이터는 양성이라 예측하고, 왼쪽에 있는 건 모두 
음성이라 생각할 것입니다. 결론적으로 보면 선형 회귀를 적용해 
좋지 않은 결과를 얻었네요. 아시다시피 이것들은 우리가 가진 양성 데이터이고 
또 이것들이 음성 데이터였습니다. 우리가 두 예시 집단들을 이쯤에서 
나누어야 하는 것은 분명합니다. 하지만 이 오른쪽 끝머리에 
데이터 하나를 더하기만 하면, 이 예시들은 우리에게 
새로운 정보를 주지 못합니다 다시 말하자면, 여기 있는 새 데이터가 
악성 종양으로 표시되는 건 학습 알고리즘에게 있어서 
별로 놀라운 일이 아님에도 불구하고 저 데이터를 추가함으로 인해 선형 회귀에서 데이터를 피팅하는 직선이 원래의 이 진홍색에서 여기 파란 선으로 바뀌게 되고, 
우리에게 악화된 가설을 가져오게 됩니다. 그러므로, 선형 회귀를 분류 문제에 적용하는 건 대부분의 경우에서 좋은 생각이 아닙니다. 이 첫번째 예시에서, 제가 데이터를 추가하기 이전의 선형 회귀는 운좋게 우리에게 특정 예시들에 대해 잘 작동하는 
가설을 주었지만 일반적으로 선형회귀를 (분류) 데이터에 적용하면,
운이 좋을 수도 있지만 대부분은 잘못될 것입니다. 그래서 저라면 분류 문제들에 대해 선형 회귀를 사용하지 않을 겁니다. 분류 문제에 선형 회귀를 사용할 경우 
어떤 일이 일어나는지 또 다른 재미있는 예제를 보여 드리겠습니다. 분류 문제에서 y는 0이나 1이 된다는 것을 
여러분은 이미 알고 있습니다. 그러나 만일 여러분이 선형 회귀를 사용한다면 가설의 결과값이 1보다 훨씬 크거나<br />0보다 작을 수 있습니다. 여러분들의 훈련용 예시들 속 y값이<br />전부 0이나 1로 설정되어 있더라도 말이죠. 우리가 결과값이 0, 1이란 것 아는데도 알고리즘의 결과값이 1보다 훨씬 크거나 0보다 작다면, 매우 이상하게 느껴집니다. 그래서 앞으로의 강의에서는 
로지스틱 회귀라 불리는 알고리즘을 공부할 건데요, 
로지스틱 회귀의 결과값, 예측값은 항상 0에서 1사이이며 1보다 크거나 0보다 작은 값을 가질 수 없습니다. 참고로 로지스틱 회귀는 '분류' 알고리즘으로 사용할 건데요 로지스틱 회귀는 분류 알고리즘이지만 '회귀'라는 단어 때문에 여러분이 헷갈릴 수도 있겠네요. 그건 그냥 역사적인 이유로 주어진 이름입니다. 그러므로 로지스틱 회귀가 y값이 0이나 1인 이산적 값을 
가지는 분류 알고리즘을 뜻한다는 것을 헷갈리지 마세요. 이번 강의에서 여러분들이 
분류 문제를 해결해야 할 때 선형 회귀를 쓰는 것은 
좋지 않다는 것을 알았을 겁니다. 다음 비디오에서는 로지스틱 회귀에 대해 자세히 알아보겠습니다.