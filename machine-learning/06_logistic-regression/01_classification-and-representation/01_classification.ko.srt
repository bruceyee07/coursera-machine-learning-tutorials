1
00:00:00,500 --> 00:00:04,650
이번 시간을 포함해 앞으로 몇 개의 동영상에서는 
분류(classification) 문제에 대해 이야기할 건데요,

2
00:00:04,650 --> 00:00:09,510
예측하고자 하는 y 변수가 이산형 값을 가지는
(discrete valued) 경우에 대해 알아보겠습니다.

3
00:00:09,510 --> 00:00:12,651
또한, 가장 널리 쓰이는 학습 알고리즘 중 하나인

4
00:00:12,651 --> 00:00:16,931
로지스틱 회귀(logistic algorithm)에 대해 배워보도록 하죠.

5
00:00:19,473 --> 00:00:23,270
우선 분류 문제에 대한 예를 몇 가지 봅시다.

6
00:00:23,270 --> 00:00:26,530
앞서 스팸 이메일 검출이

7
00:00:26,530 --> 00:00:29,390
분류 문제에 포함된다고 이야기한 바 있습니다.

8
00:00:29,390 --> 00:00:33,110
온라인 거래도 한 가지 예가 될 수 있겠네요.

9
00:00:33,110 --> 00:00:35,540
물건을 파는 쇼핑 웹사이트에서

10
00:00:35,540 --> 00:00:39,390
특정 거래가 사기인지 아닌지, 즉 누군가가

11
00:00:39,390 --> 00:00:44,590
도용된 신용카드나 다른 유저의 비밀번호를 
사용하고 있지는 않은지 분류할 수 있습니다.

12
00:00:44,590 --> 00:00:46,830
다른 예로는 무엇이 있을까요?

13
00:00:46,830 --> 00:00:50,840
악성 종양과 양성 종양을 구분하는 
문제도 분류 문제에 해당합니다.

14
00:00:50,840 --> 00:00:53,960
이 또한 앞에서 말한 적이 있죠.

15
00:00:55,120 --> 00:00:59,450
지금까지 논의한 문제들은 모두

16
00:00:59,450 --> 00:01:04,160
0 또는 1 둘 중의 하나의 값을 가지는 
변수 y를 예측하는 문제입니다.

17
00:01:04,160 --> 00:01:09,080
스팸인지 아닌지, 사기인지 아닌지, 
악성 종양인지 양성 종양인지처럼 말이죠.

18
00:01:10,500 --> 00:01:15,670
0으로 표현하는 분류를 보통 음성 분류라고도 부르고,

19
00:01:15,670 --> 00:01:20,020
1로 표현하는 분류를 양성 분류라고 합니다.

20
00:01:20,020 --> 00:01:23,930
따라서, 양성 종양은 0으로,

21
00:01:23,930 --> 00:01:27,110
악성 종양은 1로 표기할 수 있겠죠.

22
00:01:27,110 --> 00:01:31,370
스팸인지 아닌지 등의 두 가지 분류 중

23
00:01:31,370 --> 00:01:35,120
어떤 것을 음성 또는 양성, 0 또는 1로 표현할지는

24
00:01:35,120 --> 00:01:37,290
다소 임의적인 측면이 있습니다.

25
00:01:37,290 --> 00:01:42,220
항상 그런 것은 아니지만, 보통은 
우리가 찾고자 하는 무언가가 없는 경우

26
00:01:42,220 --> 00:01:46,590
예컨데, 악성 종양이 없는 경우를 음성으로 분류하고,

27
00:01:46,590 --> 00:01:51,460
해당 현상이 있는 경우, 즉, 악성 종양이 발견된 경우를

28
00:01:51,460 --> 00:01:55,170
양성으로 분류하는 것이 직관적입니다.

29
00:01:55,170 --> 00:01:58,790
그러나 반드시 이를 따를 필요는 없습니다.

30
00:02:00,150 --> 00:02:03,080
우선은 0과 1 두 가지의 값만을 갖는

31
00:02:03,080 --> 00:02:05,510
분류 문제를 먼저 다루도록 하고

32
00:02:05,510 --> 00:02:09,320
이후에 0, 1, 2, 3 등

33
00:02:09,320 --> 00:02:14,250
여러가지 y 값을 찾는 분류 문제도 
다룰 수 있도록 하겠습니다.

34
00:02:14,250 --> 00:02:17,720
이와 같은 문제를 다중분류
(multiclass classification) 문제라고 하죠.

35
00:02:17,720 --> 00:02:22,140
그러나 당분간은 2진분류
(binary-class) 문제만을 다루고

36
00:02:22,140 --> 00:02:25,978
다중분류 상황은 이후에 고려하겠습니다.

37
00:02:25,978 --> 00:02:30,580
그럼 분류 알고리즘을 어떻게 개발할까요?

38
00:02:30,580 --> 00:02:34,770
여기 악성종양 / 양성종양 분류 작업을 위한

39
00:02:34,770 --> 00:02:37,410
훈련 집합(training set)의 예시가 있습니다.

40
00:02:37,410 --> 00:02:44,570
보시는 것처럼 악성 여부는 두 가지 값만을 가집니다: 
양성을 의미하는 0과 악성을 의미하는 1이죠.

41
00:02:44,570 --> 00:02:47,520
따라서 이 학습집합에 대해 
우리가 할 수 있는 것 중 하나는

42
00:02:47,520 --> 00:02:50,309
이미 알고 있는 선형회귀 알고리즘을

43
00:02:51,410 --> 00:02:53,410
데이터에 대해 적용해 보는 일이에요.

44
00:02:53,410 --> 00:02:56,320
즉, 직선 그래프에 데이터를 맞춰 보는 것이죠.

45
00:02:56,320 --> 00:02:59,840
주어진 데이터에 대해 직선을 그어 보면

46
00:02:59,840 --> 00:03:03,730
다음과 같은 가설을 얻을 수 있습니다.

47
00:03:03,730 --> 00:03:05,695
이것이 제 가설이고,

48
00:03:05,695 --> 00:03:09,100
hθ(x)는 세타의 전치행렬(θT) X 와 같습니다.

49
00:03:09,100 --> 00:03:14,650
예측을 하고 싶은 상황에서 
여러분이 할 수 있는 건

50
00:03:14,650 --> 00:03:19,165
hθ(x)의 결과값이 0.5가 되는 곳에
수직 임계값 경계선을 긋는 것입니다.

51
00:03:19,165 --> 00:03:23,985
그런 다음, 가설이

52
00:03:23,985 --> 00:03:27,235
0.5와 동일하거나 더 큰 결과값을 
낼 경우 y는 1로 볼 수 있습니다.

53
00:03:27,235 --> 00:03:29,955
0.5보다 작을 경우, y는 0으로 보면 되겠죠.

54
00:03:29,955 --> 00:03:32,775
그렇게 바꾸면 어떻게 되나 봅시다.

55
00:03:32,775 --> 00:03:36,359
0.5에 대해 임계값을 설정하는 것이 바로

56
00:03:36,359 --> 00:03:39,990
선형회귀를 사용하는 방법입니다.

57
00:03:39,990 --> 00:03:43,520
이 지점으로부터 오른쪽에 있는 모든 데이터는

58
00:03:43,520 --> 00:03:44,710
양성(y=1)의 값을 가진다고 
예상할 수 있습니다.

59
00:03:44,710 --> 00:03:50,360
왜냐하면 결과값이 수직축의 
0.5보다 크기 때문입니다.

60
00:03:50,360 --> 00:03:54,629
또 이 지점으로부터 왼쪽의 모든 것은 
음성(y=0)임을 예측할 수 있습니다.

61
00:03:55,720 --> 00:03:57,620
이 예시에서

62
00:03:57,620 --> 00:04:01,630
선형회귀가 마치 합리적으로 
작동하는 것처럼 보일 수 있습니다.

63
00:04:01,630 --> 00:04:05,420
비록 우리가 지금 하려는 것이 
분류 문제일지라도 말이죠.

64
00:04:05,420 --> 00:04:08,120
이제 문제를 조금 바꿔봅시다.

65
00:04:08,120 --> 00:04:11,530
이제 수평축을 조금 더 확장한 다음,

66
00:04:11,530 --> 00:04:15,263
오른쪽 끝에 훈련 데이터를 
하나 더 추가해 보죠.

67
00:04:15,263 --> 00:04:18,900
이 추가된 부분을 주목해 보면

68
00:04:18,900 --> 00:04:21,960
여기에 하나가 생겼을 뿐 
다른 것은 전혀 바뀌지 않았습니다.

69
00:04:21,960 --> 00:04:26,200
이제 이렇게 생긴 훈련 집합을 보면
 어떤게 좋은 가설인지 분명히 알 수 있습니다.

70
00:04:26,200 --> 00:04:28,970
처음에 있었던 훈련 집합에서 
이 지점을 기준으로 하면

71
00:04:28,970 --> 00:04:31,010
오른쪽에 존재할 경우 양성이라고 예측하고,

72
00:04:31,010 --> 00:04:34,930
왼쪽은 음성일 거라고 예측할 수 있었죠.

73
00:04:34,930 --> 00:04:39,620
그리고 훈련 집합 중에서 
이 특정 값보다 큰 모든 종양들은

74
00:04:39,620 --> 00:04:44,210
악성으로 보이고, 그것보다 작은 종양들은 
악성이 아닌 것으로 보입니다.

75
00:04:44,210 --> 00:04:44,820
처음에 있던 훈련 집합에서는 말이죠.

76
00:04:46,200 --> 00:04:50,730
하지만 여기에 데이터를 추가하고 
선형 회귀를 진행한다고 생각해 보세요.

77
00:04:50,730 --> 00:04:54,480
그러면 추가된 데이터에 맞는 
새로운 직선을 얻을 것입니다.

78
00:04:54,480 --> 00:04:56,090
아마도 이렇게 생겼을 거예요.

79
00:04:57,600 --> 00:05:02,890
그리고 y=0.5가 되는 가설의 임계값을,

80
00:05:02,890 --> 00:05:06,350
이쯤에 설정하겠네요.

81
00:05:06,350 --> 00:05:09,750
그래서 이 지점보다 오른쪽에 있는 
데이터는 양성이라 예측하고,

82
00:05:09,750 --> 00:05:12,070
왼쪽에 있는 건 모두 
음성이라 생각할 것입니다.

83
00:05:14,590 --> 00:05:18,820
결론적으로 보면 선형 회귀를 적용해 
좋지 않은 결과를 얻었네요.

84
00:05:18,820 --> 00:05:23,110
아시다시피 이것들은 우리가 가진 양성 데이터이고 
또 이것들이 음성 데이터였습니다.

85
00:05:23,110 --> 00:05:28,090
우리가 두 예시 집단들을 이쯤에서 
나누어야 하는 것은 분명합니다.

86
00:05:28,090 --> 00:05:31,260
하지만 이 오른쪽 끝머리에 
데이터 하나를 더하기만 하면,

87
00:05:31,260 --> 00:05:34,300
이 예시들은 우리에게 
새로운 정보를 주지 못합니다

88
00:05:34,300 --> 00:05:37,050
다시 말하자면, 여기 있는 새 데이터가 
악성 종양으로 표시되는 건

89
00:05:37,050 --> 00:05:40,260
학습 알고리즘에게 있어서 
별로 놀라운 일이 아님에도 불구하고

90
00:05:40,260 --> 00:05:45,210
저 데이터를 추가함으로 인해 선형 회귀에서

91
00:05:45,210 --> 00:05:50,880
데이터를 피팅하는 직선이 원래의 이 진홍색에서

92
00:05:50,880 --> 00:05:55,670
여기 파란 선으로 바뀌게 되고, 
우리에게 악화된 가설을 가져오게 됩니다.

93
00:05:56,900 --> 00:06:01,120
그러므로, 선형 회귀를 분류 문제에 적용하는 건

94
00:06:01,120 --> 00:06:04,470
대부분의 경우에서 좋은 생각이 아닙니다.

95
00:06:04,470 --> 00:06:09,870
이 첫번째 예시에서, 제가 데이터를 추가하기

96
00:06:09,870 --> 00:06:14,760
이전의 선형 회귀는 운좋게 우리에게

97
00:06:14,760 --> 00:06:19,940
특정 예시들에 대해 잘 작동하는 
가설을 주었지만 일반적으로

98
00:06:19,940 --> 00:06:24,760
선형회귀를 (분류) 데이터에 적용하면,
운이 좋을 수도 있지만 대부분은 잘못될 것입니다.

99
00:06:24,760 --> 00:06:28,350
그래서 저라면 분류 문제들에 대해 선형 회귀를 사용하지 않을 겁니다.

100
00:06:29,700 --> 00:06:33,830
분류 문제에 선형 회귀를 사용할 경우 
어떤 일이 일어나는지

101
00:06:33,830 --> 00:06:36,740
또 다른 재미있는 예제를 보여 드리겠습니다.

102
00:06:36,740 --> 00:06:40,630
분류 문제에서 y는 0이나 1이 된다는 것을 
여러분은 이미 알고 있습니다.

103
00:06:40,630 --> 00:06:44,250
그러나 만일 여러분이 선형 회귀를 사용한다면

104
00:06:44,250 --> 00:06:48,380
가설의 결과값이 1보다 훨씬 크거나<br />0보다 작을 수 있습니다.

105
00:06:48,380 --> 00:06:52,570
여러분들의 훈련용 예시들 속 y값이<br />전부 0이나 1로 설정되어 있더라도 말이죠.

106
00:06:53,920 --> 00:06:56,739
우리가 결과값이 0, 1이란 것 아는데도

107
00:06:56,739 --> 00:07:00,786
알고리즘의 결과값이 1보다 훨씬 크거나

108
00:07:00,786 --> 00:07:05,661
0보다 작다면, 매우 이상하게 느껴집니다.

109
00:07:09,135 --> 00:07:13,795
그래서 앞으로의 강의에서는 
로지스틱 회귀라 불리는

110
00:07:13,795 --> 00:07:17,044
알고리즘을 공부할 건데요, 
로지스틱 회귀의 결과값,

111
00:07:17,044 --> 00:07:21,635
예측값은 항상 0에서 1사이이며

112
00:07:21,635 --> 00:07:25,114
1보다 크거나 0보다 작은 값을 가질 수 없습니다.

113
00:07:26,250 --> 00:07:29,260
참고로 로지스틱 회귀는

114
00:07:29,260 --> 00:07:33,370
'분류' 알고리즘으로 사용할 건데요

115
00:07:33,370 --> 00:07:38,230
로지스틱 회귀는 분류 알고리즘이지만

116
00:07:38,230 --> 00:07:42,150
'회귀'라는 단어 때문에 여러분이 헷갈릴 수도 있겠네요.

117
00:07:42,150 --> 00:07:44,720
그건 그냥 역사적인 이유로 주어진 이름입니다.

118
00:07:44,720 --> 00:07:49,210
그러므로 로지스틱 회귀가

119
00:07:49,210 --> 00:07:54,542
y값이 0이나 1인 이산적 값을 
가지는 분류 알고리즘을

120
00:07:54,542 --> 00:07:56,610
뜻한다는 것을 헷갈리지 마세요.

121
00:07:56,610 --> 00:08:01,000
이번 강의에서 여러분들이 
분류 문제를 해결해야 할 때

122
00:08:01,000 --> 00:08:03,640
선형 회귀를 쓰는 것은 
좋지 않다는 것을 알았을 겁니다.

123
00:08:03,640 --> 00:08:04,500
다음 비디오에서는

124
00:08:04,500 --> 00:08:08,080
로지스틱 회귀에 대해 자세히 알아보겠습니다.