בסרטון זה נדבר על איך להתאים את הפרמטרים θ לרגרסיה לוגיסטית. בפרט אני רוצה להגדיר את מטרת האופטימיזציה, או את פונקצית העלות בה נשתמש כדי להתאים את הפרמטרים. הנה בעיית הלמידה בפיקוח של התאמת מודל רגרסיה לוגיסטית. יש לנו סדרת אימון של m דוגמאות אימון, וכרגיל, כל אחת מהדוגמאות שלנו מיוצגת על ידי וקטור תכונות n+1 ממדי, שבו כרגיל x0 שווה ל-1. המאפיין או התכונה הראשונה או בעצם תכונה מספר 0 תמיד שווה ל-1. מאחר ומדובר בבעית סיווג, לסדרת האימון שלנו יש המאפיין שכל y, כל התוויות, הן או 0 או 1. זוהי ההשערה, והפרמטרים של ההשערה הם וקטור θ זה שכאן. והשאלה שאני רוצה לדבר עליה היא, בהינתן סדרת האימון הזו, איך אנחנו בוחרים או איך אנחנו מתאימים את הפרמטרים θ? כשעסקנו בפיתוח מודל של רגרסיה ליניארית, השתמשנו בפונקציית העלות הזו. כתבתי אותה בצורה קצת שונה כשבמקום לכתוב 1 חלקי 2m העברתי את החצי לתוך הסכימה. עכשיו אני רוצה להשתמש בדרך חלופית לכתוב את פונקצית העלות הזו. אז במקום לכתוב את הריבוע של הביטוי הזה כאן, בואו נכתוב כאן העלות של h(x),y ונגדיר שהעלות הכוללת של h(x),y שווה לזה. שווה למחצית ריבוע השגיאה. כשכותבים את זה כך, קל יותר לראות שפונקציית העלות היא סכום על כל סדרת האימון, שהיא 1 חלקי m הסכום על כל סדרת האימון של ביטוי העלות הזה. וכדי לפשט את המשוואה הזאת עוד יותר, זה יהיה לנו יותר נוח להיפטר מהסימולים העיליים האלה. אז פשוט נגדיר את העלות של h(x),y להיות שווה למחצית ריבוע השגיאה. והפרשנות של פונקציית עלות זו היא, שזה המחיר שאני רוצה שאלגוריתם הלמידה שלי ישלם אם הוא פולט את הערך הזה. אם התחזית שלו היא (h(x, והערך בפועל היה y. פשוט נמחק את הסימנים העיליים, כך, ובאופן לא מפתיע עבור הרגרסיה הליניארית העלות שהגדרנו היא חצי ריבוע ההפרש בין מה שניבאה הפונקציה ובין הערך האמיתי שראינו עבור y. פונקצית העלות הזו עבדה היטב עבור רגרסיה ליניארית. אבל כאן אנחנו מדברים על רגרסיה לוגיסטית. אם נוכל למזער את פונקצית העלות הזו שהצבנו כאן ב-J, זה יעבוד בסדר. אבל מתברר שאם נשתמש בפונקצית העלות המסוימת הזו, נקבל פונקציה לא-קמורה של וקטור הפרמטרים θ. הנה מה שאני מתכוון כשאני אומר לא קמורה. יש לנו פונקצית עלות (J(θ ובמקרה של רגרסיה לוגיסטית, הפונקציה h הזו כאן יש לה אי-ליניאריות, הערך שלה הוא אחת חלקי אחת ועוד e בחזקת θᵀx-. זאת אומרת פונקציה לא-ליניארית די מסובכת. ואם אתה לוקח את פונקצית הסיגמואיד ומציב אותה כאן ואז אתה לוקח את פונקצית העלות הזו ומציב אותה שם ואז משרטט איך נראה (J(θ. ניתן לראות ש-(J(θ יכול להיראות כך, פונקציה עם הרבה אופטימומים מקומיים. השם הפורמלי לתופעה הוא שהפונקציה היא לא-קמורה. ואפשר לראות בעין שלו הרצנו את הירידה במדרון על סוג כזה של פונקציה, לא מובטח לנו שהיא תתכנס דווקא למינימום הגלובלי. בעוד מה שהיינו רוצים הוא שפונקצית העלות (J(θ תהיה קמורה, שתהיה פונקציה אחת בצורת קערה שנראית כך, כך שאם נפעיל ירידה בגרדיינט נוכל להיות בטוחים שהיא תתכנס למִינִימוּם הגלובלי. והבעיה עם השימוש בפונקציה העלות הריבועית הזאת היא שבגלל פונקצית הסיגמואיד המאוד לא ליניארית הזו שמופיעה בתוך הביטוי כאן, (J(θ היא פונקציה לא-קמורה לו ניסינו להגדיר אותה כפונקציה העלות של ריבוע שגיאה. אז מה שאנחנו רוצים לעשות הוא למצוא פונקציית עלות שונה שהיא כן קמורה, כדי שנוכל ליישם אלגוריתם כדוגמת הירידה בגרדיינט ולדעת שאכן נמצא את המינימום הגלובלי. אז הנה פונקצית העלות בה נשתמש עבור רגרסיה לוגיסטית. אנחנו נגדיר שהעלות או הקנס שהאלגוריתם ישלם, אם הוא מנבא את הערך (h(x, זה איזה שהוא מספר כמו 0.7, זה מנבא את הערך (h(x, והתברר שהערך של הדוגמא בפועל הוא y, אז העלות היא ((log(h(x- אם y = 1 ו-((log(1- h(x- אם y = 0. זה נראה די מסובך, אבל בואו נשרטט את הפונקציה כדי לקבל קצת אינטואיציה לגבי התנהגותה בפועל. בואו נתחיל במקרה של y = 1. אם y = 1, אז פונקצית העלות היא ((log(h(x- ואם נצייר את זה, בואו נאמר שהציר האופקי הוא (h(x, ואנו יודעים כי פונקציית ההשערה מוציאה ערך פלט בין 0 ל-1. אז (h(x משתנה בין 0 ל-1. אם נצייר איך נראית פונקצית העלות הזו, נמצא שהיא נראית כך. דרך פשוטה לראות מדוע השרטוט נראה כך היא שלו שרטטנו את (log(z כש-z הוא על הציר האופקי, אז זה היה נראה ככה. והוא מתקרב אסימפטוטית למינוס אינסוף, נכון? אז כך נראית פונקצית log. כאן 0, כאן 1. ובמקרה שלנו z כמובן משחק את התפקיד של (h(x. ולכן (log(z- תראה ככה. רק נהפוך את הסימן, מינוס (log(z, ואנחנו מתעניינים רק בתחום הפונקציה שבין אפס ואחת, אז ניפטר מזה. וכך נשארנו רק עם החלק זה של העקום, וכך נראה העקום הזה משמאל. עכשיו, לפונקצית העלות הזו יש כמה תכונות מעניינות ורצויות. ראשית, אתם יכולים לראות שאם y שווה ל-1 ו-(h(x שווה ל-1, כלומר, אם ההשערה צופה בדיוק ש-h שווה 1 ו-y אכן שווה בדיוק למה שההשערה ניבאה, אז העלות = 0, נכון? זה תואם לכך ש... העקומה לא ממש נהיית מאוזנת. העקומה עדיין ממשיכה לרדת. ראשית, אתם יכולים לראות שאם (h(x שווה ל-1 אם ההשערה צופה ש-y שווה ל-1 ו-y אכן שווה ל-1, אז העלות היא 0. זה מתאים לנקודה כאן למטה. אם h(x) = 1 ואנחנו רק מסתכלים על המקרה של y = 1 כאן.אבל אם (h(x שווה 1, אז העלות היא כאן למטה, ושווה ל-0. וזה באמת מה שאנחנו רוצים שיהיה, שאם חזינו נכונה את הפלט y, אז העלות היא 0. אבל עכשיו שימו גם לב שכאשר (h(x מתקרב ל-0, כשהפלט של ההשערה מתקרב ל-0, העלות גדלה וגדלה עד אינסוף. והמשמעות של זה היא ההבנה האינטואיטיבית שלנו שאם ההשערה היא 0, זה כמו להגיד שההשערה אומרת שהסיכוי ש-y=1 היא 0. זה דומה במקרה שלנו לאמירה לחולה שלנו שההסתברות שיש לו גידול ממאיר, ההסתברות ש-y=1, היא אפס. או במלים אחרות, זה בלתי אפשרי לחלוטין שהגידול שלך ממאיר. אבל אם יתברר שהגידול שהגידול של המטופל הוא למעשה כן ממאיר, או במילים אחרות ש-y = 1 גם לאחר שסיפרנו לו שההסתברות שזה יקרה היא אפס. שזה לחלוטין בלתי אפשרי שהוא ממאיר אז אם אמרנו להם את זה ברמת וודאות גבוהה והתברר שטעינו, אז אנחנו נעניש את אלגוריתם הלמידה בעלות מאוד מאוד גדולה. וזה מה שקורה עם התכונה הזו של הפונקציה שהיא עולה עד אינסוף אם y בעצם שווה 1 ו-(h(x מתקרב ל-0. השקופית הזו דנה במקרה של y = 1. בואו נראה איך נראית פונקציית העלות עבור y = 0. אם y = 0, אז העלות נראית כך, כמו הביטוי הזה כאן, ואם נשרטט את הפונקציה, (log(1-z-, מה שנקבל הוא פונקצית עלות שנראית בעצם כך. התחום שלה הוא בין 0 ל -1, משהו כזה, ואם נשרטט אותה עבור המקרה של y = 0, נגלה שהיא נראית כך. ומה שהעקום הזה עושה עכשיו הוא עולה ועולה עד פלוס אינסוף כאשר h של x הולך ומתקרב ל-1 כי כמו שאמרתי קודם, אם יתברר ש-y = 0 למרות שצפינו ש-y = 1 כמעט בודאות, שהוא כנראה 1, אז אנחנו נשלם מחיר, או עלות, גבוהה מאוד. לעומת זאת, אם h(x)=0 ו-y=0, אז פונקצית ההשערה פגעה בול, היא חזתה ש-y=0 ומתברר ש-y אכן שווה לאפס. אז במצב כזה פונקצית המחיר תתן אפס. בסרטון הזה, הגדרנו את פונקצית העלות עבור דוגמת אימון יחידה. הנושא של ניתוח הקמירות הוא מעבר להיקף של הקורס הזה, אבל ניתן להראות שעם בחירה מסוימת של פונקצית עלות, זה ייתן בעיית אופטימיזציה קמורה. פונקצית העלות (J(θ תהיה קמורה וחסרת אופטימומים מקומיים. בסרטון הבא אנחנו ניקח את הרעיונות הללו של פונקציית העלות עבור דוגמת אימון יחידה ונפתח אותם יותר, ונגדיר את פונקציית העלות עבור כלל סדרת האימון. וגם נמצא דרך יותר פשוטה לכתוב את זה מאשר איך שבטאנו את זה עד כאן, ועל סמך זה אנחנו נפתח ירידה במדרון, ומזה נקבל את אלגוריתם הרגרסיה הלוגיסטית.