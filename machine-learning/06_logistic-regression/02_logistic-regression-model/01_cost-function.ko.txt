이번 영상에서는 로지스틱 회귀(logistic regression)에서 매개변수 θ를 어떻게 fitting하는지 알아보도록 하죠. 특히, 매개변수 θ를 fitting하기 위해서 우리가 최적화할 목표물, 즉 비용함수가 무엇인지 정의하려고 합니다. 여기 로지스틱 회귀 모델을 적용한 지도 학습 문제가 있습니다. 그리고 m개의 학습 예제로 구성된 트레이닝 집합이 있고 그 학습 예제는 n+1차원으로 표시할 수 있습니다. 그리고 평소처럼 x0=1이고요. 첫 번째 혹은 0번째 변수값은 항상 1 입니다 왜냐하면 이것은 계산 문제에서 우리가 사용할 훈련 집합에 붙어 있는 
모든 라벨 y는 0 혹은 1이기 때문입니다. 이것은 가설이고요, 그 가설식의 매개변수인 θ가 이부분 입니다. 그리고 제가 말하고자 하는 것은 여기에 주어진 트레이닝 집합을 가지고 어떻게 매개변수인 θ값을 선택 또는 찾을 것인가 입니다. 저번에 공부했던 선형 회귀 모델로 돌아가 보지요. 우리는 여기 있는 비용함수를 사용했었는데요. 이 함수를 저번과 조금 다르게 써 보았습니다. 
원래 밖에 있던 1/2m 중에 1/2을 sum 안에 집어넣었습니다. 자 이 비용함수를 또 다른 방식으로 써 볼게요, 여기 제곱 수식 전체 부분을 다 쓰지 말고 이부분을 cost( hθ(x) , y) 라고 씁시다. (hθ(x), y)의 총 비용을 정의하면 
이 식이랑 같아집니다. 즉 오차를 제곱한 다음 1/2를 곱한 식과 같게 되죠. 그래서 이제 우리는 비용함수란 훈련집합의 
총합이라는 사실을 명확히 볼 수 있게 되었어요. 즉 이 비용 항을 다 더하고 1/m을 곱한 거지요. 이 식을 좀 더 간단히 해 보면, 여기 위 첨자를 지우면 편하죠. 그래서 그냥 '(hθ(x),y)의 비용은 오차 제곱의 1/2와 같다'라고 정의할 수 있어요. 그리고 이 비용 함수를 해석해 보자면, 예상하는 값이 hθ(x)이고, 실제 라벨이 y일 때 
해당하는 값을 출력할 경우 학습 알고리즘에게 지불해 주었으면 하는 비용을 뜻합니다. 즉, 단순히 위첨자를 지웠을 뿐입니다. 그리고 선형 회귀의 경우 당연히 정의 된 비용은 저겁니다. 즉 이 비용은 1/2를 곱한 뒤 예측한 값과 실제 관측된 값의 차를 제곱한 값입니다. 이 비용 함수는 선형 회귀를 다룰 때에는 문제가 없었어요. 그치만, 우리가 지금 하고 있는 건 로지스틱 회귀입니다. 만약 여기에 J를 대입하고, 이 비용 함수를 최소화할 수 있다면 그럼 문제가 없을 거예요. 하지만 우리가 여기 이 비용 함수를 사용한다면 이 비용 함수는 매개변수 θ에 대해 
볼록 함수가 아니라고 밝혀졌어요. 볼록 함수가 아니라고 한 게 무슨 뜻이냐면, 우리에게 어떤 비용함수 J(θ)가 있어서 로지스틱 회귀 문제를 다룰 때 이 함수 h가 비선형성을 가져서, 즉, 1/(1+e^(-θT)x)가 됩니다. 즉 이건 꽤 복잡한 비선형 함수입니다. 그리고 이 sigmoid함수를 다시 이 안에 대입하고, 또 이 비용 함수를 여기에 대입한다면 J(θ)는 이런 모양이 됩니다. 여기 보이는 바와 같이 J(θ)는 이렇게 많은 극소점을 가진 함수가 되지요. 이런 함수를 수학적 용어로 비볼록함수라고 합니다. 그리고 이런 비볼록함수의 경우 
경사하강법을 적용해도 전체 함수의 최소값에 도달한다는 보장이 없습니다. 반면에 비용함수 J(θ)가 볼록함수라고 한다면 이 함수는 활 모양으로 한 번만 구부러진 함수이고, θ에 대해 경사하강법을 적용함으로서 수렴되는 값은 반드시 전체 함수의 최소값이라고 할 수 있습니다. 그러므로 이 제곱된 비용 함수를 
사용함으로 인해 생기는 문제점은 이 가운데 있는 게 비선형적인 함수이기 때문에 J(θ)가 비볼록함수가 된다는 겁니다. 그래서 우리가 하고 싶은 건, 
다른 비용 함수를 제시하는 겁니다. 볼록한 비용 함수를요. 그래서 
경사하강법 같은 좋은 알고리즘을 적용할 수 있도록 하고, 전체 함수의 
최소값을 찾을 수 있도록 하는 거죠. 이제 로지스틱 회귀에서 사용할 
비용함수를 살펴보겠습니다. 우리가 이야기할 것은 비용, 즉 알고리즘이 지불해야 할 값에 대한 겁니다. 알고리즘이 hθ(x)값을 넘어섰을 때요. 그래서 예를 들어 이게 0.7같은 숫자라면, 그건 hθ(x)값을 예측하고 있습니다. 그리고 실제 비용을 가리키는 레이블은 y가 됩니다. 이 비용은 y=1일 때 -log(hθ(x))이고요, y=0일 때 -log(1-hθ(x))입니다. 이거 좀 복잡해 보이죠? 하지만 이 함수를 그리면서 이게 뭘 하는지 직감적으로 알아봅시다. 먼저 y=1일 때 입니다. 만약 y=1이라면, 비용함수는 -log(hθ(x))입니다. 이걸 플롯해보면, 여기 x축을 h(x)라고 합시다. 그러면 우리가 알고 있는 대로 
가설 h(x)는 0 아니면 1이잖아요, 맞죠, 그래서 h(x)는 0에서 1까지만 값을 가집니다. 이 비용함수가 어떻게 생겼는지 플롯해 보면 
이렇게 된다는 걸 알 수 있어요. 왜 이런지 알고 싶으면 log(z)를 플롯해 보세요. z를 x축으로 잡고요, 그러면 이렇게 되잖아요. 마이너스 무한대로 접근하는 함수, 맞죠? 로그함수는 이렇게 생겼었어요. 여기가 0이고 여기가 1. 여기에서 z는 우리가 하고 있는 h(x)의 역할을 하고, -log(z)는 이렇게 생겼잖아요. 플러스마이너스를 뒤집으면 이게 
-log(z)예요. 그리고 우리가 알고 싶은 건 이 함수 중에서 0과 1 사이가 어떻게 생겼는지이므로, 나머지를 지웁시다. 그러면 남은 게, 이제 알겠죠. 이 부분의 곡선이에요. 그리고 이게 바로 여기 왼쪽의 곡선입니다. 이제 이 비용함수는 몇 가지 재미있고 바람직한 속성을 가지고 있어요. 첫째로, 잘 보면 y=1인 경우 h(x)=1이 됩니다. 다시 말하면, 가설을 정확하계 예측한 경우 h=1이고 y가 가설이 예측한 값과 맞아떨어진다면 
비용은 0이 됩니다. 맞죠? 이건 곡선이 평평해지지 않는다는 사실과 부합합니다. 커브는 계속 꺾어지게 됩니다. 다시 해 보죠, h(x)=1일 경우에 주목한다면 이 가설은 y=1일 것을 예측하고, 실제로 y=1이라면 비용은 0이 됩니다. 그건 바로 여기 이 점에 해당합니다. 맞죠? 만약 h(x)=1이라면 우리가 생각할 수 있는 
y=1인 경우는 여기 한군데입니다. 그런데 만약에 h(x)=1이고 비용이 여기라면 이건 0가 됩니다. 그리고 이건 우리가 원하던 건데요, 
왜냐하면 우리가 결과값 y를 정확히 예측했다면 비용은 0이기 때문이죠. 하지만 여기서 하나 더 알게 된 점은, 
여기 h(x)가 0에 가까워지면 그래서 가설의 출력값이 0에 가까워지면, 
비용은 증대하고 무한대로 가 버린다는 겁니다. 그리고 이게 무슨 역할을 하냐면, 가설 hθ(x)=0, 즉 y=1일 확률이 0일 때를 직관적으로 포착하게 해 줍니다. 예를 들어 당신은 의사이고 환자에게 이렇게 말했습니다. "당신이 악성 종양을 가지고 있을 확률, 즉 y=1일 확률은 0입니다." "그래서 당신의 종양이 악성이라는 건 절대적으로 불가능해요." 하지만 이런 경우도 있죠. 검사를 해 보니 그 종양, 그 환자의 종양이 악성이었습니다. 그래서 만약 y=1이라면, 벌써 환자에게 말했는데, '악성일 확률은 0이에요' 하고. '그래서 악성 종양이라는 건 불가능해요' 하고 말했는데, 만약 우리가 환자에게 그렇게 확신을 가지고 말했는데 결국엔 틀렸다면 이 학습 알고리즘을 처벌해야 하죠. 엄청난 비용을 부과시켜야 해요. 그게 여기에 포착된 비용의 값입니다. 만약에라도 y=1이라는 값이 나오면 
비용은 무한대이고 h(x)=0이 됩니다. 이 슬라이드는 y=1일 경우를 설명하고 있는데요, 이제 y=1일 경우에 비용함수가 
어떻게 생겼는지 살펴 봅시다. 만약 y=0라면 비용은 다음과 같습니다. 여기에 있는 표현과 같은데요, 이 함수를 플롯해 보면 -log(1-z), 당신이 얻는 비용함수는 
실제로 이렇게 생겼습니다. 그래서 0부터 1까지만 그려보면, 이렇게 되죠. 그래서 y=0일 때 비용함수를 플롯하면 
이런 그래프가 됩니다. 그리고 이 곡선이 하는 건, 이제 위로 올라가서 h(x)가 1일 때 플러스 무한대까지 가요. 
왜냐하면 제가 말했던 것처럼 y는 결국에 0일텐데 우리가 y=1이라고 예측을 한다면, 
거의 확실히 1이예요 하고 말해 버리면 우리는 엄청난 비용을 지불해야 할 겁니다. 만약 각각의 x가 0이고 y=0라면 우리의 가설이 사라집니다. 가설에 의해 보호받았던 y(z)는 0이 되고요. 그리고 결론적으로 y=0이 되어요. 그래서 여기서 비용함수는 0이 될 겁니다. 이번 수업에서 우리는 한 가지 훈련 예제에서 비용함수를 
정의했습니다. 함수가 볼록인지 비볼록인지에 관한 해석은 이 강의의 범위를 넘어서는 거지만 비용함수를 어떻게 선택해야 하는지와 
관련해서 조금 보여드렸습니다. 이걸로 볼록함수의 최적화 문제를 다룰 수 있게 되었지요. 모든 비용함수 J(θ)는 볼록함수이며 극소점 문제에 대해 생각할 
필요가 없습니다. 다음 강의에서는 우리가 이번에 했던 한 개의 훈련 
예제에 대한 비용함수에서 더 나아가 전체 훈련 예제에 대한 비용함수를 
정의할 겁니다. 그리고 또 지금까지 사용해 왔던 비용함수의 식을 더 
간단히 나타낼 방법에 대해 알아보겠습니다. 새로운 식에 기초해서 경사하강법을 적용하고, 그러면 로지스틱 회귀 알고리즘에 대해 알 수 있을 겁니다. 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다 영어자막에 오류가 있습니다