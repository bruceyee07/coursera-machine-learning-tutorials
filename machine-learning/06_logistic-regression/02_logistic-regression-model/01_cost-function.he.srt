1
00:00:00,000 --> 00:00:02,370
בסרטון זה נדבר על איך להתאים

2
00:00:02,370 --> 00:00:05,650
את הפרמטרים θ לרגרסיה לוגיסטית.

3
00:00:05,650 --> 00:00:07,650
בפרט אני רוצה להגדיר

4
00:00:07,650 --> 00:00:14,290
את מטרת האופטימיזציה, או את פונקצית העלות בה נשתמש כדי להתאים את הפרמטרים.

5
00:00:14,420 --> 00:00:19,750
הנה בעיית הלמידה בפיקוח של התאמת מודל רגרסיה לוגיסטית.

6
00:00:19,750 --> 00:00:24,650
יש לנו סדרת אימון של m דוגמאות אימון,

7
00:00:24,650 --> 00:00:27,630
וכרגיל, כל אחת מהדוגמאות שלנו

8
00:00:27,630 --> 00:00:32,250
מיוצגת על ידי וקטור תכונות n+1 ממדי,

9
00:00:32,250 --> 00:00:35,960
שבו כרגיל x0 שווה ל-1.

10
00:00:35,960 --> 00:00:40,040
המאפיין או התכונה הראשונה או בעצם תכונה מספר 0 תמיד שווה ל-1.

11
00:00:40,040 --> 00:00:43,805
מאחר ומדובר בבעית סיווג, לסדרת האימון שלנו

12
00:00:43,805 --> 00:00:48,250
יש המאפיין שכל y, כל התוויות, הן או 0 או 1.

13
00:00:48,250 --> 00:00:54,620
זוהי ההשערה, והפרמטרים של ההשערה הם וקטור θ זה שכאן.

14
00:00:54,620 --> 00:00:58,610
והשאלה שאני רוצה לדבר עליה היא, בהינתן סדרת האימון הזו,

15
00:00:58,610 --> 00:01:02,105
איך אנחנו בוחרים או איך אנחנו מתאימים את הפרמטרים θ?

16
00:01:02,105 --> 00:01:06,045
כשעסקנו בפיתוח מודל של רגרסיה ליניארית,

17
00:01:06,045 --> 00:01:08,315
השתמשנו בפונקציית העלות הזו.

18
00:01:08,315 --> 00:01:12,435
כתבתי אותה בצורה קצת שונה כשבמקום לכתוב 1 חלקי 2m

19
00:01:12,435 --> 00:01:16,100
העברתי את החצי לתוך הסכימה.

20
00:01:16,100 --> 00:01:20,360
עכשיו אני רוצה להשתמש בדרך חלופית לכתוב את פונקצית העלות הזו.

21
00:01:20,360 --> 00:01:23,770
אז במקום לכתוב את הריבוע של הביטוי הזה כאן,

22
00:01:23,770 --> 00:01:25,370
בואו נכתוב כאן

23
00:01:25,370 --> 00:01:32,560
העלות של h(x),y

24
00:01:32,560 --> 00:01:38,080
ונגדיר שהעלות הכוללת של h(x),y

25
00:01:38,080 --> 00:01:39,810
שווה לזה.

26
00:01:39,810 --> 00:01:42,355
שווה למחצית ריבוע השגיאה.

27
00:01:42,355 --> 00:01:45,015
כשכותבים את זה כך, קל יותר לראות

28
00:01:45,015 --> 00:01:50,415
שפונקציית העלות היא סכום על כל סדרת האימון, שהיא 1 חלקי m

29
00:01:50,415 --> 00:01:55,425
הסכום על כל סדרת האימון של ביטוי העלות הזה.

30
00:01:55,425 --> 00:01:59,310
וכדי לפשט את המשוואה הזאת עוד יותר,

31
00:01:59,310 --> 00:02:02,265
זה יהיה לנו יותר נוח להיפטר מהסימולים העיליים האלה.

32
00:02:02,265 --> 00:02:05,220
אז פשוט נגדיר את העלות של h(x),y

33
00:02:05,220 --> 00:02:08,005
להיות שווה למחצית ריבוע השגיאה.

34
00:02:08,005 --> 00:02:11,220
והפרשנות של פונקציית עלות זו היא,

35
00:02:11,220 --> 00:02:18,660
שזה המחיר שאני רוצה שאלגוריתם הלמידה שלי ישלם אם הוא פולט את הערך הזה.

36
00:02:18,660 --> 00:02:22,790
אם התחזית שלו היא (h(x, והערך בפועל היה y.

37
00:02:22,790 --> 00:02:26,540
פשוט נמחק את הסימנים העיליים, כך,

38
00:02:26,540 --> 00:02:29,510
ובאופן לא מפתיע

39
00:02:29,510 --> 00:02:33,050
עבור הרגרסיה הליניארית העלות שהגדרנו היא

40
00:02:33,050 --> 00:02:36,260
חצי ריבוע ההפרש

41
00:02:36,260 --> 00:02:40,010
בין מה שניבאה הפונקציה ובין הערך האמיתי שראינו עבור y.

42
00:02:40,010 --> 00:02:42,620
פונקצית העלות הזו עבדה היטב עבור

43
00:02:42,620 --> 00:02:47,140
רגרסיה ליניארית. אבל כאן אנחנו מדברים על רגרסיה לוגיסטית.

44
00:02:47,140 --> 00:02:53,670
אם נוכל למזער את פונקצית העלות הזו שהצבנו כאן ב-J, זה יעבוד בסדר.

45
00:02:53,670 --> 00:02:56,010
אבל מתברר שאם נשתמש

46
00:02:56,010 --> 00:02:58,305
בפונקצית העלות המסוימת הזו, נקבל

47
00:02:58,305 --> 00:03:01,560
פונקציה לא-קמורה של וקטור הפרמטרים θ.

48
00:03:01,560 --> 00:03:03,885
הנה מה שאני מתכוון כשאני אומר לא קמורה.

49
00:03:03,885 --> 00:03:07,680
יש לנו פונקצית עלות (J(θ ובמקרה של

50
00:03:07,680 --> 00:03:13,440
רגרסיה לוגיסטית, הפונקציה h הזו כאן יש לה אי-ליניאריות,

51
00:03:13,440 --> 00:03:16,620
הערך שלה הוא אחת חלקי אחת ועוד e בחזקת θᵀx-.

52
00:03:16,620 --> 00:03:19,225
זאת אומרת פונקציה לא-ליניארית די מסובכת.

53
00:03:19,225 --> 00:03:21,650
ואם אתה לוקח את פונקצית הסיגמואיד

54
00:03:21,650 --> 00:03:25,090
ומציב אותה כאן ואז אתה לוקח את פונקצית העלות הזו ומציב

55
00:03:25,090 --> 00:03:28,780
אותה שם ואז משרטט איך נראה (J(θ.

56
00:03:28,780 --> 00:03:33,135
ניתן לראות ש-(J(θ יכול להיראות כך,

57
00:03:33,135 --> 00:03:36,010
פונקציה עם הרבה אופטימומים מקומיים.

58
00:03:36,010 --> 00:03:40,270
השם הפורמלי לתופעה הוא שהפונקציה היא לא-קמורה. ואפשר

59
00:03:40,270 --> 00:03:42,520
לראות בעין שלו הרצנו את הירידה במדרון על

60
00:03:42,520 --> 00:03:47,380
סוג כזה של פונקציה, לא מובטח לנו שהיא תתכנס דווקא למינימום הגלובלי.

61
00:03:47,380 --> 00:03:50,380
בעוד מה שהיינו רוצים הוא שפונקצית העלות (J(θ

62
00:03:50,380 --> 00:03:53,289
תהיה קמורה,

63
00:03:53,289 --> 00:03:56,145
שתהיה פונקציה אחת בצורת קערה שנראית כך,

64
00:03:56,145 --> 00:03:58,255
כך שאם נפעיל ירידה בגרדיינט

65
00:03:58,255 --> 00:04:04,340
נוכל להיות בטוחים שהיא תתכנס למִינִימוּם הגלובלי.

66
00:04:04,340 --> 00:04:08,600
והבעיה עם השימוש בפונקציה העלות הריבועית הזאת היא

67
00:04:08,600 --> 00:04:13,625
שבגלל פונקצית הסיגמואיד המאוד לא ליניארית הזו שמופיעה בתוך הביטוי כאן,

68
00:04:13,625 --> 00:04:16,040
(J(θ היא

69
00:04:16,040 --> 00:04:20,855
פונקציה לא-קמורה לו ניסינו להגדיר אותה כפונקציה העלות של ריבוע שגיאה.

70
00:04:20,855 --> 00:04:26,015
אז מה שאנחנו רוצים לעשות הוא למצוא פונקציית עלות שונה

71
00:04:26,015 --> 00:04:29,000
שהיא כן קמורה, כדי שנוכל ליישם

72
00:04:29,000 --> 00:04:33,470
אלגוריתם כדוגמת הירידה בגרדיינט ולדעת שאכן נמצא את המינימום הגלובלי.

73
00:04:33,470 --> 00:04:37,040
אז הנה פונקצית העלות בה נשתמש עבור רגרסיה לוגיסטית.

74
00:04:37,040 --> 00:04:40,770
אנחנו נגדיר שהעלות או הקנס שהאלגוריתם

75
00:04:40,770 --> 00:04:44,580
ישלם, אם הוא מנבא את הערך (h(x,

76
00:04:44,580 --> 00:04:47,700
זה איזה שהוא מספר כמו 0.7, זה מנבא

77
00:04:47,700 --> 00:04:53,075
את הערך (h(x, והתברר שהערך של הדוגמא בפועל הוא y,

78
00:04:53,075 --> 00:04:57,480
אז העלות היא ((log(h(x- אם y = 1

79
00:04:57,480 --> 00:05:02,015
ו-((log(1- h(x- אם y = 0.

80
00:05:02,015 --> 00:05:05,100
זה נראה די מסובך, אבל בואו נשרטט

81
00:05:05,100 --> 00:05:08,035
את הפונקציה כדי לקבל קצת אינטואיציה לגבי התנהגותה בפועל.

82
00:05:08,035 --> 00:05:11,060
בואו נתחיל במקרה של y = 1.

83
00:05:11,060 --> 00:05:19,250
אם y = 1, אז פונקצית העלות היא ((log(h(x- ואם נצייר את זה,

84
00:05:19,250 --> 00:05:23,035
בואו נאמר שהציר האופקי הוא (h(x,

85
00:05:23,035 --> 00:05:28,165
ואנו יודעים כי פונקציית ההשערה מוציאה ערך פלט בין 0 ל-1.

86
00:05:28,165 --> 00:05:31,930
אז (h(x משתנה בין 0 ל-1.

87
00:05:31,930 --> 00:05:35,510
אם נצייר איך נראית פונקצית העלות הזו,

88
00:05:35,510 --> 00:05:37,655
נמצא שהיא נראית כך.

89
00:05:37,655 --> 00:05:42,855
דרך פשוטה לראות מדוע השרטוט נראה כך היא שלו שרטטנו את

90
00:05:42,855 --> 00:05:46,060
(log(z כש-z

91
00:05:46,060 --> 00:05:50,860
הוא על הציר האופקי, אז זה היה נראה ככה. והוא מתקרב אסימפטוטית למינוס אינסוף,

92
00:05:50,860 --> 00:05:55,865
נכון? אז כך נראית פונקצית log. כאן 0, כאן 1.

93
00:05:55,865 --> 00:06:02,000
ובמקרה שלנו z כמובן משחק את התפקיד של (h(x. ולכן

94
00:06:02,000 --> 00:06:05,865
(log(z- תראה ככה.

95
00:06:05,865 --> 00:06:07,935
רק נהפוך את הסימן,

96
00:06:07,935 --> 00:06:09,370
מינוס (log(z,

97
00:06:09,370 --> 00:06:13,990
ואנחנו מתעניינים רק בתחום הפונקציה שבין אפס ואחת,

98
00:06:13,990 --> 00:06:17,920
אז ניפטר מזה. וכך נשארנו רק עם

99
00:06:17,920 --> 00:06:23,105
החלק זה של העקום, וכך נראה העקום הזה משמאל.

100
00:06:23,105 --> 00:06:29,510
עכשיו, לפונקצית העלות הזו יש כמה תכונות מעניינות ורצויות.

101
00:06:29,510 --> 00:06:34,900
ראשית, אתם יכולים לראות שאם y שווה ל-1 ו-(h(x שווה ל-1,

102
00:06:34,900 --> 00:06:38,420
כלומר, אם ההשערה צופה בדיוק

103
00:06:38,420 --> 00:06:42,545
ש-h שווה 1 ו-y אכן שווה בדיוק למה שההשערה ניבאה,

104
00:06:42,545 --> 00:06:44,335
אז העלות = 0, נכון?

105
00:06:44,335 --> 00:06:47,480
זה תואם לכך ש... העקומה לא ממש נהיית מאוזנת.

106
00:06:47,480 --> 00:06:48,765
העקומה עדיין ממשיכה לרדת.

107
00:06:48,765 --> 00:06:51,695
ראשית, אתם יכולים לראות שאם (h(x שווה ל-1

108
00:06:51,695 --> 00:06:54,880
אם ההשערה צופה ש-y שווה ל-1

109
00:06:54,880 --> 00:06:58,920
ו-y אכן שווה ל-1, אז העלות היא 0.

110
00:06:58,920 --> 00:07:00,965
זה מתאים לנקודה כאן למטה.

111
00:07:00,965 --> 00:07:03,975
אם h(x) = 1 ואנחנו רק

112
00:07:03,975 --> 00:07:07,440
מסתכלים על המקרה של y = 1 כאן.אבל אם (h(x

113
00:07:07,440 --> 00:07:12,720
שווה 1, אז העלות היא כאן למטה, ושווה ל-0. וזה באמת מה שאנחנו רוצים

114
00:07:12,720 --> 00:07:18,235
שיהיה, שאם חזינו נכונה את הפלט y, אז העלות היא 0.

115
00:07:18,235 --> 00:07:22,660
אבל עכשיו שימו גם לב שכאשר

116
00:07:22,660 --> 00:07:27,220
(h(x מתקרב ל-0, כשהפלט של ההשערה מתקרב ל-0,

117
00:07:27,220 --> 00:07:30,210
העלות גדלה וגדלה עד אינסוף.

118
00:07:30,210 --> 00:07:36,895
והמשמעות של זה היא ההבנה האינטואיטיבית שלנו שאם ההשערה היא 0,

119
00:07:36,895 --> 00:07:41,595
זה כמו להגיד שההשערה אומרת שהסיכוי ש-y=1 היא 0.

120
00:07:41,595 --> 00:07:43,820
זה דומה במקרה שלנו לאמירה לחולה שלנו

121
00:07:43,820 --> 00:07:47,230
שההסתברות שיש לו גידול ממאיר,

122
00:07:47,230 --> 00:07:49,235
ההסתברות ש-y=1, היא אפס.

123
00:07:49,235 --> 00:07:54,800
או במלים אחרות, זה בלתי אפשרי לחלוטין שהגידול שלך ממאיר.

124
00:07:54,800 --> 00:07:57,530
אבל אם יתברר שהגידול

125
00:07:57,530 --> 00:08:00,110
שהגידול של המטופל הוא למעשה כן ממאיר,

126
00:08:00,110 --> 00:08:01,920
או במילים אחרות ש-y = 1

127
00:08:01,920 --> 00:08:05,390
גם לאחר שסיפרנו לו שההסתברות שזה יקרה היא אפס.

128
00:08:05,390 --> 00:08:08,410
שזה לחלוטין בלתי אפשרי שהוא ממאיר

129
00:08:08,410 --> 00:08:13,050
אז אם אמרנו להם את זה ברמת וודאות גבוהה והתברר שטעינו,

130
00:08:13,050 --> 00:08:14,970
אז אנחנו נעניש את אלגוריתם הלמידה

131
00:08:14,970 --> 00:08:17,910
בעלות מאוד מאוד גדולה. וזה מה שקורה

132
00:08:17,910 --> 00:08:24,245
עם התכונה הזו של הפונקציה שהיא עולה עד אינסוף אם y בעצם שווה 1 ו-(h(x מתקרב ל-0.

133
00:08:24,245 --> 00:08:28,045
השקופית הזו דנה במקרה של y = 1.

134
00:08:28,045 --> 00:08:31,965
בואו נראה איך נראית פונקציית העלות עבור y = 0.

135
00:08:31,965 --> 00:08:34,210
אם y = 0,

136
00:08:34,210 --> 00:08:38,855
אז העלות נראית כך, כמו הביטוי הזה כאן,

137
00:08:38,855 --> 00:08:45,285
ואם נשרטט את הפונקציה, (log(1-z-,

138
00:08:45,285 --> 00:08:49,015
מה שנקבל הוא פונקצית עלות שנראית בעצם כך.

139
00:08:49,015 --> 00:08:52,635
התחום שלה הוא בין 0 ל -1, משהו כזה,

140
00:08:52,635 --> 00:08:55,930
ואם נשרטט אותה עבור המקרה

141
00:08:55,930 --> 00:08:59,270
של y = 0, נגלה שהיא נראית כך.

142
00:08:59,270 --> 00:09:02,460
ומה שהעקום הזה עושה עכשיו

143
00:09:02,460 --> 00:09:08,975
הוא עולה ועולה עד פלוס אינסוף כאשר h של x הולך ומתקרב ל-1 כי כמו

144
00:09:08,975 --> 00:09:14,110
שאמרתי קודם, אם יתברר ש-y = 0 למרות שצפינו ש-y = 1

145
00:09:14,110 --> 00:09:19,750
כמעט בודאות, שהוא כנראה 1, אז אנחנו נשלם מחיר, או עלות, גבוהה מאוד.

146
00:09:19,750 --> 00:09:27,205
לעומת זאת, אם h(x)=0 ו-y=0, אז פונקצית ההשערה פגעה בול,

147
00:09:27,205 --> 00:09:31,950
היא חזתה ש-y=0 ומתברר ש-y אכן שווה לאפס.

148
00:09:31,950 --> 00:09:38,220
אז במצב כזה פונקצית המחיר תתן אפס.

149
00:09:38,220 --> 00:09:44,130
בסרטון הזה, הגדרנו את פונקצית העלות עבור דוגמת אימון יחידה.

150
00:09:44,130 --> 00:09:48,390
הנושא של ניתוח הקמירות הוא מעבר להיקף של הקורס הזה,

151
00:09:48,390 --> 00:09:52,320
אבל ניתן להראות שעם בחירה מסוימת של פונקצית עלות,

152
00:09:52,320 --> 00:09:56,965
זה ייתן בעיית אופטימיזציה קמורה.

153
00:09:56,965 --> 00:10:02,135
פונקצית העלות (J(θ תהיה קמורה וחסרת אופטימומים מקומיים.

154
00:10:02,135 --> 00:10:05,440
בסרטון הבא אנחנו ניקח את הרעיונות הללו

155
00:10:05,440 --> 00:10:09,870
של פונקציית העלות עבור דוגמת אימון יחידה ונפתח אותם יותר,

156
00:10:09,870 --> 00:10:12,840
ונגדיר את פונקציית העלות עבור כלל סדרת האימון.

157
00:10:12,840 --> 00:10:16,530
וגם נמצא דרך יותר פשוטה לכתוב את זה מאשר איך

158
00:10:16,530 --> 00:10:19,600
שבטאנו את זה עד כאן, ועל סמך זה אנחנו נפתח

159
00:10:19,600 --> 00:10:23,870
ירידה במדרון, ומזה נקבל את אלגוריתם הרגרסיה הלוגיסטית.