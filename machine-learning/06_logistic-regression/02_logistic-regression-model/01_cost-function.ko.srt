1
00:00:00,200 --> 00:00:04,200
이번 영상에서는 로지스틱 회귀(logistic regression)에서 

2
00:00:04,200 --> 00:00:05,930
매개변수 θ를 어떻게 fitting하는지 알아보도록 하죠.

3
00:00:05,930 --> 00:00:10,020
특히, 매개변수 θ를 fitting하기 위해서

4
00:00:10,020 --> 00:00:13,180
우리가 최적화할 목표물, 즉 비용함수가 무엇인지 정의하려고 합니다.

5
00:00:15,420 --> 00:00:19,800
여기 로지스틱 회귀 모델을 적용한 지도 학습 문제가 있습니다.

6
00:00:19,800 --> 00:00:26,310
그리고 m개의 학습 예제로 구성된 트레이닝 집합이 있고

7
00:00:26,310 --> 00:00:31,570
그 학습 예제는 n+1차원으로 표시할 수 있습니다.

8
00:00:32,820 --> 00:00:36,700
그리고 평소처럼 x0=1이고요.

9
00:00:36,700 --> 00:00:40,060
첫 번째 혹은 0번째 변수값은 항상 1 입니다

10
00:00:40,060 --> 00:00:42,860
왜냐하면 이것은 계산 문제에서

11
00:00:42,860 --> 00:00:48,220
우리가 사용할 훈련 집합에 붙어 있는 
모든 라벨 y는 0 혹은 1이기 때문입니다.

12
00:00:48,220 --> 00:00:54,340
이것은 가설이고요, 그 가설식의 매개변수인 θ가 이부분 입니다.

13
00:00:54,340 --> 00:00:58,620
그리고 제가 말하고자 하는 것은 여기에 주어진 트레이닝 집합을 가지고

14
00:00:58,620 --> 00:01:02,480
어떻게 매개변수인 θ값을 선택 또는 찾을 것인가 입니다.

15
00:01:02,480 --> 00:01:06,180
저번에 공부했던 선형 회귀 모델로 돌아가 보지요.

16
00:01:06,180 --> 00:01:08,580
우리는 여기 있는 비용함수를 사용했었는데요.

17
00:01:08,580 --> 00:01:11,940
이 함수를 저번과 조금 다르게 써 보았습니다. 
원래 밖에 있던 1/2m 중에

18
00:01:11,940 --> 00:01:16,410
1/2을 sum 안에 집어넣었습니다.

19
00:01:16,410 --> 00:01:20,190
자 이 비용함수를 또 다른 방식으로 써 볼게요,

20
00:01:20,190 --> 00:01:23,950
여기 제곱 수식 전체 부분을 다 쓰지 말고

21
00:01:23,950 --> 00:01:28,333
이부분을 cost(

22
00:01:28,333 --> 00:01:33,280
hθ(x) , y) 라고 씁시다.

23
00:01:33,280 --> 00:01:39,790
(hθ(x), y)의 총 비용을 정의하면 
이 식이랑 같아집니다.

24
00:01:39,790 --> 00:01:42,710
즉 오차를 제곱한 다음 1/2를 곱한 식과 같게 되죠.

25
00:01:42,710 --> 00:01:49,070
그래서 이제 우리는 비용함수란 훈련집합의 
총합이라는 사실을 명확히 볼 수 있게 되었어요.

26
00:01:49,070 --> 00:01:54,730
즉 이 비용 항을 다 더하고 1/m을 곱한 거지요.

27
00:01:56,050 --> 00:01:59,280
이 식을 좀 더 간단히 해 보면,

28
00:01:59,280 --> 00:02:02,660
여기 위 첨자를 지우면 편하죠.

29
00:02:02,660 --> 00:02:06,150
그래서 그냥 '(hθ(x),y)의 비용은

30
00:02:06,150 --> 00:02:08,360
오차 제곱의 1/2와 같다'라고 정의할 수 있어요.

31
00:02:08,360 --> 00:02:13,110
그리고 이 비용 함수를 해석해 보자면,

32
00:02:13,110 --> 00:02:18,650
예상하는 값이 hθ(x)이고, 실제 라벨이 y일 때 
해당하는 값을 출력할 경우 

33
00:02:18,650 --> 00:02:23,340
학습 알고리즘에게 지불해 주었으면 하는 비용을 뜻합니다.

34
00:02:23,340 --> 00:02:28,110
즉, 단순히 위첨자를 지웠을 뿐입니다.

35
00:02:28,110 --> 00:02:32,010
그리고 선형 회귀의 경우 당연히 정의 된 비용은 저겁니다.

36
00:02:32,010 --> 00:02:36,650
즉 이 비용은 1/2를 곱한 뒤 예측한 값과 

37
00:02:36,650 --> 00:02:40,010
실제 관측된 값의 차를 제곱한 값입니다. 

38
00:02:40,010 --> 00:02:43,900
이 비용 함수는 선형 회귀를 다룰 때에는 문제가 없었어요.

39
00:02:43,900 --> 00:02:47,490
그치만, 우리가 지금 하고 있는 건 로지스틱 회귀입니다.

40
00:02:47,490 --> 00:02:52,040
만약 여기에 J를 대입하고, 이 비용 함수를 최소화할 수 있다면 

41
00:02:52,040 --> 00:02:53,830
그럼 문제가 없을 거예요.

42
00:02:53,830 --> 00:02:57,650
하지만 우리가 여기 이 비용 함수를 사용한다면

43
00:02:57,650 --> 00:03:01,870
이 비용 함수는 매개변수 θ에 대해 
볼록 함수가 아니라고 밝혀졌어요.

44
00:03:01,870 --> 00:03:04,130
볼록 함수가 아니라고 한 게 무슨 뜻이냐면,

45
00:03:04,130 --> 00:03:06,930
우리에게 어떤 비용함수 J(θ)가 있어서

46
00:03:06,930 --> 00:03:10,870
로지스틱 회귀 문제를 다룰 때 이 함수 h가

47
00:03:12,150 --> 00:03:16,570
비선형성을 가져서, 즉, 1/(1+e^(-θT)x)가 됩니다.

48
00:03:16,570 --> 00:03:19,570
즉 이건 꽤 복잡한 비선형 함수입니다.

49
00:03:19,570 --> 00:03:22,800
그리고 이 sigmoid함수를 다시 이 안에 대입하고,

50
00:03:22,800 --> 00:03:26,190
또 이 비용 함수를 여기에 대입한다면

51
00:03:26,190 --> 00:03:28,980
J(θ)는 이런 모양이 됩니다.

52
00:03:28,980 --> 00:03:32,530
여기 보이는 바와 같이 J(θ)는 이렇게

53
00:03:33,710 --> 00:03:35,620
많은 극소점을 가진 함수가 되지요.

54
00:03:35,620 --> 00:03:39,370
이런 함수를 수학적 용어로 비볼록함수라고 합니다.

55
00:03:39,370 --> 00:03:42,950
그리고 이런 비볼록함수의 경우 
경사하강법을 적용해도

56
00:03:42,950 --> 00:03:47,780
전체 함수의 최소값에 도달한다는 보장이 없습니다.

57
00:03:47,780 --> 00:03:52,580
반면에 비용함수 J(θ)가 볼록함수라고 한다면

58
00:03:52,580 --> 00:03:56,490
이 함수는 활 모양으로 한 번만 구부러진 함수이고,

59
00:03:56,490 --> 00:04:00,560
θ에 대해 경사하강법을 적용함으로서 수렴되는 값은

60
00:04:01,610 --> 00:04:04,930
반드시 전체 함수의 최소값이라고 할 수 있습니다.

61
00:04:04,930 --> 00:04:10,210
그러므로 이 제곱된 비용 함수를 
사용함으로 인해 생기는 문제점은

62
00:04:10,210 --> 00:04:15,560
이 가운데 있는 게 비선형적인 함수이기 때문에 J(θ)가

63
00:04:15,560 --> 00:04:21,280
비볼록함수가 된다는 겁니다.

64
00:04:21,280 --> 00:04:25,360
그래서 우리가 하고 싶은 건, 
다른 비용 함수를 제시하는 겁니다.

65
00:04:25,360 --> 00:04:29,850
볼록한 비용 함수를요. 그래서 
경사하강법 같은 좋은 알고리즘을 

66
00:04:29,850 --> 00:04:33,680
적용할 수 있도록 하고, 전체 함수의 
최소값을 찾을 수 있도록 하는 거죠.

67
00:04:33,680 --> 00:04:37,310
이제 로지스틱 회귀에서 사용할 
비용함수를 살펴보겠습니다.

68
00:04:37,310 --> 00:04:39,400
우리가 이야기할 것은 비용, 즉 알고리즘이 지불해야 할 

69
00:04:39,400 --> 00:04:44,420
값에 대한 겁니다. 알고리즘이 hθ(x)값을 넘어섰을 때요.

70
00:04:44,420 --> 00:04:49,570
그래서 예를 들어 이게 0.7같은 숫자라면, 그건 hθ(x)값을 예측하고 있습니다.

71
00:04:49,570 --> 00:04:53,430
그리고 실제 비용을 가리키는 레이블은 y가 됩니다.

72
00:04:53,430 --> 00:04:58,414
이 비용은 y=1일 때 -log(hθ(x))이고요,

73
00:04:58,414 --> 00:05:01,960
y=0일 때 -log(1-hθ(x))입니다.

74
00:05:01,960 --> 00:05:04,380
이거 좀 복잡해 보이죠? 하지만

75
00:05:04,380 --> 00:05:08,220
이 함수를 그리면서 이게 뭘 하는지 직감적으로 알아봅시다.

76
00:05:08,220 --> 00:05:10,801
먼저 y=1일 때 입니다.

77
00:05:10,801 --> 00:05:18,030
만약 y=1이라면, 비용함수는 -log(hθ(x))입니다.

78
00:05:18,030 --> 00:05:22,940
이걸 플롯해보면, 여기 x축을 h(x)라고 합시다.

79
00:05:22,940 --> 00:05:27,678
그러면 우리가 알고 있는 대로 
가설 h(x)는 0 아니면 1이잖아요,

80
00:05:27,678 --> 00:05:31,810
맞죠, 그래서 h(x)는 0에서 1까지만 값을 가집니다.

81
00:05:31,810 --> 00:05:38,000
이 비용함수가 어떻게 생겼는지 플롯해 보면 
이렇게 된다는 걸 알 수 있어요.

82
00:05:38,000 --> 00:05:43,610
왜 이런지 알고 싶으면 log(z)를 플롯해 보세요.

83
00:05:45,040 --> 00:05:48,790
z를 x축으로 잡고요, 그러면 이렇게 되잖아요.

84
00:05:48,790 --> 00:05:50,970
마이너스 무한대로 접근하는 함수, 맞죠?

85
00:05:50,970 --> 00:05:53,770
로그함수는 이렇게 생겼었어요.

86
00:05:53,770 --> 00:05:56,610
여기가 0이고 여기가 1.

87
00:05:56,610 --> 00:06:00,078
여기에서 z는 우리가 하고 있는 h(x)의 역할을 하고,

88
00:06:00,078 --> 00:06:05,090
-log(z)는 이렇게 생겼잖아요.

89
00:06:06,510 --> 00:06:11,460
플러스마이너스를 뒤집으면 이게 
-log(z)예요. 그리고 우리가 알고 싶은 건

90
00:06:11,460 --> 00:06:16,030
이 함수 중에서 0과 1 사이가 어떻게 생겼는지이므로, 나머지를 지웁시다.

91
00:06:16,030 --> 00:06:20,120
그러면 남은 게, 이제 알겠죠. 이 부분의 곡선이에요. 그리고

92
00:06:20,120 --> 00:06:23,210
이게 바로 여기 왼쪽의 곡선입니다.

93
00:06:23,210 --> 00:06:29,740
이제 이 비용함수는 몇 가지 재미있고 바람직한 속성을 가지고 있어요.

94
00:06:29,740 --> 00:06:34,870
첫째로, 잘 보면 y=1인 경우 h(x)=1이 됩니다.

95
00:06:34,870 --> 00:06:39,870
다시 말하면, 가설을 정확하계 예측한 경우 h=1이고

96
00:06:39,870 --> 00:06:44,410
y가 가설이 예측한 값과 맞아떨어진다면 
비용은 0이 됩니다. 맞죠?

97
00:06:44,410 --> 00:06:47,380
이건 곡선이 평평해지지 않는다는 사실과 부합합니다.

98
00:06:47,380 --> 00:06:48,970
커브는 계속 꺾어지게 됩니다.

99
00:06:48,970 --> 00:06:53,534
다시 해 보죠, h(x)=1일 경우에 주목한다면 이 가설은

100
00:06:53,534 --> 00:06:58,400
y=1일 것을 예측하고, 실제로 y=1이라면 비용은 0이 됩니다.

101
00:06:58,400 --> 00:07:01,370
그건 바로 여기 이 점에 해당합니다. 맞죠?

102
00:07:01,370 --> 00:07:05,666
만약 h(x)=1이라면 우리가 생각할 수 있는 
y=1인 경우는 여기 한군데입니다.

103
00:07:05,666 --> 00:07:11,340
그런데 만약에 h(x)=1이고 비용이 여기라면 이건 0가 됩니다.

104
00:07:11,340 --> 00:07:15,520
그리고 이건 우리가 원하던 건데요, 
왜냐하면 우리가 결과값 y를 정확히 예측했다면

105
00:07:15,520 --> 00:07:17,750
비용은 0이기 때문이죠.

106
00:07:17,750 --> 00:07:24,420
하지만 여기서 하나 더 알게 된 점은, 
여기 h(x)가 0에 가까워지면

107
00:07:24,420 --> 00:07:30,500
그래서 가설의 출력값이 0에 가까워지면, 
비용은 증대하고 무한대로 가 버린다는 겁니다.

108
00:07:30,500 --> 00:07:35,710
그리고 이게 무슨 역할을 하냐면, 가설 hθ(x)=0,

109
00:07:35,710 --> 00:07:41,512
즉 y=1일 확률이 0일 때를 직관적으로 포착하게 해 줍니다.

110
00:07:41,512 --> 00:07:44,440
예를 들어 당신은 의사이고 환자에게 이렇게 말했습니다.

111
00:07:44,440 --> 00:07:49,200
"당신이 악성 종양을 가지고 있을 확률, 즉 y=1일 확률은 0입니다."

112
00:07:49,200 --> 00:07:54,120
"그래서 당신의 종양이 악성이라는 건 절대적으로 불가능해요."

113
00:07:55,190 --> 00:08:00,190
하지만 이런 경우도 있죠. 검사를 해 보니 그 종양, 그 환자의 종양이 악성이었습니다.

114
00:08:00,190 --> 00:08:03,370
그래서 만약 y=1이라면, 벌써 환자에게 말했는데,

115
00:08:03,370 --> 00:08:05,410
'악성일 확률은 0이에요' 하고.

116
00:08:05,410 --> 00:08:08,970
'그래서 악성 종양이라는 건 불가능해요' 하고 말했는데,

117
00:08:08,970 --> 00:08:13,000
만약 우리가 환자에게 그렇게 확신을 가지고 말했는데 결국엔 틀렸다면

118
00:08:13,000 --> 00:08:16,160
이 학습 알고리즘을 처벌해야 하죠. 엄청난 비용을 부과시켜야 해요.

119
00:08:16,160 --> 00:08:20,290
그게 여기에 포착된 비용의 값입니다.

120
00:08:20,290 --> 00:08:24,870
만약에라도 y=1이라는 값이 나오면 
비용은 무한대이고 h(x)=0이 됩니다.

121
00:08:24,870 --> 00:08:28,190
이 슬라이드는 y=1일 경우를 설명하고 있는데요,

122
00:08:28,190 --> 00:08:31,205
이제 y=1일 경우에 비용함수가 
어떻게 생겼는지 살펴 봅시다.

123
00:08:32,460 --> 00:08:35,810
만약 y=0라면 비용은 다음과 같습니다.

124
00:08:35,810 --> 00:08:41,045
여기에 있는 표현과 같은데요, 이 함수를 플롯해 보면

125
00:08:41,045 --> 00:08:49,240
-log(1-z), 당신이 얻는 비용함수는 
실제로 이렇게 생겼습니다.

126
00:08:49,240 --> 00:08:54,630
그래서 0부터 1까지만 그려보면, 이렇게 되죠. 그래서

127
00:08:54,630 --> 00:08:59,740
y=0일 때 비용함수를 플롯하면 
이런 그래프가 됩니다.

128
00:08:59,740 --> 00:09:04,940
그리고 이 곡선이 하는 건, 이제 위로 올라가서

129
00:09:04,940 --> 00:09:09,660
h(x)가 1일 때 플러스 무한대까지 가요. 
왜냐하면 제가 말했던 것처럼

130
00:09:09,660 --> 00:09:11,940
y는 결국에 0일텐데

131
00:09:11,940 --> 00:09:16,670
우리가 y=1이라고 예측을 한다면, 
거의 확실히 1이예요 하고 말해 버리면

132
00:09:16,670 --> 00:09:19,080
우리는 엄청난 비용을 지불해야 할 겁니다.

133
00:09:21,590 --> 00:09:25,084
만약 각각의 x가 0이고 y=0라면

134
00:09:25,084 --> 00:09:29,430
우리의 가설이 사라집니다. 가설에 의해 보호받았던 y(z)는 0이 되고요. 

135
00:09:29,430 --> 00:09:34,630
그리고 결론적으로 y=0이 되어요. 그래서 여기서

136
00:09:34,630 --> 00:09:39,880
비용함수는 0이 될 겁니다. 이번 수업에서

137
00:09:39,880 --> 00:09:45,800
우리는 한 가지 훈련 예제에서 비용함수를 
정의했습니다. 함수가 볼록인지 비볼록인지에

138
00:09:45,800 --> 00:09:48,240
관한 해석은 이 강의의 범위를 넘어서는 거지만

139
00:09:48,240 --> 00:09:52,140
비용함수를 어떻게 선택해야 하는지와 
관련해서 조금 보여드렸습니다.

140
00:09:53,380 --> 00:09:55,060
이걸로 볼록함수의 최적화 문제를 다룰 수 있게 되었지요.

141
00:09:55,060 --> 00:09:59,110
모든 비용함수 J(θ)는

142
00:09:59,110 --> 00:10:04,500
볼록함수이며 극소점 문제에 대해 생각할 
필요가 없습니다. 다음 강의에서는

143
00:10:04,500 --> 00:10:09,250
우리가 이번에 했던 한 개의 훈련 
예제에 대한 비용함수에서 더 나아가

144
00:10:09,250 --> 00:10:14,510
전체 훈련 예제에 대한 비용함수를 
정의할 겁니다. 그리고 또

145
00:10:14,510 --> 00:10:18,740
지금까지 사용해 왔던 비용함수의 식을 더 
간단히 나타낼 방법에 대해 알아보겠습니다.

146
00:10:18,740 --> 00:10:21,308
새로운 식에 기초해서 경사하강법을 적용하고, 그러면

147
00:10:21,308 --> 00:10:24,173
로지스틱 회귀 알고리즘에 대해 알 수 있을 겁니다.

148
00:10:24,173 --> 00:10:28,990
영어자막에 오류가 있습니다

149
00:10:28,990 --> 00:10:34,380
영어자막에 오류가 있습니다

150
00:10:34,380 --> 00:10:39,105
영어자막에 오류가 있습니다

151
00:10:41,150 --> 00:10:46,350
영어자막에 오류가 있습니다

152
00:10:46,350 --> 00:10:50,510
영어자막에 오류가 있습니다

153
00:10:50,510 --> 00:10:54,440
영어자막에 오류가 있습니다

154
00:10:54,440 --> 00:10:57,090
영어자막에 오류가 있습니다

155
00:10:58,880 --> 00:11:03,410
영어자막에 오류가 있습니다

156
00:11:04,420 --> 00:11:08,545
영어자막에 오류가 있습니다

157
00:11:08,545 --> 00:11:13,325
영어자막에 오류가 있습니다

158
00:11:13,325 --> 00:11:15,085
영어자막에 오류가 있습니다

159
00:11:15,085 --> 00:11:18,945
영어자막에 오류가 있습니다

160
00:11:18,945 --> 00:11:22,785
영어자막에 오류가 있습니다

161
00:11:22,785 --> 00:11:25,025
영어자막에 오류가 있습니다