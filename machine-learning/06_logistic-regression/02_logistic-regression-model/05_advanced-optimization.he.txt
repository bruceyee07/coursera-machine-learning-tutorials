בהרצאה הקודמת דיברנו על ירידה במדרון למזעור פונקציית העלות J של תטא עבור רגרסיה לוגיסטית. בהרצאה הזו, אני רוצה לספר לכם על כמה אלגוריתמים מתקדמים לאופטימיזציה ועל כמה מושגים מתקדמים לאופטימיזציה. באמצעות כמה מהרעיונות האלה, נוכל לגרום לרגרסיה לוגיסטית לרוץ הרבה יותר מהר ממה שזה אפשרי עם ירידה במדרון. וזה גם יאפשר לאלגוריתם לעבוד בקנה מידה גדול הרבה יותר ולטפל בבעיות למידה חישובית גדולות, לדוגמא עם מספר גדול מאוד של תכונות או מאפיינים. הנה תצוגה אלטרנטיבית של פעולת הירידה במדרון. יש לנו איזו פונקצית עלות J ואנחנו רוצים למזער אותה. אז מה שאנחנו צריכים הוא, אנחנו צריכים לכתוב קוד שמקבל כקלט את הפרמטרים תטא ומחשב שני דברים: J של תטא ואת הנגזרות החלקיות, אתה יודע, של J שווה 0, 1 עד n. בהינתן קוד שיכול לעשות את שני אלה, מה שעושה הירידה במדרון הוא שהיא מריצה את זה שוב ושוב ומבצעת את השלב הבא. נכון? אז בהינתן הקוד שכתבנו כדי לחשב את הנגזרות החלקיות האלה, הירידה במדרון מציבה כאן את הקוד ומשתמשת בזה כדי לעדכן את סט הפרמטרים שלנו תטא. אז דרך נוספת לחשוב על ירידה במדרון היא שאנחנו צריכים לספק קוד כדי לחשב את J של תטא ואת הנגזרות האלה, ואז מציבים אותם בירידה במדרון, שיכולה אז לנסות למזער את הפונקציה עבורנו. עבור ירידה במדרון, אני מניח שמבחינה טכנית לא באמת צריך קוד כדי לחשב את פונקציית העלות J של תטא. צריכים רק קוד לחשב את הנגזרות החלקיות. אבל אם תחשבו על הקוד כקוד שצריך גם לנטר את ההתכנסות של האלגוריתם, אז פשוט נחשוב שבעצם אנחנו אמורים לספק קוד לחשב הן את פונקציית העלות והן את הנגזרות. אז לאחר כתיבת קוד לחשב את שני הדברים האלה, אלגוריתם אחד בו אפשר להשתמש הוא ירידה במדרון. אבל ירידה במדרון אינה האלגוריתם היחיד שאנו יכולים להשתמש בו. ויש אלגוריתמים אחרים, מתקדמים יותר, מתוחכמים יותר, שאם רק נותנים להם דרך לחשב את שני הדברים האלה, אז אלה הן גישות שונות כדי למצוא את פונקציית העלות האופטימלית עבורנו. "מדרון מוטה" או "מדרון מכוון" (בו הנגזרת שבה מטפלים גורמת להטית שאר האלמנטים), BFGS וגם L-BFGS, הם דוגמאות של אלגוריתמים יותר מתוחכמים לאופטימיזציה שצריכים דרך לחשב את J של תטא ואת הנגזרות, ויכולים להשתמש באסטרטגיות מתוחכמות יותר מאשר ירידה במדרון כדי למזער את פונקצית העלות. הפרטים המדויקים של שלושת האלגוריתמים האלה הם הרבה מעבר להיקף של קורס זה. ולמעשה אנשים מבלים ימים רבים, או מספר שבועות בלימוד האלגוריתמים האלה כאשר לוקחים קורס באנליזה נומרית מתקדמת. אבל הרשו לי לספר לך קצת על כמה מהתכונות שלהם. לשלושת האלגוריתמים האלה יש מספר יתרונות. אחד היתרונות הוא שבכל אחד מהם אין צורך לבחור ידנית את שיעור הלמידה אלפא. אז אחת הדרכים לחשוב על האלגוריתמים האלה היא שבהינתן דרך לחשב את הנגזרת ואת פונקציה העלות אתה יכול לחשוב על האלגוריתמים כאילו יש להם לולאה פנימית חכמה. ולמעשה, יש להם לולאה פנימית חכמה הנקראת אלגוריתם חיפוש קו שמנסה באופן אוטומטי ערכים שונים עבור שיעור הלימוד אלפא ובוחרת באופן אוטומטי שיעור למידה אלפא טוב, והם אפילו יכולים לבחור שיעור למידה שונה עבור כל איטרציה. אז אתה לא צריך לבחור את זה בעצמך. האלגוריתמים האלה למעשה עושים דברים מתוחכמים יותר מאשר פשוט לבחור שיעור למידה טוב, ולכן הם מצליחים להתכנס הרבה יותר מהר מאשר ירידה במדרון. האלגוריתמים האלה למעשה עושים עוד דברים מתוחכמים חוץ מאשר רק לבחור שיעור למידה טוב, ולכן הם בסופו של דבר מתכנסים הרבה יותר מהר מהירידה במדרון, אבל דיון מפורט על כל מה שהם בדיוק עושים הוא מעבר להיקף של הקורס הזה. למעשה, אני השתמשתי באלגוריתמים האלה במשך זמן רב, אולי עשר שנים, לעתים קרובות למדי, וזה היה רק לפני כמה שנים, שאני באמת הבנתי בעצמי את הפרטים של מה עושים האלגוריתמים האלה, המדרון המוטה, BFGS ו- L-BFGS. אז זה אפשרי לחלוטין להשתמש באלגוריתמים האלה בהצלחה וליישם אותם על הרבה בעיות למידה שונות מבלי להבין למעשה את הלולאה הפנימית שהאלגוריתמים האלה מבצעים. אם יש לאלגוריתמים האלה חסרון, הייתי אומר שהחיסרון העיקרי הוא שהם הרבה יותר מסובכים מאשר ירידה במדרון. ולמעשה, כנראה לא כדאי לך ליישם את האלגוריתמים האלה - המדרון המוטה, BGFS ו- L-BFGS - בעצמך, אלא אם כן אתה מומחה בתחום החישוב הנומרי. במקום זאת, בדיוק כפי שלא הייתי ממליץ לכם לכתוב בעצמכם קוד לחשב שורשים ריבועיים של מספרים או לחשב את ההופכי של מטריצות, גם לגבי האלגוריתמים האלה מה שהייתי ממליץ לכם לעשות הוא פשוט להשתמש בספריית תוכנה. כמו שכדי להוציא שורש מרובע מה שכולנו עושים הוא להשתמש בפונקציה שמישהו אחר כתב כדי לחשב שורשים ריבועיים של מספרים. ולמרבה המזל, באוקטבה ובשפה הקשורה קשר הדוק MATLAB - ונשתמש גם בה - באוקטבה יש ספריה טובה מאד, ספרייה סבירה למדי ליישום של האלגוריתמים המתקדמים האלה לאופטימיזציה. אז אם פשוט תשתמשו בספרייה הקיימת, תקבלו תוצאות די טובות. אני חייב לומר שיש הבדל בין יישומים טובים ופחות טובים של האלגוריתמים האלה. אז אם אתם משתמשים בשפה אחרת עבור יישום הלמידה החישובית שלכם, אם אתם משתמשים ב- C, C++, Java, וכן הלאה, מומלץ לנסות כמה ספריות שונות כדי לוודא שאתם מוצאים ספריה טובה ליישום האלגוריתמים הללו. כי יש הבדל בביצועים בין יישום טוב של, לדוגמא, מדרון מוטה או L-PFGS לעומת יישום טוב פחות של מדרון מוטה או L-PFGS. אז עכשיו בואו נבין איך להשתמש באלגוריתמים האלה, ואני הולך לעשות זאת עם דוגמה. נניח שיש לך בעיה עם שני פרמטרים, תטא-1 ותטא-2. ונניח שפונקצית העלות שלך היא J של תטא שווה תטא-1 מינוס 5 בריבוע ועוד תטא-2 מינוס 5 בריבוע. אז עם פונקצית העלות הזו, הערך עבור תטא-1 ותטא-2, אם אתה רוצה למזער את J של תטא כפונקציה של תטא. הערך שממזער את זה יהיה תטא-1 שווה 5, תטא-2 שווה 5. עכשיו, שוב, אני יודע שחלק מכם יודעים יותר חשבון דיפרנציאלי מאחרים, אבל הנגזרות של פונקציית העלות J מתברר שהם שני הביטויים האלה. עשיתי את החישוב. אז אם אתה רוצה ליישם את אחד האלגוריתמים המתקדמים לאופטימיזציה כדי למזער את פונקצית העלות J. אז לולא ידענו שהמינימום הוא ב-5,5, אבל אם אתה רוצה שפונקציה העלות תמצא את המינימום בצורה נומרית באמצעות משהו כמו ירידה במדרון אבל עדיף משהו יותר מתקדם מאשר ירידה במדרון, מה שהיית עושה הוא ליישם פונקציה כזו באוקטבה, אז ניישם פונקציית עלות, פונקציית עלות שהיא פונקציה של תטא כמו זו, ומה שזה עושה הוא מחזיר שני איברים, הראשון, jVal, מגדיר איך אנחנו מחשבים את פונקציית העלות J. אז זה פשוט נותן כאן את הפונקציה jVal שהיא כמו שאמרנו תטא-1 מינוס 5 בריבוע ועוד תטא-2 מינוס 5 בריבוע. אז זו פשוט ההגדרה של פונקצית המחיר שלנו. והארגומנט השני שהפונקציה מחזירה הוא הגרדיינט. והגרדיינט הוא וקטור של 2 על 1, ושני האיברים בוקטור הגרדיינט מתאימים לשתי הנגזרות החלקיות כאן. אחרי שמממשים את פונקצית העלות הזו, אפשר אז לקרוא לפונקצית האופטימיזציה המתקדמת שנקראת fminunc - שפרושו פונקצית (f) מינימום (min) בלתי מאולצת (unc) באוקטבה, והדרך לקרוא לפונקציה היא זו. אתה קובע כמה אופציות. הפרמטר הזה, אופציות, הוא מבנה נתונים בו מאחסנים את האופציות הרצויות. הפרמטר הראשון, GradObj, עם ערך "on" מדליק את האופציה שנקראת "מטרת הגרדיינט" למצב "דלוק". משמעו של הפרמטר הוא שאנו נספק וקטור גרדיינט לפונקציה. הפרמטר השני מחליט לעצור את הריצה אחרי לכל היותר 100 איטרציות. הפונקציה גם מקבלת ערך או ניחוש ראשוני עבור תטא, שהוא וקטור של 2 על 1 והפקודה הזו קוראת לפונקצית ה-fminunc. הסימן @ מסמן מצביע לפונקצית המחיר שהגדרנו למעלה. וכשקוראים לפונקציה הזו היא תחשב, תשתמש באחד מאלגוריתמימי האופטימיזציה המתקדמים יותר. ואם אתם רוצים, חשבו על זה בדיוק כמו ירידה במדרון. אבל כזו שבוחרת באופן אוטומטי את קצב הלמידה אלפא כך שלא תצטרכו לעשות זאת בעצמכם. אלא היא תנסה להשתמש בסוג של אלגוריתמי אופטימיזציה מתקדמים. זה ירוץ כמו ירידה במדרון על סטרואידים, כדי לנסות למצוא את הערך האופטימלי של תטא. הרשו לי להראות לכם איך זה נראה באוקטבה. אז כתבתי את פונקציית העלות של תטא בדיוק כמו שראינו מקודם. היא מחשבת את jVal שהיא פונקציית העלות. והיא מחשבת את הגרדיינט עם שני האיברים שהם הנגזרות החלקיות של פונקציית העלות ביחס לשני הפרמטרים, תטא-1 ותטא-2. עכשיו בואו נעבור לחלון אוקטבה שלי. אני הולך להקליד את הפקודות שבדיוק דיברנו עליהן. options = optimset.... זו הגדרת האופציות עם אותם ערכים כמו שדיברנו לאלגוריתם. GradObj הוא on, ונגביל את maxIter ל-100 דהיינו עד 100 איטרציות, ואני מספק את הגרדיינט לאלגוריתם. בואו נציב בתטא הראשוני שלנו 0 בתטא-1 ובתטא-2. זה ה"ניחוש" הראשוני שלנו עבור תטא. ועכשיו אני קורא לפונקציה שמחזירה וקטור של תטא אופטימלי, ערך הפונקציה, קוד יציאה קורא לפונקציה fminunc עם מצביע לפונקצית המחיר, שולח את הניחוש הראשוני ואת האופציות. וכשאני מקיש <Enter> זה מריץ את הפונקציה. והיא חוזרת די מהר. הקשקוש על הפרומפט שלי הוא בגלל איזו גלישה של הפקודה שהיקשתי, פשוט היתה גלישה ושורת הפקודה התחרבשה קצת. אבל מה שזה אומר הוא שהריצה של הפונקציה, תחשבו עליה כעל ירידה במדרון על סטרואידים, מצאה את הערך האופטימלי של תטא והוא תטא-1 שווה 5, תטה-2 שווה 5, בדיוק כפי שקיוינו שתמצא. ערך הפונקציה באופטימום חושב כ-10 בחזקת מינוס 30. שזה בעצם אפס, וזה גם מה שקיוינו. וקוד היציאה הוא 1, והוא מסמן מה מצב ההתכנסות של הפונקציה. ואם אתם רוצים תריצו את הפקודה help fminunc ותקראו את התיעוד של כיצד לפרש את קוד היציאה. אבל קוד היציאה מאפשר לך לברר האם האלגוריתם התכנס או לא. אז ככה מפעילים את האלגוריתמים האלה באוקטבה. אני צריך לציין, דרך אגב, כי עבור היישום באוקטבה, הערך הזה של תטה, הפרמטר שהוא הוקטור תטא, חייב להיות במרחב R עם ממד גדול או שווה 2. אז אם תטא הוא פשוט מספר ממשי. אם הוא לא לפחות וקטור דו מימדי או בעל ממד גבוה יותר מ-2, פונקצית fminunc לא תוכל לעבוד, אז אם במקרה יש לך פונקציה חד מימדית שאתה צריך לייעל - לעשות לה אופטימיזציה, תראה את התיעוד באוקטבה של הפונקציה fminunc לפרטים נוספים. אז כך אנחנו מייעלים את הדוגמה הקטנה של פונקצית העלות הריבועית הפשוטה שלנו. איך אנחנו יכולים ליישם את זה לרגרסיה לוגיסטית? ברגרסיה לוגיסטית יש לנו וקטור פרמטרים תטא, ואני עומד להשתמש בתערובת של סימונים מאוקטבה וממתמטיקה. אבל אני מקווה שההסבר הזה יהיה ברור, אבל וקטור הפרמטרים תטא כולל את הפרמטרים תטא-0 עד תטא-n, ובגלל האינדקסים של אוקטבה, בה וקטורים מתחילים באינדקס 1, תטא-0 למעשה נכתב תטא-1 באוקטבה, תטא-1 נכתב כתטא-2 ותטא-n באוקטבה נכתב כתטא-n+1, נכון? וזאת מאחר שהאינדקסים באוקטבה בוקטורים מתחילים מאינדקס 1 ולא מאינדקס 0. אז מה שאנחנו צריכים לעשות הוא לכתוב פונקצית עלות שמממשת את פונקצית העלות עבור רגרסיה לוגיסטית. באופן קונקרטי, פונקציית העלות צריכה להחזיר jVal, שהוא, כידוע, jVal כפי שצריך לקודד אותו כדי לחשב את J של תטא ואנחנו גם צריכים לתת לה את הגרדיינט. אז, גרדיינט 1 יהיה קוד לחישוב הנגזרת החלקית ביחס לתטא-0, הנגזרת החלקית הבאה עבור תטא-1 וכן הלאה. שוב, זה גרדיינט 0, גרדיינט 1 ,גרדיינט 2 וכן הלאה, ולא גרדיינט 0, גרדיינט 1 כי באוקטבה האינדקסים של וקטורים מתחילים מ-1 ולא מ-0. אבל הרעיון המרכזי שאני מקווה שאתם מבינים מהשקופית הזו הוא, כי מה שעליכם לעשות, הוא לכתוב פונקציה המחזירה את העלות ואת הגרדיינט. אז כדי ליישם את זה ברגרסיה לוגיסטית או אפילו ברגרסיה ליניארית, אם תרצו להשתמש באלגוריתמי האופטימיזציה האלה עבור רגרסיה ליניארית. מה שצריך לעשות הוא לתקוע כאן את הקוד המתאים כדי לחשב את הדברים האלה שכאן. אז עכשיו אתם יודעים איך להשתמש באלגוריתמי האופטימיזציה המתקדמים האלה. מאחר ובאלגוריתמים האלה משתמשים בספריית אופטימיזציה מתוחכמת, זה עושה את הקוד קצת פחות שקוף ולכן קצת יותר קשה לנפות (לדבג) אותו. אבל בגלל שהאלגוריתמים האלה לעתים קרובות רצים הרבה יותר מהר מאשר הירידה במדרון, לעתים קרובות למדי, או בכל פעם שיש לי בעיה גדולה של למידה חישובית, אני אשתמש באלגוריתמים האלה במקום להשתמש בירידה במדרון. ועם הרעיונות האלה, אני מקווה, תוכל לגרום לרגרסיה לוגיסטית וגם לרגרסיה ליניארית להצליח לפעול בבעיות הרבה יותר גדולות. אז, זהו זה בקשר למושגים של אופטימיזציה מתקדמת. בסרטון הבא והאחרון על רגרסיה לוגיסטית, אני רוצה לתאר איך לוקחים את אלגוריתם הרגרסיה הלוגיסטית שאתם כבר מכירים, ולגרום לו לעבוד גם על בעיות סיווג רבות מחלקות.