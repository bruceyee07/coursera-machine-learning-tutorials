בסרטון האחרון, דיברנו על דיוק וקליעה כמדדי הערכה לבעיות סיווג עם מחלקות מוטות. ביישומים רבים, אנחנו רוצים איכשהו לשלוט על שיווי המשקל בין דיוק לקליעה. תנו לי לספר לכם איך לעשות את זה וגם להראות לכם כמה דרכים אפילו יותר יעילות להשתמש בדיוק וקליעה כמדדי הערכה עבור אלגוריתמי למידה. כתזכורת, להלן ההגדרות של דיוק וקליעה מהסרטון הקודם. הבה נמשיך בדוגמת סיווג הסרטן שלנו, כאשר y שווה 1 אם לחולה יש סרטן ו-y שווה 0 אחרת. ונניח שאנחנו מאמנים פונקציית סיווג ברגרסיה לוגיסטית, שמייצרת הסתברות בין 0 ל-1. וכרגיל, אנחנו הולכים לחזות 1, דהיינו y שווה 1, אם (h(x גדול או שווה ל-0.5. ולנבא 0 אם ההשערה מוציאה ערך קטן מ-0.5. והמסווג הזה עשוי לתת לנו ערך כלשהו עבור הדיוק וערך כלשהו עבור הקליעה. אבל עכשיו, נניח שאנחנו רוצים לחזות שלחולה יש סרטן רק אם אנחנו מאוד בטוחים שבאמת יש לו. כי אם אתה הולך לחולה ואומר לו שיש לו סרטן, זה הולך לתת לו הלם רציני. מה שאנחנו נותנים הוא חדשות רעות מאוד, והוא עשוי להצטרך לעבור תהליך טיפול די כואב וכו'. אז אולי אנחנו רוצים לספר למישהו שאנחנו חושבים שיש לו סרטן רק אם אנחנו בטוחים מאוד. אחת הדרכים לעשות את זה היא לשנות את האלגוריתם, כך שבמקום להגדיר את הסף הזה ב-0.5, אנו עשויים להחליט שאנחנו ננבא ש-y שווה ל-1 רק אם (h(x גדול או שווה ל-0.7. זה כמו להגיד, נאמר למישהו שיש לו סרטן רק אם אנחנו חושבים שיש סיכוי גדול או שווה ל-70% שיש לו סרטן. ואם נעשה את זה, אז אנחנו חוזים שלמישהו יש סרטן רק כאשר אנחנו יותר בטוחים ולכן מה שיוצא לנו זה מסווג עם דיוק גבוה יותר. כי כל החולים שאתה הולך ואומר להם, אנחנו חושבים שיש לך סרטן, אלה עכשיו חולים שאתה די בטוח שיש להם סרטן. אז חלק גדול יותר מהחולים שאתם צופים שיש להם סרטן, יתגלה למעשה שאכן יש להם סרטן,
 מכיוון שאנחנו עושים תחזית כזו רק אם אנחנו די בטוחים. אבל לעומת זאת למסווג הזה יהיה מדד קליעה נמוך, כי עכשיו כשאנחנו עושים תחזיות, אנחנו הולכים לחזות y=1 על מספר קטן יותר של חולים. עכשיו, אפשר לקחת את זה אפילו עוד יותר רחוק. במקום לקבוע את הסף ב-0.7, אנחנו יכולים להגדיר אותו ב-0.9. ועכשיו נחזה y=1 רק אם אנחנו בטוחים ביותר מ-90% שהחולה סובל מסרטן. אז חלק גדול מאד מהחולים הללו יתברר שיש להם סרטן. אז זה יהיה מסווג עם מדד דיוק גבוה יותר ומדד קליעה נמוך, כי אנחנו לא נזהה כראוי את כל החולים שיש להם סרטן. עכשיו בואו נשקול דוגמה הפוכה. נניח שאנחנו רוצים להימנע מפספוסים רבים מדי של סרטן, אנחנו רוצים להימנע משליליים כוזבים. כי אם לחולה יש סרטן, אבל אנחנו לא מספרים להם שיש להם סרטן אז זה יכול להיות ממש רע. כי אם אנחנו אומרים לחולה שאין לו סרטן, אז הוא לא הולך לטיפול. ואם יתברר שיש לו סרטן, אבל אנחנו לא הצלחנו להגיד לו שיש לו סרטן, אז הוא לא יקבל טיפול בכלל. אז זו תהיה תוצאה ממש רעה כי הוא ימות כי אמרנו לו שאין לו סרטן. הוא לא ילך לקבל טיפול, אבל מתברר שיש לו סרטן. אז נניח שכאשר יש ספק, אנחנו רוצים לנבא כי y=1. כאשר יש ספק, אנחנו רוצים לחזות שיש להם סרטן כך שלפחות הם ימשיכו לבדוק את זה, ואז הם יוכלו לקבל טיפול אם מתגלה שיש להם סרטן. במקרה כזה, במקום להגדיר סף הסתברות גבוה יותר, אנו עשויים במקום זאת לקחת ערך אחר, במקום 0.5 לקחת דווקא ערך נמוך יותר. אולי 0.3 ככה, כן? ובכך אנחנו אומרים, אתה יודע מה, אם אנחנו חושבים שיש יותר מ-30% סיכוי שיש לך סרטן אנחנו נהיה שמרנים יותר ונאמר לך שאתה עלול לסבול מסרטן, כך שתוכל ללכת ולקבל טיפול במידת הצורך . ומה שיהיה לנו במקרה הזה יהיה מסווג עם מדד קליעה גבוה יותר, כי אנחנו נסמן כראוי חלק גדול יותר מכל החולים שלמעשה יש להם סרטן. אבל יהיה לנו מדד דיוק נמוך יותר, כי חלק גדול יותר מהחולים שלהם אמרנו שיש להם סרטן, חלק גדול מהם יתברר שבסופו של דבר אין להם סרטן. ודרך אגב, כשאני מדבר על זה עם תלמידים אחרים, אמרו לי בעבר שזה די מדהים, כמה מהתלמידים שלי אמרו שזה מדהים איך שאני יכול לספר את הסיפור משני הכיוונים. למה אנחנו אולי רוצים לקבל דיוק גבוה יותר ולמה נרצה קליעה גבוהה יותר, ונראה שהסיפור עובד בשני הכיוונים. אבל אני מקווה שהפרטים של האלגוריתם נכונים והעיקרון הכללי יותר תלוי בשיווי המשקל הרצוי, בין אם אתה רוצה דיוק גבוה יותר - וקליעה נמוכה יותר, או קליעה גבוהה יותר - דיוק נמוך יותר. ואפשר להחליט לחזות ש-y=1 כאשר (h(x גדול יותר מסף כלשהו. אז באופן כללי, עבור רוב המסווגים יש יחסי גומלין בין דיוק לקליעה, וכאשר אתה משנה את הערך של הסף הזה שמצויר כאן, אתה יכול למעשה לשרטט איזושהי עקומה שמראה את היחס בין הדיוק ובין הקליעה. כאשר ערך כאן למעלה תואם לערך גבוה מאוד של הסף, לדוגמה סף שווה 0.99. מה שזה אומר, לחזות y=1 רק אם אנחנו בטוחים ביותר מ-99%, לפחות 99% הסתברות שזה אחד. אז לזה יהיה דיוק גבוה, קליעה נמוכה יחסית. ולעומת זאת נקודה כאן למטה תתאים לערך של סף שהוא הרבה יותר נמוך, אולי שווה 0.01, כלומר, כאשר בכלל יש איזה שהוא ספק, לחזות y=1, וכשעושים את זה כך, יוצא לנו דיוק הרבה יותר נמוך , ומדד קליעה גבוה יותר. וכאשר אתה משחק עם הסף, אם אתה רוצה אתה יכול באמת לעקוב אחר העקומה עבור המסווג שלך ולראות את מגוון הערכים השונים שאתה יכול לקבל עבור דיוק ועבור קליעה. ודרך אגב, עקומת הדיוק\קליעה יכולה לקבל צורות רבות ושונות. לפעמים זה ייראה כך, לפעמים זה ייראה כך. יש צורות רבות ושונות עבור עקומת הדיוק והקליעה, בהתאם לפרטי המסווג. אז זה מעלה שאלה מעניינת נוספת שהיא, האם יש דרך לבחור את הסף באופן אוטומטי? או באופן כללי יותר, אם יש לנו כמה אלגוריתמים שונים או כמה רעיונות שונים לאלגוריתמים, כיצד ניתן להשוות מספרים שונים של דיוק וקליעה? באופן קונקרטי, נניח שיש לנו שלושה אלגוריתמי למידה שונים. אז למעשה, אולי אלה שלושה אלגוריתמי למידה שונים, או אולי הם אותו אלגוריתם אבל עם ערכים שונים עבור הסף. כיצד מחליטים איזה מהאלגוריתמים האלה הוא הטוב ביותר? אחד הדברים שדיברנו עליהם הוא החשיבות של מדד יחיד להערכה, מספר ממשי. וזה הרעיון של מספר שאומר לך בדיוק כמה מצליח המסווג שלך. אבל על ידי מעבר למדד הדיוק והקליעה איבדנו את זה. עכשיו יש לנו שני מספרים ממשיים וכך קורה לנו שאנחנו מגיעים למצבים כמו כשאנחנו מנסים להשוות אלגוריתם 1 ואלגוריתם 2, ואנחנו שואלים את עצמנו, האם דיוק של 0.5 וקליעה של 0.4, טוב יותר או רע יותר מאשר דיוק 0.7 וקליעה 0.1? ואם בכל פעם שאתה מנסה אלגוריתם חדש אתה צריך לשבת ולחשוב, אולי 0.5/0.4 זה יותר טוב מאשר 0.7/0.1, או לא, אני לא יודע. אם בסופו של דבר אתה תמיד צריך לשבת ולחשוב ולעשות את ההחלטות האלה, זה מאט בהרבה את תהליך קבלת ההחלטות שלך עבור אילו שינויים הם שימושיים וכדאי להכניס אותם לאלגוריתם שלך. ובניגוד לכך, אם יש לנו ערך בודד להערכה, מספר ממשי אחד שאומר לנו פשוט אלגוריתם 1 טוב יותר או אלגוריתם 2 טוב יותר, אז זה עוזר לנו להחליט במהירות רבה יותר עם איזה אלגוריתם להמשיך. זה עוזר לנו גם להעריך הרבה יותר מהר שינויים שונים שאנו עשויים לחשוב עליהם באלגוריתם. אז איך אנחנו יכולים לקבל מדד שהוא ערך ממשי בודד? דבר אחד טבעי שאפשר לנסות הוא להסתכל על הממוצע בין דיוק וקליעה. אם נסמן דיוק ב-P וקליעה ב-R, מה שאפשר לעשות הוא פשוט לחשב את הממוצע ולבחור את המסווג שיש לו הערך הממוצע הגבוה ביותר. אבל מסתבר שזה לא כזה פתרון טוב, כי כמו בדוגמה שהיתה לנו קודם, מתברר שאם יש לנו מסווג שמנבא y=1 כל הזמן, אז אם עושים את הממוצע אפשר לקבל מדד קליעה גבוה מאוד, אבל ערך נמוך מאוד של דיוק. ולעומת זאת, אם יש לך מסווג שמנבא y שווה אפס, כמעט כל הזמן, שמנבא y=1 במקרים מאוד נדירים, מה שמתאים להגדרת סף גבוה מאוד של תוצאת החישוב, אז נוכל לקבל דיוק גבוה מאוד וקליעה נמוכה מאוד. אז שני הקצוות, גם סף גבוה מאוד וגם סף נמוך מאוד, אף אחד לא ייתן מסווג טוב במיוחד. ואנחנו יודעים שזה כך כי אנחנו רואים שיש לנו או דיוק נמוך מאוד או קליעה נמוכה מאוד. ואם פשוט ניקח את הממוצע שהוא P+R)/2) מהדוגמה הזו, הממוצע הוא למעשה הגבוה ביותר עבור אלגוריתם 3, למרות שאפשר לקבל רמה כזו של ביצועים על ידי חיזוי y=1 כל הזמן וזה פשוט לא מסווג הכי טוב, נכון? אתה מנבא y=1 כל הזמן, מסווג פשוט לא שימושי, שכל מה שהוא עושה הוא מוציא פלט של y=1. לכן אלגוריתם 1 או אלגוריתם 2 יהיו שימושיים יותר מאלגוריתם 3. אבל בדוגמה הזו, לאלגוריתם 3 יש ערך ממוצע גבוה יותר של קליעה ודיוק מאשר לאלגוריתמים 1 ו-2. ולכן בדרך כלל אנחנו חושבים שהממוצע הזה של דיוק וקליעה הוא לא דרך טובה במיוחד להערכת אלגוריתם הלמידה שלנו. בניגוד לדרך הזאת, יש דרך אחרת לשלב בין דיוק וקליעה. היא נקראת ציון-F והיא משתמשת בנוסחה הזו. אז בדוגמה שלנו, הנה ציוני-F. אז אנחנו רואים מציוני-F האלה שזה נראה כאילו לאלגוריתם 1 יש ציון-F הגבוה ביותר, לאלגוריתם 2 יש השני הכי גבוה, ולאלגוריתם 3 יש הנמוך ביותר. אז אם נלך לפי ציון-F נבחר כנראה באלגוריתם 1 על פני האחרים. ציון F, הנקרא גם ציון F1, ונכתב בדרך כלל כציון-F1 כמו שכתבתי כאן, אבל קוראים לו בדרך כלל ציון-F, אפשר להשתמש בשני המונחים, הוא קצת כמו לקחת את הממוצע של דיוק וקליעה, אבל הוא נותן לערך הנמוך מבין דיוק וקליעה, איזה מהם שלא יהיה, הוא נותן לו משקל גבוה יותר. אז אתם רואים במונה כאן שציון-F לוקח את המכפלה של דיוק וקליעה. אז אם הדיוק הוא 0 או הקליעה היא 0, ציון-F יהיה 0. אז במובן הזה, זה משלב בערך דיוק קליעה, אבל כדי שציון-F יהיה גבוה, גם הדיוק וגם הקליעה צריכים להיות די גדולים. אני חייב לומר שיש הרבה נוסחאות שונות לשילוב של דיוק וקליעה. הנוסחה הזו של ציון-F היא בעצם רק אחת מתוך מספר גדול מאד של אפשרויות, אבל מבחינה היסטורית או מסורתית זוהי הנוסחה שנראה שאנשים בלמידה ממוחשבת משתמשים בה. והמונח ציון-F אין לו ממש משמעות, אז אל תדאגו בקשר לסיבה שבגללה זה נקרא ציון-F או ציון-F1. אבל היא בדרך כלל נותנת לנו את התוצאה הרצויה כי אם הדיוק הוא אפס או הקליעה היא אפס, הנוסחה נותנת ציון נמוך מאוד, ולכן כשיש ציון-F גבוה, צריך שהדיוק או הקליעה יהיו קרובים לאחת. ושימו לב שאם P=0 או R=0, אז אנחנו מקבלים ציון-F שהוא 0. ולגבי ציון-F מושלם, אם גם הדיוק שווה 1 וגם הקליעה שווה 1, זה ייתן לך ציון-F, שהוא פעמיים 1 כפול 1 חלקי 2, זאת אומרת שציון-F יהיה שווה 1, במקרה של דיוק מושלם וקליעה מושלמת. וערכי ביניים בין 0 ל-1, בדרך כלל מגדירים סדר סביר על דירוג האיכות של מסווגים שונים. אז בסרטון הזה, דיברנו על הרעיון של יחסי גומלין בין דיוק לקליעה, וכיצד אנו יכולים לשנות את הסף שבו אנו משתמשים כדי להחליט אם לחזות y=1 או y=0. זה הסף שבו אנו קובעים האם אנחנו צריכים להיות לפחות 70% בטוחים או 90% בטוחים, או מה שלא יהיה לפני שנחזה y=1. ועל ידי שינוי הסף הזה, אפשר לשלוט על יחסי גומלין בין דיוק לקליעה. דיברנו גם על ציון-F, שלוקח דיוק וקליעה, ונותן לנו מספר ממשי יחיד לשימוש כהערכה. וכמובן, אם המטרה שלנו היא להגדיר באופן אוטומטי את הסף כדי להחליט מתי באמת y=1 ומתי y=0, דרך סבירה אחת לעשות זאת היא לנסות מגוון של ערכים שונים של סף. אז מנסים מגוון של ערכי סף ועושים הערכה של ערכי הסף השונים אלה, למשל, על סדרת האימות הצולב ואז בוחרים את ערך הסף שנותן לנו את ציון-F הגבוה ביותר על סדרת האימות. וזו תהיה גם דרך סבירה למדי לבחור את הסף באופן אוטומטי עבור המסווג.