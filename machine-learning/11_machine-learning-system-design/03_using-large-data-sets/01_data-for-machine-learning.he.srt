1
00:00:00,390 --> 00:00:03,570
בסרטון הקודם דיברנו על מדדי הערכה.

2
00:00:04,730 --> 00:00:05,840
בסרטון הזה, אני רוצה

3
00:00:06,080 --> 00:00:07,230
קצת לשנות את המסלול

4
00:00:07,480 --> 00:00:08,900
ולגעת בהיבט חשוב נוסף של

5
00:00:09,570 --> 00:00:10,990
עיצוב מערכת למידה ממוחשבת,

6
00:00:11,800 --> 00:00:13,290
שעולה לעתים קרובות, והוא

7
00:00:13,470 --> 00:00:14,990
הנושא של כמות

8
00:00:15,270 --> 00:00:17,110
הנתונים עליהם לאמן את המערכת.

9
00:00:17,310 --> 00:00:18,440
בכמה קטעי וידאו מוקדמים יותר,

10
00:00:18,620 --> 00:00:20,320
הזהרתי מפני הנטייה ללכת

11
00:00:20,690 --> 00:00:21,660
בעיוורון ופשוט לבזבז

12
00:00:21,980 --> 00:00:23,300
המון זמן באיסוף הרבה

13
00:00:23,420 --> 00:00:24,730
נתונים, שזה דבר

14
00:00:25,040 --> 00:00:26,360
שעוזר באמת רק לפעמים.

15
00:00:27,510 --> 00:00:28,580
אבל מתברר

16
00:00:28,820 --> 00:00:30,270
שבתנאים מסוימים, ואני

17
00:00:30,550 --> 00:00:31,580
אגדיר בסרטון הזה מהם

18
00:00:31,770 --> 00:00:33,590
התנאים האלה, להשיג

19
00:00:33,820 --> 00:00:35,440
הרבה נתונים ולאמן עליהם

20
00:00:35,730 --> 00:00:36,730
בסוגים מסוימים של אלגוריתמי

21
00:00:37,010 --> 00:00:38,160
למידה כן יכולה להיות

22
00:00:38,240 --> 00:00:39,470
דרך יעילה מאוד לגרום

23
00:00:39,770 --> 00:00:41,320
לאלגוריתם למידה להגיע לביצועים טובים מאוד.

24
00:00:42,810 --> 00:00:44,280
והדבר הזה קורה לעתים מספיק קרובות

25
00:00:44,710 --> 00:00:45,930
שאם התנאים האלה מתאימים

26
00:00:46,310 --> 00:00:47,850
לבעיה שלך, ואם

27
00:00:47,970 --> 00:00:48,770
אתה מסוגל להשיג הרבה

28
00:00:48,980 --> 00:00:50,070
נתונים, זו יכולה להיות

29
00:00:50,210 --> 00:00:51,330
דרך טובה מאוד לייצר

30
00:00:51,560 --> 00:00:52,970
אלגוריתם למידה עם ביצועים גבוהים מאוד.

31
00:00:53,990 --> 00:00:55,620
אז בסרטון הזה,

32
00:00:56,320 --> 00:00:58,960
בואו נדבר על זה עוד.

33
00:00:59,110 --> 00:00:59,820
תנו לי להתחיל עם סיפור.

34
00:01:01,080 --> 00:01:02,620
לפני שנים רבות, שני חוקרים

35
00:01:03,400 --> 00:01:05,380
שאני מכיר, מישל בנקו

36
00:01:05,520 --> 00:01:08,110
ואריק בריל ניהלו את המחקר המרתק הבא.

37
00:01:09,820 --> 00:01:11,290
הם רצו ללמוד את האפקט

38
00:01:11,550 --> 00:01:12,910
של שימוש באלגוריתמים

39
00:01:13,290 --> 00:01:15,210
שונים ללמידה לעומת אימון

40
00:01:15,380 --> 00:01:17,420
אותו אלגוריתם על סדרות אימון שונות.

41
00:01:18,020 --> 00:01:19,550
הם עסקו בבעיית

42
00:01:20,170 --> 00:01:22,120
סיווג בין מילים מבלבלות,

43
00:01:22,550 --> 00:01:23,610
כך למשל במשפט כאן:

44
00:01:24,410 --> 00:01:26,990
לארוחת הבוקר אכלתי ___ ביצים. האם "to", "two" או "too"?

45
00:01:27,940 --> 00:01:28,890
אז בדוגמה הזו,

46
00:01:29,450 --> 00:01:32,390
זה צריך להיות לארוחת בוקר אכלתי 2 (two) ביצים.

47
00:01:33,510 --> 00:01:34,530
אז זוהי דוגמה אחת

48
00:01:35,320 --> 00:01:37,800
של קבוצה של מילים שניתן לבלבל ביניהם, והנה עוד קבוצה.

49
00:01:38,240 --> 00:01:39,650
אז הם לקחו בעיות למידה

50
00:01:39,950 --> 00:01:41,580
כמו אלה, מין בעיות למידה מונחית,

51
00:01:41,780 --> 00:01:43,190
כדי לנסות לסווג

52
00:01:43,970 --> 00:01:45,210
מה היא המילה הנכונה

53
00:01:45,400 --> 00:01:46,560
שאותה צריך להשלים במקום הזה

54
00:01:47,540 --> 00:01:48,140
במשפט באנגלית.

55
00:01:51,010 --> 00:01:52,110
הם לקחו כמה אלגוריתמים שונים של

56
00:01:52,340 --> 00:01:53,520
למידה שהיו

57
00:01:53,730 --> 00:01:55,210
הדברים הכי

58
00:01:55,310 --> 00:01:56,110
מתקדמים בימים ההם,

59
00:01:56,410 --> 00:01:57,670
כאשר הם ניהלו את המחקר

60
00:01:57,730 --> 00:01:59,220
ב-2001, אז הם השתמשו

61
00:01:59,750 --> 00:02:01,140
במין גרסה אחרת, שונה,

62
00:02:01,630 --> 00:02:03,180
של רגרסיה לוגיסטית שנקראת פרספטרון.

63
00:02:03,760 --> 00:02:05,160
הם גם לקחו קצת

64
00:02:05,250 --> 00:02:06,700
אלגוריתמים שהיו די

65
00:02:07,090 --> 00:02:08,620
בשימוש בתקופה ההיא אבל קצת פחות

66
00:02:08,830 --> 00:02:10,600
בשימוש עכשיו, אלגוריתם

67
00:02:10,750 --> 00:02:11,980
ניפוי (winnow) דומה מאוד

68
00:02:12,380 --> 00:02:13,410
לרגרסיה לוגיסטית

69
00:02:13,660 --> 00:02:15,580
אבל שונה בכמה פרטים,

70
00:02:16,140 --> 00:02:18,220
שעכשיו הוא כבר

71
00:02:18,480 --> 00:02:19,380
לא כל כך בשימוש,

72
00:02:20,180 --> 00:02:21,180
וגם אלגוריתם למידה "מבוסס זיכרון"

73
00:02:21,430 --> 00:02:23,240
שגם הוא קצת פחות נפוץ בימינו.

74
00:02:23,610 --> 00:02:25,940
אני אספר לכם קצת על זה יותר מאוחר.

75
00:02:26,230 --> 00:02:27,230
והם השתמשו באלגוריתם בייס

76
00:02:27,690 --> 00:02:29,240
נאיבי, שהוא משהו שאנחנו

77
00:02:29,410 --> 00:02:32,380
בהחלט נדבר עליו בקורס.

78
00:02:32,690 --> 00:02:34,400
הפרטים המדויקים של האלגוריתמים האלה אינם חשובים.

79
00:02:35,040 --> 00:02:36,080
תחשבו על זה כעל בחירה

80
00:02:36,430 --> 00:02:40,380
של ארבעה אלגוריתמי סיווג שונים, ובאמת האלגוריתמים המדויקים אינם חשובים.

81
00:02:41,980 --> 00:02:42,990
אבל מה שהם עשו היה

82
00:02:43,140 --> 00:02:44,160
לנסות גדלים שונים של סדרת האימון

83
00:02:44,500 --> 00:02:45,390
וניסו את אלגוריתמי הלמידה

84
00:02:45,450 --> 00:02:47,070
האלה על טווח של

85
00:02:47,210 --> 00:02:49,640
גדלים של סדרת אימון וזוהי התוצאה שהם קיבלו.

86
00:02:50,300 --> 00:02:51,310
והמגמות מאוד

87
00:02:51,470 --> 00:02:53,170
ברורות. קודם כל, רוב

88
00:02:53,290 --> 00:02:55,470
האלגוריתמים האלה נותנים ביצועים דומים להפליא.

89
00:02:56,200 --> 00:02:57,760
ושנית, כאשר סדרת

90
00:02:58,150 --> 00:02:59,760
האימון גדלה, על

91
00:02:59,860 --> 00:03:00,970
הציר האופקי יש לנו

92
00:03:01,280 --> 00:03:02,510
גודל סדרת האימון במיליונים,

93
00:03:04,070 --> 00:03:05,360
אז כשהסדרה גדלה

94
00:03:05,420 --> 00:03:07,440
ממאה אלף

95
00:03:07,720 --> 00:03:09,060
לאלף מיליון דהיינו

96
00:03:09,330 --> 00:03:10,980
מיליארד דוגמאות הכשרה,

97
00:03:11,090 --> 00:03:11,860
הביצועים של האלגוריתמים

98
00:03:12,870 --> 00:03:15,360
גדלים באופן מונוטוני,

99
00:03:15,740 --> 00:03:16,610
והעובדה היא שאם

100
00:03:16,650 --> 00:03:18,600
בוחרים באלגוריתם כלשהו, ואנחנו עלולים לבחור

101
00:03:19,000 --> 00:03:21,320
אלגוריתם "נחות", אבל

102
00:03:21,490 --> 00:03:22,650
אם ניתן לאלגוריתם

103
00:03:23,190 --> 00:03:26,150
ה"נחות" הזה יותר נתונים, אז

104
00:03:26,390 --> 00:03:27,570
ממה שרואים כאן, נראה שזה

105
00:03:27,670 --> 00:03:30,330
יהיה סביר שהוא ינצח לנצח אפילו אלגוריתם "מעולה".

106
00:03:32,200 --> 00:03:33,270
לאחר המחקר המקורי הזה,

107
00:03:33,720 --> 00:03:35,850
שהיה בעל השפעה רבה, נערכו

108
00:03:36,360 --> 00:03:37,500
עוד מגוון של מחקרים

109
00:03:37,830 --> 00:03:39,020
שונים המראים תוצאות דומות,

110
00:03:39,550 --> 00:03:40,840
המראים שאלגוריתמים שונים

111
00:03:41,150 --> 00:03:42,270
של למידה נוטים

112
00:03:42,630 --> 00:03:44,290
לעיתים, בהתאם

113
00:03:44,460 --> 00:03:46,060
לפרטים, יכולים לתת טווחי ביצועים

114
00:03:46,490 --> 00:03:48,320
דומים למדי, ושמה שיכול

115
00:03:48,520 --> 00:03:51,570
באמת לשפר את הביצועים הוא אם נותנים לאלגוריתם טונות של נתוני אימון.

116
00:03:53,190 --> 00:03:54,640
ותוצאות כמו אלה

117
00:03:55,010 --> 00:03:56,030
הובילה לאימרה

118
00:03:56,130 --> 00:03:57,360
בלמידת מכונה שהרבה פעמים

119
00:03:57,510 --> 00:03:58,920
בלמידת מכונה זה לא

120
00:03:59,180 --> 00:04:00,460
מי שיש לו האלגוריתם הכי טוב

121
00:04:00,600 --> 00:04:01,720
שמנצח, זה מי שיש לו

122
00:04:02,810 --> 00:04:04,260
הכי הרבה נתונים. אז השאלה היא

123
00:04:04,460 --> 00:04:06,240
מתי זה נכון ומתי זה לא נכון.

124
00:04:06,560 --> 00:04:07,710
כי כשיש לנו אלגוריתם

125
00:04:07,850 --> 00:04:09,000
למידה שעבורו זה

126
00:04:09,150 --> 00:04:10,590
נכון, אז להשיג

127
00:04:10,820 --> 00:04:11,970
המון נתונים היא לעתים קרובות

128
00:04:12,620 --> 00:04:13,830
הדרך הכי טובה להבטיח

129
00:04:14,180 --> 00:04:15,700
שיהיה לנו אלגוריתם עם

130
00:04:15,900 --> 00:04:17,360
ביצועים גבוהים מאוד, ולא

131
00:04:17,520 --> 00:04:20,080
דיונים ממושכים לגבי באיזה אלגוריתם להשתמש.

132
00:04:21,710 --> 00:04:23,200
בואו ננסה לקבוע

133
00:04:23,330 --> 00:04:25,130
שורה של הנחות שכשהן מתקיימות

134
00:04:25,660 --> 00:04:28,230
אנחנו חושבים שזה יוכל לעזור לנו אם יהיה לנו מערך אימון ענקי.

135
00:04:29,780 --> 00:04:31,310
נניח שבבעיית

136
00:04:31,410 --> 00:04:33,210
הלמידה של המכונה שלנו, בתכונות

137
00:04:34,080 --> 00:04:36,560
x אכן קיים מספיק מידע

138
00:04:36,830 --> 00:04:38,600
כדי לחזות את y באופן מדויק.

139
00:04:40,380 --> 00:04:41,490
לדוגמה, אם ניקח

140
00:04:41,790 --> 00:04:44,860
את המילים המבלבלות שהיתה לנו בשקופית הקודמת.

141
00:04:45,740 --> 00:04:47,040
נניח שוקטור התכונות x

142
00:04:47,520 --> 00:04:48,360
כולל את מה שמסביב,

143
00:04:49,090 --> 00:04:51,620
את המרחב הריק מסביב לחלק שאנחנו מנסים למלא.

144
00:04:51,840 --> 00:04:53,630
התכונות כוללות את מה

145
00:04:54,220 --> 00:04:56,440
שאנחנו צריכים, יש לנו המשפט לפעמים לארוחת בוקר אני אוכל רווח ביצים.

146
00:04:57,350 --> 00:04:58,220
זה בעצם מספיק מידע

147
00:04:58,480 --> 00:04:59,970
כדי לומר לנו

148
00:05:00,170 --> 00:05:01,050
שהמילה שאנחנו רוצים

149
00:05:01,420 --> 00:05:03,640
במקום הריק היא "שתי" - TWO -

150
00:05:03,850 --> 00:05:06,640
ולא TO ולא TOO. אז

151
00:05:09,650 --> 00:05:11,270
התכונות כוללות

152
00:05:11,620 --> 00:05:13,390
את המילים המקיפות

153
00:05:13,560 --> 00:05:15,360
מה שנותן לנו מספיק מידע כדי

154
00:05:15,790 --> 00:05:17,640
להחליט בצורה חד משמעית מה היא

155
00:05:17,780 --> 00:05:18,830
התווית y, או

156
00:05:19,300 --> 00:05:20,190
במילים אחרות מה היא המילה

157
00:05:20,750 --> 00:05:21,760
שאני צריך להשתמש בה כדי למלא

158
00:05:22,100 --> 00:05:23,520
את המקום הריק מתוך

159
00:05:23,930 --> 00:05:25,610
הקבוצה הזו של שלוש מילים מבלבלות.

160
00:05:27,110 --> 00:05:28,320
אז זו דוגמה

161
00:05:28,460 --> 00:05:29,840
שבה סט התכונות x מכיל מספיק מידע

162
00:05:30,410 --> 00:05:32,270
לחשב את y.

163
00:05:32,470 --> 00:05:33,240
כדוגמה נגדית

164
00:05:34,690 --> 00:05:36,010
בואו נחשוב על בעיה של חיזוי

165
00:05:36,470 --> 00:05:38,090
מחירו של בית רק

166
00:05:38,340 --> 00:05:39,330
מתוך גודל

167
00:05:39,390 --> 00:05:40,350
הבית ולא משום תכונות אחרות.

168
00:05:42,060 --> 00:05:42,060
אז

169
00:05:42,820 --> 00:05:43,890
אז אם תדמיינו שאני אומר לכם

170
00:05:44,150 --> 00:05:45,270
שגודל הבית הוא

171
00:05:45,370 --> 00:05:48,100
500 רגל מרובע אבל אני לא נותן לכם שום תכונות אחרות.

172
00:05:48,530 --> 00:05:49,520
אני לא אומר לכם

173
00:05:49,590 --> 00:05:51,990
שהבית נמצא באזור יקר של העיר.

174
00:05:52,590 --> 00:05:53,710
או אם אני לא אומר לכם

175
00:05:53,840 --> 00:05:55,290
את מספר

176
00:05:55,500 --> 00:05:57,030
החדרים בבית, או כמה

177
00:05:57,180 --> 00:05:58,400
יפה מרוהט הבית,

178
00:05:58,790 --> 00:06:00,540
או אם הבית חדש או ישן.

179
00:06:01,090 --> 00:06:02,290
אם אני לא אומר לכם שום דבר

180
00:06:02,540 --> 00:06:03,360
חוץ מאשר

181
00:06:03,520 --> 00:06:05,440
שגודלו הוא 500 רגל מרובע, יש

182
00:06:05,720 --> 00:06:07,160
כל כך הרבה גורמים אחרים

183
00:06:07,340 --> 00:06:08,280
שישפיעו על מחיר של

184
00:06:08,470 --> 00:06:09,940
בית חוץ מאשר רק הגודל

185
00:06:10,320 --> 00:06:11,330
של הבית שאם כל

186
00:06:11,440 --> 00:06:12,910
מה שאנחנו יודעים הוא הגודל שלו, אז זה

187
00:06:13,050 --> 00:06:14,610
ממש קשה לחזות את המחיר במדויק.

188
00:06:16,220 --> 00:06:16,860
אז זו תהיה דוגמה

189
00:06:17,280 --> 00:06:18,230
נגדית להנחה הזו,

190
00:06:18,880 --> 00:06:20,300
ההנחה שהתכונות מכילות מספיק מידע

191
00:06:20,800 --> 00:06:23,260
כדי לחזות את המחיר ברמת הדיוק הרצוי.

192
00:06:24,090 --> 00:06:25,180
האופן שבו אני חושב על בדיקת

193
00:06:25,540 --> 00:06:26,730
ההנחה הזאת, דרך אחת שבה אני

194
00:06:26,940 --> 00:06:29,160
חושב על זה, אני שואל את עצמי במצב הזה.

195
00:06:30,260 --> 00:06:31,660
בהינתן וקטור תכונות x,

196
00:06:32,180 --> 00:06:33,320
בהינתן התכונות, בהינתן

197
00:06:33,380 --> 00:06:35,440
אותו מידע זמין, כמו גם אלגוריתם למידה.

198
00:06:36,510 --> 00:06:38,690
אם היינו הולכים למומחה אנושי בתחום הזה

199
00:06:39,680 --> 00:06:41,570
האם מומחה אנושי יכול או

200
00:06:41,720 --> 00:06:43,160
האם יכול מומחה אנושי לחזות בביטחון

201
00:06:43,490 --> 00:06:45,390
את הערך של y. עבור

202
00:06:45,630 --> 00:06:46,730
הדוגמה הראשונה הזו אם נלך

203
00:06:46,980 --> 00:06:49,420
למומחה דובר אנגלית,

204
00:06:49,810 --> 00:06:51,260
אתה הולך למישהו

205
00:06:51,390 --> 00:06:53,740
שמדבר אנגלית טוב, נכון, אז

206
00:06:53,940 --> 00:06:55,230
מומחה אנושי באנגלית

207
00:06:55,940 --> 00:06:57,260
כמו לדוגמא אני או רוב האנשים

208
00:06:57,450 --> 00:06:59,730
כמוך וכמוני כנראה

209
00:07:00,160 --> 00:07:01,080
נצליח לחזות

210
00:07:01,170 --> 00:07:02,370
איזו מילה צריכה להיכנס

211
00:07:02,620 --> 00:07:03,960
לכאן, דוברי אנגלית

212
00:07:04,290 --> 00:07:05,550
טובה יכולה לחזות את זה בביטחון,

213
00:07:05,850 --> 00:07:06,710
ולכן זה נותן לי ביטחון

214
00:07:07,470 --> 00:07:08,640
ש-x מאפשר לנו לחזות

215
00:07:08,810 --> 00:07:10,550
את y במדויק, אבל לעומת זאת

216
00:07:11,240 --> 00:07:13,550
אם אנחנו הולכים למומחה אנושי במחירים,

217
00:07:14,040 --> 00:07:16,390
אולי סוכן נדל"ן מומחה, כן, מישהו

218
00:07:16,950 --> 00:07:18,090
שמוכר בתים לפרנסתו.

219
00:07:18,610 --> 00:07:19,450
אם אני רק אומר להם

220
00:07:19,550 --> 00:07:20,440
את גודל הבית ואני

221
00:07:20,530 --> 00:07:21,860
שואל אותם מה המחיר,

222
00:07:22,240 --> 00:07:23,410
גם אם הוא מומחה

223
00:07:23,600 --> 00:07:25,210
למחירים או למכירת

224
00:07:25,600 --> 00:07:26,520
בתים, הוא לא יוכל

225
00:07:26,550 --> 00:07:28,280
לעזור לי, ולכן אני מסיק

226
00:07:29,000 --> 00:07:31,060
שעבור הדוגמה של מחיר הדיור, ידיעת

227
00:07:31,600 --> 00:07:33,300
הגודל איננה נותנת לנו

228
00:07:33,460 --> 00:07:34,960
מספיק מידע כדי לחזות

229
00:07:35,920 --> 00:07:36,870
את מחיר הבית.

230
00:07:37,690 --> 00:07:39,890
אז בואו נניח ש-x כן נותן מספיק מידע.

231
00:07:41,200 --> 00:07:42,650
בוא נראה מתי זה

232
00:07:43,040 --> 00:07:44,230
שיש לנו הרבה נתונים עשוי לעזור.

233
00:07:45,020 --> 00:07:46,370
נניח שבתכונות יש מספיק

234
00:07:46,650 --> 00:07:47,870
מידע כדי לחזות את

235
00:07:48,050 --> 00:07:49,380
הערך של y.

236
00:07:49,540 --> 00:07:50,750
ונניח שאנחנו משתמשים

237
00:07:50,960 --> 00:07:52,380
באלגוריתם למידה עם

238
00:07:52,600 --> 00:07:54,430
מספר רב של פרמטרים,

239
00:07:54,580 --> 00:07:56,020
רגרסיה לוגיסטית או רגרסיה

240
00:07:56,280 --> 00:07:58,090
ליניארית עם מספר רב של תכונות.

241
00:07:58,550 --> 00:07:59,490
דבר אחד שאני עושה

242
00:07:59,950 --> 00:08:00,740
לפעמים, דבר אחד שאני עושה

243
00:08:00,960 --> 00:08:03,300
לעתים קרובות בעצם הוא להשתמש ברשת עצבית עם הרבה יחידות נסתרות.

244
00:08:03,860 --> 00:08:05,230
זה עוד סוג של

245
00:08:05,500 --> 00:08:07,420
אלגוריתם למידה עם הרבה פרמטרים.

246
00:08:08,470 --> 00:08:10,280
אז כל אלה הם אלגוריתמי למידה

247
00:08:10,350 --> 00:08:12,350
חזקים עם הרבה פרמטרים

248
00:08:13,040 --> 00:08:14,810
שיכולים להתאים פונקציות מורכבות מאוד לנתונים.

249
00:08:16,750 --> 00:08:17,550
אני קורא להם,

250
00:08:18,630 --> 00:08:19,720
אני חושב עליהם

251
00:08:20,510 --> 00:08:21,970
כאלגוריתמים בעלי הטייה נמוכה כי

252
00:08:22,140 --> 00:08:23,540
אנחנו יכולים להתאים פונקציות מורכבות מאוד,

253
00:08:25,480 --> 00:08:26,740
ומאחר ויש לנו

254
00:08:27,260 --> 00:08:28,920
אלגוריתמי למידה חזקים מאוד,

255
00:08:29,380 --> 00:08:30,590
ולכן הם יכולים להתאים פונקציות מורכבות מאוד.

256
00:08:31,680 --> 00:08:33,470
רוב הסיכויים הם

257
00:08:34,070 --> 00:08:35,790
שאם נפעיל את האלגוריתמים הללו על

258
00:08:35,940 --> 00:08:37,250
ערכות הנתונים, הם יוכלו

259
00:08:37,430 --> 00:08:38,770
להתאים את סדרת

260
00:08:39,200 --> 00:08:40,680
האימון היטב, ולכן

261
00:08:40,940 --> 00:08:43,230
אנחנו מצפים ששגיאת האימון תהיה קטנה.

262
00:08:44,520 --> 00:08:45,520
עכשיו נניח שאנחנו משתמשים

263
00:08:46,020 --> 00:08:47,790
בסדרת אימון ממש ענקית,

264
00:08:48,190 --> 00:08:49,370
במקרה זה, אם יש לנו

265
00:08:49,430 --> 00:08:51,460
קבוצת אימון ענקית, אז

266
00:08:51,630 --> 00:08:53,490
יש לקוות שלמרות שיש לנו הרבה פרמטרים

267
00:08:53,760 --> 00:08:56,080
אבל מכיוון שסדרת האימון הוא בעצם אפילו

268
00:08:56,360 --> 00:08:57,450
הרבה יותר גדולה ממספר

269
00:08:57,840 --> 00:08:59,450
הפרמטרים אז בתקווה

270
00:08:59,640 --> 00:09:01,490
האלגוריתמים האלה לא יחוו התאמת-יתר.

271
00:09:02,590 --> 00:09:03,660
ברור? כי יש לנו כזו

272
00:09:03,710 --> 00:09:05,680
קבוצת אימון מסיבית

273
00:09:06,070 --> 00:09:07,870
והכוונה בכך ש"לא סביר שתהיה התאמת-יתר" היא

274
00:09:08,070 --> 00:09:09,090
ששגיאת האימון

275
00:09:09,390 --> 00:09:10,860
תהיה בתקווה

276
00:09:11,050 --> 00:09:13,270
קרובה לשגיאת הבדיקה.

277
00:09:13,960 --> 00:09:15,160
אז כשמחברים את שני

278
00:09:15,350 --> 00:09:16,770
המשפטים, שלסדרת האימון

279
00:09:16,990 --> 00:09:18,590
יש שגיאה קטנה

280
00:09:18,700 --> 00:09:19,870
ולסדרת המבחן יש שגיאה קרובה

281
00:09:20,360 --> 00:09:22,290
לשגיאת האימון, מה

282
00:09:22,460 --> 00:09:24,510
ששני אלה ביחד רומזים הוא

283
00:09:24,710 --> 00:09:26,630
שאנחנו מקווים שגם השגיאה של

284
00:09:27,780 --> 00:09:28,450
ערכת הבדיקה תהיה קטנה.

285
00:09:30,000 --> 00:09:32,610
דרך נוספת

286
00:09:32,720 --> 00:09:33,930
לחשוב על זה היא

287
00:09:34,700 --> 00:09:35,740
שכדי שיהיה לנו אלגוריתם

288
00:09:35,880 --> 00:09:37,630
למידה בעל ביצועים גבוהים, אנחנו רוצים

289
00:09:37,930 --> 00:09:40,470
שהוא לא יהיה בעל הטיה גבוהה וגם לא תהיה לו שונות גבוהה.

290
00:09:42,060 --> 00:09:43,270
אז את בעיית ההטיה אנחנו

291
00:09:43,350 --> 00:09:44,700
פותרים על ידי כך שמוודאים

292
00:09:44,880 --> 00:09:45,910
שיש לנו אלגוריתם למידה עם הרבה

293
00:09:46,170 --> 00:09:47,670
פרמטרים מה

294
00:09:47,840 --> 00:09:48,930
שנותן לנו אלגוריתם בעל הטיה נמוכה

295
00:09:50,110 --> 00:09:51,460
ובאמצעות

296
00:09:51,610 --> 00:09:53,240
סדרת אימון גדולה מאוד, אנחנו מבטיחים

297
00:09:53,760 --> 00:09:55,590
שאין לנו כאן גם בעיה שונות.

298
00:09:55,840 --> 00:09:57,280
אז אנחנו מקווים שלאלגוריתם

299
00:09:57,430 --> 00:09:59,100
שלנו תהיה שונות נמוכה ולכן

300
00:09:59,340 --> 00:10:00,940
על ידי חיבור שני הדברים האלה,

301
00:10:01,870 --> 00:10:02,830
אנחנו מקבלים אלגוריתם למידה

302
00:10:02,900 --> 00:10:03,990
עם הטיה נמוכה ושונות נמוכה

303
00:10:04,990 --> 00:10:06,920
וזה מאפשר לנו

304
00:10:07,140 --> 00:10:08,300
להצליח על

305
00:10:08,710 --> 00:10:10,150
ערכת הבדיקה.

306
00:10:10,430 --> 00:10:12,140
וביסודו של דבר אלה מרכיבי המפתח,

307
00:10:13,020 --> 00:10:14,560
ההנחה שהתכונות

308
00:10:14,940 --> 00:10:16,750
מכילות מספיק מידע ושיש

309
00:10:16,900 --> 00:10:17,960
לנו פונקציה מספיק כוללת,

310
00:10:18,400 --> 00:10:19,580
זה מה שמבטיח הטיה נמוכה,

311
00:10:20,760 --> 00:10:21,750
ואז סדרת אימון

312
00:10:22,110 --> 00:10:25,010
מסיבית זה מה שמבטיח גם שונות נמוכה.

313
00:10:27,150 --> 00:10:28,310
אז זה נותן לנו

314
00:10:28,410 --> 00:10:29,820
סט של תנאים שבתקווה

315
00:10:30,090 --> 00:10:31,610
שופכים קצת אור על

316
00:10:31,870 --> 00:10:33,730
הסוג הזה של בעיה שבה אם

317
00:10:33,860 --> 00:10:34,790
יש לך הרבה נתונים

318
00:10:34,960 --> 00:10:36,150
ואתה בנית אלגוריתם

319
00:10:36,380 --> 00:10:38,930
למידה עם הרבה פרמטרים, זו יכולה

320
00:10:39,120 --> 00:10:39,870
להיות דרך טובה להעניק

321
00:10:40,060 --> 00:10:42,490
לאלגוריתם הלמידה שלך ביצועים גבוהים,

322
00:10:43,480 --> 00:10:44,140
ובאמת אני חושב שמבחן המפתח

323
00:10:44,230 --> 00:10:45,520
שאני שואל את עצמי לעתים קרובות

324
00:10:45,820 --> 00:10:47,100
הוא קודם כל, האם מומחים אנושיים יכולים

325
00:10:47,200 --> 00:10:48,360
מתוך הסתכלות על התכונות x

326
00:10:48,880 --> 00:10:49,890
לחזות בביטחון את הערך של

327
00:10:50,030 --> 00:10:51,080
y. כי זה סוג של

328
00:10:51,210 --> 00:10:53,050
הבטחה ש-y

329
00:10:53,320 --> 00:10:55,040
ניתן לחיזוי במדויק מן

330
00:10:55,140 --> 00:10:57,010
התכונות x והדבר השני הוא

331
00:10:57,510 --> 00:10:58,630
האם אנחנו באמת יכולים להשיג סט

332
00:10:58,820 --> 00:11:00,150
אימון גדול, ולאמן את

333
00:11:00,350 --> 00:11:01,470
אלגוריתם הלמידה עם הרבה

334
00:11:01,540 --> 00:11:03,290
פרמטרים על סט

335
00:11:03,520 --> 00:11:04,420
האימון ואם אתה יכול לעשות את שניהם

336
00:11:04,870 --> 00:11:06,300
אז זה לעתים קרובות יתן

337
00:11:06,460 --> 00:11:08,570
לנו אלגוריתם למידה בעל ביצועים טובים מאוד.