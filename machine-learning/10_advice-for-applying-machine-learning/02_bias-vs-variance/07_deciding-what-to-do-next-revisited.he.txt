דיברנו על איך להעריך אלגוריתמים של למידה, דיברנו על בחירת המודל, דיברנו הרבה על הטיה ושונות. אז איך זה עוזר לנו להבין לאילו דברים יש פוטנציאל טוב וכדאי לנו לנסותם, ולאילו אין פוטנציאל טוב ולא כדאי לנסות לעשות אותם כדי לשפר את הביצועים של אלגוריתם למידה. בואו נחזור לדוגמה המקורית שלנו ונסתכל על התוצאות. אז הנה הדוגמה הקודמת שלנו שבה בנינו התאמה בעזרת רגרסיה ליניארית מוסדרת וגילינו שהיא לא עובדת כמו שקיוינו. אמרנו שיש לנו את התפריט הזה של אפשרויות. אז האם יש איזו דרך להבין אילו מהם עשויות להיות אפשרויות שיניבו תוצאות טובות? האפשרות הראשונה בתפריט היתה להשיג עוד דוגמאות אימון. זה טוב כאשר אנחנו רוצים לתקן בעיית שונות גבוהה. וראינו שאם יש לנו בעית הטייה גבוהה ואין לנו בעיה של שונות, אז ראינו בסרטון הקודם שלהוסיף דוגמאות אימון פשוט לא יעזור לנו הרבה בכלל. אז האפשרות הראשונה היא שימושית רק אם נעשה גרף של עקומות הלמידה ונבין ממנו שיש לנו לפחות בעייה כלשהי של שונות, כלומר ששגיאת האימות הצולב היא די הרבה יותר גדולה משְׁגִיאת האימון שלנו. מה בקשר לנסות להפחית את מספר התכונות? ובכן, לנסות קבוצה קטנה יותר של תכונות, זה שוב משהו שמתקן שונות גבוהה. במילים אחרות, אם אתה מבין, על ידי הסתכלות על עקומות למידה או משהו אחר שבו השתמשת, שיש לך בעיית הטיה גבוהה; אז למען השם, אל תבזבז את הזמן שלך בניסיון לבחור בזהירות קבוצה קטנה יותר של תכונות לשימוש. כי אם יש לך בעיית הטיה גבוהה, הורדה של מספר התכונות לא תעזור. בניגוד לכך, אם אתה מסתכל על עקומות למידה או משהו אחר ואתה מבין שיש לך בעיית שונות גבוהה, אז באמת הניסיון לבחור קבוצה קטנה יותר של תכונות עשוי בהחלט להיות שימוש יעיל מאוד בזמן שלך. מה בקשר לנסות להשיג תכונות נוספות? הוספת תכונות, בדרך כלל, לא תמיד, אבל בדרך כלל אנחנו חושבים על זה כעל פתרון לתיקון בעיית הטיה גבוהה. אז אם נוסיף תכונות זה בדרך כלל כי ההשערה הנוכחית שלך היא פשוטה מדי, ולכן אנחנו רוצים לנסות להשיג תכונות נוספות כדי לעשות את ההשערה שלנו טובה יותר בהתאמה לערכת האימון. וגם הוספת תכונות פולינומיות; זוהי דרך נוספת של הוספת תכונות ולכן היא עוד דרך לנסות לתקן בעיה של הטיה גבוהה. וכמובן, אם עקומות הלמידה מראות לך שעדיין יש לך בעיית שונות גבוהה, אז עכשיו זו אולי כבר דרך פחות טובה להשתמש בזמנך היקר. ולבסוף, הקטנת מקדם ההסדרה λ והגדלתו. זה מהיר וקל לנסות, אני מניח שזה לא באמת יבזבז לך הרבה חודשים מהחיים. בכל אופן הורדה של λ, אתם כבר אמורים לדעת, מתקנת בעייה של הטיה גבוהה. אם זה לא ברור לכם אני באמת ממליץ לכם להשהות את הווידאו ולחשוב על זה היטב ולשכנע את עצמכם שהפחתת λ מסייעת לתקן את בעיית ההטיה הגבוהה, ואילו הגדלת λ מתקנת בעיית שונות גבוהה. ואם אתם לא מצליחים לשכנע את עצמכם, תמשיכו להשהות את הוידאו ותעבדו על זה עוד עד שתצליחו לשכנע את עצמכם שזה נכון. או שתסתכלו על העקומות שציירנו בסוף הסרטון הקודם ונסו לוודא שאתם מבינים מדוע זה כך. ועכשיו בואו ניקח את כל מה שלמדנו ונקשר אותו בחזרה לרשתות עצביות. אז הנה כמה עצות מעשיות איך אני בדרך כלל בוחר את הארכיטקטורה או את תבנית הקישוריות של רשתות עצביות שבה אני משתמש. אז אם אתה מתאים רשת עצבית, אפשרות אחת תהיה להתאים, למשל, רשת עצבית קטנה למדי, עם ממש מעט יחידות נסתרות, אולי אפילו רק יחידה נסתרת אחת. אם אתה מתאים רשת עצבית, אפשרות אחת תהיה להתאים רשת עצבית קטנה יחסית עם, נניח, יחסית מעט שכבות, אולי רק שכבה אחת נסתרת ואולי רק מספר קטן יחסית של יחידות נסתרות. אז לרשת כזו יש בדרך כלל יחסית מעט פרמטרים ולכן היא נוטה יותר להתאמת-חסר. היתרון העיקרי של רשתות עצביות קטנות כאלה הוא שהחישוב הוא הרבה זול יותר. אפשרות אחרת תהיה להתאים לרשת עצבית גדולה, אולי גדולה יחסית, עם יותר יחידות נסתרות - הרבה מאוד יחידות נסתרות - או עם יותר שכבות נסתרות. וברשתות עצביות כאלה יש בדרך כלל הרבה יותר פרמטרים ולכן הן נוטות יותר להתאמת-יתר. חסרון אחד, לעתים קרובות לא כל כך גדול אבל משהו שצריך לשים אליו לב, הוא שאם יש לך מספר גדול של נוירונים ברשת שלך, אז זה יכול להיות יקר יותר מבחינה חישובית. אמנם כסיבה, יש לקוות שזו לעתים קרובות איננה בעיה ממש גדולה. הבעיה הפוטנציאלית העיקרית של רשתות עצביות הרבה יותר גדולות כאלה היא שהן יכולות להיות נוטות יותר להתאמת-יתר. ומסתבר שאם תיישם רשת עצבית באמצעות רשת עצבית גדולה, לעתים קרובות גדולה יותר היא בעצם טובה יותר אבל אם היא סובלת מהתאמת-יתר, אז אתה יכול להשתמש בהסדרה כדי לטפל בהתאמת-היתר, ובדרך כלל רשת עצבית גדולה יותר שמטפלים בהתאמת-יתר שלה באמצעות הסדרה היא לעתים קרובות יותר יעילה מאשר התאמה לרשת עצבית קטנה יותר. ואז החיסרון העיקרי האפשרי הוא שזה יכול להיות יקר יותר מבחינה חישובית. ולבסוף, עוד אחת מההחלטות היא, למשל, מספר השכבות הנסתרות שאתה רוצה. האם אתה רוצה שכבה נסתרת אחת או אולי אתה רוצה שלוש שכבות נסתרות, כפי שרואים כאן, או שאתה רוצה שתי שכבות נסתרות? ובדרך כלל, כפי שאני חושב שאמרתי בווידאו קודם, שימוש בשכבה נסתרת אחת היא ברירת מחדל סבירה, אבל אם אתה רוצה לבחור את מספר השכבות הנסתרות יש עוד דבר שאתה יכול לנסות והוא לבנות סדרות אימון, אימות צולב, ומבחן, לפצל ולנסות לאמן רשתות עצביות עם שכבה אחת נסתרת או עם שתי שכבות נסתרות או עם שלוש שכבות נסתרות ולראות איזו מבין הרשתות העצביות האלה הכי מצליחה על קבוצת האימות הצולב. אתה לוקח שלוש רשתות עצביות עם אחת, שתיים ושלוש שכבות נסתרות, ומחשב את שגיאת האימות הצולב, J-cv ומשתמש בחישוב הזה כדי לבחור איזו מהם אתה חושב שהיא הרשת העצבית הטובה ביותר. אז, זה הכל בקשר להטיה ושונות ודברים כמו עקומות למידה, שמנסות לאבחן את הבעיות האלה, לגבי מה שמשתמע מהדברים האלה. יכולות להיות פעולות יעילות יותר ופעולות יעילות פחות כשמנסים לשפר את הביצועים של אלגוריתם למידה. אם הבנתם את התוכן של מספר הסרטונים האחרונים ואם תיישמו אותם אתם בעצם תצליחו להיות הרבה יותר יעילים בלגרום לאלגוריתמי למידה לעבוד על בעיות, מחלק גדול, ואולי אפילו מרוב אנשי המקצוע שבונים מכונות למידה כאן בעמק הסיליקון היום ועושים את הדברים האלה במשרה מלאה. אז אני מקווה שהייעוץ בתחום אבחון של שונות והטייה יעזור לכם להיות הרבה יותר יעילים ביישום של מכונות למידה ובלגרום להם לעבוד בצורה טובה מאוד.