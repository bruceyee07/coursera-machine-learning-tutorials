ראינו כיצד יכולה ההסדרה לסייע במניעת התאמת-יתר. אבל איך משפיעה ההסדרה על ההטיה ועל השונות של אלגוריתם למידה? בסרטון הזה אני רוצה להתעמק יותר בסוגיות של ההטיה והשונות, ולדבר על האינטראקציה שלהן עם, ועל איך הם משפיעות ומושפעות מההסדרה של אלגוריתם הלמידה. נניח שאנחנו מתאימים פולינום מסדר גבוה, כמו זה שמוצג כאן, אבל כדי למנוע התאמת-יתר אנחנו צריכים להשתמש בהסדרה, כמו שמוצג כאן אז יש לנו מונח הסדרה כזה כדי לנסות להגביל את גודלם של ערכי תטא. וכרגיל, ההתאמות הן רק מ-j=1 עד m, ולא מ-j=0 עד m. הבה נבחן שלושה מקרים. הראשון הוא המקרה בו לפרמטר ההסדרה למבדה יש ערך ענק, כגון אם למבדה שווה ל-10,000. איזשהו ערך ענק. במקרה זה, כל הפרמטרים האלה, תטא-1, תטא-2, תטא-3, וכן הלאה יהיה להם קנס כבד ולכן רוב הערכים של הפרמטרים האלה יהיו קרובים לאפס. וההשערה תהיה בערך ש-H של x שווה או בערך שווה לתטא-0. אז נקבל פונקצית השערה שנראית פחות או יותר ככה, פחות או יותר קו ישר שטוח, קבוע. ולכן להשערה הזאת יש הטיה גבוהה והיא סובלת מהתאמת חסר לנתונים, כי קו ישר אופקי הוא פשוט לא מודל מספיק טוב עבור הקבוצה הזו של נתונים. מצד שני, אם יש לנו ערך קטן מאוד של למבדה, כגון אם למבדה שווה לאפס. במקרה זה, בהתחשב בעובדה שאנחנו מתאימים פולינום מסדר גבוה, זה המקרה הרגיל של התאמת-יתר. במקרה זה, בהתחשב בכך שאנחנו מתאימים פולינום מסדר גבוה, ביסודו של דבר ללא הסדרה או עם הסדרה מינימלית מאוד, נקבל את המצב הרגיל של שונות גבוהה והתאמת-יתר. בעצם אם למבדה שווה לאפס, אנחנו בונים התאמה ללא רגולריזציה, ולכן מקבלים התאמת-יתר. וזה רק כשיש לנו ערך ביניים של למבדה שהוא לא גדול מדי ולא קטן מדי, שאנחנו מקבלים פרמטרים תטא שנותנים לנו התאמה סבירה לנתונים. אז איך אנחנו יכולים לבחור באופן אוטומטי ערך טוב עבור למבדה, פרמטר ההסדרה? אז בואו נחזור רגע, הנה המודל שלנו, והנה המטרה של אלגוריתם הלמידה שלנו. עבור המצב הזה שבו אנחנו כן משתמשים בהסדרה, תרשו לי להגדיר את J-אימון של תטא להיות משהו אחר, להיות מטרת האופטימיזציה אבל בלי מונח ההסדרה. בעבר, בסרטון קודם כאשר לא השתמשנו בהסדרה, הגדרנו את J-אימון של תטא להיות זהה ל-J של תטא כפונקצית מחיר אבל כאשר אנו משתמשים בהסדרה ויש לנו האיבר הזה עם למבדה אנחנו נגדיר את J-אימון להיות פשוט הסכום של ריבועי השגיאות על סדרת האימון או הממוצע של השגיאה בריבוע על קבוצת האימון, בלי לקחת בחשבון את ההסדרה. ובדומה אנחנו גם נגדיר את שגיאת האימות הצולב ואת שגיאת המבחן כמו בעבר להיות הממוצע של ריבועי השגיאות על סדרות האימות הצולב והמבחן. אז רק כדי לסכם את ההגדרות: J-אימון, J-CV ו-J-מבחן הם פשוט הממוצע או בעצם חצי הממוצע של ריבועי השגיאות על סדרות האימון, האימות והמבחן, ללא המונח הנוסף של ההסדרה. אז עכשיו בואו נדבר על איך אנחנו יכולים לבחור באופן אוטומטי את פרמטר ההסדרה למבדה. מה שאני עושה בדרך כלל הוא מחליט על מגוון של ערכים של למבדה שאותם אני רוצה לנסות. אז אני עשוי לדוגמא לשקול ערכים של למבדה, 0 (ללא הסדרה) או כמו הרשימה הזו, הנה כמה ערכים שאותם אני עשוי לנסות, למבדה = 0.01, 0.02, 0.04, וכן הלאה. ואני בדרך כלל מגדיר אותם בכפולות של שתיים, עד איזה שהוא ערך גדול מספיק. לו עשיתי את הטור הזה במדויק בכפולות של 2 הייתי מגיע ל-10.24. כאן יש לנו 10, שזה מספיק קרוב. לשלוש או ארבע ספרות אחרי הנקודה העשרונית לא תהיה ממש השפעה על התוצאה. אז זה נותן לי אולי 12 מודלים שונים לנסות. ואני מנסה לבחור מבין 12 הערכים השונים האלה של ההסדרה, של הפרמטר למבדה. וכמובן אפשר גם לנסות ערכים קטנים מ-0.01 או ערכים גדולים מ-10 אבל אני פשוט חתכתי כאן את הסדרה לנוחות. בהינתן 12 המודלים האלה, מה שאנחנו יכולים לעשות הוא, אנו יכולים לקחת את המודל הראשון עם למבדה שווה אפס, למזער את פונקצית העלות J של תטא וזה נותן לנו וקטור פרמטרים תטא. ובדומה לוידאו קודם, אני אציין אותו כתטא סימן עליון אחד. ואז אני לוקח את המודל השני שלי שבו למבדה הוא 0.01 וממזער את פונקצית העלות עכשיו באמצעות למבדה שווה 0.01 כמובן. ולקבל וקטור פרמטרים תטא שונה. ואני מסמן אותו תטא(2). ואז הבא בתור ואני מקבל תטא(3) עם המודל השלישי. וכך הלאה עד שהמודל האחרון עם למבדה שהוא 10 או 10.24, אני מקבל את תטא(12). עכשיו אני יכול לקחת את כל ההשערות האלה, את כל הפרמטרים תטא האלה ולהשתמש בסדרת האימות כדי לאמת אותם, אני מסתכל על המודל הראשון, המודל השני, שמתאימים לערכים שונים של פרמטר ההסדרה, ומעריך אותם בעזרת סדרת האימות הצולב, על ידי מדידת ריבוע השגיאה הממוצע של כל הפרמטרים תטא האלה על קבוצת האימות הצולב. ואז אני בוחר את אותו מודל מבין 12 המודלים האלה שנותן לי את השגיאה הנמוכה ביותר על קבוצת האימות. ונניח, בדוגמה הזאת, שהחלטנו לבחור את תטא-5, פולינום מסדר 5, כי שגיאת האימות שלו היתה הנמוכה ביותר. אז אחרי שעשינו את זה, מה שהיינו עושים אם נרצה לחשב את השגיאה בסדרת המבחן, הוא לקחת את הפרמטר תטא-5 שבחרנו, ולראות כמה הוא מצליח בערכת המבחן. אז שוב, כאן אנחנו מתאימים את הפרמטר הזה, למבדה, כדי שיתאים לסדרת האימות, ולכן שמרנו בצד את קבוצת המבחן ובה אנחנו נשתמש כדי לקבל הערכה טובה יותר של איך מצליח וקטור הפרמטרים תטא בהכללה לדוגמאות שלא נראו בעבר. אז זו בחירת המודל כפי שמיישמים אותה לבחירת פרמטר ההסדרה למבדה. הדבר האחרון שאני רוצה לעשות בסרטון הזה הוא להשיג הבנה טובה יותר של איך משתנות שגיאת האימות ושגיאת האימון כאשר אנחנו משנים את פרמטר ההסדרה למבדה. וכזכור, זו היתה פונקצית העלות המקורית שלנו J של תטא. אבל למטרה שלנו אנחנו נגדיר שגיאת אימון ללא שימוש בפרמטר ההסדרה, וגם שגיאת אימות ללא שימוש בפרמטר ההסדרה. ומה שאני רוצה לעשות זה לשרטט את J-אימון ואת J-cv, כלומר כמה מצליחה ההשערה על סט האימון וכמה היא מצליחה על ערכת האימות הצולב כאשר אני משנה את פרמטר ההסדרה λ. כפי שראינו קודם אם λ הוא קטן אז אנחנו בעצם לא ממש משתמשים בהסדרה ומקבלים בסיכוי גבוה התאמת-יתר, ואילו אם λ הוא גדול, אם אנחנו בחלק הימני של הציר האופקי, אז עם ערך גדול של λ אנחנו מסתכנים בהטייה, אז כשנשרטט את J-אימון ואת J-cv, מה שנמצא הוא שעבור ערכים קטנים של λ, אפשר להתאים היטב את סדרת האימון כי אנחנו לא משתמשים בהסדרה. אז עבור ערכים קטנים של λ, מונח ההסדרה בעצם נעלם, ואנחנו ממזערים פשוט את ריבועי השגיאה. אז כאשר λ הוא קטן, נקבל ערך קטן עבור J-אימון, אבל אם λ הוא גדול, אז יש לנו בעיית הטייה גבוהה, ואנחנו עלולים לא להתאים היטב לסדרת האימון, אז אנחנו מקבלים ערך גבוה כאן. אז J-אימון של θ נוטה לגדול כאשר λ גדלה, כי ערך גדול של λ מתאים להטיה גבוהה שבה אי אפשר אפילו להתאים היטב את סט האימון, בעוד ערך קטן של λ כן יתאים היטב בהנחה שאפשר באמת להתאים פולינום מסדר גבוה מאוד לנתונים. לאחר שגיאת האימות אנו מגיעים לגרף כזה, כאשר כאן בצד ימין, אם יש לנו ערך גדול של λ, אנו מקבלים התאמת-חסר, זאת אומרת אנחנו באזור ההטיה. ולכן שגיאת האימות הצולב תהיה גבוהה. אני רק אשים פה כותרת (J-cv(θ, כי כך, עם הטיה גבוהה, אנחנו לא מצליחים להתאים, לא נצליח בסדרת האימות הצולב, ואילו כאן בצד שמאל, זה אזור השונות הגבוהה, שבו מכיוון שיש לנו ערך λ קטן מדי, אז אנחנו עשויים להיות באזור התאמת-יתר לנתונים. וכך בגלל התאמת-היתר לנתונים, גם שגיאת האימות תהיה גבוהה. אז כך נראות שגיאת האימות הצולב ושגיאת האימון על סדרות האימון והאימות בזמן שאנחנו משנים את פרמטר ההסדרה λ. אז כמו שאמרנו, לעתים קרובות זה יהיה איזה ערך ביניים של λ שבדיוק נכון ושעובד הכי טוב במונחים של הקטנת השגיאה על סדרות האימות והמבחן. ובעוד הקימורים שציירתי כאן הם קצת קריקטוריים וקצת להמחשה בלבד, כי בדרך כלל על סדרות של נתונים אמיתיים, הקימורים שנקבל אולי יצאו יותר מבולגנים וטיפטיפה פחות צפויים, עבור חלק מערכות הנתונים האמיתיות באמת אפשר לראות סוגים כאלה של עליות וירידות ועל ידי הסתכלות על השגיאות של ערכות האימות לגבי ערכי λ שונים אפשר או ידנית או באופן אוטומטי לנסות לבחור נקודה שממזערת את שגיאת האימות ולבחור את הערך של λ שבו נקבל שגיאת אימות צולב נמוכה. כאשר אני מנסה לבחור את פרמטר ההסדרה λ עבור אלגוריתם למידה, אני מוצא לעתים קרובות כי ציור של גרף כמו זה המוצג כאן עוזר לי להבין טוב יותר מה קורה ומסייע לי לוודא כי אני אכן בוחר ערך טוב עבור פרמטר ההסדרה λ. אז אני מקווה שזה נתן לכם יותר תובנה לגבי ההסדרה ולגבי ההשפעות ההדדיות שלה עם ההטיה והשונות של אלגוריתם למידה. עד עכשיו כבר ראיתם הטיה ושונות מהרבה נקודות מבט שונות. מה שהיינו רוצים לעשות בסרטון הבא הוא לקחת את כל התובנות שקיבלנו ולבנות עליהם כדי ליצור שיטת אבחון שנקראת עקומות למידה, שהיא כלי שבו אני משתמש לעתים קרובות כדי לאבחן אם אלגוריתם הלמידה אולי סובל מבעיית הטיה או מבעיית שונות, או קצת משתיהם.