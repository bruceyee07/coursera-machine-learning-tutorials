בסרטון זה, אני רוצה לספר לכם על עקומות למידה. עקומות למידה הם דבר שלעתים קרובות מאוד שימושי לשרטט אותן. אם אתה רוצה בדיקה מהירה שהאלגוריתם שלך עובד כראוי, או אם אתה רוצה לשפר את הביצועים של האלגוריתם. עקומות למידה הוא כלי שאני באמת משתמש בו לעתים קרובות מאוד כדי לנסות לאבחן אם אלגוריתם הלמידה שלי אכן סובל מבעיית הטייה, שונות או גם מזה וגם מזה. אז מהי עקומת למידה. כדי לשרטט עקומת למידה, מה שאני בדרך כלל עושה הוא לשרטט את J-אימון שהוא, למשל, הממוצע של ריבוע השגיאה על סט האימון שלי או J-cv שהוא הממוצע של ריבוע השגיאה על סט האימות הצולב שלי. ואני משרטט את זה כפונקציה של m, כפונקציה של מספר דוגמאות ההכשרה שיש לי. אז m הוא בדרך כלל קבוע כמו לדוגמא אולי יש לי 100 דוגמאות אימון, אבל מה שאני עושה הוא שאני מקטין באופן מלאכותי את גודל סדרת האימון שלי. אז אני מגביל את עצמי בכוונה להשתמש רק, למשל, ב-10 או 20 או 30 או 40 דוגמאות הכשרה ומשרטט את שגיאת האימון ואת שגיאת האימות עבור הקבוצה המוקטנת הזו של דוגמאות. אז בואו נראה איך נראים הגרפים האלה. נניח שיש לי רק דוגמת אימון אחת כמו זו שמופיעה בדוגמה הראשונה כאן, ונניח שאני מתאים לה פונקציה ריבועית. אז יש לי רק דוגמת הכשרה אחת. אז ברור שאני יכול לעשות כאן התאמה מושלמת, נכון? אתם מבינים, פשוט להתאים לה פונקציה ריבועית. אז תהיה לי שגיאה של 0 על דוגמת האימון האחת הזו. מה אם יש לי שתי דוגמאות הכשרה. ובכן, גם כאן אפשר להתאים בצורה מושלמת פונקציה ריבועית. וגם אם אני משתמש בהסדרה, אני יכול כנראה להתאים אותה די טוב. ואם אני לא משתמש בהסדרה, אז נוכל לקבל התאמה מושלמת. ואם יש לי שלוש דוגמאות הכשרה, שוב, כן, אפשר להתאים להן פונקציה ריבועית בצורה מושלמת. אז אם m שווה 1 או m שווה ל-2 או m שווה 3, שגיאת האימון שלי על קבוצת האימון תהיה 0 אם אני לא משתמש בהסדרה או שהיא יכולה להיות מעט גדולה מ-0 אם אני כן משתמש בהסדרה, ודרך אגב גם אם יש לי קבוצת אימון גדולה ואני מגביל באופן מלאכותי את הגודל שלה כדי לשרטט את J-אימון. כאן אם אני קובע m שווה 3, נניח, ואני מאמן רק על שלוש דוגמאות, אז עבור הגרף הזה אני אמדוד את שגיאת האימון רק על שלוש הדוגמאות שלמעשה התאמתי את הפונקציה אליהן, וכך גם אם יש לי 100 דוגמאות אימון, אבל כשאני משרטט את שגיאת האימון על m שווה 3, מה שאני אעשה הוא למדוד את שגיאת האימון רק על שלוש הדוגמאות שאליהם התאמתי את ההשערה שלי, ולא את כל הדוגמאות האחרות שהשמטתי בכוונה מתהליך ההכשרה. אז רק כדי לסכם, מה שראינו הוא שאם גודל קבוצת האימון הוא קטן אז שגיאת האימון גם היא תהיה קטנה. כי כמובן, כשיש לנו קבוצת אימון קטנה אז יהיה קל מאוד להתאים את קבוצת האימון בצורה טובה מאוד או אולי אפילו בצורה מושלמת. עכשיו בואו נחשוב על m שווה 4 למשל. ובכן, כאן פונקציה ריבועית כבר לא יכולה להיות מתאימה מאד לנתונים, ואם m שווה 5 אז אולי פונקציה ריבועית תהיה איכשהוא מתאימה לנתונים, אבל ככל שקבוצת האימון נהיית יותר גדולה, זה הופך להיות קשה יותר ויותר להבטיח שנוכל למצוא פונקציה ריבועית שתתאים לכל הדוגמאות שלנו בצורה מושלמת. אז למעשה כשקבוצת האימון גדלה, מה שאנחנו מוצאים הוא שהממוצע של שגיאת האימון למעשה גדל, אז אם נשרטט את הגרף הזה מה שאתה מוצא הוא שהשגיאה של סדרת האימון, ממוצע השגיאה על ההשערה, גדל כש-m גדל ופשוט לחזור על זה, האינטואיציה היא כי כאשר m הוא קטן, כאשר יש לך מעט דוגמאות אימון, זה די קל להתאים כל אחת מדוגמאות האימון שלך באופן מושלם, ולכן שגיאת האימון שלך תהיה קטנה, ואילו כאשר m גדול אז זה נהיה יותר קשה להתאים התאמה מושלמת לכל דוגמאות האימון ולכן שגיאת האימון גדלה. עכשיו, מה בקשר לשגיאת האימות הצולב? ובכן, שגיאת האימות הצולב היא השגיאה על קבוצת האימות שעדיין לא נכנסו למשוואה, אז כאשר יש לנו קבוצה אימון קטנה מאוד, היא בוודאי לא תתן לנו הכללה טובה, ולכן היא לא תיתן ביצועים טובים על קבוצות אחרות, אז ההשערה הזו כאן לא תהיה טובה במיוחד, ורק כאשר אנחנו משתמשים בקבוצת אימון גדולה יותר, נתחיל לקבל השערות שאולי מתאימות לנתונים של קבוצת האימות קצת יותר טוב. אז גם שגיאת האימות וגם שגיאת המבחן נוטות לרדת כאשר מגדילים את קבוצת האימון כי ככל שיש יותר נתונים, כך נקבל השערות יותר טובות בהכללות לדוגמאות חדשות. כמה שיותר נתונים יש לך, יותר טובה ההשערה שאתה מתאים. אז אם אתה משרטט את J-אימון ואת J-cv אתה תקבל מין דבר כזה. עכשיו בואו נסתכל איך נראים עקומות הלמידה אם יש לנו או בעית הטיה גבוהה או בעית שונות גבוהה. נניח שלהשערה שלנו יש הטיה גבוהה, וכדי להסביר את זה אני הולך להשתמש בדוגמא של התאמת קו ישר לנתונים, אתם מבינים, לא ממש יכולה להיות התאמה טובה כשההשערה היא קו ישר. אז יש לנו השערה שאולי נראית כך. עכשיו בואו נחשוב מה יקרה אם נגדיל את קבוצת האימון או ההכשרה. אז אם במקום חמש דוגמאות כמו שציירתי כאן, דמיינו שיש לנו הרבה יותר דוגמאות הכשרה. אז מה קורה, אם אנחנו מתאימים את הקו הישר הזה. מה שנגלה הוא שבעצם יהיה לנו פחות או יותר אותו קו ישר. והרי ברור שהקו הזה פשוט לא יכול להתאים טוב לנתונים וגם אם יהיו לנו טונות של נתונים נוספים, הקו הישר הזה לא ישתנה הרבה. זה הקו הישר הכי טוב האפשרי להתאים לנתונים, אבל קו ישר פשוט לא יכול להתאים היטב לערכת הנתונים שלנו. אז אם נשרטט את שגיאת האימות, היא תיראה כך. כאן למעלה משמאל, אם יש לנו כבר גודל מינימלי של סדרת אימון כמו לדוגמא אולי רק דוגמת אימון אחת וזה בוודאי לא ייתן התאמה טובה, אבל כשהגענו למספר מסוים של דוגמאות אימון, אנחנו נגיע כבר כמעט לקו הישר הטוב ביותר האפשרי מבחינת ההתאמה, וגם אם בסופו של דבר נקבל קבוצת אימון הרבה יותר גדולה, ערך הרבה יותר גדול של m, בעצם נקבל מחדש את אותו קו ישר, וכך, שגיאת האימות - תרשו לי לסמן את זה בתווית - וגם שגיאת קבוצת המבחן, תשתטח או ישתטחו די מהר, לאחר שהגענו מעבר למספר מסוים של דוגמאות הכשרה, מפני שדי מהר אנחנו נתאים את הקו הישר הטוב ביותר האפשרי. ומה לגבי שגיאת האימון? ובכן, שגיאת האימון תהיה קטנה כש-m קטן. ומה שנראה במקרה של הטיה גבוהה הוא ששגיאת האימון תתקרב מאד לשגיאת האימות הצולב, כי יש לנו כל כך מעט פרמטרים וכל כך הרבה נתונים, לפחות כאשר m הוא גדול, אז הביצועים של הפונקציה על מערך ההכשרה ועל מערך האימות הצולב יהיו דומים מאוד. אז כך נראית עקומת למידה אם יש לנו אלגוריתם בעל הטיה גבוהה. ולבסוף, הבעיה עם הטיה גבוהה משתקפת גם בעובדה שהן שגיאת האימות והן שגיאת האימון הן גבוהות, ואנחנו מקבלים ערך גבוה יחסית של שתיהם, J-cv ו-J-אימון. זה גם אומר משהו מעניין מאוד, דהיינו, אם לאלגוריתם למידה יש הטיה גבוהה, ככל שאנו מקבלים יותר ויותר דוגמאות אימונים, כלומר, ככל שאנחנו עוברים ימינה על הגרף הזה, נוכל להבחין ששגיאת האימות איננה יורדת בצורה משמעותית, אלא למעשה משתטחת, ולכן אם אלגוריתם הלמידה אכן סובל מהטיה גבוהה, תוספת גרידא של נתוני אימון בפני עצמה לא תועיל למעשה בהרבה, וכמו הדוגמה שלנו בגרף מימין, בה היו לנו רק חמש דוגמאות אימון, וובנינו מהן קו ישר מסוים, אז גם כאשר היו לנו המון דוגמאות אימון נוספות, נשארנו למעשה עם בערך אותו קו ישר. ולכן אם לאלגוריתם הלמידה יש הטיה גבוהה, אז להוסיף לו הרבה יותר נתוני הכשרה לא ממש עוזר לנו להוריד את שגיאת האימות או את שגיאת המבחן. אז הידיעה אם אלגוריתם הלמידה שלך סובל מהטיה גבוהה נראית שימושית כי היא עשויה למנוע ממך מבזבוז של הרבה זמן באיסוף נתוני אימון נוספים במקום שבו זה פשוט לא יכול להועיל. עכשיו בואו ונסתכל על מצב של אלגוריתם למידה שבו עשויה להיות שונות גבוהה. בואו נסתכל קודם כל על שגיאת האימון. אם יש לך סדרת אימון קטנה מאוד כמו חמש דוגמאות ההכשרה המוצגות על הגרף בצד ימין, ואם אנחנו מנסים להתאים נניח פולינום מסדר גבוה מאוד, ואני כתבתי פולינום ממעלה מאה שבאמת אף פעם לא משתמשים בו, אבל רק לשם הדוגמה. ואם אנו משתמשים בערך קטן למדי של λ, אולי לא אפס, אבל ערך קטן למדי של λ, אז נגיע להתאמה טובה מאד של הנתונים עם הפונקציה שיש לה התאמת-יתר. אז אם גודל סדרת האימון הוא קטן, שגיאת האימון שלנו, כלומר, J-אימון של תטא יהיה קטן. וכשסדרת האימון הזו גדלה קצת, אנחנו עדיין עשויים להיות במצב התאמת-יתר לנתונים אבל זה גם נהיה קצת יותר קשה להתאים את הנתונים בצורה מושלמת, ולכן, ככל שסדרת האימון הזו גדלה, אנו מוצאים ש-J-אימון עולה, כי זה נהיה קצת יותר קשה להתאים את סדרת האימון באופן מושלם ככל שיש לנו יותר דוגמאות, אבל שגיאת האימון תהיה עדיין נמוכה למדי. עכשיו, מה עם שגיאת האימות? ובכן, במצב שונות גבוהה, ההשערה במצב התאמת-יתר ולכן שגיאת האימות תישאר גבוהה, גם כאשר אנחנו מגיעים למספר בינוני של דוגמאות הכשרה, אז אולי שגיאת האימות הצולב נראית ככה. והאינדיקציה המעידה על כך שיש לנו בעיית שונות גבוהה היא העובדה שיש הפרש גדול בין שגיאת האימון לבין שגיאת האימות הצולב. וכשמסתכלים על הגרף הזה אם אנחנו חושבים על הוספת נתונים נוספים לסדרת האימון, כלומר, אם ניקח את הגרף הזה ונלך עוד ימינה, נוכל לומר ששתי העקומות, העקומה הכחולה והעקומה הורודה, מתכנסות זו לזו. אז אם היינו מאריכים את הגרף הזה ימינה, זה נראה סביר ששגיאת האימון תמשיך לעלות ושגיאת האימות הצולב תמשיך לרדת. והדבר שבאמת אכפת לנו הוא שגיאת האימות הצולב ושגיאת ערכת המבחן, נכון? אז בסוג כזה של גרף, אנחנו יכולים לומר שאם אנחנו נמשיך ונוסיף דוגמאות אימון ונמשיך את הגרף ימינה, אז שגיאת האימות הצולב תמשיך לרדת. ולכן, במצב של שונות גבוהה, הוספת נתוני אימון אכן עשויה לעזור. אז כמו מקודם זה נראה שימושי לדעת אם אלגוריתם הלמידה שלך סובל מבעיית שונות גבוהה, כי זה אומר לך, למשל, שזה כן עשוי להיות שווה להשקיע את הזמן כדי לראות אם אתה יכול ללכת ולמצוא עוד קצת נתוני אימון. עכשיו, בשקופית הקודמת ובשקופית הזאת, אני שרטטתי גרפים עם עקומות די נקיות ואידיאליות. אם תשרטט עקומות כאלה עבור אלגוריתם למידה אמיתי, לפעמים אתה באמת תראה עקומות די דומות למה שציירתי כאן. אבל הרבה פעמים תראה עקומות שהם קצת יותר רועשות ופחות מסודרות יפה מאלה. אבל השרטוט של עקומות למידה כאלה יכול לעתים קרובות לומר לך, יכול לעתים קרובות לעזור לך להבין אם אלגוריתם הלמידה שלך סובל מהטיה או שונות או אפילו קצת משתיהם. אז כשאני מנסה לשפר את הביצועים של אלגוריתם למידה, אחד הדברים שאני כמעט תמיד עושה הוא לשרטט עקומות למידה כאלה, ובדרך כלל זה ייתן לך מושג טוב יותר אם יש בעיית הטיה או שונות. ובסרטון הבא נראה כיצד זה יכול לעזור להצביע על פעולות ספציפיות שכדאי לנקוט בהם או לא לנקוט בהם כדי לנסות לשפר את הביצועים של אלגוריתם הלמידה שלך.