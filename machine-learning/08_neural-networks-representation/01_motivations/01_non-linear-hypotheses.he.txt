בסירטון הנוכחי ובסירטונים הבאים, אני מעוניין לספר לכם על אלגוריתם למידה הנקרא "רשת נוירונים" או "רשת עצבית". תחילה, אדבר על הייצוג, ולאחר מכן במקבץ הסירטונים שאחריו, נדבר על אלגוריתמי למידה המיישמים את הרשת העצבית. רשת נוירונים היא למעשה רעיון די ותיק, אבל הוא נעזב לזמן מה. אבל כיום זהו אחד הרעיונות החדשניים המשמש בטכניקות רבות בפתרון בעיות ביישומי למידת מכונה. מדוע אנו זקוקים לאלגוריתם למידה נוסף? כבר יש לנו רגרסיה ליניארית, כבר יש לנו רגרסיה לוגיסטית, אז למה צריך גם רשת עצבית? על מנת להתניע את הדיון על רשת נוירונים, אציג בפניכם מספר דוגמאות לבעיות בנושא למידת מכונה שבהן אנו צריכים ללמוד השערות לא-ליניאריות מורכבות. לצורך ההדגמה, ניקח בעיית למידה מונחית כאשר ערכת האימון נראית כך. אם נרצה ליישם רגרסיה ליניארית על הבעיה הזו, דבר אחד שנוכל לבצע הוא יישום של רגרסיה לוגיסטית עם מאפיינים בלתי ליניארים רבים כמו כאן מימין, אז הנה לדוגמה, g היא פונקציית סיגמואיד, ונוכל להוסיף איברים פולינומיאלים נוספים, אם נוסיף מספיק איברים פולינומיאלים, אז אולי נצליח להשיג השערה שמפרידה בין הדוגמאות החיוביות והשליליות. השיטה הזו עובדת היטב כאשר מדובר רק בשני צירים x1 ו-x2 מכיוון שכך נוכל להכניס את כל הביטויים הפולינומיים של x1 ו-x2. אבל בשביל הרבה בעיות בלמידה חישובית, יש לנו הרבה יותר משני מאפיינים. אנחנו מדברים זה זמן מה על חיזוי מחירי דיור. נניח שיש לנו בעיית סיווג של דיור ולא בעיית רגרסיה, כמו למשל מצב שבו יש לנו מאפיינים שונים של הבית, ואנחנו רוצים לחזות את הסיכוי שהבית יימכר בחצי השנה הקרובה. אז זוהי בעיית סיווג. כפי שראינו, אנחנו יכולים להמציא די הרבה מאפיינים, אולי אפילו מאה מאפיינים שונים של בתים שונים. בשביל בעיה כזו, אם נרצה להוסיף את כל המשתנים הפולינומיים, אפילו רק את הריבועיים, הפולינומיים מסדר שני, יהיו לנו המון משתנים. יהיו לנו ביטויים כמו x1 בריבוע, x1x2, x1x3, x1x4 וכן הלאה עד x1x100 ואז x2 בריבוע, x2x3 וכן הלאה. ואם נכלול רק את הרכיבים מהסדר השני, כלומר רכיבים שהמכפלה ביניהם מביאה לתוצאה ריבועית של שני הרכיבים, x1 כפול x1, וכן הלאה. עבור המקרה של n=100, יהיו לנו בערך 5,000 משתנים מסוגים שונים. מבחינה אסימפטוטית, מספר המשתנים הריבועיים גדל בערך בסדר גודל של n בריבוע. כאשר n מייצג את כמות המשתנים המקורית, מ-x1 עד x100. ומספר זה קרוב יותר ל-n בריבוע חלקי 2. אם ניקח את כל המשתנים הריבועיים, נראה שלא מדובר ברעיון כל כך טוב. יש יותר מדי משתנים. מספר כזה של ביטויים יגרום להתאמת-יתר וחוץ מזה עלות החישוב עלולה להיות גבוהה. שכן מדובר בכמות מאסיבית של משתנים. דבר אחד שניתן לעשות, הוא להשתמש רק בתת-קבוצה של הביטויים, לדוגמא אם נשתמש רק במשתנים x1 בריבוע, x2 בריבוע, x3 בריבוע, עד x100 בריבוע, ואז מספר המשתנים יהיה קטן משמעותית. כאן יש לנו רק 100 משתנים ריבועיים, אבל זה לא מספיק משתנים והחישוב לא יוביל לתוצאה כמו בגרף בצד שמאל למעלה. למעשה, אם נכניס את המשתנים הריבועיים יחד עם x1 המקורי, וכן הלאה עד x100, אז לא נוכל להתאים השערות ממש מעניינות. נוכל להתאים דברים כמו אליפסות מאונכות לצירים, לדוגמא, אבל בהחלט לא נוכל להתאים צורה כמו זו שבגרף. 5000 משתנים זה נשמע כמו הרבה. אם נכליל גם את החזקה השלישית, הפולינומים מסדר שלישי, ה-x1*x2*x3 וגם x1 בריבוע כפול x10, x2 כפול x17 כפול x11 וכיוצא באלה, אתם יכולים לתאר לעצמכם שיהיו לנו כאן המון משתנים. למעשה, מספר המשתנים יהיה בסדר גודל של n בשלישית, ואם n הוא 100 אז ניתן לחשב שנגיע לסדר גודל של בערך 170,000 (102 מעל 3) משתנים מסדר שלישי. והמשתנים הפולינומיים מסדר גבוה, כאשר מספר התכונות המקורי הוא גדול, מספרם הכולל יהיה גדול מאד, זה מגדיל באופן משמעותי את מרחב התכונות, זו לא נראית כמו דרך טובה להשיג משתנים נוספים שבעזרתם נוכל לבנות גבול לא-ליניארי כאשר n הוא גדול. עבור בעיות רבות של למידת מכונה, n יהיה עצום. הנה דוגמה. בואו נחשוב על בעיית ראייה ממוחשבת. ונניח שנרצה להשתמש בלמידת מכונה כדי לאמן יכולות סיווג כדי לבחון תמונה, ולספר לנו האם מדובר בתמונה של מכונית או לא. אנשים רבים תוהים מדוע ראייה ממוחשבת הינה עניין מסובך. כשאתם ואני מסתכלים על התמונה, היא ברורה לנו כשמש. הם תוהים כיצד אלגוריתם למידה יכול בכלל להיכשל בלהבין מה מייצגת התמונה. על מנת להבין למה ראייה ממוחשבת היא מסובכת, הבה נתמקד בחלק קטנטן של התמונה. האזור שמסומן בריבוע אדום, על דלת המכונית. מסתבר שכאשר אתם ואני רואים מכונית, המחשב רואה את זה. הוא רואה מטריצה של נתונים, ייצוג של עוצמה של פיקסלים, ערכים שמפרטים את הבהירות של כל פיקסל בתמונה. בעיית הראייה הממוחשבת היא להסתכל על טבלת בהירות הפיקסלים הזו, ולהסיק שהמספרים הללו מייצגים ידית של דלת מכונית. באופן קונקרטי, כשמשתמשים בלמידת מכונה כדי לבנות מערכת המזהה מכוניות, אנו בונים למערכת ערכת אימון המכילה מספר דוגמאות לתמונות שבחלקן יש מכונית ובחלקן מצולמים דברים שהם לא מכוניות. נביא את ערכת האימון לאלגוריתם הלומד כדי לאמן את יכולות הסיווג שלו. נבחן את יכולותיו, נראה לו תמונה חדשה ונשאל "מה אתה רואה?" בתקווה שהאלגוריתם יזהה את המכונית. כדי להבין למה אנו זקוקים להשערות בלתי ליניאריות, הבה נסתכל על התמונות שאנחנו מביאים לאלגוריתם: תמונות של מכוניות ושל לא-מכוניות כחלק מערכת האימון. בואו ניקח מיקומים של מספר פיקסלים בתמונה, לדוגמא, זהו המיקום של פיקסל 1 וזהו המיקום של פיקסל 2. נתווה את המכונית הזו במיקום של אחת הנקודות, כתלות בעוצמה של פיקסל 1 ופיקסל 2. ונעשה את זה גם לגבי עוד כמה תמונות. ניקח דוגמה של מכונית אחרת ונסתכל על אותו המיקום של שני הפיקסלים הקודמים בתמונה זו ישנה עצימות שונה עבור פיקסל 1 ועצימות שונה עבור פיקסל 2. מה שאומר שהם נמצאים במקומות שונים על הגרף. כעת נחקור תמונות שגויות מתוך ערכת האימון. כאן אין מכונית, וגם כאן אין. אם נעשה זאת עם הרבה דוגמאות בעודנו מסמנים מכוניות בחיוב, ומסמנים העדר מכונית בשלילה, נגלה כי המכוניות והלא-מכוניות ימצאו באזורים שונים במרחב, ולכן אנחנו נצטרך השערה בלתי ליניארית כדי לנסות להפריד בין המחלקות שנוצרו. מהו המימד של מרחב התכונות? נניח שנשתמש בתמונות בגודל 50x50 פיקסלים בלבד. מדובר בתמונות די קטנות, רק 50 פיקסלים לאורך ולרוחב. סה"כ 2500 פיקסלים לתמונה. וכך המימד של המשתנה שמייצג גודל יהיה n = 2500, כאשר וקטור המשתנים x הוא רשימה של כל בהירויות הפיקסלים, בהירות פיקסל 1, בהירות פיקסל 2, וכן הלאה עד לבהירות של הפיקסל האחרון. כאשר הייצוג הממוחשב של כל אחד מהם נע בין המספרים 0 עד 255 עבור תמונות של דרגות אפור, כלומר בשחור לבן. אז n שלנו שווה ל-2500, ואם היינו משתמשים בתמונות צבעוניות- RGB עם ערכי אדום, ירוק וכחול התמונות היו מקבלות עוצמות שונות לכל צבע וה-n שלנו היה שווה 7500. אם היינו מנסים לפתח השערה בלתי לינארית בכך שנכליל את כל המשתנים הריבועיים, כלומר את כל סוגי xi כפול xj, כאשר ברשותנו 2500 פיקסלים, אז יהיו לנו 3 מליון משתנים. וזה כבר יותר מדי, זה כבר לא סביר. החישוב יהיה יקר מאוד. וזה לא הגיוני להציג 3 מליון משתנים לכל תמונת אימון. שימוש ברגרסיה לוגיסטית פשוטה בשילוב עם משתנים ריבועיים או מסדר שלישי זו לא דרך טובה להשיג השערה בלתי ליניארית מורכבת כאשר n גדול כל כך. נוצרים יותר מדי משתנים. בסירטונים הבאים, אספר לכם על הרשת העצבית, שהיא דרך טובה יותר לשער השערות מורכבות בלתי ליניאריות. גם כאשר וקטור המשתנים הוא ענק, גם אם n גדול. באותה הזדמנות אראה לכם מספר סירטונים משעשעים על יישומים חשובים היסטורית של רשתות עיצביות. אני מקווה שהסירטונים הללו יהיו משעשעים גם עבורכם.