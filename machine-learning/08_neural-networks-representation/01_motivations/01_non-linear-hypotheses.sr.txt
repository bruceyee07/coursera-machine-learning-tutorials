U ovom i u sledećim video snimcima, pričaću o algoritmu učenja koji se zove Neuronska mreža. Prvo ćemo pričati o njegovoj reprezentaciji, a zatim ćemo u narednim video snimcima pričati i o samom algoritmu. Neuronska mreža je zapravo poprilično stara ideja, ali je neko vreme bila zapostavljena. Međutim, danas je ona najsavremenija tehnika za rešavanje mnogih problema mašinskog učenja. Zašto nam je potreban i ovaj novi algoritam učenja? Već znamo za linearnu regresiju i logističku regresiju, i zašto nam je
 onda potrebna i neuronska mreža? Da bih motivisao diskusiju o neuronskim mrežama, počeću sa prikazom nekih primera mašinskog učenja za koje je potrebno naći kompleksnu nelinearnu hipotezu. Razmotrimo problem nadgledane klasifikacije za koji imamo ovakav skup
 podataka za treniranje modela. Ukoliko želite da primenite logističku regresiju na ovaj problem, mogli biste da primenite logističku regresiju sa mnogo nelinearnih promenljivih,
 kao što je ovde prikazano. Ovde je g, kao i obično, sigmoidna funkcija i možemo uključiti puno polinomskih članova,
 kao što su ovi. I, ukoliko uključite dovoljno
 polinomskih članova, onda ćete, znate, možda dobiti hipotezu koja odvaja pozitivne od negativnih primera. Ovaj metod je dobar ukoliko imate samo, na primer, dve promenljive - x1 i x2 jer tada možete uključiti sve ove polinomske članove x1 i x2. Međutim, mnogi interesantni problemi mašinskog učenja imaju mnogo više promenljivih. Već neko vreme pominjemo predviđanje cena nekretnina,
 pa pretpostavimo da razmatramo problem klasifikacije kuća, a ne problem regresije, kao na primer da imate različite promenljive vezane za kuću, i da želite da predvidite koja je verovatnoća da će kuća biti prodata u sledećih šest meseci, tako da imate
 klasifikacioni problem. I, kao što smo videli, možemo osmisliti mnogo promenljivih, možda čak i stotinu različitih promenljivih za različite kuće. Za ovakav problem, ako uključite sve kvadrate tih promenljivih, čak i ako uključimo samo kvadrate, odnosno druge stepene, bilo bi ih previše. Imali bismo članove kao što su
 x1 na kvadrat, x1x2, x1x3, znate, x1x4 i tako sve do x1x100, pa onda imamo x2 na kvadrat, x2x3 i tako dalje. Dakle, ako uključite samo druge stepene, odnosno članove koji su proizvodi, znate, dve promenljive, na primer x1 puta x1 i tako dalje,
onda za slučaj kada je n=100, imaćete oko 5000 članova. I, asimptotički, broj kvadrata promenljivih raste otprilike brzinom reda
n na kvadrat, gde je n broj originalnih promenljivih, x1 do x100 od kojih smo krenuli. Zapravo, to je bliže n na kvadrat
podeljeno sa 2. Dakle, uključivanje svih kvadrata promenljivih ne deluje baš kao da je možda dobra ideja, jer dobijamo previše promenljivih i onda ćemo možda preprilagoditi model podacima za treniranje, i možda će biti skupo za računanje, znate, sa toliko mnogo promenljivih. Jedna stvar koju možete uraditi je da uključite samo podskup promenljivih, tako da
ako uključite samo promenljive x1 na kvadrat,
x2 na kvadrat, x3 na kvadrat, sve do možda x100 na kvdarat, onda će broj promenljivih
 biti mnogo manji. Ovde imate samo 100 članova drugog stepena, ali to nije dovoljno promenljivih i svakako neće biti dovoljno za prilagođavanje podacima kao
na slici gore levo. Zapravo, ako uključimo samo ove druge stepene,
zajedno sa polaznim promenljivama, x1 i tako dalje do x100, onda nije moguće prilagoditi model interesantnim hipotezama.
Tako da, možete ga prilagoditi, znate,
modelima za koje je dovoljna elipsa,
kao ova, ali svakako ga ne možete prilagoditi kompleksnim podacima,
poput ovih na slici. Dakle, 5000 promenljivih se čini kao mnogo, ako uključite treće stepene, ili članove trećeg reda promenljivih, x1x2x3, znate, x1 na kvadrat, x2, x10x11x17 i tako dalje. Možete zamisliti koliko će puno ovakvih članova biti. Zapravo, njihov broj će biti reda n na treći i ako je n=100 možete izračunati, imaćete broj reda oko 170 000 takvih članova trećeg reda i ako uključimo ove članove višeg reda kada je broj polaznih promenljivih, n, veliki, to zaista dramatično povećava prostor promenljivih i to više ne izgleda kao dobar način da se dođe do dodatnih članova za pravljenje nelinearnog klasifikatora kada je n veliko. Za mnoge probleme mašinskog učenja,
n će biti veoma veliko. Evo primera. Razmotrimo problem računarskog vida. Pretpostavimo da želite da iskoristite mašinsko učenje da naučite klasifikator da analizira sliku i kaže nam da li je to slika automobila ili nije. Mnogi se pitaju zašto su problemi
računarskog vida tako teški. Kada Vi i ja pogledamo ovu sliku,
očigledno nam je šta je na njoj. Pitate se kako je moguće da algoritam za učenje može da ne prepozna šta je na slici. Da bismo razumeli zašto je to tako teško, uvećajmo ovaj mali deo slike na kome je mali crveni pravougaonik. Ispostavlja se da iako Vi i ja vidimo automobil,
kompjuter vidi ovo. On vidi ovakvu matricu, ili tabelu brojeva koji predstavljaju intenzitet piksela koji nam kaže koliko je svaki piksel na slici svetao. Problem računarskog vida je da se iz ove matrice intenziteta piksela zaključi da ti brojevi predstavljaju kvaku na kolima. Konkretno, kada koristimo mašinsko učenje za izgradnju detektora automobila, mi zapravo pravimo obeleženi skup podataka za treniranje modela sa recimo nekoliko obeleženih primera automobila i par obeleženih primera nekih stvari koje nisu automobili i onda taj skup podataka iskoristimo za treniranje algoritma za učenje, dobijamo klasifikator i onda ga, znate, možete testirati i prikazati mu novu sliku i pitati
"Šta je na ovoj slici?" i, nadajmo se, on će prepoznati da je to automobil. Da biste razumeli zašto nam trebaju nelinearne hipoteze, hajde da pogledamo neke od slika automobila i možda nečega što nisu automobili koje ćemo prikazati
našem algoritmu za učenje. Izaberimo par lokacija piksela na slikama, na primer ovo je lokacija prvog piksela, ovo je lokacija rugog piksela i hajde da nacrtamo ovaj auto, znate, kao lokaciju, kao određenu tačku koja zavisi od intenziteta piksela 1 i piksela 2. I uradimo to isto sa još nekim slikama. Pogledajmo drugi primer automobila i, znate, odredimo iste te dve lokacije piksela i ova slika ima drugačiji intenzitet piksela 1 i drugačiji intenzitet piksela 2. Tako da je ona na nekom drugom mestu na grafiku. I docrtajmo neke negativne primere, takođe. Ovo nije auto, i ovo nije auto. Ako uradimo ovo za sve više i više primera, gde plusevi označavaju automobile, a minusevi označavaju sve što nije automobil, otkrićemo da se automobili i sve ostalo nalaze u različitim delovima ovog prostora i ono što nam treba je neka vrsta nelinearne hipoteze koja će razdvojiti te dve klase. Koje je dimenzije prostor promenljivih? Ako bismo koristili slike od samo 50 puta 50 piksela. Dakle, pretpostavimo da su naše slike veoma male, samo 50 piksela po dužini. Tada imamo 2500 piksela, i onda je dimenzija našeg prostora promenljivih n=2500, gde je naš vektor promenljivih, x, lista intenziteta svih piksela, znate, intenzitet piksela 1, intenzitet piksela 2 i tako dalje sve do intenziteta poslednjeg piksela gde, znate, uobičajena kompjuterska reprezentacija svakog od njih mogu biti vrednosti od 0 do 255 ako koristimo grayscale, tj. crno-bele slike. Dakle, imamo da je n=2500 i to ako koristimo crno-bele slike. Ako koristimo slike u boji one imaju RGB, vrednosti za crvenu, zelenu i plavu nijansu piksela, pa je n=7500. Tako da, ako pokušamo da dođemo do nelinearne hipoteze uključujući sve članove drugog reda, odnosno sve članove koji su oblika, znate, xi puta xj, sa 2500 piksela imali bismo ukupno 3 miliona promenljivih. I to je jednostavno previše, nije razumno jer bi izračunavanje bilo preskupo da otkrijemo i predstavimo svaku od ovih 3 miliona promenljivih za svaki primer
iz skupa za treniranje. Dakle, obična logistička regresija, zajedno sa uključivanjem možda članova drugog ili trećeg reda, to jednostavno nije dobar način za dobijanje nelinearne hipoteze kada je n veliko jer imamo previše promenljivih. U narednih nekoliko snimaka pričaću o neuronskim mrežama za koje se ispostavlja da su mnogo bolje za nalaženje kompleksnih hipoteza, kompleksnih nelinearnih hipoteza, čak i kada je prostor polaznih promenljivih,
odnosno n, veliko. Usput ću Vam, takođe, pokazati i nekoliko interesantnih snimaka istorijski važnih primena neuronskih mreža i nadam se daće ti snimci koje ćemo videti kasnije, i Vama biti zabavni.