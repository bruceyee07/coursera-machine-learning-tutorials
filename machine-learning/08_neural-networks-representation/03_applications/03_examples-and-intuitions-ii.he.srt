1
00:00:00,450 --> 00:00:04,310
בסרטון הזה אני רוצה להמשיך ולהתקדם בדוגמה שלנו כדי להראות כיצד

2
00:00:04,310 --> 00:00:07,972
רשת עצבית יכולה לחשב היפותזה מורכבת לא ליניארית.

3
00:00:10,160 --> 00:00:13,460
בסרטון האחרון ראינו כיצד ניתן להשתמש ברשת עצבית כדי

4
00:00:13,460 --> 00:00:17,640
לחשב את הפונקציות x1 AND x2, ואת הפונקציה x1 OR x2

5
00:00:17,640 --> 00:00:22,960
כאשר x1 ו-x2 הם בוליאניים, כלומר כאשר הם מקבלים ערכים 0,1.

6
00:00:22,960 --> 00:00:27,270
אנחנו יכולים גם לעשות רשת שמחשבת שלילה,

7
00:00:27,270 --> 00:00:30,949
כלומר שמחשבת את הפונקציה NOT x1.

8
00:00:30,949 --> 00:00:33,465
תרשו לי רק לרשום את המשקלות הקשורות לרשת הזו.

9
00:00:33,465 --> 00:00:38,490
יש לנו רק תכונת קלט אחת x1 במקרה הזה ויחידת הטיה 1+.

10
00:00:38,490 --> 00:00:43,652
ואם נשייך להם משקולות פלוס 10 ומינוס 20,

11
00:00:43,652 --> 00:00:50,516
אז ההשערה מחשבת את הביטוי (h(x) = g(10-20x1.

12
00:00:50,516 --> 00:00:55,529
אז כאשר x1 שווה ל-0, ההשערה שלנו

13
00:00:55,529 --> 00:01:00,808
תהיה (g(10-20x0 שהערך שלה הוא הסיגמואיד של 10.

14
00:01:00,808 --> 00:01:04,358
אז זה בערך 1, וכאשר x שווה 1,

15
00:01:04,358 --> 00:01:08,470
זה יהיה (10-)g שזה שווה בערך ל-0.

16
00:01:08,470 --> 00:01:14,550
ואם תסתכלו על ערכי התוצאה האלה, תראו שהם בעצם הפונקציה NOT x1.

17
00:01:14,550 --> 00:01:18,680
בשביל לכלול את רעיון השלילה, הרעיון הכללי הוא לשים

18
00:01:18,680 --> 00:01:22,910
משקל שלילי גדול כמשקל של המשתנה שאותו רוצים לשלול.

19
00:01:22,910 --> 00:01:25,414
20- כפול x1,

20
00:01:25,414 --> 00:01:30,520
וזה הרעיון הכללי של איך אפשר להגיע לשלילה של x1.

21
00:01:30,520 --> 00:01:34,686
אז נעבור לדוגמה שאני מקווה שאתם יכולים להבין בעצמכם בקלות,

22
00:01:34,686 --> 00:01:38,796
אם נרצה לחשב את הפונקציה NOT x1 AND NOT x2,

23
00:01:38,796 --> 00:01:44,016
חלק מהפתרון יהיה כנראה לשים משקלים שליליים גדולים על x1

24
00:01:44,016 --> 00:01:46,790
ו-x2, אבל זה צריך להיות אפשרי

25
00:01:46,790 --> 00:01:53,314
לגרום לרשת עצבית עם יחידת פלט אחת בלבד לחשב גם את זה.

26
00:01:53,314 --> 00:01:58,018
בסדר, אז הפונקציה הלוגית, NOT x1 AND

27
00:01:58,018 --> 00:02:02,390
NOT x2, תהיה שווה 1 אם ורק אם

28
00:02:06,531 --> 00:02:09,710
x1 שווה x2 שווה 0.

29
00:02:09,710 --> 00:02:13,900
מכיוון שזו פונקציה לוגית, המשמעות של NOT x1 היא ש-x1 חייב

30
00:02:13,900 --> 00:02:17,820
להיות 0 והמשמעות של NOT x2, היא שגם x2 חייב להיות שווה ל-0.

31
00:02:17,820 --> 00:02:22,180
אז הפונקציה הלוגית הזו שווה 1 אם ורק אם הן x1

32
00:02:22,180 --> 00:02:26,540
והן x2 שווים ל-0 ואני מקווה שאתם כבר מסוגלים להבין איך

33
00:02:26,540 --> 00:02:29,980
לבנות רשת עצבית קטנה כדי לחשב גם את הפונקציה הלוגית הזאת.

34
00:02:33,470 --> 00:02:38,102
עכשיו, כשניקח ונחבר את שלוש החתיכות שכבר הרכבנו, הרשת

35
00:02:38,102 --> 00:02:43,460
לחישוב x1 AND x2, והרשת לחישוב NOT x1 AND NOT x2.

36
00:02:43,460 --> 00:02:48,070
והרשת האחרונה לחישוב x1 OR x2, אנחנו צריכים להיות מסוגלים

37
00:02:48,070 --> 00:02:53,840
לחבר את שלושת החלקים כדי לחשב את הפונקציה הלוגית x1 XNOR x2.

38
00:02:53,840 --> 00:02:57,140
ותנו לי להזכיר לכם אם כאן בתרשים אלה x1 ו-x2, 

39
00:02:57,140 --> 00:03:01,870
לפונקציה הזאת שאנחנו רוצים לחשב יש דוגמאות שליליות כאן

40
00:03:01,870 --> 00:03:04,490
וכאן, ודוגמאות חיוביות שם ושם.

41
00:03:04,490 --> 00:03:07,470
ולכן ברור שיהיה צורך בגבולות החלטה

42
00:03:07,470 --> 00:03:10,829
לא ליניאריים כדי להפריד בין הדוגמאות החיוביות והשליליות.

43
00:03:12,980 --> 00:03:14,330
אז בואו נשרטט את הרשת.

44
00:03:14,330 --> 00:03:20,740
אני לוקח את הקלטים 1+, x1, ו-x2 וליצור את היחידה החבויה הראשונה שלי כאן.

45
00:03:20,740 --> 00:03:25,000
אני אקרא לה a(2)1 כי זו היחידה החבויה הראשונה שלי.

46
00:03:25,000 --> 00:03:29,586
ואני אעתיק את המשקלות מהרשת האדומה, הרשת של x1 AND x2.

47
00:03:29,586 --> 00:03:35,120
אז 30-, 20, 20.

48
00:03:35,120 --> 00:03:40,790
עכשיו אני יוצר יחידה נסתרת שניה שלה אני אקרא a(2)2.

49
00:03:40,790 --> 00:03:42,880
זוהי היחידה הנסתרת השנייה בשכבה שתיים.

50
00:03:42,880 --> 00:03:47,320
ואני אעתיק את המשקלות מהרשת התכולה שבמרכז כאן, 

51
00:03:47,320 --> 00:03:52,180
אז נקבל משקלים 10, 20-, 20-.

52
00:03:52,180 --> 00:03:56,120
ועכשיו בואו נסתכל על כמה מהערכים בטבלת האמת.

53
00:03:56,120 --> 00:04:00,664
לגבי הרשת האדומה, אנחנו יודעים שהיא מחשבת את הפונקציה x1 AND x2,

54
00:04:00,664 --> 00:04:04,649
ולכן נקבל כאן ערכים קרובים ל-1 0 0 0,

55
00:04:04,649 --> 00:04:10,429
לפי הערכים של x1 ו-x2, ולגבי a(2)2, הרשת התכולה,

56
00:04:10,429 --> 00:04:11,190
מה אנחנו יודעים?

57
00:04:11,190 --> 00:04:16,000
זו הפונקציה NOT x1 AND NOT x2, שהערכים שלה הם 0 0 0 1,

58
00:04:16,000 --> 00:04:17,470
עבור 4 האפשרויות של ערכים של x1 ו-x2.

59
00:04:18,480 --> 00:04:24,875
ועכשיו סופסוף, אני עומד ליצור את צומת הפלט שלנו, יחידת הפלט שהיא a(3)1.

60
00:04:24,875 --> 00:04:31,971
זוהי שכבת הפלט (h(x ואני אעתיק את רשת ה-OR בשביל זה.

61
00:04:31,971 --> 00:04:35,588
אז אני צריך יחידת הטייה, 1+ כאן, אז נצייר את זה,

62
00:04:35,588 --> 00:04:38,839
ואני כמובן אעתיק את המשקלות מהרשת הירוקה.

63
00:04:38,839 --> 00:04:44,932
אז זה 10-, 20, 20 ואנחנו יודעים מהסרטון הקודם שזה מחשב את הפונקציה OR.

64
00:04:46,682 --> 00:04:48,972
אז עכשיו בואו נמלא את טבלת האמת.

65
00:04:50,292 --> 00:04:55,209
אז הערך הראשון הוא אפס OR אחת שהוא 1, הבא הוא 0 OR אפס

66
00:04:55,209 --> 00:05:00,755
שהוא 0, 0 OR אפס שהוא 0, 1 OR אפס שיוצא 1.

67
00:05:00,755 --> 00:05:06,828
ולכן (h(x שווה ל-1 כאשר x1 ו-x2 הם שניהם אפס או

68
00:05:06,828 --> 00:05:12,251
כאשר x1 ו-x2 הם שניהם 1 ו-(h(x מוציא 1 למעשה

69
00:05:12,251 --> 00:05:18,019
בדיוק בשני המקומות האלה ואחרת הוא מוציא 0.

70
00:05:19,100 --> 00:05:22,520
אז זו רשת עצבית שיש לה שכבת קלט,

71
00:05:22,520 --> 00:05:25,760
שכבה מוסתרת אחת, ושכבת פלט,

72
00:05:25,760 --> 00:05:31,640
אנחנו מקבלים גבול החלטה לא ליניארי שמחשב את הפונקציה XNOR.

73
00:05:31,640 --> 00:05:35,154
וההבנה היותר כללית היא שבשכבת הקלט,

74
00:05:35,154 --> 00:05:36,954
יש לנו פשוט קלטים.

75
00:05:36,954 --> 00:05:38,302
ואז יש לנו שכבה מוסתרת,

76
00:05:38,302 --> 00:05:42,123
שמחשבת כמה פונקציות קצת יותר מורכבות על הקלטים כפי שמוצג כאן

77
00:05:42,123 --> 00:05:44,790
שהן פונקציות קצת יותר מסובכות.

78
00:05:44,790 --> 00:05:45,730
ואז על ידי הוספת

79
00:05:45,730 --> 00:05:49,490
שכבה נוספת אנו מקבלים פונקציה מורכבת ואפילו יותר בלתי ליניארית.

80
00:05:50,510 --> 00:05:54,743
וזו ההבנה לגבי מדוע רשתות עצביות יכולות לחשב

81
00:05:54,743 --> 00:05:56,829
פונקציות מסובכות למדי.

82
00:05:56,829 --> 00:06:00,251
שכשיש לנו שכבות מרובות ויש לנו פונקציה פשוטה יחסית

83
00:06:00,251 --> 00:06:02,190
על הקלט של השכבה השנייה.

84
00:06:02,190 --> 00:06:06,040
אז אפשר לבנות שכבה שלישית ולחשב בה פונקציות עוד יותר מורכבות,

85
00:06:06,040 --> 00:06:09,360
ואם צריך אפשר להויף עוד שכבה שתוכל לחשב פונקציות אפילו עוד יותר מורכבות.

86
00:06:10,390 --> 00:06:11,520
כדי לסכם את הסרטון,

87
00:06:11,520 --> 00:06:15,460
אני רוצה להראות לכם דוגמה נחמדה של יישום של רשת עצבית

88
00:06:15,460 --> 00:06:20,680
שבה רואים את האינטואיציה הזו ששכבות נוספות יכולות לחשב תכונות מורכבות יותר.

89
00:06:20,680 --> 00:06:23,565
אני רוצה להראות לכם סרטון של לקוח שהוא חבר טוב שלי

90
00:06:23,565 --> 00:06:24,925
יאן לקון.

91
00:06:24,925 --> 00:06:28,870
יאן הוא פרופסור באוניברסיטת ניו יורק, NYU

92
00:06:28,870 --> 00:06:32,550
והוא היה אחד החלוצים הראשונים של מחקר רשתות עצביות

93
00:06:32,550 --> 00:06:36,930
והוא מעין אגדה בתחום עכשיו והרעיונות שלו משמשים

94
00:06:36,930 --> 00:06:40,459
בכל מיני מוצרים ויישומים ברחבי העולם עכשיו.

95
00:06:41,470 --> 00:06:45,730
אז אני רוצה להראות לכם וידאו מתוך עבודה מוקדמת שלו שבו הוא השתמש

96
00:06:45,730 --> 00:06:51,400
ברשת עצבית לזהות כתב יד, לעשות זיהוי ספרות מכתב יד.

97
00:06:51,400 --> 00:06:54,940
אולי אתם זוכרים מוקדם בקורס הזה, בתחילת הקורס אמרתי

98
00:06:54,940 --> 00:06:59,000
שאחת ההצלחות המוקדמות ביותר של רשתות עצביות היה הניסיון להשתמש בהם כדי לקרוא קודי

99
00:06:59,000 --> 00:07:03,890
מיקוד כדי לסייע לשירות הדואר האמריקאי, לקרוא מיקודים.

100
00:07:03,890 --> 00:07:05,460
אז זה אחד הניסיונות,

101
00:07:05,460 --> 00:07:09,400
זהו אחד האלגוריתמים בהם השתמשו כדי לנסות לטפל בבעיה הזו.

102
00:07:09,400 --> 00:07:12,480
בסרטון שאני אראה לכם, האזור הזה כאן

103
00:07:12,480 --> 00:07:17,840
הוא אזור קלט המציג תו כתוב בכתב-יד שמוצג לרשת.

104
00:07:17,840 --> 00:07:21,872
העמודה זו מראה הדמיה של התכונות המחושבות

105
00:07:21,872 --> 00:07:23,324
על ידי השכבה הנסתרת הראשונה של הרשת.

106
00:07:23,324 --> 00:07:27,142
אז זו השכבה החבויה הראשונה של הרשת והשכבה החבויה הראשונה,

107
00:07:27,142 --> 00:07:29,685
ההדמיה הזו מראה תכונות שונות.

108
00:07:29,685 --> 00:07:32,355
שפות שונות וקווים שונים וכן הלאה שזוהו.

109
00:07:32,355 --> 00:07:35,555
העמודה הבאה היא הדמיה של השכבה המוסתרת הבאה.

110
00:07:35,555 --> 00:07:39,175
זה קצת יותר קשה לראות, קשה יותר להבין את השכבות העמוקות יותר,

111
00:07:39,175 --> 00:07:42,585
וזו ויזואליזציה של מה שמחשבת השכבה הסמויה הבאה.

112
00:07:42,585 --> 00:07:45,505
אתם מן הסתם מתקשים לראות מה קורה הרבה

113
00:07:45,505 --> 00:07:47,785
מעבר לשכבה החבויה הראשונה,

114
00:07:47,785 --> 00:07:53,410
אבל אז בסופו של דבר כל התכונות הללו שנלמדו מוזנות אל שכבת הפלט.

115
00:07:53,410 --> 00:07:58,384
וכאן אנחנו רואים את התשובה הסופית, זהו הערך החזוי הסופי של מה

116
00:07:58,384 --> 00:08:02,832
שהרשת העצבית חושבת שמה שהיא רואה היא הספרה הכתובה בכתב-היד הזו.

117
00:08:02,832 --> 00:08:07,437
אז בואו נראה את הסרטון.

118
00:08:07,437 --> 00:08:17,437
[מוסיקה]

119
00:09:49,712 --> 00:09:53,949
אז אני מקווה שנהניתם מהווידאו ושבתקווה זה נתן לכם קצת אינטואיציה

120
00:09:53,949 --> 00:09:58,350
על סוג הפונקציות המסובכות למדי שרשתות עצביות יכולות ללמוד.

121
00:09:58,350 --> 00:10:02,445
שבו היא מקבלת את התמונה הזו כקלט, פשוט מקבלת כקלט את הפיקסלים האלה,

122
00:10:02,445 --> 00:10:05,234
והשכבה המוסתרת הראשונה מחשבת איזה סדרה של תכונות.

123
00:10:05,234 --> 00:10:07,754
השכבה החבויה הבאה מחשבת תכונות מורכבות עוד יותר

124
00:10:07,754 --> 00:10:09,550
ועוד יותר מורכבות.

125
00:10:09,550 --> 00:10:12,980
והתכונות האלה יכולות לשמש את השכבה הסופית

126
00:10:12,980 --> 00:10:17,600
של מסווגים לוגיסטיים כדי לבצע תחזית מדויקת

127
00:10:17,600 --> 00:10:20,005
של הספרה שאותה רואה הרשת.