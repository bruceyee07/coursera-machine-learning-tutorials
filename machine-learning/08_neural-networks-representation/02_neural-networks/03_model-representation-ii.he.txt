בסרטון האחרון, נתנו הגדרה מתמטית כיצד לייצג או כיצד לחשב את פונקציות ההיפותזה המשמשות את הרשת העצבית. בסרטון הזה, הייתי רוצה להראות לכם איך לבצע את החישוב ביעילות למעשה, דהיינו להראות לכם יישום וקטורי של זה. שנית, וחשוב עוד יותר, אני רוצה להתחיל לתת לכם אינטואיציה למה ייצוג כזה של רשת עצבית הוא רעיון טוב וכיצד הוא יכול לעזור לנו ללמוד השערות בלתי לינאריות מורכבות. הסתכלו על הרשת העצבית הזו. אמרנו מקודם שרצף הצעדים שאנחנו צריכים לעשות כדי לחשב את הפלט של השערה הוא המשוואות האלה שכתובות כאן משמאל שבהם מחשבים את ערכי ההפעלה של שלושת התאים הנסתרים ואז משתמשים בתוצאות כדי לחשב את הפלט הסופי של פונקצית ההשערה h של x. אני אגדיר עכשיו עוד כמה מונחים. המונח הזה שאני מדגיש כאן, אני אגדיר אותו להיות z אינדקס עליון 2 אינדקס תחתון 1. אז a(2)1, שהוא המונח הזה, שווה ל-g של z(2)1. ודרך אגב, האינדקס העליון 2 פירושו גם ב-(z(2 וגם ב-(a(2, הסימן העליון 2 בסוגריים אומר שאלה הם ערכים שקשורים בשכבה 2, כלומר השכבה המוסתרת ברשת העצבית. את המונח הזה אנחנו מגדירים כ-z(2)2. ולבסוף, המונח האחרון כאן שאני מדגיש, אני מגדיר אותו כ-z(2)3. אז בדומה יש לנו a(2)3 שווה ל-g של z(2)3. אז ערכי z הם פשוט קומבינציות ליניאריות, קומבינציות ליניאריות משוקללות, של ערכי הקלט x0, x1, x2, x3 שנכנסים לנוירון מסוים. עכשיו אם תסתכלו על הבלוק הזה של מספרים, ייתכן שתבחינו שהבלוק הזה דומה באופן חשוד לפעולה בין וקטור ומטריצה, מכפלה של וקטור במטריצה של המטריצה Θ כפול הוקטור x. ובשימוש בתצפית הזו אנו נהיה מסוגלים לעשות יישום וקטורי של החישוב הזה של הרשת העצבית. באופן קונקרטי, בואו נגדיר את וקטור התכונות x כרגיל להיות וקטור של x0, x1, x2, x3 כאשר x0 כרגיל תמיד שווה 1, ונגדיר את z2 להיות וקטור של ערכי z האלה, של z(2)1, z(2)2 ו-z(2)3. ושימו לב, ש-z2 הוא וקטור תלת מימדי. עכשיו אנחנו יכולים לעשות חישוב וקטורי של a(2)1, a(2)2, ו-a(2)3 כדלקמן. אנחנו יכולים פשוט לכתוב את זה בשני שלבים. אנחנו יכולים לחשב z2 כ-Θ1 כפול x וזה ייתן לנו את הוקטור z2; ואז a2 הוא g של z2 וכמובן z2 כאן הוא וקטור תלת מימדי ו-a2 גם הוא וקטור תלת מימדי ופונקצית ההפעלה g, זה מריץ את פונקצית הסיגמואיד על כל אלמנט של z2. ודרך אגב, כדי לעשות את הסימונים שלנו קצת יותר עקביים עם מה שנעשה אחר כך, בשכבת הקלט יש לנו קלטים x, אבל אנחנו יכולים לחשוב עליהם גם כפונקציות ההפעלה של השכבה הראשונה אז אם אני אגדיר את a1 להיות שווה x. אז a1 הוא וקטור, עכשיו אני יכול לקחת את x כאן ולהחליף אותו ב-z2 שווה Θ1 כפול a1 פשוט על ידי הגדרת a1 להיות ההפעלה בשכבת הקלט. עכשיו, עם מה שכתבתי עד עכשיו קיבלנו את הערכים עבור a1, a2, a3, ובעצם אנחנו צריכים להוסיף שם סימנים עליונים. אבל אנחנו צריכים עוד ערך אחד, אנחנו רוצים גם את a(0)2 שמתאים ליחידת הטיה או סטייה בשכבה החבויה שהולכת לשכבת הפלט. כמובן, גם כאן היתה יחידה של הטיה, שפשוט לא ציירתי אותה כאן, אבל כדי לטפל ביחידת ההטיה הנוספת הזאת, אנחנו נוסיף a(2)0, שהוא שווה לאחד, ואז אנחנו מקבלים ש-a2 יהיה וקטור תכונות 4-ממדי כי בדיוק הוספנו את a0 שווה 1 שמתאים ליחידה ההטיה בשכבה המוסתרת. ולבסוף, כדי לחשב את הפלט הסופי של ההשערה שלנו, אנחנו פשוט צריכים לחשב את z3. אז z3 שווה למונח הזה כאן שהדגשתי. המונח הפנימי הזה הוא z3. ו-z3 הוא Θ2 כפול a2 ולבסוף ההשערה שלי מוציאה כפלט את h של x שהוא a3 שהיא ההפעלה של היחידה האחת ויחידה שלנו בשכבת הפלט. זה פשוט מספר ממשי. אפשר לכתוב אותו כ-a3 או כ-a(3)1 וערכו הוא g של z3. התהליך הזה של חישוב h של x נקרא גם מסירה או הפצה לפנים והוא נקרא כך כי אנחנו מתחילים עם הפעלה של יחידות הקלט ואז אנחנו כאילו מפיצים את זה אל השכבה המוסתרת ומחשבים את ההפעלה של השכבה המוסתרת ואז אנחנו מוסרים את זה קדימה ומחשבים את ההפעלה של שכבת הפלט, והתהליך הזה של חישוב את ההפעלה מהקלט אל המוסתרת ואז אל שכבת הפלט נקרא גם הפצה לפנים, ומה שעשינו עכשיו היה פשוט גרסה וקטורית של התהליך הזה. אז אם אתם מיישמים את זה באמצעות המשוואות כאן מימין, הן ייתנו לכם דרך יעילה יחסית לחישוב h של x. ההצגה הזו של מסירה לפנים גם עוזרת לנו להבין מה מסוגלות רשתות עצביות לעשות, ומדוע הן יכולות לעזור לנו ללמוד השערות לא לינאריות מעניינות. תחשבו על הרשת העצבית הבאה, ונניח שאני מכסה את הצד השמאלי של התמונה הזאת לעת עתה. אם מסתכלים על מה שנשאר בתמונה הזאת. זה נראה מאד דומה לרגרסיה לוגיסטית שבה אנחנו משתמשים בצומת הזה, שהוא פשוט יחידה של רגרסיה לוגיסטית ואנחנו משתמשים בו כדי לעשות תחזית של h של x. ולמעשה מה שתוציא ההשערה h של x שווה בעצם ל-g שהיא פונקציה הסיגמואיד או ההפעלה שלי כפול Θ0 כפול a0 שהוא 1, פלוס Θ1 כפול a1 פלוס Θ2 כפול a2 פלוס Θ3 כפול a3 כשהערכים a1, a2, ו-a3 הם אלה שניתנים על ידי שלוש היחידות החבויות. עכשיו, כדי להיות עקביים באמת עם הסימונים המוקדמים שלי, למעשה, אנחנו צריכים למלא את הסימנים העליונים האלה 2 כאן בכל מקום, ויש לנו גם האינדקסים 1 שם כי יש לנו רק יחידת פלט אחת, אבל אם נתמקד בחלקים הכחולים של הסימון. זה נראה מאוד דומה למודל הרגרסיה הלוגיסטית הרגיל, אלא שעכשיו יש לנו תטא גדולה (Θ) במקום תטא קטנה (θ). ומה שזה עושה הוא פשוט רגרסיה לוגיסטית. אבל התכונות שמוכנסות כקלט לרגרסיה הלוגיסטית הם הערכים האלה שמחושבים על ידי השכבה שהסתרנו. אני רק אומר את זה שוב, מה שהרשת העצבית עושה הוא בדיוק כמו רגרסיה לוגיסטית, אלא שבמקום להשתמש בתכונות המקוריות x1, x2, x3, היא משתמשת בתכונות החדשות a1, a2, a3. ושוב, נשים את האינדקסים העליונים במקום כדי להיות עקביים עם הסימונים שלנו. ומה שמגניב בזה הוא שהתכונות a1, a2 ,a3, הם עצמם נלמדו כפונקציות של הקלט. מעשית, על ידי פונקציית המיפוי משכבה 1 לשכבה 2, שנקבעה על ידי קבוצה אחרת של פרמטרים, Θ1. אז זה כאילו שברשת עצבית, במקום להיות מוגבל להאכיל את התכונות x1, x2, x3 לרגרסיה הלוגיסטית, הרשת לומדת תכונות משלה a1, a2, a3, כקלט לרגרסיה הלוגיסטית וכפי שאפשר להבין לפי הפרמטרים שהיא בוחרת עבור Θ1, היא מסוגלת ללמוד תכונות די מעניינות ומורכבות, ולכן אפשר לקבל היפותזות יותר טובות מאשר לו נאלצה להשתמש בתכונות הגלם x1, x2 או x3 או אם היא היתה מוגבלת נניח לבחירת ביטויים פולינומיים כמו x1 כפול x2 כפול x3 וכן הלאה. אצלנו, לאלגוריתם הזה יש הגמישות לנסות וללמוד אילו תכונות שהוא רוצה, תוך שימוש ב-a1, a2, a3 כדי להזין את הצומת האחרון הזה, שהוא למעשה חישוב של רגרסיה לוגיסטית. אני מבין שהדוגמה הזאת מתוארת ברמה גבוהה למדי ולכן אני לא בטוח אם האינטואיציה הזו שלרשתות עצביות יש כח לבנות תכונות מורכבות יותר מובנת לכם עדיין, אבל אם עדיין לא, אז בשני קטעי הוידאו הבאים אני אתאר דוגמה ספציפית של איך יכולה רשת עצבית להשתמש בשכבות המוסתרות כדי לחשב תכונות מורכבות יותר כדי להאכיל בהם את שכבת הפלט הסופית וכיצד ניתן ללמוד כך השערות מורכבות יותר. אז אם מה שאני אומר כאן לא ממש הגיוני, הישארו איתי בשני קטעי הוידאו הבאים ואני מקווה שתוך כדי עבודה על הדוגמאות האלה, ההסבר הזה ייעשה קצת יותר הגיוני. אבל אני רוצה להדגיש שאפשר לצייר רשתות עצביות גם עם סוגים אחרים של דיאגרמות, והאופן שבו רשתות עצביות מחוברות נקרא ארכיטקטורה. המונח ארכיטקטורה מתייחס לצורה בה נוירונים שונים מחוברים זה לזה. כאן אנחנו רואים דוגמא של ארכיטקטורה שונה של רשת עצבית ושוב אתם יכולים לקבל אינטואיציה של איך השכבה השנייה, ובה יש לנו שלוש יחידות נסתרות שמחשבות איזו פונקציה אולי מורכבת של שכבת הקלט, ואז השכבה השלישית יכולה לקחת את התכונות מהשכבה השנייה ולחשב תכונות אפילו יותר מורכבות בשכבה שלוש, כך שכאשר מגיעים לשכבת הפלט, שכבה ארבע, אפשר לקבל תכונות מורכבות עוד יותר של מה שאפשר לחשב בשכבה שלוש וכך לקבל היפותזות לא ליניאריות ומעניינות. אגב, ברשת כזאת, שכבה מספר אחת נקראת שכבת הקלט. שכבה 4 היא עדיין שכבת הפלט וברשת הזו יש שתי שכבות מוסתרות. אז כל דבר שאינו שכבת קלט או שכבת פלט נקרא שכבה מוסתרת. אז אני מקווה שמתוך הוידאו הזה קיבלתם תחושה של איך עובד שלב ההפצה קדימה ברשת עצבית כשמתחילים מההפעלה של שכבת הקלט ומעבירים קדימה אל השכבה החבויה הראשונה, ואז לשכבה המוסתרת השניה, ואז לשכבת הפלט. וראיתם גם איך אפשר לעשות את החישוב בצורה וקטורית. בסרטון הבא, אני מבין שחלק מהאינטואיציות בסרטון הזה של איך שכבות מאוחרות מחשבות תכונות מורכבות יותר על בסיס השכבות המוקדמות. אני מבין שחלק מהאינטואיציה עשוי להיות עדיין קצת מופשטת, עדיין מעורפלת. אז מה שאני רוצה לעשות בשני קטעי הוידאו הבאים הוא לעבור על דוגמה מפורטת של איך יכולה רשת עצבית לחשב פונקציות לא ליניאריות של הקלט ובתקווה זה ייתן לכם אינטואיציה טובה של מיני ההשערות הלא ליניאריות והמורכבות שאפשר לקבל מרשתות עצביות.