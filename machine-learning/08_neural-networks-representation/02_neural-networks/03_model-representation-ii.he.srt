1
00:00:00,280 --> 00:00:01,330
בסרטון האחרון, נתנו

2
00:00:01,570 --> 00:00:03,540
הגדרה מתמטית כיצד

3
00:00:03,700 --> 00:00:04,990
לייצג או כיצד

4
00:00:05,090 --> 00:00:07,160
לחשב את פונקציות ההיפותזה המשמשות את הרשת העצבית.

5
00:00:08,420 --> 00:00:09,620
בסרטון הזה, הייתי רוצה

6
00:00:09,730 --> 00:00:11,280
להראות לכם איך לבצע

7
00:00:11,450 --> 00:00:14,040
את החישוב ביעילות למעשה,

8
00:00:14,710 --> 00:00:16,050
דהיינו להראות לכם יישום וקטורי של זה.

9
00:00:17,660 --> 00:00:18,930
שנית, וחשוב עוד יותר, אני רוצה

10
00:00:19,100 --> 00:00:21,110
להתחיל לתת לכם אינטואיציה

11
00:00:21,390 --> 00:00:22,590
למה ייצוג כזה של רשת עצבית

12
00:00:23,360 --> 00:00:24,640
הוא רעיון טוב וכיצד

13
00:00:25,010 --> 00:00:27,290
הוא יכול לעזור לנו ללמוד השערות בלתי לינאריות מורכבות.

14
00:00:28,970 --> 00:00:29,880
הסתכלו על הרשת העצבית הזו.

15
00:00:30,520 --> 00:00:31,720
אמרנו מקודם

16
00:00:32,010 --> 00:00:33,070
שרצף הצעדים שאנחנו

17
00:00:33,170 --> 00:00:34,090
צריכים לעשות כדי לחשב

18
00:00:34,650 --> 00:00:35,850
את הפלט של השערה

19
00:00:36,320 --> 00:00:37,780
הוא המשוואות האלה שכתובות

20
00:00:37,950 --> 00:00:38,770
כאן משמאל שבהם

21
00:00:39,540 --> 00:00:41,330
מחשבים את ערכי ההפעלה של

22
00:00:41,450 --> 00:00:43,220
שלושת התאים הנסתרים ואז

23
00:00:43,420 --> 00:00:44,580
משתמשים בתוצאות כדי לחשב

24
00:00:44,650 --> 00:00:45,710
את הפלט הסופי של פונקצית ההשערה

25
00:00:46,680 --> 00:00:48,410
h של x. אני

26
00:00:48,480 --> 00:00:50,200
אגדיר עכשיו עוד כמה מונחים.

27
00:00:50,570 --> 00:00:52,210
המונח הזה שאני

28
00:00:52,410 --> 00:00:54,090
מדגיש כאן, אני

29
00:00:54,180 --> 00:00:55,560
אגדיר אותו להיות

30
00:00:56,230 --> 00:00:58,410
z אינדקס עליון 2 אינדקס תחתון 1.

31
00:00:58,790 --> 00:00:59,830
אז

32
00:01:00,650 --> 00:01:02,310
a(2)1, שהוא

33
00:01:02,470 --> 00:01:03,930
המונח הזה, שווה

34
00:01:04,170 --> 00:01:06,020
ל-g של z(2)1.

35
00:01:06,130 --> 00:01:08,100
ודרך

36
00:01:08,180 --> 00:01:09,750
אגב, האינדקס העליון 2

37
00:01:10,570 --> 00:01:11,580
פירושו

38
00:01:11,870 --> 00:01:12,960
גם ב-(z(2 וגם ב-(a(2,

39
00:01:13,080 --> 00:01:14,140
הסימן העליון

40
00:01:14,840 --> 00:01:16,450
2 בסוגריים אומר שאלה

41
00:01:16,740 --> 00:01:18,330
הם ערכים שקשורים בשכבה

42
00:01:18,570 --> 00:01:19,810
2, כלומר השכבה

43
00:01:20,100 --> 00:01:21,390
המוסתרת ברשת העצבית.

44
00:01:22,820 --> 00:01:25,200
את המונח הזה

45
00:01:25,990 --> 00:01:27,640
אנחנו מגדירים

46
00:01:29,530 --> 00:01:30,140
כ-z(2)2.

47
00:01:30,490 --> 00:01:31,860
ולבסוף, המונח האחרון

48
00:01:32,170 --> 00:01:33,100
כאן שאני מדגיש,

49
00:01:34,160 --> 00:01:37,040
אני מגדיר אותו כ-z(2)3.

50
00:01:37,090 --> 00:01:38,710
אז בדומה יש לנו a(2)3

51
00:01:38,850 --> 00:01:43,200
שווה ל-g של

52
00:01:44,990 --> 00:01:45,360
z(2)3.

53
00:01:45,480 --> 00:01:46,760
אז ערכי z הם פשוט

54
00:01:47,290 --> 00:01:48,940
קומבינציות ליניאריות,

55
00:01:49,360 --> 00:01:51,200
קומבינציות ליניאריות משוקללות, של

56
00:01:51,490 --> 00:01:52,800
ערכי הקלט x0, x1,

57
00:01:53,060 --> 00:01:55,350
x2, x3 שנכנסים לנוירון מסוים.

58
00:01:57,090 --> 00:01:58,260
עכשיו אם תסתכלו על

59
00:01:58,900 --> 00:02:00,470
הבלוק הזה של מספרים,

60
00:02:01,990 --> 00:02:03,310
ייתכן שתבחינו שהבלוק הזה

61
00:02:03,490 --> 00:02:05,880
דומה באופן חשוד לפעולה

62
00:02:06,950 --> 00:02:08,330
בין וקטור ומטריצה,

63
00:02:08,800 --> 00:02:10,260
מכפלה של וקטור במטריצה

64
00:02:11,070 --> 00:02:12,710
של המטריצה Θ כפול

65
00:02:12,790 --> 00:02:14,840
הוקטור x. ובשימוש בתצפית הזו

66
00:02:15,580 --> 00:02:18,730
אנו נהיה מסוגלים לעשות יישום וקטורי של החישוב הזה

67
00:02:19,700 --> 00:02:20,280
של הרשת העצבית.

68
00:02:21,470 --> 00:02:23,510
באופן קונקרטי, בואו נגדיר את

69
00:02:23,680 --> 00:02:24,810
וקטור התכונות x כרגיל

70
00:02:25,290 --> 00:02:27,020
להיות וקטור של x0, x1,

71
00:02:27,260 --> 00:02:28,550
x2, x3 כאשר x0

72
00:02:29,010 --> 00:02:30,280
כרגיל תמיד שווה

73
00:02:30,610 --> 00:02:31,860
1, ונגדיר

74
00:02:32,390 --> 00:02:33,420
את z2 להיות וקטור

75
00:02:34,360 --> 00:02:37,250
של ערכי z האלה, של z(2)1, z(2)2 ו-z(2)3.

76
00:02:38,560 --> 00:02:40,210
ושימו לב, ש-z2

77
00:02:40,440 --> 00:02:42,500
הוא וקטור תלת מימדי.

78
00:02:43,910 --> 00:02:47,200
עכשיו אנחנו יכולים לעשות חישוב וקטורי

79
00:02:48,270 --> 00:02:48,860
של a(2)1, a(2)2, ו-a(2)3 כדלקמן.

80
00:02:49,490 --> 00:02:50,690
אנחנו יכולים פשוט לכתוב את זה בשני שלבים.

81
00:02:51,500 --> 00:02:53,400
אנחנו יכולים לחשב z2

82
00:02:53,950 --> 00:02:55,490
כ-Θ1 כפול x וזה

83
00:02:55,790 --> 00:02:57,020
ייתן לנו את הוקטור z2;

84
00:02:57,400 --> 00:02:59,360
ואז a2 הוא

85
00:02:59,860 --> 00:03:02,180
g של z2

86
00:03:02,440 --> 00:03:03,860
וכמובן z2 כאן

87
00:03:04,200 --> 00:03:05,880
הוא וקטור תלת מימדי

88
00:03:06,060 --> 00:03:08,150
ו-a2 גם הוא וקטור תלת

89
00:03:08,810 --> 00:03:10,410
מימדי

90
00:03:10,690 --> 00:03:12,680
ופונקצית ההפעלה g, זה מריץ

91
00:03:12,950 --> 00:03:15,290
את פונקצית הסיגמואיד

92
00:03:15,550 --> 00:03:18,290
על כל אלמנט של z2.

93
00:03:18,380 --> 00:03:19,270
ודרך אגב, כדי לעשות את הסימונים שלנו

94
00:03:19,950 --> 00:03:21,260
קצת יותר עקביים עם

95
00:03:21,440 --> 00:03:23,330
מה שנעשה אחר כך,

96
00:03:23,590 --> 00:03:24,600
בשכבת הקלט יש לנו

97
00:03:24,670 --> 00:03:25,840
קלטים x, אבל אנחנו

98
00:03:25,960 --> 00:03:26,950
יכולים לחשוב עליהם גם

99
00:03:27,300 --> 00:03:29,270
כפונקציות ההפעלה של השכבה הראשונה

100
00:03:29,680 --> 00:03:30,430
אז אם אני אגדיר את a1 להיות

101
00:03:30,470 --> 00:03:32,510
שווה x. אז

102
00:03:32,660 --> 00:03:34,270
a1 הוא וקטור, עכשיו

103
00:03:34,500 --> 00:03:35,520
אני יכול לקחת את x כאן

104
00:03:36,230 --> 00:03:38,850
ולהחליף אותו ב-z2 שווה Θ1

105
00:03:39,570 --> 00:03:40,680
כפול a1 פשוט על ידי הגדרת

106
00:03:41,410 --> 00:03:43,350
a1 להיות ההפעלה בשכבת הקלט.

107
00:03:44,990 --> 00:03:46,000
עכשיו, עם מה שכתבתי

108
00:03:46,280 --> 00:03:47,500
עד עכשיו קיבלנו

109
00:03:47,900 --> 00:03:49,940
את הערכים עבור a1,

110
00:03:50,820 --> 00:03:52,690
a2, a3, ובעצם

111
00:03:52,780 --> 00:03:53,980
אנחנו צריכים להוסיף

112
00:03:54,290 --> 00:03:55,600
שם סימנים עליונים.

113
00:03:56,430 --> 00:03:57,530
אבל אנחנו צריכים עוד

114
00:03:57,940 --> 00:03:59,810
ערך אחד, אנחנו רוצים גם את a(0)2

115
00:04:00,050 --> 00:04:02,050
שמתאים

116
00:04:02,250 --> 00:04:04,350
ליחידת הטיה או סטייה

117
00:04:04,550 --> 00:04:06,420
בשכבה החבויה שהולכת לשכבת הפלט.

118
00:04:06,990 --> 00:04:07,780
כמובן, גם כאן

119
00:04:07,810 --> 00:04:08,850
היתה יחידה של הטיה,

120
00:04:09,000 --> 00:04:10,060
שפשוט לא ציירתי אותה

121
00:04:10,270 --> 00:04:11,820
כאן, אבל כדי

122
00:04:11,970 --> 00:04:13,100
לטפל ביחידת ההטיה הנוספת הזאת,

123
00:04:13,870 --> 00:04:15,650
אנחנו נוסיף

124
00:04:16,320 --> 00:04:18,720
a(2)0,

125
00:04:18,890 --> 00:04:20,870
שהוא שווה לאחד, ואז

126
00:04:21,010 --> 00:04:21,990
אנחנו מקבלים

127
00:04:22,290 --> 00:04:23,860
ש-a2 יהיה

128
00:04:24,010 --> 00:04:25,390
וקטור תכונות 4-ממדי

129
00:04:25,690 --> 00:04:26,820
כי בדיוק הוספנו

130
00:04:27,300 --> 00:04:28,490
את

131
00:04:28,620 --> 00:04:30,260
a0 שווה

132
00:04:30,500 --> 00:04:31,700
1 שמתאים ליחידה ההטיה

133
00:04:32,080 --> 00:04:33,550
בשכבה המוסתרת. ולבסוף,

134
00:04:35,080 --> 00:04:37,620
כדי לחשב את

135
00:04:38,070 --> 00:04:40,100
הפלט הסופי של ההשערה שלנו, אנחנו

136
00:04:40,250 --> 00:04:41,190
פשוט צריכים לחשב את

137
00:04:42,470 --> 00:04:44,980
z3. אז z3

138
00:04:45,350 --> 00:04:47,940
שווה למונח הזה כאן שהדגשתי.

139
00:04:48,800 --> 00:04:51,450
המונח הפנימי הזה הוא z3.

140
00:04:53,980 --> 00:04:55,160
ו-z3 הוא Θ2

141
00:04:55,500 --> 00:04:57,120
כפול a2 ולבסוף

142
00:04:57,810 --> 00:04:59,560
ההשערה שלי מוציאה כפלט את h של x

143
00:04:59,750 --> 00:05:01,210
שהוא a3 שהיא

144
00:05:01,360 --> 00:05:03,910
ההפעלה של

145
00:05:04,750 --> 00:05:06,040
היחידה האחת ויחידה שלנו

146
00:05:06,290 --> 00:05:09,500
בשכבת הפלט. זה פשוט מספר ממשי. אפשר לכתוב אותו כ-a3

147
00:05:10,050 --> 00:05:12,390
או כ-a(3)1 וערכו הוא g של z3.

148
00:05:13,240 --> 00:05:15,020
התהליך הזה של חישוב h של x

149
00:05:15,940 --> 00:05:18,110
נקרא גם מסירה או הפצה לפנים

150
00:05:19,130 --> 00:05:20,440
והוא נקרא כך כי אנחנו

151
00:05:20,550 --> 00:05:21,310
מתחילים עם הפעלה

152
00:05:22,010 --> 00:05:24,400
של יחידות הקלט ואז

153
00:05:24,940 --> 00:05:26,770
אנחנו כאילו מפיצים את זה אל

154
00:05:26,860 --> 00:05:29,390
השכבה המוסתרת ומחשבים את ההפעלה של

155
00:05:29,580 --> 00:05:30,400
השכבה המוסתרת ואז אנחנו

156
00:05:30,540 --> 00:05:32,040
מוסרים את זה קדימה

157
00:05:32,760 --> 00:05:36,270
ומחשבים את ההפעלה של

158
00:05:37,480 --> 00:05:39,170
שכבת הפלט, והתהליך הזה של חישוב את ההפעלה מהקלט

159
00:05:39,290 --> 00:05:40,400
אל המוסתרת ואז אל שכבת הפלט

160
00:05:40,940 --> 00:05:42,030
נקרא גם הפצה לפנים,

161
00:05:43,320 --> 00:05:44,150
ומה שעשינו עכשיו

162
00:05:44,310 --> 00:05:45,370
היה פשוט גרסה

163
00:05:45,740 --> 00:05:47,140
וקטורית של

164
00:05:47,280 --> 00:05:48,890
התהליך הזה. אז אם אתם

165
00:05:48,970 --> 00:05:50,260
מיישמים את זה באמצעות

166
00:05:50,800 --> 00:05:51,740
המשוואות כאן מימין,

167
00:05:51,850 --> 00:05:53,280
הן ייתנו לכם דרך

168
00:05:53,460 --> 00:05:54,980
יעילה יחסית

169
00:05:55,120 --> 00:05:56,130
לחישוב h של x.

170
00:05:58,250 --> 00:05:59,860
ההצגה הזו של מסירה לפנים גם

171
00:06:00,860 --> 00:06:02,270
עוזרת לנו להבין מה

172
00:06:02,550 --> 00:06:03,640
מסוגלות רשתות עצביות לעשות,

173
00:06:04,110 --> 00:06:05,290
ומדוע הן יכולות לעזור לנו

174
00:06:05,510 --> 00:06:07,170
ללמוד השערות לא לינאריות מעניינות.

175
00:06:08,670 --> 00:06:09,760
תחשבו על הרשת העצבית הבאה,

176
00:06:10,500 --> 00:06:11,820
ונניח שאני מכסה

177
00:06:12,040 --> 00:06:13,810
את הצד השמאלי של התמונה הזאת לעת עתה.

178
00:06:14,650 --> 00:06:16,170
אם מסתכלים על מה שנשאר בתמונה הזאת.

179
00:06:17,030 --> 00:06:18,020
זה נראה מאד דומה

180
00:06:18,260 --> 00:06:19,520
לרגרסיה לוגיסטית שבה

181
00:06:19,660 --> 00:06:20,570
אנחנו משתמשים

182
00:06:20,990 --> 00:06:22,000
בצומת הזה, שהוא פשוט

183
00:06:22,130 --> 00:06:23,770
יחידה של רגרסיה לוגיסטית

184
00:06:24,120 --> 00:06:26,060
ואנחנו משתמשים בו כדי לעשות

185
00:06:26,380 --> 00:06:28,290
תחזית של h של x. ולמעשה

186
00:06:28,440 --> 00:06:30,340
מה שתוציא ההשערה

187
00:06:30,710 --> 00:06:31,830
h של x

188
00:06:31,890 --> 00:06:33,760
שווה בעצם ל-g

189
00:06:33,980 --> 00:06:38,110
שהיא פונקציה הסיגמואיד או ההפעלה שלי כפול Θ0

190
00:06:38,560 --> 00:06:40,450
כפול a0 שהוא

191
00:06:41,270 --> 00:06:43,380
1, פלוס Θ1

192
00:06:45,220 --> 00:06:49,080
כפול a1 פלוס Θ2

193
00:06:49,260 --> 00:06:52,090
כפול a2 פלוס Θ3

194
00:06:52,830 --> 00:06:55,180
כפול a3

195
00:06:55,370 --> 00:06:56,910
כשהערכים a1, a2, ו-a3

196
00:06:57,050 --> 00:06:59,860
הם אלה שניתנים על ידי שלוש היחידות החבויות.

197
00:07:01,060 --> 00:07:02,790
עכשיו, כדי להיות עקביים באמת

198
00:07:03,490 --> 00:07:05,000
עם הסימונים המוקדמים שלי, למעשה,

199
00:07:05,170 --> 00:07:06,360
אנחנו צריכים למלא

200
00:07:06,470 --> 00:07:10,700
את הסימנים העליונים האלה 2 כאן בכל מקום,

201
00:07:12,260 --> 00:07:13,920
ויש לנו גם

202
00:07:14,160 --> 00:07:16,800
האינדקסים 1 שם כי

203
00:07:16,930 --> 00:07:20,610
יש לנו רק יחידת פלט אחת, אבל אם נתמקד בחלקים הכחולים של הסימון.

204
00:07:20,930 --> 00:07:21,900
זה נראה מאוד

205
00:07:22,150 --> 00:07:23,680
דומה למודל הרגרסיה

206
00:07:23,870 --> 00:07:25,530
הלוגיסטית הרגיל, אלא

207
00:07:25,600 --> 00:07:28,060
שעכשיו יש לנו תטא גדולה (Θ) במקום תטא קטנה (θ).

208
00:07:29,170 --> 00:07:30,690
ומה שזה

209
00:07:30,850 --> 00:07:32,520
עושה הוא פשוט רגרסיה לוגיסטית.

210
00:07:33,660 --> 00:07:35,240
אבל התכונות שמוכנסות

211
00:07:35,590 --> 00:07:37,250
כקלט לרגרסיה הלוגיסטית הם

212
00:07:38,200 --> 00:07:40,170
הערכים האלה שמחושבים על ידי השכבה שהסתרנו.

213
00:07:41,340 --> 00:07:42,690
אני רק אומר את זה שוב, מה

214
00:07:42,910 --> 00:07:44,420
שהרשת העצבית עושה הוא

215
00:07:45,130 --> 00:07:47,050
בדיוק כמו רגרסיה לוגיסטית, אלא

216
00:07:47,440 --> 00:07:48,900
שבמקום להשתמש

217
00:07:49,110 --> 00:07:50,770
בתכונות המקוריות x1, x2, x3,

218
00:07:52,400 --> 00:07:54,260
היא משתמשת בתכונות החדשות a1,

219
00:07:54,440 --> 00:07:56,810
a2, a3. ושוב, נשים את האינדקסים העליונים

220
00:07:58,130 --> 00:08:00,380
במקום כדי להיות עקביים עם הסימונים שלנו.

221
00:08:02,820 --> 00:08:04,610
ומה שמגניב בזה

222
00:08:05,040 --> 00:08:06,220
הוא שהתכונות a1, a2 ,a3,

223
00:08:06,720 --> 00:08:08,310
הם עצמם נלמדו

224
00:08:08,760 --> 00:08:09,930
כפונקציות של הקלט.

225
00:08:10,960 --> 00:08:12,640
מעשית, על ידי פונקציית המיפוי

226
00:08:13,320 --> 00:08:14,540
משכבה 1 לשכבה 2,

227
00:08:14,810 --> 00:08:16,390
שנקבעה על ידי

228
00:08:16,750 --> 00:08:18,550
קבוצה אחרת של פרמטרים, Θ1.

229
00:08:19,380 --> 00:08:20,210
אז זה כאילו

230
00:08:20,270 --> 00:08:22,030
שברשת עצבית, במקום להיות

231
00:08:22,240 --> 00:08:24,050
מוגבל להאכיל את התכונות

232
00:08:24,120 --> 00:08:25,760
x1, x2, x3 לרגרסיה הלוגיסטית,

233
00:08:26,210 --> 00:08:27,440
הרשת לומדת

234
00:08:27,720 --> 00:08:29,320
תכונות משלה a1,

235
00:08:29,810 --> 00:08:32,010
a2, a3, כקלט

236
00:08:32,130 --> 00:08:33,950
לרגרסיה הלוגיסטית

237
00:08:34,650 --> 00:08:36,270
וכפי שאפשר להבין לפי

238
00:08:36,360 --> 00:08:37,690
הפרמטרים שהיא בוחרת עבור

239
00:08:37,900 --> 00:08:39,880
Θ1, היא מסוגלת ללמוד תכונות

240
00:08:40,390 --> 00:08:42,460
די מעניינות ומורכבות, ולכן

241
00:08:43,780 --> 00:08:44,830
אפשר לקבל

242
00:08:45,050 --> 00:08:46,650
היפותזות יותר טובות מאשר

243
00:08:46,840 --> 00:08:47,870
לו נאלצה להשתמש

244
00:08:48,020 --> 00:08:50,520
בתכונות הגלם x1, x2 או x3 או אם

245
00:08:50,640 --> 00:08:52,530
היא היתה מוגבלת נניח לבחירת

246
00:08:52,620 --> 00:08:53,730
ביטויים פולינומיים כמו

247
00:08:53,920 --> 00:08:55,550
x1 כפול x2 כפול x3 וכן הלאה.

248
00:08:55,790 --> 00:08:57,250
אצלנו, לאלגוריתם הזה יש

249
00:08:57,530 --> 00:08:59,130
הגמישות לנסות

250
00:08:59,420 --> 00:09:01,990
וללמוד אילו תכונות שהוא רוצה, תוך שימוש

251
00:09:02,680 --> 00:09:03,990
ב-a1, a2, a3

252
00:09:04,110 --> 00:09:05,190
כדי להזין את

253
00:09:05,510 --> 00:09:07,830
הצומת האחרון הזה, שהוא למעשה

254
00:09:09,240 --> 00:09:11,920
חישוב של רגרסיה לוגיסטית. אני מבין

255
00:09:12,550 --> 00:09:13,970
שהדוגמה הזאת מתוארת

256
00:09:14,060 --> 00:09:15,500
ברמה גבוהה למדי ולכן

257
00:09:15,750 --> 00:09:16,520
אני לא בטוח אם האינטואיציה הזו

258
00:09:17,440 --> 00:09:18,870
שלרשתות עצביות יש כח לבנות

259
00:09:19,720 --> 00:09:21,420
תכונות מורכבות יותר

260
00:09:21,630 --> 00:09:23,120
מובנת לכם עדיין, אבל אם

261
00:09:23,210 --> 00:09:24,440
עדיין לא, אז בשני

262
00:09:24,810 --> 00:09:25,860
קטעי הוידאו הבאים אני

263
00:09:25,970 --> 00:09:27,300
אתאר דוגמה ספציפית

264
00:09:28,250 --> 00:09:29,590
של איך יכולה רשת עצבית

265
00:09:29,830 --> 00:09:30,860
להשתמש בשכבות המוסתרות כדי לחשב

266
00:09:31,250 --> 00:09:32,880
תכונות מורכבות יותר כדי להאכיל בהם

267
00:09:33,130 --> 00:09:34,520
את שכבת הפלט הסופית

268
00:09:35,060 --> 00:09:37,100
וכיצד ניתן ללמוד כך השערות מורכבות יותר.

269
00:09:37,920 --> 00:09:39,120
אז אם מה שאני אומר

270
00:09:39,180 --> 00:09:40,090
כאן לא ממש

271
00:09:40,230 --> 00:09:41,650
הגיוני, הישארו איתי

272
00:09:41,810 --> 00:09:42,960
בשני קטעי הוידאו הבאים

273
00:09:43,190 --> 00:09:44,370
ואני מקווה שתוך כדי עבודה

274
00:09:44,580 --> 00:09:46,690
על הדוגמאות האלה, ההסבר הזה

275
00:09:47,030 --> 00:09:48,640
ייעשה קצת יותר הגיוני.

276
00:09:49,020 --> 00:09:49,740
אבל אני רוצה להדגיש

277
00:09:49,820 --> 00:09:51,120
שאפשר לצייר רשתות עצביות

278
00:09:51,470 --> 00:09:52,990
גם עם סוגים אחרים של דיאגרמות,

279
00:09:53,080 --> 00:09:54,270
והאופן שבו

280
00:09:54,450 --> 00:09:58,000
רשתות עצביות מחוברות נקרא ארכיטקטורה.

281
00:09:58,390 --> 00:10:00,150
המונח ארכיטקטורה מתייחס

282
00:10:00,490 --> 00:10:02,380
לצורה בה נוירונים שונים מחוברים זה לזה.

283
00:10:03,220 --> 00:10:04,180
כאן אנחנו רואים דוגמא

284
00:10:04,840 --> 00:10:06,300
של ארכיטקטורה שונה של רשת עצבית

285
00:10:07,480 --> 00:10:08,750
ושוב אתם יכולים

286
00:10:09,260 --> 00:10:10,770
לקבל אינטואיציה של

287
00:10:10,940 --> 00:10:12,180
איך השכבה השנייה,

288
00:10:12,900 --> 00:10:14,120
ובה יש לנו שלוש יחידות נסתרות

289
00:10:14,910 --> 00:10:16,200
שמחשבות איזו פונקציה אולי

290
00:10:16,660 --> 00:10:17,900
מורכבת של

291
00:10:17,990 --> 00:10:19,530
שכבת הקלט, ואז

292
00:10:19,730 --> 00:10:20,750
השכבה השלישית יכולה לקחת

293
00:10:20,840 --> 00:10:22,260
את התכונות מהשכבה השנייה ולחשב

294
00:10:22,550 --> 00:10:24,070
תכונות אפילו יותר מורכבות בשכבה שלוש,

295
00:10:24,980 --> 00:10:25,880
כך שכאשר מגיעים

296
00:10:25,960 --> 00:10:27,160
לשכבת הפלט, שכבה ארבע,

297
00:10:27,900 --> 00:10:29,130
אפשר לקבל תכונות

298
00:10:29,370 --> 00:10:30,690
מורכבות עוד יותר של מה

299
00:10:30,860 --> 00:10:32,040
שאפשר לחשב

300
00:10:32,280 --> 00:10:34,710
בשכבה שלוש וכך לקבל היפותזות לא ליניאריות ומעניינות.

301
00:10:36,730 --> 00:10:37,580
אגב, ברשת

302
00:10:37,810 --> 00:10:38,980
כזאת, שכבה מספר אחת

303
00:10:39,130 --> 00:10:40,670
נקראת שכבת הקלט. שכבה 4

304
00:10:41,360 --> 00:10:43,170
היא עדיין שכבת הפלט

305
00:10:43,340 --> 00:10:45,040
וברשת הזו יש שתי שכבות מוסתרות.

306
00:10:46,000 --> 00:10:47,440
אז כל דבר שאינו

307
00:10:48,000 --> 00:10:49,020
שכבת קלט או שכבת

308
00:10:49,340 --> 00:10:50,590
פלט נקרא שכבה מוסתרת.

309
00:10:53,390 --> 00:10:54,470
אז אני מקווה שמתוך הוידאו הזה

310
00:10:54,760 --> 00:10:55,840
קיבלתם תחושה של

311
00:10:56,140 --> 00:10:58,360
איך עובד שלב ההפצה קדימה

312
00:10:58,830 --> 00:11:00,230
ברשת עצבית

313
00:11:00,390 --> 00:11:01,670
כשמתחילים מההפעלה של

314
00:11:01,720 --> 00:11:03,150
שכבת הקלט ומעבירים

315
00:11:03,450 --> 00:11:04,480
קדימה אל

316
00:11:04,570 --> 00:11:05,560
השכבה החבויה הראשונה, ואז לשכבה

317
00:11:06,070 --> 00:11:08,200
המוסתרת השניה, ואז לשכבת הפלט.

318
00:11:08,990 --> 00:11:10,250
וראיתם גם איך אפשר

319
00:11:10,560 --> 00:11:12,010
לעשות את החישוב בצורה וקטורית.

320
00:11:13,660 --> 00:11:14,830
בסרטון הבא, אני מבין

321
00:11:15,240 --> 00:11:16,680
שחלק מהאינטואיציות

322
00:11:16,850 --> 00:11:19,220
בסרטון הזה של איך

323
00:11:19,550 --> 00:11:22,570
שכבות מאוחרות מחשבות תכונות מורכבות יותר על בסיס השכבות המוקדמות.

324
00:11:22,910 --> 00:11:23,540
אני מבין שחלק מהאינטואיציה

325
00:11:24,190 --> 00:11:26,660
עשוי להיות עדיין קצת מופשטת, עדיין מעורפלת.

326
00:11:27,450 --> 00:11:28,240
אז מה שאני רוצה

327
00:11:28,350 --> 00:11:29,460
לעשות בשני קטעי הוידאו הבאים

328
00:11:30,210 --> 00:11:31,540
הוא לעבור על דוגמה מפורטת

329
00:11:32,510 --> 00:11:33,810
של איך יכולה רשת עצבית

330
00:11:33,960 --> 00:11:35,740
לחשב פונקציות לא ליניאריות

331
00:11:36,710 --> 00:11:38,030
של הקלט

332
00:11:38,330 --> 00:11:39,450
ובתקווה זה ייתן לכם

333
00:11:39,540 --> 00:11:40,860
אינטואיציה טובה של מיני

334
00:11:41,010 --> 00:11:44,630
ההשערות הלא ליניאריות והמורכבות שאפשר לקבל מרשתות עצביות.