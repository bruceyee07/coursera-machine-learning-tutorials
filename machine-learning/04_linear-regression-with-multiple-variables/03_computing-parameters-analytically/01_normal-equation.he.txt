בסרטון הזה, נדבר על "המשוואה הנורמלית", שעבור חלק מהבעיות עם רגרסיה לינארית, תניב דרך הרבה יותר טובה למצוא את הערך האופטימלי של הפרמטר תטא. למעשה, עד כה, האלגוריתם שבו השתמשנו לרגרסיה לינארית הוא "ירידה בגרדיאנט" שבו בשביל למזער את פונקציית המחיר J של תטא, היינו לוקחים את האלגוריתם האיטרטיבי הבא שלוקח צעדים רבים, איטרציות רבות של "ירידה בגרדיאנט" בשביל להתכנס למינימום הגלובלי. לעומת זאת, המשוואה הנורמלית תיתן לנו שיטה לחשב את תטא בדרך אנליטית, כך שבמקום להפעיל את האלגוריתם האיטרטיבי הזה, נוכל במקום זאת פשוט לחשב את הערך האופטימלי של תטא כך שבסך הכל בצעד אחד אתה מקבל מיד את הערך האופטימלי. מתברר שלמשוואה הנורמלית יש כמה יתרונות וכמה חסרונות, אבל לפני שנגיע לזה ונדבר על מתי צריך להשתמש בה, הבה ונקבל קצת אינטואיציה על מה עושה השיטה הזו. לשם הדוגמה, בואו נדמיין, בואו ניקח פונקצית עלות פשוטה מאוד J של תטא, שהיא פשוט פונקציה של מספר ממשי תטא. אז נדמיין שתטא היא ערך סקלרי או שתטא היא פשוט ערך ממשי. היא רק מספר, ולא וקטור. תארו לעצמכם שיש לנו פונקציית עלות J שהיא הפונקציה הריבועית של הפרמטר הזה, המספר הממשי תטא, אז J של תטא נראה כך. אז איך ממזערים פונקציה ריבועית? לאלו מכם שיודעים קצת נגזרות, אתם עשויים לדעת כי הדרך למזער פונקציה היא לגזור אותה ולמצוא איפה הנגזרת שווה לאפס. אז ניקח את הנגזרת של J ביחס לפרמטר תטא. נקבל איזה נוסחה שאותה אינני מתכוון לחשב כאן, עכשיו נגדיר את הנגזרת שווה לאפס, וזה יאפשר לנו לפתור את הערך של תטא שתמזער את J של תטא. זה היה מקרה פשוט ביותר שבו תטא היה פשוט מספר ממשי. בבעיה שבה אנחנו מעוניינים תטא היא כבר איננה פשוט מספר ממשי, אלא במקום זאת, היא וקטור פרמטרים n+1 מימדי, ופונקציית העלות J היא פונקציה של הערך הוקטורי הזה, תטא-0 עד תטא-m. ופונקציית העלות נראית ככה, כמה מחוברים רבועיים כמו כאן מימין. כיצד ניתן למזער את פונקציית העלות הזו J? החשבון הדיפרנציאלי למעשה אומר לנו שאחת הדרכים לעשות זאת, היא לקחת נגזרות חלקיות של J, לגבי כל פרמטר תטא של J בתורו, ואז להשוות את כל הנגזרות האלה ל-0. אם תעשה את זה, ותחשב את הערכים תטא-0, תטא-1, עד תטא-n, אז זה ייתן לך את הערכים של תטא שממזערים את פונקציית העלות J. ואם באמת תבצע את החשבון ותמצא את ערכי הפרמטרים תטא-0 עד תטא-n, הגזירה תהיה קצת מסובכת. ומה שאני הולך לעשות בוידאו הזה, הוא למעשה לא לעבור על תהליך הגזירה, כי הוא די ארוך ומסובך, אבל מה שאני רוצה לעשות הוא להגיד לכם את כל מה שאתם צריכים לדעת כדי ליישם את התהליך הזה, כך שתוכלו לחשב את ערכי תטא המתאימים לנקודה שבה הנגזרות החלקיות שוות לאפס. לחלופין, או במילים אחרות, הערכים של תטא שממזערים את פונקצית העלות J של תטא. אני מבין שכמה מההערות שהערתי נשמעו הגיוניות רק לאלה מכם שמכירים חשבון דיפרנציאלי. אבל לאלה מכם שלא יודעים או שפחות יודעים חשבון דיפרנציאלי, אל תדאגו. אני רק הולך להסביר לכם מה שאתם צריכים לדעת על מנת ליישם את האלגוריתם הזה ולגרום לו לעבוד. הדוגמה בה אני רוצה להשתמש כדוגמה להסבר, נניח שיש לי m = 4 דוגמאות אימון. כדי ליישם את המשוואה הנורמלית הזו, מה שאני הולך לעשות הוא הדבר הבא. אני הולך לקחת את סט הנתונים שלי, במקרה שלנו את ארבע דוגמאות האימון שלי. במקרה זה נניח שארבע הדוגמאות האלה הם כל הנתונים שיש לי. מה שאני הולך לעשות הוא לקחת את סדרת הנתונים שלי ולהוסיף עמודה נוספת שתהיה תואמת לתכונה הנוספת שלי, x0, שהערך שלה הוא תמיד 1. מה שאני הולך לעשות אחרי זה הוא לבנות מטריצה בשם X, המטריצה בעצם מכילה את כל התכונות של נתוני האימון שלי, אז באופן מעשי הנה, הנה כל התכונות שלי ואנחנו הולכים לקחת את כל המספרים האלה ולשים אותם בתוך המטריצה "X", בסדר? פשוט להעתיק את הנתונים עמודה אחת בכל פעם ואז אני אעשה משהו דומה עבור y. אני הולך לקחת את הערכים שאני מנסה לחזות ולבנות עכשיו וקטור כמו זה ולקרוא לוקטור y. אז X היא מטריצה של m כפול (n+1) , ו-Y הוא וקטור m-מימדי שבו m הוא מספר דוגמאות ההכשרה ו-n הוא, n הוא מספר התכונות, בעצם n+1, בגלל התכונה הנוספת שהוספתי, x0. ואחרי כל פעולות ההכנה האלה, אם אתה לוקח את המטריצה X ואת הוקטור y, ופשוט מחשב כדלהלן, תטא שווה (X משוחלף כפול X) הופכית כפול X משוחלף כפול y, זה ייתן לך את הערך של תטא שממזער את עלות הפונקציה. קרה כאן הרבה על השקופית ואני עובר כאן צעד אחר צעד באמצעות דוגמה אחת ספציפית של מערך נתונים אחד. הרשו לי רק לכתוב את זה בצורה קצת יותר כללית ולאחר מכן, מאוחר יותר בסרטון הזה הרשו לי להסביר את המשוואה הזאת קצת יותר. במקרה שעדיין לא ברור כיצד עושים זאת. במקרה הכללי, נניח שיש לנו m דוגמאות אימון x1, y1 עד xm, ym ו-n תכונות. אז כל דוגמת אימון (x(i אולי נראית כמו וקטור כזה, שהוא וקטור תכונות n+1 ממדי. הדרך שבה אנחנו נבנה את המטריצה X, שנקראת גם מטריצת העיצוב היא כדלקמן. כל דוגמת אמון נותנת לנו וקטור תכונות כזה. מין וקטור (1+n)-ממדי. הדרך בה נבנה את מטריצת העיצוב X היא פשוט לבנות את המטריצה ככה. ומה שאני הולך לעשות הוא לקחת את דוגמת האימון הראשונה, הוקטור הזה, לקחת את המשוחלף שלו כך שהוא נראה ככה, כמובן, אוביקט ארוך ושטוח - וקטור שורה ולעשות מ-x1 משוחלף את השורה הראשונה של מטריצת העיצוב שלי. ואז אני לוקח את דוגמת האימון השנייה שלי, x2, משחלף אותה ושם את זה בתור השורה השנייה של X וכן הלאה, עד דוגמת האימון האחרונה שלי. משחלף אותה, וזו השורה האחרונה של המטריצה X שלנו. וכך, יצרנו את המטריצה שלנו X, שהיא מטריצה m על n+1. כדוגמה מוחשית, נניח שיש לי רק תכונה אחת, רק תכונה אחת חוץ מ-x0, שהוא תמיד שווה 1. אז אם וקטור התכונות שלי הוא x0 שהוא 1, ואז איזו תכונה אמיתית, כמו אולי גודל הבית, אז מטריצת העיצוב שלי, X, תהיה שווה לזה. בשורה הראשונה, אני בעצם לוקח את הוקטור הזה ומשחלף אותו. אז השורה היא 1, ואז הערך (הראשון והיחיד) של הדוגמה הראשונה x1. עבור השורה השניה, אנחנו נקבל במקום הראשון 1, ואז ערך התכונה בדוגמה השניה, וכן הלאה עד שמגיעים לשורה עם 1 ו-xm במקום הראשון. וכך זו תהיה מטריצה עם ממד m על 2. אז כך בונים את המטריצה X. והוקטור y - לפעמים אני מציין מעליו חץ כדי לסמן שזה וקטור, אבל לעתים קרובות אני פשוט כותב אותו כ-y, בכל מקרה. הווקטור y מתקבל פשוט על ידי לקיחת כל התוויות, לדוגמא המחירים של הבתים במערך האימון, ופשוט דוחפים אותם לתוך וקטור m-מימדי, שהוא y. לבסוף, לאחר שבנינו את המטריצה X ואת הוקטור y, אנחנו פשוט מחשבים את תטא כהופכי של (X'X) כפול (X'y). אני רק רוצה לוודא שהמשוואה הזו מובנת לכם ושאתם יודעים איך ליישם את זה. אז מה פרוש ההופכי של (X'X)? אז ההופכי של (X'X) הוא היפוכה של המטריצה (X'X) באופן מעשי, אם היית שם במטריצה A את הערך של (X'X) הרי 'X היא מטריצה, וגם (X'X) היא עוד מטריצה. ואנחנו נקרא למטריצה הזו A, אז ההופכי של (X'X) הוא פשוט לוקחים את המטריצה A והופכים אותה. פשוט. נקרא לתוצאה נניח 1 חלקי A. אז ככה מחשבים את זה. מחשבים את (X'X) ואז מחשבים את ההופכית שלה. עוד לא דיברנו על אוקטבה. אנו נעשה זאת בסט מאוחר יותר של קטעי וידאו, אבל בשפת התכנות אוקטבה או בשפות דומות, וגם שפת התכנות matlab דומה מאוד. הפקודה לחשב את הדבר הזה, ההופכי של (X משוחלף כפול X) כפול X משוחלף כפול y, הוא כדלקמן. באוקטבה משתמשים בסימון X תג כדי לציין את המטריצה המשוחלפת של X. אז הביטוי הזה במסגרת האדומה, מחשב את X משוחלף כפול X. pinv היא פונקציה לחישוב המטריצה ההופכית אז החלק הזה מחשב את ההופכי של (X'X) ואז מכפילים את זה בX משוחלף, ומכפיל ב-y, ואז בעצם חישבת את הנוסחה, שאמנם לא הוכחתי, אבל אפשר להראות מתמטית, למרות שאני לא עומד לעשות את זה כאן, שהנוסחה הזו נותנת את הערך האופטימלי של תטא, במובן שאם אתה מגדיר את תטא שווה לזה, זה יהיה הערך של תטא שימזער את פונקצית העלות J של תטא עבור רגרסיה לינארית. עוד פרט אחרון. בסרטון קודם דיברתי על משקול (סקיילינג) של תכונות ועל הרעיון של לגרום לתכונות להיות בטווחים דומים או בסדרי גודל דומים של ערכים זה לזה. אם אתה משתמש בשיטה של משוואה נורמלית, אז משקול התכונות ממש לא נחוץ. והשיטה עובדת גם אם, למשל, תכונה x1 היא בין אפס ואחת, ונניח תכונה x2 היא מאפס עד אלף ואיזשהי תכונה x3 היא בטווח מאפס עד עשר בחזקת מינוס חמש, אם אתה משתמש בשיטת המשוואה הנורמלית זה יעבוד ואין שום צורך לעשות משקול תכונות, אבל כמובן אם אתה משתמש בירידה במדרון אז משקול התכונות עדיין חשוב. ולקראת סיום, נדבר על מתי כדאי להשתמש בירידה במדרון ומתי כדאי להשתמש בשיטת המשוואה הנורמלית. הנה כמה יתרונות וחסרונות של שתי השיטות. נניח שיש לך m דוגמאות אימון וn תכונות. חסרון אחד של ירידה במדרון היא שצריך לבחור את שיעור הלמידה אלפא. ולעתים קרובות, זה אומר להפעיל את האלגוריתם כמה וכמה פעמים עם שיעורי למידה אלפא שונים ואז לראות מה עובד הכי טוב. אז זה קצת עבודה נוספת וטרחה נוספת. חסרון נוסף של ירידה במדרון הוא שצריך הרבה איטרציות. ובתלות בפרטים, זה יכול להאט את החישוב יותר, אם כי זה לא כל כך שחור לבן כפי שנראה בעוד שנייה. באשר למשוואה הנורמלית, לא צריך לבחור שום שיעור למידה אלפא. מה שגורם לשיטה להיות פשוטה מאד ונוחה ליישום. פשוט צריך להפעיל אותה ובדרך כלל היא פשוט עובדת. ולא צריך להריץ איטרציות, אז לא צריך לשרטט את J של תטא או לבדוק את ההתכנסות או לעשות את כל השלבים הנוספים. עד כה, נראה שהמאזן נוטה לטובת המשוואה הנורמלית. אבל הנה כמה חסרונות של המשוואה הנורמלית, וכמה יתרונות של הירידה במדרון. הירידה במדרון פועלת די טוב גם כאשר יש מספר גדול מאוד של תכונות. דהיינו, גם אם יש לכם מיליוני תכונות אתם יכולים להפעיל את הירידה במדרון והיא תפעל באופן יעיל וסביר. והיא תוציא משהו סביר. בניגוד למשוואה הנורמלית, שבה על מנת לחשב על פי הנתונים הפרמטרים, אנחנו צריכים לחלץ את המונח הזה. אנחנו צריכים לחשב את המונח הזה, ההופכי של X משוחלף כפול X. החלק הזה הוא המטריצה X משוחלפת כפול X. זוהי מטריצה של n כפול n, אם יש לכם n תכונות. כי אם מסתכלים על הממדים של המטריצה X משוחלף כפול X שאתם מכפילים, מבינים מה הממדים של המכפלה, אז המטריצה X משוחלף כפול X היא מטריצה n כפול n, כאשר n הוא מספר התכונות, וברוב היישומים הממוחשבים העלות של הפיכת מטריצה פרופורציוני לממד של המטריצה בחזקת שלוש. אז החישוב הזה עולה סדר גודל גס של n בשלישית. לפעמים זה קצת יותר יעיל מאשר n בשלישית אבל למטרות שלנו זה קרוב מספיק. אז אם מספר התכונות n הוא גדול מאוד, אז החישוב של הדבר הזה יכול להיות איטי ושיטת המשוואה הנורמלית עלולה למעשה להיות הרבה יותר איטית. אז אם n הוא גדול אז אני בדרך כלל משתמש בירידה במדרון כי אנחנו לא רוצים לשלם בזמן n בשלישית. אבל, אם n הוא קטן יחסית, אז המשוואה הנורמלית עשויה לתת לך דרך טובה יותר כדי לחשב את תטא. מה זה קטן ומה זה גדול? ובכן, אם n הוא בסדר גודל של מאה, אז להפוך מטריצה של מאה על מאה אין שום בעיה בסטנדרטים של המחשוב המודרני. אם n הוא אלף, אני עדיין אשתמש בשיטת המשוואה הנורמלית. הפיכת מטריצה של אלף על אלף היא פעולה ממש מהירה על מחשב מודרני. אם n הוא עשרת אלפים, אני אולי כבר מתחיל לתהות. הפיכת מטריצה של עשרת אלפים על עשרת אלפים מתחילה להיות קצת איטית, ואז אני עשוי להתחיל לנטות לכיוון ירידה במדרון, אבל אולי עדיין לא. כש-n שווה עשרת אלפים, זה עדיין אפשרי להפוך מטריצה של עשרת אלפים על עשרת אלפים. אבל אם n יהיה הרבה יותר גדול מזה, אז הייתי כנראה משתמש בירידה במדרון. ואם n שווה לעשר בשישית, עם מיליון תכונות, אז להפוך מטריצה של מיליון על מיליון יהיה יקר מאוד, ואני בהחלט אעדיף ירידה במדרון אם יש לנו כל כך הרבה תכונות. אז כמה בדיוק זו קבוצה גדולה של תכונות שצריכה להיות לפני שתעבור לירידה במדרון, קשה לתת מספר מאוד מדויק. אבל בשבילי, זה בדרך כלל בסביבות עשרת אלפים ואז אני שוקל לעבור לירידה במדרון או אולי, לאחד מהאלגוריתמים האחרים שנדבר עליהם מאוחר יותר בקורס. לסיכום, כל עוד מספר התכונות אינו גדול מדי, המשוואה הנורמלית נותנת לנו שיטה חלופית מצוינת לחשב את הפרמטר תטא. מעשית, כל עוד מספר התכונות קטן מ-1000, אתם מבינים, הייתי משתמש, הייתי בדרך כלל משתמש בשיטת המשוואה הנורמלית ולא בירידה במדרון. כדי לצפות בתצוגה מקדימה של כמה רעיונות שעליהם נדבר בהמשך הקורס, כאשר נגיע לאלגוריתמי למידה מורכבים יותר, למשל, כאשר נדבר על אלגוריתמי סיווג, כמו אלגוריתם רגרסיה לוגיסטית, נראה כי באלגוריתמים האלה, למעשה, שיטת המשוואה הנורמלית למעשה לא עובדת עבור אותם אלגוריתמי למידה מתוחכמים יותר, וניאלץ לנקוט בירידה במדרון באלגוריתמים האלה. אז הירידה במדרון הוא אלגוריתם שימושי מאוד לדעת. ברגרסיה ליניארית שיש לה מספר רב של תכונות ובכמה אלגוריתמים אחרים שנראה בקורס זה שעבורם שיטת המשוואה הנורמלית פשוט לא חלה ואינה עובדת. אבל עבור המודל הספציפי הזה של רגרסיה ליניארית, המשוואה הנורמלית יכולה לתת לנו חלופה שיכולה להיות הרבה יותר מהירה מאשר הירידה במדרון. אז, בהתאם לפרטים של האלגוריתם שלך, בהתאם לפרטי הבעיות וכמה תכונות יש לך, שני האלגוריתמים האלה הם בהחלט דברים שטוב להכיר.