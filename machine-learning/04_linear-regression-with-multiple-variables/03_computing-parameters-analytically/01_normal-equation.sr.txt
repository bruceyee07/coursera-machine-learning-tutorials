U ovom videu ćemo govoriti o normalnoj jednačini koja za neke probleme linearne regresije daje mnogo bolji način pronalaženja optimalne vrednosti parametara teta. Konkretno, ako za linearnu regresiju koristimo algoritam gradijenta opadanja gde, da bismo dobili minimalnu vrednost funkcije greške J(teta), koristićemo ovaj iterativni algoritam koji se sastoji od mnogo koraka, mnogo iteracija gradijenta opadanja da bi konvergirao prema globalnom minimumu. Suprotno tome, normalna jednačina nam daje metod za analitičko dobijanje teta, dakle umesto da pokrećemo ovaj iterativni algoritam, umesto toga možemo da dobijemo optimalnu vrednost teta u samo jednom prolazu, tako da u jednome koraku dobijete optimalnu vrednost, pravo ovde. Ispostavlja se da normalna jednačina ima neke prednosti i neke nedostatke, ali pre nego što počnemo da govorimo o tome kada biste trebali da je koristite, hajde da prvo vidimo šta ovaj metod radi. Za primer ove sedmice kursa, zamislimo, uzmimo jednu veoma pojednostavljenu funkciju greške J(teta), to je funkcija realnog broja teta. Za sada zamislimo da je teta skalarna vrednost ili da je teta realan broj. To je samo broj, nije vektor. Zamislimo da imamo funkciju greške koja 
je kvadratna funkcija te realne vrednosti teta, tako da J(teta) izgleda ovako. Dakle, kako tražite minimum kvadratne funkcije? Neki od vas koji poznaju račun možda znaju da je način traženja minimuma funkcije nalaženje izvoda i izjednačavanje tog izvoda sa nulom. Dakle, tražite izvod od J u 
odnosu na parametar teta. Dobijate neku formulu koju neću da izvodim, postavite taj izvod da je jednak nuli a to vam omogućava da dobijete vrednost teta koja je minimum funkcije J(teta). To je bio jednostavan slučaj kada je teta realan broj. Problem koji nas interesuje je da teta nije više samo realan broj već, umesto toga, je to ovaj n + 1 dimenzionalni vektor parametara, a funkcija greške J je funkcija ovoga vektora vrednosti od teta0 do tetan. Funkcija greške izgleda ovako, 
neka kvadratna funkcija greške sa desne strane. Kako tražimo minimum ove funkcije greške? Račun nam u stvari govori da je jedan način da to uradite da izračunate parcijalne izvode od funkcije J,
 u odnosu na svaki parametar tetaj i stavite da je svaki jednak nula. Ako to uradite, i ako rešite za sve vrednosti teta0, teta1, pa sve do tetan, tada će ovo da vam da te vrednosti teta koje su minimumi funkcije greške J. Za vreme dok radite na računanju rešenja i prolazite kroz rešavanje parametara od teta0 do tetan, biće potrebno da se računaju izvodi. A ono što ću da uradim u ovome videu je u stvari da neću da računam izvode, što je dugačak proces i to je već uključeno, već ću da vam kažem samo ono što treba da znate da biste implementirali ovaj proces i tako izračunali vrednosti teta koje odgovaraju tačkama gde su parcijalni izvodi jednaki nula. Ili alternativno, ili ekvivalentno, vrednosti teta za koje je funkcija greške J(teta) minimalna. Shvatam da neki od komentara koje sam dao imaju smisla samo za vas koji ste upoznati sa računom. Ali ako ne znate, ako ne poznajete tako dobro račun, ne brinite se zbog toga. Ja ću da vam kažem šta vam je potrebno da biste implementirali ovaj algoritam i postigli da radi. Za primer koji želim da koristim kao radni primer, recimo da imam m = 4 trening primera. Da bismo implementirali ovu normalnu jednačinu, nameravam da uradim sledeće. Uzeću moj skup podataka, i evo mojih trening primera. U ovome slučaju pretpostavimo da su to svi podaci koje imam. Ono što ću da uradim je da uzmem moj skup podataka i da dodam dodatnu kolonu koja odgovara mojoj dodatnoj osobini, x0, koja uvek uzima ovu vrednost 1. Sledeće šta ću da uradim je da ću da napravim matricu i nazovem je X a ona u suštini sadrži sve osobine iz moga trening skupa, tako da su ovo ovde sve moje osobine i mi ćemo da uzmemo sve te brojeve i stavimo ih u ovu matricu X, u redu? Tako da, znate, samo kopirate podatke u svaku kolonu pojedinačno a slično 
ću da uradim za ipsilone (y). Ja ću da uzmem vrednosti koje pokušavam da predvidim i napravim vektor, kao što je ovaj i nazovem ga vektor y. Dakle, X će da bude jedna mxn+1 dimenzionalna matrica, a y će da bude jedan m dimenzionalni vektor gde je m broj trening primera a n je, n je broj osobina, n + 1, zbog ove dodatne osobine x0 koju sam dodao. Konačno, ako uzmete vašu matricu X i vaš vektor y i ako ovo proračunate, i ako postavite teta da bude jednako X transponovano X, pa to invertujete, puta X transponovano y, to će da vam da vrednost teta koje minimizuje vašu funkciju greške. Mnogo toga se desilo na slajdovima i prošao sam kroz njih koristeći 
jedan poseban skup podataka. Sada ću da ovo ispišem u jednoj malo opštijoj formi a tada ću, malo kasnije u ovome videu, 
da objasnim ovu jednačinu detaljnije, u slučaju da još nije jasno kako da uradite ovo. U opštem slučaju, recimo da imamo m trening primera, dakle x1, y1 do xn, yn i n osobina. Dakle, svaki od trening primera x(i) može da izgleda kao vektor kao što je ovaj, 
to je n + 1 dimenzionalni vektor osobina. Način na koji ću da napravim matricu X, ovo se takođe zove dizajn matrica, je kao što sledi. Svaki trening primer mi daje vektor osobina, kao što je ovaj, neka vrsta n + 1 dimenzionalnog vektora. Način na koji ću to da uradim je da ću da napravim dizajn matricu X, to je ovakva matrica, a ono što ću da uradim je da uzmem prvi trening primer, tako da je to vektor, uzmem njegov transponovani vektor tako da dobijem ovo, znate, dugačka ravna stvar i napravim da x1 transponovano 
bude prvi red moje dizajn matrice. Tada ću da uzmem moj drugi trening primer, x2, transponujem ga i postavim kao drugi red matrice X i tako dalje, do poslednjeg trening primera. Transponujmo ovo i to je moj poslednji red matrice X. I tako, ovo čini moju matricu X, mxn+1 dimenzionalnu matricu. Kao konkretan primer, recimo da imam samo jednu osobinu, stvarno, samo jednu osobinu pored x0, koja je uvek jednaka 1. Dakle, ako su moji vektori x(i) jednaki ovome, 1,što je u stvari x0, pa onda neka stvarna osobina, kao što je veličina kuće, tada će moja dizajn matrica X da bude jednaka ovome. Za kreiranje prvog reda, u suštini ću da uzmem ovo i transponujem ga. Dakle, dobiću 1 i onda x1(1). U drugome redu dobićemo 1 i nakon toga x1(2) i tako dalje skroz do 1 i onda x1(m). I tako, ovo je jedna mx2 dimenzionalna matrica. Dakle, ovako se kreira matrica X. A vektor y ponekad mogu da pišem sa strelicom na vrhu da bih označio da se radi o vektoru, ali veoma često ću da samo napišem y, kako god. Vektor y smo dobili uzimanjem svih podataka, svih tačnih cena kuća iz moga trening skupa, i i upisivanjem u jedan m dimenzionalni vektor, a to je upravo y. Konačno, kada imamo napravljenu matricu X i vektor y, tada treba samo da izračunamo teta kao X transponovano X na minus prvu puta X transponovano y. Samo želim da se uverim da vam ova jednačina ima smisla i da znate kako da je primenite. Dakle, konkretno, 
šta je ovo X transponovano X na minus prvu? Pa, X transponovano X na minus prvu je inverzna matrica od X transponovano X. Konkretno, ako postavite A da je jednako X transponovano puta X, X transponovano je matrica, X transponovano puta X vam daje još jednu matricu koja se zove matrica A. Tada, znate, X transponovano puta X inverzno je samo invertovana matrica A, u redu? Dobijamo, recimo, A inverzno. Tako se računa ovaj deo, izračunate X transponovano 
X i onda to invertujete. Još uvek nismo govorili o Octave. To ćemo da uradimo kasnije u videima koji slede, ali u Octave programskom jeziku ili u sličnim, kao što je Matlab programski jezik, komanda kojom računate ovu vrednost, X transponovano X, pa to invertujete, puta X transponovano y je kako sledi. U Octave X prim je oznaka koju koristimo da bismo 
obeležili X transponovano. Tako ovaj izraz uokviren crvenom bojom je izračunavanje X transponovano puta X. pinv je funkcija kojom se računa inverzna matrica, tako da ovo računa X transponovano X inverzno, i onda to množite sa X transponovano, a onda to množite sa y. Dakle, računate ovom formulom koju nisam dokazao, ali je matematički moguće pokazati iako neću to da radim ovde, ta formula vam daje optimalnu vrednost teta u smislu da ako stavite da je teta jednako ovome, to je vrednost teta koja minimizuje funkciju greške J(teta) za linearnu regresiju. Još jedan detalj, u prethodnom videu sam govorio o skaliranju osobina i ideji dobijanja osobina u sličnom rangu, da budu u međusobno sličnom rangu vrednosti. Ako koristite ovaj metod normalne jednačine tada skaliranje osobina u stvari nije neophodno i u stvari je u redu ako je, recimo, neka osobina x1 između 0 i 1, a neka druga osobina x2 između 0 i 1,000 a neka osobina x3 u opsegu od 0 do 10 na minus petu i ako koristite metod normalne jednačine ovo je u redu i nema potrebe da radite skaliranje osobina, a naravno, ako koristite gradijent opadanja, skaliranje osobina opet postaje važno. Konačno, kada treba da 
koristite gradijent opadanja a kada metod normalne jednačine? Evo nekih prednosti i nedostataka. Recimo da imate m trening primera i n osobina. Jedan nedostatak gradijenta opadanja je to što treba da odaberete stopu učenja alfa. Često to znači da treba da pokrenete proceduru nekoliko puta sa različitom stopom učenja alfa i vidite koja stopa daje najbolje rezultate. Dakle, to je više posla i više gnjavaže. Još jedan nedostatak gradijenta opadanja je što zahteva više iteracija. Tako, u zavisnosti od detalja, to bi moglo da uspori izračunavanje, iako se ima još šta za reći, kao što ćemo da vidimo uskoro. Za normalnu jednačinu ne treba da birate stopu učenja alfa. To ga dakle čini veoma praktičnim 
i jednostavnijim za implementiranje. Samo ga pokrenete i obično radi. Takođe ne treba da pravite iteracije, dakle, ne treba da iscrtavate J(teta) ili da proveravate konvergenciju niti 
da pravite sve te suvišne korake. Za sada, prednost je na strani normalne jednačine. Ovo su neki od nedostataka normalne jednačine, 
a prednosti gradijenta opadanja. Gradijent opadanja radi prilično dobro, čak iako imate veoma veliki broj osobina. Dakle, čak iako imate milione osobina, možete da pokrenete gradijent opadanja 
a on će da bude razumno efikasan. Uradiće nešto razumno. Za razliku od normalne jednačine, da bi se dobili parametri, treba da se reši ovaj izraz. Treba da izračunamo ovaj izraz, 
X transponovano X, inverzno. Ova matrica X transponovano X, to je (n+1)x(n+1) matrica, ako imate n osobina. Jer, ako pogledate dimenzije X transponovano i dimenzije od X, ako pomnožite, shvatićete koje su dimenzije proizvoda, matrica X transponovano X je (n+1)x(n+1) matrica gde je n broj osobina, a za većinu računarskih implementacija cena invertovanja matrica je otprilike kao kub dimenzije matrice. Dakle, računanje inverzne matrice ima vremensko trajanje reda veličine kuba. Ponekad je malo brže od n na kub ali je, znate, za naše potrebe, veoma blizu. Dakle, ako je n, broj osobina, veoma veliko, tada računanje ove količine može da bude sporo i metod normalne jednačine 
može da bude mnogo sporiji. Dakle, ako je n veliko, tada bih mogao da koristim gradijent opadanja jer ne želimo da platimo 
kubnu vremensku kompleksnost. Ali, ako je n relativno malo, normalna jednačina bi mogla da 
bude bolji način dobijanja parametara. Šta znače pojmovi malen i velik? Pa, ako je n reda veličine 100, tada inverzija 100x100 matrice nije problem po savremenim 
računarskim standardima. Ako je n 1,000, 
opet bih koristio metod normalne jednačine. Invertovanje matrice 1,000x1,000 je u stvari veoma brzo na 
savremenim računarima. Ako je n 10,000, tada bih se zapitao. Invertovanje matrice 10,000x10,000 počinje da bude nekako sporo, i tada bih počeo da naginjem na pravac gradijenta opadanja, ali ne skroz. Ako je n jednako 10,000, možete da invertujete matricu 10,000x10,000. Ali ako n postaje mnogo veće od toga, 
verovatno bih koristio gradijent opadanja. Dakle, ako je n jednako 10 na šestu, znači milion osobina, tada inverzija matrice 1,000,000x1,000,000 će da bude veoma skupa i definitivno bih dao prednost gradijentu 
opadanja ako imate toliko osobina. Dakle, koliki tačno treba da bude skup osobina pre nego koristite gradijent opadanja, 
teško je dati tačan broj. Što se mene tiče, to je obično oko 10,000, tada počinjem da razmatram korištenje gradijenta opadanja ili možda nekog drugog algoritma, 
o čemu ćemo da govorimo malo kasnije. Da zaključimo, dok god broj osobina nije prevelik, normalna jednačina nam daje sjajnu alternativu dobijanja parametara teta. Konkretno, dok god je broj osobina manji od 10,000, znate, koristio bih, obično, normalnu jednačinu pre nego gradijent opadanja. Da najavim neke od ideja o kojim ćemo da pričamo kasnije u ovome kursu, što se budemo približavali složenijim algoritmima učenja, na primer, kada budemo govorili o algoritmu klasifikacije, o algoritmu logističke regresije, videćemo da ti algoritmi u stvari, metod normalne jednačine u stvari ne radi za te sofisticiranije algoritme učenja, pa ćemo morati da pribegnemo gradijentu opadanja za te algoritme. Dakle, gradijent opadanja je veoma koristan algoritam. Algoritam linearne regresije će tada koristi veliki broj osobina a za neke druge algoritme koje ćemo da vidimo u ovome kursu, za njih metod normalne jednačine jednostavno nije primenjiv,
 jednostavno ne radi. Ali za specifičan model linearne regresije, normalna jednačina može da vam da alternativu koja može da bude mnogo brža od gradijenta opadanja. Dakle, u zavisnosti od detalja vašeg algoritma, u zavisnosti od detalja problema i toga koliko imate osobina, dobro je poznavati oba ova algoritma.