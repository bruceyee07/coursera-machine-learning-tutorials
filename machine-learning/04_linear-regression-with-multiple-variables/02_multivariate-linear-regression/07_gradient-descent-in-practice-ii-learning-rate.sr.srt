1
00:00:00,450 --> 00:00:03,210
U ovome videu želim da 
vam dam praktične savete

2
00:00:03,210 --> 00:00:05,070
o tome kako da vam gradijent opadanja radi.

3
00:00:05,070 --> 00:00:08,650
Centralna ideja ovoga videa 
biće stopa učenja alfa.

4
00:00:09,860 --> 00:00:13,180
Konkretno, ovo je pravilo 
promene gradijenta opadanja.

5
00:00:13,180 --> 00:00:16,270
A ono što ću da uradim u 
ovome videu je da vam kažem

6
00:00:16,270 --> 00:00:19,050
o tome šta ja mislim o uklanjanju grešaka, 
i neke savete da

7
00:00:19,050 --> 00:00:22,390
se osigurate da će gradijent 
opadanja da radi ispravno.

8
00:00:22,390 --> 00:00:26,480
I drugo, želim da vam kažem 
kako da odaberete stopu učenja alfa ili

9
00:00:26,480 --> 00:00:29,250
barem kako da ga birate.

10
00:00:29,250 --> 00:00:32,770
Ovo je nešto što često radim da bih 
bio siguran da će gradijent opadanja da

11
00:00:32,770 --> 00:00:34,150
radi ispravno.

12
00:00:34,150 --> 00:00:38,219
Posao gradijenta opadanja 
je da nađe vrednost teta za

13
00:00:38,219 --> 00:00:42,553
vas koja će da, nadajmo se, 
da minimizuje funkciju greške J(teta).

14
00:00:42,553 --> 00:00:47,483
Ono što često radim jeste da 
iscrtavam funkciju greške J(teta)

15
00:00:47,483 --> 00:00:49,750
dok se gradijent opadanja izvršava.

16
00:00:49,750 --> 00:00:53,515
X osa je broj iteracija gradijenta opadanja a

17
00:00:53,515 --> 00:00:58,659
kako se gradijent opadanja izvršava, 
nadam se da dobijate crtež kao što je ovaj.

18
00:00:59,720 --> 00:01:02,960
Zapamtite da je x osa broj iteracija.

19
00:01:02,960 --> 00:01:07,795
Prethodno smo videli da je 
kod crtanja J(teta) x osa,

20
00:01:07,795 --> 00:01:13,107
horizontalna osa, bila vektor 
parametara teta ali sada to nije slučaj.

21
00:01:13,107 --> 00:01:15,767
Konkretno, poenta je,

22
00:01:15,767 --> 00:01:20,570
pokrenuću gradijent opadanja za 100 iteracija.

23
00:01:20,570 --> 00:01:25,240
Koju god vrednost teta da 
dobijem nakon 100 iteracija,

24
00:01:25,240 --> 00:01:28,770
dobiću neku vrednost teta nakon 100 iteracija,

25
00:01:28,770 --> 00:01:32,630
ja ću da procenim funkciju greške J(teta).

26
00:01:32,630 --> 00:01:35,630
Za vrednost teta koju 
dobijem nakon 100 iteracija,

27
00:01:35,630 --> 00:01:39,760
a ova vertikalna visina je vrednost J(teta),

28
00:01:39,760 --> 00:01:43,630
za vrednost teta koju dobijem nakon 
100 iteracija gradijenta opadanja.

29
00:01:43,630 --> 00:01:49,620
A ova tačka ovde odgovara vrednosti J(teta) za

30
00:01:49,620 --> 00:01:53,810
teta koje dobijem nakon 
gradijenta opadanja za 200 iteracija.

31
00:01:55,230 --> 00:01:59,353
Ono što pokazuje crtež 
je vrednost funkcije greške

32
00:01:59,353 --> 00:02:02,020
nakon svake iteracije gradijenta opadanja.

33
00:02:02,020 --> 00:02:07,392
A ako gradijent opadanja radi kako treba, J(teta)

34
00:02:07,392 --> 00:02:11,671
bi trebalo da se smanjuje nakon svake iteracije.

35
00:02:17,058 --> 00:02:21,774
Ovaj crtež vam takođe može da 
pokaže još jednu korisnu stvar

36
00:02:21,774 --> 00:02:26,783
ako pogledate figuru koju sam nacrtao, 
izgleda da vremenom,

37
00:02:26,783 --> 00:02:31,350
možda za 300 iteracija, 
između 300 i 400 iteracija,

38
00:02:31,350 --> 00:02:35,720
u tom segmentu izgleda da se J(teta) više ne smanjuje.

39
00:02:35,720 --> 00:02:38,540
Dakle, za vreme 400 iteracija,

40
00:02:38,540 --> 00:02:41,520
izgleda da se ova kriva ispravila.

41
00:02:41,520 --> 00:02:46,090
Pa tako, za 400 iteracija, 
izgleda da je gradijent opadanja

42
00:02:46,090 --> 00:02:50,510
više ili manje konvergirao jer se 
vaša funkcija greške više ne smanjuje.

43
00:02:50,510 --> 00:02:53,800
Dakle, posmatranje ove krive može 
da vam pomogne da prosudite da li je

44
00:02:53,800 --> 00:02:55,829
ili nije gradijent opadanja konvergirao.

45
00:02:57,580 --> 00:03:01,630
Usput, broj iteracija koji je potreban 
gradijentu opadanja da konvergira

46
00:03:01,630 --> 00:03:04,850
za aplikaciju može mnogo da varira, pa za neku

47
00:03:04,850 --> 00:03:09,220
aplikaciju gradijent opadanja može 
da konvergira nakon samo 30 iteracija.

48
00:03:09,220 --> 00:03:15,110
Za neku drugu aplikaciju gradijent 
opadanja može da uzme 3000 iteracija,

49
00:03:15,110 --> 00:03:20,110
za neki drugi algoritam učenja 
može da traje 3 miliona iteracija.

50
00:03:20,110 --> 00:03:24,048
Videćete da je veoma teško 
reći unapred koliko je iteracija

51
00:03:24,048 --> 00:03:25,476
potrebno gradijentu opadanja da konvergira.

52
00:03:25,476 --> 00:03:30,026
I obično se crta ova vrsta crteža, 
crtanje funkcije greške u

53
00:03:30,026 --> 00:03:34,430
odnosu na povećanje broja iteracija, 
obično liči na ovaj crtež.

54
00:03:34,430 --> 00:03:37,725
Ali pokušavam da kažem da li 
gradijent opadanja konvergira.

55
00:03:37,725 --> 00:03:42,430
Takođe je moguće da koristimo 
automatski test konvergencije,

56
00:03:42,430 --> 00:03:47,560
koji ima algoritam koji će da vam 
kaže ako gradijent opadanja konvergira.

57
00:03:47,560 --> 00:03:52,310
A ovo je tipičan primer 
automatskog testa konvergencije.

58
00:03:52,310 --> 00:03:57,100
Takav test može da deklariše 
konvergenciju ako se funkcija greške J(teta)

59
00:03:57,100 --> 00:04:01,220
smanjuje za manje od 
neke male vrednosti epsilon,

60
00:04:01,220 --> 00:04:05,340
neke male vrednosti 10 na -3 u jednoj iteraciji.

61
00:04:05,340 --> 00:04:10,460
Ali shvatio sam da je nalaženje 
tog praga prilično teško.

62
00:04:10,460 --> 00:04:13,840
Da bih proverio konvergenciju 
gradijenta opadanja

63
00:04:13,840 --> 00:04:18,110
ja gledam grafikon kao što je ovaj, 
kao ova figura na levoj strani,

64
00:04:18,110 --> 00:04:21,740
pre nego da se oslonim na 
automatski test konvergencije.

65
00:04:21,740 --> 00:04:25,370
Posmatranje ove figure takođe 
može da vas unapred upozori

66
00:04:25,370 --> 00:04:28,730
da možda gradijent opadanja ne radi ispravno.

67
00:04:28,730 --> 00:04:33,600
Konkretno, ako iscrtate J(teta) 
kao funkciju broja iteracija,

68
00:04:33,600 --> 00:04:38,280
i ako vidite figuru kao što je 
ova gde se J(teta) u stvari povećava,

69
00:04:38,280 --> 00:04:43,110
tada je to jasan znak da 
gradijent opadanja ne radi.

70
00:04:43,110 --> 00:04:47,250
Figura kao što je ova obično znači da 
biste trebali da koristite stopu učenja alfa.

71
00:04:48,320 --> 00:04:52,885
Ako se J(teta) povećava, najčešći uzrok je,

72
00:04:52,885 --> 00:04:58,370
ako pokušavate da minimizujete 
funkciju koja izgleda možda ovako,

73
00:04:59,380 --> 00:05:02,545
ali ako je stopa učenja prevelika 
i ako počnete ovde,

74
00:05:02,545 --> 00:05:06,090
gradijent opadanja može da 
promaši minimum i pošalje vas ovde.

75
00:05:06,090 --> 00:05:07,450
A ako je stopa opadanja prevlika,

76
00:05:07,450 --> 00:05:11,525
opet možete da promašite, i tako dalje.

77
00:05:11,525 --> 00:05:15,075
Ono što biste stvarno želeli je da počnete ovde i

78
00:05:15,075 --> 00:05:17,975
da polako silazite, u redu?

79
00:05:17,975 --> 00:05:20,096
Ali ako je stopa učenja prevelika,

80
00:05:20,096 --> 00:05:24,284
gradijent opadanja može da maši minimum.

81
00:05:24,284 --> 00:05:26,617
I tako dobijate sve lošije i lošije vrednosti,

82
00:05:26,617 --> 00:05:30,690
i tako sve veće vrednosti funkcije greške J(teta).

83
00:05:30,690 --> 00:05:34,140
Dakle, dobijete graf kao što 
je ovaj i ako ovo vidite

84
00:05:34,140 --> 00:05:38,660
obično je rešenje da 
koristite manju vrednost alfa.

85
00:05:38,660 --> 00:05:41,820
I naravno, uverite se da 
vaš kod nema neku grešku.

86
00:05:41,820 --> 00:05:46,700
Ali obično prevelika vrednost 
alfa može da bude opšti problem.

87
00:05:49,020 --> 00:05:53,090
Slično, ponekad možete da vidite 
da J(teta) radi nešto poput ovoga,

88
00:05:53,090 --> 00:05:56,890
neko vreme ide dole, a onda gore, 
opet ide dole, pa gore,

89
00:05:56,890 --> 00:05:58,850
neko vreme dole, pa gore i tako dalje.

90
00:05:58,850 --> 00:06:03,130
A rešenje za ovo je isto tako 
korištenje manje vrednosti alfa.

91
00:06:04,150 --> 00:06:05,400
Nećemo ovo ovde da dokazujemo,

92
00:06:05,400 --> 00:06:09,560
ali pod ostalim pretpostavkama o 
funkciji greške J(teta), ovo je istina

93
00:06:09,560 --> 00:06:14,180
za linearnu regresiju, 
matematičari su dokazali da za dovoljno male

94
00:06:14,180 --> 00:06:19,030
stope učenja alfa, J(teta) bi 
trebalo da se smanjuje u svakoj iteraciji.

95
00:06:19,030 --> 00:06:21,979
A ako se to ne desi, verovatno je alfa preveliko,

96
00:06:21,979 --> 00:06:23,810
trebali biste da smanjite alfa.

97
00:06:23,810 --> 00:06:26,430
Ali, takođe ne želite da 
stopa učenja bude premala

98
00:06:26,430 --> 00:06:30,830
jer ako to imate, 
gradijent opadanja može da bude spor.

99
00:06:31,930 --> 00:06:36,760
Ako je alfa premalo, 
možete da počnete recimo ovde,

100
00:06:36,760 --> 00:06:40,930
i ako pravite jako male korake,

101
00:06:40,930 --> 00:06:47,100
i ako imate mnogo iteracija pre 
nego što dođete do minimuma,

102
00:06:47,100 --> 00:06:50,990
i ako je alfa premalo, 
gradijent opadanja može da sporo napreduje i

103
00:06:50,990 --> 00:06:52,360
da sporo konvergira.

104
00:06:52,360 --> 00:06:55,510
Da zaključimo, ako je stopa učenja premalena,

105
00:06:55,510 --> 00:06:59,845
imate problem spore konvergencije, 
a ako je prevelika,

106
00:06:59,845 --> 00:07:05,640
J(teta) se možda ne bude smanjivalo u 
svakoj iteraciji i možda ne konvergira.

107
00:07:05,640 --> 00:07:11,490
U nekim slučajevima je moguće da stopa učenja 
bude prevelika a da se desi spora konvergencija.

108
00:07:11,490 --> 00:07:15,220
Ali je uobičajeniji problem koji vidite da

109
00:07:15,220 --> 00:07:19,040
se J(teta) ne smanjuje u svakoj iteraciji.

110
00:07:19,040 --> 00:07:23,810
Da bismo otklonili greške u ovakvim 
slučajevima, crtajte često J(teta)

111
00:07:23,810 --> 00:07:27,810
kao funkciju broja iteracija 
da shvatite šta se dešava.

112
00:07:27,810 --> 00:07:31,620
Konkretno, ono što ja radim 
kada pokrenem gradijent opadanja

113
00:07:31,620 --> 00:07:33,500
je da pokušam opsege vrednosti.

114
00:07:33,500 --> 00:07:36,460
Dakle, samo pokušajte da pokrenete 
gradijent opadanja sa opsegom vrednosti za

115
00:07:36,460 --> 00:07:38,670
alfa, kao 0.001 i 0.01

116
00:07:38,670 --> 00:07:41,550
Dakle, razlikuju se deset puta.

117
00:07:41,550 --> 00:07:45,250
Za ove različite vrednosti alfa 
iscrtajte J(teta) kao funkciju

118
00:07:45,250 --> 00:07:47,290
broja iteracija, i

119
00:07:47,290 --> 00:07:54,160
tada uzmite vrednost alfa koja se 
čini da deluje da J(teta) opada brzo.

120
00:07:54,160 --> 00:07:57,180
U stvari, ono što ja radim nisu 
ovi koraci sa faktorom deset.

121
00:07:57,180 --> 00:08:00,970
Dakle, ovo je faktor povećanja 
deset za svaki korak.

122
00:08:00,970 --> 00:08:03,679
Ono što ja radim je da 
uzmem ovaj opseg vrednosti.

123
00:08:06,827 --> 00:08:09,985
I tako dalje gde je ovo 0.001.

124
00:08:09,985 --> 00:08:13,613
Tada povećavam stopu učenja 
trostruko da dobijem 0.003.

125
00:08:13,613 --> 00:08:15,232
A onda ovaj korak,

126
00:08:15,232 --> 00:08:20,627
evo još jednoga približnog trostrukog 
povećanja od 0.003 do 0.01.

127
00:08:20,627 --> 00:08:25,512
I tako, ovo su, približno, 
pokušaji gradijenta opadanja sa svakom

128
00:08:25,512 --> 00:08:30,640
vrednošću otprilike 3 puta većom od prethodne.

129
00:08:30,640 --> 00:08:33,316
Dakle, ono što radim je da pokušavam 
opseg vrednosti dok ne pronađem

130
00:08:33,316 --> 00:08:37,078
vrednost koja je premalena 
i jednu koja je prevelika.

131
00:08:37,078 --> 00:08:40,966
A onda pokušam da uzmem 
najveću moguću vrednost,

132
00:08:40,966 --> 00:08:45,943
ili nešto malo manje nego najveća 
razumna vrednost koju sam našao.

133
00:08:45,943 --> 00:08:51,780
A kada to uradim, obično 
mi da dobru stopu učenja za moj problem.

134
00:08:51,780 --> 00:08:55,910
Ako i vi to uradite, možda budete 
mogli da odaberete dobru stopu učenja

135
00:08:55,910 --> 00:08:58,010
za implementaciju gradijenta opadanja.