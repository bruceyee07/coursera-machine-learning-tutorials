Za sada smo videli par različitih algoritama učenja, linearnu regresiju i logističku regresiju. One su pogodne za mnoge probleme, ali kad ih primenite na određene aplikacije mašinskog učenja, bi mogle da naiđu na problem zvani preprilagođenost koji može da izazove loše performanse. Ono što bih želeo da uradim u ovom video je da vam objasnim šta je problem preprilagođenosti, a u sledećih nekoliko videa nakon ovoga, govorićemo o tehnici zvanoj regularizacija, to će nam omogućiti da poboljšamo ili da smanjimo problem preprilagođenosti i učinimo da ti algoritmi 
učenja rade mnogo bolje. Dakle, šta je to preprilagođenost? Nastavimo da koristimo naš postojeći primer predviđanja cena nekretnina sa linearnom regresijom gde želite da predvidite cenu kao funkciju od veličine kuće. Ono što možemo da uradimo je da pridružimo linearnu funkciju ovim podacima, i ako to uradimo, dobićemo otprilike ovakvu pravu liniju
 koja odgovara podacima. Ali ovo nije baš najbolji model. Ako pogledamo podatke, čini se veoma jasno da ako veličina nekretnina raste, njihova cena se ne menja mnogo, ili poravnava se kako se pomeramo na desno, dakle ovaj algoritam ne odgovara trening skupu u potpunosti i to zovemo problemom nedostatka, a drugi naziv za ovo je da algoritam ima visoko odstupanje. Oba ova gruba značenja kažu da podaci ne odgovaraju baš najbolje. Pojam odstupanje je više istorijski ili tehnički ali ideja je da ako prava linija odgovara podacima, tad, ako algoritam ima veoma jako predznanje, ili veoma jako odstupanje, tad će cene nekretnina da se menjaju linearno sa promenom njihovih veličina
 uprkos podacima. Uprkos činjenici da je predznanje još uvek odstupanje, još uvek je blizu da odgovara pravoj liniji i to na kraju ne odgovara podacima. Sad, u sredini bismo mogli da pridružimo kvadratnu funkciju i sa ovim skupom podataka, pridružujemo kvadratnu funkciju, možda dobijemo ovu vrstu krive i to radi prilično dobro. A druga krajnost bi bila da podacima pridružimo,
 recimo, polinom četvrtog stepena. Dakle, ovde imamo pet parametara, teta nula do teta četiri, i sa time možemo da nacrtamo krivu koja prolazi kroz svih pet trening primera. Mogli biste da dobijete 
krivu koja izgleda kao ova. U jednu ruku čini se da pravi veoma dobar posao pridruživanja trening skupu i barem obuhvata sve moje podatke. Ali, to je još uvek veoma
 krivudava kriva, zar ne? Ide gore, dole čitavom dužinom, i u stvari mislimo da je to dobar model za 
predviđanje cena nekretnina. Dakle, ovaj problem zovemo prekomernost, a drugi pojam za ovo je da algoritam ima veliko variranje. Pojam variranje je još jedan istorijski ili tehnički pojam. Ali je intuitivno da ako pridružimo polinom tako velikog stepena, tad pretpostavka može da odgovara, znate, skoro da može da odgovara skoro svakoj funkciji i ovakva pretpostavka je prevelika, previše je varijabilna. Nemamo dovoljno podataka da je ograničimo da nam da dobru pretpostavku i to se zove prekomernost. A u sredini, nema imena kojim zovemo ovo ali upravo ću da 
napišem, znate, samo ispravno, gde se polinom drugog stepena, 
kvadratna funkcija čini da je ispravna da bi 
se pridružila podacima. Da zaključimo, problem prekomernosti se javlja kad imamo mnogo osobina, naučena hipoteza može veoma 
dobro da odgovara trening skupu. Dakle, vaša funkcija može da bude veoma blizu nule ili da bude tačno nula, ali možete da dobijete krivu kao što je ova, koja uporno pokušava da odgovara trening skupu, tako da bezuspešno generalizuje nove primere i bezuspešno predviđa cene u novim primerima a ovaj pojam generalizovan se odnosi na to koliko dobro hipoteza odgovara novim primerima. Tako je sa podacima kuća koje se ne vide u trening skupu. Na ovom slajdu smo videli prekomernost na slučaju linearne regresije. Isto tako, slična stvar se može
 primeniti na logističku regresiju. Ovo je primer logističke regresije sa dve osobine X1 i X2. Ono što bismo mogli da uradimo je da logističkoj regresiji pridružimo jednostavnu hipotezu kao što je ova, gde je g kao i obično 
moja sigmoidna funkcija. Ako to uradite, dobićete hipotezu koja pokušava da koristi pravu liniju da odvoji pozitivne 
i negativne primere. Ovo ne odgovara baš najbolje hipotezi. Dakle, još jednom, ovo je primer nedostatka ili to da hipoteza ima veliko odstupanje. Suprotno tome, ako ste svojim osobinama dodali ove kvadratne pojmove, tad biste mogli da dobijete granicu odluke koja bi mogla da
 izgleda više ovako. Znate, to prilično dobro odgovara podacima. Verovatno najbolje što smo mogli da dobijemo,
 na ovom trening skupu. I konačno, na drugoj krajnosti, ako pokušate da pridružite polinom 
velikog stepena, ako generišete mnogo pojmova osobina u polinomu 
visokog stepena, tad logistička regresija može da se iskrivi, može uporno da pokušava da nađe granicu odluke koja odgovara vašim trening podacima ili da se poveća da bi se iskrivila i tako odgovarala svakom
 pojedinačnom trening primeru. I ako osobine X1 i X2 nude predviđanje, možda, raka, recimo, rak dojke je maligna ili benigna vrsta tumora. Ovo stvarno ne izgleda kao dobra hipoteza kada se prave predviđanja. Još jednom, ovo je primer prekomernosti a hipoteza ima veliko odstupanje i ne odgovara novim primerima. Kasnije kada budemo govorili o otkrivanju grešaka i dijagnostici stvari koje mogu da krenu na loše sa algoritmima učenja, 
daćemo vam određeni alat koji prepoznaje kad se prekomernost i, takođe, nedostatak možda dese. Za sada, govorićemo o problemu, ako pretpostavimo da se dešava prekomernost, kad ćemo moći da upotrebimo taj alat. U prethodnim primerima, imali smo jednodimenzionalne ili 
dvodimenzionalne podatke, dakle mogli smo samo da 
iscrtamo hipotezu i vidimo šta se dešava i odaberemo 
odgovarajući stepen polinoma. Tako, u ranijem primeru određivanja cena nekretnina, mogli smo samo da iscrtamo hipotezu i, možda vidimo da li odgovara vrsti neke veoma krivudave funkcije koja se proteže preko 
cele površine da bi predvidela cene nekretnina. Tada bismo mogli da koristimo ovakve oblike da bismo odabrali 
odgovarajući stepen polinoma. Dakle, iscrtavanje hipoteze bi mogao da bude način donošenja odluke koji stepen polinoma da se koristi. Ali to ne uspeva uvek. I, veoma često ćemo imati problem 
učenja sa mnogo osobina. A to se ne tiče samo odabira stepena polinoma. U stvari, kada imamo previše osobina, takođe je teže iscrtati podatke i postaje teže učiniti ih vidljivim, da bi se odlučilo koje podatke zadržati. Konkretno, ako pokušamo da predvidimo cene nekretnina 
moguće je da imamo mnogo različitih osobina. Mnoge od njih mogu da se
 čine kao da su korisne. Ali, ako imamo mnogo osobina i veoma mali trening skup tad prekomernost može da bude problem. Da bi se otklonio problem prekomernosti, postoje dve glavne opcije šta možemo da uradimo. Prva je da probamo da smanjimo broj osobina. Konkretno, mogli bismo da manuelno pregledamo listu osobina i da pokušamo da odlučimo koje osobine su važnije i stoga koje bismo trebali da zadržimo a koje da odbacimo. Kasnije u ovom kursu ćemo da govorimo o algoritmima odabira modela. Koji su algoritmi za automatsko odlučivanje koje osobine treba zadržati a koje odbaciti. Ova ideja smanjenja broja osobina može da se pokaže dobro i može da smanji prekomernost. A kada govorimo o odabiru modela, ući ćemo mnogo dublje. Ali je mana to što, odbacivanjem nekih osobina takođe odbacujemo neke informacije o problemu. Na primer, sve te osobine su u stvari korisne za predviđanje cene kuće pa možda nije dobro da odbacimo neke od informacija ili neke od osobina. Druga opcija, o kojoj ćemo da govorimo u sledećih nekoliko videa je regularizacija. Ovde ćemo da zadržimo sve osobine, ali ćemo da smanjimo veličinu ili vrednosti parametara teta J. A taj model radi dobro, videćemo, kada imamo mnogo osobina koje sve učestvuju pomalo u predviđanju vrednosti Y, kao što smo videli u primeru 
 predviđanja cena nekretnina, gde bismo mogli da imamo mnogo osobina, od kojih su sve korisne, tako da 
 ne želimo da ih odbacimo. Dakle, ovo najavljuje ideju regularizacije na 
veoma visokom nivou. I shvatio sam da vam svi ti detalji verovatno još 
uvek nemaju smisla. Ali u sledećem videu ćemo da počnemo da formulišemo kako da primenimo regularizaciju i 
šta tačno regularizacija znači. I tad ćemo početi da shvatamo kako da to koristimo, da učinimo da algoritam učenja radi dobro i da izbegnemo prekomernost.