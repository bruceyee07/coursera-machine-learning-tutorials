עבור רגרסיה לוגיסטית, דיברנו בעבר על שני סוגים של אלגוריתמי אופטימיזציה. דיברנו על השימוש בירידה במדרון כדי לייעל את פונקצית העלות J של תטא. וגם דיברנו על שיטות אופטימיזציה מתקדמות. אלה המחייבות אותך לספק דרך לחשב את פונקציית העלות J של תטא וכן לספק דרך לחשב את הנגזרות שלה. בסרטון הזה נציג כיצד ניתן להתאים את שתי הטכניקות הללו, הן את הירידה במדרון והן את טכניקות האופטימיזציה המתקדמות יותר, כדי שיוכלו לעבוד עבור רגרסיה לוגיסטית מוסדרת (לאחר הַסְדָרָה). אז הנה הרעיון. ראינו קודם לכן כי רגרסיה לוגיסטית עלולה להיות מועדת להתאמת יתר אם אתה בונה התאמה טובה מדי של סדרת האימון על ידי פונקציה פולינומית מסדר גבוה מדי. כאשר g היא פונקציית הסיגמואיד ואז אתה מקבל בסופו של דבר השערה, אתם מבינים, שהיא בעצם סוג של פונקציה מורכבת מדי ומעוותת מאוד שאיננה באמת כזו פונקצית השערה מוצלחת עבור קבוצת האימון הנתונה, ובמיוחד זה קורה אם יש לך רגרסיה לוגיסטית עם הרבה תכונות. לא בהכרח תכונות מתאימות לפולינום, אלא פשוט כשיש הרבה תכונות אתה יכול להגיע להשערה עם התאמת יתר. זו היתה פונקציית העלות שלנו עבור רגרסיה לוגיסטית. ואם אנחנו רוצים לשנות את זה ולהשתמש בהסדרה, כל מה שאנחנו צריכים לעשות הוא להוסיף לו את המחובר הבא פלוס למדא חלקי 2m, כפול הסכום מ-j שווה 1, כרגיל מ-j שווה 1. ולא הסכום מ-j שווה 0, של תטא-j בריבוע. ולזה יש האפקט של צמצום הפרמטרים תטא-1 תטא-2 וכן הלאה עד תטא-N כדי שלא יהיו גדולים מדי. ואם תעשה את זה, יהיה לזה האפקט שלמרות שאתה מתאים פולינום מסדר מאוד גבוה עם הרבה פרמטרים, כל עוד אתה מחיל עליו הסדרה ומגביל את הפרמטרים מלגדול אתה צפוי לקבל גבול החלטה שאולי נראה יותר ככה. שנראה יותר הגיוני שהוא מפריד בין הדוגמאות החיוביות לבין הדוגמאות השליליות. אז בעת שימוש בהסדרה, גם כאשר יש לך הרבה תכונות, ההסדרה יכולה לעזור לטפל בבעיית התאמת היתר. איך אנחנו באמת מיישמים את זה? ובכן, עבור האלגוריתם המקורי לירידה במדרון, הנה איך שעשינו את שלבי העדכון. אנחנו מבצעים שוב ושוב את העדכון הבא לתטא-j. השקף הזה נראה מאד דומה לקודם עבור רגרסיה ליניארית. אבל מה שאני הולך לעשות כאן הוא לכתוב את העדכון עבור תטא-0 בנפרד. אז השורה הראשונה היא העדכון עבור תטא-0 והשורה השנייה היא העדכון עבור תטא-1 עד תטא-n. כי אני מתכוון לטפל בתטא-0 בנפרד. וכדי לשנות את האלגוריתם הזה, כדי להשתמש בפונקצית מחיר מוסדרת, כל מה שאני צריך לעשות הוא די דומה למה שעשינו עבור רגרסיה ליניארית, בעצם פשוט לשנות את כלל העדכון השני כדלקמן. ושוב, זה, אתם מבינים, נראה זהה קוסמטית למה שהיה לנו עבור רגרסיה ליניארית. אבל כמובן זה לא אותו אלגוריתם שהיה לנו, כי עכשיו ההשערה מוגדרת באמצעות הנוסחה הזו. לכן זה לא אותו אלגוריתם כמו רגרסיה ליניארית מוסדרת. כי פונקצית ההשערה שונה. למרות שהעדכון הזה שכתבתי כאן באמת נראה זהה קוסמטית למה שהיה לנו קודם. אנחנו עובדים כאן על ירידה במדרון
עבור רגרסיה ליניארית מוסדרת. וכמובן, רק כדי לסכם את הדיון הזה, המונח הזה כאן בסוגריים המרובעים, המונח הזה כאן, המונח הזה הוא, כמובן, הנגזרת החלקית החדשה ביחס לתטא-j של פונקציית העלות החדשה j של תטא. כש-j של תטא כאן היא פונקציית העלות שהגדרנו בשקופית הקודמת שעושה שימוש בהסדרה. אז, זוהי הירידה במדרון עבור רגרסיה ליניארית מוסדרת. בואו נדבר על איך לגרום לרגרסיה לינארית מוסדרת לעבוד עם שיטות האופטימיזציה המתקדמות יותר. ופשוט להזכיר לכם, לגבי השיטות האלה מה שהיה עלינו לעשות היה להגדיר את הפונקציה הזו שנקראת פונקציית עלות, שמקבלת כפרמטר קלט את הוקטור תטא ושוב זכרו שבמשוואות שכתבנו כאן השתמשנו בוקטורים עם אינדקס החל מ-0. אז היה לנו תטא-0 עד תטא-n. אבל מאחר ובאוקטבה הוקטורים מתחילים באינדקס 1 תטא-0 כתוב באוקטבה כתטא-1. תטא-1 כתוב באוקטבה כתטא-2, וכן הלאה עד תטה-(1+N). ומה שהיה עלינו לעשות היה לספק פונקציה. לספק פונקציה שנקראת פונקציית עלות, שאותה נוכל לשלוח כפרמטר קלט למה שראינו קודם. אנו נשתמש בפונקציית fminunc כפונקצית עלות , וכן הלאה, נכון. אבל fminunc הוא fmin unconstrained - בלתי מוסדר והמונח הזה יעבוד עם fminunc שביחד יקחו את פונקציית העלות וימזערו אותה עבורנו. אז שני הדברים העיקריים שפונקצית העלות צריכה להחזיר היו קודם כל jVal. ובשביל זה, אנחנו צריכים לכתוב קוד כדי לחשב את פונקצית העלות j של תטא. כעת, כאשר אנו משתמשים ברגרסיה לוגיסטית מוסדרת, כמובן שפונקציית העלות j של תטא משתנה, ובאופן ספציפי, כעת, פונקצית העלות צריכה לכלול גם את מונח ההסדרה הנוסף הזה בסוף. לכן, כאשר אתה מחשב את j של תטא הקפד לכלול את המונח הזה בסוף. ואז, הדבר השני שפונקצית העלות צריכה היא דרך לעשות נגזרות - גרדיינטים. אז הגרדיינט הראשון צריך להיות הנגזרת החלקית של j של תטא ביחס לתטא-0, הגרדיינט השני הוא הנגזרת ביחס לתטא-1, וכן הלאה. וזכרו, האינדקסים מוסטים באחת. זוכרים, כי האינדקס בהם משתמשת אוקטבה הם מ-1. וכשנסתכל על המונחים האלה, המונח הזה כאן. אנחנו למעשה חישבנו אותו בשקופית הקודמת והוא למעשה שווה לזה. זה לא משתנה. כי הנגזרת עבור תטא-0 לא משתנה בהשוואה לגרסה ללא הסדרה. המונחים האחרים כן משתנים. בפרט, לגבי הנגזרת של תטא-1. גם את זה חישבנו בשקופית הקודמת. והיא שווה, אתה זוכרים, למונח המקורי פלוס למבדא חלקי m כפול תטא-1. רק כדי לוודא שאנחנו מעבירים את זה נכון כדאי לנו להוסיף כאן סוגריים. נכון, כך שהסיגמא לא יכלול את המונח שהוספנו. ובדומה לכך, אתם יודעים, המונח השני כאן נראה כך, עם המונח הנוסף הזה שגם הוא היה לנו בשקופית הקודמת, המתאים לשיפוע שלנו בפונקציה עם ההסדרה. אז אם תיישמו את פונקצית העלות הזו ותעבירו אותה ל-fminunc או לאחת מאותן טכניקות אופטימיזציה מתקדמות, זה ימזער את פונקציה העלות המוסדרת החדשה j של תטא. והפרמטרים שהפונקציה תחזיר יהיו אלה המתאימים לרגרסיה לוגיסטית עם הסדרה. זהו, עכשיו אתם יודעים איך ליישם רגרסיה לוגיסטית מסודרת. כאשר אני מסתובב בעמק הסיליקון, אני גר כאן בעמק הסיליקון, יש המון מהנדסים שבאמת עושים טונות של כסף עבור החברות שלהם
באמצעות אלגוריתמים של למידה ממוחשבת. ואני יודע שאנחנו רק, אתo יודעים, ממש התחלנו ללמוד את הדברים האלה. אבל אם אתם מבינים ויודעים ליישם רגרסיה לינארית, אלגוריתמי אופטימיזציה מתקדמים והסדרה, אז עכשיו, ברצינות, אתם בטח יודעים הרבה יותר על למידת מכונה מאשר הרבה מאד, בהחלט, אתם בטח יודעים די הרבה על למידת מכונה עכשיו יותר מבאמת הרבה מהנדסים בעמק הסיליקון
שעשו מזה קריירה מוצלחת מאוד. אתם יודעים, שעושים טונות של כסף
עבור החברות שלהם. או בונים מוצרים
באמצעות אלגוריתמים של למידה ממוחשבת. אז מזל טוב. עברתם כברת דרך ארוכה. ואתם יכולים למעשה, אתם באמת יודעים מספיק כדי ליישם את הדברים האלה
ולגרום להם לעבוד עבור בעיות רבות. אז ברכות על כך. אבל כמובן, עדיין יש עוד הרבה שאנחנו רוצים ללמד אתכם, ובקבוצה הבאה של קטעי וידאו אחרי זה, נתחיל לדבר על קבוצה חזקה מאוד של מסווגים לא ליניאריים. אז בעוד ברגרסיה ליניארית, רגרסיה לוגיסטית, אתם יודעים, אתם יכולים ליצור מונחים פולינומיים, אבל מתברר שיש מסווגים הרבה יותר חזקים שאינם לינאריים שיותר חזקים מרגרסיה פולינומית. ובסט הבא של קטעי וידאו אחרי זה,
אני אתחיל לספר לכם עליהם. כך שיהיו לכם אלגוריתמי למידה
אפילו יותר חזקים מאשר יש לכם עכשיו כדי ליישם אותם על בעיות שונות.