עד עכשיו, כבר ראיתם כמה אלגוריתמים שונים של למידה, רגרסיה ליניארית ורגרסיה לוגיסטית. הם עובדים היטב עבור בעיות רבות, אבל כאשר אתה מיישם אותם על יישומי למידת מכונה מסוימים, הם עלולים להיתקל בבעיה שנקראת overfitting או עודף התאמה, שתגרום להם לביצועים גרועים מאוד. מה שאני רוצה לעשות בסרטון הזה הוא להסביר לכם מהי בעיית עודף ההתאמה, ואז בסרטונים הבאים לאחר מכן, נדבר על טכניקה שנקראת הסדרה, שתאפשר לנו לשפר או להפחית את בעיית עודף ההתאמה הזו ולגרום לאלגוריתמי הלמידה האלה אולי לעבוד הרבה יותר טוב. אז מה הוא עודף התאמה? בואו נמשיך להשתמש בדוגמה הקבועה שלנו לחיזוי מחירי הדירות באמצעות רגרסיה ליניארית, בה אנו רוצים לחזות את המחיר כפונקציה של גודל הבית. דבר אחד שאנחנו יכולים לעשות הוא להתאים פונקציה ליניארית לנתונים אלה, ואם נעשה את זה, אולי נקבל סוג כזה של קו ישר שמתאים יפה לנתונים. אבל זה לא מודל ממש טוב. כאשר מסתכלים על הנתונים, נראה די ברור שככל שגודל הדירות עולה, מחירי הדיור מתקרבים לאסימפטוטה, או סוג של השתטחות כאשר אנחנו נעים ימינה ולכן האלגוריתם הזה לא מתאים לסדרת האימון ואנחנו קוראים לבעיה הזו חוסר התאמה, ומונח נוסף לכך הוא שלאלגוריתם הזה יש הטיה גבוהה. שני אלה בערך אומרים בפשטות שהאלגוריתם לא מתאים לנתוני האימון בצורה טובה מאוד. המונח הטיה הוא מין מונח היסטורי או טכני, אבל הרעיון הוא שאם מתאימים קו ישר לנתונים, זה כאילו לאלגוריתם יש דעה קדומה חזקה מאוד, או הטיה חזקה מאוד שמחירי הדירות הולכים להשתנות באופן ליניארי לפי הגודל שלהם למרות שהנתונים לא מראים כך. למרות הראיות שהמצב הפוך, הדעה הקדומה או ההטייה, עדיין מתעקשת להתאים לו קו ישר ובסופו של דבר מקבלים התאמה גרועה לנתונים. עכשיו, כגישת ביניים, אנחנו יכולים להתאים כאן פונקציה ריבועית, ועם נתוני האימון האלה, אנו מתאימים פונקציה ריבועית, ואז אנחנו אולי מקבלים סוג כזה של עקומה וזה עובד די טוב. ובקצה הקיצוני האחר, אנחנו יכולים להתאים,
נניח, פולינום מדרגה רביעית לנתונים. אז כאן יש לנו חמישה פרמטרים, תטא-אפס עד תטא-ארבע, ועם אלה, אנחנו יכולים למעשה ליצור עקומה שעוברת ממש דרך כל חמש דוגמאות האימון שלנו. אתה יכול לקבל עקומה שנראית ככה. זה, מצד אחד, נראה שעושה עבודה טובה מאוד בהתאמת ערכת האימון, כי הוא עובר דרך כל הנתונים שלי, לפחות. אבל, זו עדיין עקומה מתפתלת מאוד, נכון? היא עולה ויורדת בכל מיני מקומות, ולכן אנחנו לא חושבים שזה מודל כל כך טוב לניבוי מחירי הדירות. אז לבעיה הזו אנו קוראים עודף התאמה, ויש עוד מונח אחר עבור זה והוא שלאלגוריתם הזה יש שונות גבוהה. המונח 'שונות גבוהה' הוא עוד מונח היסטורי או טכני. אבל האינטואיציה היא שאם אנחנו מתאימים את הנתונים לפולינום מדרגה כזו גבוהה אזי ההשערה יכולה להתאים, כך נראה, כמעט כאילו היא יכולה להתאים לכל פונקציה, והמקדמים של ההשערה האפשרית הזו הם רבים מדי, יותר מדי משתנים. ואין לנו מספיק נתונים כדי להגביל את זה ולתת לנו השערה טובה אז זה נקרא עודף התאמה. ובאמצע, אין לזה באמת שם, אבל אני פשוט אקרא לזה "בדיוק טוב". כאשר פולינום מדרגה שניה, פונקציה ריבועית נראית מתאימה בדיוק להתאמה לנתונים האלה. כדי לסכם קצת את הבעיה של התאמת יתר, היא קורה כאשר יש לנו יותר מדי תכונות, אז ההשערה הנלמדת עשויה להתאים
לקבוצת האימון בצורה טובה מאוד. אז פונקציית העלות שלך עשויה להיות קרובה מאוד לאפס או אולי אפילו אפס בדיוק, אבל אתה בסופו של דבר מקבל עקומה כמו זו, שמנסה יותר מדי להתאים לערכת האימון, כך שהיא נכשלת בהכללה לדוגמאות חדשות ולא מצליחה לחזות מחירים בדוגמאות חדשות, וכאן המונח "הכללה" מתייחס לעד כמה ההשערה חלה גם על דוגמאות חדשות. כלומר לנתונים לגבי בתים שהיא לא ראתה במערך ההדרכה. בשקופית זו, הסתכלנו על התאמת יתר במקרה של רגרסיה ליניארית. דבר דומה יכול לחול גם על רגרסיה לוגיסטית. הנה דוגמה של רגרסיה לוגיסטית עם שתי תכונות X1 ו-X2. דבר אחד שאנחנו יכולים לעשות הוא לבנות רגרסיה לוגיסטית הולמת עם השערה פשוטה כמו זו, שבה, כרגיל, G היא הפונקציה הסיגמואידית שלי. ואם אתה עושה את זה, אתה מקבל השערה שמנסה אולי להשתמש פשוט בקו ישר להפריד בין דוגמאות חיוביות ושליליות. וזה לא נראה כהתאמה יותר מדי טובה להשערה. אז, שוב, זוהי דוגמה של התאמת חסר או של השערה בעלת הטיה גבוהה. לעומת זאת, אם היינו מוסיפים לתכונות שלנו את המונחים הריבועיים האלה, נוכל לקבל גבול או סף החלטה שעשוי להיראות יותר כך. וכפי שניתן לראות, זה מתאים לנתונים בצורה די טובה, כנראה, בערך הכי טוב שאפשר לקבל בהינתן קבוצת האימון הזו. ולבסוף, בקצה השני, אם הייתם בונים התאמה לפולינום גבוה מאוד, אם הייתם מייצרים הרבה מונחים פולינומים מסדר גבוה, אזי הרגרסיה הלוגיסטית עלולה לעוות את עצמה כדי לקבל גבול החלטה שמתאים לנתוני האימון או היא תעשה מאמצים רבים ותעוות את עצמה, כדי להתאים בצורה טובה לכל דוגמאות האימון. וכמו שאמרנו קודם, זה יחזה טוב את התכונות X1 וX2, אולי, אבל את הסרטן, אתם זוכרים, גידולים של סרטן שד
הם ממאירים או שפירים, זה לא, זה באמת לא נראה כמו ההשערה הטובה ביותר
על מנת לבצע תחזיות. וכך, שוב, זהו מקרה של התאמת יתר ושל השערה בעלת שונות גבוהה והיא לא ממש, כן, לא סביר שתהיה מוצלחת במיוחד
בהכללה לדוגמאות חדשות. מאוחר יותר, בקורס הזה, כשנדבר על איתור באגים ואבחון של דברים שיכולים להשתבש באלגוריתמים של למידה, אנחנו נספק כלים ספציפיים כדי לזהות מתי יש התאמת יתר, וגם מתי עשויה להתרחש התאמת חסר. אבל בינתיים, לעכשיו, בואו ונדבר על הבעיה שאם אנחנו חושבים שיש לנו התאמת יתר, מה אנחנו יכולים לעשות כדי לטפל בה? בדוגמאות הקודמות, היו לנו אחד או שניים ממדי נתונים אז יכולנו פשוט לשרטט את ההשערה ולראות מה קורה ולבחור את דרגת הפולינום המתאימה. לדוגמא, מוקדם יותר בדוגמת מחירי הדירות, יכולנו פשוט לשרטט את ההשערה, אתם זוכרים, ואולי לראות שהיתה לנו התאמה עם סוג של פונקציה מפותלת מאוד שעולה ויורדת בצורה מוזרה
על מנת לנסות ולחזות את מחירי הדירות. ואז יכולנו להשתמש בגרפים כאלו כדי לבחור פולינום מדרגה מתאימה. אז שרטוט של פונקצית ההשערה יכול להיות דרך אחת לנסות להחליט באיזו דרגה של פולינום להשתמש. אבל זה לא תמיד עובד. ולמעשה, לעתים קרובות יותר אנו עשויים להיתקל בבעיות שיש להן פשוט המון תכונות. וזה לא רק עניין של בחירת דרגת הפולינום. ולמעשה, כאשר יש לנו כל כך הרבה תכונות, זה גם הופך להיות הרבה יותר קשה לשרטט את הנתונים וזה הופך להיות הרבה יותר קשה לדמיין את זה, כדי להחליט אילו תכונות לשמור או לא. אז באופן קונקרטי, אם אנחנו מנסים לחזות את מחירי הדירות,
לפעמים פשוט יש לנו הרבה תכונות שונות. וכל התכונות האלה נראות, אתם מבינים,
אולי הם נראות קצת שימושיות. אבל אם יש לנו הרבה תכונות, ומעט מאוד נתוני אימון, אז התאמת יתר יכולה להיות בעיה. כדי לענות על התאמת יתר, יש שתי אפשרויות עיקריות עבור דברים שאנחנו יכולים לעשות. האפשרות הראשונה היא לנסות לצמצם את מספר התכונות. באופן קונקרטי, דבר אחד שאנחנו יכולים לעשות הוא לעבור על רשימת התכונות אחת אחת, ולהשתמש בזה כדי לנסות להחליט אילו תכונות חשובות יותר, או אילו תכונות אנחנו צריכים לשמור, ואילו תכונות אנחנו צריכים לזרוק. בהמשך הקורס, כשנדבר על אלגוריתמים לבחירת מודל. שהם אלגוריתמים היודעים אוטומטית להחליט אילו תכונות לשמור ואילו תכונות לזרוק החוצה. רעיון זה של הפחתת מספר תכונות יכול לעבוד טוב, ויכול להפחית התאמת יתר. וכאשר נדבר על בחירת המודל, נדון בזה הרבה יותר לעומק. אבל החיסרון הוא, שעל ידי זריקת חלק מהתכונות אנחנו גם זורקים חלק מהמידע שיש לנו על הבעיה. לדוגמה, אולי כל התכונות הללו הם למעשה רלוונטיות לניבוי מחיר של בית, אז אולי אנחנו לא באמת רוצים לזרוק חלק מהמידע שלנו או לזרוק חלק מהתכונות שלנו החוצה. האפשרות השנייה, שעליה נדבר בסרטונים הבאים, היא הסדרה - רגולריזציה. כאן, אנחנו הולכים לשמור את כל התכונות, אבל אנחנו הולכים להפחית את הגודל או את הערכים של הפרמטרים תטא-J. ושיטה זו פועלת היטב, כפי שנראה, כאשר יש לנו הרבה תכונות, שכל אחת מהם תורמת קצת לניבוי הערך של Y, כמו שראינו בדוגמה חיזוי מחיר הדירות. שבה אנחנו עשויים לקבל הרבה תכונות, שכל אחת מהם, אתה יודע, קצת שימושית,
אז אולי אנחנו לא רוצים לזרוק אותם. אז זה מסביר את הרעיון של הסדרה ברמה גבוהה מאוד. ואני מבין שכל הפרטים האלה כנראה עדיין לא נשמעים לכם הגיוניים. אבל בסרטון הבא נתחיל לנסח בדיוק כיצד ליישם הסדרה, בדיוק מה פירושה של הסדרה. ואז נתחיל להבין איך להשתמש בה כדי לגרום ללמידה האלגוריתמית לעבוד יותר טוב ולהימנע מהתאמת יתר.