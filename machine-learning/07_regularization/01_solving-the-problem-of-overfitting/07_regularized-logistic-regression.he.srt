1
00:00:00,160 --> 00:00:01,480
עבור רגרסיה לוגיסטית, דיברנו בעבר

2
00:00:02,110 --> 00:00:04,730
על שני סוגים של אלגוריתמי אופטימיזציה.

3
00:00:05,190 --> 00:00:06,190
דיברנו על השימוש

4
00:00:06,560 --> 00:00:09,210
בירידה במדרון כדי לייעל את פונקצית העלות J של תטא.

5
00:00:09,690 --> 00:00:10,770
וגם דיברנו על

6
00:00:11,120 --> 00:00:12,730
שיטות אופטימיזציה מתקדמות.

7
00:00:13,520 --> 00:00:14,670
אלה המחייבות אותך

8
00:00:14,790 --> 00:00:16,300
לספק דרך לחשב

9
00:00:16,940 --> 00:00:18,160
את פונקציית העלות J של

10
00:00:18,420 --> 00:00:20,920
תטא וכן לספק דרך לחשב את הנגזרות שלה.

11
00:00:22,450 --> 00:00:23,920
בסרטון הזה נציג כיצד

12
00:00:24,190 --> 00:00:25,420
ניתן להתאים את שתי

13
00:00:25,500 --> 00:00:27,570
הטכניקות הללו, הן את הירידה במדרון

14
00:00:27,720 --> 00:00:29,350
והן את טכניקות האופטימיזציה המתקדמות יותר,

15
00:00:30,280 --> 00:00:31,770
כדי שיוכלו

16
00:00:31,950 --> 00:00:33,550
לעבוד עבור רגרסיה לוגיסטית מוסדרת (לאחר הַסְדָרָה).

17
00:00:35,430 --> 00:00:36,670
אז הנה הרעיון.

18
00:00:37,260 --> 00:00:38,770
ראינו קודם לכן כי רגרסיה

19
00:00:39,190 --> 00:00:40,490
לוגיסטית עלולה להיות מועדת

20
00:00:40,850 --> 00:00:42,540
להתאמת יתר אם אתה בונה התאמה

21
00:00:42,810 --> 00:00:44,090
טובה מדי של

22
00:00:44,290 --> 00:00:45,890
סדרת האימון על ידי פונקציה פולינומית מסדר גבוה מדי.

23
00:00:46,470 --> 00:00:48,250
כאשר g היא

24
00:00:48,480 --> 00:00:49,970
פונקציית הסיגמואיד ואז

25
00:00:50,030 --> 00:00:51,330
אתה מקבל בסופו של דבר

26
00:00:51,530 --> 00:00:53,020
השערה, אתם מבינים,

27
00:00:53,150 --> 00:00:54,120
שהיא בעצם סוג

28
00:00:54,360 --> 00:00:55,930
של פונקציה מורכבת מדי

29
00:00:56,620 --> 00:00:58,600
ומעוותת מאוד

30
00:00:58,820 --> 00:00:59,680
שאיננה באמת כזו פונקצית השערה

31
00:00:59,790 --> 00:01:01,000
מוצלחת עבור קבוצת האימון

32
00:01:01,350 --> 00:01:02,990
הנתונה, ובמיוחד זה קורה אם יש לך

33
00:01:03,120 --> 00:01:04,890
רגרסיה לוגיסטית עם הרבה תכונות.

34
00:01:05,150 --> 00:01:06,630
לא בהכרח תכונות מתאימות לפולינום, אלא

35
00:01:06,790 --> 00:01:07,510
פשוט כשיש הרבה

36
00:01:07,670 --> 00:01:09,720
תכונות אתה יכול להגיע להשערה עם התאמת יתר.

37
00:01:11,620 --> 00:01:14,010
זו היתה פונקציית העלות שלנו עבור רגרסיה לוגיסטית.

38
00:01:14,810 --> 00:01:16,210
ואם אנחנו רוצים לשנות

39
00:01:16,740 --> 00:01:18,820
את זה ולהשתמש בהסדרה, כל מה

40
00:01:18,950 --> 00:01:20,630
שאנחנו צריכים לעשות הוא להוסיף

41
00:01:20,820 --> 00:01:22,290
לו את המחובר הבא

42
00:01:22,650 --> 00:01:24,860
פלוס למדא חלקי 2m, כפול הסכום

43
00:01:25,110 --> 00:01:26,580
מ-j שווה 1,

44
00:01:26,730 --> 00:01:29,670
כרגיל מ-j שווה 1.

45
00:01:29,800 --> 00:01:31,000
ולא הסכום מ-j

46
00:01:31,550 --> 00:01:33,670
שווה 0, של תטא-j בריבוע.

47
00:01:34,330 --> 00:01:35,470
ולזה יש

48
00:01:35,750 --> 00:01:36,960
האפקט של צמצום

49
00:01:37,650 --> 00:01:39,140
הפרמטרים תטא-1 תטא-2

50
00:01:39,570 --> 00:01:42,600
וכן הלאה עד תטא-N כדי שלא יהיו גדולים מדי.

51
00:01:43,610 --> 00:01:44,720
ואם תעשה את זה,

52
00:01:45,720 --> 00:01:46,450
יהיה לזה האפקט

53
00:01:46,750 --> 00:01:48,870
שלמרות שאתה מתאים

54
00:01:49,250 --> 00:01:51,500
פולינום מסדר מאוד גבוה עם הרבה פרמטרים,

55
00:01:52,210 --> 00:01:53,240
כל עוד אתה מחיל עליו הסדרה

56
00:01:53,910 --> 00:01:55,090
ומגביל את הפרמטרים מלגדול

57
00:01:55,850 --> 00:01:57,580
אתה צפוי לקבל גבול החלטה

58
00:01:58,830 --> 00:02:00,040
שאולי נראה יותר ככה.

59
00:02:00,320 --> 00:02:01,460
שנראה יותר הגיוני שהוא מפריד

60
00:02:02,500 --> 00:02:03,740
בין הדוגמאות החיוביות לבין הדוגמאות השליליות.

61
00:02:05,300 --> 00:02:06,970
אז בעת שימוש בהסדרה,

62
00:02:08,140 --> 00:02:09,080
גם כאשר יש לך הרבה

63
00:02:09,220 --> 00:02:11,110
תכונות, ההסדרה יכולה

64
00:02:11,620 --> 00:02:13,500
לעזור לטפל בבעיית התאמת היתר.

65
00:02:14,740 --> 00:02:15,790
איך אנחנו באמת מיישמים את זה?

66
00:02:16,720 --> 00:02:18,280
ובכן, עבור האלגוריתם המקורי לירידה

67
00:02:18,710 --> 00:02:20,380
במדרון, הנה איך שעשינו את שלבי העדכון.

68
00:02:20,670 --> 00:02:22,300
אנחנו מבצעים שוב ושוב את העדכון

69
00:02:22,750 --> 00:02:24,610
הבא לתטא-j. השקף

70
00:02:24,740 --> 00:02:26,940
הזה נראה מאד דומה לקודם עבור רגרסיה ליניארית.

71
00:02:27,510 --> 00:02:28,460
אבל מה שאני הולך לעשות כאן הוא

72
00:02:29,210 --> 00:02:31,390
לכתוב את העדכון עבור תטא-0 בנפרד.

73
00:02:31,670 --> 00:02:32,930
אז השורה הראשונה היא

74
00:02:33,060 --> 00:02:34,110
העדכון עבור תטא-0

75
00:02:34,230 --> 00:02:35,470
והשורה השנייה היא

76
00:02:35,590 --> 00:02:36,730
העדכון עבור תטא-1

77
00:02:36,880 --> 00:02:38,470
עד תטא-n.

78
00:02:38,900 --> 00:02:40,740
כי אני מתכוון לטפל בתטא-0 בנפרד.

79
00:02:41,700 --> 00:02:43,140
וכדי

80
00:02:43,700 --> 00:02:45,370
לשנות את האלגוריתם הזה, כדי להשתמש

81
00:02:46,770 --> 00:02:48,480
בפונקצית מחיר מוסדרת,

82
00:02:49,100 --> 00:02:50,510
כל מה שאני צריך לעשות הוא

83
00:02:50,950 --> 00:02:51,810
די דומה למה

84
00:02:51,930 --> 00:02:53,700
שעשינו עבור רגרסיה ליניארית,

85
00:02:53,870 --> 00:02:55,620
בעצם פשוט לשנות את

86
00:02:55,890 --> 00:02:57,480
כלל העדכון השני כדלקמן.

87
00:02:58,510 --> 00:02:59,800
ושוב, זה, אתם מבינים,

88
00:03:00,380 --> 00:03:02,080
נראה זהה קוסמטית למה

89
00:03:02,230 --> 00:03:03,720
שהיה לנו עבור רגרסיה ליניארית.

90
00:03:04,580 --> 00:03:05,580
אבל כמובן זה לא

91
00:03:05,660 --> 00:03:06,590
אותו אלגוריתם שהיה לנו,

92
00:03:06,890 --> 00:03:08,370
כי עכשיו ההשערה

93
00:03:08,780 --> 00:03:10,420
מוגדרת באמצעות הנוסחה הזו.

94
00:03:10,860 --> 00:03:12,550
לכן זה לא אותו אלגוריתם

95
00:03:13,130 --> 00:03:14,390
כמו רגרסיה ליניארית מוסדרת.

96
00:03:14,830 --> 00:03:16,340
כי פונקצית ההשערה שונה.

97
00:03:16,940 --> 00:03:18,360
למרות שהעדכון הזה שכתבתי כאן

98
00:03:18,630 --> 00:03:20,160
באמת נראה זהה

99
00:03:20,350 --> 00:03:22,130
קוסמטית למה שהיה לנו קודם.

100
00:03:22,480 --> 00:03:25,310
אנחנו עובדים כאן על ירידה במדרון
עבור רגרסיה ליניארית מוסדרת.

101
00:03:26,690 --> 00:03:27,720
וכמובן, רק כדי לסכם את

102
00:03:27,830 --> 00:03:29,360
הדיון הזה, המונח הזה

103
00:03:29,560 --> 00:03:30,860
כאן בסוגריים

104
00:03:31,130 --> 00:03:32,330
המרובעים, המונח הזה

105
00:03:32,670 --> 00:03:35,120
כאן, המונח הזה הוא,

106
00:03:35,410 --> 00:03:36,750
כמובן, הנגזרת החלקית

107
00:03:37,210 --> 00:03:38,590
החדשה ביחס

108
00:03:38,660 --> 00:03:41,420
לתטא-j של פונקציית העלות החדשה j של תטא.

109
00:03:42,300 --> 00:03:43,480
כש-j של תטא כאן היא

110
00:03:43,700 --> 00:03:44,980
פונקציית העלות שהגדרנו

111
00:03:45,180 --> 00:03:48,100
בשקופית הקודמת שעושה שימוש בהסדרה.

112
00:03:49,770 --> 00:03:52,060
אז, זוהי הירידה במדרון עבור רגרסיה ליניארית מוסדרת.

113
00:03:55,200 --> 00:03:56,430
בואו נדבר על איך

114
00:03:56,580 --> 00:03:58,290
לגרום לרגרסיה לינארית מוסדרת

115
00:03:58,950 --> 00:04:00,010
לעבוד עם שיטות

116
00:04:00,360 --> 00:04:02,070
האופטימיזציה המתקדמות יותר.

117
00:04:03,180 --> 00:04:05,590
ופשוט להזכיר לכם,

118
00:04:05,840 --> 00:04:06,800
לגבי השיטות האלה מה שהיה עלינו

119
00:04:07,080 --> 00:04:08,390
לעשות היה להגדיר את

120
00:04:08,450 --> 00:04:09,460
הפונקציה הזו שנקראת פונקציית

121
00:04:09,640 --> 00:04:11,160
עלות, שמקבלת

122
00:04:11,280 --> 00:04:13,660
כפרמטר קלט את הוקטור תטא

123
00:04:13,790 --> 00:04:16,180
ושוב זכרו שבמשוואות

124
00:04:16,770 --> 00:04:19,030
שכתבנו כאן השתמשנו בוקטורים עם אינדקס החל מ-0.

125
00:04:19,510 --> 00:04:20,690
אז היה לנו תטא-0

126
00:04:21,180 --> 00:04:22,810
עד תטא-n. אבל

127
00:04:23,020 --> 00:04:25,920
מאחר ובאוקטבה הוקטורים מתחילים באינדקס 1

128
00:04:26,820 --> 00:04:28,240
תטא-0 כתוב

129
00:04:28,560 --> 00:04:29,990
באוקטבה כתטא-1.

130
00:04:30,120 --> 00:04:31,630
תטא-1 כתוב

131
00:04:31,860 --> 00:04:32,930
באוקטבה כתטא-2,

132
00:04:33,280 --> 00:04:35,070
וכן הלאה עד

133
00:04:36,270 --> 00:04:36,650
תטה-(1+N).

134
00:04:36,740 --> 00:04:38,450
ומה שהיה עלינו

135
00:04:38,600 --> 00:04:40,240
לעשות היה לספק פונקציה.

136
00:04:41,170 --> 00:04:42,370
לספק פונקציה שנקראת

137
00:04:42,780 --> 00:04:44,140
פונקציית עלות, שאותה נוכל

138
00:04:44,360 --> 00:04:46,920
לשלוח כפרמטר קלט למה שראינו קודם.

139
00:04:47,300 --> 00:04:48,490
אנו נשתמש בפונקציית

140
00:04:49,060 --> 00:04:50,310
fminunc

141
00:04:50,540 --> 00:04:52,160
כפונקצית עלות ,

142
00:04:54,830 --> 00:04:55,430
וכן הלאה, נכון.

143
00:04:55,600 --> 00:04:56,870
אבל fminunc

144
00:04:57,030 --> 00:04:58,060
הוא fmin

145
00:04:58,280 --> 00:04:59,310
unconstrained - בלתי מוסדר

146
00:04:59,650 --> 00:05:01,230
והמונח הזה יעבוד עם fminunc

147
00:05:01,310 --> 00:05:02,300
שביחד יקחו את

148
00:05:02,540 --> 00:05:04,340
פונקציית העלות וימזערו אותה עבורנו.

149
00:05:05,950 --> 00:05:07,050
אז שני הדברים העיקריים

150
00:05:07,170 --> 00:05:08,600
שפונקצית העלות צריכה

151
00:05:08,700 --> 00:05:10,620
להחזיר היו קודם כל jVal.

152
00:05:11,280 --> 00:05:12,400
ובשביל זה, אנחנו צריכים

153
00:05:12,720 --> 00:05:13,950
לכתוב קוד כדי

154
00:05:14,020 --> 00:05:15,710
לחשב את פונקצית העלות j של תטא.

155
00:05:17,130 --> 00:05:19,030
כעת, כאשר אנו משתמשים ברגרסיה לוגיסטית

156
00:05:19,450 --> 00:05:20,920
מוסדרת, כמובן

157
00:05:20,990 --> 00:05:21,960
שפונקציית העלות j של תטא

158
00:05:22,280 --> 00:05:23,450
משתנה, ובאופן ספציפי,

159
00:05:24,480 --> 00:05:25,760
כעת, פונקצית העלות צריכה

160
00:05:25,870 --> 00:05:29,580
לכלול גם את מונח ההסדרה הנוסף הזה בסוף.

161
00:05:29,850 --> 00:05:30,930
לכן, כאשר אתה מחשב את j של

162
00:05:31,030 --> 00:05:33,410
תטא הקפד לכלול את המונח הזה בסוף.

163
00:05:34,590 --> 00:05:35,520
ואז, הדבר השני

164
00:05:36,050 --> 00:05:37,240
שפונקצית העלות צריכה

165
00:05:37,690 --> 00:05:39,010
היא דרך לעשות נגזרות - גרדיינטים.

166
00:05:39,530 --> 00:05:41,170
אז הגרדיינט הראשון צריך

167
00:05:41,400 --> 00:05:42,570
להיות

168
00:05:42,660 --> 00:05:44,080
הנגזרת החלקית של j

169
00:05:44,240 --> 00:05:45,520
של תטא ביחס לתטא-0,

170
00:05:45,690 --> 00:05:47,170
הגרדיינט השני הוא

171
00:05:47,580 --> 00:05:49,520
הנגזרת ביחס לתטא-1, וכן הלאה.

172
00:05:49,780 --> 00:05:50,900
וזכרו, האינדקסים מוסטים באחת.

173
00:05:51,220 --> 00:05:52,850
זוכרים, כי האינדקס בהם

174
00:05:53,110 --> 00:05:54,450
משתמשת אוקטבה הם מ-1.

175
00:05:55,940 --> 00:05:56,780
וכשנסתכל על המונחים האלה,

176
00:05:57,850 --> 00:05:58,680
המונח הזה כאן.

177
00:05:59,410 --> 00:06:00,640
אנחנו למעשה חישבנו אותו

178
00:06:00,720 --> 00:06:02,840
בשקופית הקודמת והוא למעשה שווה לזה.

179
00:06:03,230 --> 00:06:03,640
זה לא משתנה.

180
00:06:04,120 --> 00:06:07,250
כי הנגזרת עבור תטא-0 לא משתנה

181
00:06:07,650 --> 00:06:09,540
בהשוואה לגרסה ללא הסדרה.

182
00:06:10,960 --> 00:06:13,210
המונחים האחרים כן משתנים.

183
00:06:13,840 --> 00:06:16,340
בפרט, לגבי הנגזרת של תטא-1.

184
00:06:17,010 --> 00:06:18,830
גם את זה חישבנו בשקופית הקודמת.

185
00:06:19,110 --> 00:06:20,670
והיא שווה, אתה זוכרים,

186
00:06:20,890 --> 00:06:22,560
למונח המקורי פלוס

187
00:06:23,450 --> 00:06:24,870
למבדא חלקי m כפול תטא-1.

188
00:06:25,310 --> 00:06:27,140
רק כדי לוודא שאנחנו מעבירים את זה נכון

189
00:06:27,800 --> 00:06:29,370
כדאי לנו להוסיף כאן סוגריים.

190
00:06:29,830 --> 00:06:30,980
נכון, כך שהסיגמא לא יכלול את המונח שהוספנו.

191
00:06:31,570 --> 00:06:33,160
ובדומה לכך, אתם יודעים,

192
00:06:33,380 --> 00:06:34,800
המונח השני כאן נראה

193
00:06:35,130 --> 00:06:36,180
כך, עם המונח הנוסף

194
00:06:37,070 --> 00:06:37,950
הזה שגם הוא היה לנו

195
00:06:38,030 --> 00:06:39,770
בשקופית הקודמת, המתאים

196
00:06:39,950 --> 00:06:41,450
לשיפוע שלנו בפונקציה עם ההסדרה.

197
00:06:42,230 --> 00:06:43,650
אז אם תיישמו את

198
00:06:43,820 --> 00:06:45,140
פונקצית העלות הזו ותעבירו

199
00:06:45,720 --> 00:06:47,370
אותה ל-fminunc או

200
00:06:48,190 --> 00:06:49,160
לאחת מאותן טכניקות אופטימיזציה

201
00:06:50,050 --> 00:06:51,940
מתקדמות, זה ימזער

202
00:06:52,540 --> 00:06:55,990
את פונקציה העלות המוסדרת החדשה j של תטא.

203
00:06:56,990 --> 00:06:58,220
והפרמטרים שהפונקציה תחזיר

204
00:06:59,530 --> 00:07:00,740
יהיו אלה המתאימים

205
00:07:01,450 --> 00:07:02,940
לרגרסיה לוגיסטית עם הסדרה.

206
00:07:04,410 --> 00:07:05,540
זהו, עכשיו אתם יודעים

207
00:07:05,780 --> 00:07:08,210
איך ליישם רגרסיה לוגיסטית מסודרת.

208
00:07:09,780 --> 00:07:10,920
כאשר אני מסתובב בעמק הסיליקון,

209
00:07:11,380 --> 00:07:12,900
אני גר כאן בעמק הסיליקון, יש

210
00:07:13,100 --> 00:07:14,900
המון מהנדסים שבאמת עושים

211
00:07:15,420 --> 00:07:16,490
טונות של כסף עבור

212
00:07:16,610 --> 00:07:18,090
החברות שלהם
באמצעות אלגוריתמים של למידה ממוחשבת.

213
00:07:19,180 --> 00:07:20,390
ואני יודע שאנחנו

214
00:07:20,600 --> 00:07:22,860
רק, אתo יודעים, ממש התחלנו ללמוד את הדברים האלה.

215
00:07:23,620 --> 00:07:25,410
אבל אם אתם מבינים ויודעים ליישם רגרסיה

216
00:07:26,510 --> 00:07:28,360
לינארית, אלגוריתמי אופטימיזציה

217
00:07:29,210 --> 00:07:30,710
מתקדמים והסדרה, אז

218
00:07:30,950 --> 00:07:32,520
עכשיו, ברצינות, אתם בטח יודעים

219
00:07:32,950 --> 00:07:34,270
הרבה יותר על למידת מכונה

220
00:07:35,010 --> 00:07:36,290
מאשר הרבה מאד, בהחלט,

221
00:07:36,750 --> 00:07:38,050
אתם בטח יודעים די

222
00:07:38,180 --> 00:07:39,580
הרבה על למידת מכונה עכשיו

223
00:07:40,240 --> 00:07:41,670
יותר מבאמת הרבה

224
00:07:41,820 --> 00:07:44,760
מהנדסים בעמק הסיליקון
שעשו מזה קריירה מוצלחת מאוד.

225
00:07:45,300 --> 00:07:46,420
אתם יודעים, שעושים טונות של כסף
עבור החברות שלהם.

226
00:07:47,050 --> 00:07:49,250
או בונים מוצרים
באמצעות אלגוריתמים של למידה ממוחשבת.

227
00:07:50,370 --> 00:07:50,960
אז מזל טוב.

228
00:07:52,080 --> 00:07:53,120
עברתם כברת דרך ארוכה.

229
00:07:53,490 --> 00:07:54,550
ואתם יכולים למעשה, אתם

230
00:07:54,780 --> 00:07:55,990
באמת יודעים מספיק

231
00:07:56,310 --> 00:07:58,210
כדי ליישם את הדברים האלה
ולגרום להם לעבוד עבור בעיות רבות.

232
00:07:59,260 --> 00:08:00,580
אז ברכות על כך.

233
00:08:00,780 --> 00:08:01,880
אבל כמובן, עדיין

234
00:08:02,350 --> 00:08:03,280
יש עוד הרבה שאנחנו

235
00:08:03,400 --> 00:08:05,180
רוצים ללמד אתכם,

236
00:08:05,380 --> 00:08:06,540
ובקבוצה הבאה של קטעי וידאו אחרי

237
00:08:06,560 --> 00:08:07,850
זה, נתחיל לדבר

238
00:08:08,030 --> 00:08:10,890
על קבוצה חזקה מאוד של מסווגים לא ליניאריים.

239
00:08:11,680 --> 00:08:13,350
אז בעוד ברגרסיה ליניארית, רגרסיה

240
00:08:13,690 --> 00:08:14,940
לוגיסטית, אתם יודעים, אתם יכולים

241
00:08:15,080 --> 00:08:17,310
ליצור מונחים פולינומיים, אבל

242
00:08:17,460 --> 00:08:18,350
מתברר שיש מסווגים הרבה

243
00:08:18,510 --> 00:08:21,150
יותר חזקים שאינם לינאריים

244
00:08:21,460 --> 00:08:23,650
שיותר חזקים מרגרסיה פולינומית.

245
00:08:24,640 --> 00:08:25,780
ובסט הבא

246
00:08:25,810 --> 00:08:28,280
של קטעי וידאו אחרי זה,
אני אתחיל לספר לכם עליהם.

247
00:08:28,510 --> 00:08:29,560
כך שיהיו לכם

248
00:08:29,760 --> 00:08:30,440
אלגוריתמי למידה
אפילו יותר חזקים מאשר יש לכם

249
00:08:31,380 --> 00:08:32,870
עכשיו כדי ליישם אותם על בעיות שונות.