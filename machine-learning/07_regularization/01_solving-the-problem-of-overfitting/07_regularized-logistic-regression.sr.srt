1
00:00:00,160 --> 00:00:01,480
Što se tiče logističke
 regresije, prethodno smo

2
00:00:02,110 --> 00:00:04,730
govorili o dva tipa 
algoritama za optimizaciju.

3
00:00:05,190 --> 00:00:06,190
Govorili smo kako da koristimo

4
00:00:06,560 --> 00:00:09,210
opadajući gradijent da bismo 
optimizovali funkciju koštanja J od teta.

5
00:00:09,690 --> 00:00:10,770
Takođe smo govorili o

6
00:00:11,120 --> 00:00:12,730
naprednim metodama optimizacije.

7
00:00:13,520 --> 00:00:14,670
One koje zahtevaju da im

8
00:00:14,790 --> 00:00:16,300
obezbedite način da računate

9
00:00:16,940 --> 00:00:18,160
vašu funkciju cena J od teta

10
00:00:18,420 --> 00:00:20,920
i da obezbedite način da računate izvode.

11
00:00:22,450 --> 00:00:23,920
U ovome videu, videćemo kako

12
00:00:24,190 --> 00:00:25,420
možete da prilagodite obe

13
00:00:25,500 --> 00:00:27,570
tehnike, oba opadajuća gradijenta i

14
00:00:27,720 --> 00:00:29,350
i naprednije tehnike optimizacije

15
00:00:30,280 --> 00:00:31,770
da biste postigli da

16
00:00:31,950 --> 00:00:33,550
rade na regularizovanim 
logističkim regresijama.

17
00:00:35,430 --> 00:00:36,670
Dakle, ovo je ideja.

18
00:00:37,260 --> 00:00:38,770
Ranije smo videli da logistička

19
00:00:39,190 --> 00:00:40,490
regresija takođe može da bude sklona

20
00:00:40,850 --> 00:00:42,540
prekomernosti ako joj

21
00:00:42,810 --> 00:00:44,090
dodelite osobine sa polinomom

22
00:00:44,290 --> 00:00:45,890
visokog stepena, kao što je ova.

23
00:00:46,470 --> 00:00:48,250
Gde je g

24
00:00:48,480 --> 00:00:49,970
sigmoid funkcija i

25
00:00:50,030 --> 00:00:51,330
obavezno dobijete

26
00:00:51,530 --> 00:00:53,020
hipotezu, znate,

27
00:00:53,150 --> 00:00:54,120
čija odluka je vezana za

28
00:00:54,360 --> 00:00:55,930
neku vrstu preterano složene

29
00:00:56,620 --> 00:00:58,600
i krajnje iskrivljenu funkciju koja

30
00:00:58,820 --> 00:00:59,680
stvarno nije sjajna

31
00:00:59,790 --> 00:01:01,000
hipoteza za ovaj trening

32
00:01:01,350 --> 00:01:02,990
skup, i uopšte, ako imate

33
00:01:03,120 --> 00:01:04,890
logističku regresiju sa mnogo osobina.

34
00:01:05,150 --> 00:01:06,630
Ne obavezno one polinomijalne, već

35
00:01:06,790 --> 00:01:07,510
samo sa mnogo

36
00:01:07,670 --> 00:01:09,720
osobina sa kojima možete
da dobijete prekomernost.

37
00:01:11,620 --> 00:01:14,010
Ovo je bila naša funkcija 
cena za logističku regresiju.

38
00:01:14,810 --> 00:01:16,210
I ako želimo da je izmenimo

39
00:01:16,740 --> 00:01:18,820
da bismo koristili regularizaciju, sve što

40
00:01:18,950 --> 00:01:20,630
treba da uradimo je da je

41
00:01:20,820 --> 00:01:22,290
dodamo u sledeći pojam

42
00:01:22,650 --> 00:01:24,860
plus lambda kroz 2m, suma

43
00:01:25,110 --> 00:01:26,580
od j = 1, i

44
00:01:26,730 --> 00:01:29,670
kao i obično suma od j = 1.

45
00:01:29,800 --> 00:01:31,000
Bolje nego suma od j = 0

46
00:01:31,550 --> 00:01:33,670
od teta j na kvadrat.

47
00:01:34,330 --> 00:01:35,470
A ovo ima

48
00:01:35,750 --> 00:01:36,960
efekat, dakle, kažnjavanja

49
00:01:37,650 --> 00:01:39,140
parametara teta1,

50
00:01:39,570 --> 00:01:42,600
teta2, i tako dalje do tetan 
zato što su preveliki.

51
00:01:43,610 --> 00:01:44,720
A ako uradite ovo,

52
00:01:45,720 --> 00:01:46,450
tad će to da ima efekat

53
00:01:46,750 --> 00:01:48,870
da vam ipak odgovara

54
00:01:49,250 --> 00:01:51,500
polinom visokog stepena 
sa mnogo parametara.

55
00:01:52,210 --> 00:01:53,240
Dokle god primenjujete regularizaciju

56
00:01:53,910 --> 00:01:55,090
i držite parametre malim

57
00:01:55,850 --> 00:01:57,580
veća je verovatnoće da 
dobijete granicu odluke.

58
00:01:58,830 --> 00:02:00,040
Znate, to izgleda više ovako.

59
00:02:00,320 --> 00:02:01,460
Izgleda više razumno za razdvajanje

60
00:02:02,500 --> 00:02:03,740
pozitivnih i negativnih primera.

61
00:02:05,300 --> 00:02:06,970
Dakle, kada koristite regularizaciju

62
00:02:08,140 --> 00:02:09,080
čak kada imate mnogo

63
00:02:09,220 --> 00:02:11,110
osobina, regularizacija može da

64
00:02:11,620 --> 00:02:13,500
pomogne oko 
problema prekomernosti.

65
00:02:14,740 --> 00:02:15,790
Kako u stvari primenjujemo ovo?

66
00:02:16,720 --> 00:02:18,280
Za originalni algoritam opadajućeg

67
00:02:18,710 --> 00:02:20,380
gradijenta, ovo je 
dodatak koji smo imali.

68
00:02:20,670 --> 00:02:22,300
Ponavljaćemo izvršenje sledećeg

69
00:02:22,750 --> 00:02:24,610
dodatka na teta j. Ovaj

70
00:02:24,740 --> 00:02:26,940
slajd izgleda kao onaj 
za linearnu regresiju.

71
00:02:27,510 --> 00:02:28,460
A ono šta ću da uradim je

72
00:02:29,210 --> 00:02:31,390
da ću da napišem 
dodatak za teta0 odvojeno.

73
00:02:31,670 --> 00:02:32,930
Dakle, prva linija je

74
00:02:33,060 --> 00:02:34,110
za dodatak za teta0 a

75
00:02:34,230 --> 00:02:35,470
druga je sada

76
00:02:35,590 --> 00:02:36,730
moj dodatak za teta1

77
00:02:36,880 --> 00:02:38,470
sve do tetan.

78
00:02:38,900 --> 00:02:40,740
Zato što ću da 
tretiram teta0 odvojeno.

79
00:02:41,700 --> 00:02:43,140
Da bih

80
00:02:43,700 --> 00:02:45,370
modifikovao ovaj algoritam za

81
00:02:46,770 --> 00:02:48,480
regularizaciju funkcije cena,

82
00:02:49,100 --> 00:02:50,510
sve što treba fa uradim jeste

83
00:02:50,950 --> 00:02:51,810
dosta slično onome šta smo

84
00:02:51,930 --> 00:02:53,700
uradili kod linearne regresije,

85
00:02:53,870 --> 00:02:55,620
samo treba da se modifikuje ovo

86
00:02:55,890 --> 00:02:57,480
drugo pravilo dodatka
 kao što sledi.

87
00:02:58,510 --> 00:02:59,800
Još jednom, ovo je

88
00:03:00,380 --> 00:03:02,080
kozmetički identično onome što

89
00:03:02,230 --> 00:03:03,720
smo imali kod linearne regresije.

90
00:03:04,580 --> 00:03:05,580
Ali naravno nije

91
00:03:05,660 --> 00:03:06,590
isti algoritam kao što smo imali,

92
00:03:06,890 --> 00:03:08,370
jer je sad hipoteza

93
00:03:08,780 --> 00:03:10,420
definisana pomoću ovoga.

94
00:03:10,860 --> 00:03:12,550
Dakle, ovo nije isti algoritam

95
00:03:13,130 --> 00:03:14,390
kao regularizovana
 linearna regresija.

96
00:03:14,830 --> 00:03:16,340
Zato što je hipoteza drugačija.

97
00:03:16,940 --> 00:03:18,360
Čak i ovaj dodatak 
koji sam dole napisao.

98
00:03:18,630 --> 00:03:20,160
On kozmetički izgleda isto

99
00:03:20,350 --> 00:03:22,130
kao što smo imali ranije.

100
00:03:22,480 --> 00:03:25,310
Razrađujemo opadajući gradijent za 
regularizovanu linearnu regresiju.

101
00:03:26,690 --> 00:03:27,720
I da zaokružimo

102
00:03:27,830 --> 00:03:29,360
ovu diskusiju, ovaj pojam

103
00:03:29,560 --> 00:03:30,860
ovde u uglastim

104
00:03:31,130 --> 00:03:32,330
zagradama, ovaj pojam

105
00:03:32,670 --> 00:03:35,120
ovde, to je

106
00:03:35,410 --> 00:03:36,750
naravno, novi parcijalni

107
00:03:37,210 --> 00:03:38,590
izvod u odnosu na

108
00:03:38,660 --> 00:03:41,420
tetaj nove funkcije cena J od teta.

109
00:03:42,300 --> 00:03:43,480
Ovo J od teta ovde je

110
00:03:43,700 --> 00:03:44,980
funkcija cena definisana u

111
00:03:45,180 --> 00:03:48,100
prethodnom slajdu koja koristi regularizaciju.

112
00:03:49,770 --> 00:03:52,060
Dakle, to je opadajući gradijent za
 regularizovanu linearnu regresiju.

113
00:03:55,200 --> 00:03:56,430
Sada ćemo da govorimo o tome kako

114
00:03:56,580 --> 00:03:58,290
da koristimo regularizovanu linearnu

115
00:03:58,950 --> 00:04:00,010
regresiju koristeći se više

116
00:04:00,360 --> 00:04:02,070
naprednim metodama optimizacije.

117
00:04:03,180 --> 00:04:05,590
Samo da vas podsetim na

118
00:04:05,840 --> 00:04:06,800
te metode, ono šta smo

119
00:04:07,080 --> 00:04:08,390
trebali da uradimo je da definišemo

120
00:04:08,450 --> 00:04:09,460
funkciju zvanu funkcija

121
00:04:09,640 --> 00:04:11,160
koštanja, ona uzima

122
00:04:11,280 --> 00:04:13,660
vektor ulaznih podataka teta i

123
00:04:13,790 --> 00:04:16,180
još jednom u jednačini

124
00:04:16,770 --> 00:04:19,030
koju smo napisali ovde, 
koristili smo 0 indeks vektor.

125
00:04:19,510 --> 00:04:20,690
Dakle, imamo teta0 do

126
00:04:21,180 --> 00:04:22,810
tetan. Ali

127
00:04:23,020 --> 00:04:25,920
oktava indeksi vektora počinju od 1

128
00:04:26,820 --> 00:04:28,240
teta0 je u oktavi

129
00:04:28,560 --> 00:04:29,990
napisan kao teta1.

130
00:04:30,120 --> 00:04:31,630
Teta1 je napisan u

131
00:04:31,860 --> 00:04:32,930
oktavi kao teta2, i

132
00:04:33,280 --> 00:04:35,070
tako dalje do

133
00:04:36,270 --> 00:04:36,650
tetan plus 1.

134
00:04:36,740 --> 00:04:38,450
Šta smo trebali da uradimo je

135
00:04:38,600 --> 00:04:40,240
da obezbedimo funkciju.

136
00:04:41,170 --> 00:04:42,370
Napravićemo funkciju koja se zove

137
00:04:42,780 --> 00:04:44,140
funkcija cena koju ćemo da

138
00:04:44,360 --> 00:04:46,920
prosledimo u ono 
što smo videli ranije.

139
00:04:47,300 --> 00:04:48,490
Koristićemo fminfunc

140
00:04:49,060 --> 00:04:50,310
i tada

141
00:04:50,540 --> 00:04:52,160
znate, u funkciji cena,

142
00:04:54,830 --> 00:04:55,430
i tako dalje, u redu.

143
00:04:55,600 --> 00:04:56,870
A fminunc

144
00:04:57,030 --> 00:04:58,060
znači fmin unconstrained

145
00:04:58,280 --> 00:04:59,310
(bez ograničenja) i ono

146
00:04:59,650 --> 00:05:01,230
šta radi fminunc

147
00:05:01,310 --> 00:05:02,300
je da uzme

148
00:05:02,540 --> 00:05:04,340
funkciju koštanja i minimizuje je.

149
00:05:05,950 --> 00:05:07,050
Dakle, dve glavne stvari koje

150
00:05:07,170 --> 00:05:08,600
funkcija koštanja treba da

151
00:05:08,700 --> 00:05:10,620
vrati su, kao prvo, jVal,

152
00:05:11,280 --> 00:05:12,400
a za to treba

153
00:05:12,720 --> 00:05:13,950
da napišemo kod

154
00:05:14,020 --> 00:05:15,710
da bi računao funkciju 
koštanja J od teta.

155
00:05:17,130 --> 00:05:19,030
Sada kada koristimo 
regularizovanu logističku

156
00:05:19,450 --> 00:05:20,920
regresiju, naravno,

157
00:05:20,990 --> 00:05:21,960
funkcija J od teta

158
00:05:22,280 --> 00:05:23,450
se menja i pojedinačno

159
00:05:24,480 --> 00:05:25,760
funkcija cena treba da

160
00:05:25,870 --> 00:05:29,580
uključi ovaj dodatni 
pojam regularizacije na kraju.

161
00:05:29,850 --> 00:05:30,930
Kada računate J od teta

162
00:05:31,030 --> 00:05:33,410
uverite se da ste uključili taj deo na kraju.

163
00:05:34,590 --> 00:05:35,520
A onda, druga stvar koju

164
00:05:36,050 --> 00:05:37,240
ova funkcija koštanja treba

165
00:05:37,690 --> 00:05:39,010
da izvede je gradijent.

166
00:05:39,530 --> 00:05:41,170
Dakle, gradijent 1 treba

167
00:05:41,400 --> 00:05:42,570
da bude postavljen na

168
00:05:42,660 --> 00:05:44,080
parcijalni izvod funkcije J

169
00:05:44,240 --> 00:05:45,520
od teta uzimajući u obzir

170
00:05:45,690 --> 00:05:47,170
teta0, gradijent dva treba

171
00:05:47,580 --> 00:05:49,520
da bude postavljen 
na to, i tako dalje.

172
00:05:49,780 --> 00:05:50,900
Još jednom, indeks 
se razlikuje za jedan.

173
00:05:51,220 --> 00:05:52,850
U redu, zbog indeksiranja od jedinice

174
00:05:53,110 --> 00:05:54,450
koje oktava koristi.

175
00:05:55,940 --> 00:05:56,780
Pogledajte ove pojmove.

176
00:05:57,850 --> 00:05:58,680
Ovaj pojam ovde.

177
00:05:59,410 --> 00:06:00,640
Ovo smo u stvari radili

178
00:06:00,720 --> 00:06:02,840
na prethodnom slajdu 
i jednak je ovome.

179
00:06:03,230 --> 00:06:03,640
Nije se promenio.

180
00:06:04,120 --> 00:06:07,250
Jer se izvod za teta0 ne menja.

181
00:06:07,650 --> 00:06:09,540
U poređenju sa 
verzijom bez regularizacije.

182
00:06:10,960 --> 00:06:13,210
A ostali pojmovi se menjaju.

183
00:06:13,840 --> 00:06:16,340
I pojedinačno izvod po teta1,

184
00:06:17,010 --> 00:06:18,830
ovo smo takođe radili na prethodnom slajdu,

185
00:06:19,110 --> 00:06:20,670
je jednak, znate,

186
00:06:20,890 --> 00:06:22,560
originalni pojam minus

187
00:06:23,450 --> 00:06:24,870
lambda kroz m puta teta1.

188
00:06:25,310 --> 00:06:27,140
Da budemo sigurni da 
smo ovo prešli kako treba.

189
00:06:27,800 --> 00:06:29,370
Ovde možemo da dodamo zagrade.

190
00:06:29,830 --> 00:06:30,980
U redu, sumiranje se neće širiti.

191
00:06:31,570 --> 00:06:33,160
Slično tome, znate,

192
00:06:33,380 --> 00:06:34,800
ovaj ovde pojam izgleda

193
00:06:35,130 --> 00:06:36,180
kao ovo, sa ovim dodatnim

194
00:06:37,070 --> 00:06:37,950
pojmom koji smo imali na

195
00:06:38,030 --> 00:06:39,770
prethodnom slajdu, on odgovara

196
00:06:39,950 --> 00:06:41,450
gradijentu objekta regularizacije.

197
00:06:42,230 --> 00:06:43,650
Dakle, ako implementirate ovu

198
00:06:43,820 --> 00:06:45,140
funkciju koštanja i prosledite

199
00:06:45,720 --> 00:06:47,370
ovo u fminunc

200
00:06:48,190 --> 00:06:49,160
ili u neku od tih naprednih tehnika

201
00:06:50,050 --> 00:06:51,940
optimizacije, to će da umanji

202
00:06:52,540 --> 00:06:55,990
novu regularizovanu 
funkciju cena J od teta.

203
00:06:56,990 --> 00:06:58,220
A parametri koje dobijete

204
00:06:59,530 --> 00:07:00,740
su oni koji odgovaraju

205
00:07:01,450 --> 00:07:02,940
logističkoj regresiji sa regularizacijom.

206
00:07:04,410 --> 00:07:05,540
Dakle, sada znate

207
00:07:05,780 --> 00:07:08,210
kako da implementirate 
regularizovanu logističku regresiju.

208
00:07:09,780 --> 00:07:10,920
Kada obilazim Silikonsku dolinu,

209
00:07:11,380 --> 00:07:12,900
ja živim ovde u 
Silikonskoj dolini, postoji

210
00:07:13,100 --> 00:07:14,900
mnogo inženjera koji, 
iskreno, zarađuju

211
00:07:15,420 --> 00:07:16,490
mnogo novca svojim

212
00:07:16,610 --> 00:07:18,090
kompanijama koristeći 
algoritme mašinskog učenja.

213
00:07:19,180 --> 00:07:20,390
A znam da smo,

214
00:07:20,600 --> 00:07:22,860
znate, studirali ovo malo duže.

215
00:07:23,620 --> 00:07:25,410
Ali ako razumete linearnu

216
00:07:26,510 --> 00:07:28,360
regresiju, napredne algoritme

217
00:07:29,210 --> 00:07:30,710
optimizacije i regularizaciju,

218
00:07:30,950 --> 00:07:32,520
za sada, iskreno, verovatno znate

219
00:07:32,950 --> 00:07:34,270
mnogo više o mašinskom učenju

220
00:07:35,010 --> 00:07:36,290
od mnogih, zasigurno,

221
00:07:36,750 --> 00:07:38,050
ali verovatno znate mnogo

222
00:07:38,180 --> 00:07:39,580
više o mašinskom učenju baš sada

223
00:07:40,240 --> 00:07:41,670
od, iskreno, mnogih

224
00:07:41,820 --> 00:07:44,760
inženjera Silikonske doline koji tamo 
negde imaju veoma uspešne karijere.

225
00:07:45,300 --> 00:07:46,420
Znate, koji zarađuju mnogo 
novca svojim kompanijama.

226
00:07:47,050 --> 00:07:49,250
Ili izgrađujući sjajne proizvode koristeći 
algoritme mašinskog učenja.

227
00:07:50,370 --> 00:07:50,960
Dakle, čestitam.

228
00:07:52,080 --> 00:07:53,120
U stvari ste prešli dugačak put.

229
00:07:53,490 --> 00:07:54,550
I u stvari možete,

230
00:07:54,780 --> 00:07:55,990
znate dovoljno da biste

231
00:07:56,310 --> 00:07:58,210
primenili ovu stvar i 
rešili mnoge probleme.

232
00:07:59,260 --> 00:08:00,580
Pa, čestitam za to.

233
00:08:00,780 --> 00:08:01,880
Ali naravno, ima

234
00:08:02,350 --> 00:08:03,280
još uvek mnogo stvari kojima

235
00:08:03,400 --> 00:08:05,180
želim da vas naučim, i u

236
00:08:05,380 --> 00:08:06,540
sledećem nizu videa posle

237
00:08:06,560 --> 00:08:07,850
ovog, počećemo govoriti

238
00:08:08,030 --> 00:08:10,890
o veoma moćnom
 nelinearnom klasifikatoru.

239
00:08:11,680 --> 00:08:13,350
Dakle, gde su linearna regresija, logistička

240
00:08:13,690 --> 00:08:14,940
regresija, znate, možete da

241
00:08:15,080 --> 00:08:17,310
napravite polinomske pojmove, ali

242
00:08:17,460 --> 00:08:18,350
ispada da postoje mnogo

243
00:08:18,510 --> 00:08:21,150
moćniji nelinearni klasifikatori koji

244
00:08:21,460 --> 00:08:23,650
mogu da sortiraju polinomsku regresiju.

245
00:08:24,640 --> 00:08:25,780
A u sledećem nizu

246
00:08:25,810 --> 00:08:28,280
videa posle ovoga, počeću 
da govorim o njima.

247
00:08:28,510 --> 00:08:29,560
Tako da ćete da imate čak

248
00:08:29,760 --> 00:08:30,440
moćnije algoritme 
učenja od onih koje ste

249
00:08:31,380 --> 00:08:32,870
do sada primenjivali 
na različite probleme.