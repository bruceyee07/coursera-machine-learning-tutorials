עד עכשיו דיברנו על SVM-ים ברמה די מופשטת. בסרטון הזה אני רוצה לדבר על מה שבאמת צריך לעשות כדי להריץ או להשתמש ב-SVM. אלגוריתם מכונת התמך הוקטורי מציב בעיית אופטימיזציה מיוחדת. אבל כפי שציינתי בקצרה בוידאו מוקדם יותר, אני לא ממליץ לכם לכתוב בעצמכם תוכנה כדי לחשב את וקטור הפרמטרים θ בעצמכם. בדיוק כמו שבימינו רק מעטים מאוד מאיתנו, או בעצם כמעט אף אחד מאיתנו, לא חושב לכתוב בעצמו קוד להפוך מטריצה או להוציא שורש ריבועי של מספר, או דברים דומים, אלא אנחנו פשוט קוראים לפונקצית ספריה כלשהי כדי לעשות זאת. באותו אופן, תוכנה לפתרון בעיית האופטימיזציה של SVM היא מורכבת מאוד, והיו חוקרים שעסקו במחקר המיטבי בתחום הנומרי במשך שנים רבות. אז כעת יש לנו ספריות תוכנה טובות וחבילות תוכנה טובות כדי לעשות את זה. ולכן אני ממליץ בחום להשתמש באחת מספריות התוכנה לאופטימיזציה ולא לנסות וליישם משהו בעצמכם. ויש הרבה ספריות תוכנה טובות. השתיים בהן אני משתמש הכי הרבה הן liblinear ו-libsvm, אבל יש באמת הרבה ספריות תוכנה טובות לעשות את זה, שאפשר לקרוא להן מהרבה משפות התכנות הגדולות שאפשר להשתמש כדי לכתוב אלגוריתם למידה. אבל, למרות שאתם לא צריכים לכתוב בעצמכם את התוכנה לעשות אופטימיזציה ב-SVM, יש בכל זאת כמה דברים שאתם כן צריכים לעשות. הראשון הוא לבחור את הפרמטר C. דיברנו קצת על מאפייני הטיה ושונות ב-SVM בסרטון הקודם. הדבר השני, אתם צריכים לבחור את הקרנל או את פונקצית הדמיון שבה אתם רוצים להשתמש. אפשרות אחת שבאה בחשבון היא להחליט שלא להשתמש בקרנל בכלל. הרעיון של אי-שימוש בקרנל נקרא גם קרנל ליניארי. אז אם אתם שומעים מישהו אומר "אני משתמש ב-SVM עם ליבה ליניארית", מה שזה אומר הוא שהוא משתמש ב-SVM ללא שימוש בליבה והגירסה הזו של SVM פשוט משתמשת ב-θᵀx, כן, שעושה ניבוי 1 אם θ0 פלוס θ1 כפול x1 פלוס וכו' פלוס θn כפול xn גדול או שווה 0. הקרנל הלינארי, אפשר לחשוב עליו כגירסה של SVM שפשוט נותנת לך מסווג ליניארי סטנדרטי. זה היא בחירה סבירה עבור כמה בעיות, ויש ספריות תוכנה רבות, liblinear היא דוגמה אחת מתוך רבות, דוגמה של ספריית תוכנה שיכולה לאמן SVM ללא שימוש בקרנל, או עם מה שנקרא קרנל או ליבה ליניארית. אבל למה שנרצה לעשות את זה? אז אם יש לנו מספר גדול של תכונות, אם n הוא גדול, ומספר דוגמאות האימון m הוא קטן, יש מספר עצום של תכונות וכל וקטור x, הוא וקטור בעל ממד n או n +1. אז במקרה שיש לנו מספר עצום של תכונות אבל סט אימון קטן, אולי כדאי להתאים להם גבול החלטה ליניארי ולא לנסות להתאים פונקציה לא ליניארית מאוד מסובכת, כי אולי אין מספיק נתונים ואנחנו עלולים להסתכן בהתאמת-יתר, אם ננסה להתאים פונקציה מסובכת מאוד במרחב תכונות עם ממד גבוה מאוד, אבל עם סדרת אימון קטנה. אז זה יהיה מצב סביר אחד שבו אנחנו יכולים להחליט פשוט לא להשתמש בקרנל, או במילים אחרות להשתמש במה שנקרא ליבה ליניארית. אופציה שנייה לקרנל שאפשר לעשות, היא הקרנל הגאוסיאני, זהו שדיברנו עליו כאן. ואם מחליטים ללכת על זה, אז צריך לעשות עוד בחירה והיא לבחור את הפרמטר σ² שנוגע גם הוא ביחסים שבין ההטיה והשונות, כאשר אם σ² הוא גדול, אז יש סיכוי טוב להטיה גבוהה יותר ושונות נמוכה יותר, ואם σ² הוא קטן, אז נקבל שונות גבוהה יותר והטייה נמוכה. אז מתי בוחרים ליבה גאוסיאנית? ובכן, אם התכונות המקוריות שלנו, x, הם ב-Rn, ואם n הוא קטן, ובאופן אידיאלי, אם m הוא גדול, אז לדוגמא אם יש לנו נניח ערכת אימון דו מימדית, כמו הדוגמה שציירתי קודם. שבה n שווה ל-2, אבל יש לנו קבוצת אימון גדולה למדי, כמו שאני ציירתי שם בגרף, מספר גדול למדי של דוגמאות אימון, אז אולי כדאי להשתמש בליבה כדי להתאים גבול החלטה לא ליניארי ומורכבת יותר, ואז ליבה גאוסיאנית תהיה דרך מצוינת לעשות את זה. לקראת סוף הווידאו, אני אומר קצת יותר על מתי כדאי לבחור ליבה ליניארית, ליבה גאוסיאנית וכן הלאה. אבל באופן קונקרטי, אם תחליט להשתמש בקרנל גאוסיאני, אז הנה מה שאתה צריך לעשות. בהתאם לחבילת התמיכה של מכונת התמך הווקטורי שבה אתה משתמש, היא עשויה לבקש ממך ליישם פונקצית ליבה, או ליישם פונקציית דמיון. אם אתה משתמש באוקטבה או ביישום MATLAB של SVM, הוא עשוי לבקש ממך לספק פונקציה כדי לחשב תכונה מסוימת של הקרנל. כאן יש לנו בעצם חישוב של (f(i עבור איזשהו ערך של i, והתוצאה f היא מספר ממשי, אולי הייתי צריך לקרוא לו (f(i, אבל מה שאתם צריכים לעשות הוא לכתוב פונקציית דמיון, או פונקצית קרנל, שמקבלת את הקלט הזה, דוגמה שיכולה להיות דוגמת אימון או מבחן או אימות, איזה שהוא וקטור x1, ועוד קלט שהוא ציון-דרך, שכאן קראתי להם פשוט x1 ו-x2, כי בעצם גם ציוני דרך הם דוגמאות אימון, אז מה שאתם צריכים לעשות הוא לכתוב תוכנה שמקבלת את הקלט זה, x1, x2 ומחשבת את פונקצית הדמיון ביניהם ומחזירה מספר ממשי. אז חבילות מסוימות של מכונת תמך וקטורי יצפו מכם לספק את פונקצית הליבה הזו, שמקבלת את הקלט x1, x2 ומחזירה מספר ממשי. ומשם והלאה התוכנה תעבוד עצמאית ותיצור את כל התכונות, ובצורה אוטומטית היא תקח את x ותמפה אותו ל-f1, f2, עד (f(m באמצעות פונקצית הדמיון שסיפקתם, והיא תיצור את כל התכונות ותאמן את מכונת התמך הוקטורי בעצמה. אבל לפעמים אתם תצטרכו בעצמכם לספק את הפונקציה הזו. אם אתה משתמש בקרנל גאוסיאני, חלק מיישומי SVM כוללים גם את הגרעין הגאוסיאני ועוד כמה גרעינים אחרים, מכיוון שהקרנל הגאוסיאני הוא כנראה הקרנל הנפוץ ביותר. גרעינים גאוסיאנים וליניאריים הם למעשה שני הגרעינים הפופולריים ביותר הרבה מעל האחרים. רק נקודה אחד. אם יש לכם תכונות מקני מידה שונים מאוד, חשוב לבצע מישקול - התאמת קנה מידה לפני השימוש בגרעין הגאוסיאני. והסיבה היא זו. תחשבו על חישוב הנורמה של x-l, כן, המונח הזה כאן, המונה של האקספוננט כאן למעלה. מה שזה עושה, הנורמה של x-l, בעצם זה אומר שצריך לחשב וקטור v, שהוא שווה x-l. ואז לחשב את הנורמה של הוקטור v, שהוא ההפרש בין x ו-l. הנורמה של v מורכבת בעצם מ-v1 בריבוע פלוס v2 בריבוע וכולי וכולי, עד פלוס vn בריבוע. כי כאן x נמצא ב-Rn, או בעצם ב-Rn+1, אבל אנחנו כזכור מתעלמים מ-x0. אז בואו פשוט נאמר ש-x נמצא ב-Rn, הריבוע של הביטוי השמאלי הוא מה שעושה את המשוואה הזו נכונה. זה שווה לזה, נכון? אפשר לכתוב את זה אחרת, x1 מינוס l1 בריבוע, פלוס x2 מינוס l2 בריבוע, וכולי עד xn מינוס ln בריבוע, ועכשיו אם לתכונות יש טווחים שונים מאוד של ערכים, ניקח לדוגמא את חיזוי מחירי הבתים, אם הנתונים שלנו הוא מספר נתונים בקשר לבתים. ואם x1 הוא בטווח של אלפים, לדוגמא הגודל ברגל מרובעת, זו התכונה הראשונה, x1. אבל התכונה השנייה שלנו, x2 היא מספר חדרי השינה. אז נניח שזה בטווח של אחד עד חמישה חדרי שינה, אז x1 מינוס l1 הולך להיות ענק. זה יכול להיות משהו כמו אלף בריבוע, ולעומתו x2 מינוס l2 הולך להיות הרבה יותר קטן, ואם זה המקרה, אז בביטוי הזה, פונקצית המרחק תישלט בעיקר על ידי הגדלים של הבתים, ומספר החדרים יהיה במידה רבה לא משמעותי. אז כדי להימנע מזה, ועל מנת לגרום למכונה לעבוד היטב, חשוב לעשות מִישְקוּל של התכונות. וכך להבטיח שה-SVM נותן תשומת לב דומה לכל התכונות השונות, ולא רק כמו בדוגמה הזו לגודל של הבתים שבהם התכונות הם במספרים גדולים. כשמנסים מכונות תמך וקטורי הסיכויים הם ששני הגרעינים הנפוצים ביותר שמשתמשים בהם יהיו הליבה הליניארית, כלומר בלי ליבה, או הגרעין הגאוסיאני שדיברנו עליו. ורק הערה אחת, שימו לב שלא כל פונקצית דמיון שאתה מעלה על דעתך תיתן לך קרנל תקף. הקרנל הגאוסיאני, הקרנל הליניארי וכל גרעין אחר שתשתמש בו לפעמים, כולם צריכים לעמוד בתנאי הטכני הבא. הוא נקרא המשפט של מרסר והסיבה שצריך את זה היא כי באלגוריתם מכונת תמך וקטורי או במימושים של SVM יש הרבה טריקים נומריים של אופטימיזציה חכמה שנעשו כדי שיהיה אפשר לחשב את הפרמטר θ ביעילות, וכשעשו את התכנון המקורי של SVM, עשו את ההחלטות האלה להגביל את תשומת לבנו רק לקרנלים המספקים את המצב הטכני הזה שקרוי 'משפט מרסר'. וזה גרם לכך שאנחנו יודעים בביטחון שכל חבילות ה-SVM האלה, כל חבילות התוכנה האלה של SVM יכולות להשתמש בקבוצה גדולה של אופטימיזציות ולחשב את הפרמטר θ מהר מאוד. אז מה שרוב האנשים עושים בסופו של דבר הוא להשתמש בגרעין הליניארי או הגאוסיאני, אבל קיימים עוד כמה גרעינים שגם הם מספקים את משפט מרסר, ושאתם עשויים להיפגש בהם, אם כי אני באופן אישי משתמש בגרעינים אחרים לעתים רחוקות מאוד מאוד, אם בכלל. רק להזכיר כמה גרעינים אחרים שאתם עשויים לפגוש. האחד הוא הקרנל הפולינומי. ועבורו פונקצית הדמיון בין x ו-l מוגדר כך, יש הרבה אפשרויות, אתה יכול להשתמש ב-xᵀl)²). אז הנה מדד אחד של כמה x ו-l דומים. אם x ו-l קרובים מאוד זה לזה, אזי המכפלה הפנימית נוטה להיות גדולה. אז כפי שאתם מבינים, זוהי ליבה חריגה במקצת. לא משתמשים בה לעתים קרובות, אבל אפשר למצוא כמה אנשים שכן משתמשים בה. זוהי גרסה אחת של ליבה פולינומית. גרסה אחרת היא xᵀl)³). כל הדברים האלה על השקופית הם דוגמאות של קרנל פולינומי. xᵀl+1)³) xᵀl פלוס מספר שונה מ-1, אולי 5, ובחזקת 4, אז לקרנל פולינומי יש למעשה שני פרמטרים. האחד הוא, איזה מספר להוסיף? זה יכול להיות 0. בדוגמה הזו זה באמת פלוס 0, והפרמטר השני הוא דרגת הפולינום. אז הדרגה, המספרים האלה, והצורה הכללית יותר של הקרנל הפולינומי היא x משוחלף כפול l, פלוס איזה קבוע מועלה בחזקה מסוימת, אז שני אלה הם הפרמטרים של הקרנל הפולינומי. לקרנל הפולינומי בדרך כלל או כמעט תמיד יש ביצועים גרועים יותר מאשר לקרנל הגאוסיאני ולא משתמשים בו הרבה, אבל זה משהו שאתם עשויים להיתקל בו. בדרך כלל משתמשים בו רק עבור נתונים בהם x ו-l הם לגמרי לא שליליים, וזה מבטיח כי המכפלות הפנימיות האלה לעולם לא יהיו שליליות. וזה מתאים לאינטואיציה כי כאשר x ו-l מאוד דומים זה לזה, אז אולי המכפלה בין שניהם תהיה גדולה. יש לו עוד תכונות, אבל בכל אופן אנשים נוטים לא להשתמש בו הרבה. ואז, תלוי במה שאתה עושה, יש עוד כמה גרעינים איזוטריים אחרים, שאתה עלול להיתקל בהם. יש ליבת מחרוזת, זה דבר שמשתמשים בו לפעמים אם נתוני הקלט הם מחרוזות טקסט או סוגים אחרים של מחרוזות. יש דברים כמו ליבת חִי בריבוע, ליבת מפגש היסטוגרמה, וכן הלאה. אלה סוגים של גרעינים אזוטריים נוספים שבהם ניתן להשתמש כדי למדוד דמיון בין דברים מסוגים שונים. אז לדוגמה, אם אתה מנסה לפתור איזושהי בעיה של סיווג טקסט, כאשר הקלט x הוא מחרוזת אז אולי אנחנו רוצים למצוא את הדמיון בין שתי מחרוזות באמצעות ליבת מחרוזת, אבל אני אישית רק באופן נדיר מאוד, אם בכלל, משתמש בגרעינים היותר איזוטריים. אני חושב שיצא לי להשתמש בקרנל חי בריבוע אולי פעם אחת בחיים שלי, ובליבת ההיסטוגרמה, יכול להיות פעם או פעמיים בחיי. אני עצמי אף פעם לא השתמשתי בקרנל מחרוזת בעצמי. אבל אם במקרה נתקלתם בזה ביישומים אחרים, אם תעשו חיפוש מהיר באינטרנט בגוגל או בבינג אתם בוודאי תמצאו הגדרות של הגרעינים האלה גם כן. אז רק שני פרטים אחרונים שאני רוצה לדבר עליהם בוידאו הזה. אחד הוא סיווג רב-מחלקתי. אז נניח שיש לכם ארבע מחלקות, או באופן יותר כללי k מחלקות, ואתם רוצים שה-SVM יוציא גבול החלטה מתאים בין המחלקות הרבות. לרוב ה-SVM, לחבילות SVM רבות כבר יש פונקציונליות מובנית לסיווג רב-מחלקתי. אז אם אתם משתמשים בחבילה כזו, אתם פשוט צריכים להשתמש בפונקציונליות המובנית וזה צריך לעבוד בסדר. אחרת, דרך אחת לעשות זאת היא להשתמש בשיטת האחד-מול-השאר עליה דיברנו כאשר עסקנו ברגרסיה לוגיסטית. אז מה שעושים הוא לאמן k מכונות SVM נפרדות עבור k המחלקות, כל אחת מבדילה מחלקה אחת משאר המחלקות. וזה ייתן לכם k וקטורי פרמטרים, אז הראשון ייתן לכם את θ1 שמנסה להבחין בין המחלקה y שווה אחת לבין שאר המחלקות, והפרמטר השני, הוקטור θ2, זה מה שאתה מקבל כדי להבחין בין y שווה 2 כמחלקה החיובית וכל השאר כמחלקות שליליות וכן הלאה, עד וקטור הפרמטרים θk, שהוא וקטור פרמטרים המבדיל בין המחלקה האחרונה ובין השאר. ואז בעצם יש לנו בדיוק אותו דבר כמו בשיטת האחד-מול-השאר שהשתמשנו בה ברגרסיה לוגיסטית, שבה פשוט ניבאנו את המחלקה i שבה θᵀx היה המספר הגדול ביותר. אז זהו הסיווג הרב-מחלקתי במקרה שלנו. עבור המקרים הנפוצים יותר, יש סיכוי טוב שכל חבילת תוכנה בה תשתמש, יש סיכוי סביר שיש בה פונקציונליות מובנה לסיווג רב-מחלקתי, ולכן אתה לא צריך לדאוג בקשר לזה. עוד דבר אחרון, פיתחנו את מכונת התמך הוקטורי על ידי שהתחלנו עם רגרסיה לוגיסטית ואז שינינו קצת את פונקציית העלות. אז הדבר האחרון שאנחנו רוצים לעשות בוידאו הזה, הוא לדבר קצת על מתי תשתמש בכל אחד משני האלגוריתמים האלה. אז בואו נניח ש-n הוא מספר התכונות ו-m הוא מספר דוגמאות האימון. אז מתי אנחנו צריכים להשתמש באלגוריתם אחד ומתי בשני? אז אם n הוא גדול יחסית לגודל של סדרת האימון, למשל אם יש לך מצב של מספר תכונות הרבה יותר גדול מ-m, וזה יכול להיות, למשל, אם יש לך בעית סיווג טקסט, שבה המאפיין של וקטור התכונות יכול אולי להיות 10,000. וגודל סדרת האימון הוא אולי 10 או אולי עד 1000. דמיינו בעיה של סיווג דואר זבל, דואר זבל בתוך הדוא"ל, שבו יש נניח 10,000 תכונות שזה מקביל ל-10,000 מלים אבל מצד שני יש אולי 10 דוגמאות הכשרה או אולי עד 1,000 דוגמאות. אז אם n הוא גדול בהרבה יחסית ל-m, אז מה שאני הייתי עושה באופן נורמלי הוא להשתמש ברגרסיה לוגיסטית או להשתמש ב-SVM בלי גרעין או עם ליבה ליניארית. כי אם יש לך כל כך הרבה תכונות עם ערכת אימון קטנה יותר, אז פונקציה ליניארית תהיה כנראה בסדר גמור, וממילא אין לך מספיק נתונים כדי להתאים פונקציה לא ליניארית מסובכת מאוד. עכשיו אם n הוא קטן ו-m הוא בינוני, ומה שאני מתכוון בזה הוא ש-n הוא אולי איפשהו בין 1-1000, 1 זה קטן מאוד. אבל אולי עד 1000 תכונות, ואם מספר דוגמאות האימון הוא אולי איפשהו בין 10, נניח 10 עד אולי 10,000 דוגמאות. אולי אפילו עד 50,000 דוגמאות. אם m הוא די גדול, בסביבות 10,000 אבל לא מיליון. בסדר? אז אם m הוא בגודל בינוני אז SVM עם ליבה ליניארית יעבוד לעתים קרובות טוב. דיברנו על זה גם יותר מוקדם, עם דוגמה קונקרטית אחת, כאשר היתה לנו ערכת אימון דו מימדית. n היה שווה ל-2 והיה לנו שם תרשים עם מספר לא מבוטל של דוגמאות הכשרה. אז הגרעין הגאוסיאני יעשה עבודה די טובה להפריד בין דוגמאות חיוביות ושליליות. מצב שלישי מעניין הוא אם n הוא קטן אבל m הוא גדול. אם גם כאן n הוא אולי מ-1 עד 1000, או אולי יכול להיות גדול יותר. אבל m הוא אולי בין 50,000 ויותר ועד מיליונים. 50,000, 100,000, מיליון, שני מיליון. יש לנו סדרת אימון מאוד מאוד גדולה, כן. אז אם זה המקרה, אז SVM עם ליבה גאוסיאנית ירוץ בצורה קצת איטית. חבילות SVM של היום, אם נשתמש בקרנל גאוסיאני, יצטרכו להתאמץ קצת. אם יש לך אולי 50 אלף זה עוד בסדר, אבל אם יש לך מיליון דוגמאות הכשרה, אולי אפילו 100,000. עם ערך מסיבי של m. חבילות SVM של היום הן טובות מאוד, אבל הן עדיין עלולות להיתקל בקושי כאשר הן צריכות לעבוד על גדלים מסיביים של ערכות אימון כאשר משתמשים בקרנל גאוסיאני. אז במקרה זה, מה שאני הייתי בוחר לעשות הוא לנסות ליצור באופן ידני תכונות נוספות ואז להשתמש ברגרסיה לוגיסטית או ב-SVM בלי קרנל. ואם אתה מסתכל על השקופית הזאת ואתה רואה שרגרסיה לוגיסטית ו-SVM בלי ליבה נמצאים ביחד, בשני המקומות האלה קיבצתי אותם ביחד. ויש סיבה לכך, רגרסיה לוגיסטית ו-SVM בלי קרנל הם בעצם אלגוריתמים די דומים, גם רגרסיה לוגיסטית וגם SVM בלי קרנל בדרך כלל עושים דברים דומים למדי, ויש להם ביצועים דומים למדי, אבל בהתאם לפרטי היישום שלך, אחד מהם עשוי להיות יותר יעיל מאשר האחר. אבל כאשר אחד האלגוריתמים הללו מתאים, רגרסיה לוגיסטית או SVM ללא ליבה, השני גם הוא צפוי לעבוד די טוב. אבל הרבה מהעוצמה של SVM הוא כאשר אנחנו משתמשים בגרעינים שונים כדי ללמוד פונקציות לא לינאריות מורכבות. באזור הזה, כשיש לך אולי עד 10,000 דוגמאות, אולי עד 50,000. ומספר התכונות שלך גדול למדי. זה אזור נפוץ מאוד ואולי זה האזור שבו מכונת תמך וקטורי עם קרנל גאוסיאני תהיה הכוכב. אפשר לעשות דברים שהרבה יותר קשה לעשות בעזרת רגרסיה לוגיסטית. ולבסוף, היכן נכנסות כאן רשתות עצביות? אז עבור כל הבעיות הללו, עבור כל האזורים השונים האלה, רשת עצבית מתוכננת היטב גם היא עשויה לעבוד היטב. החיסרון היחיד, או הסיבה היחידה שאולי אני לא משתמש לפעמים ברשת העצבים, היא שבכמה מהבעיות האלה, הרשת העצבית עלולה ללמוד בצורה איטית. ולעומתה אם יש לך חבילת יישום טובה מאוד של SVM, היא יכולה לרוץ מהר יותר, ודי הרבה מהר יותר, מאשר הרשת העצבית שלך. ולמרות שלא הראינו את זה מקודם, מתברר כי בעיית האופטימיזציה של SVM היא בעיית אופטימיזציה קמורה ולכן חבילות התוכנה הטובות לאופטימיזציה של SVM ימצאו תמיד את המינימום הגלובלי או משהו קרוב אליו. ולכן עבור SVM אתה לא צריך לדאוג שהיא תמצא אופטימום מקומי. בפועל אופטימום מקומי הוא לא בעיה גדולה מדי גם עבור רשתות עצביות אבל הן כולם פחות אופטימליות, אז זה דבר אחד פחות לדאוג אם אתה משתמש ב-SVM. ובהתאם לבעיה שלך, הרשת העצבית עשויה להיות, במיוחד בסוג הזה האמצעי של אזור, איטית יותר מאשר SVM. במקרה שההנחיות שנתנו כאן נראות קצת מעורפלות, ואם אתה מסתכל על קצת בעיות, ההנחיות קצת מעורפלות, ואתה חושב, אני עדיין לא בטוח לגמרי, האם אני צריך להשתמש באלגוריתם זה או באלגוריתם ההוא, זה ממש בסדר. כאשר אני נתקל בבעיה של למידת מכונה, לפעמים זה פשוט לא ברור איזה אלגוריתם הוא הטוב ביותר לשימוש, אבל כפי שראית בקטעי הוידאו הקודמים, אתה יודע בעצם, שלמרות שהאלגוריתם אכן משנה, אבל מה שלעתים קרובות עוד יותר חשוב הוא דברים כמו, כמה נתונים יש לך, כמה אתה מיומן, כמה טוב אתה בניתוח שגיאות וניפוי של אלגוריתמי למידה, כמה אתה מבין איך לעצב תכונות חדשות, אתה מבין אילו תכונות נוספות לתת לאלגוריתם הלמידה שלך וכן הלאה. ולעתים קרובות הדברים האלה חשובים יותר מאשר האם אתה משתמש ברגרסיה לוגיסטית או SVM. אבל לאחר שאמרנו את זה, SVM עדיין באופן נרחב נתפס כאחד מאלגוריתמי הלמידה החזקים ביותר, ויש טווחי גדלים שבהם הוא דרך יעילה מאוד ללמוד פונקציות מורכבות לא ליניאריות. אז בעצם, עם רגרסיות לוגיסטיות, רשתות עצביות, SVM, כאשר תשתמש באלגוריתמים האלה כדי לזרז את הלמידה, אני מאמין שאתה ממוקם היטב כדי לבנות מערכות למידת מכונה שנמצאות בפסגת הטכנולוגיה עבור תחום יישומים רחב וזהו עוד כלי חזק מאוד שיש לך בארגז הכלים. כלי הנמצא בשימוש בכל מקום בעמק הסיליקון, כמו גם בתעשייה ובאקדמיה, והמשמש לבנות מערכות למידת מכונה בעלות ביצועים גבוהים.