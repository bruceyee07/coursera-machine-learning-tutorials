U ovome video voleo bih da počnemo da prilagođavamo
 pomoćne vektorske mašine da bismo razvili 
složene nelinearne klasifikatore. Glavna tehnika za to je 
nešto što se zove kernel. Hajde da vidimo šta su 
kerneli i kako da ih koristimo. Ako imate trening skup koji izgleda kao ovaj, i želite da nađete nelinearnu granicu odluke 
da biste razlikovali pozitivne i negativne primere, možda granicu odluke kao što je ova. Jedan način da to uradite je da nađete skup složenih polinomskih 
osobina, u redu? Dakle, skup osobina koji izgleda ovako, tako da dobijete hipotezu x koja predviđa 1 ako teta0 + teta1 * x1 * ... sve te polinomske osobine je veće od 0, a u suprotnom predviđa 0. A drugi način pisanja ovoga, da bi se uveo nivo novih obeležja koja ću da koristim kasnije, je da možemo da razmišljamo o hipotezi kao o proračunu granice odluke koristeći ovo. Tako, teta0 plus teta1 f1 plus teta2 f2, plus teta3 f3 plus i tako dalje. Gde ću da koristim ovo novo označavanje f1, f2, f3, i tako dalje da bih označio novu vrstu osobina koje izračunavam, tako f1 je samo x1, f2 je jednako x2, f3 je jednako ovome ovde. Dakle, x1 x2. Tako, f4 je jednako x1 na kvadrat a f5 je x2 na kvadrat i tako dalje i vidimo iz prethodnog da nalaženje ovoga polinoma visokog reda je jedan način da se nađe mnogo više osobina, pitanje je, da li postoji različit izbor osobina ili da li postoji bolji način 
biranja osobina od ovoga polinoma visokog reda. Jer, znate, nije baš jasno da je ovaj polinom visokog reda ono što želimo, a o čemu smo pričali, kompjuterska vizija, pričali smo o ulazima koji su slike sa mnogo piksla. Takođe smo videli kako korištenje 
polinoma visokog stepena postaje veoma zahtevno sa strane proračuna jer postoji mnogo članova polinoma visokog reda. Dakle, da li postoji drugačiji ili bolji izbor osobina koje bismo mogli da primenimo u ovu vrstu hipoteze? Evo jedne ideje kako da definišemo nove osobine f1, f2, f3. Na ovoj liniji ću da definišem samo tri nove osobine, ali u stvarnim problemima moraćemo da definišemo mnogo veći broj. Evo šta ću ovde da uradim, u ovoj fazi osobina x1, x2, ostaviću x0 izvan ovoga, presretač x0, ali u ovoj fazi x1, x2, samo ću da ručno pokupim par tačaka, i tada ću da pozovem te tačke l1, pokupićemo drugu tačku, zvaćemo je l2 i hajde da pokupimo ovu treću i zvaćemo je l3, i za sada, recimo samo da ću da izaberem te tri tačke ručno. Poredaću ove tri tačke, jedan, dva tri. Sledeće šta ću da uradim je da definišem moje nove 
osobine kao što sledi, za dati primer x, hajde da definišem moju prvu osobinu f1 da bude neka mera sličnosti između moga trening primera x i moga prvog obeležja a ova specifična formula koju ću da koristim da bih merio sličnost će da bude ovo e na minus dužina od x minus l1 na kvadrat podeljeno sa dva sigma na kvadrat. Dakle, u zavisnosti da li ste gledali prethodni video, ovo obeležavanje, to je dužina vektora w. I tako, ovo ovde, ovo x minus l1, to je u stvari Euklidovo rastojanje na kvadrat, to je Euklidovo rastojanje između tačke x i oznake l1. Kasnije ćemo pričati više o ovome. A to je moja prva osobina, a moja druga osobina f2 će da bude, znate, funkcija sličnosti koja meri koliko su slične x i l2 i igra 
će da bude definisana kao sledeća funkcija. A to je e na minus kvadrat Euklidovog rastojanja između x i druge oznake, za to služi enumerator, podeljeno sa dva sigma na kvadrat i slično f3 je, znate, sličnost između x i l3, što je jednako, ponovo, sličnoj formuli. A šta je ova funkcija sličnosti, matematički pojam za ovo je da je to funkcija jezgra. A određeno jezgro koje ovde koristim to je u stvari Gausovo jezgro. I tako ova formula, ovaj određeni izbor funkcije sličnosti 
se zove Gausovo jezgro. Način na koji se imenuje ovo je, sažeto, ove različite funkcije sličnosti se zovu jezgra a možemo da imamo više funkcija sličnosti a primer koji sam naveo 
ovde je Gausovo jezgro. Videćemo ostale primere jezgara. Za sada, ovo ćemo da 
smatramo samo funkcijama sličnosti. I tako, umesto pisanja sličnosti između x i l, ponekad takođe pišemo ovo kao malo slovo k od x i 
jedne od mojih oznaka, u redu? Pa hajde da vidimo šta jezgra u stvari rade i zašto ova vrsta funkcija sličnosti, zašto ti izrazi imaju smisla. Uzmimo moju prvu oznaku. Moja oznaka l1, što je jedna od tačaka koje sam odabrao na slici. Sličnost kernela između 
x i l1 je data ovim izrazom. Da budemo sigurni, znate, mi smo na istoj strani gde je pojam enumerator, enumerator takođe može da bude napisan kao suma od j = 1 do n koja sumira rastojanja. To je rastojanje između komponenata, između vektora x i vektora l. I opet, zbog ovih linija ignorisaću x0. Samo ignorisati član x0, koji je uvek jednak 1. Dakle, ovako računamo jezgro sa 
sličnošću između x i oznake. Hajde da vidimo šta ova funkcija radi. Pretpostavimo da je x blizu jednoj od oznaka. Tada će formula Euklidovog rastojanja i enumerator biti blizu 0, u redu? Tako, ovaj član ovde, rastojanje je sjajno, rastojanje između x i l će da bude 0 i tako, f1, to je jednostavno osobina, će da bude približno e na minus 0 i enumerator na kvadrat kroz 2 sigma je jednako dakle, e na minus 0, e na minus 0, e na 0 će da bude blizu 1. Staviću znak približno ovde jer rastojanje ne mora da bude tačno 0, ali ako je x blizu oznake ovaj član će da bude blizu 0 i tako f1 će da bude blizu 1. Suprotno tome, ako je x daleko od l1, tada će ova prva osobina f1 da bude e na minus neki veliki broj na kvadrat, podeljeno sa dva sigma na kvadrat i e na minus veliki broj će da bude blizu 0. Dakle, ono šta te osobine rade je da mere koliko su slični x i jedna od vaših oznaka, osobina f će da bude blizu 1 kada je x blizu vašoj oznaci a blizu je 0 ili će da bude 0 kada je x daleko od vaše oznake. Svaka od tih oznaka u prethodnoj liniji, nacrtao sam tri oznake, l1, l2, l3. Svaka od tih oznaka 
definiše novu osobinu f1, f2 i f3. To znači, za dati trening primer x možemo da izračunamo tri nove osobine: f1, f2 i f3 od, znate, tri oznake koje sam upravo napisao. Prvo, hajde da vidimo ovu eksponencijalnu 
funkciju, da vidimo ovu funkciju sličnosti i iscrtajmo neke oblike da biste razumeli bolje kako to stvarno izgleda. U ovom primeru, recimo da
 imam dve osobine x1 i x2. Recimo da je moja prva oznaka, l1, na lokaciji 3, 5. Isto tako, recimo da je sigma na 
kvadrat jednako 1 za sada. Ako iscrtam kako ova osobina izgleda, dobiću ovaj oblik. Vertikalna osa, visina površi, je vrednost f1 a ovde dole na horizontalnoj osi su, ako imam neki trening primer, tamo je x1 a tamo x2. Za dati trening primer, trening primer ovde koji pokazuje vrednost x1 i x2 na visini iznad površi, pokazuje odgovarajuću vrednost f1 a ispod ovoga je isti oblik koji sam bio pokazao, koristeći kvantifikovan oblik, sa x1 na horizontalnoj osi, x2 na horizontalnoj osi i tako, ovaj oblik na dnu je samo konturni oblik 3D površi. Primećujete da, kada je x jednako tačno 3, 5 tada f1 uzima vrednost 1 jer je na maksimumu a x se pomera, kako x odmiče dalje tada ova osobina uzima vrednost koja je blizu 0. I tako, to je stvarno osobina, f1 meri, znate, koliko je x blizu prve oznake i ona se menja između 0 i 1 u zavisnosti od toga koliko je x blizu prvoj oznaci l1. Druga stvar u vezi sa ovim slajdom je efekat promene ovoga parametra
 sigma na kvadrat. Sigma na kvadrat je parametar Gausovog jezgra i kako ga menjate,
 dobijaćete blago različite efekte. Recimo da je sigma na kvadrat jednaka 0.5 i da vidimo šta dobijamo. Postavimo
 sigma na kvadrat na 0.5, ono što vidite je da jezgro izgleda slično, osim što širina ispupčenja postaje uža. Konture se takođe skupljaju. Dakle, ako je sigma 
na kvadrat jednako 0.5 i ako počnete od x jednako 3, 5 i ako se mičete dalje, tada osobina f1 pada na 0 mnogo brže i obrnuto, ako povećate sigma na kvadrat na 3, u tom slučaju ako se mičem dalje od, znate, l, ova tačka ovde je stvarno l, u redu, to je l1 na lokaciji 3, 5, u redu? 
To je pokazano ovde. Ako je sigma na kvadrat veliko, tada ako se mičete dalje od l1, vrednost osobine pada mnogo sporije. Dakle, za datu definiciju osobina, hajde da vidimo koji izvor hipoteze možemo da obučimo. Za dati trening primer x, izračunaćemo ove osobine f1, f2, f3 a hipoteza će da predvidi jedan kada teta0 plus teta1 f1 plus teta2 f2, i tako dalje je veće ili jednako 0. Za ovaj određeni primer, recimo da sam već našao algoritam učenja i recimo da, znate, nekako sam našao ove vrednosti parametara. Tako, teta0 je jednako -0.5, teta1 je jednako 1, teta2 je jednako 1 i teta3 je jednako 0. Ono šta hoću da uradim je da razmotrim šta se dešava ako imamo trening primer koji uzima lokacije na ovoj magenta tački, baš gde sam 
upravo nacrtao ovu tačku ovde. Recimo da imam trening primer x, šta će moja hipoteza da predvidi? Pa, pogledajmo ovu formulu, moj trening primer x je blizu l1, imamo to da će f1 da bude blizu 1 zbog toga što moj trening primer x je daleko od l2 i l3 imamo da će f2 da bude blizu 0 i f3 će da bude blizu 0. Ako pogledamo ovu formulu, imamo teta0 plus teta1 puta f1 plus teta2 puta neka vrednost. Ne baš 0 ali neka vrednost blizu 0. I plus teta3 puta nešto blizu 0. A to će da bude 
jednako ovim vrednostima. Dakle, dobijamo -0.5 plus 1 puta 1 je jedan, i tako dalje, što je jednako 0.5 što 
je veće ili jednako 0. Dakle, za ovu tačku, predvidećemo da je y jednako 1, jer je ovo veće ili jednako 0. Hajde sada da uzmemo drugu tačku. Recimo da ću sada da uzmem drugu tačku, nacrtaću ovu tačku u drugoj boji, recimo cian, za tačku tamo, ako je to moj trening primer x, tada ako napravite sličan proračun, dobićete da f1, f2, f3 će da budu blizu 0. I tako, imamo teta0 plus teta1 f1 plus i tako dalje a ovo će da bude otprilike jednako -0.5, zbog toga što teta0 je -0.5 a f1, f2, f3 su sve 0. Dakle, ovo će da bude 
-0.5. ovo je manje od 0. I tako, za ovu tačku tamo, predvidećemo da je y jednako 0. A ako ovo uradite za opseg različitih tačaka, budite ubeđeni da ako imate trening primer koji je blizu l2, recimo, tada će za tu tačku 
predviđanje za y biti 1. I u stvari, ono šta radite je, znate, ako pregledate ovu granicu, ovaj prostor, ono šta ćemo da nađemo je da za tačke blizu l1 i l2 predviđanje će da bude pozitivno. A za tačke udaljene od l1 i l2, za tačke udaljene od ove dve oznake, dobićemo predviđanje koje je blizu 0. I tako, ono što radimo je da granica odluke ove hipoteze će da izgleda ovako nekako, gde će predviđanje unutar granice odluke da predvidi y jednako 1 a izvan će da predvidi y jednako 0. Dakle, ovako sa ovom definicijom oznaka i funkcijom jezgra možemo da obučimo 
veoma složenu nelinearnu granicu odluke, onako kako sam upravo nacrtao, gde predviđamo pozitivne vrednosti ako 
smo blizu jednoj od dve oznake. A predviđamo negativne kada smo veoma daleko od bilo koje oznake. I tako, ovo je deo ideje o jezgrima i kako ih koristimo sa mašinom vektora podrške, a to je da definišemo dodatne osobine koristeći oznake i funkcije sličnosti da bismo naučili složenije 
nelinearne klasifikatore. Nadam se da sam vam dao osećaj o ideji jezgara i kako bismo mogli da ih koristimo da bismo definisali nove
 osobine za mašinu vektora podrške. Ali postoji nekoliko pitanja 
na koja nismo odgovorili. Jedno je, kako dobijamo te oznake? Kako biramo te oznake? A drugo je, koje još funkcije sličnosti, ako ih ima, možemo da koristimo a da nije ona o kojoj smo pričali, koju zovemo Gausov kernel. U sledećem videu daćemo odgovore na ta pitanja i spojiti sve zajedno da bismo pokazali kako mašina vektora podrške sa jezgrima može da bude moćan način učenja složenijih nelinearnih funkcija.