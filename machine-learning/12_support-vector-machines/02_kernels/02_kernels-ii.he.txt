בסרטון האחרון, התחלנו לדבר על הרעיון של הגרעין - קרנל וכיצד ניתן להשתמש בו כדי להגדיר תכונות חדשות עבור מכונת תמך וקטורי. בסרטון הזה, אני רוצה להשלים כמה פרטים חסרים וגם לומר כמה מילים על איך להשתמש ברעיונות האלה בפועל. כגון, איך הם נוגעים, למשל, ליחסי הגומלין בין ההטיה והשונות במכונת תמך וקטורי. בסרטון האחרון דיברתי על תהליך של בחירת מספר ציוני דרך. אתם זוכרים, l1, l2, l3, מה שאפשר לנו להגדיר את פונקצית הדמיון המכונה גם הקרנל או בדוגמה הזו היתה לנו פונקצית דמיון שהיתה ליבה גאוסיאנית. וזה אפשר לנו לבנות את הסוג הזה של פונקציה השערה. אבל מאיפה אנחנו מקבלים את ציוני הדרך? מאיפה אנחנו מקבלים את l1, l2, l3? חוץ מזה, נראה שלבעיות למידה מורכבות, אנחנו מן הסתם רוצים הרבה יותר ציוני דרך מאשר שלושה שאפשר לבחור ידנית. אז בפועל כך נבחרות הנקודות. כשיש לנו בעית למידה ממוחשבת, יש לנו נתונים של מספר דוגמאות חיוביות ושליליות. אז הרעיון כאן הוא שאנחנו לוקחים את כל הדוגמאות ועל כל דוגמת אימון שיש לנו, אנחנו פשוט, אנחנו פשוט נשים ציוני דרך בדיוק באותן מקומות כמו דוגמאות האימון. אז אם יש לי דוגמת אימון אחת שנקרא לה x1, אז אני אבחר את ציון הדרך הראשון שלי l1 להיות בדיוק באותו מיקום כמו דוגמת האימון הראשונה שלי. ואם יש לי עוד דוגמת אימון x2, אז אנחנו נגדיר את ציון הדרך השני l2 להיות המיקום של דוגמת האימון השנייה שלי. על התרשים מימין השתמשתי בנקודות אדומות וכחולות רק להדגמה, הצבע של התרשים, הצבע של הנקודות על התרשים בצד ימין איננו משמעותי. אבל מה שיהיה לי בסופו של דבר בשימוש בשיטה הזו הוא שיהיו לנו m ציוני דרך l1, l2 וכו' עד lm אם יש לי m דוגמאות אימון, עם ציון דרך אחד לכל מיקום לפי המיקום של כל אחת מדוגמאות האימון שלנו. וזה נחמד כי זה אומר שהתכונות שלי בעצם ימדדו עד כמה קרובה דוגמה לאחד מהדברים שראיתי בסדרת האימון שלי. אז רק כדי לכתוב את התיאור הזה בצורה קצת יותר מעשית, בהינתן m דוגמאות אימון, אני אבחר את המיקום של ציוני הדרך שלי להיות בדיוק על המקומות של m דוגמאות האימון שלי. כאשר אנחנו מקבלים דוגמה x, והדוגמה הזו x יכולה להיות משהו ממערך האימון, היא יכולה להיות משהו ממערך האימות הצולב, או שהיא יכולה להיות משהו ממערך המבחן. בהינתן דוגמה x אנחנו נחשב, כפי שאתם רואים, תכונות כאלה כמו f1, f2, וכן הלאה. שבהם l1 למעשה שווה ל-x1 וכן הלאה. ואז אלה הם מה שנותן לי וקטור תכונות. אז אני אכתוב את זה כוקטור תכונות. אני לוקח את f1, f2 וכן הלאה, ופשוט מקבץ אותם לוקטור תכונות. מההתחלה ועד fm. ואתם יודעים, רק בגלל הקונבנציה, אם אנחנו רוצים, אנחנו יכולים להוסיף תכונה נוספת f0, שתמיד שווה 1. אז זה משחק תפקיד דומה למה שהיה לנו בעבר עבור x0, שהיה ההיסט - האינטרספטור - שלנו. אז לדוגמה, אם יש לנו דוגמת הדרכה (x(i), y(i התכונות שנחשב עבור הדוגמא הזו תהיינה כדלקמן: בהינתן ⁽x⁽ⁱ אנחנו נְמַפֶּה אותו ל-(f1(i, שערכו פונקצית הדמיון בין (x(i ו-l1. שאני הולך לקצר לאותיות SIM במקום לכתוב את כל המילה הארוכה "דמיון", בסדר? ו-(f2(i שווה לפונקצית הדמיון בין (x(i ו-l2, וכן הלאה עד (fm(i שהוא הדמיון בין (x(i ו-lm. ואיפשהו באמצע, אי שם ברשימה הזו, ברכיב ה-i, אנחנו למעשה נקבל תכונה אחת שהיא (fi(i, שהיא הדמיון בין x ו-(l(i. אבל הרי li שווה בדיוק ל-(x(i, ולכן כמובן (fi(i יהיה פשוט הדמיון בין x לבין עצמו. ואם אתה משתמש בליבה הגאוסיאנית אתה מקבל בעצם e בחזקת מינוס 0 חלקי 2 סיגמא בריבוע אז זה יהיה שווה ל-1 וזה בסדר. אז אחת התכונות שלי עבור דוגמת האימון הזו יהיה שווה ל-1. ואז בדומה למה שיש לנו אנחנו יכולים לקחת את כל m התכונות האלה ולרשום אותם כוקטור של תכונות. אז במקום לייצג את הדוגמה שלי, באמצעות (x(i שהוא וקטור מממד (R(n או (R(n+1, תלוי אם הגדרנו מונח קבוע (0)f, הוקטור הוא או ב-(R(n או ב-(R(n+1. כעת אנו יכולים לייצג את דוגמאות ההדרכה שלנו באמצעות וקטור התכונות הזה f. אני אכתוב את זה כ-f סימן עילי i. שלוקח את כל הדברים האלה ומציב אותם בתוך וקטור. דהיינו (f1(i עד (fm(i, וכמו שאמרנו קודם אפשר וגם נהוג להוסיף את (f0(i, כאשר (f0(i שווה תמיד 1. אז הוקטור הזה כאן נותן לי וקטור חדש של תכונות שמייצגות את דוגמאות האימון. אז בהינתן פונקציות הדמיון והקרנלים האלה, הנה שיטה לשימוש במכונת תמך וקטורי פשוטה. אם כבר חישבנו ולמדנו את קבוצת הפרמטרים θ, אז אם עכשיו אנחנו מקבלים ערך של x ורוצים לעשות חיזוי, מה שאנחנו עושים הוא לחשב את התכונות f, שהם עכשיו וקטור תכונות בעל ממד m+1. ויש לנו m כאן כי יש לנו m דוגמאות אימון ולכן יש לנו m ציוני דרך ומה שאנחנו עושים הוא לחזות 1 אם θᵀf גדול או שווה ל-0. בסדר. θᵀf, כמובן, זה שווה פשוט ל-θ0*f0 + θ1*f1 פלוס נקודה נקודה, פלוס θm כפול (f(m. וגם וקטור הפרמטרים θ שלנו הוא עכשיו וקטור בעל ממד m + 1. ויש לנו כאן m כי מספר ציוני הדרך שווה לגודל סדרת האימון. אז m היה גודל סדרת האימון ועכשיו וקטור הפרמטרים θ יהיה בעל ממד m + 1. אז כך עושים חיזוי אם כבר יש לנו הגדרה עבור וקטור הפרמטרים θ. אבל איך אתה מקבל את הערכים של וקטור הפרמטרים θ? ובכן את זה אנחנו עושים באמצעות אלגוריתם הלמידה SVM, וספציפית מה שעושים הוא לפתור את בעיית המינימיזציה הזו. אנחנו צריכים למזער את הביטוי C כפול פונקצית העלות הזו שהגדרנו מקודם. אלא שעכשיו, במקום לעשות תחזיות באמצעות (θᵀx(i, בעזרת התכונות המקוריות שלנו, (x(i. במקום זאת לקחנו את התכונות (x(i והחלפנו אותם בתכונות חדשות, אז אנחנו משתמשים ב-θᵀ כפול (f(i כדי לבצע חיזוי על דוגמת האימון ה-i, צריך להחליף בשני המקומות, ועל ידי פתרון בעית המזעור הזו אנחנו מקבלים את הפרמטרים עבור מכונת התמך הוקטורי שלנו. עוד פרט אחד אחרון, בבעיית האופטימיזציה הזו בעצם יש לנו m תכונות וגם m וקטורים. המספר הזה כאן. זה מספר התכונות שיש לנו. ובעצם המספר המעשי של תכונות שיש לנו הוא המימד של f. אז הוא למעשה שווה ל-m. אז אם תרצו אתם יכולים לחשוב על זה כסכום וזה אכן הסכום מ-j שווה 1 עד m. דרך אחת לחשוב על זה, אפשר לחשוב על זה כמצב שבו n שווה ל-m, כי אם הגודל של f הוא מספר התכונות, אז יש לנו m+1 תכונות, כשה1+ מגיע מההיסט f0. וכאן במחובר השני, אנחנו עדיין עושים סכום מ j שווה 1 עד n או m, כי כמו בקטעי הוידאו הקודמים שלנו לגבי ההסדרה, אנחנו עדיין לא מסדירים את הפרמטר θ₀, ולכן זה סכום עבור j הולך מ-1 עד m במקום j שווה 0 עד m. אז זהו אלגוריתם הלמידה של מכונת תמך וקטורי. יש עוד איזה פרט מתמטי שאני צריך להזכיר, והוא שבדרך שבה מיושמת מכונת תמך וקטורי, הביטוי האחרון הזה נעשה למעשה קצת אחרת. אתם לא באמת חייבים לדעת את הפרט האחרון הזה כדי להשתמש במכונת תמך וקטורי, ולמעשה המשוואות הכתובות כאן כבר אמורות לתת לכם את כל האינטואיציות שאתם זקוקים לה. אבל באופן שבו מכונת תמך וקטורי מיושמת, הביטוי הזה, הסכום של θj בריבוע, כן? אפשר לכתוב את זה בדרך נוספת והיא פשוט θᵀ כפול θ, אם נתעלם מהפרמטר θ₀. θ1 עד θm. תוך התעלמות מ-θ0. הסכום הזה של j מ-1 עד m של θj בריבוע, יכול גם להיכתב כ-θᵀθ. ומה שרוב התוכנות שעושות יישומי מכונת תמך וקטורי עושות, הוא למעשה להחליף את θᵀθ ובמקומו לכתוב θᵀ, כפול איזושהי מטריצה שתלויה בקרנל בו משתמשים, כפול θ. ולכן זה נותן לנו פונקצית מרחק מעט שונה. הוא משתמש במדידה שונה במקצת, במקום למזער בדיוק את הנורמה של θ בריבוע, אנחנו נמזער משהו די דומה לזה. זה משהו כמו גרסה בקנה מידה קצת שונה של וקטור הפרמטרים θ, תלוי בקרנל. אבל זה רק איזה עניין מתמטי שולי. שמאפשר לתוכנה של מכונת התמך הוקטורי לרוץ הרבה יותר ביעילות. והסיבה שמכונת התמך הוקטורי עושה את זה עם השינוי הקטן הזה היא שזה מאפשר למכונה לרוץ בקנה מידה הרבה יותר גדול של ערכות אימון. כי למשל, אם יש לנו קבוצת אימון עם 10,000 דוגמאות הכשרה. אז לפי הדרך בה אנו מגדירים ציוני דרך, אנחנו נגיע ל-10,000 ציוני דרך. אז הוקטור θ נהיה 10,000 מימדי. זה עדיין אולי עובד, אבל כאשר m הופך להיות באמת, באמת גדול אז לחשב כשיש לנו כל הפרמטרים האלה, נניח אם m היה 50,000 או 100,000 אז החישוב של כל הפרמטרים האלה יכול להיות יקר עבור תוכנת האופטימיזציה של מכונת תמך וקטורי, ולכן יקר לפתור את בעיית המינימיזציה שתיארתי כאן. אז כל זה הוא קצת פרטים מתמטיים, שאתם באמת לא חייבים לדעת. אז בשביל הייעול, אנחנו בעצם משנים קצת את הביטוי האחרון למזעור של משהו קצת שונה מאשר פשוט למזער את הנורמה בריבוע של θ. אבל אם אתם רוצים, תרגישו חופשיים לחשוב על זה כעל סוג של פרט יישומי שאכן משנה מעט את מטרת האופטימיזציה, אבל הוא נעשה בעיקר מסיבות של יעילות חישובית, אז בדרך כלל זה באמת לא צריך לענייו אתכם. ודרך אגב, אם אתם תוהים מדוע אנחנו לא מחילים את הרעיון של קרנל על אלגוריתמים אחרים כגון על רגרסיה לוגיסטית, מתברר שאם נרצה, נוכל ליישם את הרעיון של הקרנל ולהגדיר את סוג התכונות האלה באמצעות ציוני דרך על רגרסיה לוגיסטית. אבל הטריקים החישוביים שעובדים טוב במכונות תמך וקטורי אי אפשר להכליל אותם היטב לעבודה על אלגוריתמים אחרים כמו רגרסיה לוגיסטית. אז שימוש בקרנלים עם רגרסיה לוגיסטית יעבוד יותר מדי לאט, בעוד שבזכות הטריקים החישוביים, כמו המטריצה הנוספת כאן וכיצד היא משנה את החישוב והפרטים של אופן היישום של מכונת תמך וקטורי, יוצא שמכונת תמך וקטורי וקרנל נוטים ללכת טוב במיוחד יד ביד. ולעומת זאת עם רגרסיה לוגיסטית וקרנלים, אפשר לעשות את זה, אבל זה יפעל לאט מאוד. וזה לא יוכל לנצל את טכניקות האופטימיזציה המתקדמות שאנשים המציאו עבור המקרה המיוחד של הפעלת מכונת תמך וקטורי בעזרת גרעין. אבל כל זה נוגע בעצם רק לצורה בה מיישמים תוכנה כדי למזער את פונקצית העלות. אני אדבר על זה עוד בסרטון הבא, אבל בעצם אתם לא צריכים ממש לדעת איך לכתוב תוכנה למזער את פונקצית העלות הזו, כי אפשר למצוא בקלות תוכנות-מדף מעולות לעשות את זה. ובדיוק כמו שאני לא הייתי ממליץ על כתיבת קוד כדי להפוך מטריצות או כדי לחשב שורש ריבועי, אני גם לא ממליץ לכתוב תוכנה למזער את פונקצית העלות הזו בעצמכם, אלא במקום זאת כדאי לכם להשתמש בחבילות תוכנת מדף שאנשים כבר פיתחו ולכן חבילות תוכנה אלה כבר מגלמות את הטריקים האלה של אופטימיזציה נומרית, אז אתם לא באמת צריכים להתעמק בזה. אבל דבר אחד שכן ראוי לדעת עליו הוא זה, כאשר מפעילים מכונת תמך וקטורי, איך בוחרים את הפרמטרים של מכונת התמך הוקטורי? והדבר האחרון שאני רוצה לעשות בסרטון הזה הוא לומר איזו מילה בקשר ליחסי הגומלין בין ההטיה והשונות בעת שימוש במכונת תמך וקטורי. כאשר אנחנו משתמשים ב-SVM, אחד הדברים שצריך לבחור הוא הפרמטר C שנמצא במטרת האופטימיזציה, ואתם זוכרים ש-C שיחק תפקיד דומה ל-1 חלקי λ, כש-λ הוא פרמטר ההסדרה שהיה לנו ברגרסיה לוגיסטית. זאת אומרת שאם יש לנו ערך גדול של C, זה מתאים למצב ברגרסיה לוגיסטית שבו יש לנו ערך קטן של λ שמשמעותו הוא שההסדרה משחקת תפקיד שולי. וכשמשתמשים ב-λ קטנה או ב-C גדול, בדרך כלל ההשערה תהיה בעלת הטיה נמוכה אבל שונות גבוהה. בעוד שאם משתמשים בערך קטן יותר של C זה מתאים למצב של רגרסיה לוגיסטית עם ערך גדול של λ שזה מתאים להשערה בעלת הטיה גבוהה יותר ושונות נמוכה יותר. במילים אחרות, להשערה עם C גדול יש שונות גבוהה יותר, ולכן היא נוטה יותר להתאמת-יתר, ואילו להשערה עם C קטן יש הטיה גבוהה יותר ולכן היא נוטה יותר להתאמת-חסר. אז הפרמטר C הוא אחד מהפרמטרים שאנחנו צריכים לבחור. הפרמטר השני שאנחנו צריכים לבחור הוא הפרמטר σ² שהופיע בגרעין הגאוסיאני. אז אם בגרעין הגאוסיאני σ² הוא גדול, אז בפונקציה הדמיון, שכזכור היא e בחזקת מינוס הנורמה של x מינוס ציון-הדרך בריבוע חלקי 2σ². נסתכל על דוגמא; נניח שיש לי רק תכונה אחת, x1, אם יש לי ציון דרך שם באותו מיקום, אם σ² הוא גדול, אז הקרנל הגאוסיאני נוטה לרדת יחסית לאט ולכן כך ייראה המאפיין (f(i, ולכן זו תהיה פונקציה חלקה יותר, פונקציה המשתנה בצורה חלקה יותר, ולכן זה ייתן לנו פונקצית השערה עם הטיה גבוהה יותר ושונות נמוכה יותר, כי הקרנל הגאוסיאני יורד בצורה חלקה, ולכן אנחנו נקבל השערה שמשתנה לאט, או שמשתנה בצורה חלקה כשמשנים את הקלט x. לעומת זאת, אם σ² הוא קטן, ואם זהו ציון הדרך וזו התכונה הראשונה, הגרעין הגאוסיאני, אזי פונקציית הדמיון תשתנה בצורה חדה יותר. ובשני המקרים הפונקציה מקבלת מקסימום ב-1, ולכן אם סיגמא בריבוע הוא קטן, אז התכונות משתנות בצורה פחות חלקה. ואנחנו מקבלים כאן מדרונות גבוהים יותר או נגזרות גבוהות יותר. ואם נשתמש ב-σ² קטן, נקבל השערות עם הטיה נמוכה יותר וכנראה שונות גבוהה יותר. וכשתפתרו את שיעורי הבית של השבוע, אתם בעצם תקבלו הזדמנות לשחק עם כמה מהרעיונות האלה בעצמכם ולראות את ההשפעות האלה בעצמכם. אז זה היה האלגוריתם של מכונת תמך וקטורי עם קרנל. ואני מקווה שהדיון הזה על הטיה ושונות ייתן לכם קצת הרגשה גם בקשר לאיך אפשר לצפות מהאלגוריתם הזה להתנהג.