בהרצאה הזו, אני רוצה לספר לכם קצת על המתמטיקה מאחורי "סיווג בעל שוליים רחבים". הסרטון הזה הוא אופציונלי, לכן דלגו עליו ללא חשש אם תרצו. זה יכול גם לתת לכם אינטואיציה טובה יותר על איך מובילה בעיית האופטימיזציה של מכונת תמך וקטורי, איך היא מובילה לסיווג עם שוליים רחבים. וכדי להתחיל, תרשו לי קודם כל להזכיר לכם כמה מאפיינים של איך נראית מכפלה פנימית וקטורית. נניח שיש לי שני וקטורים u ו-v, שנראים ככה. שני וקטורים דו-ממדיים. אז בואו נראה איך נראית uᵀv ו-uᵀv נקרא גם המכפלה הפנימית בין הוקטורים u ו-v. u הוא וקטור דו-מימדי, אז אני יכול להתוות אותו על הגרף הזה. אז נניח שזה הוקטור u. וההסבר לדבר הזה הוא שאנחנו שמים נקודה שבה הערך על הציר האופקי מקבל את הערך של u1, והערך על הציר האנכי, הגובה הוא מה שיש ב-u2, הרכיב השני של וקטור u. עכשיו, יש גודל אחד שיהיה נחמד לחשב והוא הנורמה של הוקטור u|| ,u||. הקווים האנכיים הכפולים משמאל ומימין לוקטור מציינים את הנורמה או האורך של u. מה שזה אומר בפשטות זה האורך האוקלידי של הוקטור u. שהוא לפי משפט פיתגוראס שווה פשוט לשורש של u1 בריבוע פלוס u2 בריבוע, נכון? וזהו האורך של וקטור u. זה מספר ממשי. וזה פשוט האורך של הוקטור הזה. זהו האורך של החץ שציירתי כרגע, זוהי הנורמה של u. עכשיו בואו נחזור ונסתכל על הוקטור v כי אנחנו רוצים לחשב את המכפלה הפנימית. אז v יהיה וקטור אחר עם איזה שהם ערכים v1, v2. אז הוקטור v ייראה כך, הנה v כאן. עכשיו בואו נחזור ונסתכל איך לחשב את המכפלה הפנימית בין u ו-v. הנה איך אפשר לעשות את זה. תרשו לי לקחת את הוקטור v ולהוריד ממנו אנך - הטל - על וקטור u. אז אני הולך לקחת הטל מאונך או הטל של 90 מעלות, ולהוריד אנך על u, כך. ומה שאני הולך לעשות הוא למדוד את אורך הקו האדום הזה שכרגע ציירתי. אז אני הולך לסמן את האורך של הקו האדום ב-p. אז p הוא האורך או הגודל של ההטל של הוקטור v על הוקטור u. תרשו לי לכתוב את זה. אז p הוא אורך ההטל של הוקטור v על הוקטור u. ואפשר להראות שהמכפלה הפנימית uᵀv, זה שווה p כפול הנורמה או האורך של הוקטור u. אז זוהי דרך אחת לחשב את המכפלה הפנימית. ואם תחשבו באמת את הגיאומטריה ותחשבו את p ותחשבו את הנורמה של u זה אמור לתת לכם את אותה תוצאה, אותה תשובה כמו הדרך האחרת של חישוב מכפלה פנימית. בסדר. אז אם אתם לוקחים את uᵀv, אז uᵀ זה האיברים u1 ו-u2, שהם מטריצה של אחת על שתיים, כפול v. וזה צריך לתת לכם u1*v1 פלוס u2*v2. וזה משפט באלגברה ליניארית ששתי הנוסחאות נותנות לך את אותה תשובה. ודרך אגב, uᵀv גם שווה vᵀu. אז אם הייתם עושים את אותו התהליך אחורה, במקום להטיל את v על u, אתם יכולים להטיל את u על v. עושים את אותו התהליך, אבל כשהתפקידים של u ו-v מתהפכים. אתם בעצם צריכים למעשה לקבל את אותו מספר מה שלא יהיה המספר הזה. ורק כדי להבהיר מה קורה במשוואה הזו הנורמה של u הוא מספר ממשי וגם p (ההטל) הוא מספר ממשי. זאת אומרת ש-uᵀv הוא הכפל הרגיל של מספרים ממשיים של האורך של ההיטל p כפול הנורמה. עוד פרט אחד אחרון, שהוא זה: אם מסתכלים על הנורמה של p ,p הוא מספר עם סימן, הוא יכול להיות חיובי או שלילי. אני אסביר מה שאני מתכוון בקשר לזה, אם u הוא וקטור שנראה ככה ו-v הוא וקטור שנראה ככה. זאת אומרת, אם הזווית בין u ו-v גדולה מ-90 מעלות. אז אם אני משליך היטל של v על u, מה שאני מקבל הוא היטל שנראה ככה, אז זה האורך P. וגם במקרה זה, עדיין יהיה נכון ש-uᵀv שווה p כפול הנורמה של u. אבל בדוגמה הזו p יהיה שלילי. אז במכפלות פנימיות, אם הזווית בין u ו-v היא פחות מ-90 מעלות, אז p הוא אורך חיובי, האורך של הקו האדום בסימן פלוס, ואם הזווית הזו כאן היא יותר מ-90 מעלות אז p יצא שלילי ואורך הקו הזה, הקטע הקצר הזה כאן, שלילי. אז המכפלה הפנימית בין שני וקטורים יכולה להיות גם שלילית אם הזווית ביניהם גדולה מ-90 מעלות. אז כך פועלת מכפלה פנימית בוקטורים. אנחנו נשתמש במאפיינים האלה של מכפלה פנימית וקטורית כדי לנסות להבין את מטרת האופטימיזציה של מכונת תמך וקטורי. הנה יעד האופטימיזציה עבור מכונת תמך וקטורי שחישבנו מקודם. לצורך השקופית הזו בלבד, אני אעשה פישוט אחד, פעם אחת נעשה את המטרה יותר קלה לניתוח, ומה שנעשה הוא להתעלם מההסטה האנכית או ההטיה. אנחנו פשוט נתעלם מ-θ0 ונגדיר אותו להיות שווה ל-0. וכדי להקל על ההתווייה, אני גם אגדיר את מספר התכונות n להיות שווה 2. אז יש לנו רק 2 תכונות, x1 ו-x2. עכשיו, בואו נסתכל על פונקצית המטרה. מטרת האופטימיזציה של SVM. כאשר יש לנו רק שתי תכונות, כאשר n שווה ל -2. אפשר לכתב את זה כך: חצי θ₁² + θ₂². כי יש לנו רק שני פרמטרים, θ1 ו-θ2. מה שאני הולך לעשות הוא לשכתב את זה קצת. אני הולך לכתוב את זה כחצי של השורש הריבועי של θ₁² פלוס θ₂² בריבוע. והסיבה שאני יכול לעשות את זה היא כי עבור כל מספר w, השורש הריבועי של w בריבוע, פשוט שווה w. אז השורש הריבועי בריבוע צריך לתת לנו את אותו הדבר. ואולי שמתם לב שמה שכתוב כאן בתוך הסוגריים שווה לנורמה או האורך של הוקטור θ ומה שאני מתכוון בזה הוא שאם נכתוב את הוקטור θ ככה, θ₁, θ₂. אז הביטוי הזה שסימנתי בקו תחתון אדום, זה בדיוק האורך, או הנורמה, של הוקטור θ. אם תזכרו את ההגדרה של הנורמה של וקטור שהיתה לנו בשקופית הקודמת. ולמעשה זה שווה לאורך של הוקטור θ, בין אם אתה כותב אותו כ-θ0, θ1, θ2. דהיינו אם θ0 שווה לאפס, כמו שהנחנו כאן. או רק כאורך של θ1, θ2, כי לגבי הקו הזה אנחנו מתעלמים מ-θ0. אז תרשו לי לטפל ב-θ כך, תרשו לי לכתוב פשוט הנורמה של θ כך, θ1, θ2 בלי θ0, אבל המתמטיקה עובדת בכל מקרה, בין אם אנחנו כוללים את θ0 כאן ובין אם לא. זה לא ישנה את שאר הדיון שלנו. אז בסופו של דבר, זה אומר שמטרת האופטימיזציה שלנו היא חצי מהנורמה של θ בריבוע. אז כל מה שעושה מכונת התמך הוקטורי בחישוב מטרת האופטימיזציה הוא למזער את הריבוע של הנורמה או ריבוע האורך של וקטור הפרמטרים θ. מה שאני רוצה לעשות עכשיו זה להסתכל על המונחים האלה, θᵀx, ולהבין יותר טוב מה הם עושים. אז בהינתן וקטור פרמטרים θ ודוגמה x, מה הערך של הביטוי (θᵀx(i? ובשקופית הקודמת, הבנו איך נראה uᵀv עם וקטורים שונים u ו-v. אז אנחנו הולכים לקחת את ההגדרות האלה, כשכאן θ ו-(x(i משחקים את התפקידים של u ו-v. ונראה איך נראית כאן התמונה. אז נניח שאני משרטט. נניח שאני מסתכל רק על דוגמת אימון בודדת. נניח ששרטטתי דוגמה חיובית כאן ונניח שכאן הערך של הדוגמה שלי (x(i, מה שזה באמת אומר הוא שאנחנו מסמנים על הציר האופקי איזשהו ערך x(i)1, ועל הציר האנכי x(i)2. כך אני מתווה את דוגמאות האימונים שלי. ולמרות שאנחנו לא באמת חשבנו על זה כעל וקטור, מה שזה באמת הוא וקטור מראשית הצירים מ-(0,0) אל המיקום של דוגמת האימון הזו. ועכשיו נניח שיש לנו וקטור פרמטרים θ ואני אשרטט גם אותו כוקטור. מה שאני מתכוון זה אם אני מתווה את θ₁ כאן ו-θ₂ כאן, אז מהי המכפלה הפנימית (θᵀx(i? אז בשימוש בשיטה הקודמת שלנו, הדרך שבה אנו מחשבים היא שאנחנו לוקחים את הדוגמה ומטילים אותה על גבי וקטור הפרמטרים θ. ואז אנחנו מסתכלים על אורך הקטע הזה שאני צובע כאן באדום. ואני אסמן את זה ב-p סימן עילי i כדי לציין שזה ההיטל של דוגמת האימון ה-i על וקטור הפרמטרים θ. אז מה שאנחנו מקבלים הוא ש-(θᵀx(i שווה למה שהיה לנו בשקופית הקודמת, זה יהיה שווה p כפול האורך או הנורמה של הוקטור θ. וכמובן זה גם שווה θ1 כפול x1 פלוס θ2 כפול x2. כל אחת מהדרכים האלה היא דרך תקפה באותה מידה לחישוב המכפלה הפנימית בין θ ו-(x(i. בסדר. אז איפה זה משאיר אותנו? מה משמעותו של האילוץ הזה ש-(θᵀx(i חייב להיות גדול או שווה 1 או קטן ממינוס אחד? משמעות הדבר היא שאפשר להחליף את האילוץ באילוץ ש-(p(i כפול x יהיה גדול או שווה אחד. כי (θᵀx(i שווה בדיוק ל-(p(i כפול הנורמה של θ. אז כשכותבים את זה בתוך מטרת האופטימיזציה שלנו, זה מה שאנחנו מקבלים, כשבמקום (θᵀx(i עכשיו יש לנו (p(i כפול הנורמה של θ. ורק כדי להזכיר לכם חישבנו מקודם שאת מטרת האופטימיזציה הזו ניתן גם לכתוב כחצי הנורמה של θ בריבוע. אז עכשיו בואו נחשוב על דוגמת ההדרכה שיש לנו כאן למטה ולעת עתה אנחנו ממשיכים להשתמש בפישוט ש-θ0 שווה 0. בואו נראה איזה גבול החלטה תבחר מכונת התמך הוקטורי. הנה אפשרות אחת, נניח שמכונת התמך הוקטורי תחליט לבחור את גבול ההחלטה הזה. זו לא ממש בחירה טובה כי יש לה שוליים צרים מאוד. גבול ההחלטה הזה מגיע קרוב מאוד לדוגמאות ההכשרה. בוא נראה מדוע מכונת התמך הוקטורי לא תעשה את זה. עבור בחירה כזו של פרמטרים ניתן להראות שוקטור הפרמטרים θ ממוקם למעשה ב-90 מעלות לגבול ההחלטה. אז גבול ההחלטה הירוק תואם לוקטור פרמטרים θ המצביע בכיוון הזה. דרך אגב, הפישוט ש-θ0 שווה 0 כל מה שהוא אומר הוא שגבול ההחלטה חייב לעבור דרך ראשית הצירים, הנקודה (0,0) שם. אז עכשיו, בואו נראה איך זה משליך על מטרת האופטימיזציה. נניח שהדוגמה הזו כאן נניח שזו הדוגמה הראשונה שלנו, (x(1. ... אם נבחן את ההיטל של הדוגמה הזו על וקטור הפרמטרים θ. זה ההיטל. אז הקטע האדום הקטן הזה זה שווה (p(1. וזה די קטן, נכון. עכשיו נניח שהדוגמה הזו כאן, אם היא במקרה (x(2, הדוגמה השנייה שלנו. אז אם אנחנו מסתכלים על ההיטל של הדוגמה הזו על θ, אתם רואים. טוב, אני אצייר את זה בוורוד. הקטע הקצר הזה בוורוד, זה (p(2. זהו ההיטל של הדוגמה השנייה אל הכיוון של וקטור הפרמטרים θ שהולך ככה. אז הקטע הקטן הזה הוא ההיטל והוא די קטן. (p(2 יהיה למעשה מספר שלילי, זאת אומרת ש-(p(2 הוא בכיוון ההפוך. לוקטור הזה יש זווית של יותר מ-90 מעלות עם וקטור הפרמטרים שלנו θ, ולכן ההיטל יהיה קטן מ-0. אז מה שאנו מוצאים הוא שהערכים האלה (p(i הולכים להיות מספרים קטנים למדי. אז אם נבחן את מטרת האופטימיזציה ונראה שעבור דוגמאות חיוביות אנחנו צריכים ש-(p(i כפול הנורמה של θ יהיה גדול או שווה אחת. אבל אם (p(i כאן, לדוגמה אם (p(1 כאן הוא די קטן, זה אומר שאנחנו צריכים שהנורמה של תטה תהיה די גדולה, נכון? אם (p(1 הוא קטן ואנחנו רוצים ש-(p(1 כפול הנורמה של θ להיות גדול או שווה אחת, אז הדרך היחידה שזה יהיה נכון שהמכפלה של שני המספרים תהיה גדולה אם (p(1 הוא קטן, כמו שאמרתי, הנורמה של θ חייבת להיות גדולה. ובאופן דומה עבור דוגמאות שליליות, אנחנו צריכים ש-(p(2 כפול הנורמה של θ תהיה קטנה או שווה 1-. וראינו כבר בדוגמה הזו ש-(p(2 יהיה מספר שלילי קטן למדי, ולכן הדרך היחידה שזה יקרה היא אם הנורמה של θ היא גדולה, אבל כשאנחנו עושים את מטרת האופטימיזציה, אנחנו מנסים למצוא קבוצה של הפרמטרים θ כך שהנורמה של θ תהיה קטנה, ולכן, כאמור, זה לא נראה כיוון טוב עבור וקטור הפרמטרים θ. לעומת זאת, הבה ונסתכל על גבול החלטה שונה. כאן, נניח, ה-SVM בחר כך את גבול ההחלטה. עכשיו זה הולך להיות שונה מאוד. אם זהו גבול ההחלטה, אז הנה הכיוון המתאים לזה של θ. אז, עם גבול החלטה כזה, הקו האנכי הזה, ניתן להראות באלגברה ליניארית שהדרך לקבל את גבול ההחלטה הירוק הזה היא שהוקטור תטה יהיה ב-90 מעלות אליו, ועכשיו אם אתה מסתכל על ההיטל של θ על הוקטור x, נניח הדוגמה שהיתה לי מקודם שקראנו לה x1. אז כשאני מטיל את x על θ, מה שאני מוצא הוא שזהו (p(1. האורך הזה הוא (p(1. הדוגמה השנייה שלנו x2, כשאני עושה את אותו היטל אז מה שאני מוצא הוא שהאורך הזה כאן הוא (p(2 ובאמת הוא יהיה קטן מ-0. ואתם יכולים לראות שעכשיו (p(1 ו-(p(2, האורכים האלה של ההיטלים, הם הרבה יותר גדולים, ולכן למרות שאנחנו עדיין צריכים לאכוף את האילוץ ש-(p(1 כפול הנורמה של θ יהיה גדול או שווה 1, אבל הפעם זה קל כי (p(1 הוא כל כך הרבה יותר גדול עכשיו. ולכן הנורמה יכולה להיות קטנה יותר. אז מה שזה אומר הוא שעל ידי בחירת גבול ההחלטה המוצג בצד ימין ולא זה המוצג בצד שמאל, SVM יכול לגרום לנורמה של הפרמטרים θ להיות הרבה יותר קטנה. אז אם אנחנו יכולים לעשות את הנורמה של θ קטנה יותר ולכן גם לעשות את הנורמה בריבוע של θ קטן, אז זו הסיבה ש-SVM יעדיף לבחור את ההשערה מימין. וכך SVM גורם לסיווג בעל שולים רחבים. בעיקר, אם מסתכלים על הקו הירוק הזה, אם מסתכלים על קו ההשערה הירוקה הזה, אנחנו רוצים שההיטלים של הדוגמאות החיוביות והשליליות שלי על θ יהיו גדולים, והדרך היחידה לעשות את זה היא להקיף את הקו הירוק, בעזרת השוליים הרחבים האלה, יש פער גדול שמפריד בין דוגמאות חיוביות ושליליות, ולמעשה באמת גודל הפער הזה, גודל השוליים האלה הוא בדיוק הערכים של (p(1), p(2), p(3 וכן הלאה. וכך על ידי יצירת שוליים רחבים, בעזרת (p(1, (p(2), p(3 וכן הלאה, SVM יכול לקבל ערך יותר קטן עבור הנורמה של θ וזוהי המטרה, זה מה שהיא מנסה לעשות. ובגלל זה מכונת התמך הוקטורי משיגה סיווג עם שוליים רחבים כי היא מנסה למקסם את הנורמה של (p(i שהוא המרחק מדוגמאות ההכשרה לגבול ההחלטה. לבסוף, עשינו את כל העבודה הזאת באמצעות הפישוט הזה שהפרמטר θ0 חייב להיות שווה ל-0. ההשפעה של זה כפי שכבר הזכרתי בקצרה, היא שאם θ0 שווה ל-0 מה שזה אומר הוא שאנחנו מתעסקים רק בגבולות החלטה שעוברים דרך ראשית הצירים, גבולות החלטה שעוברים דרך ראשית הצירים ככה. אם נרשה ל-θ0 להיות שונה מ-0 מה שזה יעשה הוא להרשות גבולות החלטה שאינם עוברים דרך הראשית, כמו זה שכרגע ציירתי. ואני לא מתכוון לעשות את הגזירה המלאה. מתברר כי אותה הוכחה של השוליים הרחבים עובדת פחות או יותר בדיוק באותו אופן. ויש הכללה של הטענה הזו, שעברנו עליה לפני זמן לא רב, והיא מראה שגם כאשר θ0 איננו 0, מה ש-SVM מנסה לעשות כאשר יש לך מטרת אופטימיזציה ששוב מתאימה למקרה ש-C הוא גדול מאוד. אבל אפשר להראות שכאשר θ0 אינו שווה ל-0 מכונת התמך הוקטורי עדיין מוצאת או עדיין באמת מנסה למצוא את השוליים הרחבים ביותר האפשריים בין דוגמאות חיוביות ושליליות. אז זה מסביר איך מכונת התמך הוקטורי היא מסווג שוליים רחבים. בסרטון הבא נתחיל לדבר על איך לקחת כמה רעיונות כאלה מ-SVM ולהתחיל ליישם אותם כדי לבנות מסווגים לא לינאריים מורכבים.