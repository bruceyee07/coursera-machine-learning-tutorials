1
00:00:00,750 --> 00:00:02,160
לפעמים מדברים על

2
00:00:02,520 --> 00:00:04,380
מכונות תמך וקטורי

3
00:00:04,990 --> 00:00:06,950
כ"מסווגים בעלי שוליים רחבים". בסרטון הזה

4
00:00:07,080 --> 00:00:08,030
אני רוצה להסביר לכם מה

5
00:00:08,410 --> 00:00:09,500
זה אומר, מה שגם

6
00:00:09,780 --> 00:00:10,520
ייתן לנו תמונה

7
00:00:11,030 --> 00:00:12,780
שימושית על איך עשויה להיראות

8
00:00:13,020 --> 00:00:17,460
פונקצית ההשערה של SVM.

9
00:00:18,070 --> 00:00:19,290
הנה פונקציית העלות עבור מכונות תמך וקטורי

10
00:00:21,310 --> 00:00:22,290
שבה כאן בצד שמאל

11
00:00:22,790 --> 00:00:24,300
אני שרטטתי את פונקצית

12
00:00:24,560 --> 00:00:28,100
עלות-1 של z כי השתמשתי בדוגמאות בהם y הוא 1, ובצד ימין התוויתי את

13
00:00:30,080 --> 00:00:31,510
עלות-0 של z ,

14
00:00:31,950 --> 00:00:33,850
כאן z על הציר האופקי.

15
00:00:34,380 --> 00:00:35,520
עכשיו, בואו נחשוב על מה

16
00:00:35,650 --> 00:00:38,380
נדרש כדי לגרום לפונקציות העלות האלה להיות קטנות.

17
00:00:39,660 --> 00:00:40,970
אם יש לך דוגמה חיובית,

18
00:00:41,950 --> 00:00:43,170
אם y שווה ל-1,

19
00:00:43,490 --> 00:00:45,060
אז עלות-1 של z

20
00:00:45,200 --> 00:00:46,750
היא אפס רק כאשר

21
00:00:47,700 --> 00:00:50,070
z גדול או שווה ל 1.

22
00:00:50,180 --> 00:00:51,370
במילים אחרות, אם יש לך

23
00:00:51,510 --> 00:00:52,860
דוגמה חיובית, אנחנו

24
00:00:53,110 --> 00:00:54,550
שואפים ש-θᵀx תהיה

25
00:00:54,870 --> 00:00:55,760
גדולה או שווה 1

26
00:00:56,450 --> 00:00:58,030
ולעומת זאת אם y

27
00:00:58,150 --> 00:00:59,300
שווה לאפס, תראו את

28
00:00:59,510 --> 00:01:00,490
פונקצית עלות-אפס של z,

29
00:01:01,560 --> 00:01:03,000
אז זה רק באזור

30
00:01:03,200 --> 00:01:04,310
כאן שבו z

31
00:01:04,460 --> 00:01:05,810
קטן שווה 1-

32
00:01:06,150 --> 00:01:07,320
שבו העלות היא אפס

33
00:01:07,610 --> 00:01:10,150
כש-z שווה לאפס,

34
00:01:10,640 --> 00:01:12,270
וזה מאפיין מעניין של מכונות

35
00:01:12,560 --> 00:01:13,630
תמך וקטורי, והוא,

36
00:01:13,800 --> 00:01:15,060
שאם יש לך דוגמה חיובית,

37
00:01:15,440 --> 00:01:17,650
דהיינו אם y שווה 1,

38
00:01:18,370 --> 00:01:19,250
אז כל מה שאנחנו באמת צריכים

39
00:01:19,550 --> 00:01:21,950
הוא ש-θᵀx יהיה גדול שווה לאפס.

40
00:01:22,970 --> 00:01:25,270
וזה אומר שאנחנו מסווגים בצורה נכונה,

41
00:01:25,860 --> 00:01:26,950
כי אם θᵀx גדול שווה אפס

42
00:01:27,510 --> 00:01:28,980
אז ההשערה שלנו תחזה אפס.

43
00:01:29,840 --> 00:01:30,710
ומצד שני אם יש לך

44
00:01:31,340 --> 00:01:34,090
דוגמה שלילית, אז באמת כל מה שאנחנו רוצים הוא ש-θᵀx

45
00:01:34,850 --> 00:01:37,290
יהיה קטן מאפס וזה מוודא שחזינו את הדוגמה נכונה.

46
00:01:37,670 --> 00:01:40,230
אבל מכונת תמך וקטורי רוצה קצת יותר מזה.

47
00:01:40,580 --> 00:01:43,360
היא אומרת, אל תחזה את הדוגמה בצורה נכונה רק בקושי.

48
00:01:44,320 --> 00:01:45,990
אני לא רוצה שהביטוי

49
00:01:46,240 --> 00:01:47,580
יהיה רק קצת יותר גדול מאפס. מה

50
00:01:47,890 --> 00:01:48,870
שאני באמת רוצה זה שהביטוי יהיה

51
00:01:49,060 --> 00:01:50,370
די הרבה יותר גדול מאפס,

52
00:01:50,490 --> 00:01:51,430
אולי

53
00:01:51,680 --> 00:01:52,530
גדול או שווה אחת,

54
00:01:52,870 --> 00:01:54,400
ואני גם רוצה שבמקרה השני זה יהיה קטן בהרבה מאפס.

55
00:01:54,800 --> 00:01:55,970
אני רוצה שזה יהיה קטן

56
00:01:56,230 --> 00:01:58,140
או שווה 1-.

57
00:01:58,830 --> 00:02:00,000
זה בונה

58
00:02:00,120 --> 00:02:01,660
גורם בטיחות נוסף או גורם

59
00:02:02,070 --> 00:02:03,630
בטיחות של שוליים לתוך מכונת התמך הוקטורי.

60
00:02:04,030 --> 00:02:05,700
גם רגרסיה לוגיסטית

61
00:02:06,340 --> 00:02:07,620
עושה משהו דומה כמובן,

62
00:02:07,820 --> 00:02:08,900
אבל בואו ונראה

63
00:02:09,110 --> 00:02:10,350
מה קורה או

64
00:02:10,460 --> 00:02:11,290
מה ההשלכות של זה,

65
00:02:11,360 --> 00:02:13,180
בהקשר של מכונות תמך וקטורי.

66
00:02:14,830 --> 00:02:15,740
באופן קונקרטי, מה שאני רוצה לעשות עכשיו הוא

67
00:02:16,010 --> 00:02:17,760
לשקול מקרה

68
00:02:17,900 --> 00:02:19,130
שבו קבענו את

69
00:02:19,460 --> 00:02:21,240
הקבוע C להיות

70
00:02:21,400 --> 00:02:23,340
מספר בעל ערך גדול מאוד, בואו

71
00:02:23,530 --> 00:02:24,700
נתאר לעצמנו שהגדרנו את C

72
00:02:24,820 --> 00:02:28,080
להיות ערך גדול מאוד, אולי מאה אלף, איזה מספר עצום.

73
00:02:29,370 --> 00:02:31,290
בואו נראה מה תעשה מכונת התמך הוקטורי.

74
00:02:31,580 --> 00:02:33,510
מכיוון ש-C הוא מאוד

75
00:02:33,820 --> 00:02:35,340
גדול, אז כאשר מטרת האופטימיזציה

76
00:02:36,350 --> 00:02:38,080
היא מזעור, תהיה לנו

77
00:02:38,300 --> 00:02:39,640
מוטיבציה מאוד גבוהה לבחור

78
00:02:39,950 --> 00:02:41,240
ערך כך

79
00:02:41,380 --> 00:02:43,180
שהביטוי הראשון שווה לאפס.

80
00:02:44,810 --> 00:02:46,250
אז בואו ננסה

81
00:02:46,670 --> 00:02:48,320
להבין את בעיית האופטימיזציה

82
00:02:48,430 --> 00:02:49,820
בהקשר של מה

83
00:02:50,050 --> 00:02:51,520
יידרש כדי לאפס

84
00:02:51,880 --> 00:02:53,060
את המונח הראשון הזה,

85
00:02:53,470 --> 00:02:54,890
כי

86
00:02:55,000 --> 00:02:56,100
כפי שאתם מבינים אולי נציב ב-C

87
00:02:56,250 --> 00:02:59,420
איזה קבוע ענק,

88
00:02:59,590 --> 00:03:00,780
ובתקווה זה ייתן לנו

89
00:03:01,300 --> 00:03:02,920
אינטואיציה נוספת על

90
00:03:03,110 --> 00:03:05,520
איזה סוג של השערות או היפותזות מכונת תמך וקטורי לומדת.

91
00:03:06,440 --> 00:03:07,720
אז ראינו כבר

92
00:03:08,140 --> 00:03:09,260
שבכל פעם שיש לך

93
00:03:09,480 --> 00:03:11,350
דוגמת אימון עם תווית

94
00:03:11,690 --> 00:03:13,850
של y=1, אם אתה

95
00:03:13,950 --> 00:03:15,050
רוצה לעשות את הביטוי הראשון

96
00:03:15,240 --> 00:03:16,280
אפס, מה שאתה צריך

97
00:03:16,450 --> 00:03:17,680
הוא למצוא ערך של θ

98
00:03:17,990 --> 00:03:20,380
כך ש-(θᵀx(i יהיה גדול

99
00:03:20,690 --> 00:03:22,800
או שווה 1.

100
00:03:23,220 --> 00:03:24,250
וגם בכל פעם שיש לנו דוגמה

101
00:03:24,960 --> 00:03:26,910
עם תווית אפס, על מנת

102
00:03:27,240 --> 00:03:28,060
לוודא שהעלות,

103
00:03:29,000 --> 00:03:30,520
עלות-אפס של z, על מנת

104
00:03:30,610 --> 00:03:31,530
לוודא כי העלות הזו

105
00:03:31,790 --> 00:03:33,250
היא אפס אנחנו צריכים ש-(θᵀx(i 

106
00:03:33,810 --> 00:03:36,180
תהיה קטנה

107
00:03:37,900 --> 00:03:38,740
או שווה 1-.

108
00:03:39,510 --> 00:03:40,770
אז, אם אנחנו חושבים

109
00:03:41,050 --> 00:03:43,030
על בעיית האופטימיזציה שלנו

110
00:03:43,360 --> 00:03:45,000
כעל בחירת פרמטרים

111
00:03:45,710 --> 00:03:46,750
שיבטיחו שהביטוי הראשון

112
00:03:47,020 --> 00:03:48,170
הזה שווה לאפס,

113
00:03:49,130 --> 00:03:50,230
אנחנו נשארים

114
00:03:50,330 --> 00:03:51,670
עם בעית האופטימיזציה הבאה.

115
00:03:52,050 --> 00:03:53,720
אנחנו הולכים לאפס את הביטוי הראשון

116
00:03:53,980 --> 00:03:55,360
הזה, כך שנקבל כאן C

117
00:03:55,590 --> 00:03:56,710
כפול אפס, כי אנחנו

118
00:03:56,870 --> 00:03:58,040
נבחר פרמטרים כך שזה יהיה

119
00:03:58,150 --> 00:03:59,710
שווה לאפס, ועוד חצי

120
00:04:00,330 --> 00:04:01,330
הביטוי השני

121
00:04:01,460 --> 00:04:05,440
כי המונח הראשון

122
00:04:05,620 --> 00:04:06,880
הוא C כפול אפס,

123
00:04:07,160 --> 00:04:08,020
אז בואו פשוט נמחק אותו

124
00:04:08,130 --> 00:04:11,210
כי אני יודע שהוא יהיה אפס.

125
00:04:11,380 --> 00:04:12,570
וזה כפוף לאילוץ

126
00:04:13,400 --> 00:04:15,410
ש-(θᵀx(i

127
00:04:16,390 --> 00:04:17,560
גדול או שווה

128
00:04:18,700 --> 00:04:20,930
אחד, אם (y(i

129
00:04:22,180 --> 00:04:24,150
שווה אחד,

130
00:04:24,940 --> 00:04:26,560
וש-(θᵀx(i הוא קטן

131
00:04:26,690 --> 00:04:28,060
או שווה 1-

132
00:04:29,030 --> 00:04:31,680
בכל פעם שיש לך

133
00:04:32,110 --> 00:04:34,460
דוגמה שלילית.

134
00:04:34,540 --> 00:04:35,520
ומתברר שכאשר אתה

135
00:04:35,660 --> 00:04:37,930
פותר את בעיית האופטימיזציה הזו, כאשר אתה

136
00:04:38,070 --> 00:04:39,440
ממזער את זה כפונקציה של הפרמטרים θ

137
00:04:40,710 --> 00:04:42,090
אתה מקבל גבול החלטה

138
00:04:42,590 --> 00:04:44,870
מאוד מעניין. באופן קונקרטי, אם אתה

139
00:04:45,010 --> 00:04:46,470
מסתכל על נתונים

140
00:04:46,750 --> 00:04:49,660
כמו אלה עם דוגמאות חיוביות ושליליות, הנתונים האלה

141
00:04:50,920 --> 00:04:52,430
ניתנים להפריד ליניארית,

142
00:04:52,710 --> 00:04:54,960
זאת אומרת שקיים קו ישר,

143
00:04:55,530 --> 00:04:56,830
יש הרבה קווים ישרים שונים,

144
00:04:56,920 --> 00:04:57,810
שיכולים להפריד בין הדוגמאות החיוביות

145
00:04:58,720 --> 00:05:01,060
והשליליות באופן מושלם.

146
00:05:01,560 --> 00:05:02,710
לדוגמה, הנה גבול החלטה אחד

147
00:05:04,270 --> 00:05:05,430
המפריד בין דוגמאות חיוביות

148
00:05:05,570 --> 00:05:06,840
ושליליות, אבל איכשהו הוא

149
00:05:07,030 --> 00:05:07,810
לא נראה כמו גבול

150
00:05:07,900 --> 00:05:09,680
טבעי מאוד, נכון?

151
00:05:09,810 --> 00:05:11,050
או על ידי ציור של גבול אפילו יותר גרוע,

152
00:05:11,230 --> 00:05:13,540
אתם רואים שזה עוד גבול החלטה

153
00:05:13,710 --> 00:05:14,830
שמפריד בין דוגמאות חיוביות ושליליות

154
00:05:14,900 --> 00:05:15,960
אבל רק בקושי.

155
00:05:16,120 --> 00:05:18,530
אבל אף אחד מהם לא נראה כמו בחירה טובה במיוחד.

156
00:05:20,420 --> 00:05:22,880
מכונות תמך וקטורי בוחרות במקום זה

157
00:05:23,140 --> 00:05:26,450
את גבול ההחלטה הזה, שציירתי בשחור.

158
00:05:29,010 --> 00:05:30,030
וזה נראה כמו גבול החלטה הרבה יותר טוב

159
00:05:30,760 --> 00:05:32,310
מאשר כל אחד

160
00:05:32,420 --> 00:05:34,450
מאלה שציירתי בורוד או בירוק.

161
00:05:34,750 --> 00:05:35,790
נראה שהקו השחור מפריד

162
00:05:36,050 --> 00:05:37,840
יותר טוב, עושה

163
00:05:38,610 --> 00:05:39,710
עבודה טובה יותר של הפרדת בין הדוגמאות החיוביות והשליליות.

164
00:05:39,800 --> 00:05:42,830
ומה שזה אומר מבחינה מתמטית,

165
00:05:43,530 --> 00:05:45,680
זה שלגבול ההחלטה השחור יש מרחק גדול יותר.

166
00:05:49,160 --> 00:05:50,580
המרחק הזה נקרא שוליים. כאשר אני

167
00:05:50,760 --> 00:05:51,790
מצייר את שני הקווים

168
00:05:52,380 --> 00:05:54,320
הכחולים הנוספים האלה, אנחנו רואים

169
00:05:54,540 --> 00:05:56,010
שלגבול ההחלטה השחור יש

170
00:05:56,240 --> 00:05:59,990
מרחק מינימלי יותר גדול מכל אחת מדוגמאות האימון,

171
00:06:00,120 --> 00:06:01,350
לעומת הקווים בורוד וירוק

172
00:06:01,580 --> 00:06:02,600
שמתקרבים מאוד לדוגמאות האימון.

173
00:06:04,640 --> 00:06:06,100
ונראה שהם עושים עבודה פחות טובה בהפרדה

174
00:06:06,500 --> 00:06:08,910
בין הקבוצה החיובית והקבוצה השלילית מאשר הקו השחור.

175
00:06:09,850 --> 00:06:11,500
וכך

176
00:06:11,800 --> 00:06:13,600
המרחק הזה נקרא

177
00:06:13,960 --> 00:06:16,500
השוליים של

178
00:06:16,600 --> 00:06:21,300
מכונת התמך הוקטורי, וזה

179
00:06:21,500 --> 00:06:22,480
נותן ל-SVM חוסן

180
00:06:22,940 --> 00:06:24,010
מסוים, משום שהוא מנסה

181
00:06:24,360 --> 00:06:25,530
להפריד את הנתונים

182
00:06:25,700 --> 00:06:27,440
עם שוליים רחבים ככל האפשר.

183
00:06:29,210 --> 00:06:30,250
אז מכונות תמך וקטורי

184
00:06:30,380 --> 00:06:31,650
נקראות לעתים קרובות

185
00:06:31,830 --> 00:06:33,930
מסווגים בעלי שוליים רחבים,

186
00:06:34,170 --> 00:06:36,180
וזוהי למעשה תוצאה של

187
00:06:36,430 --> 00:06:39,370
בעיית האופטימיזציה שניסחנו בשקופית הקודמת.

188
00:06:40,140 --> 00:06:40,950
אני יודע שאולי אתם

189
00:06:41,100 --> 00:06:42,250
תוהים איך זה

190
00:06:42,400 --> 00:06:43,900
שבעיית האופטימיזציה שכתבנו

191
00:06:44,070 --> 00:06:45,080
בשקופית הקודמת, איך

192
00:06:45,280 --> 00:06:47,270
זה מוביל למסווג עם שוליים רחבים.

193
00:06:48,350 --> 00:06:49,700
אני יודע שעדיין לא הסברתי את זה.

194
00:06:50,520 --> 00:06:51,570
בסרטון הבא

195
00:06:51,810 --> 00:06:53,340
אני הולך לשרטט

196
00:06:53,500 --> 00:06:55,180
קצת אינטואיציה על למה

197
00:06:55,430 --> 00:06:57,080
בעיית האופטימיזציה הזו נותנת לנו

198
00:06:57,570 --> 00:06:59,630
מסווג עם שוליים רחבים. אבל

199
00:06:59,790 --> 00:07:00,860
זו בכל אופן תכונה שימושית

200
00:07:00,970 --> 00:07:01,780
לזכור

201
00:07:01,920 --> 00:07:03,150
כשננסה להבין מהו

202
00:07:03,290 --> 00:07:05,600
סוג הההשערה שתיבחר על ידי SVM.

203
00:07:06,140 --> 00:07:07,200
דהיינו, השערה שמנסה להפריד

204
00:07:07,270 --> 00:07:10,310
בין דוגמאות חיוביות ושליליות עם שוליים רחבים ככל האפשר.

205
00:07:12,890 --> 00:07:13,950
אני רוצה לומר דבר אחד אחרון

206
00:07:14,180 --> 00:07:15,930
על מסווגים עם שוליים רחבים

207
00:07:16,070 --> 00:07:17,900
באינטואיציה הזאת.

208
00:07:18,030 --> 00:07:19,340
כתבנו את הגדרת הסיווג שגרמה לשוליים הרחבים

209
00:07:20,010 --> 00:07:21,040
במקרה של

210
00:07:21,420 --> 00:07:23,640
כאשר C, מקדם ההסדרה,

211
00:07:24,160 --> 00:07:25,190
היה גדול מאוד, אני חושב

212
00:07:25,390 --> 00:07:27,750
שהצבנו בו מאה אלף או משהו .

213
00:07:28,310 --> 00:07:29,760
אז בהינתן מערך נתונים

214
00:07:30,110 --> 00:07:31,630
כזה, אולי נבחר

215
00:07:32,110 --> 00:07:34,000
סף החלטה כזה

216
00:07:34,140 --> 00:07:36,210
שמפריד בין דוגמאות חיוביות ושליליות עם שוליים רחבים.

217
00:07:37,370 --> 00:07:39,020
עכשיו, SVM הוא למעשה דבר קצת

218
00:07:39,370 --> 00:07:41,120
יותר מתוחכם מאשר אתם עשויים לחשוב

219
00:07:41,440 --> 00:07:42,920
למראה השוליים הרחבים.

220
00:07:43,630 --> 00:07:45,130
ובפרט, אם כל מה שאתם עושים

221
00:07:45,310 --> 00:07:46,490
הוא להשתמש בסיווג

222
00:07:46,680 --> 00:07:48,850
עם שוליים רחבים, אז

223
00:07:49,020 --> 00:07:50,270
אלגוריתם הלמידה שלכם יכול להיות רגיש

224
00:07:50,920 --> 00:07:52,260
למצבי קיצון. בואו נוסיף

225
00:07:52,450 --> 00:07:53,990
דוגמה חיובית אחת נוספת

226
00:07:54,520 --> 00:07:56,540
כמו שמוצג כאן על המסך.

227
00:07:57,230 --> 00:07:58,830
לו הוספנו את הדוגמה האחת הזו,

228
00:07:58,950 --> 00:08:00,060
אז זה נראה כאילו כדי להפריד

229
00:08:00,300 --> 00:08:01,410
נתונים עם שוליים רחבים,

230
00:08:02,680 --> 00:08:04,300
אולי אני אקבל מין

231
00:08:05,270 --> 00:08:07,260
גבול החלטה כזה, נכון?

232
00:08:07,540 --> 00:08:09,130
הקו הורוד הזה,

233
00:08:09,180 --> 00:08:10,210
וזה ממש לא ברור

234
00:08:10,440 --> 00:08:11,950
שכשיש לנו תוצאת קיצון יחידה

235
00:08:12,180 --> 00:08:13,560
המבוססת על דוגמה אחת,

236
00:08:13,790 --> 00:08:14,720
זה באמת לא ברור שזה

237
00:08:14,890 --> 00:08:16,460
באמת רעיון טוב לשנות

238
00:08:17,060 --> 00:08:17,980
את גבול ההחלטה שלנו מהקו השחור

239
00:08:18,290 --> 00:08:19,960
לקו הורוד.

240
00:08:20,980 --> 00:08:23,430
אם C, אם

241
00:08:23,640 --> 00:08:25,740
פרמטר ההסדרה C הוא

242
00:08:25,970 --> 00:08:27,110
גדול מאוד, אז זה

243
00:08:27,300 --> 00:08:28,130
למעשה מה ש-SVM יעשה, הוא

244
00:08:28,360 --> 00:08:29,820
ישנה את גבול ההחלטה

245
00:08:30,270 --> 00:08:31,530
מהשחור אל

246
00:08:31,650 --> 00:08:33,650
הורוד, אבל אם

247
00:08:33,810 --> 00:08:35,390
C הוא קטן,

248
00:08:35,550 --> 00:08:36,720
אם אתה משתמש ב-C

249
00:08:37,320 --> 00:08:39,090
שאיננו גדול מדי אז

250
00:08:39,260 --> 00:08:40,400
עדיין תקבל את

251
00:08:40,610 --> 00:08:44,500
גבול ההחלטה השחור הזה.

252
00:08:44,830 --> 00:08:46,880
וכמובן אם הנתונים לא ניתנים להפרדה ליניארית, אם היו לך כמה דוגמאות

253
00:08:47,250 --> 00:08:48,790
חיוביות כאן, או אם

254
00:08:49,170 --> 00:08:50,440
היו לך כמה דוגמאות שליליות

255
00:08:50,980 --> 00:08:52,300
כאן, אז עדיין SVM

256
00:08:52,570 --> 00:08:53,830
יעשה את הדבר הנכון.

257
00:08:54,260 --> 00:08:55,710
אז התמונה הזאת של

258
00:08:56,060 --> 00:08:57,770
סיווג שוליים רחבים היא

259
00:08:58,090 --> 00:08:59,410
באמת

260
00:08:59,530 --> 00:09:01,720
תמונה שנותנת אינטואיציה טובה

261
00:09:01,970 --> 00:09:03,440
רק אם

262
00:09:03,560 --> 00:09:05,050
פרמטר ההסדרה C הוא

263
00:09:05,190 --> 00:09:07,170
גדול מאוד, ורק כדי

264
00:09:07,420 --> 00:09:08,810
להזכיר לכם כאן C משחק

265
00:09:09,650 --> 00:09:11,300
תפקיד דומה

266
00:09:11,850 --> 00:09:13,600
לאחת חלקי λ, כש-λ

267
00:09:13,930 --> 00:09:15,950
הוא פרמטר ההסדרה

268
00:09:16,110 --> 00:09:17,970
שהיה לנו בעבר. אז רק

269
00:09:18,080 --> 00:09:18,880
כאשר אחת חלקי λ

270
00:09:19,080 --> 00:09:21,060
הוא גדול מאוד או באופן שווה ערך

271
00:09:21,280 --> 00:09:23,110
אם λ הוא קטן מאוד,

272
00:09:23,560 --> 00:09:24,640
שאנחנו מקבלים דברים כמו

273
00:09:24,850 --> 00:09:27,600
גבול ההחלטה הורוד הזה, אבל

274
00:09:28,870 --> 00:09:29,560
בפועל בעת הפעלת מכונות תמך וקטורי,

275
00:09:30,190 --> 00:09:31,620
כאשר C הוא לא ממש

276
00:09:31,910 --> 00:09:33,180
מאוד גדול,

277
00:09:34,840 --> 00:09:36,390
המכונה יכולה לעשות עבודה טובה יותר

278
00:09:36,980 --> 00:09:38,590
ולהתעלם מכמה מצבי קיצון כמו כאן.

279
00:09:39,150 --> 00:09:40,320
וגם לתפקד מצוין ולעשות דברים סבירים

280
00:09:40,620 --> 00:09:44,400
גם אם הנתונים שלך לא ניתנים להפרדה ליניארית.

281
00:09:44,690 --> 00:09:46,810
כשנדבר על הטיה ושונות בהקשר של מכונות תמך וקטורי,

282
00:09:46,980 --> 00:09:47,990
דבר שנעשה

283
00:09:48,170 --> 00:09:50,170
קצת יותר מאוחר, אני מקווה

284
00:09:50,410 --> 00:09:51,990
שכל נושא יחסי הגומלין המעורבים בפרמטר ההסדרה

285
00:09:52,410 --> 00:09:53,710
יתברר יותר אז.

286
00:09:53,830 --> 00:09:55,280
אז אני מקווה

287
00:09:55,580 --> 00:09:57,290
שזה נתן לכם קצת אינטואיציה על

288
00:09:57,600 --> 00:09:59,680
איך מכונות תמך וקטורי מתפקדות

289
00:09:59,850 --> 00:10:01,810
כמסווגים בעלי שוליים רחבים

290
00:10:01,950 --> 00:10:03,040
שמנסים להפריד את הנתונים

291
00:10:03,610 --> 00:10:05,210
עם שוליים רחבים, מבחינה טכנית

292
00:10:06,140 --> 00:10:07,160
התמונה הזו נכונה

293
00:10:07,460 --> 00:10:08,710
רק כאשר הפרמטר C הוא גדול מאוד,

294
00:10:10,230 --> 00:10:11,720
שזו דרך שימושית לחשוב על מכונות תמך וקטורי.

295
00:10:13,120 --> 00:10:14,450
צעד אחד היה חסר

296
00:10:14,560 --> 00:10:15,990
בסרטון הזה, והוא למה

297
00:10:16,110 --> 00:10:17,670
בעיית האופטימיזציה שכתבנו

298
00:10:17,770 --> 00:10:18,770
בשקפים האלה,

299
00:10:19,040 --> 00:10:19,930
איך היא באמת

300
00:10:20,740 --> 00:10:22,490
מובילה למסווג שוליים רחבים.

301
00:10:22,600 --> 00:10:23,520
לא עשיתי את זה בסרטון הזה,

302
00:10:23,930 --> 00:10:25,830
בווידאו הבא אני

303
00:10:25,870 --> 00:10:26,940
אשרטט קצת יותר

304
00:10:27,120 --> 00:10:28,370
את המתמטיקה שמאחוריה

305
00:10:28,750 --> 00:10:29,750
כדי להסביר

306
00:10:29,850 --> 00:10:31,660
את ההיגיון של האופן

307
00:10:31,930 --> 00:10:33,410
שבו בעיית האופטימיזציה שכתבנו

308
00:10:33,840 --> 00:10:34,990
גורמת לסווג שוליים רחבים.