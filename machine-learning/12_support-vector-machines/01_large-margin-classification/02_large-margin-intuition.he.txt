לפעמים מדברים על מכונות תמך וקטורי כ"מסווגים בעלי שוליים רחבים". בסרטון הזה אני רוצה להסביר לכם מה זה אומר, מה שגם ייתן לנו תמונה שימושית על איך עשויה להיראות פונקצית ההשערה של SVM. הנה פונקציית העלות עבור מכונות תמך וקטורי שבה כאן בצד שמאל אני שרטטתי את פונקצית עלות-1 של z כי השתמשתי בדוגמאות בהם y הוא 1, ובצד ימין התוויתי את עלות-0 של z , כאן z על הציר האופקי. עכשיו, בואו נחשוב על מה נדרש כדי לגרום לפונקציות העלות האלה להיות קטנות. אם יש לך דוגמה חיובית, אם y שווה ל-1, אז עלות-1 של z היא אפס רק כאשר z גדול או שווה ל 1. במילים אחרות, אם יש לך דוגמה חיובית, אנחנו שואפים ש-θᵀx תהיה גדולה או שווה 1 ולעומת זאת אם y שווה לאפס, תראו את פונקצית עלות-אפס של z, אז זה רק באזור כאן שבו z קטן שווה 1- שבו העלות היא אפס כש-z שווה לאפס, וזה מאפיין מעניין של מכונות תמך וקטורי, והוא, שאם יש לך דוגמה חיובית, דהיינו אם y שווה 1, אז כל מה שאנחנו באמת צריכים הוא ש-θᵀx יהיה גדול שווה לאפס. וזה אומר שאנחנו מסווגים בצורה נכונה, כי אם θᵀx גדול שווה אפס אז ההשערה שלנו תחזה אפס. ומצד שני אם יש לך דוגמה שלילית, אז באמת כל מה שאנחנו רוצים הוא ש-θᵀx יהיה קטן מאפס וזה מוודא שחזינו את הדוגמה נכונה. אבל מכונת תמך וקטורי רוצה קצת יותר מזה. היא אומרת, אל תחזה את הדוגמה בצורה נכונה רק בקושי. אני לא רוצה שהביטוי יהיה רק קצת יותר גדול מאפס. מה שאני באמת רוצה זה שהביטוי יהיה די הרבה יותר גדול מאפס, אולי גדול או שווה אחת, ואני גם רוצה שבמקרה השני זה יהיה קטן בהרבה מאפס. אני רוצה שזה יהיה קטן או שווה 1-. זה בונה גורם בטיחות נוסף או גורם בטיחות של שוליים לתוך מכונת התמך הוקטורי. גם רגרסיה לוגיסטית עושה משהו דומה כמובן, אבל בואו ונראה מה קורה או מה ההשלכות של זה, בהקשר של מכונות תמך וקטורי. באופן קונקרטי, מה שאני רוצה לעשות עכשיו הוא לשקול מקרה שבו קבענו את הקבוע C להיות מספר בעל ערך גדול מאוד, בואו נתאר לעצמנו שהגדרנו את C להיות ערך גדול מאוד, אולי מאה אלף, איזה מספר עצום. בואו נראה מה תעשה מכונת התמך הוקטורי. מכיוון ש-C הוא מאוד גדול, אז כאשר מטרת האופטימיזציה היא מזעור, תהיה לנו מוטיבציה מאוד גבוהה לבחור ערך כך שהביטוי הראשון שווה לאפס. אז בואו ננסה להבין את בעיית האופטימיזציה בהקשר של מה יידרש כדי לאפס את המונח הראשון הזה, כי כפי שאתם מבינים אולי נציב ב-C איזה קבוע ענק, ובתקווה זה ייתן לנו אינטואיציה נוספת על איזה סוג של השערות או היפותזות מכונת תמך וקטורי לומדת. אז ראינו כבר שבכל פעם שיש לך דוגמת אימון עם תווית של y=1, אם אתה רוצה לעשות את הביטוי הראשון אפס, מה שאתה צריך הוא למצוא ערך של θ כך ש-(θᵀx(i יהיה גדול או שווה 1. וגם בכל פעם שיש לנו דוגמה עם תווית אפס, על מנת לוודא שהעלות, עלות-אפס של z, על מנת לוודא כי העלות הזו היא אפס אנחנו צריכים ש-(θᵀx(i תהיה קטנה או שווה 1-. אז, אם אנחנו חושבים על בעיית האופטימיזציה שלנו כעל בחירת פרמטרים שיבטיחו שהביטוי הראשון הזה שווה לאפס, אנחנו נשארים עם בעית האופטימיזציה הבאה. אנחנו הולכים לאפס את הביטוי הראשון הזה, כך שנקבל כאן C כפול אפס, כי אנחנו נבחר פרמטרים כך שזה יהיה שווה לאפס, ועוד חצי הביטוי השני כי המונח הראשון הוא C כפול אפס, אז בואו פשוט נמחק אותו כי אני יודע שהוא יהיה אפס. וזה כפוף לאילוץ ש-(θᵀx(i גדול או שווה אחד, אם (y(i שווה אחד, וש-(θᵀx(i הוא קטן או שווה 1- בכל פעם שיש לך דוגמה שלילית. ומתברר שכאשר אתה פותר את בעיית האופטימיזציה הזו, כאשר אתה ממזער את זה כפונקציה של הפרמטרים θ אתה מקבל גבול החלטה מאוד מעניין. באופן קונקרטי, אם אתה מסתכל על נתונים כמו אלה עם דוגמאות חיוביות ושליליות, הנתונים האלה ניתנים להפריד ליניארית, זאת אומרת שקיים קו ישר, יש הרבה קווים ישרים שונים, שיכולים להפריד בין הדוגמאות החיוביות והשליליות באופן מושלם. לדוגמה, הנה גבול החלטה אחד המפריד בין דוגמאות חיוביות ושליליות, אבל איכשהו הוא לא נראה כמו גבול טבעי מאוד, נכון? או על ידי ציור של גבול אפילו יותר גרוע, אתם רואים שזה עוד גבול החלטה שמפריד בין דוגמאות חיוביות ושליליות אבל רק בקושי. אבל אף אחד מהם לא נראה כמו בחירה טובה במיוחד. מכונות תמך וקטורי בוחרות במקום זה את גבול ההחלטה הזה, שציירתי בשחור. וזה נראה כמו גבול החלטה הרבה יותר טוב מאשר כל אחד מאלה שציירתי בורוד או בירוק. נראה שהקו השחור מפריד יותר טוב, עושה עבודה טובה יותר של הפרדת בין הדוגמאות החיוביות והשליליות. ומה שזה אומר מבחינה מתמטית, זה שלגבול ההחלטה השחור יש מרחק גדול יותר. המרחק הזה נקרא שוליים. כאשר אני מצייר את שני הקווים הכחולים הנוספים האלה, אנחנו רואים שלגבול ההחלטה השחור יש מרחק מינימלי יותר גדול מכל אחת מדוגמאות האימון, לעומת הקווים בורוד וירוק שמתקרבים מאוד לדוגמאות האימון. ונראה שהם עושים עבודה פחות טובה בהפרדה בין הקבוצה החיובית והקבוצה השלילית מאשר הקו השחור. וכך המרחק הזה נקרא השוליים של מכונת התמך הוקטורי, וזה נותן ל-SVM חוסן מסוים, משום שהוא מנסה להפריד את הנתונים עם שוליים רחבים ככל האפשר. אז מכונות תמך וקטורי נקראות לעתים קרובות מסווגים בעלי שוליים רחבים, וזוהי למעשה תוצאה של בעיית האופטימיזציה שניסחנו בשקופית הקודמת. אני יודע שאולי אתם תוהים איך זה שבעיית האופטימיזציה שכתבנו בשקופית הקודמת, איך זה מוביל למסווג עם שוליים רחבים. אני יודע שעדיין לא הסברתי את זה. בסרטון הבא אני הולך לשרטט קצת אינטואיציה על למה בעיית האופטימיזציה הזו נותנת לנו מסווג עם שוליים רחבים. אבל זו בכל אופן תכונה שימושית לזכור כשננסה להבין מהו סוג הההשערה שתיבחר על ידי SVM. דהיינו, השערה שמנסה להפריד בין דוגמאות חיוביות ושליליות עם שוליים רחבים ככל האפשר. אני רוצה לומר דבר אחד אחרון על מסווגים עם שוליים רחבים באינטואיציה הזאת. כתבנו את הגדרת הסיווג שגרמה לשוליים הרחבים במקרה של כאשר C, מקדם ההסדרה, היה גדול מאוד, אני חושב שהצבנו בו מאה אלף או משהו . אז בהינתן מערך נתונים כזה, אולי נבחר סף החלטה כזה שמפריד בין דוגמאות חיוביות ושליליות עם שוליים רחבים. עכשיו, SVM הוא למעשה דבר קצת יותר מתוחכם מאשר אתם עשויים לחשוב למראה השוליים הרחבים. ובפרט, אם כל מה שאתם עושים הוא להשתמש בסיווג עם שוליים רחבים, אז אלגוריתם הלמידה שלכם יכול להיות רגיש למצבי קיצון. בואו נוסיף דוגמה חיובית אחת נוספת כמו שמוצג כאן על המסך. לו הוספנו את הדוגמה האחת הזו, אז זה נראה כאילו כדי להפריד נתונים עם שוליים רחבים, אולי אני אקבל מין גבול החלטה כזה, נכון? הקו הורוד הזה, וזה ממש לא ברור שכשיש לנו תוצאת קיצון יחידה המבוססת על דוגמה אחת, זה באמת לא ברור שזה באמת רעיון טוב לשנות את גבול ההחלטה שלנו מהקו השחור לקו הורוד. אם C, אם פרמטר ההסדרה C הוא גדול מאוד, אז זה למעשה מה ש-SVM יעשה, הוא ישנה את גבול ההחלטה מהשחור אל הורוד, אבל אם C הוא קטן, אם אתה משתמש ב-C שאיננו גדול מדי אז עדיין תקבל את גבול ההחלטה השחור הזה. וכמובן אם הנתונים לא ניתנים להפרדה ליניארית, אם היו לך כמה דוגמאות חיוביות כאן, או אם היו לך כמה דוגמאות שליליות כאן, אז עדיין SVM יעשה את הדבר הנכון. אז התמונה הזאת של סיווג שוליים רחבים היא באמת תמונה שנותנת אינטואיציה טובה רק אם פרמטר ההסדרה C הוא גדול מאוד, ורק כדי להזכיר לכם כאן C משחק תפקיד דומה לאחת חלקי λ, כש-λ הוא פרמטר ההסדרה שהיה לנו בעבר. אז רק כאשר אחת חלקי λ הוא גדול מאוד או באופן שווה ערך אם λ הוא קטן מאוד, שאנחנו מקבלים דברים כמו גבול ההחלטה הורוד הזה, אבל בפועל בעת הפעלת מכונות תמך וקטורי, כאשר C הוא לא ממש מאוד גדול, המכונה יכולה לעשות עבודה טובה יותר ולהתעלם מכמה מצבי קיצון כמו כאן. וגם לתפקד מצוין ולעשות דברים סבירים גם אם הנתונים שלך לא ניתנים להפרדה ליניארית. כשנדבר על הטיה ושונות בהקשר של מכונות תמך וקטורי, דבר שנעשה קצת יותר מאוחר, אני מקווה שכל נושא יחסי הגומלין המעורבים בפרמטר ההסדרה יתברר יותר אז. אז אני מקווה שזה נתן לכם קצת אינטואיציה על איך מכונות תמך וקטורי מתפקדות כמסווגים בעלי שוליים רחבים שמנסים להפריד את הנתונים עם שוליים רחבים, מבחינה טכנית התמונה הזו נכונה רק כאשר הפרמטר C הוא גדול מאוד, שזו דרך שימושית לחשוב על מכונות תמך וקטורי. צעד אחד היה חסר בסרטון הזה, והוא למה בעיית האופטימיזציה שכתבנו בשקפים האלה, איך היא באמת מובילה למסווג שוליים רחבים. לא עשיתי את זה בסרטון הזה, בווידאו הבא אני אשרטט קצת יותר את המתמטיקה שמאחוריה כדי להסביר את ההיגיון של האופן שבו בעיית האופטימיזציה שכתבנו גורמת לסווג שוליים רחבים.