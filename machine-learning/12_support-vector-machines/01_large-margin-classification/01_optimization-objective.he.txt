בשלב הזה, כבר ראיתם מגוון של אלגוריתמי למידה שונים. בלמידה מבוקרת, הביצועים של אלגוריתמי למידה מבוקרים רבים יהיו דומים למדי, וזה פחות חשוב אם תשתמש באלגוריתם למידה א' או באלגוריתם הלמידה ב', מה שחשוב יותר יהיה הרבה פעמים כמות הנתונים שעליהם אתה מאמן את האלגוריתם, כמו גם הכשרון שלך בבניית האלגוריתמים האלה. דברים כמו הבחירה של התכונות שאתה מעצב לתת לאלגוריתמי למידה, וכיצד אתה בוחר את פרמטר ההסדרה, ודברים מהסוג הזה. אבל יש עוד אלגוריתם חזק מאוד שנמצא בשימוש נרחב מאוד הן בתעשייה והן באקדמיה, והוא נקרא מכונת תמך וקטורי או מכונת וקטורים תומכים או SVM. ובהשוואה לרגרסיה לוגיסטית ולרשתות עצביות, ה-Support Vector Machine, או SVM נותן לפעמים דרך נקייה ולפעמים יותר חזקה לאימון של פונקציות לא לינאריות מורכבות. אז בואו נקדיש את הסרטונים הבאים כדי לדבר על זה. בהמשך הקורס, אעשה סקר מהיר של מגוון אלגוריתמים שונים של למידה בפיקוח, בזמן שאתאר אותם בקצרה. אבל מכונת תמך וקטורי, בהתחשב בפופולריות שלה וכמה היא חזקה, זה יהיה אחרון האלגוריתמים בפיקוח בו אבלה כמות משמעותית של זמן בקורס הזה.
כמו בפיתוח שלנו של אלגוריתמי למידה אחרים, אנחנו נתחיל לדבר על מטרת האופטימיזציה. אז בואו נתחיל עם האלגוריתם הזה. כדי לתאר את מכונת התמך הוקטורי, אני בעצם הולך להתחיל עם רגרסיה לוגיסטית, ולהראות איך אנחנו יכולים לשנות את זה קצת, ולקבל את מה שהוא למעשה מכונת תמך וקטורי. אז ברגרסיה הלוגיסטית יש לנו את הצורה המוכרת של ההשערה ואת פונקציית ההפעלה או הסיגמואיד שמוצגת כאן מימין. וכדי להסביר חלק מהמתמטיקה, אני אשתמש ב- z כדי לציין θ משוחלפת כפול x. עכשיו בואו נחשוב על מה שאנחנו רוצים שהרגרסיה הלוגיסטית תעשה. אם יש לנו דוגמה עם y=1 וכשאני אומר דוגמה אני מתכוון או בסדרת האימון או בסדרת הבדיקה או בסדרת האימות הצולב, כאשר y=1 אז אנחנו מקווים כי (h(x יהיה קרוב לאחד. נכון, אנחנו מקווים שהפונקציה תסווג נכון את הדוגמה. ומשמעותו של "(h(x קרוב לאחד" היא θ משוחלפת כפול x חייב להיות גדול בהרבה מ-0. הסימן גדול-מ כפול (<<) משמעותו גדול בהרבה, או הרבה יותר גדול מ-0. וזה בגלל ש-z, שהוא θ משוחלפת כפול x, כאשר z הוא הרבה יותר גדול מ-0 והוא נמצא רחוק ימינה בגרף הזה, רק אז הפלט של הרגרסיה הלוגיסטית מתקרב לאחת. לעומת זאת, אם יש לנו דוגמה שבה y=0, אז מה שאנחנו מקווים הוא שההשערה תפיק ערך קרוב לאפס. וזה מתאים למצב שבו θ משוחלפת כפול x הוא קטן בהרבה מאפס שזה מתאים להשערה שמוציאה כפלט ערך קרוב לאפס. אם אתה מסתכל על פונקציית העלות של רגרסיה לוגיסטית, מה שתמצא הוא שכל דוגמה (x, y) תורמת מונח כזה לפונקציית העלות הכוללת, נכון? אז עבור פונקציית העלות הכוללת, יהיה לנו סכום על כל דוגמאות האימון והביטוי 1 חלקי m, הביטוי הזה כאן, זה המונח שאותו תורמת דוגמת אימון אחת לפונקצית המטרה הכוללת של הרגרסיה הלוגיסטית. עכשיו אם אני לוקח את ההגדרה של פונקציית ההשערה h ומציב אותה כאן, אז מה שאני מקבל הוא שכל דוגמת אימון תורמת את המונח הזה, אנחנו מתעלמים מאחת חלקי m אבל זה תורם את המונח הזה לעלות הכוללת שלי עבור רגרסיה לוגיסטית. עכשיו בואו נחשוב על שני מקרים, כאשר y=1 וכאשר y=0. במקרה הראשון, נניח ש-y=1. במקרה זה, רק המונח הראשון בנוסחה הזו מעניין, כי 1 מינוס y שווה לאפס כש-y=1. אז כאשר y=1, כאשר בדוגמה שלנו, (x,y), כאשר y=1 מה שאנחנו מקבלים זה המונח הזה. מינוס לוג של אחת חלקי אחת פלוס e בחזקת מינוס z ואני כמו שהגדרנו בשקופית הקודמת משתמש כאן ב-z כדי לציין θ משוחלפת כפול x ושימו לב כאן למינוס אחת שיש לנו אם y=1, אני פשוט מפשט קצת את הביטוי שכתוב כאן. ואם אנחנו משרטטים את הפונקציה הזו כפונקציה של z, מה שנמצא הוא שנקבל את העקומה הזו שמופיעה כאן בצד שמאל למטה של השקופית. ואנחנו גם רואים שכאשר z הוא גדול, כלומר, כאשר θ משוחלפת כפול x הוא גדול, זה מתאים לערך של z שנותן לנו ערך קטן יחסית, תרומה מאוד קטנה מאוד לפונקציית המחיר. וזה מסביר למה כאשר רגרסיה לוגיסטית רואה דוגמה חיובית, שבה y=1, היא מנסה להגדיר את θ משוחלפת כפול x לערך גדול מאוד כי זה מתאים לכך שהמונח הזה בפונקצית המחיר יהיה קטן. עכשיו, כדי לבנות את מכונת התמך הוקטורי, הנה מה שאנחנו נעשה. אנחנו ניקח את פונקצית המחיר, מינוס לוג של 1 חלקי אחת פלוס e בחזקת מינוס z, ונשנה אותה קצת. אני אקח את הנקודה כאן, 1 על ציר ה-x, ואני אצייר את פונקציית המחיר בה נשתמש. פונקציית המחיר החדשה תהיה שטוחה מהנקודה הזו והלאה, ואנחנו מציירים משהו שעולה כקו ישר אלכסוני, דומה לרגרסיה לוגיסטית אבל זה יהיה קו ישר בחלק הזה. אז העקומה הזו שציירתי בורוד, העקומה שאני ציירתי בסגול וורוד, היא די קרובה לפונקצית המחיר שאנו משתמשים בה ברגרסיה לוגיסטית. אלא שעכשיו היא מורכבת משני קטעים של קו ישר, יש החלק הזה בצד ימין, ויש החלק הזה שגם הוא קו ישר בצד שמאל. ואל תשימו לב לשיפוע של החלק של הקו הישר. זה לא משנה כל כך. אבל זוהי פונקציית העלות החדשה שבה אנחנו הולכים להשתמש כאשר y שווה לאחד, ואתם יכולים להבין שזה צריך לעשות משהו די דומה לרגרסיה לוגיסטית. אבל מתברר שזה ייתן למכונת התמך הוקטורי יתרונות חישוביים וזה ייתן לנו מאוחר יותר בעיית אופטימיזציה קלה יותר שתהיה קלה יותר לתוכנה לפתור. דיברנו עד עכשיו על המקרה של y=1. המקרה השני הוא y=0. במקרה הזה, אם אתה מסתכל על פונקצית העלות, אז רק המונח השני משמעותי כי הראשון הוא אפס, נכון? אם y=0, אז כאן יש לנו 0, ואנחנו נשארים רק עם המונח השני של הביטוי שלמעלה. ולכן העלות של דוגמה, או התרומה שלה לפונקציית העלות, ניתנת על ידי הביטוי הזה כאן. ואם אתה משרטט את זה כפונקציה של z, z נמצא כאן על הציר האופקי, אתה מקבל את זה. וגם כאן, עבור מכונת התמך הוקטורי, אנחנו נחליף את הקו הכחול במשהו דומה, כאן אנחנו נחליף את העלות בעלות חדשה, משמאל היא 0 עד ל-z = -1, ואז העלות גדלה כקו ישר, כך. אז הרשו לי לתת שמות לשתי הפונקציות. לפונקציה בצד שמאל אני אקרא עלות₁ (אינדקס 1) של z, ולפונקציה הזאת בימין אני אקרא עלות₀ של z. והאינדקס פשוט מזכיר לנו שכאן זו העלות כאשר y=1, וכאן כאשר y=0. חמושים בהגדרות אלה, אנו מוכנים כעת לבנות מכונת תמך וקטורי. הנה פונקציית העלות, J של θ, שבה אנחנו משתמשים ברגרסיה לוגיסטית. אם המשוואה הזו נראית קצת לא מוכרת, זה משום שבעבר היה לנו סימן מינוס בחוץ, וכאן מה שעשיתי היה להעביר את סימני המינוס לתוך הביטויים האלה, אז זה פשוט גורם לזה להראות קצת שונה. עבור מכונת התמך הוקטורי מה שנעשה הוא בעצם לקחת את זה ולהחליף את זה בעלות₁ של z, שהוא עלות₁ של θᵀ*x. ואנחנו הולכים לקחת את זה ולהחליף אותו בעלות₀ של z, דהיינו עלות₀ של θᵀ*x (תטא משוחלפת כפול x). כשעלות-1 היא מה שהיה לנו בשקופית הקודמת, הפונקציה שנראית ככה. ועלות-0 גם היא היתה לנו על השקופית הקודמת, ונראתה ככה. אז מה שאנחנו מחשבים במכונת התמך הוקטורי הוא בעיית מזעור של אחד חלקי m, כפול הסכום מ-1 עד m של (y(i כפול עלות-1 של θᵀ כפול (x(i, ועוד 1 מינוס (y(i כפול עלות-0 של θᵀ כפול (x(i, ועוד הפרמטר הרגיל של הרגולריזציה. ככה. עכשיו, לפי הקונוונציה, במכונת תמך וקטורי אנחנו בעצם כותבים דברים בצורה קצת שונה. אנחנו כותבים את הפרמטרים מחדש פשוט בצורה קצת שונה. דבר ראשון אנחנו נפטרים מה-1 חלקי m, וזו פשוט קונוונציה טיפה שונה בה משתמשים עבור מכונות תמך וקטורי לעומת רגרסיה לוגיסטית. הנה מה שאני מתכוון. אנחנו פשוט נמחק וניפטר מהמונחים האלה של 1 חלקי m בכל המחוברים במשוואה וזה אמור לתת לנו אותו ערך אופטימלי עבור θ, ברור למה? כי 1 חלקי m הוא, בדיוק כמו כל קבוע, לא משנה אם אני פותר בעיית מזעור איתו או בלעדיו. אני אקבל אותו ערך אופטימלי עבור θ. אני אתן לכם דוגמה, נניח שיש לי בעית מינימיזציה למזער מספר ממשי u בפונקציה (u-5) בריבוע ועוד 1. אז המינימום של זה הוא במקרה u שווה 5. עכשיו לו לקחתי את הפונקציה הזו והכפלתי אותה ב-10. אז עכשיו בעיית המזעור שלי היא הפונקציה 10*(u-5) בריבוע ועוד 10. ובכן הערך של u שממזער את זה הוא עדיין u שווה חמש נכון? אז כשכופלים משהו שממזערים בקבוע כלשהו, 10 במקרה שלנו, זה לא משנה את הערך של u שנותן לנו את המינימום של הפונקציה. אז ממש באותו אופן, מה שעשיתי הוא על ידי שהורדנו את ה-1 חלקי m, כל מה שעשינו הוא הכפלת פונקצית המטרה שלנו בקבוע כלשהו m וזה לא ישנה את הערך של תטא שבו מושג המינימום. החלק השני של שינוי של סימונים, שהוא שוב פשוט הקונבנציה הסטנדרטית יותר בשימוש ב-SVM במקום ברגרסיה לוגיסטית, הוא השינוי הבא. ברגרסיה לוגיסטית, אנו מחברים שני איברים בפונקצית המטרה. הראשון הוא הביטוי הזה, שהוא העלות שמקורה בסדרת האימון והשני הוא הביטוי הזה, שהוא מונח ההסדרה. ומה שעשינו היה לשלוט ביחסי הגומלין בין שני הביטויים האלה על ידי שהגדרנו שמה שאנחנו רוצים הוא הביטוי הגדול A ועוד פרמטר ההסדרה λ כפול המונח השני B, זאת אומרת אנחנו משתמשים ב-A כדי לציין את המונח הראשון, וב-B כדי לציין את המונח השני בלי ה-λ. ובמקום לרשום את הפרמטרים האלה כ-A+λB, ואז מה שעשינו היה משחק בערכים שונים עבור פרמטר ההסדרה הזה λ, יכולנו להחליט על המשקל היחסי לפי השאלה כמה חשוב לנו התוצאות של סדרת האימון, דהיינו למזער את A, לעומת כמה אכפת לנו לשמור על ערכי הפרמטר θ קטנים, שעבורם נוצר הפרמטר B.
עבור מכונת תמך וקטורי, וזה רק עניין של מוסכמה, אנחנו משתמשים בפרמטר אחר. אז במקום להשתמש ב-λ כאן כדי לשלוט על שיווי המשקל היחסי בין הביטויים הראשון והשני. אנחנו במקום זה משתמשים בפרמטר שונה, שלפי המוסכמות נקרא C ומטרת האופטימיזציה היא מזעור של CA+B. אז עבור רגרסיה לוגיסטית, אם אנחנו קובעים ערך גדול מאוד של λ, זה אומר לתת ל-B משקל גבוה מאוד. וכאן אם אנחנו קובעים ערך קטן מאוד עבור C, אז זה שקול לנתינת משקל הרבה יותר גדול ל-B מאשר ל-A. זו פשוט דרך שונה לשליטה ביחסי הגומלין, זו דרך אחרת של מתן עדיפות כמה אכפת לנו האופטימיזציה של המונח הראשון, לעומת כמה אכפת לנו האופטימיזציה של המונח השני. ואם תרצו אתם יכולים לחשוב על זה כאילו שהפרמטר C משחק תפקיד דומה ל-1 חלקי λ. זה לא ששתי המשוואות או שני הביטויים האלה יהיו שווים. שהוא ממש שווה 1 חלקי λ, לא זה המקרה. אפשר לומר שאם C שווה ל-1 חלקי λ, אז שתי מטרות האופטימיזציה יתנו לכם בדיוק ערך אופטימלי זהה עבור θ. אז אני מסדר פה את הנוסחה ומוחק כאן את λ וכותב קבוע C. אז זה נותן לנו את פונקצית האופטימיזציה הכוללת שלנו עבור מכונת תמך וקטורי. ואם נמזער את הפונקציה הזו, אז מה שנקבל הוא הפרמטרים שנלמדו על ידי SVM. לבסוף, בניגוד לרגרסיה לוגיסטית, SVM אינה מפיקה הסתברות. מה שיש לנו הוא פונקצית העלות הזו שאנחנו ממזערים כדי לקבל את הפרמטר θ, ומה שעושה מכונת התמך הוקטורי הוא פשוט תחזית ישירה של האם y שווה אחד או אפס. אז ההשערה תחזה 1 אם θᵀx ≥ 0, והיא תחזה אפס אחרת, וכך אחרי שלמדה את הפרמטרים θ, זו היא צורת ההשערה עבור מכונת תמך וקטורי. אז זו היתה הגדרה מתמטית של מה עושה מכונת תמך וקטורי. בסרטונים הבאים, ננסה לחזור לאינטואיציה על לאן מובילה מטרת האופטימיזציה הזו ומה צורת ההשערות ש-SVM תלמד ואנחנו נדבר גם על איך לשנות את זה קצת עבור פונקציות לא לינאריות מורכבות.