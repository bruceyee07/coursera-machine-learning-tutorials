1
00:00:00,690 --> 00:00:03,738
בשלב הזה, כבר ראיתם מגוון של אלגוריתמי למידה שונים.

2
00:00:03,738 --> 00:00:08,029
בלמידה מבוקרת, הביצועים של אלגוריתמי למידה מבוקרים רבים

3
00:00:08,029 --> 00:00:11,782
יהיו דומים למדי, וזה פחות חשוב אם תשתמש

4
00:00:11,782 --> 00:00:14,345
באלגוריתם למידה א' או באלגוריתם הלמידה ב',

5
00:00:14,345 --> 00:00:17,920
מה שחשוב יותר יהיה הרבה פעמים כמות הנתונים שעליהם אתה מאמן

6
00:00:17,920 --> 00:00:21,703
את האלגוריתם, כמו גם הכשרון שלך בבניית האלגוריתמים האלה.

7
00:00:21,703 --> 00:00:25,922
דברים כמו הבחירה של התכונות שאתה מעצב לתת

8
00:00:25,922 --> 00:00:29,795
לאלגוריתמי למידה, וכיצד אתה בוחר את פרמטר ההסדרה,

9
00:00:29,795 --> 00:00:30,920
ודברים מהסוג הזה.

10
00:00:30,920 --> 00:00:36,470
אבל יש עוד אלגוריתם חזק מאוד שנמצא בשימוש נרחב מאוד

11
00:00:36,470 --> 00:00:42,370
הן בתעשייה והן באקדמיה, והוא נקרא מכונת תמך וקטורי או מכונת וקטורים תומכים או SVM.

12
00:00:42,370 --> 00:00:46,410
ובהשוואה לרגרסיה לוגיסטית ולרשתות עצביות,

13
00:00:46,410 --> 00:00:50,970
ה-Support Vector Machine, או SVM נותן לפעמים דרך נקייה

14
00:00:50,970 --> 00:00:55,070
ולפעמים יותר חזקה לאימון של פונקציות לא לינאריות מורכבות.

15
00:00:55,070 --> 00:00:59,930
אז בואו נקדיש את הסרטונים הבאים כדי לדבר על זה.

16
00:00:59,930 --> 00:01:03,240
בהמשך הקורס, אעשה סקר מהיר של מגוון

17
00:01:03,240 --> 00:01:07,460
אלגוריתמים שונים של למידה בפיקוח, בזמן שאתאר אותם בקצרה.

18
00:01:07,460 --> 00:01:11,630
אבל מכונת תמך וקטורי, בהתחשב בפופולריות שלה וכמה היא חזקה,

19
00:01:11,630 --> 00:01:16,320
זה יהיה אחרון האלגוריתמים בפיקוח בו אבלה כמות משמעותית

20
00:01:16,320 --> 00:01:21,240
של זמן בקורס הזה.
כמו בפיתוח שלנו של אלגוריתמי למידה אחרים,

21
00:01:21,240 --> 00:01:24,710
אנחנו נתחיל לדבר על מטרת האופטימיזציה.

22
00:01:24,710 --> 00:01:27,008
אז בואו נתחיל עם האלגוריתם הזה.

23
00:01:29,121 --> 00:01:33,461
כדי לתאר את מכונת התמך הוקטורי, אני בעצם הולך להתחיל עם

24
00:01:33,461 --> 00:01:36,685
רגרסיה לוגיסטית, ולהראות איך אנחנו יכולים לשנות את זה קצת,

25
00:01:36,685 --> 00:01:40,240
ולקבל את מה שהוא למעשה מכונת תמך וקטורי.

26
00:01:40,240 --> 00:01:46,030
אז ברגרסיה הלוגיסטית יש לנו את הצורה המוכרת של ההשערה

27
00:01:46,030 --> 00:01:48,880
ואת פונקציית ההפעלה או הסיגמואיד שמוצגת כאן מימין.

28
00:01:50,430 --> 00:01:52,570
וכדי להסביר חלק מהמתמטיקה,

29
00:01:52,570 --> 00:01:56,130
אני אשתמש ב- z כדי לציין θ משוחלפת כפול x.

30
00:01:57,690 --> 00:02:01,260
עכשיו בואו נחשוב על מה שאנחנו רוצים שהרגרסיה הלוגיסטית תעשה.

31
00:02:01,260 --> 00:02:06,150
אם יש לנו דוגמה עם y=1 וכשאני אומר דוגמה אני מתכוון

32
00:02:06,150 --> 00:02:10,185
או בסדרת האימון או בסדרת הבדיקה או בסדרת האימות הצולב,

33
00:02:10,185 --> 00:02:15,200
כאשר y=1 אז אנחנו מקווים כי (h(x יהיה קרוב לאחד.

34
00:02:15,200 --> 00:02:18,520
נכון, אנחנו מקווים שהפונקציה תסווג נכון את הדוגמה.

35
00:02:18,520 --> 00:02:20,260
ומשמעותו של "(h(x קרוב לאחד"

36
00:02:20,260 --> 00:02:24,620
היא θ משוחלפת כפול x חייב להיות גדול בהרבה מ-0.

37
00:02:24,620 --> 00:02:29,460
הסימן גדול-מ כפול (<<) משמעותו גדול בהרבה, או הרבה יותר גדול מ-0.

38
00:02:29,460 --> 00:02:33,600
וזה בגלל ש-z, שהוא θ

39
00:02:33,600 --> 00:02:38,670
משוחלפת כפול x, כאשר z הוא הרבה יותר גדול מ-0 והוא נמצא רחוק ימינה בגרף הזה,

40
00:02:38,670 --> 00:02:42,670
רק אז הפלט של הרגרסיה הלוגיסטית מתקרב לאחת.

41
00:02:44,570 --> 00:02:49,250
לעומת זאת, אם יש לנו דוגמה שבה y=0, אז מה שאנחנו מקווים הוא

42
00:02:49,250 --> 00:02:52,530
שההשערה תפיק ערך קרוב לאפס.

43
00:02:52,530 --> 00:02:57,460
וזה מתאים למצב שבו θ משוחלפת כפול x הוא קטן בהרבה מאפס

44
00:02:57,460 --> 00:03:01,150
שזה מתאים להשערה שמוציאה כפלט ערך קרוב לאפס.

45
00:03:02,850 --> 00:03:06,340
אם אתה מסתכל על פונקציית העלות של רגרסיה לוגיסטית,

46
00:03:06,340 --> 00:03:10,310
מה שתמצא הוא שכל דוגמה (x, y) תורמת

47
00:03:10,310 --> 00:03:13,070
מונח כזה לפונקציית העלות הכוללת, נכון?

48
00:03:14,290 --> 00:03:19,080
אז עבור פונקציית העלות הכוללת, יהיה לנו סכום על כל

49
00:03:19,080 --> 00:03:24,150
דוגמאות האימון והביטוי 1 חלקי m, הביטוי הזה כאן,

50
00:03:24,150 --> 00:03:28,490
זה המונח שאותו תורמת דוגמת אימון אחת

51
00:03:28,490 --> 00:03:31,700
לפונקצית המטרה הכוללת של הרגרסיה הלוגיסטית.

52
00:03:32,970 --> 00:03:37,390
עכשיו אם אני לוקח את ההגדרה של פונקציית ההשערה h

53
00:03:37,390 --> 00:03:41,470
ומציב אותה כאן, אז מה שאני מקבל הוא שכל דוגמת אימון

54
00:03:41,470 --> 00:03:46,270
תורמת את המונח הזה, אנחנו מתעלמים מאחת חלקי m אבל

55
00:03:46,270 --> 00:03:50,620
זה תורם את המונח הזה לעלות הכוללת שלי עבור רגרסיה לוגיסטית.

56
00:03:51,810 --> 00:03:57,450
עכשיו בואו נחשוב על שני מקרים, כאשר y=1 וכאשר y=0.

57
00:03:57,450 --> 00:04:01,280
במקרה הראשון, נניח ש-y=1.

58
00:04:01,280 --> 00:04:05,648
במקרה זה, רק המונח הראשון בנוסחה הזו מעניין,

59
00:04:05,648 --> 00:04:10,612
כי 1 מינוס y שווה לאפס כש-y=1.

60
00:04:13,200 --> 00:04:17,045
אז כאשר y=1, כאשר בדוגמה שלנו, (x,y),

61
00:04:17,045 --> 00:04:20,344
כאשר y=1 מה שאנחנו מקבלים זה המונח הזה.

62
00:04:20,344 --> 00:04:25,200
מינוס לוג של אחת חלקי אחת פלוס e בחזקת מינוס z ואני כמו שהגדרנו בשקופית

63
00:04:25,200 --> 00:04:30,940
הקודמת משתמש כאן ב-z כדי לציין θ משוחלפת כפול x

64
00:04:30,940 --> 00:04:35,060
ושימו לב כאן למינוס אחת שיש לנו אם y=1,

65
00:04:35,060 --> 00:04:40,169
אני פשוט מפשט קצת את הביטוי שכתוב כאן.

66
00:04:41,970 --> 00:04:47,450
ואם אנחנו משרטטים את הפונקציה הזו כפונקציה של z, מה שנמצא הוא שנקבל את

67
00:04:47,450 --> 00:04:51,100
העקומה הזו שמופיעה כאן בצד שמאל למטה של השקופית.

68
00:04:51,100 --> 00:04:54,850
ואנחנו גם רואים שכאשר z הוא גדול, כלומר,

69
00:04:54,850 --> 00:05:00,380
כאשר θ משוחלפת כפול x הוא גדול, זה מתאים לערך של z

70
00:05:00,380 --> 00:05:06,020
שנותן לנו ערך קטן יחסית, תרומה מאוד קטנה מאוד לפונקציית המחיר.

71
00:05:06,020 --> 00:05:10,680
וזה מסביר למה כאשר רגרסיה לוגיסטית רואה דוגמה חיובית,

72
00:05:10,680 --> 00:05:15,620
שבה y=1, היא מנסה להגדיר את θ משוחלפת כפול x לערך גדול מאוד

73
00:05:15,620 --> 00:05:21,090
כי זה מתאים לכך שהמונח הזה בפונקצית המחיר יהיה קטן.

74
00:05:21,090 --> 00:05:23,670
עכשיו, כדי לבנות את מכונת התמך הוקטורי, הנה מה שאנחנו נעשה.

75
00:05:23,670 --> 00:05:28,310
אנחנו ניקח את פונקצית המחיר, מינוס לוג של 1 חלקי אחת פלוס e בחזקת מינוס z,

76
00:05:28,310 --> 00:05:31,370
ונשנה אותה קצת.

77
00:05:31,370 --> 00:05:35,140
אני אקח את הנקודה כאן, 1 על ציר ה-x,

78
00:05:35,140 --> 00:05:37,950
ואני אצייר את פונקציית המחיר בה נשתמש.

79
00:05:37,950 --> 00:05:42,240
פונקציית המחיר החדשה תהיה שטוחה מהנקודה הזו והלאה,

80
00:05:42,240 --> 00:05:46,626
ואנחנו מציירים משהו שעולה כקו ישר אלכסוני,

81
00:05:46,626 --> 00:05:50,280
דומה לרגרסיה לוגיסטית

82
00:05:50,280 --> 00:05:53,850
אבל זה יהיה קו ישר בחלק הזה.

83
00:05:53,850 --> 00:05:58,080
אז העקומה הזו שציירתי בורוד, העקומה שאני ציירתי בסגול

84
00:05:58,080 --> 00:06:02,130
וורוד, היא די קרובה

85
00:06:02,130 --> 00:06:04,470
לפונקצית המחיר שאנו משתמשים בה ברגרסיה לוגיסטית.

86
00:06:04,470 --> 00:06:09,090
אלא שעכשיו היא מורכבת משני קטעים של קו ישר, יש החלק הזה

87
00:06:09,090 --> 00:06:14,180
בצד ימין, ויש החלק הזה שגם הוא קו ישר בצד שמאל.

88
00:06:14,180 --> 00:06:17,770
ואל תשימו לב לשיפוע של החלק של הקו הישר.

89
00:06:17,770 --> 00:06:20,020
זה לא משנה כל כך.

90
00:06:20,020 --> 00:06:24,490
אבל זוהי פונקציית העלות החדשה שבה אנחנו הולכים להשתמש כאשר y שווה לאחד,

91
00:06:24,490 --> 00:06:28,285
ואתם יכולים להבין שזה צריך לעשות משהו די דומה לרגרסיה לוגיסטית.

92
00:06:28,285 --> 00:06:31,970
אבל מתברר שזה ייתן למכונת התמך הוקטורי

93
00:06:31,970 --> 00:06:36,590
יתרונות חישוביים וזה ייתן לנו מאוחר יותר בעיית אופטימיזציה קלה יותר

94
00:06:38,100 --> 00:06:41,080
שתהיה קלה יותר לתוכנה לפתור.

95
00:06:41,080 --> 00:06:43,120
דיברנו עד עכשיו על המקרה של y=1.

96
00:06:43,120 --> 00:06:45,588
המקרה השני הוא y=0.

97
00:06:45,588 --> 00:06:48,131
במקרה הזה, אם אתה מסתכל על פונקצית העלות,

98
00:06:48,131 --> 00:06:53,390
אז רק המונח השני משמעותי כי הראשון הוא אפס, נכון?

99
00:06:53,390 --> 00:06:55,810
אם y=0, אז כאן יש לנו 0,

100
00:06:55,810 --> 00:06:59,140
ואנחנו נשארים רק עם המונח השני של הביטוי שלמעלה.

101
00:06:59,140 --> 00:07:01,620
ולכן העלות של דוגמה, או

102
00:07:01,620 --> 00:07:06,380
התרומה שלה לפונקציית העלות, ניתנת על ידי הביטוי הזה כאן.

103
00:07:06,380 --> 00:07:08,430
ואם אתה משרטט את זה כפונקציה של z,

104
00:07:08,430 --> 00:07:13,180
z נמצא כאן על הציר האופקי, אתה מקבל את זה.

105
00:07:13,180 --> 00:07:15,190
וגם כאן, עבור מכונת התמך הוקטורי,

106
00:07:15,190 --> 00:07:18,834
אנחנו נחליף את הקו הכחול במשהו דומה,

107
00:07:18,834 --> 00:07:24,780
כאן אנחנו נחליף את העלות בעלות חדשה, משמאל היא 0 עד ל-z = -1,

108
00:07:24,780 --> 00:07:29,020
ואז העלות גדלה כקו ישר, כך.

109
00:07:29,020 --> 00:07:32,696
אז הרשו לי לתת שמות לשתי הפונקציות.

110
00:07:32,696 --> 00:07:37,582
לפונקציה בצד שמאל אני אקרא עלות₁ (אינדקס 1) של z,

111
00:07:37,582 --> 00:07:42,660
ולפונקציה הזאת בימין אני אקרא עלות₀ של z.

112
00:07:42,660 --> 00:07:47,652
והאינדקס פשוט מזכיר לנו שכאן זו העלות כאשר y=1,

113
00:07:47,652 --> 00:07:49,990
וכאן כאשר y=0.

114
00:07:49,990 --> 00:07:53,820
חמושים בהגדרות אלה, אנו מוכנים כעת לבנות מכונת תמך וקטורי.

115
00:07:53,820 --> 00:07:58,360
הנה פונקציית העלות, J של θ, שבה אנחנו משתמשים ברגרסיה לוגיסטית.

116
00:07:58,360 --> 00:08:02,560
אם המשוואה הזו נראית קצת לא מוכרת, זה משום שבעבר היה לנו

117
00:08:02,560 --> 00:08:07,140
סימן מינוס בחוץ, וכאן מה שעשיתי היה להעביר את סימני המינוס

118
00:08:07,140 --> 00:08:10,529
לתוך הביטויים האלה, אז זה פשוט גורם לזה להראות קצת שונה.

119
00:08:12,790 --> 00:08:17,670
עבור מכונת התמך הוקטורי מה שנעשה הוא בעצם לקחת את זה

120
00:08:17,670 --> 00:08:25,070
ולהחליף את זה בעלות₁ של z, שהוא עלות₁ של θᵀ*x.

121
00:08:25,070 --> 00:08:30,210
ואנחנו הולכים לקחת את זה ולהחליף אותו בעלות₀ של z,

122
00:08:30,210 --> 00:08:34,120
דהיינו עלות₀ של θᵀ*x (תטא משוחלפת כפול x).

123
00:08:34,120 --> 00:08:36,730
כשעלות-1 היא

124
00:08:36,730 --> 00:08:39,920
מה שהיה לנו בשקופית הקודמת, הפונקציה שנראית ככה.

125
00:08:39,920 --> 00:08:44,470
ועלות-0 גם היא היתה לנו על השקופית הקודמת,

126
00:08:44,470 --> 00:08:46,130
ונראתה ככה.

127
00:08:46,130 --> 00:08:52,082
אז מה שאנחנו מחשבים במכונת התמך הוקטורי

128
00:08:52,082 --> 00:08:57,406
הוא בעיית מזעור של אחד חלקי m, כפול הסכום מ-1 עד m של (y(i

129
00:08:57,406 --> 00:09:01,163
כפול עלות-1 של θᵀ

130
00:09:01,163 --> 00:09:06,172
כפול (x(i, ועוד 1 מינוס (y(i

131
00:09:06,172 --> 00:09:11,495
כפול עלות-0 של θᵀ כפול (x(i,

132
00:09:11,495 --> 00:09:18,258
ועוד הפרמטר הרגיל של הרגולריזציה.

133
00:09:21,253 --> 00:09:23,459
ככה.

134
00:09:23,459 --> 00:09:26,448
עכשיו, לפי הקונוונציה, במכונת תמך וקטורי

135
00:09:26,448 --> 00:09:29,132
אנחנו בעצם כותבים דברים בצורה קצת שונה.

136
00:09:29,132 --> 00:09:32,180
אנחנו כותבים את הפרמטרים מחדש פשוט בצורה קצת שונה.

137
00:09:33,320 --> 00:09:36,870
דבר ראשון אנחנו נפטרים מה-1 חלקי m,

138
00:09:36,870 --> 00:09:41,170
וזו פשוט קונוונציה טיפה שונה בה משתמשים

139
00:09:41,170 --> 00:09:44,690
עבור מכונות תמך וקטורי לעומת רגרסיה לוגיסטית.

140
00:09:44,690 --> 00:09:46,190
הנה מה שאני מתכוון.

141
00:09:46,190 --> 00:09:50,173
אנחנו פשוט נמחק וניפטר מהמונחים האלה של 1 חלקי m בכל המחוברים במשוואה

142
00:09:50,173 --> 00:09:53,546
וזה אמור לתת לנו אותו ערך אופטימלי עבור θ, ברור למה?

143
00:09:53,546 --> 00:09:56,030
כי 1 חלקי m הוא, בדיוק כמו כל קבוע,

144
00:09:56,030 --> 00:10:00,793
לא משנה אם אני פותר בעיית מזעור איתו או בלעדיו.

145
00:10:00,793 --> 00:10:04,750
אני אקבל אותו ערך אופטימלי עבור θ.

146
00:10:04,750 --> 00:10:05,740
אני אתן לכם דוגמה,

147
00:10:05,740 --> 00:10:10,380
נניח שיש לי בעית מינימיזציה

148
00:10:10,380 --> 00:10:18,340
למזער מספר ממשי u בפונקציה (u-5) בריבוע ועוד 1.

149
00:10:18,340 --> 00:10:22,110
אז המינימום של זה הוא במקרה u שווה 5.

150
00:10:23,130 --> 00:10:28,500
עכשיו לו לקחתי את הפונקציה הזו והכפלתי אותה ב-10.

151
00:10:28,500 --> 00:10:36,050
אז עכשיו בעיית המזעור שלי היא הפונקציה 10*(u-5) בריבוע ועוד 10.

152
00:10:36,050 --> 00:10:40,910
ובכן הערך של u שממזער את זה הוא עדיין u שווה חמש נכון?

153
00:10:40,910 --> 00:10:45,190
אז כשכופלים משהו שממזערים בקבוע כלשהו,

154
00:10:45,190 --> 00:10:50,470
10 במקרה שלנו, זה לא משנה את הערך של u שנותן לנו

155
00:10:50,470 --> 00:10:52,650
את המינימום של הפונקציה.

156
00:10:52,650 --> 00:10:56,350
אז ממש באותו אופן, מה שעשיתי הוא על ידי שהורדנו את ה-1 חלקי m,

157
00:10:56,350 --> 00:11:01,080
כל מה שעשינו הוא הכפלת פונקצית המטרה שלנו בקבוע כלשהו m

158
00:11:01,080 --> 00:11:03,350
וזה לא ישנה את הערך של תטא

159
00:11:03,350 --> 00:11:05,470
שבו מושג המינימום.

160
00:11:05,470 --> 00:11:09,170
החלק השני של שינוי של סימונים, שהוא שוב פשוט הקונבנציה

161
00:11:09,170 --> 00:11:14,250
הסטנדרטית יותר בשימוש ב-SVM במקום ברגרסיה לוגיסטית, הוא השינוי הבא.

162
00:11:14,250 --> 00:11:19,370
ברגרסיה לוגיסטית, אנו מחברים שני איברים בפונקצית המטרה.

163
00:11:19,370 --> 00:11:23,920
הראשון הוא הביטוי הזה, שהוא העלות שמקורה בסדרת האימון

164
00:11:23,920 --> 00:11:26,530
והשני הוא הביטוי הזה, שהוא מונח ההסדרה.

165
00:11:27,540 --> 00:11:32,710
ומה שעשינו היה לשלוט ביחסי הגומלין בין שני הביטויים האלה על ידי שהגדרנו

166
00:11:32,710 --> 00:11:39,370
שמה שאנחנו רוצים הוא הביטוי הגדול A ועוד פרמטר ההסדרה λ

167
00:11:39,370 --> 00:11:44,950
כפול המונח השני B, זאת אומרת אנחנו משתמשים ב-A

168
00:11:44,950 --> 00:11:50,080
כדי לציין את המונח הראשון, וב-B כדי לציין את המונח השני בלי ה-λ.

169
00:11:51,130 --> 00:11:57,330
ובמקום לרשום את הפרמטרים האלה כ-A+λB,

170
00:11:57,330 --> 00:12:00,880
ואז מה שעשינו היה משחק בערכים שונים

171
00:12:00,880 --> 00:12:05,010
עבור פרמטר ההסדרה הזה λ, יכולנו להחליט על המשקל היחסי

172
00:12:05,010 --> 00:12:09,273
לפי השאלה כמה חשוב לנו התוצאות של סדרת האימון, דהיינו למזער את A,

173
00:12:09,273 --> 00:12:13,630
לעומת כמה אכפת לנו לשמור על ערכי הפרמטר θ קטנים,

174
00:12:13,630 --> 00:12:17,230
שעבורם נוצר הפרמטר B.
עבור מכונת תמך וקטורי,

175
00:12:17,230 --> 00:12:20,300
וזה רק עניין של מוסכמה, אנחנו משתמשים בפרמטר אחר.

176
00:12:20,300 --> 00:12:25,200
אז במקום להשתמש ב-λ כאן כדי לשלוט על שיווי המשקל היחסי בין

177
00:12:25,200 --> 00:12:26,160
הביטויים הראשון והשני.

178
00:12:26,160 --> 00:12:31,530
אנחנו במקום זה משתמשים בפרמטר שונה, שלפי המוסכמות נקרא C

179
00:12:31,530 --> 00:12:37,550
ומטרת האופטימיזציה היא מזעור של CA+B.

180
00:12:37,550 --> 00:12:43,260
אז עבור רגרסיה לוגיסטית, אם אנחנו קובעים ערך גדול מאוד של λ,

181
00:12:43,260 --> 00:12:46,040
זה אומר לתת ל-B משקל גבוה מאוד.

182
00:12:46,040 --> 00:12:49,762
וכאן אם אנחנו קובעים ערך קטן מאוד עבור C,

183
00:12:49,762 --> 00:12:54,600
אז זה שקול לנתינת משקל הרבה יותר גדול ל-B מאשר ל-A.

184
00:12:54,600 --> 00:12:58,070
זו פשוט דרך שונה לשליטה ביחסי הגומלין,

185
00:12:58,070 --> 00:13:02,450
זו דרך אחרת של מתן עדיפות כמה אכפת לנו האופטימיזציה של המונח הראשון,

186
00:13:02,450 --> 00:13:05,650
לעומת כמה אכפת לנו האופטימיזציה של המונח השני.

187
00:13:05,650 --> 00:13:10,050
ואם תרצו אתם יכולים לחשוב על זה כאילו שהפרמטר C

188
00:13:10,050 --> 00:13:13,551
משחק תפקיד דומה ל-1 חלקי λ.

189
00:13:13,551 --> 00:13:18,429
זה לא ששתי המשוואות או שני הביטויים האלה יהיו שווים.

190
00:13:18,429 --> 00:13:20,736
שהוא ממש שווה 1 חלקי λ, לא זה המקרה.

191
00:13:20,736 --> 00:13:25,511
אפשר לומר שאם C שווה ל-1 חלקי λ, אז שתי מטרות האופטימיזציה

192
00:13:25,511 --> 00:13:30,211
יתנו לכם בדיוק ערך אופטימלי זהה עבור θ.

193
00:13:30,211 --> 00:13:33,810
אז אני מסדר פה את הנוסחה ומוחק כאן את λ

194
00:13:33,810 --> 00:13:35,740
וכותב קבוע C.

195
00:13:37,290 --> 00:13:41,598
אז זה נותן לנו את פונקצית האופטימיזציה הכוללת שלנו

196
00:13:41,598 --> 00:13:43,564
עבור מכונת תמך וקטורי.

197
00:13:43,564 --> 00:13:45,948
ואם נמזער את הפונקציה הזו,

198
00:13:45,948 --> 00:13:49,690
אז מה שנקבל הוא הפרמטרים שנלמדו על ידי SVM.

199
00:13:51,966 --> 00:13:56,748
לבסוף, בניגוד לרגרסיה לוגיסטית, SVM אינה מפיקה

200
00:13:56,748 --> 00:14:00,570
הסתברות. מה שיש לנו הוא פונקצית העלות הזו

201
00:14:00,570 --> 00:14:05,213
שאנחנו ממזערים כדי לקבל את הפרמטר θ, ומה שעושה מכונת התמך הוקטורי

202
00:14:05,213 --> 00:14:09,650
הוא פשוט תחזית ישירה של האם y שווה אחד או אפס.

203
00:14:09,650 --> 00:14:12,350
אז ההשערה תחזה 1

204
00:14:13,400 --> 00:14:17,670
אם θᵀx ≥ 0,

205
00:14:17,670 --> 00:14:22,690
והיא תחזה אפס אחרת, וכך אחרי שלמדה את הפרמטרים θ,

206
00:14:22,690 --> 00:14:26,850
זו היא צורת ההשערה עבור מכונת תמך וקטורי.

207
00:14:26,850 --> 00:14:31,730
אז זו היתה הגדרה מתמטית של מה עושה מכונת תמך וקטורי.

208
00:14:31,730 --> 00:14:35,390
בסרטונים הבאים, ננסה לחזור לאינטואיציה

209
00:14:35,390 --> 00:14:37,800
על לאן מובילה מטרת האופטימיזציה הזו

210
00:14:37,800 --> 00:14:42,530
ומה צורת ההשערות ש-SVM תלמד ואנחנו נדבר גם על איך

211
00:14:42,530 --> 00:14:46,870
לשנות את זה קצת עבור פונקציות לא לינאריות מורכבות.