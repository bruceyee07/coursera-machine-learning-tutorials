1
00:00:00,251 --> 00:00:05,622
באלגוריתמים רבים של למידה, ביניהם רגרסיה לינארית, רגרסיה לוגיסטית ורשתות עצביות,

2
00:00:05,622 --> 00:00:11,955
הדרך בה אנו מפתחים את האלגוריתם היא על ידי יצירה של פונקצית עלות או מציאה של יעד אופטימיזציה.

3
00:00:11,955 --> 00:00:16,476
ואז שימוש באלגוריתם כמו ירידה במדרון כדי למזער את פונקצית העלות.

4
00:00:16,476 --> 00:00:22,461
כשיש לנו סדרת אימון גדולה מאוד, הירידה במדרון הופכת להיות תהליך חישובי יקר מאוד.

5
00:00:22,461 --> 00:00:29,300
בסרטון הזה נדבר על שינוי באלגוריתם הבסיסי של ירידה במדרון הנקרא "ירידה סטוכסטית במדרון" (Stochastic gradient),

6
00:00:29,300 --> 00:00:37,841
שיאפשר לנו להרחיב את האלגוריתמים הללו לקבוצות אימון גדולות בהרבה.

7
00:00:37,841 --> 00:00:41,928
נניח שאתה מאמן מודל של רגרסיה ליניארית באמצעות ירידה במדרון.

8
00:00:41,928 --> 00:00:48,055
כתזכורת מהירה, פונקציית ההשערה תיראה כך ופונקציית העלות תיראה כך,

9
00:00:48,055 --> 00:00:54,459
שזה הסכום של מחצית השגיאה הריבועית הממוצעת של ההשערה ב-m דוגמאות האימון,

10
00:00:54,459 --> 00:00:59,705
ופונקציית העלות שכבר ראינו נראית כמו מין פונקציה כזו בצורת קערה.

11
00:00:59,705 --> 00:01:06,659
דהיינו, כשמשרטטים אותה כפונקציה של הפרמטרים θ0 ו-θ1, פונקציית העלות J נראית בצורת קערה.

12
00:01:06,659 --> 00:01:10,999
והירידה במדרון נראית ככה, כשבלולאה הפנימית של הירידה במדרון

13
00:01:10,999 --> 00:01:15,594
אנחנו מעדכנים שוב ושוב את הפרמטרים θ באמצעות הביטוי הזה.

14
00:01:15,594 --> 00:01:22,574
עכשיו מכאן והלאה בווידאו הזה, אני מתכוון להמשיך ולהשתמש ברגרסיה ליניארית כדוגמה חיה.

15
00:01:22,574 --> 00:01:29,371
אבל הרעיונות כאן, הרעיונות של ירידה סטוכסטית במדרון הם כלליים לחלוטין וחלים גם על אלגוריתמי למידה אחרים

16
00:01:29,371 --> 00:01:38,011
כמו רגרסיה לוגיסטית, רשתות עצביות ואלגוריתמים אחרים המבוססים על אימון על ידי ירידה הדרגתית על קבוצת הכשרה ספציפית.

17
00:01:38,011 --> 00:01:43,236
אז הנה תמונה של מה עושה ירידה במדרון, אם הפרמטרים מאותחלים לנקודה כאן

18
00:01:43,236 --> 00:01:50,072
אז כשמפעילים ירידה במדרון, האיטרציות המתקדמות של הירידה במדרון ימשכו את הפרמטרים לכיוון המינימום הגלובלי.

19
00:01:50,072 --> 00:01:55,193
הם יסמנו מסלול שנראה כמו זה ויתכווננו די ישירות אל המינימום הגלובלי.

20
00:01:55,193 --> 00:01:59,561
עכשיו, הבעיה עם ירידה הדרגתית היא שאם m הוא גדול

21
00:01:59,561 --> 00:02:08,382
אז החישוב של הביטוי הזה של הנגזרת יכול להיות יקר מאוד, כי הוא מערב סיכום של איבר מכל m הדוגמאות.

22
00:02:08,382 --> 00:02:15,644
אז אם m הוא 300 מיליון, נניח. בארצות הברית יש כ-300 מיליון אנשים.

23
00:02:15,644 --> 00:02:20,783
אז הנתונים של מפקד האוכלוסין בארה"ב עשויים להיות בסדר הגודל הזה.

24
00:02:20,783 --> 00:02:26,715
אז כשרוצים להתאים מודל של רגרסיה ליניארית לזה צריך לסכם 300 מיליון רשומות.

25
00:02:26,715 --> 00:02:36,385
וזה יקר מאוד. אחד השמות של האלגוריתם הזה הוא, הגרסה המסוימת הזו של ירידה במדרון, נקראת גם ירידה במדרון באצווה (batch).

26
00:02:36,385 --> 00:02:41,352
והמונח אצווה מתייחס לעובדה שאנחנו מסתכלים על כל דוגמאות ההכשרה כל פעם.

27
00:02:41,352 --> 00:02:44,303
אנחנו קוראים לזה כעין אצווה של כל דוגמאות האימון.

28
00:02:44,303 --> 00:02:51,853
זה אולי בעצם לא השם המתאים ביותר, אבל כך קוראים לזה המקצוענים של למידת מכונה לגרסה המסוימת הזו של ירידה הדרגתית.

29
00:02:51,853 --> 00:02:57,157
ואם תדמיינו באמת שיש לכם 300 מיליון רשומות של מפקד האוכלוסין מאוחסנים בדיסק.

30
00:02:57,157 --> 00:03:05,945
אז האופן שבו עובד האלגוריתם הזה הוא שצריך לקרוא לתוך זיכרון המחשב את כל 300 מיליון הרשומות כדי לחשב את ביטויי הנגזרות.

31
00:03:05,945 --> 00:03:11,508
צריכים להזרים את כל הרשומות האלה דרך המחשב, כי אי אפשר לאחסן את כל הרשומות בזיכרון של המחשב.

32
00:03:11,508 --> 00:03:16,425
אז צריכים לקרוא אותם לאט לאט ולאגור את הסכום על מנת לחשב את הנגזרת.

33
00:03:16,425 --> 00:03:21,452
ואז לאחר שעשינו את כל העבודה הזאת, זה מאפשר לנו בסך הכל לעשות צעד אחד של ירידה במדרון.

34
00:03:21,452 --> 00:03:24,749
ואז צריך לעשות את כל העניין הזה מחדש.

35
00:03:24,749 --> 00:03:28,424
לסרוק את כל 300 מיליון הרשומות, לאגור את הסכומים האלה.

36
00:03:28,424 --> 00:03:32,578
ולאחר שעשינו שוב את כל העבודה, אפשר לעשות עוד צעד קטן של ירידה במדרון.

37
00:03:32,578 --> 00:03:36,959
ואז לעשות את זה שוב. ואז אפשר לעשות עוד צעד שלישי. וכולי.

38
00:03:36,959 --> 00:03:40,819
אז זה הולך לקחת המון זמן עד שהאלגוריתם הזה יתכנס.

39
00:03:40,819 --> 00:03:45,375
בניגוד לירידה במדרון באצווה, מה שאנחנו הולכים לעשות הוא לבנות אלגוריתם אחר,

40
00:03:45,375 --> 00:03:50,465
שבו לא צריך להסתכל על כל דוגמאות ההכשרה בכל איטרציה ואיטרציה,

41
00:03:50,465 --> 00:03:55,118
אלא צריך להסתכל רק על דוגמת אימון אחת לכל איטרציה.

42
00:03:55,118 --> 00:03:59,617
לפני שנעבור לאלגוריתם החדש, הנה שוב אלגוריתם הירידה במדרון באצווה שאני כותב אותו שוב

43
00:03:59,617 --> 00:04:05,794
כשכאן נמצאת פונקציית העלות וכאן שלב העדכון וכמובן המונח הזה כאן,

44
00:04:05,794 --> 00:04:10,678
שמשתמשים בו בשלב הירידה, הוא הנגזרת החלקית

45
00:04:10,678 --> 00:04:17,933
ביחס לפרמטרים θj של מטרת האופטימיזציה שלנו, J-אימון של θ.

46
00:04:17,933 --> 00:04:23,386
עכשיו בואו נסתכל על האלגוריתם היעיל יותר שעובד יותר טוב במערכי נתונים גדולים.

47
00:04:23,386 --> 00:04:26,489
כדי להסביר את האלגוריתם הנקרא "ירידה סטוכסטית במדרון" (Stochastic gradient descent),

48
00:04:26,489 --> 00:04:32,657
נגדיר את פונקציית העלות בצורה שונה במקצת, נגדיר את העלות של הפרמטר θ

49
00:04:32,657 --> 00:04:40,471
ביחס לדוגמת אימון (x(i),y(i כחצי המרחק בריבוע

50
00:04:40,471 --> 00:04:44,791
בין ההשערה לדוגמה (x(i),y(i.

51
00:04:44,791 --> 00:04:53,386
אז המונח הזה של פונקצית העלות בעצם מודד את טיב ההשערה ביחס לדוגמא הבודדת (x(i),y(i.

52
00:04:53,386 --> 00:05:01,010
עכשיו אפשר לראות שניתן לכתוב את פונקצית העלות הכוללת J-אימון בצורה השקולה הזו.

53
00:05:01,010 --> 00:05:09,606
J-אימון היא פשוט הממוצע על m דוגמאות האימון של העלות של ההשערה על הדוגמא (x(i),y(i.

54
00:05:09,606 --> 00:05:13,522
חמוש בהבנה הזו של פונקציית העלות עבור רגרסיה ליניארית,

55
00:05:13,522 --> 00:05:17,636
תרשו לי עכשיו לכתוב מה עושה הירידה הסטוכסטית בשיפוע.

56
00:05:17,636 --> 00:05:26,940
הצעד הראשון של ירידה סטוכסטית במדרון היא לערבב אקראית את סדרת הנתונים.

57
00:05:26,940 --> 00:05:32,539
כשאני אומר אקראית, אני מתכוון פשוט לסדר מחדש באופן אקראי את m דוגמאות האימון.

58
00:05:32,539 --> 00:05:37,450
זה מין צעד סטנדרטי של עיבוד מראש, תיכף נחזור לזה.

59
00:05:37,450 --> 00:05:42,997
אבל העבודה העיקרית של הירידה הסטוכסטית בשיפוע נעשית אחר כך בצעדים הבאים.

60
00:05:42,997 --> 00:05:48,150
אנחנו הולכים לחזור על זה בלולאה עבור i שווה מ-1 עד m.

61
00:05:48,150 --> 00:05:53,067
נעבור שוב ושוב על דוגמאות ההדרכה ונבצע את העדכון הבא.

62
00:05:53,067 --> 00:06:06,523
נעדכן את הפרמטר θj ל-θj פחות α כפול ( (h(x(i)) - y(i ) כפול x(i)j.

63
00:06:06,523 --> 00:06:12,961
וכרגיל נעשה את העדכון הזה לכל הערכים של j.

64
00:06:12,961 --> 00:06:24,708
עכשיו, אתם רואים שהמונח הזה כאן הוא בדיוק מה שהיה לנו בתוך הסיכום עבור הירידה במדרון באצווה.

65
00:06:24,708 --> 00:06:31,256
למעשה, למתמטיקאים ביניכם ניתן להראות שהמונח הזה כאן,

66
00:06:31,256 --> 00:06:43,511
שווה לנגזרת החלקית ביחס לפרמטר θj של העלות של הפרמטרים θ, על (x(i),y(i.

67
00:06:43,511 --> 00:06:47,383
כשהעלות היא כמובן הדבר הזה שהוגדר מקודם.

68
00:06:47,383 --> 00:06:52,081
ורק כדי לסכם את האלגוריתם, הרשו לי לסגור את הסוגריים המסולסלים שלי כאן.

69
00:06:52,081 --> 00:06:59,365
אז מה שעושה הירידה הסטוכסטית בשיפוע הוא בעצם מעבר על כל דוגמאות ההכשרה.

70
00:06:59,365 --> 00:07:04,349
בפעם הראשונה היא מסתכלת על דוגמת האימון הראשונה (1)x(1),y.

71
00:07:04,349 --> 00:07:09,399
ואז מהסתכלות רק על הדוגמה הראשונה, היא תעשה מין צעד קטן כזה של ירידה במדרון הדרגתית

72
00:07:09,399 --> 00:07:13,725
רק לפי העלות של דוגמת ההכשרה הראשונה.

73
00:07:13,725 --> 00:07:15,717
במילים אחרות, אנחנו הולכים להסתכל רק על הדוגמה הראשונה

74
00:07:15,717 --> 00:07:21,214
ולשנות את הפרמטרים טיפטיפה כדי להתאים רק לדוגמת האימון הראשונה קצת יותר טוב.

75
00:07:21,214 --> 00:07:29,244
ואחרי זה, בתוך הלולאה הפנימית אנחנו נעבור לדוגמת האימון השנייה.

76
00:07:29,244 --> 00:07:33,848
ומה שנעשה שם הוא עוד צעד קטן במרחב הפרמטרים,

77
00:07:33,848 --> 00:07:39,682
דהיינו לשנות את הפרמטרים רק במקצת כדי לנסות להתאים רק לדוגמת ההכשרה השנייה קצת יותר טוב.

78
00:07:39,682 --> 00:07:44,130
ואחרי שעשינו את זה, עוברים לדוגמת האימון השלישית.

79
00:07:44,130 --> 00:07:51,722
לשנות את הפרמטרים רק במקצת כדי לנסות להתאים רק לדוגמת ההכשרה השלישית קצת יותר טוב. וכן הלאה

80
00:07:51,722 --> 00:07:55,114
עד שעברנו על כל סדרת האימון.

81
00:07:55,114 --> 00:08:01,297
ואז הלולאה החיצונית עשויה לגרום לו לעשות מספר מעברים כאלה על כל סדרת האימון.

82
00:08:01,297 --> 00:08:07,346
ההסתכלות הזו על ירידה סטוכסטית במדרון גם מסבירה למה רצינו להתחיל בערבוב אקראי של סדרת הנתונים.

83
00:08:07,346 --> 00:08:10,772
זה מבטיח לנו שכאשר אנו עוברים על סדרת האימון כאן,

84
00:08:10,772 --> 00:08:15,197
אנחנו עוברים על דוגמאות ההכשרה בסדר אקראי.

85
00:08:15,197 --> 00:08:21,229
לפי המצב, אם הנתונים כבר הגיעו ממוינים באופן אקראי או אם הם הגיעו במקור ממוינים,

86
00:08:21,229 --> 00:08:26,391
למעשה זה רק מאיץ את השינויים של הירידה הסטוכסטית רק באופן מינורי.

87
00:08:26,391 --> 00:08:30,985
אז לשם הבטיחות, זה בדרך כלל טוב יותר לערבב אקראית את סדרת הנתונים אם אנחנו לא בטוחים

88
00:08:30,985 --> 00:08:34,056
אם הם הגיעו ממוינים אקראית.

89
00:08:34,056 --> 00:08:37,240
אבל חשוב יותר, יש הסתכלות אחרת על הירידה הסטוכסטית

90
00:08:37,240 --> 00:08:45,504
והיא שהיא מאוד דומה לירידה הרגילה במדרון, אבל במקום לחכות ולסכם את ביטויי הנגזרת על כל m דוגמאות האימון,

91
00:08:45,504 --> 00:08:50,624
מה שאנחנו עושים הוא לקחת את ביטוי הנגזרת רק על דוגמת אימון אחת,

92
00:08:50,624 --> 00:08:54,810
ומייד מתחילים להתקדם בשיפור הפרמטרים.

93
00:08:54,810 --> 00:09:02,248
אז במקום לחכות עד שנעבור למשל על כל השלוש מאות מיליון רשומות של מפקד האוכלוסין של ארצות הברית,

94
00:09:02,248 --> 00:09:05,632
במקום להיות חייבים לעבור על כל דוגמאות ההכשרה

95
00:09:05,632 --> 00:09:09,947
לפני שנוכל לשנות קצת את הפרמטרים כדי להתקדם לעבר המינימום הגלובלי.

96
00:09:09,947 --> 00:09:14,975
בירידה סטוכסטית בשיפוע אנחנו צריכים להסתכל רק על דוגמת אימון אחת,

97
00:09:14,975 --> 00:09:22,188
וכבר אנחנו מתחילים להתקדם ולשנות את הפרמטרים, לקדם אותם לכיוון המינימום הגלובלי.

98
00:09:22,188 --> 00:09:27,558
אז הנה שוב האלגוריתם, שבו הצעד הראשון הוא לערבב את הנתונים באופן אקראי,

99
00:09:27,558 --> 00:09:35,089
והצעד השני שבו נעשית העבודה האמיתית, הוא העדכון ביחס לדוגמת אימון אחת (x(i),y(i.

100
00:09:35,089 --> 00:09:40,139
אז בואו נראה מה עושה האלגוריתם הזה לפרמטרים.

101
00:09:40,139 --> 00:09:43,467
ראינו בעבר שכאשר אנו משתמשים בירידה במדרון באצווה,

102
00:09:43,467 --> 00:09:46,331
זהו האלגוריתם שמסתכל על כל דוגמאות האימון כל פעם,

103
00:09:46,331 --> 00:09:53,397
הירידה במדרון באצווה נוטה לרשום מסלול קו די ישר בדרך למינימום הגלובלי כמו הקו הזֶה.

104
00:09:53,397 --> 00:09:59,956
בניגוד לכך, בירידה הסטוכסטית במדרון, כל איטרציה תהיה הרבה יותר מהירה,

105
00:09:59,956 --> 00:10:03,108
כי אנחנו לא צריכים לסכם את כל דוגמאות האימון.

106
00:10:03,108 --> 00:10:07,259
כל איטרציה מנסה רק להתאים יותר טוב לדוגמת אימון אחת.

107
00:10:07,259 --> 00:10:13,931
אז אם אנחנו מתחילים ירידה סטוכסטית במדרון, נניח, בנקודה הזאת,

108
00:10:13,931 --> 00:10:19,556
האיטרציה הראשונה עשויה לקחת את הפרמטרים בכיוון הזה,

109
00:10:19,556 --> 00:10:23,791
ואולי האיטרציה השניה שמסתכלת רק על הדוגמה השנייה, אז אולי פשוט במקרה,

110
00:10:23,791 --> 00:10:28,278
יש לנו פחות מזל והיא מסתובבת לכיוון לא טוב עם הפרמטרים ככה.

111
00:10:28,278 --> 00:10:33,731
באיטרציה השלישית שבה ניסינו לשנות את הפרמטרים כך שיתאימו רק לדוגמת ההדרכה השלישית,

112
00:10:33,731 --> 00:10:36,418
אולי נלך לכיוון הזה.

113
00:10:36,418 --> 00:10:42,717
ואז נתבונן בדוגמת ההדרכה הרביעית ונלך הנה. ואז הדוגמה החמישית, הדוגמה השישית, השביעית וכן הלאה.

114
00:10:42,717 --> 00:10:46,725
וכאשר אנחנו מפעילים ירידה סטוכסטית במדרון, אנחנו מגלים

115
00:10:46,725 --> 00:10:52,923
שהיא בדרך כלל תוליך את הפרמטרים לכיוון המינימום הגלובלי, אבל לא תמיד.

116
00:10:52,923 --> 00:11:00,117
היא תלך בנתיב אקראי יותר, מתפתל יותר, בדרך אל המינימום הגלובלי.

117
00:11:00,117 --> 00:11:07,630
ולמעשה כאשר אנחנו מפעילים ירידה סטוכסטית במדרון היא לא באמת מתכנסת באותו מובן כמו הירידה במדרון באצווה

118
00:11:07,630 --> 00:11:15,196
והיא מסתובבת ללא הפסק באזור כלשהו קרוב למינימום הגלובלי,

119
00:11:15,196 --> 00:11:18,740
אבל היא לא פשוט מגיעה אל המינימום הגלובלי ונשארת שם.

120
00:11:18,740 --> 00:11:21,676
אבל בפועל זו לא בעיה

121
00:11:21,676 --> 00:11:26,788
כל עוד הפרמטרים מגיעים לאזור כלשהו שם, זה קרוב מספיק למינימום הגלובלי.

122
00:11:26,788 --> 00:11:32,164
אז, מכיוון שהפרמטרים מגיעים קרוב למדי למינימום הגלובלי, זו תהיה פונקצית השערה די טובה

123
00:11:32,164 --> 00:11:36,340
ולכן בדרך כלל כשמריצים ירידה סטוכסטית במדרון

124
00:11:36,340 --> 00:11:43,658
אנו מקבלים פרמטרים על יד המינימום הגלובלי וזה מספיק טוב, למעשה, לכל מטרה מעשית.

125
00:11:43,658 --> 00:11:47,121
רק עוד פרט אחרון. בירידה סטוכסטית במדרון,

126
00:11:47,121 --> 00:11:51,099
היתה לנו הלולאה החיצונית הזו שאומרת לנו להריץ את הלולאה הפנימית מספר פעמים.

127
00:11:51,099 --> 00:11:53,892
אז כמה פעמים צריך לחזור על הלולאה החיצונית?

128
00:11:53,892 --> 00:11:59,336
בהתאם לגודל של קבוצת האימון, לפעמים להריץ את הלולאה הזו פעם אחת יכול להיות מספיק.

129
00:11:59,336 --> 00:12:02,064
ואולי עד 10 פעמים עשוי להיות מספר אופייני,

130
00:12:02,064 --> 00:12:05,852
אז יתכן שנחזור על הלולאה הפנימית מספר פעמים בין 1 לעשר פעמים.

131
00:12:05,852 --> 00:12:12,309
אז אם יש לנו ערכת נתונים באמת מסיבית כמו מפקד האוכלוסין האמריקאי הזה שזו היתה הדוגמה

132
00:12:12,309 --> 00:12:15,260
שעליה דיברתי עם 300 מיליון דוגמאות,

133
00:12:15,260 --> 00:12:19,609
ייתכן שעד שעברנו על ערכת האימון הזו פעם אחת,

134
00:12:19,609 --> 00:12:23,073
עם i שווה 1 עד 300 מיליון,

135
00:12:23,073 --> 00:12:25,720
ייתכן כי אחרי שנעבור פעם יחידה על ערכת הנתונים,

136
00:12:25,720 --> 00:12:29,872
ייתכן שכבר מצאנו השערה מושלמת.

137
00:12:29,872 --> 00:12:36,613
במקרה כזה, ייתכן שיהיה עלינו להריץ את הלולאה הפנימית רק פעם אחת אם m הוא מאוד מאוד גדול.

138
00:12:36,613 --> 00:12:43,071
אבל באופן כללי זה לוקח מספר כלשהו בין 1 ל-10 מעברים על ערכת הנתונים, אלה אולי המספרים השכיחים למדי.

139
00:12:43,071 --> 00:12:45,439
אבל למעשה זה תלוי בגודל של סדרת האימון.

140
00:12:45,439 --> 00:12:49,413
ואם נשווה את זה לירידה במדרון באצווה.

141
00:12:49,413 --> 00:12:53,905
בירידה במדרון באצווה, אחרי שעברנו על כל סדרת האימון,

142
00:12:53,905 --> 00:12:57,034
אנחנו עושים רק צעד אחד של הירידה במדרון באצווה.

143
00:12:57,034 --> 00:13:01,983
צעד תינוק אחד קטן של הירידה במדרון באצווה, אנחנו עושים רק צעד אחד קטן של הירידה במדרון באצווה.

144
00:13:01,983 --> 00:13:05,776
וזו הסיבה שהירידה הסטוכסטית במדרון יכולה להיות הרבה יותר מהירה.

145
00:13:05,776 --> 00:13:10,880
אז זה היה אלגוריתם הירידה הסטוכסטית במדרון.

146
00:13:10,880 --> 00:13:15,594
ואם תיישמו אותו, אני מקווה שזה ייתן לכם אפשרות להריץ אלגוריתמי למידה

147
00:13:15,594 --> 99:59:59,000
על ערכות נתונים הרבה יותר גדולות ולהשיג ביצועים הרבה יותר טובים.