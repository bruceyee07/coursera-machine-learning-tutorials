1
00:00:00,332 --> 00:00:04,284
בסרטונים הבאים נדבר על למידה בקנה מידה גדול.

2
00:00:04,284 --> 00:00:08,316
כלומר, אלגוריתמים שמטפלים בערכות נתונים גדולות.

3
00:00:08,316 --> 00:00:12,839
אם תסתכלו אחורה על ההיסטוריה של 5 או 10 השנים האחרונות של למידת מכונה.

4
00:00:12,839 --> 00:00:17,853
אחת הסיבות לכך שלמידה אלגוריתמית עובדת כל כך הרבה יותר טוב עכשיו מאשר אפילו נניח לפני 5 שנים,

5
00:00:17,853 --> 00:00:22,657
היא פשוט כמות הנתונים העצומה שיש לנו עכשיו, שבעזרתה אנחנו יכולים לאמן את האלגוריתמים שלנו.

6
00:00:22,657 --> 00:00:29,741
בקטעי הווידאו הבאים, נדבר על אלגוריתמים להתמודדות עם ערכות נתונים כל כך מסיביות.

7
00:00:32,926 --> 00:00:35,527
אז למה אנחנו רוצים להשתמש בערכות נתונים כל כך גדולות?

8
00:00:35,527 --> 00:00:40,564
כבר ראינו שאחת הדרכים הטובות ביותר להשיג מערכת למידת מכונה בעלת ביצועים גבוהים,

9
00:00:40,564 --> 00:00:46,168
היא לקחת אלגוריתם למידה בעל הטיה נמוכה, ולאמן אותו על הרבה נתונים.

10
00:00:46,168 --> 00:00:53,561
אז דוגמה אחת שכבר ראינו מקודם היא הדוגמה של הבחנה בין מילים מבלבלות.

11
00:00:53,561 --> 00:01:00,726
כמו, לארוחת הבוקר, אכלתי שני (2) ביצים וראינו בדוגמה הזו, הסוגים האלה של תוצאות,

12
00:01:00,726 --> 00:01:06,436
שבהם כל עוד אתה מאכיל את האלגוריתם בהרבה נתונים, נראה שהוא פועל טוב מאוד.

13
00:01:06,436 --> 00:01:10,419
אז תוצאות כאלה הן שהובילו לאימרה בלמידת מכונה, 

14
00:01:10,419 --> 00:01:15,151
שלעתים קרובות לא מי שיש לו האלגוריתם הטוב ביותר שמנצח. אלא מי שיש לו הכי הרבה נתונים.

15
00:01:15,151 --> 00:01:19,568
אז רצוי ללמוד ממגוון גדול של נתונים, לפחות כאשר אפשר להשיג ערכת נתונים גדולה.

16
00:01:19,568 --> 00:01:27,027
אבל למידה עם ערכות נתונים גדולות סובלת מבעיות ייחודיות משלה, בפרט, בעיות חישוביות.

17
00:01:27,027 --> 00:01:33,870
נניח שערכת האימון שלך היא של m שווה ל-100,000,000.

18
00:01:33,870 --> 00:01:37,934
וזה בעצם מספר די מציאותי עבור הרבה ערכות נתונים מודרניות.

19
00:01:37,934 --> 00:01:40,518
אם תסתכל על נתוני מפקד האוכלוסין של ארה"ב, אם יש, נניח,

20
00:01:40,518 --> 00:01:44,663
300 מיליון אנשים בארה"ב, אפשר הרבה פעמים להגיע למאות מיליוני רשומות.

21
00:01:44,663 --> 00:01:47,856
אם תסתכל על כמות התנועה שעוברת באתרי אינטרנט פופולריים,

22
00:01:47,856 --> 00:01:52,509
אפשר להשיג בקלות ערכות אימון שהן הרבה יותר מאשר מאות מיליוני דוגמאות.

23
00:01:52,509 --> 00:01:57,407
ונניח שאנחנו רוצים לאמן מודל רגרסיה לינארית, או אולי מודל רגרסיה לוגיסטי,

24
00:01:57,407 --> 00:02:01,692
ובמקרה הזה הנה הכלל של הירידה במדרון.

25
00:02:01,692 --> 00:02:05,372
אז אם נסתכל על מה שצריך לעשות כדי לחשב את השיפוע,

26
00:02:05,372 --> 00:02:09,992
הביטוי הזה כאן, אז כאשר m הוא מאה מיליון,

27
00:02:09,992 --> 00:02:13,976
צריך לבצע סיכום של מאה מיליון ביטויים

28
00:02:13,976 --> 00:02:18,977
על מנת לחשב את ביטויי הנגזרות האלה ולבצע צעד אחד של ירידה.

29
00:02:18,977 --> 00:02:25,627
בגלל ההוצאות החישוביות של סיכום של מאה מיליון רשומות

30
00:02:25,627 --> 00:02:28,628
רק כדי לחשב צעד אחד של ירידה במדרון,

31
00:02:28,628 --> 00:02:31,530
בסרטונים הבאים נדבר על טכניקות

32
00:02:31,530 --> 00:02:38,413
להחלפה של האלגוריתם הזה במשהו אחר או למצוא דרכים יעילות יותר לחישוב הנגזרת הזו.

33
00:02:38,413 --> 00:02:41,709
בסופה של הסדרה הזו של קטעי וידאו על למידת מכונה בקנה מידה גדול,

34
00:02:41,709 --> 00:02:47,045
אתם תדעו איך להתאים דגמים, רגרסיה ליניארית, רגרסיה לוגיסטית, רשתות עצביות וכן הלאה

35
00:02:47,045 --> 00:02:50,990
גם לערכות נתונים של, למשל, מאה מיליון דוגמאות.

36
00:02:50,990 --> 00:02:56,035
כמובן, לפני שאנחנו משקיעים את המאמץ לאימון מודל עם מאה מיליון דוגמאות,

37
00:02:56,035 --> 00:03:01,276
אנחנו צריכים לשאול את עצמנו, טוב, בעצם למה לא נשתמש רק באלף דוגמאות.

38
00:03:01,276 --> 00:03:04,923
אולי אנחנו יכולים לבחור באופן אקראי תת קבוצה של אלף דוגמאות

39
00:03:04,923 --> 00:03:10,254
מתוך מאה מיליון הדוגמאות ולאמן את האלגוריתם שלנו על אלף דוגמאות בלבד.

40
00:03:10,254 --> 00:03:16,076
אז לפני שמשקיעים את המאמץ בפיתוח בפועל של התוכנה הדרושה כדי להכשיר מודלים מסיביים אלה,

41
00:03:16,076 --> 00:03:22,461
לעתים קרובות זו תהיה בדיקת שפיות טובה, האם אימון על אלף דוגמאות יכול להיות טוב באותה מידה.

42
00:03:22,461 --> 00:03:29,731
הדרך לבדוק את ההיגיון של האם שימוש במערך אימון קטן בהרבה יכול לתת תוצאה טובה באותה איכות,

43
00:03:29,731 --> 00:03:33,958
כלומר האם שימוש ב-m הרבה יותר קטן, גודל קבוצת אימון שווה 1000,

44
00:03:33,958 --> 00:03:37,797
עשוי לתת תוצאה טובה בדיוק באותה מידה, היא השיטה הרגילה של שרטוט של עקומות למידה,

45
00:03:37,797 --> 00:03:46,872
אז אם נשרטט את עקומות הלמידה ואם האופטימיזציה של סדרת האימון שלך נראית ככה,

46
00:03:46,872 --> 00:03:49,553
זה J-אימון של θ.

47
00:03:49,553 --> 00:03:56,422
ואם האופטימיזציה של סדרת האימות, (Jcv(θ, נראית כך,

48
00:03:56,422 --> 00:04:00,310
אז נראה שזהו אלגוריתם למידה בעל שונות גבוהה,

49
00:04:00,310 --> 00:04:05,913
וכך נדע יותר בוודאות שהוספת דוגמאות אימון תשפר את הביצועים.

50
00:04:05,913 --> 00:04:10,462
לעומת זאת, אם נשרטט את עקומות הלמידה,

51
00:04:10,462 --> 00:04:20,339
ומטרת האימון נראית כך, ומטרת סדרת האימות נראית כך,

52
00:04:20,339 --> 00:04:24,292
אז זה נראה כמו אלגוריתם למידה קלאסי בעל הטיה גבוהה.

53
00:04:24,292 --> 00:04:28,084
ובמקרה השני, אם נשרטט את זה עד,

54
00:04:28,084 --> 00:04:33,437
נניח, m שווה 1000, זה כאן m שווה 500 עד m שווה 1000,

55
00:04:33,437 --> 00:04:39,400
אז זה נראה לא סביר שהגדלת m ל-100 מיליון תשפר בהרבה את התוצאה

56
00:04:39,400 --> 00:04:42,736
ואז יהיה בסדר גמור להישאר עם m שווה 1000,

57
00:04:42,736 --> 00:04:47,000
במקום להשקיע הרבה מאמץ כדי להבין איך לגרום לאלגוריתם לעבוד בקנה מידה גדול יותר.

58
00:04:47,000 --> 00:04:51,029
כמובן, אם אנחנו במצב המוצג על ידי הגרף מימין,

59
00:04:51,029 --> 00:04:53,885
אז דבר אחד טבעי לעשות יהיה להוסיף תכונות נוספות,

60
00:04:53,885 --> 00:04:58,484
או להוסיף יחידה נסתרת נוספת לרשת העצבית וכיוצא בכך,

61
00:04:58,484 --> 00:05:04,627
כך שנגיע למצב יותר קרוב לזה שמשמאל, שבו הציור מראה אולי m שווה 1000,

62
00:05:04,627 --> 00:05:09,553
ואחרי כן נקבל יותר ביטחון שהניסיון לשפר את התשתית ולשנות את האלגוריתם

63
00:05:09,553 --> 00:05:14,735
כך שיעבוד עם הרבה יותר מאלף דוגמאות עשוי להיות ניצול יעיל של הזמן.

64
00:05:14,735 --> 00:05:19,642
אז בלמידת מכונה בקנה מידה גדול, היינו רוצים למצוא דרכים סבירות מבחינה חישובית,

65
00:05:19,642 --> 00:05:24,026
או דרכים יעילות מבחינה חישובית, להתמודד עם ערכות נתונים גדולות מאוד.

66
00:05:24,026 --> 00:05:26,826
בסרטונים הבאים נראה שני רעיונות עיקריים.

67
00:05:26,826 --> 00:05:33,464
הראשון נקרא ירידה סטוכסטית במדרון והשני נקרא מיפוי וצמצום (map-reduce), לטיפול בערכות נתונים גדולים מאוד.

68
00:05:33,464 --> 00:05:39,986
ואחרי שנלמד על השיטות האלה, אני תקווה שזה יאפשר לכם להרחיב את אלגוריתמי הלמידה לערכות נתונים גדולות

69
00:05:39,986 --> 00:05:43,986
ולאפשר לכם להשיג ביצועים טובים יותר על יישומים רבים ושונים.