בסרטונים הבאים נדבר על למידה בקנה מידה גדול. כלומר, אלגוריתמים שמטפלים בערכות נתונים גדולות. אם תסתכלו אחורה על ההיסטוריה של 5 או 10 השנים האחרונות של למידת מכונה. אחת הסיבות לכך שלמידה אלגוריתמית עובדת כל כך הרבה יותר טוב עכשיו מאשר אפילו נניח לפני 5 שנים, היא פשוט כמות הנתונים העצומה שיש לנו עכשיו, שבעזרתה אנחנו יכולים לאמן את האלגוריתמים שלנו. בקטעי הווידאו הבאים, נדבר על אלגוריתמים להתמודדות עם ערכות נתונים כל כך מסיביות. אז למה אנחנו רוצים להשתמש בערכות נתונים כל כך גדולות? כבר ראינו שאחת הדרכים הטובות ביותר להשיג מערכת למידת מכונה בעלת ביצועים גבוהים, היא לקחת אלגוריתם למידה בעל הטיה נמוכה, ולאמן אותו על הרבה נתונים. אז דוגמה אחת שכבר ראינו מקודם היא הדוגמה של הבחנה בין מילים מבלבלות. כמו, לארוחת הבוקר, אכלתי שני (2) ביצים וראינו בדוגמה הזו, הסוגים האלה של תוצאות, שבהם כל עוד אתה מאכיל את האלגוריתם בהרבה נתונים, נראה שהוא פועל טוב מאוד. אז תוצאות כאלה הן שהובילו לאימרה בלמידת מכונה, שלעתים קרובות לא מי שיש לו האלגוריתם הטוב ביותר שמנצח. אלא מי שיש לו הכי הרבה נתונים. אז רצוי ללמוד ממגוון גדול של נתונים, לפחות כאשר אפשר להשיג ערכת נתונים גדולה. אבל למידה עם ערכות נתונים גדולות סובלת מבעיות ייחודיות משלה, בפרט, בעיות חישוביות. נניח שערכת האימון שלך היא של m שווה ל-100,000,000. וזה בעצם מספר די מציאותי עבור הרבה ערכות נתונים מודרניות. אם תסתכל על נתוני מפקד האוכלוסין של ארה"ב, אם יש, נניח, 300 מיליון אנשים בארה"ב, אפשר הרבה פעמים להגיע למאות מיליוני רשומות. אם תסתכל על כמות התנועה שעוברת באתרי אינטרנט פופולריים, אפשר להשיג בקלות ערכות אימון שהן הרבה יותר מאשר מאות מיליוני דוגמאות. ונניח שאנחנו רוצים לאמן מודל רגרסיה לינארית, או אולי מודל רגרסיה לוגיסטי, ובמקרה הזה הנה הכלל של הירידה במדרון. אז אם נסתכל על מה שצריך לעשות כדי לחשב את השיפוע, הביטוי הזה כאן, אז כאשר m הוא מאה מיליון, צריך לבצע סיכום של מאה מיליון ביטויים על מנת לחשב את ביטויי הנגזרות האלה ולבצע צעד אחד של ירידה. בגלל ההוצאות החישוביות של סיכום של מאה מיליון רשומות רק כדי לחשב צעד אחד של ירידה במדרון, בסרטונים הבאים נדבר על טכניקות להחלפה של האלגוריתם הזה במשהו אחר או למצוא דרכים יעילות יותר לחישוב הנגזרת הזו. בסופה של הסדרה הזו של קטעי וידאו על למידת מכונה בקנה מידה גדול, אתם תדעו איך להתאים דגמים, רגרסיה ליניארית, רגרסיה לוגיסטית, רשתות עצביות וכן הלאה גם לערכות נתונים של, למשל, מאה מיליון דוגמאות. כמובן, לפני שאנחנו משקיעים את המאמץ לאימון מודל עם מאה מיליון דוגמאות, אנחנו צריכים לשאול את עצמנו, טוב, בעצם למה לא נשתמש רק באלף דוגמאות. אולי אנחנו יכולים לבחור באופן אקראי תת קבוצה של אלף דוגמאות מתוך מאה מיליון הדוגמאות ולאמן את האלגוריתם שלנו על אלף דוגמאות בלבד. אז לפני שמשקיעים את המאמץ בפיתוח בפועל של התוכנה הדרושה כדי להכשיר מודלים מסיביים אלה, לעתים קרובות זו תהיה בדיקת שפיות טובה, האם אימון על אלף דוגמאות יכול להיות טוב באותה מידה. הדרך לבדוק את ההיגיון של האם שימוש במערך אימון קטן בהרבה יכול לתת תוצאה טובה באותה איכות, כלומר האם שימוש ב-m הרבה יותר קטן, גודל קבוצת אימון שווה 1000, עשוי לתת תוצאה טובה בדיוק באותה מידה, היא השיטה הרגילה של שרטוט של עקומות למידה, אז אם נשרטט את עקומות הלמידה ואם האופטימיזציה של סדרת האימון שלך נראית ככה, זה J-אימון של θ. ואם האופטימיזציה של סדרת האימות, (Jcv(θ, נראית כך, אז נראה שזהו אלגוריתם למידה בעל שונות גבוהה, וכך נדע יותר בוודאות שהוספת דוגמאות אימון תשפר את הביצועים. לעומת זאת, אם נשרטט את עקומות הלמידה, ומטרת האימון נראית כך, ומטרת סדרת האימות נראית כך, אז זה נראה כמו אלגוריתם למידה קלאסי בעל הטיה גבוהה. ובמקרה השני, אם נשרטט את זה עד, נניח, m שווה 1000, זה כאן m שווה 500 עד m שווה 1000, אז זה נראה לא סביר שהגדלת m ל-100 מיליון תשפר בהרבה את התוצאה ואז יהיה בסדר גמור להישאר עם m שווה 1000, במקום להשקיע הרבה מאמץ כדי להבין איך לגרום לאלגוריתם לעבוד בקנה מידה גדול יותר. כמובן, אם אנחנו במצב המוצג על ידי הגרף מימין, אז דבר אחד טבעי לעשות יהיה להוסיף תכונות נוספות, או להוסיף יחידה נסתרת נוספת לרשת העצבית וכיוצא בכך, כך שנגיע למצב יותר קרוב לזה שמשמאל, שבו הציור מראה אולי m שווה 1000, ואחרי כן נקבל יותר ביטחון שהניסיון לשפר את התשתית ולשנות את האלגוריתם כך שיעבוד עם הרבה יותר מאלף דוגמאות עשוי להיות ניצול יעיל של הזמן. אז בלמידת מכונה בקנה מידה גדול, היינו רוצים למצוא דרכים סבירות מבחינה חישובית, או דרכים יעילות מבחינה חישובית, להתמודד עם ערכות נתונים גדולות מאוד. בסרטונים הבאים נראה שני רעיונות עיקריים. הראשון נקרא ירידה סטוכסטית במדרון והשני נקרא מיפוי וצמצום (map-reduce), לטיפול בערכות נתונים גדולים מאוד. ואחרי שנלמד על השיטות האלה, אני תקווה שזה יאפשר לכם להרחיב את אלגוריתמי הלמידה לערכות נתונים גדולות ולאפשר לכם להשיג ביצועים טובים יותר על יישומים רבים ושונים.