באלגוריתמים רבים של למידה, ביניהם רגרסיה לינארית, רגרסיה לוגיסטית ורשתות עצביות, הדרך בה אנו מפתחים את האלגוריתם היא על ידי יצירה של פונקצית עלות או מציאה של יעד אופטימיזציה. ואז שימוש באלגוריתם כמו ירידה במדרון כדי למזער את פונקצית העלות. כשיש לנו סדרת אימון גדולה מאוד, הירידה במדרון הופכת להיות תהליך חישובי יקר מאוד. בסרטון הזה נדבר על שינוי באלגוריתם הבסיסי של ירידה במדרון הנקרא "ירידה סטוכסטית במדרון" (Stochastic gradient), שיאפשר לנו להרחיב את האלגוריתמים הללו לקבוצות אימון גדולות בהרבה. נניח שאתה מאמן מודל של רגרסיה ליניארית באמצעות ירידה במדרון. כתזכורת מהירה, פונקציית ההשערה תיראה כך ופונקציית העלות תיראה כך, שזה הסכום של מחצית השגיאה הריבועית הממוצעת של ההשערה ב-m דוגמאות האימון, ופונקציית העלות שכבר ראינו נראית כמו מין פונקציה כזו בצורת קערה. דהיינו, כשמשרטטים אותה כפונקציה של הפרמטרים θ0 ו-θ1, פונקציית העלות J נראית בצורת קערה. והירידה במדרון נראית ככה, כשבלולאה הפנימית של הירידה במדרון אנחנו מעדכנים שוב ושוב את הפרמטרים θ באמצעות הביטוי הזה. עכשיו מכאן והלאה בווידאו הזה, אני מתכוון להמשיך ולהשתמש ברגרסיה ליניארית כדוגמה חיה. אבל הרעיונות כאן, הרעיונות של ירידה סטוכסטית במדרון הם כלליים לחלוטין וחלים גם על אלגוריתמי למידה אחרים כמו רגרסיה לוגיסטית, רשתות עצביות ואלגוריתמים אחרים המבוססים על אימון על ידי ירידה הדרגתית על קבוצת הכשרה ספציפית. אז הנה תמונה של מה עושה ירידה במדרון, אם הפרמטרים מאותחלים לנקודה כאן אז כשמפעילים ירידה במדרון, האיטרציות המתקדמות של הירידה במדרון ימשכו את הפרמטרים לכיוון המינימום הגלובלי. הם יסמנו מסלול שנראה כמו זה ויתכווננו די ישירות אל המינימום הגלובלי. עכשיו, הבעיה עם ירידה הדרגתית היא שאם m הוא גדול אז החישוב של הביטוי הזה של הנגזרת יכול להיות יקר מאוד, כי הוא מערב סיכום של איבר מכל m הדוגמאות. אז אם m הוא 300 מיליון, נניח. בארצות הברית יש כ-300 מיליון אנשים. אז הנתונים של מפקד האוכלוסין בארה"ב עשויים להיות בסדר הגודל הזה. אז כשרוצים להתאים מודל של רגרסיה ליניארית לזה צריך לסכם 300 מיליון רשומות. וזה יקר מאוד. אחד השמות של האלגוריתם הזה הוא, הגרסה המסוימת הזו של ירידה במדרון, נקראת גם ירידה במדרון באצווה (batch). והמונח אצווה מתייחס לעובדה שאנחנו מסתכלים על כל דוגמאות ההכשרה כל פעם. אנחנו קוראים לזה כעין אצווה של כל דוגמאות האימון. זה אולי בעצם לא השם המתאים ביותר, אבל כך קוראים לזה המקצוענים של למידת מכונה לגרסה המסוימת הזו של ירידה הדרגתית. ואם תדמיינו באמת שיש לכם 300 מיליון רשומות של מפקד האוכלוסין מאוחסנים בדיסק. אז האופן שבו עובד האלגוריתם הזה הוא שצריך לקרוא לתוך זיכרון המחשב את כל 300 מיליון הרשומות כדי לחשב את ביטויי הנגזרות. צריכים להזרים את כל הרשומות האלה דרך המחשב, כי אי אפשר לאחסן את כל הרשומות בזיכרון של המחשב. אז צריכים לקרוא אותם לאט לאט ולאגור את הסכום על מנת לחשב את הנגזרת. ואז לאחר שעשינו את כל העבודה הזאת, זה מאפשר לנו בסך הכל לעשות צעד אחד של ירידה במדרון. ואז צריך לעשות את כל העניין הזה מחדש. לסרוק את כל 300 מיליון הרשומות, לאגור את הסכומים האלה. ולאחר שעשינו שוב את כל העבודה, אפשר לעשות עוד צעד קטן של ירידה במדרון. ואז לעשות את זה שוב. ואז אפשר לעשות עוד צעד שלישי. וכולי. אז זה הולך לקחת המון זמן עד שהאלגוריתם הזה יתכנס. בניגוד לירידה במדרון באצווה, מה שאנחנו הולכים לעשות הוא לבנות אלגוריתם אחר, שבו לא צריך להסתכל על כל דוגמאות ההכשרה בכל איטרציה ואיטרציה, אלא צריך להסתכל רק על דוגמת אימון אחת לכל איטרציה. לפני שנעבור לאלגוריתם החדש, הנה שוב אלגוריתם הירידה במדרון באצווה שאני כותב אותו שוב כשכאן נמצאת פונקציית העלות וכאן שלב העדכון וכמובן המונח הזה כאן, שמשתמשים בו בשלב הירידה, הוא הנגזרת החלקית ביחס לפרמטרים θj של מטרת האופטימיזציה שלנו, J-אימון של θ. עכשיו בואו נסתכל על האלגוריתם היעיל יותר שעובד יותר טוב במערכי נתונים גדולים. כדי להסביר את האלגוריתם הנקרא "ירידה סטוכסטית במדרון" (Stochastic gradient descent), נגדיר את פונקציית העלות בצורה שונה במקצת, נגדיר את העלות של הפרמטר θ ביחס לדוגמת אימון (x(i),y(i כחצי המרחק בריבוע בין ההשערה לדוגמה (x(i),y(i. אז המונח הזה של פונקצית העלות בעצם מודד את טיב ההשערה ביחס לדוגמא הבודדת (x(i),y(i. עכשיו אפשר לראות שניתן לכתוב את פונקצית העלות הכוללת J-אימון בצורה השקולה הזו. J-אימון היא פשוט הממוצע על m דוגמאות האימון של העלות של ההשערה על הדוגמא (x(i),y(i. חמוש בהבנה הזו של פונקציית העלות עבור רגרסיה ליניארית, תרשו לי עכשיו לכתוב מה עושה הירידה הסטוכסטית בשיפוע. הצעד הראשון של ירידה סטוכסטית במדרון היא לערבב אקראית את סדרת הנתונים. כשאני אומר אקראית, אני מתכוון פשוט לסדר מחדש באופן אקראי את m דוגמאות האימון. זה מין צעד סטנדרטי של עיבוד מראש, תיכף נחזור לזה. אבל העבודה העיקרית של הירידה הסטוכסטית בשיפוע נעשית אחר כך בצעדים הבאים. אנחנו הולכים לחזור על זה בלולאה עבור i שווה מ-1 עד m. נעבור שוב ושוב על דוגמאות ההדרכה ונבצע את העדכון הבא. נעדכן את הפרמטר θj ל-θj פחות α כפול ( (h(x(i)) - y(i ) כפול x(i)j. וכרגיל נעשה את העדכון הזה לכל הערכים של j. עכשיו, אתם רואים שהמונח הזה כאן הוא בדיוק מה שהיה לנו בתוך הסיכום עבור הירידה במדרון באצווה. למעשה, למתמטיקאים ביניכם ניתן להראות שהמונח הזה כאן, שווה לנגזרת החלקית ביחס לפרמטר θj של העלות של הפרמטרים θ, על (x(i),y(i. כשהעלות היא כמובן הדבר הזה שהוגדר מקודם. ורק כדי לסכם את האלגוריתם, הרשו לי לסגור את הסוגריים המסולסלים שלי כאן. אז מה שעושה הירידה הסטוכסטית בשיפוע הוא בעצם מעבר על כל דוגמאות ההכשרה. בפעם הראשונה היא מסתכלת על דוגמת האימון הראשונה (1)x(1),y. ואז מהסתכלות רק על הדוגמה הראשונה, היא תעשה מין צעד קטן כזה של ירידה במדרון הדרגתית רק לפי העלות של דוגמת ההכשרה הראשונה. במילים אחרות, אנחנו הולכים להסתכל רק על הדוגמה הראשונה ולשנות את הפרמטרים טיפטיפה כדי להתאים רק לדוגמת האימון הראשונה קצת יותר טוב. ואחרי זה, בתוך הלולאה הפנימית אנחנו נעבור לדוגמת האימון השנייה. ומה שנעשה שם הוא עוד צעד קטן במרחב הפרמטרים, דהיינו לשנות את הפרמטרים רק במקצת כדי לנסות להתאים רק לדוגמת ההכשרה השנייה קצת יותר טוב. ואחרי שעשינו את זה, עוברים לדוגמת האימון השלישית. לשנות את הפרמטרים רק במקצת כדי לנסות להתאים רק לדוגמת ההכשרה השלישית קצת יותר טוב. וכן הלאה עד שעברנו על כל סדרת האימון. ואז הלולאה החיצונית עשויה לגרום לו לעשות מספר מעברים כאלה על כל סדרת האימון. ההסתכלות הזו על ירידה סטוכסטית במדרון גם מסבירה למה רצינו להתחיל בערבוב אקראי של סדרת הנתונים. זה מבטיח לנו שכאשר אנו עוברים על סדרת האימון כאן, אנחנו עוברים על דוגמאות ההכשרה בסדר אקראי. לפי המצב, אם הנתונים כבר הגיעו ממוינים באופן אקראי או אם הם הגיעו במקור ממוינים, למעשה זה רק מאיץ את השינויים של הירידה הסטוכסטית רק באופן מינורי. אז לשם הבטיחות, זה בדרך כלל טוב יותר לערבב אקראית את סדרת הנתונים אם אנחנו לא בטוחים אם הם הגיעו ממוינים אקראית. אבל חשוב יותר, יש הסתכלות אחרת על הירידה הסטוכסטית והיא שהיא מאוד דומה לירידה הרגילה במדרון, אבל במקום לחכות ולסכם את ביטויי הנגזרת על כל m דוגמאות האימון, מה שאנחנו עושים הוא לקחת את ביטוי הנגזרת רק על דוגמת אימון אחת, ומייד מתחילים להתקדם בשיפור הפרמטרים. אז במקום לחכות עד שנעבור למשל על כל השלוש מאות מיליון רשומות של מפקד האוכלוסין של ארצות הברית, במקום להיות חייבים לעבור על כל דוגמאות ההכשרה לפני שנוכל לשנות קצת את הפרמטרים כדי להתקדם לעבר המינימום הגלובלי. בירידה סטוכסטית בשיפוע אנחנו צריכים להסתכל רק על דוגמת אימון אחת, וכבר אנחנו מתחילים להתקדם ולשנות את הפרמטרים, לקדם אותם לכיוון המינימום הגלובלי. אז הנה שוב האלגוריתם, שבו הצעד הראשון הוא לערבב את הנתונים באופן אקראי, והצעד השני שבו נעשית העבודה האמיתית, הוא העדכון ביחס לדוגמת אימון אחת (x(i),y(i. אז בואו נראה מה עושה האלגוריתם הזה לפרמטרים. ראינו בעבר שכאשר אנו משתמשים בירידה במדרון באצווה, זהו האלגוריתם שמסתכל על כל דוגמאות האימון כל פעם, הירידה במדרון באצווה נוטה לרשום מסלול קו די ישר בדרך למינימום הגלובלי כמו הקו הזֶה. בניגוד לכך, בירידה הסטוכסטית במדרון, כל איטרציה תהיה הרבה יותר מהירה, כי אנחנו לא צריכים לסכם את כל דוגמאות האימון. כל איטרציה מנסה רק להתאים יותר טוב לדוגמת אימון אחת. אז אם אנחנו מתחילים ירידה סטוכסטית במדרון, נניח, בנקודה הזאת, האיטרציה הראשונה עשויה לקחת את הפרמטרים בכיוון הזה, ואולי האיטרציה השניה שמסתכלת רק על הדוגמה השנייה, אז אולי פשוט במקרה, יש לנו פחות מזל והיא מסתובבת לכיוון לא טוב עם הפרמטרים ככה. באיטרציה השלישית שבה ניסינו לשנות את הפרמטרים כך שיתאימו רק לדוגמת ההדרכה השלישית, אולי נלך לכיוון הזה. ואז נתבונן בדוגמת ההדרכה הרביעית ונלך הנה. ואז הדוגמה החמישית, הדוגמה השישית, השביעית וכן הלאה. וכאשר אנחנו מפעילים ירידה סטוכסטית במדרון, אנחנו מגלים שהיא בדרך כלל תוליך את הפרמטרים לכיוון המינימום הגלובלי, אבל לא תמיד. היא תלך בנתיב אקראי יותר, מתפתל יותר, בדרך אל המינימום הגלובלי. ולמעשה כאשר אנחנו מפעילים ירידה סטוכסטית במדרון היא לא באמת מתכנסת באותו מובן כמו הירידה במדרון באצווה והיא מסתובבת ללא הפסק באזור כלשהו קרוב למינימום הגלובלי, אבל היא לא פשוט מגיעה אל המינימום הגלובלי ונשארת שם. אבל בפועל זו לא בעיה כל עוד הפרמטרים מגיעים לאזור כלשהו שם, זה קרוב מספיק למינימום הגלובלי. אז, מכיוון שהפרמטרים מגיעים קרוב למדי למינימום הגלובלי, זו תהיה פונקצית השערה די טובה ולכן בדרך כלל כשמריצים ירידה סטוכסטית במדרון אנו מקבלים פרמטרים על יד המינימום הגלובלי וזה מספיק טוב, למעשה, לכל מטרה מעשית. רק עוד פרט אחרון. בירידה סטוכסטית במדרון, היתה לנו הלולאה החיצונית הזו שאומרת לנו להריץ את הלולאה הפנימית מספר פעמים. אז כמה פעמים צריך לחזור על הלולאה החיצונית? בהתאם לגודל של קבוצת האימון, לפעמים להריץ את הלולאה הזו פעם אחת יכול להיות מספיק. ואולי עד 10 פעמים עשוי להיות מספר אופייני, אז יתכן שנחזור על הלולאה הפנימית מספר פעמים בין 1 לעשר פעמים. אז אם יש לנו ערכת נתונים באמת מסיבית כמו מפקד האוכלוסין האמריקאי הזה שזו היתה הדוגמה שעליה דיברתי עם 300 מיליון דוגמאות, ייתכן שעד שעברנו על ערכת האימון הזו פעם אחת, עם i שווה 1 עד 300 מיליון, ייתכן כי אחרי שנעבור פעם יחידה על ערכת הנתונים, ייתכן שכבר מצאנו השערה מושלמת. במקרה כזה, ייתכן שיהיה עלינו להריץ את הלולאה הפנימית רק פעם אחת אם m הוא מאוד מאוד גדול. אבל באופן כללי זה לוקח מספר כלשהו בין 1 ל-10 מעברים על ערכת הנתונים, אלה אולי המספרים השכיחים למדי. אבל למעשה זה תלוי בגודל של סדרת האימון. ואם נשווה את זה לירידה במדרון באצווה. בירידה במדרון באצווה, אחרי שעברנו על כל סדרת האימון, אנחנו עושים רק צעד אחד של הירידה במדרון באצווה. צעד תינוק אחד קטן של הירידה במדרון באצווה, אנחנו עושים רק צעד אחד קטן של הירידה במדרון באצווה. וזו הסיבה שהירידה הסטוכסטית במדרון יכולה להיות הרבה יותר מהירה. אז זה היה אלגוריתם הירידה הסטוכסטית במדרון. ואם תיישמו אותו, אני מקווה שזה ייתן לכם אפשרות להריץ אלגוריתמי למידה על ערכות נתונים הרבה יותר גדולות ולהשיג ביצועים הרבה יותר טובים.