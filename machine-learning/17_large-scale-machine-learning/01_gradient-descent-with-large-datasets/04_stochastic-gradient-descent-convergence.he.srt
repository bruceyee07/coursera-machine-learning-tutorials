1
00:00:00,493 --> 00:00:03,492
אז עכשיו אתם מכירים את האלגוריתם לירידה במדרון סטוכסטית.

2
00:00:03,492 --> 00:00:09,907
אבל כאשר מפעילים את האלגוריתם, איך ניתן לוודא
שהוא נקי לחלוטין מבאגים ושהוא מתכנס בסדר?

3
00:00:09,907 --> 00:00:15,813
ולא פחות חשוב, איך ניתן לכוון את שיעור הלמידה אלפא
עם ירידה במדרון סטוכסטית.

4
00:00:15,813 --> 00:00:25,950
בסרטון זה נדבר על כמה טכניקות לעשות את הדברים האלה,
על מנת לוודא שהוא מתכנס וגם על בחירת שיעור הלימוד אלפא.

5
00:00:25,950 --> 00:00:30,600
מקודם כאשר עסקנו בירידה במדרון באצווה,
הדרך הסטנדרטית שלנו לוודא

6
00:00:30,600 --> 00:00:36,493
שהירידה בשיפוע מתכנסת היתה על ידי שרטוט המחיר הכולל
של פונקציית העלות כפונקציה של מספר האיטרציות.

7
00:00:36,493 --> 00:00:44,366
אז זו היתה פונקציית העלות ואנחנו היינו מוודאים
שפונקציית העלות הזו יורדת בכל איטרציה.

8
00:00:44,366 --> 00:00:50,438
כאשר גודל סדרת האימון הוא קטן, יכולנו לעשות את זה
כי יכולנו לחשב את הסכום די ביעילות.

9
00:00:50,438 --> 00:00:57,950
אבל כאשר יש לנו סדרת אימון ענקית,
אז לא נרצה להשהות כל פעם את האלגוריתם.

10
00:00:57,950 --> 00:01:04,045
לא נרצה להשהות את הירידה ההדרגתית הסטוכסטית
בכל כמה איטרציות כדי לחשב את פונקציית העלות הזו

11
00:01:04,045 --> 00:01:07,442
כי החישוב דורש סיכום של המחיר
של כל סדרת האימון הגדולה שלנו.

12
00:01:07,442 --> 00:01:12,466
וכל הנקודה של הירידה במדרון הסטוכסטית היתה
שרצינו להתחיל להתקדם

13
00:01:12,466 --> 00:01:19,130
אחרי שהסתכלנו רק על דוגמה אחת
מבלי שנצטרך לסרוק מפעם לפעם את כל סט האימון שלנו

14
00:01:19,130 --> 00:01:25,583
באמצע ריצת האלגוריתם, רק כדי לחשב דברים
כמו המחיר הכולל של פונקציית העלות של כל סדרת האימון.

15
00:01:25,583 --> 00:01:32,472
אז עבור ירידה במדרון סטוכסטית, כדי לבדוק האם האלגוריתם מתכנס,
הנה מה שאנחנו יכולים לעשות במקום חישוב המחיר.

16
00:01:32,472 --> 00:01:36,367
בואו ניקח את הגדרת העלות שהיתה לנו בעבר.

17
00:01:36,367 --> 00:01:42,647
אז העלות של וקטור הפרמטרים תטא לגבי דוגמת אימון אחת
הוא פשוט חצי ריבוע השגיאה על אותה דוגמה.

18
00:01:42,647 --> 00:01:49,754
אז בזמן שאלגוריתם הירידה הסטוכסטית במדרון מתאמן ולומד,
ממש לפני שאנחנו מתחילים להתאמן או ללמוד מדוגמה ספציפית,

19
00:01:49,754 --> 00:01:54,601
כי בירידה סטוכסטית במדרון אנחנו מסתכלים
על הדוגמאות xi, yi, לפי הסדר,

20
00:01:54,601 --> 00:01:57,329
ואז בערך עושים עדכון קטן לפי מה שלמדנו בדוגמה הזו.

21
00:01:57,329 --> 00:02:04,095
ואז אנחנו הולכים לדוגמה הבאה, x אינדקס i פלוס 1, y אינדקס i פלוס 1, וכן הלאה, נכון?

22
00:02:04,095 --> 00:02:05,880
זה מה שעושה ירידה סטוכסטית במדרון.

23
00:02:05,880 --> 00:02:15,024
אז בזמן שהאלגוריתם מתבונן בדוגמה xi, yi,
אבל לפני שהוא מעדכן את סט הפרמטרים תטא

24
00:02:15,024 --> 00:02:20,255
לפי הדוגמה הזו, הבה ונחשב את העלות של הדוגמה.

25
00:02:20,255 --> 00:02:23,577
אני אומר את אותו הדבר שוב, אבל במילים מעט שונות.

26
00:02:23,577 --> 00:02:33,294
הירידה הסטוכסטית במדרון סורקת את סדרת האימון שלנו
וממש לפני שאנחנו מעדכנים את תטא באמצעות דוגמת אימון ספציפית (x(i פסיק (y(i,

27
00:02:33,294 --> 00:02:38,198
אנחנו נחשב עד כמה טובה ההשערה שלנו על דוגמת האימון הנוכחית.

28
00:02:38,198 --> 00:02:43,852
ואנחנו רוצים לעשות את זה לפני שמעדכנים את תטא, כי אם כבר עדכנו את תטא באמצעות הדוגמא הזו,

29
00:02:43,852 --> 00:02:49,061
אתם מבינים, אז העדכון יכול לגרום למחיר להיראות בדוגמה הזו יותר נמוך מאשר האמת האובייקטיבית.

30
00:02:49,061 --> 00:02:57,438
לבסוף, כדי לבדוק את ההתכנסות של ירידה סטוכסטית במדרון,
מה שאנחנו יכולים לעשות הוא כל, נניח, אלף איטרציות,

31
00:02:57,438 --> 00:03:01,511
אנחנו יכולים לחשב ולשרטט את העלות של מה שחישבנו בשלב הקודם.

32
00:03:01,511 --> 00:03:07,450
אנחנו יכולים לעשות גרף של העלויות האלה בממוצע על פני, למשל,
אלף הדוגמאות האחרונות שחושבו על ידי האלגוריתם.

33
00:03:07,450 --> 00:03:12,714
ואם נעשה את זה, זה ייתן לנו הערכה חיה של כמה מוצלח האלגוריתם.

34
00:03:12,714 --> 00:03:17,049
על 1000 דוגמאות ההכשרה האחרונות שהאלגוריתם שלך ראה.

35
00:03:17,049 --> 00:03:23,974
אז במקום לחשב מדי פעם את הפונקציה J-train מה שמצריך לסרוק את כל סט האימון<u></u>

36
00:03:23,974 --> 00:03:27,973
אז באמצעות התהליך הזה, כחלק מהירידה ההדרגתית,

37
00:03:27,973 --> 00:03:32,965
זה לא עולה הרבה לחשב גם את העלויות האלה לפני עדכון סט הפרמטרים תטא.

38
00:03:32,965 --> 00:03:40,276
וכל מה שאנחנו עושים הוא בערך כל אלף איטרציות
אנחנו פשוט עושים ממוצע של 1,000 העלויות האחרונות שחישבנו ושמים את זה על גרף.

39
00:03:40,276 --> 00:03:47,537
ועל ידי הסתכלות על הגרפים האלה, זה מאפשר לנו לבדוק אם הירידה הסטוכסטית בשיפוע מתכנסת.

40
00:03:47,537 --> 00:03:51,708
אז הנה כמה דוגמאות של איך נראים הגרפים האלה.

41
00:03:51,708 --> 00:03:55,519
נניח ששרטטת את ממוצע העלות של אלף הדוגמאות האחרונות.

42
00:03:55,519 --> 00:04:01,073
מאחר ואלה ממוצעים רק על אלף דוגמאות, הם יהיו קצת רועשים

43
00:04:01,073 --> 00:04:03,873
ולכן הממוצע לא יכול לרדת ממש בכל איטרציה.

44
00:04:03,873 --> 00:04:07,828
אז נקבל צורה שנראית ככה, אז הגרף "רועש"

45
00:04:07,828 --> 00:04:11,721
כי הוא מתאר ממוצע רק על תת-קבוצה קטנה, נאמר אלף דוגמאות אימון.

46
00:04:11,721 --> 00:04:17,283
אז אם אתם מקבלים גרף שנראה ככה, אתם יודעים שהאלגוריתם עושה עבודה די טובה,

47
00:04:17,283 --> 00:04:24,195
ואז אולי איפה שזה נראה כאילו בהתחלה העלות ירדה ואז הגיעה לאיזו רמה מסוימת
והגרף משתטח, נניח החל מסביב לנקודה הזו,

48
00:04:24,195 --> 00:04:29,603
זה נראה שזו בעצם העלות, ונראה ששלב הלמידה של האלגוריתם התכנס.

49
00:04:29,603 --> 00:04:34,252
אולי כדאי לכם לנסות להשתמש בקצב למידה קטן יותר,
ואז משהו שאתם עשויים לראות

50
00:04:34,252 --> 00:04:39,229
הוא שהאלגוריתם עשוי בתחילה ללמוד לאט יותר ולכן העלות יורדת לאט יותר.

51
00:04:39,229 --> 00:04:47,585
אבל אז בסופו של דבר בגלל שיעור הלמידה הנמוך זה נהיה אפשרי
עבור האלגוריתם להגיע אולי לפתרון מעט יותר טוב.

52
00:04:47,585 --> 00:04:53,426
אז הקו האדום עשוי לייצג את ההתנהגות של ירידה הסטוכסטית במדרון באמצעות שיעור נמוך יותר ואיטי יותר.

53
00:04:53,426 --> 00:05:00,594
והסיבה לכך היא שירידה הסטוכסטית לא מתכנסת למינימום הגלובלי,

54
00:05:00,594 --> 00:05:05,068
אלא מה שהיא עושה הוא שהפרמטרים יתנדנדו קצת באזור המינימום הגלובלי.

55
00:05:05,068 --> 00:05:09,231
וכך בזכות השימוש בשיעור למידה קטן יותר, אנחנו גורמים לכך שהתנודות תהיינה קטנות יותר.

56
00:05:09,231 --> 00:05:12,896
ולפעמים ההבדל הקטן הזה יהיה זניח

57
00:05:12,896 --> 00:05:19,686
ולפעמים עם ההבדל הקטן הזה נוכל לקבל ערך קצת יותר טוב עבור הפרמטרים.

58
00:05:19,686 --> 00:05:22,269
הנה עוד מספר דברים שעשויים לקרות.

59
00:05:22,269 --> 00:05:27,986
נניח שאתה מפעיל ירידה סטוכסטית ועושה ממוצע כל אלף דוגמאות,
כאשר משרטטים את גרף העלויות

60
00:05:27,986 --> 00:05:32,369
אז השרטוט הזה הוא דוגמא אפשרית של הגרף.

61
00:05:32,369 --> 00:05:34,353
ואז שוב, זה נראה כאילו זה מתכנס.

62
00:05:34,353 --> 00:05:42,119
אבל אם היית לוקח את המספר הזה, אלף,ומגדיל אותו לעשות ממוצע כל חמשת אלפים דוגמאות.

63
00:05:42,119 --> 00:05:47,913
אז ייתכן שתקבל עקומה חלקה יותר, שנראית יותר כמו זה.

64
00:05:47,913 --> 00:05:56,547
ועל ידי כך שנפעיל ממוצע כל, לדוגמא, 5,000 דוגמאות במקום 1,000,
ייתכן שנוכל לקבל עקומה חלקה כמו זו.

65
00:05:56,547 --> 00:06:00,248
וזו היא ההשפעה שיש להגדלת מספר הדוגמאות שעושים עליהם ממוצע.

66
00:06:00,248 --> 00:06:06,229
החיסרון של לעשות את המספר הזה גדול מדי הוא כמובן
שעכשיו אתה מקבל נקודת מדידה אחת רק כל 5,000 דוגמאות.

67
00:06:06,229 --> 00:06:12,001
ולכן המשוב שאתה מקבל על כמה טוב אלגוריתם הלימוד שלך עובד מתעכב,

68
00:06:12,001 --> 00:06:17,681
כי אתה מקבל נקודת נתונים אחת על הגרף שלך רק כל 5,000 דוגמאות
ולא כל 1,000 דוגמאות.

69
00:06:17,681 --> 00:06:23,911
באופן דומה לפעמים אתה יכול להריץ את הירידה הסטוכסטית
ולקבל גרף שנראה כמו זה.

70
00:06:23,911 --> 00:06:32,079
ועם גרף שנראה ככה, זה נראה כאילו המחיר פשוט לא יורד.

71
00:06:32,079 --> 00:06:34,023
נראה שהאלגוריתם פשוט לא לומד.

72
00:06:34,023 --> 00:06:39,261
זה נראה כאן כאילו העקומה היא שטוחה והעלות פשוט לא יורדת.

73
00:06:39,261 --> 00:06:46,260
אבל שוב לו הגדלנו את הדגימה לממוצע על מספר גדול יותר של דוגמאות

74
00:06:46,260 --> 00:06:49,729
ייתכן שנראה משהו כמו הקו האדום הזה,

75
00:06:49,729 --> 00:06:55,127
ונראה כאילו העלות בפועל כן יורדת, וזה רק מאחר
והקו הכחול מתאר ממוצע על מספר קטן מדי של דוגמאות,

76
00:06:55,127 --> 00:07:01,374
הקו הכחול רועש מדי אז אי אפשר לראות את המגמה בפועל במחיר שלמעשה כן יורד

77
00:07:01,374 --> 00:07:06,688
ואולי ממוצע של כל 5,000 דוגמאות במקום 1,000 עשוי לעזור.

78
00:07:06,688 --> 00:07:12,358
כמובן אנחנו ממצעים על מספר דוגמאות גדול יותר,
כי יש לנו כאן ממוצע על 5,000 דוגמאות,

79
00:07:12,358 --> 00:07:16,998
אני פשוט משתמש בצבע שונה.
זה גם אפשרי שתראו עקומת למידה שבסופו של דבר נראית כך.

80
00:07:16,998 --> 00:07:21,197
שהיא עדיין שטוחה גם כאשר ממצעים על מספר גדול יותר של דוגמאות.

81
00:07:21,197 --> 00:07:25,908
וכשמקבלים כזו עקומה, אז זה כנראה מאמת בצורה יותר איתנה

82
00:07:25,908 --> 00:07:29,287
שלמרבה הצער האלגוריתם פשוט לא ממש לומד מאיזושהי סיבה.

83
00:07:29,287 --> 00:07:34,969
ואז צריך לשנות את קצב הלמידה או את התכונות
או לשנות משהו אחר באלגוריתם.

84
00:07:34,969 --> 00:07:39,235
דבר אחד אחרון שאתם עשויים לראות יהיה אם תעשו גרף של העקומות האלה

85
00:07:39,235 --> 00:07:43,273
ותראו עקומה שנראית כך, שנראה ממש כאילו הגרף עולה, המחיר הולך וגדל.

86
00:07:43,273 --> 00:07:48,066
ואם זה המקרה אז זה סימן שהאלגוריתם מתבדר.

87
00:07:48,066 --> 00:07:53,965
ומה שז באמת צריך לעשות הוא להשתמש בערך קטן יותר של שיעור הלמידה אלפא.

88
00:07:53,965 --> 00:07:58,143
אז אני מקווה שזה נותן לכם תחושה על טווח התופעות שאתם עשויים לראות

89
00:07:58,143 --> 00:08:02,946
כאשר תשרטטו את ממוצע העלות על פני איזה מספר כלשהו של דוגמאות, וגם

90
00:08:02,946 --> 00:08:07,765
מציע לכם כל מיני דברים שאתם יכולים לנסות לעשות בתגובה לגרפים שונים.

91
00:08:07,765 --> 00:08:15,070
אז אם הגרף נראה רועש מדי, או אם הוא מתנדנד יותר מדי מעלה ומטה,
אז אפשר לנסות להגדיל את מספר הדוגמאות

92
00:08:15,070 --> 00:08:18,734
עליהם ממצעים כדי שתוכלו לראות את המגמה הכללית של הגרף בצורה טובה יותר.

93
00:08:18,734 --> 00:08:25,836
ואם אתם רואים שהשגיאות גדלות, העלויות למעשה גדלות,
נסו להשתמש בערך קטן יותר של אלפא.

94
00:08:25,836 --> 00:08:31,649
לבסוף, כדאי לבחון את הנושא של שיעור הלמידה מעט יותר לעומק.

95
00:08:31,649 --> 00:08:38,922
ראינו כי כאשר אנו מריצים ירידה סטוכסטית בשיפוע, האלגוריתם יתחיל מעין שיטוט לעבר המינימום

96
00:08:38,922 --> 00:08:43,494
ואז הוא לא באמת יתכנס, במקום זה הוא ישוטט לנצח בסביבת המינימום.

97
00:08:43,494 --> 00:08:50,225
וכך בסופו של דבר נקבל ערך פרמטר שיהיה מאוד קרוב
למינימום הגלובלי אבל לא יהיה בדיוק המינימום הגלובלי.

98
00:08:50,225 --> 00:08:57,991
ברוב היישומים האופייניים לירידה הסטוכסטית ההדרגתית,
שיעור הלימוד הוא בדרך כלל קבוע.

99
00:08:57,991 --> 00:09:02,022
ואז מה שנקבל בסופו של דבר הוא תמונה בדיוק כזאת.

100
00:09:02,022 --> 00:09:06,523
אם אתם רוצים שהירידה הסטוכסטית במדרון תתכנס ממש למינימום הגלובלי,

101
00:09:06,523 --> 00:09:11,825
יש דבר אחד שבעזרתו נוכל לעשות את זה. נוכל להפחית את שיעור הלמידה אלפא באופן איטי לאורך זמן.

102
00:09:11,825 --> 00:09:22,240
דרך אופיינית למדי לעשות את זה יהיה להגדיר את אלפא להיות שווה
לאיזה שהוא קבוע-1 חלקי מספר האיטרציה פלוס קבוע-2.

103
00:09:22,240 --> 00:09:28,169
אז, מספר האיטרציה הוא מספר האיטרציות שהפעלתם בירידה הסטוכסטית במדרון,

104
00:09:28,169 --> 00:09:29,519
שהוא למעשה מספר דוגמאות האימון שראיתם

105
00:09:29,519 --> 00:09:34,103
וקבוע-1 וקבוע-2 הם פרמטרים נוספים של האלגוריתם

106
00:09:34,103 --> 00:09:38,160
שייתכן שיהיה עליכם לשחק איתם קצת כדי לקבל ביצועים טובים.

107
00:09:38,160 --> 00:09:43,004
אחת מהסיבות שאנשים לא אוהבים לעשות את זה היא שבסופו של דבר צריך להשקיע זמן

108
00:09:43,004 --> 00:09:48,122
ולשחק עם 2 הפרמטרים הנוספים האלה, קבוע-1 וקבוע-2,
וזה עושה את האלגוריתם יותר רגיש לשינויים, "עצבני".

109
00:09:48,122 --> 00:09:52,113
זה פשוט עוד שני פרמטרים שצריך לשחק איתם כדי לגרום לאלגוריתם לעבוד היטב.

110
00:09:52,113 --> 00:09:57,246
אבל אם תצליחו לכוון את הפרמטרים היטב, אז התמונה שתוכלו לקבל היא

111
00:09:57,246 --> 00:10:02,834
שהאלגוריתם ינדוד כהרגלו לסביבת המינימום, אבל כשהוא יתקרב,

112
00:10:02,834 --> 00:10:07,024
מאחר ואתם מקטינים את שיעור הלמידה התנודות תעשינה קטנות יותר ויותר

113
00:10:07,024 --> 00:10:12,729
עד שהוא ממש יגיע אל המינימום הגלובלי. אני מקווה שזה הגיוני, נכון?

114
00:10:12,729 --> 00:10:21,608
והסיבה לכך שהנוסחה הזאת הגיונית היא שתוך כדי פעולת האלגוריתם,
מספר האיטרציה גדל בהתמדה ולכן אלפא קטן והולך,

115
00:10:21,608 --> 00:10:27,506
ולכן האלגוריתם עושה צעדים קטנים והולכים
עד שבתקווה הוא מתכנס אל המינימום הגלובלי.

116
00:10:27,506 --> 00:10:33,484
אז אם נקרב את אלפא לאט לאט לכיוון האפס
נסיים בסופו של דבר עם פונקציית השערה קצת יותר טובה.

117
00:10:33,484 --> 00:10:40,078
אבל בגלל העבודה הנוספת הדרושה להתעסק עם הקבועים
ובגלל שבדרך כלל אנחנו די מרוצים

118
00:10:40,078 --> 00:10:43,892
עם כל ערך פרמטר שהוא קרוב מספיק למינימום הגלובלי,

119
00:10:43,892 --> 00:10:50,863
בדרך כלל תהליך זה של הקטנה הדרגתית של אלפא בדרך כלל לא נעשה
והגישה של שמירה על קצב למידה אלפא קבוע

120
00:10:50,863 --> 00:10:56,983
היא הגישה הנפוצה יותר של ירידה סטוכסטית במדרון
למרות שאפשר לראות אנשים שמשתמשים בשתי הגרסאות.

121
00:10:56,983 --> 00:11:03,595
כדי לסכם, בסרטון זה דיברנו על דרך מקורבת לניטור

122
00:11:03,595 --> 00:11:08,256
את הצלחתה של הירידה הסטוכסטית בשיפוע במונחים של אופטימיזציה של פונקציית העלות.

123
00:11:08,256 --> 00:11:17,043
וזו שיטה שאינה דורשת סריקה על פני סדרת האימון כולה מדי פעם
כדי לחשב את פונקציית העלות על כל סט האימון.

124
00:11:17,043 --> 00:11:20,693
במקום זה אנתנו מסתכלים רק על, לדוגמא, אלף הדוגמאות האחרונות.

125
00:11:20,693 --> 00:11:27,592
ואתה יכול להשתמש בשיטה זו הן כדי לוודא שהירידה הסטוכסטית במדרון היא בסדר ומתכנסת,

126
00:11:27,592 --> 00:11:31,468
או כדי לכוון את קצב הלמידה אלפא.