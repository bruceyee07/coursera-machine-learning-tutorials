אז עכשיו אתם מכירים את האלגוריתם לירידה במדרון סטוכסטית. אבל כאשר מפעילים את האלגוריתם, איך ניתן לוודא
שהוא נקי לחלוטין מבאגים ושהוא מתכנס בסדר? ולא פחות חשוב, איך ניתן לכוון את שיעור הלמידה אלפא
עם ירידה במדרון סטוכסטית. בסרטון זה נדבר על כמה טכניקות לעשות את הדברים האלה,
על מנת לוודא שהוא מתכנס וגם על בחירת שיעור הלימוד אלפא. מקודם כאשר עסקנו בירידה במדרון באצווה,
הדרך הסטנדרטית שלנו לוודא שהירידה בשיפוע מתכנסת היתה על ידי שרטוט המחיר הכולל
של פונקציית העלות כפונקציה של מספר האיטרציות. אז זו היתה פונקציית העלות ואנחנו היינו מוודאים
שפונקציית העלות הזו יורדת בכל איטרציה. כאשר גודל סדרת האימון הוא קטן, יכולנו לעשות את זה
כי יכולנו לחשב את הסכום די ביעילות. אבל כאשר יש לנו סדרת אימון ענקית,
אז לא נרצה להשהות כל פעם את האלגוריתם. לא נרצה להשהות את הירידה ההדרגתית הסטוכסטית
בכל כמה איטרציות כדי לחשב את פונקציית העלות הזו כי החישוב דורש סיכום של המחיר
של כל סדרת האימון הגדולה שלנו. וכל הנקודה של הירידה במדרון הסטוכסטית היתה
שרצינו להתחיל להתקדם אחרי שהסתכלנו רק על דוגמה אחת
מבלי שנצטרך לסרוק מפעם לפעם את כל סט האימון שלנו באמצע ריצת האלגוריתם, רק כדי לחשב דברים
כמו המחיר הכולל של פונקציית העלות של כל סדרת האימון. אז עבור ירידה במדרון סטוכסטית, כדי לבדוק האם האלגוריתם מתכנס,
הנה מה שאנחנו יכולים לעשות במקום חישוב המחיר. בואו ניקח את הגדרת העלות שהיתה לנו בעבר. אז העלות של וקטור הפרמטרים תטא לגבי דוגמת אימון אחת
הוא פשוט חצי ריבוע השגיאה על אותה דוגמה. אז בזמן שאלגוריתם הירידה הסטוכסטית במדרון מתאמן ולומד,
ממש לפני שאנחנו מתחילים להתאמן או ללמוד מדוגמה ספציפית, כי בירידה סטוכסטית במדרון אנחנו מסתכלים
על הדוגמאות xi, yi, לפי הסדר, ואז בערך עושים עדכון קטן לפי מה שלמדנו בדוגמה הזו. ואז אנחנו הולכים לדוגמה הבאה, x אינדקס i פלוס 1, y אינדקס i פלוס 1, וכן הלאה, נכון? זה מה שעושה ירידה סטוכסטית במדרון. אז בזמן שהאלגוריתם מתבונן בדוגמה xi, yi,
אבל לפני שהוא מעדכן את סט הפרמטרים תטא לפי הדוגמה הזו, הבה ונחשב את העלות של הדוגמה. אני אומר את אותו הדבר שוב, אבל במילים מעט שונות. הירידה הסטוכסטית במדרון סורקת את סדרת האימון שלנו
וממש לפני שאנחנו מעדכנים את תטא באמצעות דוגמת אימון ספציפית (x(i פסיק (y(i, אנחנו נחשב עד כמה טובה ההשערה שלנו על דוגמת האימון הנוכחית. ואנחנו רוצים לעשות את זה לפני שמעדכנים את תטא, כי אם כבר עדכנו את תטא באמצעות הדוגמא הזו, אתם מבינים, אז העדכון יכול לגרום למחיר להיראות בדוגמה הזו יותר נמוך מאשר האמת האובייקטיבית. לבסוף, כדי לבדוק את ההתכנסות של ירידה סטוכסטית במדרון,
מה שאנחנו יכולים לעשות הוא כל, נניח, אלף איטרציות, אנחנו יכולים לחשב ולשרטט את העלות של מה שחישבנו בשלב הקודם. אנחנו יכולים לעשות גרף של העלויות האלה בממוצע על פני, למשל,
אלף הדוגמאות האחרונות שחושבו על ידי האלגוריתם. ואם נעשה את זה, זה ייתן לנו הערכה חיה של כמה מוצלח האלגוריתם. על 1000 דוגמאות ההכשרה האחרונות שהאלגוריתם שלך ראה. אז במקום לחשב מדי פעם את הפונקציה J-train מה שמצריך לסרוק את כל סט האימון<u></u> אז באמצעות התהליך הזה, כחלק מהירידה ההדרגתית, זה לא עולה הרבה לחשב גם את העלויות האלה לפני עדכון סט הפרמטרים תטא. וכל מה שאנחנו עושים הוא בערך כל אלף איטרציות
אנחנו פשוט עושים ממוצע של 1,000 העלויות האחרונות שחישבנו ושמים את זה על גרף. ועל ידי הסתכלות על הגרפים האלה, זה מאפשר לנו לבדוק אם הירידה הסטוכסטית בשיפוע מתכנסת. אז הנה כמה דוגמאות של איך נראים הגרפים האלה. נניח ששרטטת את ממוצע העלות של אלף הדוגמאות האחרונות. מאחר ואלה ממוצעים רק על אלף דוגמאות, הם יהיו קצת רועשים ולכן הממוצע לא יכול לרדת ממש בכל איטרציה. אז נקבל צורה שנראית ככה, אז הגרף "רועש" כי הוא מתאר ממוצע רק על תת-קבוצה קטנה, נאמר אלף דוגמאות אימון. אז אם אתם מקבלים גרף שנראה ככה, אתם יודעים שהאלגוריתם עושה עבודה די טובה, ואז אולי איפה שזה נראה כאילו בהתחלה העלות ירדה ואז הגיעה לאיזו רמה מסוימת
והגרף משתטח, נניח החל מסביב לנקודה הזו, זה נראה שזו בעצם העלות, ונראה ששלב הלמידה של האלגוריתם התכנס. אולי כדאי לכם לנסות להשתמש בקצב למידה קטן יותר,
ואז משהו שאתם עשויים לראות הוא שהאלגוריתם עשוי בתחילה ללמוד לאט יותר ולכן העלות יורדת לאט יותר. אבל אז בסופו של דבר בגלל שיעור הלמידה הנמוך זה נהיה אפשרי
עבור האלגוריתם להגיע אולי לפתרון מעט יותר טוב. אז הקו האדום עשוי לייצג את ההתנהגות של ירידה הסטוכסטית במדרון באמצעות שיעור נמוך יותר ואיטי יותר. והסיבה לכך היא שירידה הסטוכסטית לא מתכנסת למינימום הגלובלי, אלא מה שהיא עושה הוא שהפרמטרים יתנדנדו קצת באזור המינימום הגלובלי. וכך בזכות השימוש בשיעור למידה קטן יותר, אנחנו גורמים לכך שהתנודות תהיינה קטנות יותר. ולפעמים ההבדל הקטן הזה יהיה זניח ולפעמים עם ההבדל הקטן הזה נוכל לקבל ערך קצת יותר טוב עבור הפרמטרים. הנה עוד מספר דברים שעשויים לקרות. נניח שאתה מפעיל ירידה סטוכסטית ועושה ממוצע כל אלף דוגמאות,
כאשר משרטטים את גרף העלויות אז השרטוט הזה הוא דוגמא אפשרית של הגרף. ואז שוב, זה נראה כאילו זה מתכנס. אבל אם היית לוקח את המספר הזה, אלף,ומגדיל אותו לעשות ממוצע כל חמשת אלפים דוגמאות. אז ייתכן שתקבל עקומה חלקה יותר, שנראית יותר כמו זה. ועל ידי כך שנפעיל ממוצע כל, לדוגמא, 5,000 דוגמאות במקום 1,000,
ייתכן שנוכל לקבל עקומה חלקה כמו זו. וזו היא ההשפעה שיש להגדלת מספר הדוגמאות שעושים עליהם ממוצע. החיסרון של לעשות את המספר הזה גדול מדי הוא כמובן
שעכשיו אתה מקבל נקודת מדידה אחת רק כל 5,000 דוגמאות. ולכן המשוב שאתה מקבל על כמה טוב אלגוריתם הלימוד שלך עובד מתעכב, כי אתה מקבל נקודת נתונים אחת על הגרף שלך רק כל 5,000 דוגמאות
ולא כל 1,000 דוגמאות. באופן דומה לפעמים אתה יכול להריץ את הירידה הסטוכסטית
ולקבל גרף שנראה כמו זה. ועם גרף שנראה ככה, זה נראה כאילו המחיר פשוט לא יורד. נראה שהאלגוריתם פשוט לא לומד. זה נראה כאן כאילו העקומה היא שטוחה והעלות פשוט לא יורדת. אבל שוב לו הגדלנו את הדגימה לממוצע על מספר גדול יותר של דוגמאות ייתכן שנראה משהו כמו הקו האדום הזה, ונראה כאילו העלות בפועל כן יורדת, וזה רק מאחר
והקו הכחול מתאר ממוצע על מספר קטן מדי של דוגמאות, הקו הכחול רועש מדי אז אי אפשר לראות את המגמה בפועל במחיר שלמעשה כן יורד ואולי ממוצע של כל 5,000 דוגמאות במקום 1,000 עשוי לעזור. כמובן אנחנו ממצעים על מספר דוגמאות גדול יותר,
כי יש לנו כאן ממוצע על 5,000 דוגמאות, אני פשוט משתמש בצבע שונה.
זה גם אפשרי שתראו עקומת למידה שבסופו של דבר נראית כך. שהיא עדיין שטוחה גם כאשר ממצעים על מספר גדול יותר של דוגמאות. וכשמקבלים כזו עקומה, אז זה כנראה מאמת בצורה יותר איתנה שלמרבה הצער האלגוריתם פשוט לא ממש לומד מאיזושהי סיבה. ואז צריך לשנות את קצב הלמידה או את התכונות
או לשנות משהו אחר באלגוריתם. דבר אחד אחרון שאתם עשויים לראות יהיה אם תעשו גרף של העקומות האלה ותראו עקומה שנראית כך, שנראה ממש כאילו הגרף עולה, המחיר הולך וגדל. ואם זה המקרה אז זה סימן שהאלגוריתם מתבדר. ומה שז באמת צריך לעשות הוא להשתמש בערך קטן יותר של שיעור הלמידה אלפא. אז אני מקווה שזה נותן לכם תחושה על טווח התופעות שאתם עשויים לראות כאשר תשרטטו את ממוצע העלות על פני איזה מספר כלשהו של דוגמאות, וגם מציע לכם כל מיני דברים שאתם יכולים לנסות לעשות בתגובה לגרפים שונים. אז אם הגרף נראה רועש מדי, או אם הוא מתנדנד יותר מדי מעלה ומטה,
אז אפשר לנסות להגדיל את מספר הדוגמאות עליהם ממצעים כדי שתוכלו לראות את המגמה הכללית של הגרף בצורה טובה יותר. ואם אתם רואים שהשגיאות גדלות, העלויות למעשה גדלות,
נסו להשתמש בערך קטן יותר של אלפא. לבסוף, כדאי לבחון את הנושא של שיעור הלמידה מעט יותר לעומק. ראינו כי כאשר אנו מריצים ירידה סטוכסטית בשיפוע, האלגוריתם יתחיל מעין שיטוט לעבר המינימום ואז הוא לא באמת יתכנס, במקום זה הוא ישוטט לנצח בסביבת המינימום. וכך בסופו של דבר נקבל ערך פרמטר שיהיה מאוד קרוב
למינימום הגלובלי אבל לא יהיה בדיוק המינימום הגלובלי. ברוב היישומים האופייניים לירידה הסטוכסטית ההדרגתית,
שיעור הלימוד הוא בדרך כלל קבוע. ואז מה שנקבל בסופו של דבר הוא תמונה בדיוק כזאת. אם אתם רוצים שהירידה הסטוכסטית במדרון תתכנס ממש למינימום הגלובלי, יש דבר אחד שבעזרתו נוכל לעשות את זה. נוכל להפחית את שיעור הלמידה אלפא באופן איטי לאורך זמן. דרך אופיינית למדי לעשות את זה יהיה להגדיר את אלפא להיות שווה
לאיזה שהוא קבוע-1 חלקי מספר האיטרציה פלוס קבוע-2. אז, מספר האיטרציה הוא מספר האיטרציות שהפעלתם בירידה הסטוכסטית במדרון, שהוא למעשה מספר דוגמאות האימון שראיתם וקבוע-1 וקבוע-2 הם פרמטרים נוספים של האלגוריתם שייתכן שיהיה עליכם לשחק איתם קצת כדי לקבל ביצועים טובים. אחת מהסיבות שאנשים לא אוהבים לעשות את זה היא שבסופו של דבר צריך להשקיע זמן ולשחק עם 2 הפרמטרים הנוספים האלה, קבוע-1 וקבוע-2,
וזה עושה את האלגוריתם יותר רגיש לשינויים, "עצבני". זה פשוט עוד שני פרמטרים שצריך לשחק איתם כדי לגרום לאלגוריתם לעבוד היטב. אבל אם תצליחו לכוון את הפרמטרים היטב, אז התמונה שתוכלו לקבל היא שהאלגוריתם ינדוד כהרגלו לסביבת המינימום, אבל כשהוא יתקרב, מאחר ואתם מקטינים את שיעור הלמידה התנודות תעשינה קטנות יותר ויותר עד שהוא ממש יגיע אל המינימום הגלובלי. אני מקווה שזה הגיוני, נכון? והסיבה לכך שהנוסחה הזאת הגיונית היא שתוך כדי פעולת האלגוריתם,
מספר האיטרציה גדל בהתמדה ולכן אלפא קטן והולך, ולכן האלגוריתם עושה צעדים קטנים והולכים
עד שבתקווה הוא מתכנס אל המינימום הגלובלי. אז אם נקרב את אלפא לאט לאט לכיוון האפס
נסיים בסופו של דבר עם פונקציית השערה קצת יותר טובה. אבל בגלל העבודה הנוספת הדרושה להתעסק עם הקבועים
ובגלל שבדרך כלל אנחנו די מרוצים עם כל ערך פרמטר שהוא קרוב מספיק למינימום הגלובלי, בדרך כלל תהליך זה של הקטנה הדרגתית של אלפא בדרך כלל לא נעשה
והגישה של שמירה על קצב למידה אלפא קבוע היא הגישה הנפוצה יותר של ירידה סטוכסטית במדרון
למרות שאפשר לראות אנשים שמשתמשים בשתי הגרסאות. כדי לסכם, בסרטון זה דיברנו על דרך מקורבת לניטור את הצלחתה של הירידה הסטוכסטית בשיפוע במונחים של אופטימיזציה של פונקציית העלות. וזו שיטה שאינה דורשת סריקה על פני סדרת האימון כולה מדי פעם
כדי לחשב את פונקציית העלות על כל סט האימון. במקום זה אנתנו מסתכלים רק על, לדוגמא, אלף הדוגמאות האחרונות. ואתה יכול להשתמש בשיטה זו הן כדי לוודא שהירידה הסטוכסטית במדרון היא בסדר ומתכנסת, או כדי לכוון את קצב הלמידה אלפא.