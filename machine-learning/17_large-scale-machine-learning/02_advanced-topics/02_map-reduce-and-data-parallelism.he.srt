1
00:00:00,320 --> 00:00:01,510
בסרטונים האחרונים דיברנו

2
00:00:01,810 --> 00:00:03,430
על ירידה סטוכסטית במדרון,

3
00:00:03,620 --> 00:00:05,020
ועל וריאציות שונות של

4
00:00:05,120 --> 00:00:06,530
אלגוריתם הירידה הסטוכסטית,

5
00:00:06,910 --> 00:00:09,150
כולל ההתאמות ללמידה מקוונת,

6
00:00:09,490 --> 00:00:10,420
אך כל

7
00:00:10,610 --> 00:00:11,810
האלגוריתמים האלה ניתנים להפעלה

8
00:00:12,110 --> 00:00:13,740
על מכונה או מחשב אחד.

9
00:00:14,800 --> 00:00:15,870
חלק מהבעיות בלמידת מכונה

10
00:00:16,310 --> 00:00:17,270
הן פשוט גדולות מכדי שיוכלו

11
00:00:17,520 --> 00:00:19,160
לרוץ על מחשב אחד, לפעמים אולי

12
00:00:19,300 --> 00:00:21,050
יש פשוט כל כך הרבה נתונים

13
00:00:21,170 --> 00:00:22,350
שפשוט לא נרצה להריץ

14
00:00:22,670 --> 00:00:23,980
את כל הנתונים האלה דרך

15
00:00:24,100 --> 00:00:26,270
מחשב אחד, לא משנה באיזה אלגוריתם היינו משתמשים במחשב הזה.

16
00:00:28,470 --> 00:00:29,640
אז בוידאו הזה אני

17
00:00:29,740 --> 00:00:31,240
רוצה לדבר על גישה שונה

18
00:00:31,770 --> 00:00:33,610
ללמידת מכונה בקנה מידה גדול, גישה

19
00:00:34,010 --> 00:00:36,190
שנקראת מיפוי וצמצום (map/reduce).

20
00:00:37,030 --> 00:00:38,080
ולמרות שהיו לנו

21
00:00:38,380 --> 00:00:39,400
לא מעט קטעי וידאו על ירידה

22
00:00:39,970 --> 00:00:41,230
סטוכסטית במדרון ואנחנו עומדים

23
00:00:41,550 --> 00:00:43,100
לבלות פחות זמן

24
00:00:43,460 --> 00:00:45,350
על מיפוי וצמצום, אל תשפטו

25
00:00:45,560 --> 00:00:46,750
את החשיבות היחסית של מיפוי וצמצום

26
00:00:47,160 --> 00:00:48,240
לעומת הירידה הסטוכסטית במדרון

27
00:00:48,690 --> 00:00:49,590
על בסיס כמות

28
00:00:49,660 --> 00:00:51,480
הזמן שאנחנו נבלה על הרעיונות האלה בפרט.

29
00:00:52,230 --> 00:00:53,380
לפי דעת רבים,

30
00:00:53,790 --> 00:00:54,840
מיפוי וצמצום חשוב

31
00:00:55,090 --> 00:00:56,330
לפחות באותה מידה, ולפי דעתם

32
00:00:56,580 --> 00:00:57,850
של רבים אחרים הרעיון חשוב אפילו יותר,

33
00:00:58,500 --> 00:01:00,620
מאשר הירידה הסטוכסטית במדרון, אבל

34
00:01:01,460 --> 00:01:03,040
הרעיון הזה הוא יחסית

35
00:01:03,160 --> 00:01:04,620
פשוט להסביר, ולכן אנחנו

36
00:01:04,720 --> 00:01:05,580
נבלה בו פחות זמן,

37
00:01:05,830 --> 00:01:07,040
אבל באמצעות הרעיונות האלה

38
00:01:07,670 --> 00:01:08,400
ייתכן שתוכלו לבנות

39
00:01:09,070 --> 00:01:10,640
אלגוריתמי למידה לבעיות

40
00:01:10,880 --> 00:01:12,520
גדולות בהרבה מאשר מה

41
00:01:12,630 --> 00:01:14,530
שאפשרי באמצעות ירידה סטוכסטית במדרון.

42
00:01:18,720 --> 00:01:19,000
הנה הרעיון.

43
00:01:19,810 --> 00:01:21,020
נניח שאנחנו רוצים להתאים

44
00:01:21,490 --> 00:01:22,960
מודל רגרסיה ליניארית או

45
00:01:23,140 --> 00:01:24,440
מודל רגרסיה לוגיסטית או משהו

46
00:01:24,540 --> 00:01:26,100
כזה, ונתחיל שוב

47
00:01:26,430 --> 00:01:27,660
עם הירידה במדרון באצווה, אז

48
00:01:27,840 --> 00:01:30,300
הנה כלל הלימוד של הירידה במדרון באצווה.

49
00:01:31,240 --> 00:01:32,430
וכדי לפשט את

50
00:01:32,850 --> 00:01:34,170
הכתיבה בשקופית הזו, אני

51
00:01:34,340 --> 00:01:36,990
כל הזמן מניח שיש לנו 400=m דוגמאות.

52
00:01:37,530 --> 00:01:39,560
כמובן, לפי

53
00:01:39,750 --> 00:01:40,850
הסטנדרטים שלנו, במונחים של למידת מכונה

54
00:01:41,090 --> 00:01:42,050
בקנה מידה גדול, m

55
00:01:42,170 --> 00:01:43,210
כזה הוא די קטן ובעצם

56
00:01:43,770 --> 00:01:45,390
המיפוי וצמצום עשוי להיות

57
00:01:45,870 --> 00:01:46,920
מיושם יותר על בעיות שבהם

58
00:01:47,050 --> 00:01:48,190
יש לנו אולי מספר יותר קרוב ל-m שווה

59
00:01:48,740 --> 00:01:49,940
400 מיליון דוגמאות או משהו כזה,

60
00:01:50,080 --> 00:01:51,310
אבל רק כדי

61
00:01:51,390 --> 00:01:52,330
לעשות את הכתיבה על השקופית

62
00:01:52,770 --> 00:01:55,000
פשוטה יותר, אני אעמיד פנים שיש לנו 400 דוגמאות.

63
00:01:55,690 --> 00:01:57,460
אז במקרה הזה,

64
00:01:57,790 --> 00:01:59,080
כלל הלימוד של ירידה במדרון באצווה

65
00:01:59,570 --> 00:02:00,930
יש לו ה-m=400

66
00:02:01,500 --> 00:02:02,930
ויש הסכום מ-i שווה 1 עד

67
00:02:03,330 --> 00:02:05,050
400 ועוברים על 400 הדוגמאות

68
00:02:05,590 --> 00:02:06,890
כאן, וכזכור אם m

69
00:02:07,050 --> 00:02:09,780
הוא גדול, אז זה צעד יקר מבחינה חישובית.

70
00:02:10,890 --> 00:02:12,830
אז הרעיון של MapReduce

71
00:02:13,250 --> 00:02:14,470
עושה את הדברים הבאים,

72
00:02:14,890 --> 00:02:15,740
ואני חייב לומר שהרעיון של המיפוי

73
00:02:15,950 --> 00:02:16,940
וצמצום הוא פרי עבודתם של

74
00:02:17,680 --> 00:02:20,190
שני חוקרים, ג'ף דין

75
00:02:20,700 --> 00:02:22,060
וסאנג'יי גימאוואט.

76
00:02:22,640 --> 00:02:23,490
ג'ף דין, אגב, הוא

77
00:02:24,190 --> 00:02:26,520
אחד המהנדסים האגדיים ביותר

78
00:02:26,660 --> 00:02:28,300
בכל עמק הסיליקון והוא

79
00:02:28,420 --> 00:02:29,530
מעין בנה חלק גדול

80
00:02:29,820 --> 00:02:31,670
מהתשתית הארכיטקטונית

81
00:02:32,310 --> 00:02:34,770
עליה פועלת כל גוגל כיום.

82
00:02:36,000 --> 00:02:37,320
אבל הנה הרעיון של מיפוי וצמצום.

83
00:02:37,850 --> 00:02:38,570
נניח שיש לי

84
00:02:38,700 --> 00:02:39,840
איזו סדרת אימון, נניח

85
00:02:39,900 --> 00:02:41,220
שנציין אותם על ידי התיבה זו כאן

86
00:02:41,610 --> 00:02:42,760
של זוגות x,y

87
00:02:44,250 --> 00:02:47,730
כשכאן x1,y1,

88
00:02:47,990 --> 00:02:49,640
ומכאן דרך סדרת האימון עד דוגמת ההכשרה ה-400

89
00:02:50,520 --> 00:02:51,660
xm,ym.

90
00:02:52,190 --> 00:02:53,780
אז זוהי סדרת ההכשרה עם 400 דוגמאות.

91
00:02:55,060 --> 00:02:56,550
ברעיון ה-MapReduce, דרך אחת

92
00:02:56,690 --> 00:02:58,190
לבצע את המשימה היא לפצל

93
00:02:58,570 --> 00:03:00,510
את סדרת האימון לתת-קבוצות.

94
00:03:01,890 --> 00:03:02,590
אני

95
00:03:02,950 --> 00:03:04,150
אניח לשם הדוגמה הזו

96
00:03:04,290 --> 00:03:05,530
שיש לנו 4 מחשבים,

97
00:03:06,160 --> 00:03:07,160
או 4 מכונות שירוצו

98
00:03:07,300 --> 00:03:08,670
במקביל על סדרת ההדרכה,

99
00:03:08,890 --> 00:03:10,570
ולכן אני מחלק את זה ל-4 מכונות.

100
00:03:10,920 --> 00:03:12,290
אם יש לנו 10 מכונות או

101
00:03:12,400 --> 00:03:13,810
100 מכונות, אז

102
00:03:13,970 --> 00:03:15,890
נפצל את ערכת האימון ל-10 תת-קבוצות או 100, או מה שיהיה.

103
00:03:18,040 --> 00:03:19,710
ומה שתעשה המכונה הראשונה

104
00:03:19,850 --> 00:03:20,840
מ-4 המכונות,

105
00:03:21,100 --> 00:03:23,170
למשל, הוא להשתמש רק

106
00:03:23,270 --> 00:03:25,170
ברבע הראשון של

107
00:03:25,300 --> 00:03:28,680
קבוצת האימון - היא תשתמש רק ב-100 דוגמאות ההכשרה הראשונות.

108
00:03:30,020 --> 00:03:31,440
ובפרט, מה

109
00:03:31,480 --> 00:03:32,520
שהיא תעשה הוא להסתכל על

110
00:03:32,630 --> 00:03:34,800
הסכימה הזו, ולחשב

111
00:03:35,490 --> 00:03:38,560
את הסכום רק על פי 100 דוגמאות ההכשרה הראשונות.

112
00:03:40,030 --> 00:03:40,960
אז תנו לי לכתוב את זה,

113
00:03:41,110 --> 00:03:42,530
אני עומד לחשב משתנה

114
00:03:43,560 --> 00:03:46,230
שנקרא לו (temp(1

115
00:03:46,320 --> 00:03:49,410
עם אינדקס j, שווה

116
00:03:50,450 --> 00:03:52,150
לסכום מ-1 עד

117
00:03:52,260 --> 00:03:53,160
100,

118
00:03:53,500 --> 00:03:56,610
ואז אנחנו נחבר בדיוק את הביטוי כאן -

119
00:03:57,260 --> 00:04:00,140
דהיינו יש לי (hθ(xi פחות yi,

120
00:04:01,800 --> 00:04:03,230
כפול xij, כן?

121
00:04:03,740 --> 00:04:05,680
אז זה פשוט

122
00:04:05,910 --> 00:04:07,460
המונח של הירידה במדרון שם למעלה.

123
00:04:08,300 --> 00:04:09,780
ובאופן דומה, אנחנו

124
00:04:10,010 --> 00:04:11,330
ניקח את הרבע השני

125
00:04:11,600 --> 00:04:13,130
של הנתונים ונשלח אותו

126
00:04:13,320 --> 00:04:14,520
אל המחשב השני,

127
00:04:14,690 --> 00:04:15,680
והמכונה השנייה תשתמש

128
00:04:15,900 --> 00:04:18,750
בסדרת האימון באינדקסים 101 עד 200

129
00:04:19,350 --> 00:04:21,170
ותחשב משתנה דומה,

130
00:04:21,720 --> 00:04:22,880
דהיינו temp(2)j

131
00:04:23,110 --> 00:04:24,450
שהוא יהיה סכום דומה של האינדקסים,

132
00:04:24,890 --> 00:04:26,620
של הדוגמאות 101 עד 200.

133
00:04:26,840 --> 00:04:29,680
ומכונות 3

134
00:04:29,830 --> 00:04:32,720
ו-4 ישתמשו

135
00:04:32,830 --> 00:04:34,110
ברבע השלישי והרביעי

136
00:04:34,570 --> 00:04:36,550
של קבוצת האימון.

137
00:04:37,530 --> 00:04:38,950
אז עכשיו כל מכונה

138
00:04:39,190 --> 00:04:40,580
צריכה לסכם 100

139
00:04:41,060 --> 00:04:42,570
במקום 400 דוגמאות

140
00:04:42,760 --> 00:04:43,750
ולכן היא צריכה לעשות רק

141
00:04:44,050 --> 00:04:45,220
רבע מהעבודה ולכן היא

142
00:04:45,900 --> 00:04:48,000
יכולה לעשות את זה פי ארבעה מהר.

143
00:04:49,380 --> 00:04:50,630
לבסוף, אחרי שכל המכונות האלה

144
00:04:50,990 --> 00:04:51,740
עשו את העבודה הזאת, אנחנו

145
00:04:51,850 --> 00:04:53,560
לוקחים את המשתנים הזמניים האלה

146
00:04:55,350 --> 00:04:56,480
ומחברים אותם.

147
00:04:56,870 --> 00:04:58,400
אז אנחנו לוקחים את המשתנים האלה

148
00:04:58,530 --> 00:04:59,950
ושולחים את כולם

149
00:05:00,090 --> 00:05:03,080
לשרת מרכזי

150
00:05:03,300 --> 00:05:04,750
והוא

151
00:05:05,140 --> 00:05:06,720
ישלב את התוצאות הללו.

152
00:05:07,360 --> 00:05:08,470
ובפרט, הוא

153
00:05:08,780 --> 00:05:10,780
יעדכן את הפרמטרים

154
00:05:11,000 --> 00:05:13,160
θj לפי: θj

155
00:05:13,410 --> 00:05:14,720
מקבל θj

156
00:05:15,730 --> 00:05:17,560
פחות

157
00:05:17,680 --> 00:05:19,510
שיעור הלמידה α כפול אחד

158
00:05:20,120 --> 00:05:22,940
חלקי 400 כפול

159
00:05:23,300 --> 00:05:27,410
temp(1)j, ועוד temp(2)j

160
00:05:27,760 --> 00:05:30,290
ועוד temp(3)j

161
00:05:32,400 --> 00:05:35,470
ועוד temp(4)j

162
00:05:35,560 --> 00:05:37,890
וכמובן יש לעשות זאת בנפרד עבור j שווה 0.

163
00:05:37,980 --> 00:05:39,570
וכמובן

164
00:05:39,820 --> 00:05:41,220
לכל אחת מהתכונות בנפרד.

165
00:05:42,550 --> 00:05:45,420
פתרון המשוואה העליונה יעשה צעד בירידה במדרון.

166
00:05:45,670 --> 00:05:47,870
ומה שהמשוואה הזו

167
00:05:50,930 --> 00:05:53,220
עושה הוא בדיוק

168
00:05:53,290 --> 00:05:54,570
אותו דבר כמו זאת, כי כאשר

169
00:05:54,660 --> 00:05:56,140
יש לנו שרת מרכזי

170
00:05:56,680 --> 00:05:57,950
שלוקח את התוצאות הזמניות,

171
00:05:58,040 --> 00:05:58,780
temp(1)j

172
00:05:59,000 --> 00:05:59,850
עד temp(4)j

173
00:05:59,970 --> 00:06:01,760
ומחבר אותם

174
00:06:02,030 --> 00:06:03,430
מה שנותן כמובן את הסכום

175
00:06:04,090 --> 00:06:04,960
של ארבעת הדברים האלה,

176
00:06:06,360 --> 00:06:07,810
כן, זה פשוט הסכום

177
00:06:08,060 --> 00:06:09,440
הזה, ועוד הסכום הזה,

178
00:06:09,760 --> 00:06:11,490
ועוד הסכום הזה,

179
00:06:11,630 --> 00:06:13,000
ועוד הסכום הזה,

180
00:06:13,120 --> 00:06:14,290
וארבעת המחוברים

181
00:06:14,470 --> 00:06:15,830
האלה פשוט מסתכמים

182
00:06:15,920 --> 00:06:17,740
בסכום זה

183
00:06:17,880 --> 00:06:19,580
שאותו היינו צריכים מלכתחילה כדי לחשב את הצעד של הירידה במדרון באצווה.

184
00:06:20,590 --> 00:06:21,550
ואז יש לנו את α כפול

185
00:06:21,860 --> 00:06:22,910
1 חלקי 400, α כפול

186
00:06:23,350 --> 00:06:24,690
1 חלקי 400, שזה

187
00:06:25,020 --> 00:06:27,020
בדיוק שווה

188
00:06:27,140 --> 00:06:29,390
לאלגוריתם של ירידה במדרון באצווה,

189
00:06:29,910 --> 00:06:30,880
אלא שבמקום להיאלץ לעשות את הסיכום

190
00:06:31,290 --> 00:06:32,540
של כל ארבע מאות דוגמאות

191
00:06:32,810 --> 00:06:33,900
ההדרכה על מכונה

192
00:06:34,040 --> 00:06:35,280
אחת, אנחנו יכולים

193
00:06:35,760 --> 00:06:37,460
לחלק את עומס העבודה על ארבע מכונות.

194
00:06:39,090 --> 00:06:40,190
אז כך נראית

195
00:06:40,630 --> 00:06:43,410
התמונה הכללית של טכניקת MapReduce.

196
00:06:45,060 --> 00:06:46,510
יש לנו איזו ערכת אימון,

197
00:06:46,670 --> 00:06:48,200
ואנחנו רוצים לחלק אותה לארבע

198
00:06:48,420 --> 00:06:49,100
מכונות, אז אנחנו

199
00:06:49,170 --> 00:06:51,670
לוקחים את ערכת האימון ומפצלים אותה לקבוצות שוות.

200
00:06:52,120 --> 00:06:54,640
מפצלים אותה באופן שווה ככל האפשר לארבע תת-קבוצות.

201
00:06:56,470 --> 00:06:57,110
ואז אנחנו לוקחים את

202
00:06:57,180 --> 00:06:59,560
4 הקבוצות של נתוני האימון ושולחים אותם ל-4 מחשבים שונים.

203
00:07:00,530 --> 00:07:01,660
וכל אחד מארבעת המחשבים

204
00:07:02,130 --> 00:07:03,570
מחשב סיכום רק

205
00:07:03,950 --> 00:07:04,850
על רבע

206
00:07:04,910 --> 00:07:06,230
ממערך ההדרכה, ואז

207
00:07:06,340 --> 00:07:07,720
בסוף אנחנו לוקחים, כל אחד

208
00:07:07,780 --> 00:07:09,310
מהמחשבים לוקח את התוצאות,

209
00:07:09,580 --> 00:07:12,720
שולח אותם לשרת מרכזי, שמסכם את התוצאות.

210
00:07:13,570 --> 00:07:14,900
בשקופית הקודמת

211
00:07:15,190 --> 00:07:16,540
בדוגמא הזו, רוב

212
00:07:16,800 --> 00:07:17,910
העבודה של הירידה במדרון

213
00:07:18,330 --> 00:07:20,140
היתה החישוב של סכום של

214
00:07:20,430 --> 00:07:22,270
i שווה 1-400 של משהו.

215
00:07:22,670 --> 00:07:24,110
או באופן כללי יותר, סכום

216
00:07:24,370 --> 00:07:25,570
מ-i שווה 1 עד m

217
00:07:26,320 --> 00:07:28,180
של הנוסחה עבור הירידה במדרון.

218
00:07:29,160 --> 00:07:30,430
ועכשיו, מכיוון שכל

219
00:07:30,550 --> 00:07:31,890
אחד מארבעת המחשבים צריך לעשות

220
00:07:32,190 --> 00:07:33,800
רק רבע מהעבודה, אפשר

221
00:07:34,350 --> 00:07:35,940
להשיג מהירות גדולה פי ארבעה.

222
00:07:38,820 --> 00:07:39,980
בעצם, אם אין

223
00:07:40,190 --> 00:07:41,900
ברשת הזו השהיות (latencies)

224
00:07:42,100 --> 00:07:42,970
ועלויות תקשורת

225
00:07:43,400 --> 00:07:44,450
של שליחת

226
00:07:44,600 --> 00:07:45,450
הנתונים הנה ושמה, אפשר

227
00:07:45,610 --> 00:07:47,820
לקבל עד פי 4 מהירות.

228
00:07:48,050 --> 00:07:49,410
כמובן, בפועל,

229
00:07:50,100 --> 00:07:52,080
בגלל השהיות הרשת,

230
00:07:52,810 --> 00:07:54,500
ובגלל התקורה של הסיכום

231
00:07:54,600 --> 00:07:55,880
הסופי של התוצאות וגורמים אחרים,

232
00:07:56,640 --> 00:07:59,150
בפועל נקבל האצה קצת קטנה מפי 4.

233
00:08:00,140 --> 00:08:01,280
אבל למרות זאת,

234
00:08:01,360 --> 00:08:02,710
הגישה הזו של MapReduce

235
00:08:03,110 --> 00:08:04,560
מציעה לנו דרך לעבד

236
00:08:04,870 --> 00:08:05,940
ערכות נתונים הרבה יותר גדולות

237
00:08:06,270 --> 00:08:07,550
מאשר זה אפשרי באמצעות מחשב אחד.

238
00:08:08,770 --> 00:08:10,060
כשתשקלו יישום

239
00:08:10,730 --> 00:08:12,200
של מיפוי וצמצום בשביל

240
00:08:12,350 --> 00:08:14,260
איזשהו אלגוריתם למידה, כדי להאיץ אותו

241
00:08:14,750 --> 00:08:16,160
על ידי מיקבול החישוב

242
00:08:16,900 --> 00:08:18,480
על פני מחשבים שונים,

243
00:08:18,730 --> 00:08:20,040
שאלת המפתח שתשאלו את עצמכם היא,

244
00:08:20,760 --> 00:08:22,190
האם אלגוריתם הלמידה שלכם

245
00:08:22,880 --> 00:08:25,150
ניתן להיכתב כסיכום על מערך ההדרכה?

246
00:08:25,440 --> 00:08:26,430
ומתברר שאלגוריתמים

247
00:08:26,670 --> 00:08:28,100
רבים ללמידה יכולים למעשה

248
00:08:28,410 --> 00:08:29,880
להיות מבוטאים כחישוב של סכומים

249
00:08:30,170 --> 00:08:31,820
של פונקציות על מערך האימון

250
00:08:32,610 --> 00:08:34,030
והעלות החישובית של הפעלתם

251
00:08:34,250 --> 00:08:35,480
על קבוצות נתונים גדולות היא

252
00:08:35,600 --> 00:08:37,810
כי הם צריכים לסכם על קבוצה אימון גדולה מאוד.

253
00:08:38,620 --> 00:08:39,870
אז כל פעם שאלגוריתם הלמידה שלכם

254
00:08:40,200 --> 00:08:41,350
יכול להיות מנוסח

255
00:08:41,450 --> 00:08:42,410
כסיכום של קבוצת הדרכה,

256
00:08:42,660 --> 00:08:43,760
כאשר עיקר העבודה

257
00:08:43,860 --> 00:08:44,810
של אלגוריתם הלמידה

258
00:08:45,200 --> 00:08:46,170
ניתן לביטוי כסכימה

259
00:08:46,320 --> 00:08:47,780
על קבוצת ההדרכה, אזי

260
00:08:48,030 --> 00:08:49,030
מיפוי וצמצום יכול להיות מועמד טוב

261
00:08:50,100 --> 00:08:52,830
לשיפור הזמנים של אלגוריתם הלמידה על סדרות נתונים מאוד מאוד גדולות.

262
00:08:53,880 --> 00:08:54,910
בואו נסתכל רק על עוד דוגמה אחת.

263
00:08:56,020 --> 00:08:58,120
נניח שאנחנו רוצים להשתמש באחד מאלגוריתמי האופטימיזציה המתקדמים.

264
00:08:58,410 --> 00:08:59,430
דברים כמו

265
00:08:59,550 --> 00:09:00,460
L-BFGS,

266
00:09:00,900 --> 00:09:02,960
מדרון מוטה, וכן הלאה,

267
00:09:03,070 --> 00:09:05,190
נניח שאנחנו רוצים לאמן רגרסיה לוגיסטית של האלגוריתם.

268
00:09:06,080 --> 00:09:08,680
לשם כך, אנחנו צריכים לחשב שני חלקים עיקריים.

269
00:09:09,300 --> 00:09:10,460
האחד הוא, עבור

270
00:09:10,960 --> 00:09:13,520
אלגוריתמי אופטימיזציה מתקדמים כמו L-BFGS ומדרון מוטה,

271
00:09:14,310 --> 00:09:15,270
אנחנו צריכים לספק

272
00:09:15,530 --> 00:09:17,210
פונקציה כדי לחשב

273
00:09:17,460 --> 00:09:18,760
את פונקציית העלות של מטרת האופטימיזציה.

274
00:09:20,220 --> 00:09:21,690
אז עבור רגרסיה לוגיסטית,

275
00:09:21,820 --> 00:09:22,870
אתם זוכרים כי בפונקציית העלות

276
00:09:23,660 --> 00:09:24,700
יש סיכום כזה על

277
00:09:24,960 --> 00:09:26,340
סדרת האימון,

278
00:09:26,970 --> 00:09:28,980
ולכן אם אנחנו ממקבלים אותה על

279
00:09:29,110 --> 00:09:29,970
עשר מכונות, היינו מפצלים

280
00:09:30,310 --> 00:09:31,640
את סדרת האימון על

281
00:09:31,910 --> 00:09:33,150
עשר מכונות, ועל כל אחת

282
00:09:33,360 --> 00:09:35,380
מעשר המכונות עכשיו מוטל לחשב את הסכום

283
00:09:35,860 --> 00:09:37,460
של הביטוי הזה רק

284
00:09:37,760 --> 00:09:38,660
על עשירית מנתוני האימון.

285
00:09:40,370 --> 00:09:40,370
...

286
00:09:40,670 --> 00:09:41,550
והדבר השני

287
00:09:42,110 --> 00:09:43,400
שאלגוריתם אופטימיזציה מתקדם צריך

288
00:09:43,660 --> 00:09:44,790
הוא פונקציה לחישוב

289
00:09:45,190 --> 00:09:47,160
הנגזרות החלקיות האלה.

290
00:09:47,280 --> 00:09:48,980
שוב, הביטויים של הנגזרות האלה,

291
00:09:49,100 --> 00:09:50,350
עבור הרגרסיה הלוגיסטית,

292
00:09:50,540 --> 00:09:51,840
ניתנים לביטוי כסכום על

293
00:09:52,010 --> 00:09:53,130
מערך ההדרכה, ולכן

294
00:09:53,330 --> 00:09:54,600
שוב, בדומה לדוגמה הקודמת

295
00:09:54,950 --> 00:09:56,060
שלנו, אנחנו יכולים

296
00:09:56,520 --> 00:09:57,800
לחלק קטע לכל מחשב ולחשב את הסיכום הזה

297
00:09:58,800 --> 00:10:01,170
על חלק קטן בלבד של נתוני ההכשרה.

298
00:10:02,440 --> 00:10:04,590
ולבסוף, לאחר חישוב

299
00:10:05,050 --> 00:10:06,260
כל הדברים האלה, הם יכולים

300
00:10:06,400 --> 00:10:07,520
לשלוח את התוצאות שלהם

301
00:10:07,680 --> 00:10:09,400
לשרת מרכזי, שיכול אז

302
00:10:09,640 --> 00:10:12,760
לחבר את הסכומים החלקיים.

303
00:10:13,320 --> 00:10:14,410
דהיינו לחבר את

304
00:10:14,500 --> 00:10:17,000
המשתנים (temp(i

305
00:10:17,550 --> 00:10:21,880
או temp(i)j,

306
00:10:22,100 --> 00:10:23,610
שחושבו מקומית על מכונה

307
00:10:23,980 --> 00:10:25,390
מספר i,

308
00:10:25,420 --> 00:10:26,800
והשרת המרכזי יכול

309
00:10:27,050 --> 00:10:28,220
לסכם את הדברים האלה

310
00:10:28,450 --> 00:10:30,230
ולחשב את פונקצית העלות הכוללת

311
00:10:30,870 --> 00:10:32,750
ואת הנגזרת החלקית הכוללת,

312
00:10:33,390 --> 00:10:35,710
שאותה אפשר להעביר לאלגוריתם האופטימיזציה המתקדמת.

313
00:10:36,890 --> 00:10:38,100
אז באופן כללי יותר, על ידי

314
00:10:39,080 --> 00:10:40,790
לקיחת אלגוריתמים שונים של למידה

315
00:10:41,020 --> 00:10:42,430
וניסוחם במין צורה

316
00:10:42,720 --> 00:10:43,800
של סיכום או הבעתם

317
00:10:44,340 --> 00:10:45,660
במונחים של חישוב של

318
00:10:45,990 --> 00:10:47,100
סכומי פונקציות על סדרת האימון,

319
00:10:47,740 --> 00:10:49,290
אפשר להשתמש בטכניקת MapReduce

320
00:10:49,440 --> 00:10:51,420
כדי למקבל אלגוריתמי למידה שונים,

321
00:10:51,710 --> 00:10:53,310
ולשפר אותם כך שיעבדו היטב עם ערכות אימון גדולות מאוד.

322
00:10:54,340 --> 00:10:55,850
לבסוף, הערה אחת אחרונה,

323
00:10:56,390 --> 00:10:57,170
עד כה

324
00:10:57,510 --> 00:10:59,630
דנו ב-MapReduce

325
00:10:59,850 --> 00:11:01,400
כאלגוריתמים המאפשרים לנו לבצע מיקבול

326
00:11:02,090 --> 00:11:03,630
בין מחשבים מרובים, לפעמים

327
00:11:03,940 --> 00:11:05,020
מחשבים מרובים באשכול של מחשבים

328
00:11:05,220 --> 00:11:08,060
או מחשבים מרובים במרכז הנתונים.

329
00:11:09,150 --> 00:11:10,580
מתברר שלפעמים, גם אם

330
00:11:10,770 --> 00:11:12,010
יש לנו רק מחשב אחד,

331
00:11:13,090 --> 00:11:14,390
MapReduce עדיין יכול להיות ישים.

332
00:11:15,530 --> 00:11:16,970
בפרט, על מחשבים בודדים

333
00:11:17,320 --> 00:11:18,510
רבים בימינו אפשר לקבל

334
00:11:18,780 --> 00:11:20,520
ליבות עיבוד מרובות.

335
00:11:21,170 --> 00:11:21,860
אפשר לקבל מספר מעבדים,

336
00:11:22,180 --> 00:11:23,120
ובתוך כל מעבד

337
00:11:23,240 --> 00:11:26,170
יכולות להיות ליבות מרובות.

338
00:11:26,310 --> 00:11:27,170
אז אם יש לנו

339
00:11:27,520 --> 00:11:28,460
ערכת אימון גדולה, מה

340
00:11:28,570 --> 00:11:29,540
שאפשר לעשות אם, למשל, יש לנו

341
00:11:29,740 --> 00:11:31,520
מחשב עם 4 ליבות,

342
00:11:31,880 --> 00:11:33,400
מה שאפשר לעשות

343
00:11:33,460 --> 00:11:34,390
הוא, אפילו

344
00:11:34,550 --> 00:11:35,580
במחשב אחד אפשר לפצל

345
00:11:35,760 --> 00:11:37,680
את ערכות האימון לחתיכות

346
00:11:37,810 --> 00:11:39,140
ולשלוח את ערכות האימון

347
00:11:39,660 --> 00:11:40,960
לליבות שונות בתוך קופסא אחת,

348
00:11:41,220 --> 00:11:42,570
כגון בתוך מחשב שולחני אחד

349
00:11:43,240 --> 00:11:45,070
או שרת יחיד, ולהשתמש

350
00:11:45,370 --> 00:11:47,200
ב-MapReduce בדרך הזה כדי לפצל את עומס העבודה.

351
00:11:48,000 --> 00:11:49,010
כל אחת מהליבות יכולה

352
00:11:49,200 --> 00:11:50,240
לבצע את הסיכום,

353
00:11:50,950 --> 00:11:52,000
למשל, של רבע

354
00:11:52,050 --> 00:11:53,440
מהסדרה, ואז הם

355
00:11:53,510 --> 00:11:55,090
יכולים לקחת את הסכומים החלקיים

356
00:11:55,510 --> 00:11:56,890
ולאחד אותם, כדי

357
00:11:57,220 --> 00:11:59,360
לקבל את הסיכום על כל סדרת האימון.

358
00:11:59,750 --> 00:12:01,280
היתרון של לחשוב

359
00:12:01,600 --> 00:12:02,880
על MapReduce בדרך זו,

360
00:12:03,350 --> 00:12:04,760
כעל מיקבול על ליבות בתוך

361
00:12:04,900 --> 00:12:06,720
מחשב בודד, ולאו דווקא מיקבול על

362
00:12:06,910 --> 00:12:08,480
מספר מכונות הוא,

363
00:12:09,060 --> 00:12:09,970
שבדרך הזו לא צריך

364
00:12:10,100 --> 00:12:11,740
לדאוג להשהיות ברשת,

365
00:12:12,020 --> 00:12:13,380
כי כל התקשורת, כל

366
00:12:13,460 --> 00:12:14,810
שליחת המידע הזמני

367
00:12:15,890 --> 00:12:18,020
קדימה ואחורה, כל זה קורה בתוך מכונה אחת.

368
00:12:18,420 --> 00:12:20,170
וכך ההשהיות הופכות

369
00:12:20,590 --> 00:12:21,530
להיות בעיה הרבה יותר קטנה

370
00:12:21,960 --> 00:12:23,050
מאשר אם היינו משתמשים בזה

371
00:12:23,540 --> 00:12:26,080
כדי למקבל על פני מחשבים שונים בתוך מרכז הנתונים.

372
00:12:27,040 --> 00:12:27,930
ולבסוף, אזהרה אחרונה

373
00:12:27,990 --> 00:12:30,740
על מקביליות בתוך מכונת מרובת ליבות.

374
00:12:31,580 --> 00:12:32,600
בהתאם לפרטי

375
00:12:32,930 --> 00:12:34,290
היישום שלך, אם יש לך

376
00:12:34,610 --> 00:12:35,920
מכונה מרובת ליבות ואם יש לך

377
00:12:36,190 --> 00:12:38,130
ספריות נומריות מסוימות של אלגברה ליניארית.

378
00:12:39,350 --> 00:12:40,490
מתברר שחלק מהספריות של אלגברה ליניארית

379
00:12:41,490 --> 00:12:43,940
יודעות באופן אוטומטי למקבל

380
00:12:44,680 --> 00:12:47,500
פעולות אלגברה ליניארית על ליבות מרובות בתוך המכונה.

381
00:12:48,770 --> 00:12:50,140
אז אם אתה מספיק בר מזל

382
00:12:50,280 --> 00:12:51,300
כדי להשתמש באחת מאותן ספריות נומריות

383
00:12:51,710 --> 00:12:52,980
של אלגברה ליניארית וזה בהחלט

384
00:12:53,640 --> 00:12:55,120
איננו נכון לגבי כל ספריה וספריה.

385
00:12:55,830 --> 00:12:57,800
אבל אם אתה משתמש באחת מהספריות האלה,

386
00:12:58,200 --> 00:13:00,680
ואם יש לך יישום וקטורי טוב מאוד של אלגוריתם הלמידה.

387
00:13:01,720 --> 00:13:02,710
לפעמים אתה יכול פשוט ליישם

388
00:13:03,160 --> 00:13:05,060
אלגוריתם למידה סטנדרטית

389
00:13:05,150 --> 00:13:06,460
בצורה וקטורית

390
00:13:06,710 --> 00:13:08,630
ולא לדאוג למיקבול בכלל, והספריה של האלגברה הלינארית

391
00:13:10,030 --> 00:13:12,480
תטפל בחלק מהעסק הזה בשבילך.

392
00:13:12,620 --> 00:13:14,710
אז אתה לא צריך ליישם MapReduce. אבל

393
00:13:14,860 --> 00:13:16,570
עבור בעיות אחרות, ניצול

394
00:13:17,180 --> 00:13:18,660
של הסוג הזה של מיפוי וצמצום,

395
00:13:19,240 --> 00:13:20,690
מציאה ושימוש

396
00:13:20,880 --> 00:13:22,070
בניסוח שיאפשר מיקבול

397
00:13:22,170 --> 00:13:23,410
של MapReduce של חישובי הנתונים

398
00:13:23,890 --> 00:13:24,970
עשוי להיות רעיון טוב

399
00:13:25,070 --> 00:13:27,310
וגם עשוי לאפשר לך להאיץ את אלגוריתם הלמידה שלך.

400
00:13:29,860 --> 00:13:31,390
בסרטון זה, דיברנו על

401
00:13:31,730 --> 00:13:33,650
הגישה שנקראת MapReduce למיקבול

402
00:13:34,460 --> 00:13:35,850
למידת המכונה על ידי

403
00:13:36,070 --> 00:13:37,450
לקיחת הנתונים ופיזורם על פני

404
00:13:37,830 --> 00:13:39,660
מחשבים רבים במרכז הנתונים.

405
00:13:40,160 --> 00:13:41,930
למרות שהרעיונות האלה

406
00:13:42,290 --> 00:13:43,970
ניתנים ליישום גם כדי לאפשר עיבוד מקבילי על פני

407
00:13:44,290 --> 00:13:45,400
ליבות מרובות גם בתוך מחשב

408
00:13:46,870 --> 00:13:47,150
בודד.

409
00:13:47,650 --> 00:13:48,600
היום יש כמה יישומי

410
00:13:49,260 --> 00:13:51,080
קוד פתוח טובים של MapReduce,

411
00:13:51,440 --> 00:13:52,210
ישנם משתמשים רבים

412
00:13:52,710 --> 00:13:54,480
במערכת קוד פתוח בשם Hadoop

413
00:13:54,890 --> 00:13:55,820
ושימוש

414
00:13:56,010 --> 00:13:57,580
אם ביישום משלך ואם ביישום

415
00:13:57,850 --> 00:13:59,770
קוד פתוח של מישהו אחר,

416
00:13:59,920 --> 00:14:01,090
אפשר להשתמש ברעיונות אלה

417
00:14:01,410 --> 00:14:02,730
כדי למקבל אלגוריתמי למידה

418
00:14:03,540 --> 00:14:04,580
ולגרום להם לרוץ על

419
00:14:04,950 --> 00:14:05,980
ערכות נתונים הרבה יותר גדולות

420
00:14:06,320 --> 00:14:07,770
מאשר זה אפשרי באמצעות מכונה בודדת.