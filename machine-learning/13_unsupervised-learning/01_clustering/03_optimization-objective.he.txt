ברוב האלגוריתמים של למידה בפיקוח שראינו, דוגמאות כמו רגרסיה ליניארית, רגרסיה לוגיסטית וכדומה, לכל האלגוריתמים הללו יש פונקצית מחיר או עלות שזו היתה מטרת האופטימיזציה של האלגוריתם. אז מתברר שגם לK-מרכזים יש מטרת אופטימיזציה או פונקצית עלות שהוא מנסה למזער. בסרטון זה אני רוצה להסביר לכם מהי מטרת האופטימיזציה. הסיבה שאני רוצה לעשות זאת היא כי זה יהיה לנו שימושי לשתי מטרות. המטרה הראשונה היא שלדעת את מטרת האופטימיזציה של K-מרכזים
יעזור לנו לנפות באגים באלגוריתם הלמידה ופשוט לוודא שK-מרכזים פועל כראוי. והסיבה השנייה ואולי החשובה יותר היא,
שבסרטון מאוחר יותר נדבר על איך אנחנו יכולים להשתמש בזה כדי לעזור לK-מרכזים למצוא אשכולות טובים יותר
ולהימנע ממינימומים מקומיים. אבל את זה נעשה בוידאו מאוחר יותר אחר כך. כתזכורת מהירה, בזמן שK-מרכזים פועל אנחנו עוקבים אחר שתי קבוצות של משתנים. הראשונה היא (c(i, שהם האינדקסים או מספרי האשכולות, אליו מוקצה בשלב הנוכחי הדוגמה (x(i. ועוד קבוצה של משתנים שמעניינים אותנו היא μk שהם המיקומים הנוכחיים של מרכז האשכול ה-k. זכרו, בK-מרכזים אנחנו משתמשים ב-K כדי לציין את המספר הכולל של אשכולות. וכאן ב-k קטנה כאינדקס ברשימה מרכזי האשכולות, אז k קטנה היא אינדקס בין 1 ל-K. הנה עוד סימון אחד, שמשתמש ב-μ סימן תחתון (c(i כדי לציין את מרכז האשכול של האשכול שאליו שוייך (x(i, ברור למה? כדי להסביר את הסימון קצת יותר טוב, נניח ש-(x(i הוקצה לאשכול מספר חמש. משמעות הדבר היא ש-(c(i, שהוא האינדקס של (x(i, שווה ל-5. נכון? ומאחר ש-(c(i שווה חמש, זה אומר שהדוגמה (x(i שוייכה לאשכול מספר חמש. אז לכן μ סימן תחתון (c(i בעצם שווה μ סימן תחתון 5. כי (c(i שווה 5. אז μ במקום (c(i הוא מרכזו של אשכול מספר חמש, שהוא האשכול אליו משויכת הדוגמה (x(i. חמושים בסימונים האלה, אנחנו מוכנים עכשיו לכתוב מהי מטרת האופטימיזציה של אלגוריתם האשכולות K-מרכזים והנה היא. פונקציית העלות שK-מרכזים ממזערת היא פונקציה J של כל הפרמטרים האלה, (c(1 עד (c(m ו-μ1 עד μK. אלה המרכזים שזזים בתהליך הריצה של אלגוריתם K-מרכזים. ויעד האופטימיזציה המוצג מימין, הוא הממוצע של 1 חלקי m כפול סכום של i שווה 1 עד m של הביטוי הזה כאן. שכרגע הקפתי במלבן האדום, כן? זה ריבוע המרחק בין כל דוגמה (x(i לבין המיקום של מרכז האשכול אליו משויך (x(i. אז בואו נשרטט את זה ותנו לי להסביר את זה. טוב, אז הנה המיקום של דוגמת האימון (x(i והנה המיקום של מרכז האשכול שאליו שוייך (x(i. אז כדי להסביר את זה בתמונות, אם כאן יש לנו x1, x2, ואם הנקודה כאן היא הדוגמה (x(i, זהו המיקום של דוגמת האימון (x(i, ואם (x(i שוייך לאיזה מרכז אשכול, אני אסמן את מרכז האשכול בצלב, אז אם זה המיקום של μ5, נניח. אם (x(i הוקצה למרכז אשכול מספר חמש כמו בדוגמה שם למעלה, אז המרחק בריבוע, זאת אומרת ריבוע המרחק בין הנקודה (x(i ומרכז האשכול אליו שוייך (x(i. ומה שניתן להוכיח שK-מרכזים עושה הוא שהוא מנסה למצוא פרמטרים (c(i ו-μi. מנסה למצוא c ו-μ כך שימזערו את פונקצית העלות J. פונקצית עלות זו מכונה לעתים גם פונקציית העיוות - distortion, או העיוות של אלגוריתם K-מרכזים. כדי להסביר את זה קצת יותר בפירוט, הנה אלגוריתם K-מרכזים. הנה האלגוריתם בדיוק כמו שכתבנו אותו בשקופית הקודמת. והצעד הראשון של האלגוריתם היה שלב הקצאת האשכולות שבו שייכנו כל נקודה למרכז האשכול הקרוב ביותר. ניתן להראות מתמטית שצעד הקצאת האשכולות עושה בדיוק מזעור של J ביחס למשתנים (c(1), c(2 וכן הלאה עד (c(m, תוך החזקת מרכזי האשכולות μ1 עד μK קבועים. צעד הקצאת האשכולות לא משנה את מרכזי האשכולות, אבל מה שהוא כן עושה זה בדיוק לבחור את הערכים (c(1), c(2 עד (c(m. שממזערים את פונקציית העלות, או את פונקצית העיוות J. ואפשר להוכיח את זה מתמטית, אבל אני לא אעשה את זה כאן. אבל בצורה אינטואיטיבית זה די ברור שאנחנו אומרים, בואו נקצה כל נקודה למרכז האשכול הקרוב ביותר אליה, כי זה ממזער את ריבוע המרחק בין הנקודות למרכזי האשכולות. והשלב השני של K-מרכזים, השלב השני כאן, השלב השני הוא צעד הזזת המרכזים. ושוב אני לא אוכיח את זה, אבל אפשר להראות מתמטית כי מה שעושה צעד הזזת המרכזים הוא לבחור את הערכים של μ שממזערים את J, ממזערים את פונקצית העלות J ביחס, wrt הוא קיצור עבור "ביחס ל", הוא ממזער את J ביחס למיקומם של מרכזי האשכולות μ1 עד μK. אז מה שקורה פה הוא שהאלגוריתם לוקח שתי קבוצות של משתנים ומחלק אותם לשני חלקים. ראשית קבוצת המשתנים c ולאחריה קבוצת המשתנים μ. והאלגוריתם בהתחלה ממזער את J ביחס לקבוצת המשתנים c ואחר כך ממזער את J ביחס למשתנים μ ואז הוא חוזר על זה. אז זה כל מה שK-מרכזים עושה. ועכשיו, כשאנחנו מבינים שK-מרכזים מנסה למזער את פונקצית העלות J, אנחנו יכולים גם להשתמש בזה כדי לנסות לנפות שגיאות באלגוריתם הלמידה שלנו ופשוט לוודא כי היישום שלנו של K-מרכזים פועל כראוי. אז עכשיו אנחנו מבינים שאלגוריתם K-מרכזים מנסה לייעל את פונקציית העלות J, שנקראת גם פונקצית עיוות, ואנחנו יכולים להשתמש בהבנה הזו כדי לנפות את K-מרכזים כדי לוודא ש-K-מרכזים מתכנס ופועל כראוי. ובסרטון הבא נוכל גם לראות איך אנחנו יכולים להשתמש בזה כדי לעזור לK-מרכזים למצוא אשכולות טובים יותר ולהימנע ממינימום מקומיים.