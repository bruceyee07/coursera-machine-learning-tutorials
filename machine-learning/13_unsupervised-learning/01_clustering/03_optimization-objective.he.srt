1
00:00:00,110 --> 00:00:03,780
ברוב האלגוריתמים של למידה בפיקוח שראינו, דוגמאות כמו רגרסיה ליניארית,

2
00:00:03,780 --> 00:00:08,140
רגרסיה לוגיסטית וכדומה, לכל האלגוריתמים הללו יש פונקצית מחיר

3
00:00:08,140 --> 00:00:11,940
או עלות שזו היתה מטרת האופטימיזציה של האלגוריתם.

4
00:00:11,940 --> 00:00:15,820
אז מתברר שגם לK-מרכזים יש מטרת אופטימיזציה

5
00:00:15,820 --> 00:00:18,150
או פונקצית עלות שהוא מנסה למזער.

6
00:00:18,150 --> 00:00:22,790
בסרטון זה אני רוצה להסביר לכם מהי מטרת האופטימיזציה.

7
00:00:22,790 --> 00:00:26,740
הסיבה שאני רוצה לעשות זאת היא כי זה יהיה לנו שימושי

8
00:00:26,740 --> 00:00:28,010
לשתי מטרות.

9
00:00:28,010 --> 00:00:32,605
המטרה הראשונה היא שלדעת את מטרת האופטימיזציה של K-מרכזים
יעזור לנו

10
00:00:32,605 --> 00:00:36,620
לנפות באגים באלגוריתם הלמידה ופשוט לוודא שK-מרכזים פועל כראוי.

11
00:00:36,620 --> 00:00:41,850
והסיבה השנייה ואולי החשובה יותר היא,
שבסרטון מאוחר יותר נדבר על איך אנחנו

12
00:00:41,850 --> 00:00:46,170
יכולים להשתמש בזה כדי לעזור לK-מרכזים למצוא אשכולות טובים יותר
ולהימנע ממינימומים מקומיים.

13
00:00:46,170 --> 00:00:49,110
אבל את זה נעשה בוידאו מאוחר יותר אחר כך.

14
00:00:49,110 --> 00:00:53,714
כתזכורת מהירה, בזמן שK-מרכזים פועל

15
00:00:53,714 --> 00:00:56,470
אנחנו עוקבים אחר שתי קבוצות של משתנים.

16
00:00:56,470 --> 00:01:00,840
הראשונה היא (c(i, שהם האינדקסים

17
00:01:00,840 --> 00:01:05,080
או מספרי האשכולות, אליו מוקצה בשלב הנוכחי הדוגמה (x(i.

18
00:01:05,080 --> 00:01:08,460
ועוד קבוצה של משתנים שמעניינים אותנו היא μk

19
00:01:08,460 --> 00:01:12,088
שהם המיקומים הנוכחיים של מרכז האשכול ה-k.

20
00:01:12,088 --> 00:01:17,885
זכרו, בK-מרכזים אנחנו משתמשים ב-K כדי לציין את המספר הכולל של אשכולות.

21
00:01:17,885 --> 00:01:23,625
וכאן ב-k קטנה כאינדקס ברשימה מרכזי האשכולות,

22
00:01:23,625 --> 00:01:27,545
אז k קטנה היא אינדקס בין 1 ל-K.

23
00:01:29,375 --> 00:01:35,020
הנה עוד סימון אחד, שמשתמש ב-μ סימן תחתון (c(i

24
00:01:35,020 --> 00:01:40,220
כדי לציין את מרכז האשכול של האשכול שאליו שוייך (x(i, ברור למה?

25
00:01:40,220 --> 00:01:43,470
כדי להסביר את הסימון קצת יותר טוב,

26
00:01:43,470 --> 00:01:48,040
נניח ש-(x(i הוקצה לאשכול מספר חמש.

27
00:01:48,040 --> 00:01:54,150
משמעות הדבר היא ש-(c(i, שהוא האינדקס של (x(i, שווה ל-5.

28
00:01:54,150 --> 00:01:55,160
נכון?

29
00:01:55,160 --> 00:01:58,877
ומאחר ש-(c(i שווה חמש, זה אומר

30
00:01:58,877 --> 00:02:04,760
שהדוגמה (x(i שוייכה לאשכול מספר חמש.

31
00:02:04,760 --> 00:02:10,860
אז לכן μ סימן תחתון (c(i בעצם שווה μ סימן תחתון 5.

32
00:02:10,860 --> 00:02:13,650
כי (c(i שווה 5.

33
00:02:13,650 --> 00:02:19,200
אז μ במקום (c(i הוא מרכזו של אשכול מספר חמש,

34
00:02:19,200 --> 00:02:23,240
שהוא האשכול אליו משויכת הדוגמה (x(i.

35
00:02:23,240 --> 00:02:27,510
חמושים בסימונים האלה, אנחנו מוכנים עכשיו לכתוב מהי

36
00:02:27,510 --> 00:02:31,360
מטרת האופטימיזציה של אלגוריתם האשכולות K-מרכזים והנה היא.

37
00:02:31,360 --> 00:02:35,995
פונקציית העלות שK-מרכזים ממזערת היא פונקציה J של כל

38
00:02:35,995 --> 00:02:39,630
הפרמטרים האלה, (c(1 עד (c(m ו-μ1 עד μK.

39
00:02:39,630 --> 00:02:42,390
אלה המרכזים שזזים בתהליך הריצה של אלגוריתם K-מרכזים.

40
00:02:42,390 --> 00:02:44,610
ויעד האופטימיזציה המוצג מימין,

41
00:02:44,610 --> 00:02:49,020
הוא הממוצע של 1 חלקי m כפול סכום של i שווה 1 עד m של הביטוי הזה כאן.

42
00:02:50,410 --> 00:02:53,610
שכרגע הקפתי במלבן האדום, כן?

43
00:02:53,610 --> 00:02:58,600
זה ריבוע המרחק בין כל דוגמה (x(i לבין המיקום

44
00:02:58,600 --> 00:03:03,920
של מרכז האשכול אליו משויך (x(i.

45
00:03:03,920 --> 00:03:05,890
אז בואו נשרטט את זה ותנו לי להסביר את זה.

46
00:03:05,890 --> 00:03:09,680
טוב, אז הנה המיקום של דוגמת האימון (x(i

47
00:03:09,680 --> 00:03:14,510
והנה המיקום של מרכז האשכול שאליו שוייך (x(i.

48
00:03:14,510 --> 00:03:19,696
אז כדי להסביר את זה בתמונות, אם כאן יש לנו x1, x2,

49
00:03:19,696 --> 00:03:24,926
ואם הנקודה כאן היא הדוגמה (x(i, זהו המיקום של דוגמת האימון (x(i,

50
00:03:24,926 --> 00:03:29,210
ואם (x(i שוייך לאיזה מרכז אשכול, אני אסמן

51
00:03:29,210 --> 00:03:34,860
את מרכז האשכול בצלב, אז אם זה המיקום של μ5, נניח.

52
00:03:34,860 --> 00:03:39,522
אם (x(i הוקצה למרכז אשכול מספר חמש כמו בדוגמה שם למעלה,

53
00:03:39,522 --> 00:03:44,188
אז המרחק בריבוע, זאת אומרת ריבוע המרחק

54
00:03:44,188 --> 00:03:48,728
בין הנקודה (x(i ומרכז האשכול אליו שוייך (x(i.

55
00:03:48,728 --> 00:03:52,010
ומה שניתן להוכיח שK-מרכזים עושה

56
00:03:52,010 --> 00:03:56,015
הוא שהוא מנסה למצוא פרמטרים (c(i ו-μi.

57
00:03:56,015 --> 00:04:01,440
מנסה למצוא c ו-μ כך שימזערו את פונקצית העלות J.

58
00:04:01,440 --> 00:04:05,710
פונקצית עלות זו מכונה לעתים גם פונקציית העיוות - distortion,

59
00:04:06,800 --> 00:04:10,480
או העיוות של אלגוריתם K-מרכזים.

60
00:04:10,480 --> 00:04:13,850
כדי להסביר את זה קצת יותר בפירוט, הנה אלגוריתם K-מרכזים.

61
00:04:13,850 --> 00:04:18,930
הנה האלגוריתם בדיוק כמו שכתבנו אותו בשקופית הקודמת.

62
00:04:18,930 --> 00:04:26,210
והצעד הראשון של האלגוריתם היה שלב הקצאת האשכולות

63
00:04:27,980 --> 00:04:32,900
שבו שייכנו כל נקודה למרכז האשכול הקרוב ביותר.

64
00:04:32,900 --> 00:04:38,356
ניתן להראות מתמטית

65
00:04:38,356 --> 00:04:44,928
שצעד הקצאת האשכולות עושה בדיוק מזעור של J

66
00:04:44,928 --> 00:04:50,136
ביחס למשתנים (c(1), c(2 וכן הלאה

67
00:04:50,136 --> 00:04:57,460
עד (c(m, תוך החזקת מרכזי האשכולות μ1 עד μK קבועים.

68
00:04:58,570 --> 00:05:01,935
צעד הקצאת האשכולות לא משנה את מרכזי האשכולות,

69
00:05:01,935 --> 00:05:06,050
אבל מה שהוא כן עושה זה בדיוק לבחור את הערכים (c(1), c(2

70
00:05:06,050 --> 00:05:07,810
עד (c(m.

71
00:05:07,810 --> 00:05:13,530
שממזערים את פונקציית העלות, או את פונקצית העיוות J.

72
00:05:13,530 --> 00:05:17,210
ואפשר להוכיח את זה מתמטית, אבל אני לא אעשה את זה כאן.

73
00:05:17,210 --> 00:05:20,700
אבל בצורה אינטואיטיבית זה די ברור שאנחנו אומרים, בואו נקצה כל נקודה

74
00:05:20,700 --> 00:05:24,310
למרכז האשכול הקרוב ביותר אליה, כי זה

75
00:05:24,310 --> 00:05:27,800
ממזער את ריבוע המרחק בין הנקודות למרכזי האשכולות.

76
00:05:27,800 --> 00:05:33,980
והשלב השני של K-מרכזים, השלב השני כאן,

77
00:05:33,980 --> 00:05:38,770
השלב השני הוא צעד הזזת המרכזים.

78
00:05:38,770 --> 00:05:42,790
ושוב אני לא אוכיח את זה, אבל אפשר להראות מתמטית כי

79
00:05:42,790 --> 00:05:48,350
מה שעושה צעד הזזת המרכזים הוא לבחור את הערכים של μ

80
00:05:48,350 --> 00:05:53,800
שממזערים את J, ממזערים את פונקצית העלות J ביחס,

81
00:05:53,800 --> 00:05:58,800
wrt הוא קיצור עבור "ביחס ל", הוא ממזער את J ביחס

82
00:05:58,800 --> 00:06:03,471
למיקומם של מרכזי האשכולות μ1 עד μK.

83
00:06:03,471 --> 00:06:08,349
אז מה שקורה פה הוא שהאלגוריתם לוקח שתי קבוצות של משתנים

84
00:06:08,349 --> 00:06:11,820
ומחלק אותם לשני חלקים.

85
00:06:11,820 --> 00:06:15,460
ראשית קבוצת המשתנים c ולאחריה קבוצת המשתנים μ.

86
00:06:15,460 --> 00:06:18,860
והאלגוריתם בהתחלה ממזער את J ביחס לקבוצת המשתנים c

87
00:06:18,860 --> 00:06:25,200
ואחר כך ממזער את J ביחס למשתנים μ ואז הוא חוזר על זה.

88
00:06:25,200 --> 00:06:27,680
אז זה כל מה שK-מרכזים עושה.

89
00:06:27,680 --> 00:06:32,490
ועכשיו, כשאנחנו מבינים שK-מרכזים מנסה למזער את פונקצית העלות J,

90
00:06:32,490 --> 00:06:37,220
אנחנו יכולים גם להשתמש בזה כדי לנסות לנפות שגיאות באלגוריתם הלמידה שלנו

91
00:06:37,220 --> 00:06:41,230
ופשוט לוודא כי היישום שלנו של K-מרכזים פועל כראוי.

92
00:06:41,230 --> 00:06:44,490
אז עכשיו אנחנו מבינים שאלגוריתם K-מרכזים מנסה

93
00:06:44,490 --> 00:06:49,200
לייעל את פונקציית העלות J, שנקראת גם פונקצית עיוות,

94
00:06:50,560 --> 00:06:54,286
ואנחנו יכולים להשתמש בהבנה הזו כדי לנפות את K-מרכזים כדי לוודא

95
00:06:54,286 --> 00:06:55,740
ש-K-מרכזים מתכנס ופועל כראוי.

96
00:06:55,740 --> 00:06:58,960
ובסרטון הבא נוכל גם לראות איך אנחנו יכולים להשתמש בזה

97
00:06:58,960 --> 00:07:03,738
כדי לעזור לK-מרכזים למצוא אשכולות טובים יותר ולהימנע ממינימום מקומיים.