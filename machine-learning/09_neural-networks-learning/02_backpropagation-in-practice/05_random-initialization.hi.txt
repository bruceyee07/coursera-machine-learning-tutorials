पिछले पिछले  विडि हमनेइकट्ठेकियेहैंलगभगसभीपीसजोआपकोचाहिए करनेक इम्प्लमेंट एंड ट्रेन आपका नेटवर्क. एक सिर्फ़ अंतिम सुझाव जो मुझे आपको बताना है, जो है देना रैंडम प्रारंभिक वैल्यूज़. जब आप चला रहे हैं एक अल्गोरिद्म ग्रेडीयंट डिसेंट या एडवांस्ड ऑप्टिमायज़ेशन अल्गोरिद्म्स भी, हमें ज़रूरत है लेने की कुछ प्रारम्भिक वैल्यू पेरमिटर्स थीटा की. तो एडवांस्ड ऑप्टिमायज़ेशन अल्गोरिद्म के लिए, यह मान कर चलता है कि आप देंगे इसे कुछ प्रारम्भिक वैल्यू पेरमिटर्स थीटा की. चलो देखते हैं ग्रेडीयंट डिसेंट को. उसके लिए, हमें आवश्यकता है थीटा को कुछ आरम्भिक वेल्यु देने की, और फिर हम धीरे-धीरे ले सकते हैं स्टेप्स नीचे की ओर जाने के लिए इस्तेमाल करके ग्रेडीयंट डिसेंट. नीचे जाने के लिए, न्यूनतम / मिनमायज़ करने के लिए कॉस्ट फ़ंक्शन जे ऑफ़ थीटा को. तो हम क्या सेट करें थीटा की प्रारंभिक वैल्यूज़? क्या यह सम्भव है थीटा की आरम्भिक वैल्यू सेट करना ज़ीरोज़ के वेक्टर पर? जबकि यह सही काम करेगा जब हम कर रहे थे लॉजिस्टिक रेग्रेशन, आपके सारे पेरमिटर्स को शुरू में ज़ीरो करना वास्तव में नहीं काम करता जब आप ट्रेन कर रहे हैं आपका न्यूरल नेटवर्क. मान लो आप ट्रेन कर रहे हैं निम्न न्यूरल नेटवर्क, और और मान लो आप रखते हो सारे पेरमिटर्स नेटवर्क के 0 पर शुरू में. और यदि आप वह करते हैं, तब आप क्या, उसका क्या मतलब है कि शुरू में यह नीला वेट, रंग किया नीले से होगा बराबर उस वेट के, तो ये दोनो 0 हैं. और यह वेट जो मैं रंग कर रहा हूँ लाल से, बराबर है उस वेट के, रंग किया है लाल से, और यह वेट भी, जो मैं रंग कर रहा हूँ हरे से, बराबर होगा उस वेट की वैल्यू से. उसका क्या मतलब है कि दोनो आपके हिडन यूनिट्स, A1 और A2, कम्प्यूट करेंगे समान फ़ंक्शन आपकी इन्पुट्स का. और इसलिए आपको मिलेगा आपके प्रत्येक ट्रेनिंग इग्ज़ाम्पल्ज़ के लिए, आपको मिलेगा A2 1 बराबर A 2 2. और इसके अलावा, क्योंकि मैं बहुत अधिक विस्तार में यह दिखाने के लिए नहीं जा रहा हूँ, लेकिन क्योंकि ये बाहर जाने वाले वेट हैं समान आप दिखा भी सकते हैं डेल्टा वैल्यूज़ भी समान ही होंगी. तो वस्तुत: आपको मिलता है डेल्टा 1 1, डेल्टा 2 1 बराबर डेल्टा 2 2, और यदि आप मैप में और आगे जाते है, आप क्या दिखा सकते हैं कि पर्शियल डेरिवेटिव विद रिस्पेक्ट टु आपके पेरमिटर्स संतुष्ट करेंगे निम्न, कि पर्शियल डेरिवेटिव कॉस्ट फ़ंक्शन के विद रिस्पेक्ट टु अलग कर रहा हूँ डेरिवेटिव्स विद रिस्पेक्ट टु ये दो नीली लाइन आपके नेटवर्क में. आपको मिलेगा कि ये दो पर्शियल डेरिवेटिव्स होंगे बराबर एक दूसरे के. और उसका क्या मतलब है कि ग्रेडीयंट डिसेंट के एक अप्डेट के बाद भी, आप करेंगे अप्डेट, मान लो, यह पहला नीला वेट जो था लर्निंग रेट गुणा यह, और आप करेंगे अप्डेट दूसरा नीला वेट जो था लर्निंग रेट गुणा यह. और उसका क्या मतलब है कि ग्रेडीयंट डिसेंट के एक अप्डेट के बाद भी, वे दोनो नीले वेट, वे नीले पेरमिटर्स रहेंगे समान एक दूसरे के. तो वहाँ कोई नॉन-ज़ीरो वैल्यू, लेकिन यह वैल्यू होगी बराबर उस वैल्यू के. और इसी प्रकार, ग्रेडीयंट डिसेंट के एक अप्डेट के बाद भी, यह वैल्यू होगी बराबर उस वैल्यू के. अभी भी होंगी कुछ नॉन-ज़ीरो वैल्यूज़, केवल यह कि दो लाल वैल्यूज़ हैं बराबर एक दूसरे के. और इसी प्रकार, दो हरे वेट्स. ठीक, दोनो की वैल्यूज़ बदल जाएँगी, लेकिन वे होंगे समान के दूसरे के. तो प्रत्येक अप्डेट के बाद, पेरमिटर्स कॉरेस्पॉंडिंग तो इन्पुट्स जो जा रही हैं दोनो हिडन यूनिट्स में हैं समान. उसका मतलब है कि दोनो हरे वेट्स हैं अभी भी वही, दो लाल वेट्स हैं अभी भी वही, डॉन नीले वेट्स हैं अभी भी वही, और उसका क्या मतलब है कि एक इटरेशन के बाद भी, मान लो ग्रेडीयंट डिसेंट की आपको मिलेगा कि आपकी दोनो हिडन यूनिट्स अभी भी कम्प्यूट कर रही हैं बिल्कुल एक जैसे फ़ंक्शन इनपुट के. आपके पास अभी भी हैं a1(2) = a2(2). और आप वापिस वही स्थिति में हैं. और जैसे आप चलाते रहते हैं ग्रेडीयंट डिसेंट को, दोनो नीले वेट्स, फिर भी रहेंगे समान एक दूसरे के. दोनो लाल वेट्स रहेंगे समान एक दूसरे के और दोनो हरे वेट्स रहेंगे समान एक दूसरे के. और उसका क्या मतलब है कि आपका न्यूरल नेटवर्क वास्तव में कम्प्यूट कर सकता है बहुत दिलचस्प फ़ंक्शन्स, ठीक है? कल्पना करें कि आपके पास सिर्फ़ दो हिडन यूनिट्स नहीं थी, लेकिन कल्पना करें कि आपके पास होती बहुत सी हिडन यूनिट्स. तब इसका क्या मतलब हैं कि सारी आपकी हिडन यूनिट्स कम्प्यूट कर रही हैं बिल्कुल एक सा फ़ंक्शन. सारी आपकी हिडन यूनिट्स कम्प्यूट कर रही हैं एक समान फ़ंक्शन इन्पुट्स का. और यह है एक अत्यधिक व्यर्थ रेप्रेज़ेंटेशन क्योंकि आप पाते हैं लॉजिस्टिक रेग्रेशन यूनिट. इसको वास्तव में देखने की ज़रूरत है केवल एक फ़ंक्शन क्योंकि सारे ये हैं समान. और यह रोकता है आपको और आपके नेटवर्क को करने से कुछ दिलचस्प. इस समस्या से बचने के लिए, जिस तरह हम देते हैं प्रारम्भिक वैल्यूज़ पेरमिटर्स को एक न्यूरल नेटवर्क के इसलिए है देना रैंडम प्रारम्भिक वैल्यूज़. वस्तुतः, समस्या जो हमने देखी पिछली स्लाइड पर है कुछ जिसे कहते हैं समस्या सिमेट्रिक वेट्स की, कि वेट्स हैं समान. तो यह रैंडम प्रारम्भिक वैल्यूज़ देना है जिससे हम तोड़ते हैं सिमट्री. तो हम क्या करते हैं हम देते हैं शुरू की प्रत्येक वैल्यू थीटा की एक रैंडम नम्बर माइनस एप्सिलोन और एप्सिलोन के बीच. तो यह है नोटेशन नम्बर्ज़ का माइनस एप्सिलोन और प्लस एप्सिलोन के बीच. तो मेरे वेट्स मेरे पेरमिटर्स के लिए होंगे रैंडम ढंग से दी हुई वैल्यूज़ माइनस एप्सिलोन और प्लस एप्सिलोन के बीच. जिस तरह मैं लिखता हूँ कोड इसे करने के लिए ओकटेव में, मैं कहता हूँ थीटा 1 होना चाहिए बराबर इसके. तो यह है रैंड 10 बाई 11, इस तरह आप कम्प्यूट करते हैं एक रैंडम 10 बाई 11 डिमेन्शनल मेट्रिक्स. सारी वैल्यूज़ हैं 0 और 1 के बीच, तो ये होंगे रैंडम नम्बर्ज़ जो लेते हैं कोई भी कोंटिनयूस वैल्यू 0 और 1 के बीच. और इसलिए यदि आप लेते है एक नम्बर ज़ीरो और एक के बीच, गुणा करते हैं दो गुणा INIT_EPSILON तब माइनस INITI_EPSILON, तब आपको मिलता है एक नम्बर जो है माइनस एप्सिलोन और प्लस एप्सिलोन के बीच. और उससे एक और बात कि, इस एप्सिलोन का यहाँ कुछ लेना देना नहीं है उस एप्सिलोन से जो हम इस्तेमाल कर रहे थे जब हम कर रहे थे ग्रेडीयंट चेकिंग. जब कर रहे थे नूमेरिकल ग्रेडीयंट चेकिंग तब हम जोड़ रहे थे कुछ वैल्यूज़ एप्सिलोन और थीटा की. यह है आपकी कोई अन्य वैल्यू एप्सिलोन की. हम सिर्फ़ चाहते थे नोटेट करना init_epsilon सिर्फ़ भेद करने के लिए इसे उस एप्सिलोन की वैल्यू से जो हम इस्तेमाल कर रहे थे ग्रेडीयंट चेकिंग में. और इसी प्रकार यदि आप चाहते हैं देना प्रारम्भिक वैल्यू थीटा 2 को एक रैंडम 1 बाई 11 की मेट्रिक्स आप वैसा कर सकते हैं इस कोड से यहाँ. तो संक्षेप में, बनाने के लिए एक न्यूरल नेटवर्क आपको क्या चाहिए करना कि रैंडम ढंग से दें शुरू के वेट्स छोटी वैल्यूज़ जो ज़ीरो के आसपास की हैं, मान लो -एप्सिलोन और +एप्सिलोन के बीच. और फिर इम्प्लमेंट करें बैक प्रॉपगेशन, करें ग्रेडीयंट चेकिंग, और इस्तेमाल करें ग्रेडीयंट चेकिंग या कोई एक एडवांसड ऑप्टिमायज़ेशन अल्गोरिद्म करने के लिए मिनमायज़ जे(थीटा) जो है एक फ़ंक्शन पेरमिटर्स थीटा का शुरू करते हुए केवल रैंडम ढंग से ली हुई प्रारंभिक वैल्यूज़ से पेरमिटर्स की. और तोड़ने से सिमट्री, जो है यह प्रक्रिया, उम्मीद है ग्रेडीयंट डिसेंट या एडवांस्ड ऑप्टिमायज़ेशन अल्गोरिद्म ढूँढ पाएगा एक अच्छी वैल्यू थीटा के लिए.