בסרטון הקודם חיברנו כמעט את כל החלקים שצריך כדי ליישם את שלב הלימוד ברשת עצבית. יש עוד רק רעיון אחד אחרון שאני צריך לחלוק איתכם, והוא הרעיון של אתחול אקראי. כאשר מפעילים אלגוריתם של ירידה במדרון או אלגוריתמי אופטימיזציה מתקדמים, צריך לבחור ערך ראשוני עבור הפרמטרים Θ. אז אלגוריתמים מתקדמים של אופטימיזציה מניחים שתעביר להם איזשהו ערך ראשוני עבור הפרמטרים Θ. עכשיו בואו נדון בירידה במדרון. אנחנו צריכים לאתחל את Θ למשהו ואז נוכל להתחיל לרדת לאט, להשתמש בירידה במדרון, לרדת במורד, כדי למזער את הפונקציה (J(Θ. אז איך אנחנו יכולים להגדיר את הערך הראשוני של Θ? האם ניתן פשוט לקבוע את הערך ההתחלתי של Θ להיות וקטור של אפסים? למרות שזה עבד בסדר כאשר הפעלנו רגרסיה לוגיסטית, אתחול כל הפרמטרים לאפס לא עובד מעשית כאשר אתה מאמן את הרשת שלך. חישבו על אימון של הרשת העצבית הבאה, ונניח שאתחלנו את כל הפרמטרים של הרשת ל-0. ואם תעשו את זה אז מה שזה אומר הוא שבזמן האתחול המשקל הכחול הזה, בצבע כחול, יהיה שווה למשקל הזה ושניהם יהיו 0. והמשקל הזה שאני צובע באדום שווה למשקל הזה בצבע אדום, וגם המשקל הזה שאני צובע בירוק יהיה שווה לערך של המשקל הזה. ופירוש הדבר הוא ששתי היחידות הנסתרות שלנו a1 ו-a2 יקבלו ערכים המחושבים באותה פונקציה של הקלטים שלך. וכך בסוף השלב, לכל אחת מדוגמאות האימון שלנו אנחנו נקבל ש-a(2)1 שווה ל-a(2)2. יתרה מזאת, אני לא עומד להראות את זה בפירוט רב מדי, אבל מאחר והמשקלים היוצאים הם בעלי אותו ערך, אפשר גם להראות שגם ערכי δ יהיו עם אותו ערך. ואז נקבל שδ2,1 שווה δ2,2. ואם נמשיך ונגיע לשלבים הבאים, מה שאפשר להראות הוא שהנגזרות החלקיות לגבי הפרמטרים שלך יקיימו את התנאים הבאים, הנגזרת החלקית של פונקציית העלות, אני כותב את הנגזרות ביחס לשני המשקלים הכחולים ברשת. אפשר להראות ששתי הנגזרות החלקיות האלה תמיד תהיינה שוות זו לזו. ופירוש הדבר הוא שאפילו לאחר נניח עדכון אחד של הירידה במדרון, מה שהעידכון אומר הוא שהערך של המשקל הכחול הראשון
יתעדכן לשיעור הלמידה כפול משהו, ואנחנו נעדכן את המשקל הכחול השני בשיעור הלמידה כפול אותו דבר. ופירוש הדבר הזה הוא שאפילו לאחר שנעדכן את המשקלים של שני הכחולים בתהליך הירידה במדרון שני הפרמטרים הכחולים האלה יישארו זהים. אז יהיה להם עכשיו איזה ערך שהוא לא אפס, אבל הערך הזה יהיה שווה לערך הזה. ובדומה, גם לאחר עדכון אחד של הירידה במדרון, הערך הזה יהיה שווה לערך הזה. שוב, יהיו אלו ערכים שאינם אפס, אבל עדיין שני הערכים האדומים יהיו שווים זה לזה. וכך גם שני המשקלים הירוקים. הערכים של שניהם ישתנו בין איטרציות, אבל שניהם יקבלו ערכים זהים זה לזה. אז אחרי כל עדכון, הפרמטרים השייכים לקלטים הנכנסים לכל אחת מהיחידות הנסתרות יהיו זהים. ומה שזה אומר הוא ששני המשקלים הירוקים הם עדיין אותו דבר, שני המשקלים האדומים הם עדיין אותו הדבר, ושני המשקלים הכחולים עדיין זהים.
ומה שזה אומר הוא כי גם לאחר איטרציה של ירידה במדרון, תגלה ששתי היחידות הנסתרות עדיין מקבלות בדיוק אותם פונקציות של הקלטים הזהים שלהם. עדיין (a1(2) = a2(2. וכך אנחנו חוזרים על אותו סיפור, וככל שאנחנו ממשיכים להריץ את הירידה במדרון,
המשקלים הכחולים, שני המשקלים הכחולים. יישארו זהים זה לזה. שני המשקלים האדומים יישארו זהים ושני המשקלים הירוקים יישארו זהים זה לזה. ומה שזה אומר הוא שהרשת העצבית שלך באמת לא מסוגלת לחשב פונקציות ממש מעניינות. נכון? תאר לעצמך שאין לך רק שתי יחידות נסתרות, אלא דמיין שיש לך הרבה, המון יחידות נסתרות. אז מה שזה אומר הוא שכל היחידות הנסתרות שלך מחשבות את אותה תכונה בדיוק. כל היחידות הנסתרות שלך מחשבות את אותה פונקציה של הקלט. וזה ייצוג בעל יתירות גבוהה מאד כי משמעותו של דבר היא שהתוצאה הסופית של החישוב של הרגרסיה הלוגיסטית שלך בעצם מחשבת רק תכונה אחת, כי כל התוצאות זהות, וזה מונע מהרשת שלך לעשות משהו מעניין. כדי לעקוף את הבעיה, האופן שבו אנו מאתחלים את הפרמטרים של הרשת העצבית היא באמצעות אתחול אקראי או רנדומלי. הבעיה שראינו בשקופית הקודמת היא משהו שנקרא בעית המשקלים הסימטריים. זאת אומרת שכל המשקלים הם זהים. אז האתחול האקראי הוא השיטה בה אנחנו שוברים את הסימטריה. אז מה שאנחנו עושים הוא לאתחל כל ערך של Θ למספר אקראי בין ε- ו-ε. זו היא הנוטציה, כך מסמנים מספרים בין ε- לε+. אז המשקלים של כל הפרמטרים שלי יאותחלו באופן אקראי למספר בין ε- ל-ε+. הדרך שבה כותבים את זה באוקטבה הוא כמו כאן, Theta1 מקבל את הערכים האלה, Rand היא פונקציה שמחזירה מספרים אקראיים, אז כך מחשבים מטריצה דו-ממדית אקראית 10 על 11. כל הערכים הם בין 0 ל-1. אז אלה יהיו מספרים ממשיים שמקבלים ערכים על הטווח הרציף בין 0 ל-1. אז אם אתה לוקח מספר בין אפס ואחת. מכפיל אותו בפעמיים INIT_EPSILON ואז מפחית INIT_EPSILON. אז מה שאתה מקבל הוא מספר בין ε- ל-ε+. ודרך אגב, ל-ε כאן אין שום קשר עם ה-ε בו השתמשנו כאשר עשינו בדיקת שיפוע. כאשר עשינו בדיקת שיפוע נומרית. שם חיברנו אי-אילו ערכים של ε ותטא. זה ערך אחר לגמרי של ε. ולכן רצינו לסמן אותו ב-ֹINIT_EPSILON כדי להבדיל אותו מהערך של ε שהשתמשנו בו בבדיקת השיפוע. ואנחנו רוצים לאתחל גם את Theta2 לוקטור ערכים אקראיים, או מטריצה של 1 על 11, וזה נעשה על ידי המשפט הזה באוקטבה. אז לסיכום, כדי לאמן רשת עצבית מה שצריך לעשות הוא לאתחל באופן אקראי את המשקלים לערכים קטנים קרובים לאפס. נניח בין ε- ל-ε+. ואז ליישם מסירה לאחור, לעשות בדיקת שיפוע, ולהשתמש או בירידה במדרון או באלגוריתמי אופטימיזציה אחרים כדי לנסות למזער את (J(Θ כפונקציה של הפרמטרים Θ כשמתחילים מתוך הערכים הראשונים שנבחרו באופן אקראי עבור הפרמטרים. ועל ידי ביצוע שבירת סימטריה, שהוא התהליך הזה שעברנו עליו כרגע, יש לקוות שהירידה במדרון, או אלגוריתם אופטימיזציה מתקדם כלשהו יוכלו למצוא ערך טוב של Θ.