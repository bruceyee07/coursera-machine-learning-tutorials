1
00:00:00,310 --> 00:00:04,060
בסרטונים האחרונים דיברנו על איך לעשות מסירה לפנים

2
00:00:04,060 --> 00:00:08,800
ומסירה לאחור ברשת עצבית כדי לחשב נגזרות.

3
00:00:08,800 --> 00:00:12,560
אבל במסירה לאחור כאלגוריתם יש הרבה פרטים

4
00:00:12,560 --> 00:00:14,840
והוא יכול להיות קצת מסובך ליישום.

5
00:00:14,840 --> 00:00:18,900
ותכונה מצערת אחת היא שישנן דרכים רבות

6
00:00:18,900 --> 00:00:21,070
לייצר באגים מתוחכמים במסירה לאחור.

7
00:00:21,070 --> 00:00:23,600
אז אם אתה מפעיל אותה בתוך ירידה במדרון

8
00:00:23,600 --> 00:00:27,230
או איזשהו אלגוריתם אופטימיזציה אחר, זה יכול להיראות כאילו זה עובד.

9
00:00:27,230 --> 00:00:28,500
ופונקצית העלות

10
00:00:28,500 --> 00:00:32,920
(J(Θ באמת יכולה לרדת בכל איטרציה של הירידה במדרון.

11
00:00:32,920 --> 00:00:36,240
אבל זה יכול לקרות למרות שאולי יש באג כלשהו

12
00:00:36,240 --> 00:00:38,380
ביישום של המסירה לאחור.

13
00:00:38,380 --> 00:00:40,820
אז זה נראה כאילו (J(Θ יורד, אבל

14
00:00:40,820 --> 00:00:45,080
מה שתקבל בסופו של דבר הוא רשת עצבית שיש לה רמה גבוהה יותר

15
00:00:45,080 --> 00:00:47,570
של שגיאה מאשר היית מקבל עם יישום נקי מבאגים.

16
00:00:47,570 --> 00:00:50,890
ואולי אתה פשוט לא יודע שיש שם מין באג נסתר כזה

17
00:00:50,890 --> 00:00:52,330
שגורם לביצועים הגרועים יותר.

18
00:00:52,330 --> 00:00:54,170
אז, מה אנחנו יכולים לעשות בקשר לזה?

19
00:00:54,170 --> 00:00:56,810
יש רעיון שנקרא בדיקת שיפוע

20
00:00:56,810 --> 00:00:59,180
שמבטל כמעט את כל הבעיות הללו.

21
00:00:59,180 --> 00:01:02,160
אז בכל פעם שאני מיישם מסירה לאחור או

22
00:01:02,160 --> 00:01:04,780
משהו דומה לירידה במדרון על רשת עצבית

23
00:01:04,780 --> 00:01:09,680
או כל מודל מספיק מסובך אחר, אני תמיד מכניס שם בדיקת שיפוע.

24
00:01:09,680 --> 00:01:13,570
ואם תעשה את זה, זה יעזור לך לוודא, או לקבל רמה גבוהה של ביטחון,

25
00:01:13,570 --> 00:01:17,360
שהיישום שלך של מסירה לפנים ומסירה לאחור או מה שזה יהיה הוא 100% בסדר.

26
00:01:17,360 --> 00:01:22,320
וממה שאני ראיתי זה די פותר את כל הבעיות הקשורות

27
00:01:22,320 --> 00:01:25,860
ליישום שנוטה לבאגים כמו מסירה אחורה.

28
00:01:25,860 --> 00:01:31,570
בסרטונים הקודמים ביקשתי שפשוט תאמינו לי שהנוסחאות שנתתי

29
00:01:31,570 --> 00:01:34,910
לחישוב של הדלתות והv-ים וכן הלאה, ביקשתי שפשוט תאמינו לי

30
00:01:34,910 --> 00:01:39,480
שהנוסחאות האלה באמת מחשבות את הגרדיינטים של פונקציית העלות.

31
00:01:39,480 --> 00:01:43,120
אבל ברגע שתיישמו בצורה נומרית את בדיקת השיפוע, שהוא הנושא של

32
00:01:43,120 --> 00:01:47,070
הוידאו הזה, תוכלו לאמת בעצמכם שהקוד שאתם כותבים

33
00:01:47,070 --> 00:01:51,270
אכן מחשב את הנגזרות של פונקצית העלות J.

34
00:01:52,440 --> 00:01:55,480
אז הנה הרעיון, תחשבו על הדוגמה הבאה.

35
00:01:55,480 --> 00:02:01,320
נניח שיש לי את הפונקציה (J(Θ ויש לי איזשהו ערך Θ

36
00:02:01,320 --> 00:02:05,450
ועבור הדוגמה הזו נניח כי Θ הוא פשוט מספר ממשי.

37
00:02:05,450 --> 00:02:08,560
ונניח שאני רוצה להעריך את הנגזרת של הפונקציה הזו בנקודה הזו,

38
00:02:08,560 --> 00:02:13,130
אז הנגזרת שווה לשיפוע של המשיק לפונקציה בנקודה.

39
00:02:14,300 --> 00:02:17,850
אז הנה איך אני יכול להעריך את הנגזרת בצורה נומרית, או

40
00:02:17,850 --> 00:02:21,770
ליתר דיוק הנה פרוצדורה לקירוב נומרי של הנגזרת.

41
00:02:21,770 --> 00:02:26,320
דבר ראשון נחשב Θ+ε, זאת אומרת קצת ימינה מ-Θ.

42
00:02:26,320 --> 00:02:31,260
ואז נחשב Θ-ε, ואז נסתכל על שתי הנקודות,

43
00:02:31,260 --> 00:02:36,183
ונחבר אותם בקו ישר,

44
00:02:43,426 --> 00:02:47,586
ואני מחבר את שתי הנקודות האלה בקו ישר, ואז אני אשתמש

45
00:02:47,586 --> 00:02:51,620
בשיפוע של הקו האדום הקטן הזה בתור הקירוב שלי לנגזרת.

46
00:02:51,620 --> 00:02:54,860
כלומר, הנגזרת האמיתית היא השיפוע של הקו הכחול שם.

47
00:02:54,860 --> 00:02:56,719
אז אתם רואים שזה נראה כאילו זה קירוב די טוב.

48
00:02:58,260 --> 00:03:03,060
מתמטית, השיפוע של הקו האדום הזה הוא הגובה האנכי הזה

49
00:03:03,060 --> 00:03:05,470
חלקי הרוחב האופקי הזה.

50
00:03:05,470 --> 00:03:10,650
אז הנקודה הזו למעלה היא (J(Θ+ε.

51
00:03:10,650 --> 00:03:13,990
והנקודה כאן היא (J(Θ-ε, אז

52
00:03:13,990 --> 00:03:17,870
ההפרש האנכי הוא (J(Θ+ε מינוס

53
00:03:17,870 --> 00:03:21,920
(J(Θ-ε והמרחק האופקי הוא בדיוק 2ε.

54
00:03:23,620 --> 00:03:28,730
אז הקירוב שלי הוא שהנגזרת

55
00:03:28,730 --> 00:03:34,970
לפי Θ של הפונקציה (J(Θ בערך הזה של Θ, היא בערך

56
00:03:34,970 --> 00:03:41,319
(J(Θ+ε) - J(Θ-ε חלקי 2ε.

57
00:03:42,350 --> 00:03:44,430
בדרך כלל, אני משתמש בערך קטן למדי

58
00:03:44,430 --> 00:03:48,800
עבור ε, מציב ב-ε ערך של בסדר גודל של 10 בחזקת מינוס 4.

59
00:03:48,800 --> 00:03:53,060
בדרך כלל יש טווח רחב של ערכים שונים עבור ε שבהם זה עובד בסדר גמור.

60
00:03:53,060 --> 00:03:58,010
ולמעשה, אם אתה שם ב-ε ערך ממש קטן, אז מתמטית

61
00:03:58,010 --> 00:04:02,000
המונח הזה כאן, למעשה מתמטית, זה הופך להיות הנגזרת.

62
00:04:02,000 --> 00:04:05,090
וזה בדיוק השיפוע של הפונקציה בנקודה הזו.

63
00:04:05,090 --> 00:04:07,020
זה פשוט שאנחנו לא רוצים להשתמש ב-ε קטן מדי,

64
00:04:07,020 --> 00:04:10,110
כי אז עלולים להיתקל בבעיות נומריות.

65
00:04:10,110 --> 00:04:13,900
אז אני בדרך כלל משתמש ב-ε בסביבות עשר בחזקת מינוס ארבע.

66
00:04:13,900 --> 00:04:17,860
ודרך אגב חלק מכם אולי ראו נוסחה חלופית

67
00:04:17,860 --> 00:04:20,180
להערכת הנגזרת שהיא הנוסחה הזו.

68
00:04:21,610 --> 00:04:24,000
זו מימין נקראת הפרש חד צדדי,

69
00:04:24,000 --> 00:04:27,630
ואילו הנוסחה בשמאל, זו נקראת הפרש דו-צדדי.

70
00:04:27,630 --> 00:04:30,360
ההפרש הדו-צדדי נותן לנו אומדן קצת יותר מדויק, ולכן

71
00:04:30,360 --> 00:04:33,920
אני בדרך כלל משתמש בו, במקום בהערכה החד-צדדית.

72
00:04:35,960 --> 00:04:39,560
אז כשאתם כותבים באוקטבה, עליכם ליישם את הדברים הבאים,

73
00:04:39,560 --> 00:04:42,040
את הקריאה לחשב את gradApprox,

74
00:04:42,040 --> 00:04:46,840
שהוא הקירוב שלנו לנגזרת בדיוק כמו הנוסחה הזו כאן,

75
00:04:46,840 --> 00:04:52,080
(J(Θ+ε מינוס (J(Θ-ε חלקי 2ε.

76
00:04:52,080 --> 00:04:56,650
וזה ייתן לכם הערכה מספרית של השיפוע בנקודה הזו.

77
00:04:56,650 --> 00:04:59,290
ובדוגמה הזו זה נראה כאילו זה אומדן די טוב.

78
00:05:01,913 --> 00:05:03,618
עכשיו, בשקופית הקודמת,

79
00:05:03,618 --> 00:05:08,060
דנו במקרה שבו Θ היתה מספר ממשי סקלרי.

80
00:05:08,060 --> 00:05:12,450
עכשיו בואו נסתכל על מקרה יותר כללי כאשר θ הוא פרמטר וקטורי,

81
00:05:12,450 --> 00:05:14,000
נניח ש-θ הוא ב-Rn.

82
00:05:14,000 --> 00:05:18,030
זה יכול גם להיות הגרסה הפרושה של הפרמטרים של הרשת העצבית שלנו.

83
00:05:18,030 --> 00:05:21,270
אז θ הוא וקטור שיש לו n אלמנטים, θ1 עד θn.

84
00:05:21,270 --> 00:05:25,720
אז אנחנו

85
00:05:25,720 --> 00:05:30,260
יכולים להשתמש ברעיון דומה כדי להעריך את כל ביטויי הנגזרות החלקיות.

86
00:05:30,260 --> 00:05:35,140
מה שצריך לעשות הוא כך, הערכה לנגזרת החלקית של פונקציית העלות ביחס

87
00:05:35,140 --> 00:05:40,980
לפרמטר הראשון, θ1, ניתן לקבל על ידי לקיחת J והגדלת θ1.

88
00:05:40,980 --> 00:05:43,530
מחשבים את (J(θ1+ε,

89
00:05:43,530 --> 00:05:48,130
פחות (J(θ1-ε חלקי 2ε.

90
00:05:48,130 --> 00:05:52,320
הנגזרת החלקית לפרמטר השני θ2, הוא שוב אותו

91
00:05:52,320 --> 00:05:56,620
הדבר חוץ מזה שכאן ניקח את (J(θ2+ε,

92
00:05:56,620 --> 00:06:00,820
ונחסר מθ2 אפסילון (ונחלק ב2ε) וכך הלאה עד הנגזרת

93
00:06:00,820 --> 00:06:04,958
ביחס ל-θn שבה צריך להגדיל ולהקטין את

94
00:06:04,958 --> 00:06:06,393
θn ב-ε.

95
00:06:09,691 --> 00:06:15,380
אז המשוואות האלה נותנות לנו דרך נומרית להעריך את הנגזרת

96
00:06:15,380 --> 00:06:20,450
החלקית של J לגבי כל אחד מהפרמטרים θi.

97
00:06:23,590 --> 00:06:26,450
אז מה שצריך ליישם הוא לפיכך

98
00:06:27,930 --> 00:06:32,230
אנו מיישמים את הדברים הבאים באוקטבה כדי לחשב נומרית את הנגזרות.

99
00:06:32,230 --> 00:06:33,000
אנו אומרים, עבור

100
00:06:33,000 --> 00:06:37,780
i = 1:n, כאשר n הוא הממד של וקטור הפרמטרים שלנו θ,

101
00:06:37,780 --> 00:06:41,260
ואני בדרך כלל עושה את זה עם הגירסה הפרושה של הפרמטר.

102
00:06:41,260 --> 00:06:46,250
אז θ היא פשוט רשימה ארוכה של כל הפרמטרים ברשת העצבית שלי, למשל.

103
00:06:46,250 --> 00:06:48,050
אני מגדיר thetaPlus = theta,

104
00:06:48,050 --> 00:06:51,700
ואז מגדיל את thetaPlus של אלמנט i ב-ε.

105
00:06:51,700 --> 00:06:55,785
אז בעצם thetaPlus שווה ל-θ חוץ

106
00:06:55,785 --> 00:06:58,260
מהאינדקס ה-i שלו שעכשיו גדול ב-ε.

107
00:06:58,260 --> 00:07:03,390
ε, אז thetaPlus שווה, נכון, θ1, θ2 וכן הלאה.

108
00:07:03,390 --> 00:07:07,250
וב-θi אנחנו מוסיפים ε ואז אנחנו ממשיכים עד θn.

109
00:07:07,250 --> 00:07:09,670
אז זהו thetaPlus.

110
00:07:09,670 --> 00:07:15,070
ובדומה בשתי השורות האלה נגדיר את thetaMinus למשהו דומה, אלא

111
00:07:15,070 --> 00:07:19,329
שבמקום θi+ε, עכשיו אנו עושים θi-ε.

112
00:07:20,670 --> 00:07:25,690
ואז ניישם את (gradApprox(i

113
00:07:25,690 --> 00:07:29,840
שייתן לנו קירוב לנגזרת החלקית

114
00:07:29,840 --> 00:07:32,770
לפי θi של J של θ.

115
00:07:35,310 --> 00:07:38,900
וכדי להשתמש בזה ביישום שלנו של רשת עצבית,

116
00:07:38,900 --> 00:07:45,250
אנחנו מממשים את הלולאה הזו שמחשבת את הנגזרת החלקית

117
00:07:45,250 --> 00:07:49,650
של פונקציית העלות ביחס לכל פרמטר ברשת,

118
00:07:49,650 --> 00:07:53,720
ועכשיו הגענו לנקודה:
נוכל לקחת את הגרדיינט שקיבלנו מהמסירה לאחור.

119
00:07:53,720 --> 00:07:58,382
כזכור DVec היה הנגזרת שקיבלנו מהמסירה לאחור.

120
00:07:58,382 --> 00:08:00,640
בסדר, אז המסירה לאחור

121
00:08:00,640 --> 00:08:04,810
היתה דרך יעילה יחסית לחישוב נגזרת או נגזרת חלקית

122
00:08:04,810 --> 00:08:07,850
של פונקציית עלות ביחס לכל הפרמטרים שלנו.

123
00:08:07,850 --> 00:08:12,670
ומה שאני עושה בדרך כלל עכשיו הוא לקחת את הנגזרת המחושבת נומרית שלי,

124
00:08:12,670 --> 00:08:15,820
זו שנמצאת במשתנה gradApprox,

125
00:08:15,820 --> 00:08:20,780
ולוודא כי היא שווה או בערך שווה עד כדי

126
00:08:20,780 --> 00:08:24,110
שגיאת עיגול, שזה קרוב מספיק,

127
00:08:24,110 --> 00:08:26,540
למה שיש לנו ב-DVec שנתנה לנו המסירה לאחור.

128
00:08:26,540 --> 00:08:30,850
ואם שתי הדרכים האלה לחישוב הנגזרת נותנות לי את אותה תשובה, או

129
00:08:30,850 --> 00:08:34,750
תשובות מספיק דומות, עד כדי כמה ספרות אחרי הנקודה העשרונית,

130
00:08:34,750 --> 00:08:39,850
אז אני הרבה יותר בטוח שהיישום שלי של המסירה לאחור הוא נכון.

131
00:08:39,850 --> 00:08:43,920
ואז כאשר אני מעביר את וקטורי ה-DVec האלה לתוך הירידה במדרון או

132
00:08:43,920 --> 00:08:47,280
איזה אלגוריתם אופטימיזציה מתקדם, אני יכול להיות הרבה יותר

133
00:08:47,280 --> 00:08:51,640
בטוח שאני מחשב את הנגזרות כראוי, ולכן יש לי יסוד לתקווה

134
00:08:51,640 --> 00:08:55,779
שהקוד שלי יפעל כראוי וייתן תוצאה טובה באופטימיזציה של J של θ.

135
00:08:57,680 --> 00:08:59,870
לקראת הסוף, אני רוצה לחבר את כל זה

136
00:08:59,870 --> 00:09:03,670
ולהסביר לכם איך ליישם את הבדיקה הנומרית של הנגזרות.

137
00:09:03,670 --> 00:09:05,070
אז הנה מה שאני עושה בדרך כלל.

138
00:09:05,070 --> 00:09:08,460
הדבר הראשון שאני עושה הוא ליישם מסירה לאחור כדי לחשב את DVec.

139
00:09:08,460 --> 00:09:11,290
יש תהליך שדיברנו עליו בסרטון יותר מוקדם

140
00:09:11,290 --> 00:09:14,230
שמחשב את DVec, שעשוי להיות הגירסה הפרושה של המטריצות האלה.

141
00:09:14,230 --> 00:09:16,280
ואז מה שאני עושה

142
00:09:16,280 --> 00:09:20,220
הוא ליישם את בדיקת הנגזרות הנומרית ומחשב את gradApprox.

143
00:09:20,220 --> 00:09:23,830
זה התהליך שתיארתי מקודם בסרטון הזה בשקופית הקודמת.

144
00:09:24,930 --> 00:09:29,410
ואז צריך לוודא כי DVec ו-gradApprox נותנים ערכים דומים

145
00:09:29,410 --> 00:09:31,079
עד כדי שגיאת עיגול של כמה ספרות אחרי הנקודה.

146
00:09:32,270 --> 00:09:36,740
ולבסוף, וזהו צעד חשוב, לפני שתתחיל להשתמש בקוד

147
00:09:36,740 --> 00:09:40,390
שלך ללמידה, ממש לאימון רציני ברשת שלך, חשוב לכבות את

148
00:09:40,390 --> 00:09:45,100
בדיקת השיפוע ולא לחשב עוד את הדבר הזה gradApprox באמצעות

149
00:09:45,100 --> 00:09:49,119
נוסחאות נומריות של גזירות עליה דיברנו קודם בוידאו הזה.

150
00:09:50,530 --> 00:09:54,160
והסיבה לכך היא שהקוד לחישוב קירוב לנגזרת,

151
00:09:54,160 --> 00:09:58,400
החומר שדיברנו עליו בסרטון הזה, הוא יקר מאוד מבחינה חישובית,

152
00:09:58,400 --> 00:10:02,030
זוהי דרך איטית מאוד לנסות למצוא קירוב של הנגזרת.

153
00:10:02,030 --> 00:10:05,730
בעוד לעומת זאת, אלגוריתם המסירה לאחור שדיברנו עליו קודם,

154
00:10:05,730 --> 00:10:08,190
זה הדבר שדיברנו עליו קודם לכן על איך לחשב

155
00:10:08,190 --> 00:10:11,030
את D1, D2, D3 או עבור DVec.

156
00:10:11,030 --> 00:10:14,970
המסירה לאחור היא דרך הרבה יותר יעילה חישובית עבור חישוב

157
00:10:14,970 --> 00:10:15,660
של נגזרות

158
00:10:17,050 --> 00:10:20,770
אז לאחר שתאמתו שהיישום שלכם של המסירה לאחור הוא

159
00:10:20,770 --> 00:10:25,090
נכון, אתם צריכים לכבות את בדיקת השיפוע ופשוט להפסיק להשתמש בה.

160
00:10:25,090 --> 00:10:28,690
אז רק לחזור על הדברים, אתה חייב לזכור להשבית את קוד בדיקת השיפוע

161
00:10:28,690 --> 00:10:32,910
לפני הפעלת האלגוריתם שלך על באמת עם הרבה איטרציות ירידה במדרון או

162
00:10:32,910 --> 00:10:35,880
הרבה איטרציות של אלגוריתמי אופטימיזציה מתקדמים

163
00:10:35,880 --> 00:10:38,020
שאתה מריץ כדי לאמן את פונקצית הסיווג.

164
00:10:38,020 --> 00:10:41,600
באופן קונקרטי, לו הפעלת את בדיקת השיפוע על

165
00:10:41,600 --> 00:10:43,800
כל איטרציה של ירידה במדרון

166
00:10:43,800 --> 00:10:46,690
או לו הרצת את זה בלולאה הפנימית של פונקצית המחיר שלך,

167
00:10:46,690 --> 00:10:48,370
אז הקוד שלך יהיה איטי מאוד.

168
00:10:48,370 --> 00:10:52,000
מכיוון שהקוד שמחשב את הקירוב הנומרי לנגזרת הוא הרבה יותר איטי

169
00:10:52,000 --> 00:10:56,270
מאשר אלגוריתם המסירה לאחור, מאשר שיטת המסירה לאחור שבה,

170
00:10:56,270 --> 00:11:00,440
כזכור, חישבנו את δ4, δ3, δ2 וכן הלאה.

171
00:11:00,440 --> 00:11:02,460
זה היה אלגוריתם המסירה לאחור.

172
00:11:02,460 --> 00:11:06,610
וזוהי דרך הרבה יותר מהירה לחשב נגזרות מאשר בדיקת השיפוע.

173
00:11:06,610 --> 00:11:10,880
אז כשתהיה מוכן, לאחר שאמתת את נכונות המסירה לאחור,

174
00:11:10,880 --> 00:11:15,130
הקפד לכבות או להשבית את קוד בדיקת השיפוע

175
00:11:15,130 --> 00:11:18,220
בזמן שאתה מאמן את האלגוריתם שלך, אחרת אתה עלול לקבל קוד מאד איטי.

176
00:11:20,430 --> 00:11:24,390
אז ככה מריצים בדיקה נומרית של הגרדיינט, וכך מאמתים

177
00:11:24,390 --> 00:11:27,260
את היישום של המסירה לאחור.

178
00:11:27,260 --> 00:11:31,210
בכל פעם שאני מיישם מסירה לאחור או אלגוריתם ירידה במדרון עבור

179
00:11:31,210 --> 00:11:33,970
מצב מסובך, אני תמיד משתמש בבדיקת השיפוע

180
00:11:33,970 --> 00:11:36,750
וזה באמת עוזר לי לוודא שהקוד שלי נכון.