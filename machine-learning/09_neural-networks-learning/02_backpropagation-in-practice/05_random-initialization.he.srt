1
00:00:00,560 --> 00:00:05,200
בסרטון הקודם חיברנו כמעט את כל החלקים שצריך כדי

2
00:00:05,200 --> 00:00:07,240
ליישם את שלב הלימוד ברשת עצבית.

3
00:00:07,240 --> 00:00:10,010
יש עוד רק רעיון אחד אחרון שאני צריך לחלוק איתכם,

4
00:00:10,010 --> 00:00:12,180
והוא הרעיון של אתחול אקראי.

5
00:00:13,260 --> 00:00:15,480
כאשר מפעילים אלגוריתם של ירידה במדרון או

6
00:00:15,480 --> 00:00:19,900
אלגוריתמי אופטימיזציה מתקדמים, צריך לבחור ערך ראשוני עבור

7
00:00:19,900 --> 00:00:21,620
הפרמטרים Θ.

8
00:00:21,620 --> 00:00:23,840
אז אלגוריתמים מתקדמים של אופטימיזציה

9
00:00:23,840 --> 00:00:27,880
מניחים שתעביר להם איזשהו ערך ראשוני עבור הפרמטרים Θ.

10
00:00:29,060 --> 00:00:31,280
עכשיו בואו נדון בירידה במדרון.

11
00:00:31,280 --> 00:00:34,690
אנחנו צריכים לאתחל את Θ למשהו

12
00:00:34,690 --> 00:00:38,910
ואז נוכל להתחיל לרדת לאט, להשתמש בירידה במדרון,

13
00:00:38,910 --> 00:00:41,980
לרדת במורד, כדי למזער את הפונקציה (J(Θ.

14
00:00:41,980 --> 00:00:45,490
אז איך אנחנו יכולים להגדיר את הערך הראשוני של Θ?

15
00:00:45,490 --> 00:00:51,900
האם ניתן פשוט לקבוע את הערך ההתחלתי של Θ להיות וקטור של אפסים?

16
00:00:51,900 --> 00:00:55,660
למרות שזה עבד בסדר כאשר הפעלנו רגרסיה לוגיסטית,

17
00:00:55,660 --> 00:00:59,210
אתחול כל הפרמטרים לאפס לא עובד מעשית

18
00:00:59,210 --> 00:01:01,390
כאשר אתה מאמן את הרשת שלך.

19
00:01:01,390 --> 00:01:03,820
חישבו על אימון של הרשת העצבית הבאה,

20
00:01:03,820 --> 00:01:07,580
ונניח שאתחלנו את כל הפרמטרים של הרשת ל-0.

21
00:01:07,580 --> 00:01:12,940
ואם תעשו את זה אז מה שזה אומר הוא שבזמן האתחול

22
00:01:12,940 --> 00:01:18,620
המשקל הכחול הזה, בצבע כחול, יהיה שווה למשקל הזה ושניהם יהיו 0.

23
00:01:18,620 --> 00:01:21,040
והמשקל הזה שאני צובע באדום

24
00:01:21,040 --> 00:01:25,640
שווה למשקל הזה בצבע אדום, וגם המשקל הזה

25
00:01:25,640 --> 00:01:30,030
שאני צובע בירוק יהיה שווה לערך של המשקל הזה.

26
00:01:30,030 --> 00:01:32,770
ופירוש הדבר הוא ששתי היחידות הנסתרות שלנו a1 ו-a2

27
00:01:32,770 --> 00:01:37,830
יקבלו ערכים המחושבים באותה פונקציה של הקלטים שלך.

28
00:01:37,830 --> 00:01:42,744
וכך בסוף השלב, לכל אחת מדוגמאות האימון שלנו

29
00:01:42,744 --> 00:01:45,480
אנחנו נקבל ש-a(2)1 שווה ל-a(2)2.

30
00:01:46,940 --> 00:01:50,860
יתרה מזאת, אני לא עומד להראות את זה בפירוט רב מדי, אבל

31
00:01:50,860 --> 00:01:54,360
מאחר והמשקלים היוצאים הם בעלי אותו ערך, אפשר גם להראות

32
00:01:54,360 --> 00:01:56,770
שגם ערכי δ יהיו עם אותו ערך.

33
00:01:56,770 --> 00:02:02,536
ואז נקבל שδ2,1 שווה δ2,2.

34
00:02:02,536 --> 00:02:08,582
ואם נמשיך ונגיע לשלבים הבאים, מה שאפשר להראות הוא שהנגזרות

35
00:02:08,582 --> 00:02:14,538
החלקיות לגבי הפרמטרים שלך יקיימו את התנאים הבאים,

36
00:02:14,538 --> 00:02:19,831
הנגזרת החלקית של פונקציית העלות, אני כותב את הנגזרות

37
00:02:19,831 --> 00:02:26,103
ביחס לשני המשקלים הכחולים ברשת.

38
00:02:26,103 --> 00:02:29,911
אפשר להראות ששתי הנגזרות החלקיות האלה תמיד תהיינה שוות

39
00:02:29,911 --> 00:02:30,660
זו לזו.

40
00:02:31,930 --> 00:02:35,906
ופירוש הדבר הוא שאפילו לאחר נניח עדכון אחד של הירידה במדרון,

41
00:02:35,906 --> 00:02:40,469
מה שהעידכון אומר הוא שהערך של המשקל הכחול הראשון
יתעדכן לשיעור הלמידה כפול משהו,

42
00:02:40,469 --> 00:02:44,990
ואנחנו נעדכן את המשקל הכחול השני בשיעור הלמידה כפול אותו דבר.

43
00:02:44,990 --> 00:02:50,386
ופירוש הדבר הזה הוא שאפילו לאחר שנעדכן את המשקלים של שני הכחולים

44
00:02:50,386 --> 00:02:55,183
בתהליך הירידה במדרון שני הפרמטרים הכחולים האלה יישארו זהים.

45
00:02:55,183 --> 00:03:00,550
אז יהיה להם עכשיו איזה ערך שהוא לא אפס, אבל הערך הזה יהיה שווה לערך הזה.

46
00:03:00,550 --> 00:03:01,420
ובדומה,

47
00:03:01,420 --> 00:03:06,150
גם לאחר עדכון אחד של הירידה במדרון, הערך הזה יהיה שווה לערך הזה.

48
00:03:06,150 --> 00:03:07,818
שוב, יהיו אלו ערכים שאינם אפס,

49
00:03:07,818 --> 00:03:10,230
אבל עדיין שני הערכים האדומים יהיו שווים זה לזה.

50
00:03:10,230 --> 00:03:12,500
וכך גם שני המשקלים הירוקים.

51
00:03:12,500 --> 00:03:14,010
הערכים של שניהם ישתנו בין איטרציות, אבל

52
00:03:14,010 --> 00:03:17,590
שניהם יקבלו ערכים זהים זה לזה.

53
00:03:17,590 --> 00:03:21,447
אז אחרי כל עדכון, הפרמטרים השייכים לקלטים הנכנסים לכל

54
00:03:21,447 --> 00:03:23,656
אחת מהיחידות הנסתרות יהיו זהים.

55
00:03:23,656 --> 00:03:27,101
ומה שזה אומר הוא ששני המשקלים הירוקים הם עדיין אותו דבר, שני המשקלים

56
00:03:27,101 --> 00:03:30,758
האדומים הם עדיין אותו הדבר, ושני המשקלים הכחולים עדיין זהים.
ומה שזה אומר הוא

57
00:03:30,758 --> 00:03:34,270
כי גם לאחר איטרציה של ירידה במדרון,

58
00:03:34,270 --> 00:03:39,114
תגלה ששתי היחידות הנסתרות עדיין מקבלות בדיוק אותם

59
00:03:39,114 --> 00:03:40,897
פונקציות של הקלטים הזהים שלהם.

60
00:03:40,897 --> 00:03:44,092
עדיין (a1(2) = a2(2.

61
00:03:44,092 --> 00:03:45,729
וכך אנחנו חוזרים על אותו סיפור,

62
00:03:45,729 --> 00:03:49,634
וככל שאנחנו ממשיכים להריץ את הירידה במדרון,
המשקלים הכחולים, שני המשקלים הכחולים.

63
00:03:49,634 --> 00:03:51,470
יישארו זהים זה לזה.

64
00:03:51,470 --> 00:03:53,410
שני המשקלים האדומים יישארו זהים

65
00:03:53,410 --> 00:03:54,889
ושני המשקלים הירוקים יישארו זהים זה לזה.

66
00:03:56,030 --> 00:03:59,220
ומה שזה אומר הוא שהרשת העצבית שלך באמת לא מסוגלת לחשב

67
00:03:59,220 --> 00:04:00,740
פונקציות ממש מעניינות. נכון?

68
00:04:00,740 --> 00:04:04,960
תאר לעצמך שאין לך רק שתי יחידות נסתרות, אלא

69
00:04:04,960 --> 00:04:08,070
דמיין שיש לך הרבה, המון יחידות נסתרות.

70
00:04:08,070 --> 00:04:11,670
אז מה שזה אומר הוא שכל היחידות הנסתרות שלך מחשבות

71
00:04:11,670 --> 00:04:13,150
את אותה תכונה בדיוק.

72
00:04:13,150 --> 00:04:17,060
כל היחידות הנסתרות שלך מחשבות את אותה פונקציה של הקלט.

73
00:04:17,060 --> 00:04:20,190
וזה ייצוג בעל יתירות גבוהה מאד

74
00:04:20,190 --> 00:04:22,620
כי משמעותו של דבר היא שהתוצאה הסופית של החישוב של הרגרסיה הלוגיסטית שלך

75
00:04:22,620 --> 00:04:26,320
בעצם מחשבת רק תכונה אחת, כי כל התוצאות זהות,

76
00:04:26,320 --> 00:04:29,190
וזה מונע מהרשת שלך לעשות משהו מעניין.

77
00:04:31,640 --> 00:04:35,350
כדי לעקוף את הבעיה, האופן שבו אנו מאתחלים את הפרמטרים של

78
00:04:35,350 --> 00:04:38,449
הרשת העצבית היא באמצעות אתחול אקראי או רנדומלי.

79
00:04:41,264 --> 00:04:45,370
הבעיה שראינו בשקופית הקודמת היא משהו

80
00:04:45,370 --> 00:04:49,990
שנקרא בעית המשקלים הסימטריים. זאת אומרת שכל המשקלים הם זהים.

81
00:04:49,990 --> 00:04:55,510
אז האתחול האקראי הוא השיטה בה אנחנו שוברים את הסימטריה.

82
00:04:55,510 --> 00:05:00,313
אז מה שאנחנו עושים הוא לאתחל כל ערך של Θ למספר אקראי בין

83
00:05:00,313 --> 00:05:02,177
ε- ו-ε.

84
00:05:02,177 --> 00:05:06,290
זו היא הנוטציה, כך מסמנים מספרים בין ε- לε+.

85
00:05:06,290 --> 00:05:08,794
אז המשקלים של כל הפרמטרים שלי יאותחלו

86
00:05:08,794 --> 00:05:12,183
באופן אקראי למספר בין ε- ל-ε+.

87
00:05:12,183 --> 00:05:16,782
הדרך שבה כותבים את זה באוקטבה הוא כמו כאן, Theta1 מקבל את

88
00:05:16,782 --> 00:05:17,369
הערכים האלה,

89
00:05:17,369 --> 00:05:19,749
Rand היא פונקציה שמחזירה מספרים אקראיים,

90
00:05:19,749 --> 00:05:26,750
אז כך מחשבים מטריצה דו-ממדית אקראית 10 על 11.

91
00:05:26,750 --> 00:05:31,740
כל הערכים הם בין 0 ל-1. אז אלה יהיו

92
00:05:31,740 --> 00:05:35,460
מספרים ממשיים שמקבלים ערכים על הטווח הרציף בין 0 ל-1.

93
00:05:35,460 --> 00:05:37,140
אז אם אתה לוקח מספר בין אפס ואחת.

94
00:05:37,140 --> 00:05:40,940
מכפיל אותו בפעמיים INIT_EPSILON ואז מפחית INIT_EPSILON.

95
00:05:40,940 --> 00:05:44,530
אז מה שאתה מקבל הוא מספר בין ε- ל-ε+.

96
00:05:45,640 --> 00:05:48,490
ודרך אגב, ל-ε כאן אין שום

97
00:05:48,490 --> 00:05:52,590
קשר עם ה-ε בו השתמשנו כאשר עשינו בדיקת שיפוע.

98
00:05:52,590 --> 00:05:54,870
כאשר עשינו בדיקת שיפוע נומרית.

99
00:05:54,870 --> 00:05:57,420
שם חיברנו אי-אילו ערכים של ε ותטא.

100
00:05:57,420 --> 00:05:59,860
זה ערך אחר לגמרי של ε.

101
00:05:59,860 --> 00:06:02,940
ולכן רצינו לסמן אותו ב-ֹINIT_EPSILON כדי להבדיל אותו

102
00:06:02,940 --> 00:06:06,490
מהערך של ε שהשתמשנו בו בבדיקת השיפוע.

103
00:06:06,490 --> 00:06:11,240
ואנחנו רוצים לאתחל גם את Theta2 לוקטור ערכים אקראיים,

104
00:06:11,240 --> 00:06:13,650
או מטריצה של 1 על 11, וזה נעשה על ידי המשפט הזה באוקטבה.

105
00:06:16,120 --> 00:06:19,625
אז לסיכום, כדי לאמן רשת עצבית מה שצריך

106
00:06:19,625 --> 00:06:23,552
לעשות הוא לאתחל באופן אקראי את המשקלים לערכים קטנים קרובים לאפס.

107
00:06:23,552 --> 00:06:25,879
נניח בין ε- ל-ε+.

108
00:06:25,879 --> 00:06:29,699
ואז ליישם מסירה לאחור, לעשות בדיקת שיפוע,

109
00:06:29,699 --> 00:06:34,694
ולהשתמש או בירידה במדרון או באלגוריתמי אופטימיזציה אחרים כדי לנסות

110
00:06:34,694 --> 00:06:39,470
למזער את (J(Θ כפונקציה של הפרמטרים Θ כשמתחילים מתוך

111
00:06:39,470 --> 00:06:42,716
הערכים הראשונים שנבחרו באופן אקראי עבור הפרמטרים.

112
00:06:42,716 --> 00:06:44,377
ועל ידי ביצוע שבירת סימטריה,

113
00:06:44,377 --> 00:06:47,403
שהוא התהליך הזה שעברנו עליו כרגע, יש לקוות שהירידה במדרון, או

114
00:06:47,403 --> 00:06:51,452
אלגוריתם אופטימיזציה מתקדם כלשהו יוכלו למצוא ערך טוב של Θ.