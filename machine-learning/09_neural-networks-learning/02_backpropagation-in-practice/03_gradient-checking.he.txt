בסרטונים האחרונים דיברנו על איך לעשות מסירה לפנים ומסירה לאחור ברשת עצבית כדי לחשב נגזרות. אבל במסירה לאחור כאלגוריתם יש הרבה פרטים והוא יכול להיות קצת מסובך ליישום. ותכונה מצערת אחת היא שישנן דרכים רבות לייצר באגים מתוחכמים במסירה לאחור. אז אם אתה מפעיל אותה בתוך ירידה במדרון או איזשהו אלגוריתם אופטימיזציה אחר, זה יכול להיראות כאילו זה עובד. ופונקצית העלות (J(Θ באמת יכולה לרדת בכל איטרציה של הירידה במדרון. אבל זה יכול לקרות למרות שאולי יש באג כלשהו ביישום של המסירה לאחור. אז זה נראה כאילו (J(Θ יורד, אבל מה שתקבל בסופו של דבר הוא רשת עצבית שיש לה רמה גבוהה יותר של שגיאה מאשר היית מקבל עם יישום נקי מבאגים. ואולי אתה פשוט לא יודע שיש שם מין באג נסתר כזה שגורם לביצועים הגרועים יותר. אז, מה אנחנו יכולים לעשות בקשר לזה? יש רעיון שנקרא בדיקת שיפוע שמבטל כמעט את כל הבעיות הללו. אז בכל פעם שאני מיישם מסירה לאחור או משהו דומה לירידה במדרון על רשת עצבית או כל מודל מספיק מסובך אחר, אני תמיד מכניס שם בדיקת שיפוע. ואם תעשה את זה, זה יעזור לך לוודא, או לקבל רמה גבוהה של ביטחון, שהיישום שלך של מסירה לפנים ומסירה לאחור או מה שזה יהיה הוא 100% בסדר. וממה שאני ראיתי זה די פותר את כל הבעיות הקשורות ליישום שנוטה לבאגים כמו מסירה אחורה. בסרטונים הקודמים ביקשתי שפשוט תאמינו לי שהנוסחאות שנתתי לחישוב של הדלתות והv-ים וכן הלאה, ביקשתי שפשוט תאמינו לי שהנוסחאות האלה באמת מחשבות את הגרדיינטים של פונקציית העלות. אבל ברגע שתיישמו בצורה נומרית את בדיקת השיפוע, שהוא הנושא של הוידאו הזה, תוכלו לאמת בעצמכם שהקוד שאתם כותבים אכן מחשב את הנגזרות של פונקצית העלות J. אז הנה הרעיון, תחשבו על הדוגמה הבאה. נניח שיש לי את הפונקציה (J(Θ ויש לי איזשהו ערך Θ ועבור הדוגמה הזו נניח כי Θ הוא פשוט מספר ממשי. ונניח שאני רוצה להעריך את הנגזרת של הפונקציה הזו בנקודה הזו, אז הנגזרת שווה לשיפוע של המשיק לפונקציה בנקודה. אז הנה איך אני יכול להעריך את הנגזרת בצורה נומרית, או ליתר דיוק הנה פרוצדורה לקירוב נומרי של הנגזרת. דבר ראשון נחשב Θ+ε, זאת אומרת קצת ימינה מ-Θ. ואז נחשב Θ-ε, ואז נסתכל על שתי הנקודות, ונחבר אותם בקו ישר, ואני מחבר את שתי הנקודות האלה בקו ישר, ואז אני אשתמש בשיפוע של הקו האדום הקטן הזה בתור הקירוב שלי לנגזרת. כלומר, הנגזרת האמיתית היא השיפוע של הקו הכחול שם. אז אתם רואים שזה נראה כאילו זה קירוב די טוב. מתמטית, השיפוע של הקו האדום הזה הוא הגובה האנכי הזה חלקי הרוחב האופקי הזה. אז הנקודה הזו למעלה היא (J(Θ+ε. והנקודה כאן היא (J(Θ-ε, אז ההפרש האנכי הוא (J(Θ+ε מינוס (J(Θ-ε והמרחק האופקי הוא בדיוק 2ε. אז הקירוב שלי הוא שהנגזרת לפי Θ של הפונקציה (J(Θ בערך הזה של Θ, היא בערך (J(Θ+ε) - J(Θ-ε חלקי 2ε. בדרך כלל, אני משתמש בערך קטן למדי עבור ε, מציב ב-ε ערך של בסדר גודל של 10 בחזקת מינוס 4. בדרך כלל יש טווח רחב של ערכים שונים עבור ε שבהם זה עובד בסדר גמור. ולמעשה, אם אתה שם ב-ε ערך ממש קטן, אז מתמטית המונח הזה כאן, למעשה מתמטית, זה הופך להיות הנגזרת. וזה בדיוק השיפוע של הפונקציה בנקודה הזו. זה פשוט שאנחנו לא רוצים להשתמש ב-ε קטן מדי, כי אז עלולים להיתקל בבעיות נומריות. אז אני בדרך כלל משתמש ב-ε בסביבות עשר בחזקת מינוס ארבע. ודרך אגב חלק מכם אולי ראו נוסחה חלופית להערכת הנגזרת שהיא הנוסחה הזו. זו מימין נקראת הפרש חד צדדי, ואילו הנוסחה בשמאל, זו נקראת הפרש דו-צדדי. ההפרש הדו-צדדי נותן לנו אומדן קצת יותר מדויק, ולכן אני בדרך כלל משתמש בו, במקום בהערכה החד-צדדית. אז כשאתם כותבים באוקטבה, עליכם ליישם את הדברים הבאים, את הקריאה לחשב את gradApprox, שהוא הקירוב שלנו לנגזרת בדיוק כמו הנוסחה הזו כאן, (J(Θ+ε מינוס (J(Θ-ε חלקי 2ε. וזה ייתן לכם הערכה מספרית של השיפוע בנקודה הזו. ובדוגמה הזו זה נראה כאילו זה אומדן די טוב. עכשיו, בשקופית הקודמת, דנו במקרה שבו Θ היתה מספר ממשי סקלרי. עכשיו בואו נסתכל על מקרה יותר כללי כאשר θ הוא פרמטר וקטורי, נניח ש-θ הוא ב-Rn. זה יכול גם להיות הגרסה הפרושה של הפרמטרים של הרשת העצבית שלנו. אז θ הוא וקטור שיש לו n אלמנטים, θ1 עד θn. אז אנחנו יכולים להשתמש ברעיון דומה כדי להעריך את כל ביטויי הנגזרות החלקיות. מה שצריך לעשות הוא כך, הערכה לנגזרת החלקית של פונקציית העלות ביחס לפרמטר הראשון, θ1, ניתן לקבל על ידי לקיחת J והגדלת θ1. מחשבים את (J(θ1+ε, פחות (J(θ1-ε חלקי 2ε. הנגזרת החלקית לפרמטר השני θ2, הוא שוב אותו הדבר חוץ מזה שכאן ניקח את (J(θ2+ε, ונחסר מθ2 אפסילון (ונחלק ב2ε) וכך הלאה עד הנגזרת ביחס ל-θn שבה צריך להגדיל ולהקטין את θn ב-ε. אז המשוואות האלה נותנות לנו דרך נומרית להעריך את הנגזרת החלקית של J לגבי כל אחד מהפרמטרים θi. אז מה שצריך ליישם הוא לפיכך אנו מיישמים את הדברים הבאים באוקטבה כדי לחשב נומרית את הנגזרות. אנו אומרים, עבור i = 1:n, כאשר n הוא הממד של וקטור הפרמטרים שלנו θ, ואני בדרך כלל עושה את זה עם הגירסה הפרושה של הפרמטר. אז θ היא פשוט רשימה ארוכה של כל הפרמטרים ברשת העצבית שלי, למשל. אני מגדיר thetaPlus = theta, ואז מגדיל את thetaPlus של אלמנט i ב-ε. אז בעצם thetaPlus שווה ל-θ חוץ מהאינדקס ה-i שלו שעכשיו גדול ב-ε. ε, אז thetaPlus שווה, נכון, θ1, θ2 וכן הלאה. וב-θi אנחנו מוסיפים ε ואז אנחנו ממשיכים עד θn. אז זהו thetaPlus. ובדומה בשתי השורות האלה נגדיר את thetaMinus למשהו דומה, אלא שבמקום θi+ε, עכשיו אנו עושים θi-ε. ואז ניישם את (gradApprox(i שייתן לנו קירוב לנגזרת החלקית לפי θi של J של θ. וכדי להשתמש בזה ביישום שלנו של רשת עצבית, אנחנו מממשים את הלולאה הזו שמחשבת את הנגזרת החלקית של פונקציית העלות ביחס לכל פרמטר ברשת, ועכשיו הגענו לנקודה:
נוכל לקחת את הגרדיינט שקיבלנו מהמסירה לאחור. כזכור DVec היה הנגזרת שקיבלנו מהמסירה לאחור. בסדר, אז המסירה לאחור היתה דרך יעילה יחסית לחישוב נגזרת או נגזרת חלקית של פונקציית עלות ביחס לכל הפרמטרים שלנו. ומה שאני עושה בדרך כלל עכשיו הוא לקחת את הנגזרת המחושבת נומרית שלי, זו שנמצאת במשתנה gradApprox, ולוודא כי היא שווה או בערך שווה עד כדי שגיאת עיגול, שזה קרוב מספיק, למה שיש לנו ב-DVec שנתנה לנו המסירה לאחור. ואם שתי הדרכים האלה לחישוב הנגזרת נותנות לי את אותה תשובה, או תשובות מספיק דומות, עד כדי כמה ספרות אחרי הנקודה העשרונית, אז אני הרבה יותר בטוח שהיישום שלי של המסירה לאחור הוא נכון. ואז כאשר אני מעביר את וקטורי ה-DVec האלה לתוך הירידה במדרון או איזה אלגוריתם אופטימיזציה מתקדם, אני יכול להיות הרבה יותר בטוח שאני מחשב את הנגזרות כראוי, ולכן יש לי יסוד לתקווה שהקוד שלי יפעל כראוי וייתן תוצאה טובה באופטימיזציה של J של θ. לקראת הסוף, אני רוצה לחבר את כל זה ולהסביר לכם איך ליישם את הבדיקה הנומרית של הנגזרות. אז הנה מה שאני עושה בדרך כלל. הדבר הראשון שאני עושה הוא ליישם מסירה לאחור כדי לחשב את DVec. יש תהליך שדיברנו עליו בסרטון יותר מוקדם שמחשב את DVec, שעשוי להיות הגירסה הפרושה של המטריצות האלה. ואז מה שאני עושה הוא ליישם את בדיקת הנגזרות הנומרית ומחשב את gradApprox. זה התהליך שתיארתי מקודם בסרטון הזה בשקופית הקודמת. ואז צריך לוודא כי DVec ו-gradApprox נותנים ערכים דומים עד כדי שגיאת עיגול של כמה ספרות אחרי הנקודה. ולבסוף, וזהו צעד חשוב, לפני שתתחיל להשתמש בקוד שלך ללמידה, ממש לאימון רציני ברשת שלך, חשוב לכבות את בדיקת השיפוע ולא לחשב עוד את הדבר הזה gradApprox באמצעות נוסחאות נומריות של גזירות עליה דיברנו קודם בוידאו הזה. והסיבה לכך היא שהקוד לחישוב קירוב לנגזרת, החומר שדיברנו עליו בסרטון הזה, הוא יקר מאוד מבחינה חישובית, זוהי דרך איטית מאוד לנסות למצוא קירוב של הנגזרת. בעוד לעומת זאת, אלגוריתם המסירה לאחור שדיברנו עליו קודם, זה הדבר שדיברנו עליו קודם לכן על איך לחשב את D1, D2, D3 או עבור DVec. המסירה לאחור היא דרך הרבה יותר יעילה חישובית עבור חישוב של נגזרות אז לאחר שתאמתו שהיישום שלכם של המסירה לאחור הוא נכון, אתם צריכים לכבות את בדיקת השיפוע ופשוט להפסיק להשתמש בה. אז רק לחזור על הדברים, אתה חייב לזכור להשבית את קוד בדיקת השיפוע לפני הפעלת האלגוריתם שלך על באמת עם הרבה איטרציות ירידה במדרון או הרבה איטרציות של אלגוריתמי אופטימיזציה מתקדמים שאתה מריץ כדי לאמן את פונקצית הסיווג. באופן קונקרטי, לו הפעלת את בדיקת השיפוע על כל איטרציה של ירידה במדרון או לו הרצת את זה בלולאה הפנימית של פונקצית המחיר שלך, אז הקוד שלך יהיה איטי מאוד. מכיוון שהקוד שמחשב את הקירוב הנומרי לנגזרת הוא הרבה יותר איטי מאשר אלגוריתם המסירה לאחור, מאשר שיטת המסירה לאחור שבה, כזכור, חישבנו את δ4, δ3, δ2 וכן הלאה. זה היה אלגוריתם המסירה לאחור. וזוהי דרך הרבה יותר מהירה לחשב נגזרות מאשר בדיקת השיפוע. אז כשתהיה מוכן, לאחר שאמתת את נכונות המסירה לאחור, הקפד לכבות או להשבית את קוד בדיקת השיפוע בזמן שאתה מאמן את האלגוריתם שלך, אחרת אתה עלול לקבל קוד מאד איטי. אז ככה מריצים בדיקה נומרית של הגרדיינט, וכך מאמתים את היישום של המסירה לאחור. בכל פעם שאני מיישם מסירה לאחור או אלגוריתם ירידה במדרון עבור מצב מסובך, אני תמיד משתמש בבדיקת השיפוע וזה באמת עוזר לי לוודא שהקוד שלי נכון.