אז לקח לנו הרבה קטעי וידאו כדי לעבור על אלגוריתם הלמידה של רשתות עצביות. בסרטון הזה מה שאני רוצה לעשות הוא לנסות לחבר את כל החלקים, לתת סיכום כולל או להציג את התמונה הרחבה יותר של איך מתאימים כל החלקים זה לזה ומהווים את התהליך הכולל של יישום אלגוריתם למידה של רשת עצבית. כאשר מאמנים רשת עצבית, הדבר הראשון שצריך לעשות הוא לבחור איזו ארכיטקטורת רשת ובארכיטקטורה אני פשוט מתכוון לאיזה דגם של קישוריות בין הנוירונים. זאת אומרת, אנחנו יכולים לבחור בין, נניח, רשת עצבית עם שלוש יחידות קלט, חמש יחידות נסתרות וארבע יחידות פלט ובין רשת של 3, 5 מוסתרות, 5 מוסתרות, 4 פלט וכאן יש 3, 5, 5, 5 יחידות בשלוש שכבות נסתרות וארבע יחידות פלט, אז הבחירות האלה של כמה יחידות נסתרות יש בכל שכבה וכמה שכבות נסתרות יש, אלה הן אפשרויות וארכיטקטורה. אז איך עושים את הבחירות האלה? מספר יחידות הקלט כבר מוגדר היטב, ברגע שמחליטים על קבוצת התכונות המדויקת, אז x, מספר יחידות קלט יהיה פשוט מספר המאפיינים של התכונות (x(i וייקבע על ידי כך. ואם אתה עושה סיווגים רב-מחלקתיים מספר היציאות גם הוא נקבע על ידי מספר המחלקות בבעיית הסיווג. תזכורת, אם יש לך סיווג רב-מחלקתי שבו נניח y מקבל ערכים בין 1 ל 10, אז יש לך עשר מחלקות אפשריות. אז זכרו לכתוב את הפלט שלכם כוקטורים כאלה. אז במקום שהפלט יהיה "מחלקה 1", אנחנו כותבים אותו כוקטור של 1 בהתחלה והשאר אפסים, ובמקום "מחלקה 2", הוקטור הזה. אז אם אחת הדוגמאות האלה מקבלת את התוצאה החמישית, דהיינו y שווה 5, אז מה שמוציאים מהרשת העצבית הוא לא באמת ערך של y שווה 5, אלא במקום זה כאן בשכבת הפלט שהיא וקטור פלט של 10 יחידות, אנחנו נקבל וקטור שבו יש 1 במיקום החמישי ואפסים בכל המקומות האחרים. אז נראה שהבחירה של מספר יחידות הקלט ומספר יחידות הפלט היא טבעית למדי. ובאשר למספר היחידות הנסתרות בשכבה, ומספר השכבות הנסתרות, יש ברירת מחדל סבירה והיא להשתמש בשכבה נסתרת אחת, זה סוג הרשת העצבית שמוצג בצד שמאל עם שכבה אחת נסתרת והוא כנראה הנפוץ ביותר. אם תחליטו להשתמש ביותר משכבה מוסתרת אחת, אז שוב יש ברירת מחדל סבירה והיא להציב אותו מספר של יחידות מוסתר בכל שכבה מוסתרת. אז כאן יש לנו שתי שכבות נסתרות ובכל אחת מהשכבות הנסתרות האלה יש אותו מספר, חמש יחידות נסתרות וכאן יש לנו שלוש שכבות נסתרות ובכל אחת מהן יש אותו מספר, כלומר חמש יחידות נסתרות. אבל במקום זה, ארכיטקטורת הרשת בצד שמאל עם שכבה נסתרת אחת
תהיה ברירת מחדל סבירה בהחלט. ובאשר למספר היחידות הנסתרות - הכלל הוא שכמה שיותר יחידות נסתרות יותר טוב; אבל שים לב שאם יש לך הרבה יחידות נסתרות, זה יכול להיות יקר יותר מבחינה חישובית, אבל לעתים קרובות מאוד, כמה שיותר יחידות מוסתרות זה יותר טוב. ובדרך כלל מספר היחידות הנסתרות בכל שכבה יהיה אולי שווה לממד של x, דהיינו שווה למספר התכונות, או שזה יכול להיות איזה מספר ממספר תכונות הקלט ועד אולי פי שלושה או פי ארבעה מזה. אז כאשר מספר היחידות הנסתרות הוא דומה או פי כמה, יותר גדול מאשר מספר תכונות הקלט זה לעתים קרובות רעיון סביר. אז אני מקווה שזה נותן לכם קבוצה סבירה של ברירות מחדל של אפשרויות עבור הארכיטקטורה של רשתות עצביות ואם תפעלו לפי ההנחיות האלה, סביר להניח שתקבלו משהו שעובד היטב, אבל מאוחר יותר תהיה עוד קבוצה של קטעי וידאו שבהם אני אדבר בצורה מפורשת על רעיונות כיצד ליישם אלגוריתמים, ושם אני באמת אדבר הרבה יותר על איך לבחור ארכיטקטורת רשת עצבית. או בעצם יש לי הרבה דברים שאני רוצה לומר מאוחר יותר על איך לעשות בחירות טובות עבור מספר היחידות הנסתרות, מספר השכבות הנסתרות, וכן הלאה. העניין הבא בקשר למה שאנחנו צריכים ליישם כדי צריכים ליישם כדי ללמד רשת עצבית, ישנם למעשה שישה צעדים; יש לנו ארבעה בשקופית הזו ועוד שני צעדים בשקופית הבאה. הצעד הראשון הוא להקים את הרשת העצבית, ולאתחל באופן אקראי את ערכי המשקלים. ואנחנו בדרך כלל מאתחלים משקולות לערכים קטנים בסביבת אפס. לאחר מכן אנו מיישמים את המסירה קדימה, כך שנוכל להזין ערכים לתוך הרשת העצבית ולחשב את H של x שהוא וקטור הפלט של ערכי y. לאחר מכן אנו מיישמים גם קוד לחישוב הפונקציה (J(Θ. ולאחר מכן אנו מיישמים מסירה אחורה, או את אלגוריתם ההפצה לאחור, כדי לחשב את הנגזרות החלקיות במונחים של Θ, נגזרות חלקיות של (J(Θ ביחס לפרמטרים Θ. באופן קונקרטי, מיישמים מסירה אחורה. בדרך כלל אנחנו נעשה את זה בלולאה על דוגמאות האימון. חלק מכם אולי שמעו על שיטות מתקדמות, בכנות מאוד מתקדמות שבהן אין לולאה מעל m דוגמאות האימון, אבל בפעם הראשונה שתיישמו את המסירה אחורה אתם צריכים כמעט בוודאות ליישם את הלולאה בקוד, שבה אתם עוברים על כל הדוגמאות, x1, y1, ועושים מסירה לפנים ומסירה לאחור על הדוגמה הראשונה, ואז באיטרציה השניה של הלולאה, עושים מסירה לפנים ומסירה לאחור על הדוגמה השנייה, וכן הלאה, עד שעברתם על כל הדוגמאות. צריכה להיות לולאה ביישום שלכם של המסירה לאחור, לפחות בפעם הראשונה שאתם מיישמים אותה. ואז יש דרכים שהן באמת מסובכות למדי לעשות את זה בלי לולאה, אבל אני בהחלט לא ממליץ לנסות לעשות גרסה הרבה יותר מסובכת בפעם הראשונה שאתם מנסים ליישם מסירה לאחור. אז מעשית, יש לנו לולאה מעל m דוגמאות האימון שלנו, ובתוך הלולאה אנחנו הולכים לבצע את המסירה קדימה ואת המסירה לאחור כל פעם באמצעות דוגמה אחת. מה שזה אומר הוא שאנחנו הולכים לקחת את (x(i, להאכיל בו את שכבת הקלט שלנו, לבצע מסירה קדימה, לבצע מסירה לאחור ולחשב ולהשתמש בכל המונחים האלה, קודי הפעלה וביטויי δ לכל היחידות בכל השכבות של כל הרשת העצבית, וכל זה עדיין בתוך הלולאה הזו, הרשו לי לצייר כאן סוגריים מסולסלים רק כדי להראות את היקף הלולאה, וכל זה לא קוד אוקטבה כמובן, זה נראה כאן יותר כקוד בג'אווה, ולולאה מקיפה את כל זה. אז בתוך הלולאה אנחנו מחשבים את אותם ביטויי Δ, שהם הנוסחה שראינו קודם. וגם את (δ(l+1, כפול a משוחלפת. ואז לבסוף מחוץ ללולאה אחרי שחישבנו את ביטויי Δ האלה, את הערך באוגרים האלה, יהיה לנו איזה שהוא קוד אחר, והוא יאפשר לנו לחשב את הנגזרות החלקיות האלה. והביטויים של הנגזרות החלקיות האלה צריכים לכלול גם את ביטוי ההסדרה λ. והנוסחאות האלה ניתנו בסרטון הקודם. אז אחרי שעשינו את זה עכשיו יש לקוות שיש לנו קוד לחשב את הנגזרות החלקיות. הגענו לשלב 5, שבו מה שאנחנו עושים הוא להשתמש בבדיקת הגרדיינט כדי לבדוק שהתוצאה שווה בערך לביטויים המחושבים של הנגזרות החלקיות. אז אנחנו משווים את הגרסאות המחושבות באמצעות המסירה לאחור, לנגזרות החלקיות המחושבות באמצעות האומדנים הנומריים (עם שני אפסילון) של הנגזרות. אנחנו עושים בדיקת שיפוע כדי לוודא ששתי השיטות נותנים לנו ערכים דומים מאוד. לאחר ביצוע בדיקת שיפוע, אנחנו רגועים בקשר לכך שהיישום שלנו של המסירה לאחור הוא נכון, וזכרו שלאחר מכן מאוד חשוב לבטל את בדיקת השיפוע, כי הקוד של בדיקת השיפוע הוא איטי מאוד מבחינה חישובית. ולבסוף, בשלב השישי אנחנו משתמשים באלגוריתם אופטימיזציה כגון ירידה במדרון, או אחת משיטות האופטימיזציה המתקדמות כגון L-BFGS, מדרון מוטה כפי שהוא ממומש בתוך fminunc או שיטות אופטימיזציה אחרות. אנו משתמשים בהם ביחד עם מסירה לאחור, המסירה לאחור היא מה שמחשב עבורנו את הנגזרות החלקיות. אז אנו יודעים כיצד לחשב את פונקציית העלות, אנו יודעים כיצד לחשב את הנגזרות החלקיות באמצעות מסירה אחורה, אז אנחנו יכולים להשתמש באחת משיטות האופטימיזציה הללו כדי לנסות למזער את J של Θ כפונקציה של הפרמטרים Θ. ודרך אגב, עבור רשתות עצביות, פונקציית העלות (J(Θ היא לא קמורה, ולכן היא יכולה תיאורטית לסבול ממינימומים מקומיים, ולמעשה אלגוריתמים כמו ירידה במדרון ושיטות אופטימיזציה מתקדמות עלולות בתיאוריה להיתקע באיזשהו מינימום מקומי, אבל מתברר שבפועל זו בדרך כלל לא בעיה רצינית, ולמרות שאי אפשר להבטיח בוודאות שהאלגוריתמים הללו ימצאו מינימום גלובלי, בדרך כלל אלגוריתמים כמו ירידה במדרון יעשו עבודה טובה מאוד במזעור של פונקצית העלות J של Θ ויקבלו מינימום מקומי טוב מאוד, גם אם הן לא בהכרח תגענה למינימום הגלובלי. עוד עניין הוא שהירידה במדרון ברשת עצבית עשויה עדיין להיראות כמין כישוף. אז הרשו לי להראות עוד גרף אחד כדי לנסות להעביר לכם את האינטואיציה לגבי מה עושה הירידה במדרון עבור רשת עצבית. הגרף הזה דומה למעשה לגרף בו השתמשתי פעם כדי להסביר את הירידה במדרון. אז יש לנו איזו פונקצית עלות, ויש לנו מספר פרמטרים ברשת העצבית שלנו. אני ציינתי כאן שני ערכי פרמטרים. במציאות, כמובן, ברשת עצבית, יכולים להיות לנו הרבה פרמטרים כאלה. Θ1, Θ2 - כל אלה הן מטריצות, נכון? אז יכולים להיות לנו פרמטרים בעלי מימדים גבוהים מאוד, אבל בגלל המגבלות על סוגי הדברים שאנחנו יכולים לצייר, אני מעמיד פנים שיש לנו רק שני פרמטרים ברשת העצבית הזאת. למרות שברור שבפועל יש לנו הרבה יותר. עכשיו, פונקציית העלות הזו J של Θ מודדת עד כמה תואמת הרשת העצבית את נתוני האימון. אז אם אתה לוקח נקודה כמו זו כאן למטה, זו נקודה שבה J של Θ הוא די נמוך, ולכן הנקודה הזו מתאימה להגדרה של הפרמטרים, הגדרה של הפרמטרים Θ, שבה עבור רוב דוגמאות האימון, הפלט של ההשערה כנראה די קרוב ל-(y(i כי זה בדיוק מה שגורם לפונקצית העלות להיות די נמוכה. בעוד לעומת זאת, אם היינו לוקחים ערך כזה, נקודה כמו זו, אז בנקודה הזו עבור דוגמאות אימון רבות, הפלט של הרשת העצבית רחוק מהערך האמיתי (y(i שנצפה בקבוצת האימון . אז נקודות כמו זו מימין מתאימות לΘ שבה ההשערה, שבה הרשת העצבית פולטת ערכים רחוקים מ-(y(i לגבי קבוצת האימון. זאת אומרת, היא אינה מתאימה היטב לקבוצת האימון, ואילו נקודות כאלה עם ערכים נמוכים של פונקציית העלות תואמות לתטא שבה J של Θ נמוך, ולכן מתאימות לאזורים שבהם הרשת העצבית במקרה מתאימה היטב לקבוצת האימון שלנו, כי זה מה שצריך לקרות על מנת ש-J של Θ יהיה קטן. אז מה שהירידה במדרון עושה הוא שהיא מתחילה מנקודת מוצא אקראית כמו זו לדוגמא, ויורדת שוב ושוב. ומה שהמסירה לאחור עושה הוא לחשב את כיוון השיפוע, ומה שהירידה במדרון עושה הוא לצעוד צעדים קטנים במורד הגבעה עד שהיא מגיעה, בתקווה, במקרה זה, למינימום מקומי טוב למדי. אז כאשר מיישמים מסירה לאחור ומשתמשים בירידה במדרון או באחת משיטות האופטימיזציה המתקדמות, הגרף הזה מסביר לנו מה האלגוריתם עושה. הוא מנסה למצוא ערך של הפרמטרים שבהם ערכי הפלט ברשת העצבית קרובים לערכים (y(i שנצפו בסדרת האימון שלנו. אז אני מקווה שזה נותן לכם תחושה יותר טובה של איך מתאימים חלקים שונים של הלמידה ברשת עצבית זה לזה. אבל אם אפילו אחרי הסרטון הזה, אם אתה עדיין מרגיש כאילו יש כאן הרבה חלקים שונים ולא לגמרי ברור מה עושים חלק מהם או איך כל החלקים האלה מתחברים, גם זה בסדר גמור. למידה ברשת עצבית ומסירה לאחור הוא אלגוריתם מסובך. ולמרות שאני מכיר את המתמטיקה שמאחורי המסירה לאחור כבר במשך שנים רבות, והשתמשתי במסירה לאחור, אני חושב בהצלחה רבה, במשך שנים רבות, אפילו היום אני עדיין מרגיש כאילו לא תמיד יש לי הבנה מושלמת של מה עושה המסירה לאחור לפעמים. ואיך נראה תהליך האופטימיזציה של המזעור של J של Θ. זה אלגוריתם הרבה יותר קשה להבין, ואני מתמודד הרבה פחות טוב עם מה בדיוק הוא עושה לעומת נניח רגרסיה ליניארית או רגרסיה לוגיסטית. שמתמטית ומושגית הם אלגוריתמים הרבה יותר פשוטים והרבה יותר נקיים. אז אם אתם מרגישים אותו הדבר, זה בעצם בסדר גמור, אבל אם תיישמו ותשתמשו במסירה לאחור, אני מקווה שמה שתגלו הוא שזה אחד מאלגוריתמי הלמידה החזקים ביותר, אם תיישמו את האלגוריתם הזה, את המסירה לאחור, תיישמו את אחת משיטות האופטימיזציה האלה, תגלו שהמסירה לאחור מסוגלת להתאים פונקציות מורכבות מאוד, חזקות ולא ליניאריות לנתונים שלך, ושזה אחד מאלגוריתמי הלמידה היעילים ביותר שיש לנו כיום.