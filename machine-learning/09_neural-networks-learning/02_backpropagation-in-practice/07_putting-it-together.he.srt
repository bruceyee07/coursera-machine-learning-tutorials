1
00:00:00,240 --> 00:00:01,560
אז לקח לנו

2
00:00:01,700 --> 00:00:02,690
הרבה קטעי וידאו כדי לעבור

3
00:00:03,120 --> 00:00:04,480
על אלגוריתם הלמידה של רשתות עצביות.

4
00:00:05,620 --> 00:00:06,640
בסרטון הזה מה שאני רוצה

5
00:00:06,800 --> 00:00:08,090
לעשות הוא לנסות

6
00:00:08,350 --> 00:00:10,040
לחבר את כל החלקים,

7
00:00:10,370 --> 00:00:12,120
לתת סיכום כולל או

8
00:00:12,360 --> 00:00:13,410
להציג את התמונה הרחבה יותר של איך

9
00:00:13,650 --> 00:00:15,290
מתאימים כל החלקים זה לזה

10
00:00:15,530 --> 00:00:16,990
ומהווים את התהליך הכולל של

11
00:00:17,260 --> 00:00:18,830
יישום אלגוריתם למידה של רשת עצבית.

12
00:00:21,870 --> 00:00:23,210
כאשר מאמנים רשת עצבית,

13
00:00:23,280 --> 00:00:24,290
הדבר הראשון שצריך לעשות

14
00:00:24,400 --> 00:00:25,920
הוא לבחור איזו ארכיטקטורת רשת

15
00:00:26,680 --> 00:00:27,950
ובארכיטקטורה אני פשוט

16
00:00:28,200 --> 00:00:30,510
מתכוון לאיזה דגם של קישוריות בין הנוירונים.

17
00:00:31,080 --> 00:00:31,840
זאת אומרת, אנחנו יכולים לבחור

18
00:00:32,700 --> 00:00:33,770
בין, נניח, רשת עצבית

19
00:00:34,230 --> 00:00:35,440
עם שלוש יחידות קלט,

20
00:00:35,960 --> 00:00:37,400
חמש יחידות נסתרות

21
00:00:37,500 --> 00:00:39,560
וארבע יחידות פלט ובין רשת

22
00:00:39,800 --> 00:00:41,460
של 3, 5 מוסתרות, 5

23
00:00:41,700 --> 00:00:43,430
מוסתרות, 4 פלט

24
00:00:43,910 --> 00:00:45,220
וכאן יש 3, 5,

25
00:00:45,550 --> 00:00:47,060
5, 5 יחידות

26
00:00:47,320 --> 00:00:48,870
בשלוש שכבות נסתרות וארבע

27
00:00:49,120 --> 00:00:50,250
יחידות פלט, אז

28
00:00:50,430 --> 00:00:52,000
הבחירות האלה של כמה יחידות

29
00:00:52,270 --> 00:00:53,410
נסתרות יש בכל שכבה

30
00:00:53,810 --> 00:00:55,560
וכמה שכבות נסתרות יש, אלה

31
00:00:55,780 --> 00:00:57,580
הן אפשרויות וארכיטקטורה.

32
00:00:57,910 --> 00:00:58,680
אז איך עושים את הבחירות האלה?

33
00:00:59,710 --> 00:01:01,270
מספר יחידות

34
00:01:01,530 --> 00:01:03,840
הקלט כבר מוגדר היטב,

35
00:01:04,680 --> 00:01:05,960
ברגע שמחליטים על קבוצת

36
00:01:06,580 --> 00:01:07,870
התכונות המדויקת, אז x,

37
00:01:08,080 --> 00:01:09,420
מספר יחידות קלט יהיה פשוט

38
00:01:10,140 --> 00:01:12,180
מספר המאפיינים של התכונות (x(i

39
00:01:12,330 --> 00:01:14,470
וייקבע על ידי כך.

40
00:01:14,760 --> 00:01:15,970
ואם אתה עושה סיווגים

41
00:01:16,210 --> 00:01:17,370
רב-מחלקתיים מספר

42
00:01:17,520 --> 00:01:18,320
היציאות גם הוא

43
00:01:18,420 --> 00:01:19,720
נקבע על ידי מספר

44
00:01:20,060 --> 00:01:22,860
המחלקות בבעיית הסיווג.

45
00:01:23,260 --> 00:01:24,890
תזכורת, אם יש לך

46
00:01:25,160 --> 00:01:27,290
סיווג רב-מחלקתי שבו נניח y

47
00:01:27,570 --> 00:01:28,970
מקבל ערכים בין

48
00:01:30,040 --> 00:01:31,350
1 ל 10, אז יש

49
00:01:31,470 --> 00:01:33,560
לך עשר מחלקות אפשריות.

50
00:01:34,690 --> 00:01:37,200
אז זכרו לכתוב

51
00:01:37,820 --> 00:01:39,340
את הפלט שלכם כוקטורים כאלה.

52
00:01:40,130 --> 00:01:41,560
אז במקום שהפלט יהיה "מחלקה 1", אנחנו

53
00:01:41,730 --> 00:01:42,840
כותבים אותו כוקטור של 1 בהתחלה

54
00:01:43,150 --> 00:01:44,600
והשאר אפסים, ובמקום

55
00:01:44,670 --> 00:01:47,280
"מחלקה 2", הוקטור הזה.

56
00:01:48,130 --> 00:01:49,080
אז אם אחת

57
00:01:49,210 --> 00:01:51,000
הדוגמאות האלה מקבלת

58
00:01:51,140 --> 00:01:53,910
את התוצאה החמישית, דהיינו y שווה 5, אז

59
00:01:54,120 --> 00:01:55,130
מה שמוציאים מהרשת

60
00:01:55,380 --> 00:01:56,840
העצבית הוא לא באמת ערך

61
00:01:57,250 --> 00:01:59,520
של y שווה 5, אלא במקום זה כאן

62
00:02:00,030 --> 00:02:00,950
בשכבת הפלט שהיא

63
00:02:01,280 --> 00:02:02,650
וקטור פלט של 10 יחידות, אנחנו

64
00:02:02,740 --> 00:02:03,920
נקבל

65
00:02:04,070 --> 00:02:05,710
וקטור שבו

66
00:02:07,470 --> 00:02:08,430
יש 1 במיקום

67
00:02:08,770 --> 00:02:11,050
החמישי ואפסים בכל המקומות האחרים.

68
00:02:11,420 --> 00:02:12,470
אז נראה שהבחירה של

69
00:02:12,890 --> 00:02:14,330
מספר יחידות הקלט ומספר יחידות

70
00:02:14,970 --> 00:02:16,600
הפלט היא טבעית למדי.

71
00:02:18,000 --> 00:02:18,950
ובאשר למספר

72
00:02:19,410 --> 00:02:21,040
היחידות הנסתרות בשכבה,

73
00:02:21,140 --> 00:02:23,110
ומספר השכבות הנסתרות,

74
00:02:23,210 --> 00:02:24,350
יש ברירת מחדל סבירה

75
00:02:24,540 --> 00:02:26,010
והיא להשתמש בשכבה נסתרת אחת,

76
00:02:26,660 --> 00:02:28,040
זה סוג הרשת

77
00:02:28,880 --> 00:02:30,400
העצבית שמוצג בצד שמאל עם

78
00:02:30,580 --> 00:02:33,270
שכבה אחת נסתרת והוא כנראה הנפוץ ביותר.

79
00:02:34,490 --> 00:02:35,870
אם תחליטו להשתמש ביותר

80
00:02:36,140 --> 00:02:38,410
משכבה מוסתרת אחת, אז שוב

81
00:02:38,670 --> 00:02:39,600
יש ברירת מחדל סבירה והיא

82
00:02:39,760 --> 00:02:40,950
להציב אותו מספר של

83
00:02:41,130 --> 00:02:42,560
יחידות מוסתר בכל שכבה מוסתרת.

84
00:02:42,810 --> 00:02:44,600
אז כאן יש לנו שתי

85
00:02:45,020 --> 00:02:46,370
שכבות נסתרות ובכל

86
00:02:46,610 --> 00:02:47,650
אחת מהשכבות הנסתרות האלה יש

87
00:02:47,860 --> 00:02:49,500
אותו מספר, חמש יחידות

88
00:02:49,790 --> 00:02:50,740
נסתרות וכאן יש לנו

89
00:02:51,600 --> 00:02:53,020
שלוש שכבות נסתרות

90
00:02:53,170 --> 00:02:54,790
ובכל אחת מהן יש אותו

91
00:02:54,980 --> 00:02:56,400
מספר, כלומר חמש יחידות נסתרות.

92
00:02:57,440 --> 00:02:59,440
אבל במקום זה,

93
00:02:59,740 --> 00:03:02,850
ארכיטקטורת הרשת בצד שמאל עם שכבה נסתרת אחת
תהיה ברירת מחדל סבירה בהחלט.

94
00:03:04,020 --> 00:03:04,780
ובאשר למספר

95
00:03:05,120 --> 00:03:07,040
היחידות הנסתרות - הכלל הוא

96
00:03:07,120 --> 00:03:08,100
שכמה שיותר יחידות נסתרות יותר טוב;

97
00:03:08,560 --> 00:03:09,640
אבל שים לב שאם יש לך

98
00:03:09,900 --> 00:03:11,110
הרבה יחידות נסתרות, זה

99
00:03:11,330 --> 00:03:13,150
יכול להיות יקר יותר מבחינה חישובית, אבל

100
00:03:13,300 --> 00:03:15,850
לעתים קרובות מאוד, כמה שיותר יחידות מוסתרות זה יותר טוב.

101
00:03:17,250 --> 00:03:18,560
ובדרך כלל מספר היחידות

102
00:03:18,720 --> 00:03:20,820
הנסתרות בכל שכבה יהיה אולי

103
00:03:21,080 --> 00:03:22,130
שווה לממד

104
00:03:22,490 --> 00:03:23,670
של x, דהיינו שווה

105
00:03:23,810 --> 00:03:24,950
למספר התכונות, או שזה יכול

106
00:03:25,140 --> 00:03:26,880
להיות איזה מספר ממספר

107
00:03:27,180 --> 00:03:29,590
תכונות הקלט ועד

108
00:03:29,770 --> 00:03:32,430
אולי פי שלושה או פי ארבעה מזה.

109
00:03:32,680 --> 00:03:34,770
אז כאשר מספר היחידות הנסתרות הוא דומה

110
00:03:35,140 --> 00:03:36,350
או פי כמה,

111
00:03:36,410 --> 00:03:37,380
יותר גדול מאשר מספר

112
00:03:37,430 --> 00:03:38,750
תכונות הקלט זה לעתים קרובות

113
00:03:39,280 --> 00:03:41,320
רעיון סביר. אז

114
00:03:42,150 --> 00:03:43,490
אני מקווה שזה נותן לכם

115
00:03:43,810 --> 00:03:45,140
קבוצה סבירה של ברירות מחדל של אפשרויות

116
00:03:45,650 --> 00:03:47,770
עבור הארכיטקטורה של רשתות עצביות

117
00:03:48,200 --> 00:03:49,460
ואם תפעלו לפי ההנחיות האלה, סביר

118
00:03:49,540 --> 00:03:50,580
להניח שתקבלו משהו שעובד

119
00:03:50,930 --> 00:03:52,180
היטב, אבל מאוחר

120
00:03:52,360 --> 00:03:53,770
יותר תהיה עוד קבוצה של קטעי וידאו

121
00:03:54,050 --> 00:03:55,270
שבהם אני אדבר בצורה מפורשת

122
00:03:55,580 --> 00:03:56,900
על רעיונות כיצד ליישם

123
00:03:57,410 --> 00:03:58,770
אלגוריתמים, ושם אני באמת

124
00:03:58,840 --> 00:04:01,880
אדבר הרבה יותר על איך לבחור ארכיטקטורת רשת עצבית.

125
00:04:02,540 --> 00:04:03,920
או בעצם יש לי הרבה

126
00:04:03,970 --> 00:04:04,960
דברים שאני רוצה

127
00:04:04,960 --> 00:04:06,180
לומר מאוחר יותר על איך לעשות בחירות טובות

128
00:04:06,710 --> 00:04:08,780
עבור מספר היחידות הנסתרות, מספר השכבות הנסתרות, וכן הלאה.

129
00:04:10,620 --> 00:04:12,310
העניין הבא בקשר למה שאנחנו

130
00:04:12,420 --> 00:04:13,740
צריכים ליישם כדי

131
00:04:13,860 --> 00:04:15,360
צריכים ליישם כדי ללמד רשת עצבית, ישנם

132
00:04:15,510 --> 00:04:16,820
למעשה שישה צעדים;

133
00:04:17,080 --> 00:04:18,030
יש לנו ארבעה

134
00:04:18,160 --> 00:04:19,100
בשקופית הזו ועוד שני צעדים

135
00:04:19,380 --> 00:04:21,480
בשקופית הבאה.

136
00:04:21,620 --> 00:04:22,220
הצעד הראשון הוא להקים את הרשת

137
00:04:22,430 --> 00:04:23,510
העצבית, ולאתחל באופן

138
00:04:24,080 --> 00:04:25,570
אקראי את ערכי המשקלים.

139
00:04:25,790 --> 00:04:27,000
ואנחנו בדרך כלל מאתחלים

140
00:04:27,080 --> 00:04:29,710
משקולות לערכים קטנים בסביבת אפס.

141
00:04:31,100 --> 00:04:33,120
לאחר מכן אנו מיישמים את המסירה קדימה,

142
00:04:34,080 --> 00:04:35,060
כך שנוכל להזין

143
00:04:35,480 --> 00:04:37,150
ערכים לתוך הרשת העצבית

144
00:04:37,490 --> 00:04:38,860
ולחשב את H של x שהוא

145
00:04:39,070 --> 00:04:40,820
וקטור הפלט של ערכי y.

146
00:04:44,260 --> 00:04:45,910
לאחר מכן אנו מיישמים גם קוד

147
00:04:46,010 --> 00:04:47,500
לחישוב הפונקציה (J(Θ.

148
00:04:49,770 --> 00:04:51,160
ולאחר מכן אנו מיישמים

149
00:04:52,120 --> 00:04:53,330
מסירה אחורה, או את אלגוריתם

150
00:04:54,400 --> 00:04:55,680
ההפצה לאחור, כדי לחשב את

151
00:04:55,910 --> 00:04:58,000
הנגזרות החלקיות במונחים

152
00:04:58,440 --> 00:04:59,830
של Θ, נגזרות חלקיות של (J(Θ

153
00:05:00,340 --> 00:05:04,240
ביחס לפרמטרים Θ. באופן קונקרטי, מיישמים מסירה אחורה.

154
00:05:04,960 --> 00:05:05,880
בדרך כלל אנחנו נעשה את זה

155
00:05:06,250 --> 00:05:08,460
בלולאה על דוגמאות האימון.

156
00:05:09,700 --> 00:05:10,650
חלק מכם אולי שמעו על

157
00:05:10,830 --> 00:05:12,640
שיטות מתקדמות, בכנות מאוד

158
00:05:12,940 --> 00:05:14,500
מתקדמות שבהן אין

159
00:05:14,670 --> 00:05:15,720
לולאה מעל

160
00:05:16,570 --> 00:05:18,580
m דוגמאות האימון, אבל

161
00:05:18,660 --> 00:05:19,900
בפעם הראשונה שתיישמו את המסירה אחורה

162
00:05:20,250 --> 00:05:21,420
אתם צריכים כמעט בוודאות ליישם את

163
00:05:21,420 --> 00:05:22,980
הלולאה בקוד,

164
00:05:23,800 --> 00:05:25,010
שבה אתם עוברים על כל הדוגמאות,

165
00:05:25,810 --> 00:05:27,760
x1, y1,

166
00:05:28,030 --> 00:05:29,510
ועושים מסירה לפנים

167
00:05:29,640 --> 00:05:30,400
ומסירה לאחור על הדוגמה

168
00:05:30,850 --> 00:05:32,510
הראשונה, ואז

169
00:05:32,710 --> 00:05:33,730
באיטרציה השניה של

170
00:05:33,780 --> 00:05:35,360
הלולאה, עושים מסירה לפנים

171
00:05:35,980 --> 00:05:38,050
ומסירה לאחור על הדוגמה השנייה, וכן הלאה,

172
00:05:38,170 --> 00:05:40,900
עד שעברתם על כל הדוגמאות.

173
00:05:41,680 --> 00:05:43,110
צריכה להיות

174
00:05:43,230 --> 00:05:44,250
לולאה ביישום שלכם

175
00:05:45,050 --> 00:05:47,180
של המסירה לאחור, לפחות בפעם הראשונה שאתם מיישמים אותה.

176
00:05:48,120 --> 00:05:49,160
ואז יש דרכים שהן באמת

177
00:05:49,390 --> 00:05:50,520
מסובכות למדי לעשות

178
00:05:50,890 --> 00:05:52,660
את זה בלי לולאה, אבל

179
00:05:52,810 --> 00:05:53,950
אני בהחלט לא ממליץ

180
00:05:54,360 --> 00:05:55,340
לנסות לעשות גרסה הרבה יותר

181
00:05:55,660 --> 00:05:58,420
מסובכת בפעם הראשונה שאתם מנסים ליישם מסירה לאחור.

182
00:05:59,850 --> 00:06:00,920
אז מעשית, יש לנו

183
00:06:01,010 --> 00:06:02,200
לולאה מעל m דוגמאות האימון שלנו,

184
00:06:03,240 --> 00:06:04,630
ובתוך הלולאה אנחנו

185
00:06:04,770 --> 00:06:06,300
הולכים לבצע את המסירה

186
00:06:06,580 --> 00:06:08,090
קדימה ואת המסירה לאחור כל פעם באמצעות דוגמה אחת.

187
00:06:09,310 --> 00:06:10,320
מה שזה אומר הוא

188
00:06:10,560 --> 00:06:12,470
שאנחנו הולכים לקחת את (x(i,

189
00:06:12,690 --> 00:06:14,010
להאכיל בו את שכבת הקלט שלנו,

190
00:06:14,770 --> 00:06:16,370
לבצע מסירה קדימה, לבצע מסירה לאחור

191
00:06:17,370 --> 00:06:18,360
ולחשב ולהשתמש בכל המונחים האלה,

192
00:06:18,430 --> 00:06:19,840
קודי הפעלה

193
00:06:19,930 --> 00:06:22,090
וביטויי δ לכל

194
00:06:22,300 --> 00:06:23,440
היחידות בכל השכבות

195
00:06:23,770 --> 00:06:24,720
של כל הרשת

196
00:06:24,950 --> 00:06:27,170
העצבית, וכל זה

197
00:06:27,610 --> 00:06:28,760
עדיין בתוך הלולאה הזו, הרשו

198
00:06:29,180 --> 00:06:30,450
לי לצייר כאן סוגריים מסולסלים

199
00:06:30,940 --> 00:06:31,950
רק כדי להראות את

200
00:06:32,030 --> 00:06:32,930
היקף הלולאה, וכל

201
00:06:34,160 --> 00:06:35,480
זה לא קוד אוקטבה כמובן, זה נראה כאן יותר

202
00:06:36,190 --> 00:06:38,350
כקוד בג'אווה, ולולאה מקיפה את כל זה.

203
00:06:39,060 --> 00:06:40,060
אז בתוך הלולאה אנחנו מחשבים את אותם ביטויי

204
00:06:40,480 --> 00:06:43,690
Δ, שהם הנוסחה שראינו קודם.

205
00:06:45,540 --> 00:06:47,370
וגם את (δ(l+1, כפול

206
00:06:48,630 --> 00:06:51,150
a משוחלפת.

207
00:06:51,490 --> 00:06:53,540
ואז לבסוף מחוץ ללולאה

208
00:06:54,180 --> 00:06:55,630
אחרי שחישבנו את ביטויי Δ

209
00:06:55,970 --> 00:06:57,550
האלה, את הערך באוגרים האלה, יהיה

210
00:06:57,870 --> 00:06:59,050
לנו איזה שהוא קוד

211
00:06:59,170 --> 00:07:00,430
אחר, והוא

212
00:07:00,720 --> 00:07:03,240
יאפשר לנו לחשב את הנגזרות החלקיות האלה.

213
00:07:03,860 --> 00:07:05,450
והביטויים של הנגזרות

214
00:07:05,970 --> 00:07:07,020
החלקיות האלה צריכים

215
00:07:07,210 --> 00:07:10,270
לכלול גם את ביטוי ההסדרה λ.

216
00:07:11,050 --> 00:07:13,240
והנוסחאות האלה ניתנו בסרטון הקודם.

217
00:07:14,830 --> 00:07:15,720
אז אחרי שעשינו את זה

218
00:07:16,680 --> 00:07:18,080
עכשיו יש לקוות שיש לנו קוד

219
00:07:18,180 --> 00:07:20,050
לחשב את הנגזרות החלקיות.

220
00:07:21,190 --> 00:07:23,030
הגענו לשלב 5, שבו

221
00:07:23,240 --> 00:07:24,420
מה שאנחנו עושים הוא להשתמש בבדיקת

222
00:07:24,730 --> 00:07:26,700
הגרדיינט כדי לבדוק שהתוצאה

223
00:07:27,120 --> 00:07:28,530
שווה בערך לביטויים המחושבים של הנגזרות החלקיות. אז

224
00:07:29,420 --> 00:07:30,980
אנחנו משווים את הגרסאות המחושבות באמצעות

225
00:07:31,270 --> 00:07:33,990
המסירה לאחור,

226
00:07:34,430 --> 00:07:36,470
לנגזרות החלקיות המחושבות באמצעות האומדנים

227
00:07:37,710 --> 00:07:39,850
הנומריים (עם שני אפסילון) של הנגזרות.

228
00:07:40,350 --> 00:07:41,810
אנחנו עושים בדיקת שיפוע כדי לוודא

229
00:07:41,970 --> 00:07:44,340
ששתי השיטות נותנים לנו ערכים דומים מאוד.

230
00:07:45,830 --> 00:07:47,410
לאחר ביצוע בדיקת שיפוע, אנחנו רגועים בקשר

231
00:07:47,910 --> 00:07:49,280
לכך שהיישום שלנו של המסירה

232
00:07:49,590 --> 00:07:51,470
לאחור הוא נכון, וזכרו

233
00:07:51,610 --> 00:07:52,850
שלאחר מכן מאוד חשוב לבטל

234
00:07:53,530 --> 00:07:54,710
את בדיקת השיפוע, כי הקוד של

235
00:07:55,080 --> 00:07:57,150
בדיקת השיפוע הוא איטי מאוד מבחינה חישובית.

236
00:07:59,020 --> 00:08:00,880
ולבסוף, בשלב השישי אנחנו

237
00:08:01,120 --> 00:08:03,280
משתמשים באלגוריתם אופטימיזציה כגון

238
00:08:03,510 --> 00:08:04,940
ירידה במדרון, או אחת

239
00:08:04,960 --> 00:08:07,520
משיטות האופטימיזציה המתקדמות כגון

240
00:08:07,740 --> 00:08:10,020
L-BFGS, מדרון מוטה כפי

241
00:08:10,250 --> 00:08:13,120
שהוא ממומש בתוך fminunc או שיטות אופטימיזציה אחרות.

242
00:08:13,940 --> 00:08:15,500
אנו משתמשים בהם ביחד עם

243
00:08:15,730 --> 00:08:17,380
מסירה לאחור,

244
00:08:17,620 --> 00:08:18,670
המסירה לאחור היא

245
00:08:18,770 --> 00:08:20,640
מה שמחשב עבורנו את הנגזרות החלקיות.

246
00:08:21,730 --> 00:08:22,680
אז אנו יודעים כיצד

247
00:08:22,860 --> 00:08:24,020
לחשב את פונקציית העלות, אנו יודעים

248
00:08:24,100 --> 00:08:25,550
כיצד לחשב את הנגזרות החלקיות באמצעות

249
00:08:25,830 --> 00:08:27,410
מסירה אחורה, אז אנחנו

250
00:08:27,480 --> 00:08:28,830
יכולים להשתמש באחת משיטות האופטימיזציה הללו

251
00:08:29,580 --> 00:08:30,850
כדי לנסות למזער את J של

252
00:08:31,130 --> 00:08:33,500
Θ כפונקציה של הפרמטרים Θ.

253
00:08:34,330 --> 00:08:35,410
ודרך אגב, עבור

254
00:08:35,660 --> 00:08:37,330
רשתות עצביות, פונקציית העלות

255
00:08:38,300 --> 00:08:39,630
(J(Θ היא לא קמורה,

256
00:08:40,530 --> 00:08:42,490
ולכן היא יכולה

257
00:08:43,260 --> 00:08:45,600
תיאורטית לסבול

258
00:08:46,250 --> 00:08:47,480
ממינימומים מקומיים,

259
00:08:47,650 --> 00:08:49,580
ולמעשה אלגוריתמים כמו ירידה במדרון

260
00:08:49,840 --> 00:08:51,950
ושיטות אופטימיזציה מתקדמות עלולות

261
00:08:52,400 --> 00:08:53,660
בתיאוריה להיתקע באיזשהו מינימום

262
00:08:55,190 --> 00:08:56,300
מקומי, אבל מתברר

263
00:08:56,480 --> 00:08:57,680
שבפועל זו בדרך

264
00:08:57,870 --> 00:08:59,230
כלל לא בעיה רצינית,

265
00:08:59,560 --> 00:09:00,800
ולמרות שאי אפשר להבטיח בוודאות

266
00:09:01,210 --> 00:09:02,320
שהאלגוריתמים הללו ימצאו

267
00:09:02,510 --> 00:09:04,260
מינימום גלובלי, בדרך כלל אלגוריתמים כמו

268
00:09:04,390 --> 00:09:05,870
ירידה במדרון יעשו

269
00:09:05,930 --> 00:09:07,700
עבודה טובה מאוד במזעור

270
00:09:07,850 --> 00:09:09,230
של פונקצית העלות J

271
00:09:09,280 --> 00:09:10,350
של Θ ויקבלו

272
00:09:10,420 --> 00:09:11,820
מינימום מקומי טוב מאוד, גם

273
00:09:12,060 --> 00:09:13,690
אם הן לא בהכרח תגענה למינימום הגלובלי.

274
00:09:14,500 --> 00:09:16,950
עוד עניין הוא שהירידה במדרון

275
00:09:17,230 --> 00:09:19,500
ברשת עצבית עשויה עדיין להיראות כמין כישוף.

276
00:09:20,170 --> 00:09:21,680
אז הרשו לי להראות עוד גרף

277
00:09:21,890 --> 00:09:22,990
אחד כדי לנסות להעביר לכם

278
00:09:23,170 --> 00:09:25,660
את האינטואיציה לגבי מה עושה הירידה במדרון עבור רשת עצבית.

279
00:09:27,020 --> 00:09:28,460
הגרף הזה דומה למעשה לגרף

280
00:09:28,590 --> 00:09:31,190
בו השתמשתי פעם כדי להסביר את הירידה במדרון.

281
00:09:31,730 --> 00:09:32,750
אז יש לנו איזו פונקצית

282
00:09:33,090 --> 00:09:34,480
עלות, ויש לנו

283
00:09:34,710 --> 00:09:36,590
מספר פרמטרים ברשת העצבית שלנו.

284
00:09:36,810 --> 00:09:39,190
אני ציינתי כאן שני ערכי פרמטרים.

285
00:09:40,080 --> 00:09:41,250
במציאות, כמובן,

286
00:09:41,520 --> 00:09:43,570
ברשת עצבית, יכולים להיות לנו הרבה פרמטרים כאלה.

287
00:09:44,190 --> 00:09:46,980
Θ1, Θ2 - כל אלה הן מטריצות, נכון?

288
00:09:47,030 --> 00:09:48,130
אז יכולים להיות לנו פרמטרים בעלי

289
00:09:48,580 --> 00:09:49,870
מימדים גבוהים מאוד, אבל בגלל

290
00:09:49,960 --> 00:09:51,620
המגבלות על סוגי

291
00:09:51,790 --> 00:09:52,970
הדברים שאנחנו יכולים לצייר, אני מעמיד פנים

292
00:09:53,410 --> 00:09:55,840
שיש לנו רק שני פרמטרים ברשת העצבית הזאת.

293
00:09:56,270 --> 00:09:56,890
למרות שברור שבפועל יש לנו הרבה יותר.

294
00:09:59,280 --> 00:10:00,700
עכשיו, פונקציית העלות הזו J של

295
00:10:00,800 --> 00:10:02,470
Θ מודדת עד כמה

296
00:10:02,880 --> 00:10:04,730
תואמת הרשת העצבית את נתוני האימון.

297
00:10:06,000 --> 00:10:06,920
אז אם אתה לוקח נקודה

298
00:10:07,120 --> 00:10:08,590
כמו זו כאן למטה,

299
00:10:10,270 --> 00:10:11,180
זו נקודה שבה J

300
00:10:11,460 --> 00:10:12,580
של Θ הוא די נמוך,

301
00:10:12,870 --> 00:10:16,170
ולכן הנקודה הזו מתאימה להגדרה של הפרמטרים,

302
00:10:17,020 --> 00:10:17,840
הגדרה של הפרמטרים

303
00:10:18,350 --> 00:10:19,920
Θ, שבה עבור רוב

304
00:10:20,140 --> 00:10:22,450
דוגמאות האימון, הפלט

305
00:10:24,120 --> 00:10:26,270
של ההשערה כנראה

306
00:10:26,410 --> 00:10:27,420
די קרוב ל-(y(i

307
00:10:27,650 --> 00:10:28,720
כי זה בדיוק

308
00:10:28,840 --> 00:10:31,560
מה שגורם לפונקצית העלות להיות די נמוכה.

309
00:10:32,690 --> 00:10:33,770
בעוד לעומת זאת, אם היינו

310
00:10:33,820 --> 00:10:35,140
לוקחים ערך כזה, נקודה כמו זו,

311
00:10:35,510 --> 00:10:37,260
אז בנקודה הזו

312
00:10:38,080 --> 00:10:39,260
עבור דוגמאות אימון רבות,

313
00:10:39,890 --> 00:10:40,780
הפלט של הרשת

314
00:10:41,040 --> 00:10:42,860
העצבית רחוק

315
00:10:43,110 --> 00:10:44,340
מהערך האמיתי (y(i

316
00:10:44,540 --> 00:10:45,850
שנצפה בקבוצת האימון .

317
00:10:46,610 --> 00:10:47,480
אז נקודות כמו זו

318
00:10:47,590 --> 00:10:50,100
מימין מתאימות לΘ שבה

319
00:10:50,450 --> 00:10:51,450
ההשערה, שבה הרשת

320
00:10:51,740 --> 00:10:53,330
העצבית פולטת ערכים

321
00:10:53,770 --> 00:10:54,810
רחוקים מ-(y(i לגבי

322
00:10:55,020 --> 00:10:56,260
קבוצת האימון. זאת אומרת, היא אינה

323
00:10:56,470 --> 00:10:57,970
מתאימה היטב לקבוצת האימון, ואילו

324
00:10:58,170 --> 00:10:59,640
נקודות כאלה עם ערכים

325
00:10:59,970 --> 00:11:01,300
נמוכים של פונקציית העלות תואמות

326
00:11:02,130 --> 00:11:03,380
לתטא שבה J של Θ

327
00:11:04,130 --> 00:11:05,270
נמוך, ולכן מתאימות

328
00:11:05,950 --> 00:11:07,590
לאזורים שבהם הרשת העצבית

329
00:11:07,850 --> 00:11:09,290
במקרה מתאימה היטב לקבוצת

330
00:11:09,510 --> 00:11:11,340
האימון שלנו, כי זה מה

331
00:11:11,550 --> 00:11:14,070
שצריך לקרות על מנת ש-J של Θ יהיה קטן.

332
00:11:15,480 --> 00:11:16,810
אז מה שהירידה במדרון עושה

333
00:11:16,870 --> 00:11:18,330
הוא שהיא מתחילה מנקודת מוצא

334
00:11:18,730 --> 00:11:20,300
אקראית כמו זו

335
00:11:20,430 --> 00:11:22,990
לדוגמא, ויורדת שוב ושוב.

336
00:11:24,040 --> 00:11:25,400
ומה שהמסירה לאחור

337
00:11:25,570 --> 00:11:27,220
עושה הוא לחשב את כיוון

338
00:11:27,940 --> 00:11:29,370
השיפוע, ומה

339
00:11:29,520 --> 00:11:30,740
שהירידה במדרון עושה הוא

340
00:11:31,040 --> 00:11:32,060
לצעוד צעדים קטנים במורד הגבעה

341
00:11:32,880 --> 00:11:34,220
עד שהיא מגיעה, בתקווה,

342
00:11:34,610 --> 00:11:36,410
במקרה זה, למינימום מקומי טוב למדי.

343
00:11:37,880 --> 00:11:39,250
אז כאשר מיישמים מסירה

344
00:11:39,410 --> 00:11:40,840
לאחור ומשתמשים בירידה

345
00:11:41,200 --> 00:11:42,420
במדרון או באחת

346
00:11:42,840 --> 00:11:44,750
משיטות האופטימיזציה המתקדמות, הגרף הזה

347
00:11:45,330 --> 00:11:47,290
מסביר לנו מה האלגוריתם עושה.

348
00:11:47,450 --> 00:11:48,820
הוא מנסה למצוא ערך

349
00:11:49,260 --> 00:11:50,920
של הפרמטרים שבהם

350
00:11:51,260 --> 00:11:52,180
ערכי הפלט ברשת

351
00:11:52,450 --> 00:11:54,300
העצבית קרובים

352
00:11:54,410 --> 00:11:55,520
לערכים (y(i

353
00:11:55,660 --> 00:11:58,800
שנצפו בסדרת האימון שלנו.

354
00:11:58,910 --> 00:12:00,250
אז אני מקווה שזה נותן לכם

355
00:12:00,400 --> 00:12:01,610
תחושה יותר טובה של איך

356
00:12:01,920 --> 00:12:03,930
מתאימים חלקים שונים

357
00:12:04,120 --> 00:12:05,760
של הלמידה ברשת עצבית זה לזה.

358
00:12:07,120 --> 00:12:09,010
אבל אם אפילו אחרי הסרטון הזה, אם

359
00:12:09,120 --> 00:12:10,130
אתה עדיין מרגיש כאילו

360
00:12:10,360 --> 00:12:11,420
יש כאן הרבה חלקים שונים

361
00:12:12,070 --> 00:12:13,450
ולא לגמרי ברור מה

362
00:12:13,690 --> 00:12:14,670
עושים חלק מהם או איך כל

363
00:12:14,860 --> 00:12:17,760
החלקים האלה מתחברים, גם זה בסדר גמור.

364
00:12:18,790 --> 00:12:21,780
למידה ברשת עצבית ומסירה לאחור הוא אלגוריתם מסובך.

365
00:12:23,000 --> 00:12:23,960
ולמרות שאני מכיר

366
00:12:24,290 --> 00:12:25,340
את המתמטיקה שמאחורי המסירה לאחור

367
00:12:25,860 --> 00:12:26,710
כבר במשך שנים רבות, והשתמשתי

368
00:12:27,030 --> 00:12:28,470
במסירה לאחור, אני חושב בהצלחה

369
00:12:28,680 --> 00:12:30,210
רבה, במשך שנים רבות, אפילו

370
00:12:30,380 --> 00:12:31,510
היום אני עדיין מרגיש כאילו

371
00:12:31,570 --> 00:12:32,670
לא תמיד יש לי הבנה

372
00:12:33,400 --> 00:12:35,610
מושלמת של מה עושה המסירה לאחור לפעמים.

373
00:12:36,200 --> 00:12:37,850
ואיך נראה תהליך האופטימיזציה

374
00:12:38,520 --> 00:12:41,480
של המזעור של J של Θ.

375
00:12:41,920 --> 00:12:42,830
זה אלגוריתם הרבה יותר קשה

376
00:12:43,450 --> 00:12:44,680
להבין, ואני מתמודד הרבה

377
00:12:44,830 --> 00:12:46,590
פחות טוב עם מה בדיוק

378
00:12:46,690 --> 00:12:47,690
הוא עושה

379
00:12:48,240 --> 00:12:49,360
לעומת נניח רגרסיה ליניארית או רגרסיה לוגיסטית.

380
00:12:51,390 --> 00:12:53,180
שמתמטית ומושגית הם אלגוריתמים

381
00:12:53,510 --> 00:12:55,090
הרבה יותר פשוטים והרבה יותר נקיים.

382
00:12:56,200 --> 00:12:57,030
אז אם אתם מרגישים אותו

383
00:12:57,070 --> 00:12:58,560
הדבר, זה בעצם בסדר

384
00:12:58,970 --> 00:13:01,010
גמור, אבל אם

385
00:13:01,170 --> 00:13:02,790
תיישמו ותשתמשו במסירה לאחור, אני מקווה

386
00:13:03,160 --> 00:13:04,260
שמה שתגלו הוא שזה

387
00:13:04,460 --> 00:13:05,410
אחד מאלגוריתמי הלמידה

388
00:13:05,790 --> 00:13:08,030
החזקים ביותר, אם

389
00:13:08,130 --> 00:13:09,510
תיישמו את האלגוריתם הזה, את המסירה לאחור,

390
00:13:10,250 --> 00:13:11,230
תיישמו את אחת משיטות האופטימיזציה

391
00:13:11,340 --> 00:13:13,260
האלה, תגלו

392
00:13:13,610 --> 00:13:14,940
שהמסירה לאחור מסוגלת

393
00:13:15,390 --> 00:13:17,330
להתאים פונקציות מורכבות מאוד, חזקות ולא ליניאריות

394
00:13:17,830 --> 00:13:19,370
לנתונים שלך,

395
00:13:20,080 --> 00:13:21,060
ושזה אחד

396
00:13:21,190 --> 00:13:22,790
מאלגוריתמי הלמידה היעילים ביותר שיש לנו כיום.