1
00:00:00,090 --> 00:00:01,798
בסרטון הקודם, דיברנו על

2
00:00:01,857 --> 00:00:03,868
פונקציית עלות לרשת עצבית.

3
00:00:04,139 --> 00:00:07,079
בסרטון זה, נתחיל לדבר על אלגוריתם

4
00:00:07,200 --> 00:00:09,062
שמנסה למזער את פונקציית העלות.

5
00:00:09,240 --> 00:00:12,735
בפרט, נדבר על אלגוריתם ה"מסירה אחורה" (Back Propagation)

6
00:00:13,834 --> 00:00:15,380
הנה פונקציית העלות

7
00:00:15,520 --> 00:00:17,905
שכתבנו בסרטון הקודם.

8
00:00:17,972 --> 00:00:19,438
מה שאנחנו רוצים לעשות

9
00:00:19,484 --> 00:00:21,161
הוא לנסות למצוא פרמטרים Θ

10
00:00:21,246 --> 00:00:23,440
לנסות למזער את J של Θ.

11
00:00:23,530 --> 00:00:25,782
כדי להשתמש בירידה במדרון

12
00:00:25,832 --> 00:00:28,625
או באחד מאלגוריתמי האופטימיזציה המתקדמים

13
00:00:28,675 --> 00:00:30,206
מה שאנחנו צריכים לעשות הוא

14
00:00:30,249 --> 00:00:31,598
לכתוב קוד שלוקח

15
00:00:31,645 --> 00:00:33,487
כקלט את הפרמטרים Θ

16
00:00:33,540 --> 00:00:34,965
ומחשב את J של Θ

17
00:00:35,014 --> 00:00:37,364
ואת הנגזרות החלקיות.

18
00:00:37,425 --> 00:00:38,763
זכרו שהפרמטרים

19
00:00:38,790 --> 00:00:40,710
ברשת העצבית הם הדברים האלה,

20
00:00:40,760 --> 00:00:43,435
Θ סימן עילי l אינדקס ij,

21
00:00:43,492 --> 00:00:44,868
והוא מספר ממשי

22
00:00:44,930 --> 00:00:47,185
ואלה הם הביטויים של הנגזרות החלקיות

23
00:00:47,249 --> 00:00:48,869
שאנחנו צריכים לחשב.

24
00:00:48,900 --> 00:00:50,077
כדי לחשב את

25
00:00:50,115 --> 00:00:51,840
פונקצית העלות J של Θ,

26
00:00:51,883 --> 00:00:53,986
אנחנו פשוט משתמשים בנוסחה הזו בראש העמוד

27
00:00:54,042 --> 00:00:55,617
ולכן מה שאני רוצה לעשות

28
00:00:55,655 --> 00:00:56,850
ברוב הווידאו הזה הוא

29
00:00:56,897 --> 00:00:58,595
להתמקד ולדבר על

30
00:00:58,636 --> 00:00:59,952
איך אנחנו מחשבים

31
00:00:59,989 --> 00:01:01,994
את הנגזרות החלקיות.

32
00:01:02,031 --> 00:01:03,812
נתחיל לדבר על

33
00:01:03,858 --> 00:01:05,512
המקרה הפשוט בו יש לנו רק

34
00:01:05,556 --> 00:01:06,839
דוגמת אימון אחת,

35
00:01:06,872 --> 00:01:09,385
דמיינו אם תרצו כי כל סדרת האימון

36
00:01:09,432 --> 00:01:11,301
שלנו כוללת רק דוגמת

37
00:01:11,351 --> 00:01:14,006
אימון אחת שהיא הזוג x,y.

38
00:01:14,049 --> 00:01:15,591
אני לא אכתוב x1,y1,

39
00:01:15,629 --> 00:01:16,375
אלא פשוט את זה.

40
00:01:16,410 --> 00:01:17,665
אכתוב את דוגמת האימון האחת

41
00:01:17,718 --> 00:01:19,980
כ-x,y ובואו נעבור על

42
00:01:20,031 --> 00:01:21,423
רצף החישובים

43
00:01:21,462 --> 00:01:24,332
שהיינו עושים עם דוגמת האימון האחת שלנו.

44
00:01:25,754 --> 00:01:27,129
הדבר הראשון שאנחנו עושים הוא

45
00:01:27,167 --> 00:01:29,175
ליישם את ההפצה או המסירה קדימה

46
00:01:29,212 --> 00:01:31,773
כדי לחשב מה ההשערה

47
00:01:31,813 --> 00:01:34,238
בכלל מוציאה על הקלט הנתון.

48
00:01:34,272 --> 00:01:36,734
באופן קונקרטי, זכרו כי

49
00:01:36,769 --> 00:01:39,025
(1)a הוא ערך ההפעלה

50
00:01:39,071 --> 00:01:41,541
של השכבה הראשונה שהיא שכבת הקלט כאן.

51
00:01:41,600 --> 00:01:43,452
אז הקלט שלנו הוא (1)a שהוא x

52
00:01:43,505 --> 00:01:45,389
ועכשיו עלינו לחשב את

53
00:01:45,435 --> 00:01:47,506
(2)z שהוא תטא אינדקס 1 כפול a אינדקס 1

54
00:01:47,552 --> 00:01:49,919
ועכשיו (a(2 שווה ל-g, פונקצית הסיגמואיד

55
00:01:49,980 --> 00:01:52,250
עם הפרמטר (2)z וזה

56
00:01:52,310 --> 00:01:53,753
נותן לנו את קוד

57
00:01:53,800 --> 00:01:56,115
הקלט עבור השכבה האמצעית הראשונה.

58
00:01:56,162 --> 00:01:58,208
עבור שכבה 2 של הרשת,

59
00:01:58,241 --> 00:02:00,649
ואנחנו גם זוכרים להוסיף את מונח ההטיה.

60
00:02:01,315 --> 00:02:03,132
ואז אנחנו עושים עוד 2 שלבים

61
00:02:03,176 --> 00:02:04,966
של המסירה קדימה

62
00:02:05,013 --> 00:02:08,328
כדי לחשב את (3)a ו- (4)a

63
00:02:08,360 --> 00:02:11,458
שהוא בעצם הפלט

64
00:02:11,505 --> 00:02:14,089
של פונקצית ההשערה (h(x.

65
00:02:14,711 --> 00:02:18,103
אז זה היישום הוקטורי שלנו

66
00:02:18,145 --> 00:02:19,228
של המסירה קדימה

67
00:02:19,276 --> 00:02:20,888
וזה מאפשר לנו לחשב

68
00:02:20,938 --> 00:02:22,280
את ערכי ההפעלה או הקלט

69
00:02:22,345 --> 00:02:24,056
עבור כל הנוירונים

70
00:02:24,110 --> 00:02:25,948
ברשת העצבים שלנו.

71
00:02:27,934 --> 00:02:29,608
עכשיו, כדי לחשב

72
00:02:29,650 --> 00:02:30,967
את הנגזרות, אנחנו נשתמש

73
00:02:31,026 --> 00:02:33,589
באלגוריתם שנקרא מסירה או הפצה אחורה.

74
00:02:34,904 --> 00:02:37,765
כדי להבין בצורה אינטואיטיבית את אלגוריתם המסירה לאחור

75
00:02:37,807 --> 00:02:38,430
חשבו שלכל צומת

76
00:02:38,430 --> 00:02:41,065
אנחנו נחשב את המונח

77
00:02:41,126 --> 00:02:43,642
δ (דלתא) סימן עילי l אינדקס j

78
00:02:43,676 --> 00:02:45,130
וזה ייצג איכשהו

79
00:02:45,171 --> 00:02:46,310
את השגיאה

80
00:02:46,361 --> 00:02:48,511
של צומת j בשכבה l.

81
00:02:48,552 --> 00:02:49,682
זכרו כי

82
00:02:49,716 --> 00:02:52,313
a עם סימן עילי l אינדקס j

83
00:02:52,355 --> 00:02:54,138
זה ערך ההפעלה של

84
00:02:54,185 --> 00:02:56,182
היחידה j בשכבה l.

85
00:02:56,224 --> 00:02:58,001
ולכן המונח הזה, δ סימן עילי l אינדקס j

86
00:02:58,045 --> 00:02:59,037
במובן מסוים

87
00:02:59,082 --> 00:03:00,978
משקף את השגיאה

88
00:03:01,012 --> 00:03:03,618
שלנו בהפעלת היחידה הזו.

89
00:03:03,650 --> 00:03:05,798
זאת אומרת, בכמה אנחנו רוצים

90
00:03:05,823 --> 00:03:07,975
שההפעלה של הצומת הזה תהיה שונה.

91
00:03:08,047 --> 00:03:09,670
אז לשם הדוגמא אני לוקח את הדוגמא

92
00:03:10,270 --> 00:03:11,100
של הרשת העצבית שיש לנו

93
00:03:11,360 --> 00:03:12,700
כאן מימין שבה יש ארבע שכבות.

94
00:03:13,440 --> 00:03:15,710
אז L שווה ל-4.

95
00:03:16,060 --> 00:03:17,120
עבור כל יחידת פלט, אנחנו נחשב את המונח δ.

96
00:03:17,400 --> 00:03:19,130
אז, δ עבור היחידה j בשכבה הרביעית שווה

97
00:03:23,380 --> 00:03:24,490
פשוט ערך ההפעלה של

98
00:03:24,720 --> 00:03:26,350
היחידה שהוא הפלט פחות

99
00:03:26,490 --> 00:03:28,650
הערך האמיתי של הדגימה בדוגמת האימון שלנו.

100
00:03:29,900 --> 00:03:32,420
אז המונח הזה כאן

101
00:03:32,580 --> 00:03:34,510
יכול גם להיכתב כ-(h(x

102
00:03:34,710 --> 00:03:38,040
אינדקס j, נכון.

103
00:03:38,330 --> 00:03:39,640
אז המונח הזה δ הוא בעצם רק

104
00:03:39,930 --> 00:03:40,900
ההפרש בין מה

105
00:03:41,290 --> 00:03:43,200
שפלטה ההשערה לבין

106
00:03:43,370 --> 00:03:44,870
הערך של y

107
00:03:45,570 --> 00:03:46,900
בדוגמת האימון שלנו ואילו

108
00:03:47,060 --> 00:03:48,610
y אינדקס j

109
00:03:48,750 --> 00:03:49,910
הוא האלמנט ה-j

110
00:03:50,090 --> 00:03:53,340
של הוקטור y, וקטור התויות בסדרת האימון שלנו.

111
00:03:56,200 --> 00:03:57,790
ודרך אגב, אם

112
00:03:57,970 --> 00:04:00,460
תחשבו על δ, a

113
00:04:01,000 --> 00:04:02,350
ו-y כעל וקטורים אז אתם בעצם

114
00:04:02,520 --> 00:04:03,760
יכולים לכתוב

115
00:04:04,030 --> 00:04:05,890
יישום וקטורי של החישוב

116
00:04:06,010 --> 00:04:07,310
הזה, שהוא פשוט

117
00:04:07,690 --> 00:04:09,840
δ4 מקבל

118
00:04:10,700 --> 00:04:14,330
(a(4 מינוס y.

119
00:04:14,560 --> 00:04:15,820
כאשר כאן, כל אלה, δ4,

120
00:04:16,540 --> 00:04:18,080
a4 ו-y, כל אחד

121
00:04:18,180 --> 00:04:19,860
מהם הוא וקטור עם

122
00:04:20,640 --> 00:04:22,040
מימד ששווה

123
00:04:22,250 --> 00:04:24,150
למספר יחידות הפלט ברשת שלנו.

124
00:04:25,210 --> 00:04:26,880
אז עכשיו חישבנו את

125
00:04:27,320 --> 00:04:28,670
מונח השגיאה δ4

126
00:04:29,020 --> 00:04:30,170
עבור הרשת שלנו.

127
00:04:31,440 --> 00:04:32,950
מה שצריך לעשות עכשיו הוא ללכת אחורה

128
00:04:33,620 --> 00:04:36,280
ולחשב את הביטוי ל-δ עבור השכבות הקודמות ברשת שלנו.

129
00:04:37,210 --> 00:04:38,690
הנה נוסחה לחישוב δ3:

130
00:04:39,010 --> 00:04:39,830
δ3 שווה

131
00:04:40,310 --> 00:04:42,050
Θ3 משוחלף כפול δ4.

132
00:04:42,560 --> 00:04:44,190
והאופרטור "נקודה כפול",

133
00:04:44,390 --> 00:04:46,390
הוא אופרטור המכפלה לפי אלמנט

134
00:04:47,580 --> 00:04:48,380
שאנחנו מכירים מ-MATLAB.

135
00:04:49,160 --> 00:04:50,760
אז Θ3 משוחלפת כפול

136
00:04:51,020 --> 00:04:52,860
δ4, זה וקטור; g תג

137
00:04:53,480 --> 00:04:55,080
של z3 גם הוא וקטור

138
00:04:55,800 --> 00:04:57,370
ולכן "נקודה כפול" ביניהם

139
00:04:57,530 --> 00:04:59,670
הוא הכפל של שני הוקטורים איבר איבר.

140
00:05:01,460 --> 00:05:02,650
המונח הזה g תג

141
00:05:02,740 --> 00:05:04,560
של z3, באופן רשמי הוא למעשה

142
00:05:04,950 --> 00:05:06,420
הנגזרת של פונקצית ההפעלה

143
00:05:06,720 --> 00:05:08,740
או הסיגמואיד g

144
00:05:08,890 --> 00:05:10,620
בנקודה z3.

145
00:05:10,760 --> 00:05:12,620
מי שיודע דיפרנציאלים

146
00:05:12,710 --> 00:05:13,470
יכול לנסות לחשב את זה

147
00:05:13,850 --> 00:05:16,100
בעצמו ולראות שהוא מצליח לפשט ולהגיע לאותה תשובה שאני מקבל.

148
00:05:16,860 --> 00:05:19,690
אבל אני פשוט אגיד לכם מה זה אומר באופן מעשי.

149
00:05:20,000 --> 00:05:21,260
מה שעושים כדי לחשב את g

150
00:05:21,460 --> 00:05:23,310
תג, את הנגזרות החלקיות האלה הוא

151
00:05:23,510 --> 00:05:25,660
פשוט (3)a נקודה כפול

152
00:05:26,010 --> 00:05:27,900
(1 פחות (3)a) כש-(a(3

153
00:05:28,160 --> 00:05:29,420
הוא וקטור ההפעלה של השכבה השלישית,

154
00:05:30,150 --> 00:05:31,440
1 הוא הוקטור של

155
00:05:31,600 --> 00:05:33,240
אחדים ו-(a(3 הוא

156
00:05:34,020 --> 00:05:35,970
שוב וקטור

157
00:05:36,290 --> 00:05:38,850
ההפעלה של השכבה השלישית.

158
00:05:39,170 --> 00:05:40,210
ואז עושים אותו דבר

159
00:05:40,540 --> 00:05:42,850
כדי לחשב את δ2

160
00:05:43,220 --> 00:05:45,230
שזה דבר שניתן לחשב

161
00:05:45,670 --> 00:05:47,410
באמצעות נוסחה דומה.

162
00:05:48,450 --> 00:05:49,950
אלא שעכשיו האיבר הזה הוא (2)a

163
00:05:50,120 --> 00:05:53,850
כך, וכאן אני

164
00:05:53,960 --> 00:05:55,020
לא הוכחתי את זה אבל

165
00:05:55,110 --> 00:05:56,400
זה אפשרי למעשה, אפשר

166
00:05:56,490 --> 00:05:57,520
להוכיח את זה אם יודעים חשבון דיפרנציאלי

167
00:05:58,240 --> 00:05:59,520
שהביטוי הזה שווה

168
00:05:59,860 --> 00:06:02,010
מתמטית לנגזרת של

169
00:06:02,190 --> 00:06:03,570
הפונקציה g, פונקציית

170
00:06:04,040 --> 00:06:05,460
ההפעלה, שאני מסמן כאן

171
00:06:05,910 --> 00:06:08,540
ב-g תג. וכאן,

172
00:06:09,270 --> 00:06:10,690
זה הסוף ואין כאן

173
00:06:10,860 --> 00:06:13,650
המונח של δ1, כי

174
00:06:13,720 --> 00:06:15,590
השכבה הראשונה תואמת

175
00:06:15,630 --> 00:06:16,940
לשכבת הקלט שהוא פשוט

176
00:06:17,000 --> 00:06:18,200
ערך התכונה שראינו

177
00:06:18,300 --> 00:06:20,380
בקבוצת האימון שלנו, כך שאין שום שגיאה המשויכת אליו.

178
00:06:20,600 --> 00:06:22,080
אתם מבינים שאנחנו

179
00:06:22,120 --> 00:06:23,680
לא באמת רוצים לנסות לשנות את הערכים האלה.

180
00:06:24,320 --> 00:06:25,240
ולכן יש לנו ביטוי שמכיל

181
00:06:25,510 --> 00:06:28,090
δ רק לשכבות 2, 3, ו-4 בדוגמה הזו,

182
00:06:30,170 --> 00:06:32,120
השם "מסירה אחורה" מגיע מן

183
00:06:32,170 --> 00:06:33,260
העובדה שאנחנו מתחילים

184
00:06:33,350 --> 00:06:34,720
בחישוב מונח δ עבור

185
00:06:34,740 --> 00:06:36,190
שכבת הפלט ואז

186
00:06:36,370 --> 00:06:37,480
אנחנו חוזרים שכבה אחורה

187
00:06:37,880 --> 00:06:39,670
ומחשבים את מונח δ עבור

188
00:06:39,850 --> 00:06:41,050
השכבה השלישית ואז אנחנו

189
00:06:41,180 --> 00:06:42,540
חוזרים צעד נוסף אחורה ומחשבים

190
00:06:42,770 --> 00:06:44,070
את δ2 וכך, אנחנו כאילו

191
00:06:44,660 --> 00:06:46,060
מוסרים אחורה, מעבירים את השגיאות

192
00:06:46,280 --> 00:06:47,270
של שכבת הפלט לשכבה 3

193
00:06:47,650 --> 00:06:50,180
ומשם עוד אחורה ומכאן השם מסירה לאחור.

194
00:06:51,270 --> 00:06:53,120
עוד נקודה, הגזירה היא

195
00:06:53,340 --> 00:06:56,510
מסובכת להפליא, קשה באופן מפתיע, אבל

196
00:06:56,820 --> 00:06:58,100
אם פשוט תעשו את מספר הצעדים

197
00:06:58,280 --> 00:07:00,130
האלה של חישוב, ניתן

198
00:07:00,680 --> 00:07:02,540
להוכיח את הדבר שהוא בכנות

199
00:07:02,810 --> 00:07:04,440
הוכחה מתמטית מסובכת.

200
00:07:05,200 --> 00:07:07,410
אפשר להוכיח שאם

201
00:07:07,560 --> 00:07:09,690
מתעלמים מההסדרה אז

202
00:07:09,800 --> 00:07:11,080
מונחי הנגזרות החלקיות שאנחנו מחפשים

203
00:07:12,220 --> 00:07:14,650
מתקבלים בדיוק על ידי

204
00:07:14,780 --> 00:07:17,690
הביטויים של ערכי ההפעלה ו-δ.

205
00:07:17,870 --> 00:07:20,630
זה מתעלם מ-λ (למבדא) או

206
00:07:20,780 --> 00:07:22,730
לחילופין אם

207
00:07:23,770 --> 00:07:24,630
הערך של λ

208
00:07:25,000 --> 00:07:25,170
שווה ל-0.

209
00:07:25,680 --> 00:07:27,130
אנו נסדר את הפרט הזה מאוחר

210
00:07:27,470 --> 00:07:29,430
יותר ונכניס פנימה את ההסדרה,

211
00:07:29,620 --> 00:07:30,740
וכך על ידי ביצוע מסירה לאחור

212
00:07:31,610 --> 00:07:32,820
וחשוב של ביטויי ה-δ האלה,

213
00:07:33,180 --> 00:07:34,240
אנחנו מצליחים

214
00:07:34,530 --> 00:07:36,320
לחשב את הנגזרות החלקיות

215
00:07:36,380 --> 00:07:38,150
האלה עבור כל הפרמטרים במהירות גבוהה.

216
00:07:38,920 --> 00:07:40,020
אז יש כאן הרבה פרטים.

217
00:07:40,570 --> 00:07:41,900
בואו ניקח את כל זה ונארגן

218
00:07:42,320 --> 00:07:43,660
את זה כך שנוכל לדבר על

219
00:07:44,120 --> 00:07:45,490
איך ליישם את המסירה אחורה

220
00:07:46,560 --> 00:07:48,590
כדי לחשב נגזרות ביחס לכל הפרמטרים.

221
00:07:49,790 --> 00:07:50,770
ובמקרה

222
00:07:51,000 --> 00:07:52,460
שיש לנו סדרת אימון

223
00:07:52,830 --> 00:07:53,850
גדולה, לא סדרת אימון

224
00:07:54,100 --> 00:07:56,320
של דוגמה אחת, הנה מה שאנחנו עושים.

225
00:07:57,290 --> 00:07:58,140
נניח שיש לנו קבוצת

226
00:07:58,270 --> 00:07:59,750
אימון של m דוגמאות כמו

227
00:07:59,900 --> 00:08:01,610
זו שמוצגת כאן.

228
00:08:01,850 --> 00:08:02,600
הדבר הראשון שאנחנו הולכים לעשות הוא

229
00:08:03,220 --> 00:08:04,560
להגדיר את Δ (דלתא גדולה)

230
00:08:05,100 --> 00:08:07,270
l אינדקס ij. הסמל המשולש הזה?

231
00:08:08,090 --> 00:08:09,990
זוהי האות הגדולה

232
00:08:10,310 --> 00:08:11,980
היוונית Δ.

233
00:08:12,050 --> 00:08:14,080
האות שהיתה לנו בשקופית הקודמת היתה האות הקטנה דלתא (δ).

234
00:08:14,390 --> 00:08:16,810
והמשולש הזה, Δ, הוא דלתא גדולה.

235
00:08:17,430 --> 00:08:18,490
אנחנו הולכים להגדיר שזה שווה לאפס

236
00:08:18,680 --> 00:08:21,930
עבור כל הערכים של l,i,j.

237
00:08:22,110 --> 00:08:23,850
בסופו של דבר, נשתמש באות

238
00:08:24,530 --> 00:08:25,830
הגדולה דלתא Δ הזו

239
00:08:26,860 --> 00:08:29,920
כדי לחשב את הביטויים

240
00:08:30,290 --> 00:08:31,570
של הנגזרות החלקיות, הנגזרות

241
00:08:32,380 --> 00:08:35,240
החלקיות ביחס לתטא lij של

242
00:08:35,430 --> 00:08:37,190
J של Θ.

243
00:08:39,040 --> 00:08:40,210
אז כפי שנראה

244
00:08:40,480 --> 00:08:41,550
בעוד רגע, הדֶלְתות האלה (Δ) עומדות

245
00:08:41,670 --> 00:08:43,700
להיות אוגרים

246
00:08:43,950 --> 00:08:45,360
שיצברו אליהם ערכים לאט לאט

247
00:08:45,700 --> 00:08:47,130
כדי לחשב את הנגזרות החלקיות האלה.

248
00:08:49,570 --> 00:08:51,920
עכשיו אנחנו עומדים לרוץ בלולאה על ערכת האימון שלנו.

249
00:08:52,150 --> 00:08:53,270
אז נאמר ש-i שווה

250
00:08:53,610 --> 00:08:55,400
1 עד m וכשעובדים על

251
00:08:55,620 --> 00:08:57,270
האיטרציה ה-i, אנחנו

252
00:08:57,410 --> 00:08:59,180
עובדים עם דוגמת האימון xi, yi.

253
00:09:00,480 --> 00:09:03,220
אז

254
00:09:03,720 --> 00:09:04,590
הדבר הראשון שאנחנו הולכים לעשות

255
00:09:04,690 --> 00:09:06,120
הוא להגדיר את a1 שהוא

256
00:09:06,570 --> 00:09:07,830
ההפעלה של שכבת הקלט,

257
00:09:08,190 --> 00:09:09,030
להגדיר אותו להיות שווה

258
00:09:09,950 --> 00:09:11,800
ל-xi והוא הקלט עבור דוגמת

259
00:09:12,670 --> 00:09:15,070
האימון i שלנו, אז עכשיו

260
00:09:15,340 --> 00:09:17,590
אנחנו מבצעים מסירה קדימה כדי

261
00:09:17,730 --> 00:09:19,400
לחשב את ההפעלה עבור

262
00:09:19,790 --> 00:09:20,900
שכבה 2, שכבה שלוש וכן

263
00:09:21,170 --> 00:09:22,050
הלאה עד השכבה הסופית,

264
00:09:22,500 --> 00:09:25,190
שכבה L גדולה. אחר כך

265
00:09:25,570 --> 00:09:26,970
אנחנו נשתמש בתווית

266
00:09:27,280 --> 00:09:28,530
הפלט yi מהדוגמה

267
00:09:28,680 --> 00:09:29,870
הספציפית עליה אנחנו מסתכלים

268
00:09:30,340 --> 00:09:31,650
כדי לחשב את ביטוי

269
00:09:31,950 --> 00:09:34,140
השגיאה δL עבור הפלט הזה.

270
00:09:34,480 --> 00:09:35,730
אז δL היא

271
00:09:35,880 --> 00:09:38,190
הפלט של פונקצית ההשערה פחות

272
00:09:38,660 --> 00:09:39,870
הערך הנצפה של תווית היעד.

273
00:09:41,840 --> 00:09:42,560
ואז אנחנו נשתמש

274
00:09:42,850 --> 00:09:44,550
באלגוריתם ההפצה או המסירה לאחור כדי

275
00:09:44,740 --> 00:09:46,020
לחשב את (δ(L-1

276
00:09:46,220 --> 00:09:47,250
(δ(L-2

277
00:09:47,350 --> 00:09:49,880
וכן הלאה עד δ2 ושוב

278
00:09:50,270 --> 00:09:51,380
אין לנו δ1 כי

279
00:09:51,460 --> 00:09:54,380
אנחנו לא משייכים מונח שגיאה לשכבת הקלט עצמה.

280
00:09:57,000 --> 00:09:58,160
ולבסוף, אנחנו הולכים

281
00:09:58,340 --> 00:10:00,650
להשתמש בביטויי ה-Δ

282
00:10:01,190 --> 00:10:02,800
כדי לאגור את הנגזרות

283
00:10:03,400 --> 00:10:05,670
החלקיות שרשמנו בשקף הקודם.

284
00:10:06,870 --> 00:10:07,870
ודרך אגב, אם

285
00:10:07,960 --> 00:10:11,340
תסתכלו על הביטוי הזה, אפשר גם אותו לעשות בצורה וקטורית.

286
00:10:12,020 --> 00:10:13,040
למעשה, אם תחשבו

287
00:10:13,310 --> 00:10:14,860
על Δij כעל

288
00:10:15,000 --> 00:10:18,090
מטריצה, שהאינדקסים שלה הם ij.

289
00:10:19,220 --> 00:10:20,590
אז אם ΔL היא

290
00:10:20,780 --> 00:10:22,040
מטריצה אנחנו יכולים לשכתב

291
00:10:22,130 --> 00:10:24,100
את זה Δl מתעדכן

292
00:10:24,350 --> 00:10:26,710
להיות Δl פלוס

293
00:10:27,830 --> 00:10:29,370
(δ(l+1

294
00:10:29,640 --> 00:10:32,780
כפול (a(l משוחלפת.

295
00:10:33,570 --> 00:10:35,380
אז זהו יישום וקטורי של

296
00:10:35,520 --> 00:10:37,150
החישוב הזה שעושה עדכון

297
00:10:37,590 --> 00:10:38,850
באופן אוטומטי עבור כל הערכים של

298
00:10:39,010 --> 00:10:41,250
i ו-j. לבסוף, לאחר

299
00:10:41,500 --> 00:10:43,480
ביצוע החישובים

300
00:10:43,580 --> 00:10:45,350
בלולאה אנחנו יוצאים מהלולאה

301
00:10:46,330 --> 00:10:47,000
ואנו מחשבים את הדברים הבאים.

302
00:10:47,440 --> 00:10:49,690
אנו מחשבים את D גדולה

303
00:10:50,020 --> 00:10:51,400
כדלקמן, ויש לנו

304
00:10:51,510 --> 00:10:52,750
שני מקרים נפרדים עבור j

305
00:10:52,980 --> 00:10:54,890
שווה ל-0 ועבור j שונה מ-0.

306
00:10:56,080 --> 00:10:57,250
המקרה של j=0

307
00:10:57,680 --> 00:10:58,730
מתאים לביטוי

308
00:10:59,150 --> 00:11:00,030
ההטיה, כאשר j=0

309
00:11:00,390 --> 00:11:01,320
מה שחסר לנו

310
00:11:01,800 --> 00:11:03,320
הוא מונח ההסדרה.

311
00:11:05,470 --> 00:11:06,850
לבסוף, למרות שההוכחה הרשמית

312
00:11:07,180 --> 00:11:08,970
היא די מסובכת, מה

313
00:11:09,030 --> 00:11:10,410
שאפשר להראות הוא שברגע

314
00:11:10,640 --> 00:11:12,530
שחישבתם את ביטויי D האלה,

315
00:11:13,510 --> 00:11:15,230
מה שקיבלתם הוא בדיוק הנגזרת

316
00:11:15,640 --> 00:11:17,610
החלקית של פונקציית

317
00:11:17,920 --> 00:11:19,230
העלות ביחס לכל אחד

318
00:11:19,470 --> 00:11:20,890
מהפרמטרים אז עכשיו

319
00:11:21,040 --> 00:11:22,470
אפשר להשתמש בהם או בירידה

320
00:11:22,610 --> 00:11:23,530
בשיפוע או באחד מהאלגוריתמים המתקדמים.

321
00:11:25,450 --> 00:11:25,450
...

322
00:11:28,310 --> 00:11:29,360
אז זהו אלגוריתם המסירה

323
00:11:29,990 --> 00:11:31,110
לאחור וכיצד מחשבים

324
00:11:31,470 --> 00:11:33,080
נגזרות של פונקציית

325
00:11:33,340 --> 00:11:34,710
העלות שלך עבור רשתות עצביות.

326
00:11:35,470 --> 00:11:36,330
אני יודע שזה נראה כאילו

327
00:11:36,470 --> 00:11:38,810
היו כאן הרבה פרטים והרבה צעדים מחוברים.

328
00:11:39,460 --> 00:11:40,770
אבל הן במשימות

329
00:11:41,100 --> 00:11:43,010
התכנות שתקבלו תיכף והן מאוחר יותר

330
00:11:43,110 --> 00:11:44,580
בסרטונים הבאים, אנו ניתן

331
00:11:44,720 --> 00:11:45,900
לכם סיכום של זה כדי

332
00:11:46,050 --> 00:11:46,830
שזה יהיה אפשרי לחבר את כל החלקים

333
00:11:47,260 --> 00:11:48,780
של האלגוריתם, כך

334
00:11:48,920 --> 00:11:50,550
שתדעו בדיוק מה שאתם צריכים

335
00:11:50,610 --> 00:11:51,760
ליישם כאשר תרצו

336
00:11:51,940 --> 00:11:53,460
ליישם מסירה לאחור כדי לחשב

337
00:11:53,890 --> 00:11:56,432
את הנגזרות של פונקציית העלות של

338
00:11:56,574 --> 00:11:59,348
הרשת העצבית שלכם ביחס לפרמטרים האלה.