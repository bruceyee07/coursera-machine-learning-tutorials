1
00:00:00,280 --> 00:00:04,180
בסרטון הקודם, דיברנו על אלגוריתם המסירה אחורה.

2
00:00:04,180 --> 00:00:06,470
לרבים הרואים אותו בפעם הראשונה,

3
00:00:06,470 --> 00:00:11,390
הרושם הראשון שלהם הוא לעתים קרובות שזה אלגוריתם מסובך באמת,

4
00:00:11,390 --> 00:00:14,700
ויש בו כל מיני שלבים שונים, ואני לא בטוח איך הם משתלבים זה בזה.

5
00:00:14,700 --> 00:00:17,940
וזו מין קופסה שחורה בגלל כל הצעדים המסובכים האלה.

6
00:00:17,940 --> 00:00:21,973
אם כך אתה מרגיש לגבי המסירה אחורה, זה ממש בסדר.

7
00:00:21,973 --> 00:00:26,950
המסירה אחורה אולי לצערי הוא אלגוריתם פחות נקי מתמטית,

8
00:00:26,950 --> 00:00:28,900
או פחות פשוט מתמטית,

9
00:00:28,900 --> 00:00:32,550
לעומת רגרסיה ליניארית או רגרסיה לוגיסטית.

10
00:00:32,550 --> 00:00:36,770
ואני משתמש במסירה אחורה די בהצלחה במשך

11
00:00:36,770 --> 00:00:37,330
שנים רבות.

12
00:00:37,330 --> 00:00:41,800
וגם היום אני לפעמים עדיין לא מרגיש שיש לי הבנה טובה מאוד של

13
00:00:41,800 --> 00:00:45,620
מה שהוא עושה, או אינטואיציה על מה עושה המסירה לאחור.

14
00:00:45,620 --> 00:00:49,780
בשביל אלה מכם שעושים את תרגילי התכנות,

15
00:00:49,780 --> 00:00:52,850
הוידאו הזה יוליך אתכם לפחות מכנית

16
00:00:52,850 --> 00:00:55,240
דרך השלבים השונים של איך ליישם את המסירה אחורה.

17
00:00:55,240 --> 00:00:57,880
כדי שתוכלו בעצמכם לגרום לו לעבוד.

18
00:00:57,880 --> 00:01:02,670
ומה שאני רוצה לעשות בסרטון הזה הוא לבחון קצת יותר לעומק את הצעדים

19
00:01:02,670 --> 00:01:07,230
המכניים של המסירה אחורה, ולנסות לתת לכם קצת יותר אינטואיציה על

20
00:01:07,230 --> 00:01:11,160
הצעדים המכניים שעושה המסירה לאחור בתקווה לשכנע אתכם

21
00:01:11,160 --> 00:01:12,910
שזה לפחות אלגוריתם סביר.

22
00:01:13,940 --> 00:01:19,960
אם גם אחרי הוידאו הזה עדיין נראית המסירה אחורה כקופסה שחורה מאוד

23
00:01:19,960 --> 00:01:22,390
עם צעדים מורכבים מדי

24
00:01:22,390 --> 00:01:25,540
וקצת כמו קסם או כישוף, זה בסדר גמור.

25
00:01:25,540 --> 00:01:30,283
ולמרות שהשתמשתי במסירה אחורה במשך שנים רבות, לפעמים גם לי זה אלגוריתם

26
00:01:30,283 --> 00:01:34,697
קשה להבנה, אבל אני מקווה שהוידאו הזה יעזור קצת.

27
00:01:34,697 --> 00:01:37,896
כדי להבין טוב יותר את המסירה לאחור,

28
00:01:37,896 --> 00:01:42,544
הבה נבחן מקרוב את המסירה קדימה.

29
00:01:42,544 --> 00:01:47,938
הנה רשת עצבית עם שתי יחידות קלט מלבד יחידת ההטיה,

30
00:01:47,938 --> 00:01:53,310
ויש כאן שתי יחידות נסתרות בשכבה הזו, ושתי יחידות נסתרות בשכבה הבאה.

31
00:01:53,310 --> 00:01:55,630
ואז, בסוף, יחידת פלט אחת.

32
00:01:55,630 --> 00:02:01,650
שוב, אנחנו סופרים שתיים, שתיים, שתיים, ולא סופרים את יחידות ההטיה האלה למעלה.

33
00:02:01,650 --> 00:02:04,360
כדי להדגים מסירה קדימה,

34
00:02:04,360 --> 00:02:06,640
אני אצייר את הרשת טיפה אחרת.

35
00:02:08,030 --> 00:02:11,780
בפרט אני אצייר את הרשת העצבית הזאת עם הצמתים מוארכים,

36
00:02:11,780 --> 00:02:15,870
כמו האליפסה הזו, כדי שאוכל לכתוב בהם טקסט.

37
00:02:15,870 --> 00:02:19,750
בעת ביצוע מסירה קדימה, ייתכן שיש לנו דוגמה מסוימת.

38
00:02:19,750 --> 00:02:22,660
נניח איזו דוגמה xi,yi.

39
00:02:22,660 --> 00:02:27,140
ואנחנו מאכילים את שכבת הקלט שלנו בערך xi.

40
00:02:27,140 --> 00:02:33,046
לדוגמא מאכילים את שתי יחידות הקלט ב-x(i)1 וב-x(i)2.

41
00:02:33,046 --> 00:02:37,833
וכאשר אנו מוסרים קדימה אל השכבה הראשונה הנסתרת כאן,

42
00:02:37,833 --> 00:02:41,460
מה שאנחנו עושים הוא לחשב z(2)1 ו -z(2)2.

43
00:02:41,460 --> 00:02:46,780
אז אלה הם הסכומים המשוקללים של הקלט מיחידות הקלט.

44
00:02:46,780 --> 00:02:50,380
ואז אנו מפעילים את פונקצית הסיגמואיד, הפונקציה הלוגיסטית,

45
00:02:50,380 --> 00:02:55,490
מפעילים את פונקציית ההפעלה על הערך z.

46
00:02:55,490 --> 00:02:57,850
אלה ערכי ההפעלה.

47
00:02:57,850 --> 00:03:00,970
מה שנותן לנו את a(2)1 ואת a(2)2.

48
00:03:00,970 --> 00:03:05,640
ואז אנחנו מוסרים קדימה שוב כדי להגיע ל- z(3)1.

49
00:03:05,640 --> 00:03:08,490
מפעילים את הסיגמואיד או הפונקציה הלוגיסטית,

50
00:03:08,490 --> 00:03:11,720
פונקציית ההפעלה כדי לקבל את a(3)1.

51
00:03:11,720 --> 00:03:17,910
וכן הלאה, עד שאנחנו מקבלים את z(4)1,

52
00:03:17,910 --> 00:03:19,210
מפעילים את פונקצית ההפעלה,

53
00:03:19,210 --> 00:03:23,710
מה שנותן לנו את a(4)1, שהוא ערך הפלט הסופי של הרשת העצבית.

54
00:03:24,910 --> 00:03:27,800
בואו נמחק את החץ הזה כדי לתת לי עוד מקום.

55
00:03:27,800 --> 00:03:32,320
ואם נסתכל על מה באמת עושה החישוב הזה,

56
00:03:32,320 --> 00:03:35,500
תוך התמקדות, נניח, ביחידה הנסתרת הזו.

57
00:03:35,500 --> 00:03:37,550
אנחנו צריכים לחבר את המשקל הזה

58
00:03:37,550 --> 00:03:44,780
המוצג בסגול כאן, המשקל הוא Θ(2)1,0, האינדקס לא חשוב.

59
00:03:44,780 --> 00:03:49,290
והמשקל הזה כאן, שאני מדגיש באדום,

60
00:03:49,290 --> 00:03:53,840
זה Θ(2)1,1 והמשקל הזה כאן,

61
00:03:53,840 --> 00:03:59,118
שאני מצייר בטורקיז, הוא Θ(2)1,2.

62
00:03:59,118 --> 00:04:04,532
אז הדרך בה אנו מחשבים את הערך הזה, z(3)1 היא,

63
00:04:04,532 --> 00:04:11,620
z(3)1 שווה למשקל הסגול כפול הערך הזה.

64
00:04:11,620 --> 00:04:15,530
שהוא Θ(2)1,0 כפול 1.

65
00:04:15,530 --> 00:04:20,500
ועוד המשקל האדום כפול הערך הזה,

66
00:04:20,500 --> 00:04:25,740
שהוא Θ(2)1,1 כפול a(2)1.

67
00:04:25,740 --> 00:04:30,680
ועוד המשקל בטורקיז כפול הערך הזה,

68
00:04:30,680 --> 00:04:39,320
דהיינו פלוס Θ(2)1,2 כפול 2(2)a.

69
00:04:39,320 --> 00:04:41,084
וזו היא המסירה קדימה.

70
00:04:41,084 --> 00:04:44,839
כפי שנראה בהמשך הסרטון הזה,

71
00:04:44,839 --> 00:04:49,840
המסירה אחורה עושה תהליך מאוד דומה לזה.

72
00:04:49,840 --> 00:04:54,330
אלא שבמקום החישובים שזורמים במסירה קדימה משמאל לימין,

73
00:04:54,330 --> 00:04:59,610
החישובים במסירה לאחור זורמים מימין לשמאל של הרשת.

74
00:04:59,610 --> 00:05:02,460
תוך שימוש בחישוב דומה מאוד לזה.

75
00:05:02,460 --> 00:05:06,560
ואני אסביר תוך שתי שקופיות בדיוק למה אני מתכוון.

76
00:05:06,560 --> 00:05:10,630
כדי להבין טוב יותר מה עושה המסירה אחורה, בואו נסתכל על פונקציית העלות.

77
00:05:10,630 --> 00:05:15,410
זו אותה פונקציית העלות שהיתה לנו כאשר היתה לנו רק יחידת פלט אחת.

78
00:05:15,410 --> 00:05:17,320
אם יש לנו יותר מאשר יחידת פלט אחת,

79
00:05:17,320 --> 00:05:21,670
אנחנו צריכים לעשות סכימה על יחידות הפלט עם אינדקס k שם.

80
00:05:21,670 --> 00:05:25,200
אבל אם יש רק יחידת פלט אחת אז זו פשוט פונקציה עלות.

81
00:05:25,200 --> 00:05:30,410
ואנחנו עושים מסירה לפנים ומסירה לאחור על דוגמת לימוד אחת בכל פעם.

82
00:05:30,410 --> 00:05:35,010
אז בואו פשוט נתמקד בדוגמה אחת, (x(i),y(i

83
00:05:35,010 --> 00:05:37,210
ונתמקד במקרה של יחידת פלט אחת.

84
00:05:37,210 --> 00:05:40,400
אז (y(i כאן הוא פשוט מספר ממשי.

85
00:05:40,400 --> 00:05:43,420
ובואו נתעלם כרגע מההסדרה, אז λ שווה 0.

86
00:05:43,420 --> 00:05:47,360
וגם המונח האחרון הזה, מונח ההסדרה, פשוט נעלם.

87
00:05:47,360 --> 00:05:49,667
עכשיו אם נסתכל בתוך הסיכום,

88
00:05:49,667 --> 00:05:54,208
נמצא שהביטוי של העלות המשויך לדוגמת האימון הספציפית,

89
00:05:54,208 --> 00:05:58,627
דהיינו העלות המשויכת לדוגמת האימון (x(i),y(i

90
00:05:58,627 --> 00:06:01,690
ניתנת על ידי הביטוי הזה.

91
00:06:01,690 --> 00:06:06,729
אז העלות המשויכת לדוגמא i נכתבת כך.

92
00:06:06,729 --> 00:06:11,230
ופונקציית העלות משחקת תפקיד דומה לריבוע השגיאה.

93
00:06:11,230 --> 00:06:14,010
אז במקום להסתכל על הביטוי המסובך הזה,

94
00:06:14,010 --> 00:06:18,040
אם תרצו אתם יכולים לחשוב על העלות של דוגמא i כבערך ריבוע

95
00:06:18,040 --> 00:06:22,750
ההפרש בין מה שפלטה רשת עצבית לבין הערך של הדוגמה בפועל.

96
00:06:22,750 --> 00:06:25,830
בדיוק כמו ברגרסיה לוגיסטית, אנחנו למעשה מעדיפים להשתמש בפונקצית עלות

97
00:06:25,830 --> 00:06:28,330
קצת יותר מסובכת שמשתמשת בלוג.

98
00:06:28,330 --> 00:06:32,060
אבל למטרות אינטואיציה, אל תהססו לחשוב על פונקציית העלות

99
00:06:32,060 --> 00:06:34,880
כמין פונקציית עלות השגיאה בריבוע.

100
00:06:34,880 --> 00:06:38,820
אז העלות של דוגמא i מודדת עד כמה טובה הרשת

101
00:06:38,820 --> 00:06:40,820
בניבוי התוצאה של דוגמה i.

102
00:06:40,820 --> 00:06:45,630
כמה קרוב הפלט לתווית שנצפתה בפועל (y(i

103
00:06:45,630 --> 00:06:48,510
עכשיו בואו נראה מה עושה המסירה אחורה.

104
00:06:48,510 --> 00:06:53,640
אינטואיציה שימושית אחת היא שהמסירה אחורה מחשבת את

105
00:06:53,640 --> 00:06:56,690
ערכי ה"שגיאות" דלתא l)j).

106
00:06:56,690 --> 00:07:02,080
ואנחנו יכולים לחשוב על אלה כמו ה"שגיאה" בערך ההפעלה

107
00:07:02,080 --> 00:07:05,969
שקיבלנו עבור היחידה j בשכבה ה-l.

108
00:07:07,120 --> 00:07:10,060
או בצורה יותר רשמית, אולי רק עבור אלה

109
00:07:10,060 --> 00:07:12,690
מכם שיודעים חשבון דיפרנציאלי.

110
00:07:12,690 --> 00:07:15,950
מבחינה פורמלית יותר, מונחי ה-δ האלה

111
00:07:15,950 --> 00:07:19,050
הם הנגזרות החלקיות ביחס ל- z(l)j,

112
00:07:19,050 --> 00:07:23,440
כלומר זהו הסכום המשוקלל של הקלטים שתרמו לחישוב ערכי z.

113
00:07:23,440 --> 00:07:25,929
הנגזרות החלקיות ביחס לדברים אלה בפונקציית העלות.

114
00:07:27,000 --> 00:07:31,230
אז בצורה מדויקת, פונקציית העלות היא פונקציה של התווית y

115
00:07:31,230 --> 00:07:34,958
והערך h של x שהוא הפלט של הרשת העצבית.

116
00:07:34,958 --> 00:07:37,800
ואם היינו יכולים להיכנס לתוך הרשת העצבית

117
00:07:37,800 --> 00:07:41,480
ופשוט לשנות את הערכים האלה z(l)j קצת,

118
00:07:41,480 --> 00:07:45,610
אז זה ישפיע על הערכים אותם פולטת הרשת העצבית,

119
00:07:45,610 --> 00:07:49,080
וזה בסופו של דבר יגרום לשינוי פונקציית העלות.

120
00:07:49,080 --> 00:07:53,030
ושוב באמת, זה רק לאלה מכם שמרגישים טוב עם נגזרות.

121
00:07:53,030 --> 00:07:56,560
לאלה שמרגישים בנוח עם נגזרות חלקיות,

122
00:07:56,560 --> 00:08:00,830
אז ביטויי δ האלה הם נגזרות חלקיות של

123
00:08:00,830 --> 00:08:04,529
פונקציית העלות, בנקודות של צמתי הביניים האלה שאנחנו מחשבים.

124
00:08:06,000 --> 00:08:10,430
ולכן הם מדד טוב לכמה היינו רוצים לשנות את המשקלות של הרשת העצבית

125
00:08:10,430 --> 00:08:15,870
כדי להשפיע על ערכי הביניים האלה של החישוב.

126
00:08:15,870 --> 00:08:19,190
שזה יגרום להשפיע על הפלט הסופי (h(x של הרשת העצבית, וכך

127
00:08:19,190 --> 00:08:21,570
להשפיע על העלות הכוללת.

128
00:08:21,570 --> 00:08:25,190
במקרה שהקטע האחרון של האינטואיציה על נגזרות חלקיות,

129
00:08:25,190 --> 00:08:26,780
אם זה לא נראה לכם הגיוני.

130
00:08:26,780 --> 00:08:28,540
אבל אל תטריחו את עצמכם אם קשה לכם,

131
00:08:28,540 --> 00:08:32,350
אנחנו בהחלט יכולים לעשות את זה בלי לדבר על נגזרות חלקיות.

132
00:08:32,350 --> 00:08:35,950
אבל בואו נראה בפירוט רב יותר מה עושה בדיוק המסירה אחורה.

133
00:08:35,950 --> 00:08:40,366
עבור שכבת הפלט, אנחנו מחשבים בהתחלה את ה-δ הזה,

134
00:08:40,366 --> 00:08:45,610
δ(4)1, כ-(y(i כשאנחנו עושים מסירה קדימה

135
00:08:45,610 --> 00:08:49,880
ומסירה אחורה על דוגמת האימון הזו, i.

136
00:08:49,880 --> 00:08:52,750
זאת אומרת y(i)-a(4)1.

137
00:08:52,750 --> 00:08:54,440
אז זו בעצם השגיאה, נכון?

138
00:08:54,440 --> 00:08:57,640
זה ההבדל בין הערך בפועל של (y(i פחות

139
00:08:57,640 --> 00:09:01,910
הערך החזוי על ידי הפונקציה, ולכן אנחנו מחשבים את δ(4)1 כך.

140
00:09:01,910 --> 00:09:06,920
בשלב הבא אנחנו מפיצים או מוסרים את הערכים האלה לאחור.

141
00:09:06,920 --> 00:09:10,360
אני אסביר את זה בשנייה, וכך מחשבים את הביטויים של δ עבור

142
00:09:10,360 --> 00:09:11,040
השכבה הקודמת.

143
00:09:11,040 --> 00:09:13,020
יצא לנו δ(3)1.

144
00:09:13,020 --> 00:09:14,520
δ(3)2.

145
00:09:14,520 --> 00:09:19,982
ואז אנחנו מוסרים את זה שוב אחורה,

146
00:09:19,982 --> 00:09:25,464
ומחשבים את δ(2)1 ו-δ(2)2.

147
00:09:25,464 --> 00:09:29,680
החישוב של המסירה אחורה דומה מאוד

148
00:09:29,680 --> 00:09:33,290
להפעלת אלגוריתם המסירה קדימה, אבל הוא עושה את זה לאחור.

149
00:09:33,290 --> 00:09:34,230
הנה מה שאני מתכוון.

150
00:09:34,230 --> 00:09:37,490
בואו נראה איך הגענו לערך הזה של δ(2)2.

151
00:09:37,490 --> 00:09:40,442
אז יש לנו δ(2)2.

152
00:09:40,442 --> 00:09:45,060
ובדומה למסירה קדימה, הרשו לי לשים תוויות על כמה מהמשקלות.

153
00:09:45,060 --> 00:09:47,590
אז המשקל הזה, שאני אצייר בטורקיז,

154
00:09:47,590 --> 00:09:54,040
נניח כי המשקל הזה הוא Θ(2)1,2,

155
00:09:54,040 --> 00:09:57,280
וזה כאן למטה שאנו מדגישים באדום,

156
00:09:57,280 --> 00:10:02,990
זה יהיה נניח Θ(2)2,2.

157
00:10:02,990 --> 00:10:06,690
אז אם אנחנו מסתכלים איך מחושב δ(2)2,

158
00:10:06,690 --> 00:10:09,440
איך מחושב הצומת הזה,

159
00:10:09,440 --> 00:10:12,040
אז מה שאנחנו צריכים לעשות, הוא לקחת את הערך הזה

160
00:10:12,040 --> 00:10:19,080
להכפיל אותו במשקל הזה, ולחבר אליו את הערך הזה מוכפל במשקל הזה.

161
00:10:19,080 --> 00:10:23,450
אז זה בעצם סכום של ערכי δ האלה

162
00:10:23,450 --> 00:10:25,590
משוקללים על ידי המשקל של הקשתות המתאימות.

163
00:10:25,590 --> 00:10:31,351
אז באופן מעשי, הרשו לי למלא את זה, δ(2)2 יהיה שווה

164
00:10:31,351 --> 00:10:38,100
ל-Θ(2)1,2 הסגולה כפול δ(3)1.

165
00:10:38,100 --> 00:10:41,533
ועוד הדבר הזה באדום,

166
00:10:41,533 --> 00:10:46,700
זה Θ(2)2,2 כפול δ(3)2.

167
00:10:46,700 --> 00:10:50,440
אז זה ממש המשקל האדום כפול הערך הזה,

168
00:10:50,440 --> 00:10:53,530
ועוד המשקל הסגול כפול הערך הזה.

169
00:10:53,530 --> 00:10:56,880
וכך הגענו לערך הזה של δ.

170
00:10:56,880 --> 00:10:59,980
ורק בתור דוגמה נוספת, בואו נסתכל על הערך הזה.

171
00:10:59,980 --> 00:11:01,300
איך הגענו לערך הזה?

172
00:11:01,300 --> 00:11:03,531
ובכן באופן דומה.

173
00:11:03,531 --> 00:11:08,224
אם המשקל הזה, שאני מדגיש בירוק,

174
00:11:08,224 --> 00:11:12,826
אם המשקל הזה שווה, למשל, Θ(3)1,2.

175
00:11:12,826 --> 00:11:20,282
אז δ(3)2 יהיה שווה למשקל הירוק הזה,

176
00:11:20,282 --> 00:11:24,515
Θ(3)1,2 כפול δ(4)1.

177
00:11:24,515 --> 00:11:28,950
ודרך אגב, עד כה כתבתי את ערכי δ רק עבור

178
00:11:28,950 --> 00:11:33,640
היחידות הנסתרות, אבל לא ליחידות ההטיה.

179
00:11:33,640 --> 00:11:36,660
תלוי איך אתה מגדיר את אלגוריתם המסירה אחורה, או

180
00:11:36,660 --> 00:11:40,500
בהתאם לאופן שבו אתה מיישם את זה, אתה עשוי ליישם

181
00:11:40,500 --> 00:11:44,980
משהו שמחשב ערכי δ גם עבור יחידות ההטיה.

182
00:11:44,980 --> 00:11:48,700
יחידות ההטיה תמיד מוציאות את הערך 1, הן פשוט מה שהן,

183
00:11:48,700 --> 00:11:51,190
ואין לנו אפשרות לשנות את הערך.

184
00:11:51,190 --> 00:11:53,790
וכך, בהתאם ליישום שלך למסירה אחורה,

185
00:11:53,790 --> 00:11:55,380
הדרך שבה אני בדרך כלל מיישם את זה.

186
00:11:55,380 --> 00:11:57,710
אני בסופו של דבר כן מחשב את ערכי δ האלה, אבל

187
00:11:57,710 --> 00:11:59,760
אנחנו פשוט זורקים אותם, אנחנו לא משתמשים בהם.

188
00:11:59,760 --> 00:12:03,910
כי הם לא באמת חלק מהחישוב הדרוש כדי

189
00:12:03,910 --> 00:12:05,710
לחשב נגזרת.

190
00:12:05,710 --> 00:12:08,790
אז אני מקווה שזה נותן לך קצת יותר אינטואיציה

191
00:12:08,790 --> 00:12:12,500
על מה עושה המסירה אחורה.

192
00:12:12,500 --> 00:12:14,760
במקרה שכל זה עדיין נראה מין כישוף כזה,

193
00:12:14,760 --> 00:12:19,658
סוג של קופסה שחורה, אז בסרטון מאוחר יותר,
וידאו של "מחברים את הכל לתמונה אחת", אני אנסה

194
00:12:19,658 --> 00:12:23,290
לתת עוד קצת אינטואיציה על מה עושה המסירה אחורה.

195
00:12:23,290 --> 00:12:27,690
אבל לצערי זה אלגוריתם שקשה לנסות לראות בעיני רוחנו

196
00:12:27,690 --> 00:12:29,540
ולהבין מה הוא באמת עושה.

197
00:12:29,540 --> 00:12:31,260
אבל למרבה המזל אני,

198
00:12:31,260 --> 00:12:35,480
ואני מניח עוד אנשים רבים משתמשים בו בהצלחה רבה במשך שנים רבות.

199
00:12:35,480 --> 00:12:40,130
ואם תיישם את האלגוריתם אתה יכול לקבל אלגוריתם למידה יעיל מאוד.

200
00:12:40,130 --> 00:12:43,580
למרות שפעולתו הפנימית או איך הוא עובד יכול להיות קשה יותר לדמיין.