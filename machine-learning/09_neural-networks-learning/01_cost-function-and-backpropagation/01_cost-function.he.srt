1
00:00:00,260 --> 00:00:03,190
רשתות עצביות הן אחד מאלגוריתמי הלמידה החזקים ביותר

2
00:00:03,190 --> 00:00:04,360
שיש לנו כיום.

3
00:00:04,360 --> 00:00:08,390
בסרטון זה ובסרטונים הבאים, אני רוצה להתחיל לדבר על אלגוריתם למידה

4
00:00:08,390 --> 00:00:13,360
עבור התאמת הפרמטרים של רשת עצבית בהינתן קבוצת אימון.

5
00:00:13,360 --> 00:00:16,570
כמו בדיון של רוב אלגוריתמי הלמידה שלנו, אנחנו נתחיל

6
00:00:16,570 --> 00:00:20,729
בדיון על פונקציית העלות עבור התאמת הפרמטרים של הרשת.

7
00:00:22,290 --> 00:00:25,840
אני אתמקד ביישום של רשתות עצביות

8
00:00:25,840 --> 00:00:27,700
בבעיות סיווג.

9
00:00:27,700 --> 00:00:31,340
אז נניח שיש לנו רשת כזאת כמו שמוצגת משמאל.

10
00:00:31,340 --> 00:00:34,820
ונניח שיש לנו קבוצת אימון כמו זה של זוגות ⁽x⁽ⁱ

11
00:00:34,820 --> 00:00:36,869
⁽y⁽ⁱ ויש לנו m דוגמאות אימון.

12
00:00:38,030 --> 00:00:41,790
אני הולך להשתמש ב-L גדולה כדי לציין את המספר הכולל

13
00:00:41,790 --> 00:00:43,340
של שכבות ברשת זו.

14
00:00:43,340 --> 00:00:47,420
אז עבור הרשת המוצגת בצד שמאל יש לנו L גדולה שווה 4.

15
00:00:47,420 --> 00:00:52,390
ואני אשתמש ב-sₗ דהיינו s עם אינדקס l, לציין את מספר היחידות,

16
00:00:52,390 --> 00:00:54,390
כלומר מספר הנוירונים,

17
00:00:54,390 --> 00:00:57,830
חוץ מיחידת ההטיה, בשכבה l של הרשת.

18
00:00:57,830 --> 00:01:02,360
כך, למשל, יהיה לנו s₁, שהוא שווה כאן,

19
00:01:02,360 --> 00:01:06,900
שווה ל-3, ו-S₂ בדוגמה שלנו הוא חמש יחידות.

20
00:01:06,900 --> 00:01:09,590
ושכבת הפלט s₄,

21
00:01:09,590 --> 00:01:13,280
שהיא גם שווה ל-s עם אינדקס L גדולה כי L גדולה שווה ל-4,

22
00:01:13,280 --> 00:01:16,470
בשכבת הפלט בדוגמה שלנו כאן יש ארבע יחידות.

23
00:01:17,650 --> 00:01:20,470
אנחנו נדון בשני סוגים של בעיות סיווג.

24
00:01:20,470 --> 00:01:25,370
הראשון הוא סיווג בינארי, שבו התוויות y הן 0 או 1.

25
00:01:25,370 --> 00:01:30,340
במקרה זה, תהיה לנו יחידת פלט אחת, אז ברשת העצבית הזו

26
00:01:30,340 --> 00:01:35,210
יש 4 יחידות פלט, אבל כשיש לנו סיווג בינארי, יש לנו רק

27
00:01:35,210 --> 00:01:40,310
יחידת פלט אחת שמחשבת את (h(x.

28
00:01:40,310 --> 00:01:46,670
והפלט של הרשת העצבית הוא (h(x שיהיה מספר ממשי.

29
00:01:46,670 --> 00:01:49,460
ובמקרה זה מספר יחידות הפלט,

30
00:01:49,460 --> 00:01:54,050
s אינדקס L, כי שוב L הוא האינדקס של השכבה האחרונה.

31
00:01:54,050 --> 00:01:56,810
כי זה מספר השכבות שיש לנו ברשת

32
00:01:56,810 --> 00:02:00,770
אז מספר היחידות שיש לנו בשכבת הפלט יהיה 1.

33
00:02:00,770 --> 00:02:05,410
במקרה שלנו כדי לפשט את הסימון מאוחר יותר, אני גם אגדיר ש-K=1,

34
00:02:05,410 --> 00:02:11,430
אתם יכולים לחשוב על K כעוד סימון של מספר היחידות בשכבת הפלט.

35
00:02:11,430 --> 00:02:14,080
הסוג השני של בעיית סיווג שנדון בו יהיה

36
00:02:14,080 --> 00:02:19,180
בעיית סיווג רב מחלקתית, שבה יש לנו K מחלקות שונות.

37
00:02:19,180 --> 00:02:24,910
אז לדוגמה הייצוג בדוגמא הראשונה שלנו היה כזה עבור y אם יש לנו 4 מחלקות,

38
00:02:24,910 --> 00:02:28,940
ובמקרה זה יהיו לנו K יחידות פלט

39
00:02:28,940 --> 00:02:34,980
והפלט של פונקציית ההשערה שלנו הוא וקטורים בעלי ממד K.

40
00:02:34,980 --> 00:02:38,990
ומספר יחידות הפלט יהיה שווה ל-K.

41
00:02:38,990 --> 00:02:44,020
ובדרך כלל יש לנו K גדול שווה 3 במקרה כזה, כי

42
00:02:44,020 --> 00:02:48,660
אם היו לנו רק שתי מחלקות, אז אנחנו לא צריכים להשתמש בשיטת אחד-מול-השאר.

43
00:02:48,660 --> 00:02:53,170
אנו משתמשים בשיטת אחד-מול-השאר רק אם יש לנו K גדול או שווה

44
00:02:53,170 --> 00:02:58,640
3 מחלקות, וכשיש לנו רק שתי מחלקות אנחנו פשוט משתמשים ביחידת פלט אחת.

45
00:02:58,640 --> 00:03:01,030
עכשיו בואו נגדיר את פונקציית העלות עבור הרשת העצבית שלנו.

46
00:03:03,910 --> 00:03:08,140
פונקציית העלות שבה אנו משתמשים עבור הרשת העצבית תהיה הכללה

47
00:03:08,140 --> 00:03:10,960
של הפונקציה בה אנו משתמשים עבור רגרסיה לוגיסטית.

48
00:03:10,960 --> 00:03:15,450
עבור רגרסיה לוגיסטית מזערנו את פונקצית העלות (J(θ,

49
00:03:15,450 --> 00:03:20,480
שהיתה מינוס 1 חלקי m כפול ביטוי העלות הזה, והוספנו

50
00:03:20,480 --> 00:03:25,790
את ביטוי ההסדרה כאן, וזה היה סכום מ-J=1 עד n,

51
00:03:25,790 --> 00:03:29,580
כי אנחנו לא מסדירים את מונח ההטיה θ₀.

52
00:03:31,060 --> 00:03:35,670
עבור רשת עצבית, פונקציית העלות שלנו תהיה הכללה של זה.

53
00:03:35,670 --> 00:03:40,360
כאשר במקום שיש לנו רק ערך אחד של הפלט של הרגרסיה הלוגיסטית,

54
00:03:40,360 --> 00:03:42,600
יש לנו במקום זה K כאלה.

55
00:03:42,600 --> 00:03:44,790
אז הנה פונקציית העלות שלנו.

56
00:03:44,790 --> 00:03:49,296
הרשת העצבית שלנו מוציאה עכשיו קלטים שהם וקטורים ב-Rᴷ כאשר K יכול להיות שווה 1 אם יש לנו

57
00:03:49,296 --> 00:03:51,430
בעיה סיווג בינארית.

58
00:03:51,430 --> 00:03:56,894
אני אשתמש בסימון (h(x אינדקס i כדי לציין את הפלט ה-i.

59
00:03:56,894 --> 00:04:02,990
כלומר, (h(x הוא וקטור k-מימדי והאינדקס i פשוט

60
00:04:02,990 --> 00:04:07,750
בוחר את האלמנט ה-i של וקטור הפלט של הרשת העצבית.

61
00:04:08,930 --> 00:04:12,690
פונקצית העלות שלנו (J(Θ עכשיו תהיה הדבר הבא.

62
00:04:12,690 --> 00:04:17,480
1- חלקי m כפול הסכום של המונח הזה שהוא דומה למה

63
00:04:17,480 --> 00:04:22,510
שהיה לנו ברגרסיה לוגיסטית, אלא שיש לנו כאן הסכום מ-k=1 עד K.

64
00:04:22,510 --> 00:04:25,655
סיכום זה הוא בעצם סכום על K יחידות

65
00:04:25,655 --> 00:04:28,945
הפלט שלנו. אז אם יש לנו ארבע יחידות פלט,

66
00:04:28,945 --> 00:04:33,635
כלומר אם בשכבה האחרונה של הרשת העצבית יש ארבע יחידות פלט,

67
00:04:33,635 --> 00:04:39,125
אז זה סכום מ-k=1 עד 4 של

68
00:04:39,125 --> 00:04:43,355
בעצם הפונקציה של העלות של אלגוריתם הרגרסיה הלוגיסטית אבל

69
00:04:43,355 --> 00:04:48,545
מסכמים את פונקצית העלות על כל אחד מארבע יחידות הפלט.

70
00:04:48,545 --> 00:04:52,880
אז אתם יכולים לראות בפרט שזה חל על yₖ hₖ,

71
00:04:52,880 --> 00:04:58,700
כי אנחנו בעצם לוקחים את K יחידות הפלט, ומשווים אותם לערך של

72
00:04:58,700 --> 00:05:06,210
yₖ שהוא אחד מאותם וקטורים שלפיהם אנחנו מחשבים את המחיר.

73
00:05:06,210 --> 00:05:10,810
ולבסוף, המונח השני כאן הוא מונח ההסדרה,

74
00:05:10,810 --> 00:05:14,120
בדומה למה שהיה לנו ברגרסיה לוגיסטית.

75
00:05:14,120 --> 00:05:18,040
מונח הסיכום הזה נראה באמת מסובך, אבל כל מה שהוא עושה זה

76
00:05:18,040 --> 00:05:24,370
סיכום על הביטויים האלה ⁽Θⱼᵢ⁽ˡ עבור כל הערכים של i, j ו-l.

77
00:05:24,370 --> 00:05:28,930
חוץ מזה שאנחנו לא מסכמים את המונחים המקבילים לערכי ההטיה האלה,

78
00:05:28,930 --> 00:05:30,950
כמו ברגרסיה לוגיסטית.

79
00:05:30,950 --> 00:05:36,730
מעשית אנחנו לא מסכמים את הביטויים שמתאימים למקום שבו i שווה 0.

80
00:05:36,730 --> 00:05:41,670
מפני שכאשר אנחנו מחשבים את ההפעלה של נוירון,

81
00:05:41,670 --> 00:05:43,411
יש לנו מונחים כאלה.

82
00:05:43,411 --> 00:05:44,356
Θᵢ₀x₀

83
00:05:44,356 --> 00:05:51,410
ועוד Θᵢ₁x₁ וכו'

84
00:05:51,410 --> 00:05:55,240
ואני משער שנשים כאן 2, זה הראשון שם.

85
00:05:55,240 --> 00:05:57,750
אז הערכים עם אפס שם,

86
00:05:57,750 --> 00:06:01,590
שמתאימים למשהו שמכפילים אותו ב-x0 או a0.

87
00:06:01,590 --> 00:06:06,450
אז זה די דומה ליחידת הטיה ובאנלוגיה למה שעשינו עבור

88
00:06:06,450 --> 00:06:10,539
רגרסיה לוגיסטית, לא נסכם את המונחים האלה במונח ההסדרה שלנו,

89
00:06:10,539 --> 00:06:15,030
כי אנחנו לא רוצים להסדיר אותם ולהוריד את ערכיהם לאפס.

90
00:06:15,030 --> 00:06:20,820
אבל זו רק קונוונציה אפשרית אחת, וגם לו סיכמנו על i=0

91
00:06:20,820 --> 00:06:25,560
עד sl, זה יעבוד בערך אותו דבר ולא ישנה הרבה.

92
00:06:25,560 --> 00:06:29,410
אבל אולי המוסכמה הזו של לא להסדיר את מונח ההטייה

93
00:06:29,410 --> 00:06:30,470
היא פשוט קצת יותר נפוצה.

94
00:06:33,090 --> 00:06:36,890
אז זוהי פונקציית העלות שבה אנחנו נשתמש עבור הרשת העצבית שלנו.

95
00:06:36,890 --> 00:06:41,290
בסרטון הבא נתחיל לדבר על אלגוריתם

96
00:06:41,290 --> 00:06:42,900
שינסה לבצע אופטימיזציה של פונקציית העלות.