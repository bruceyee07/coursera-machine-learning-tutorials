בסרטון הקודם, דיברנו על פונקציית עלות לרשת עצבית. בסרטון זה, נתחיל לדבר על אלגוריתם שמנסה למזער את פונקציית העלות. בפרט, נדבר על אלגוריתם ה"מסירה אחורה" (Back Propagation) הנה פונקציית העלות שכתבנו בסרטון הקודם. מה שאנחנו רוצים לעשות הוא לנסות למצוא פרמטרים Θ לנסות למזער את J של Θ. כדי להשתמש בירידה במדרון או באחד מאלגוריתמי האופטימיזציה המתקדמים מה שאנחנו צריכים לעשות הוא לכתוב קוד שלוקח כקלט את הפרמטרים Θ ומחשב את J של Θ ואת הנגזרות החלקיות. זכרו שהפרמטרים ברשת העצבית הם הדברים האלה, Θ סימן עילי l אינדקס ij, והוא מספר ממשי ואלה הם הביטויים של הנגזרות החלקיות שאנחנו צריכים לחשב. כדי לחשב את פונקצית העלות J של Θ, אנחנו פשוט משתמשים בנוסחה הזו בראש העמוד ולכן מה שאני רוצה לעשות ברוב הווידאו הזה הוא להתמקד ולדבר על איך אנחנו מחשבים את הנגזרות החלקיות. נתחיל לדבר על המקרה הפשוט בו יש לנו רק דוגמת אימון אחת, דמיינו אם תרצו כי כל סדרת האימון שלנו כוללת רק דוגמת אימון אחת שהיא הזוג x,y. אני לא אכתוב x1,y1, אלא פשוט את זה. אכתוב את דוגמת האימון האחת כ-x,y ובואו נעבור על רצף החישובים שהיינו עושים עם דוגמת האימון האחת שלנו. הדבר הראשון שאנחנו עושים הוא ליישם את ההפצה או המסירה קדימה כדי לחשב מה ההשערה בכלל מוציאה על הקלט הנתון. באופן קונקרטי, זכרו כי (1)a הוא ערך ההפעלה של השכבה הראשונה שהיא שכבת הקלט כאן. אז הקלט שלנו הוא (1)a שהוא x ועכשיו עלינו לחשב את (2)z שהוא תטא אינדקס 1 כפול a אינדקס 1 ועכשיו (a(2 שווה ל-g, פונקצית הסיגמואיד עם הפרמטר (2)z וזה נותן לנו את קוד הקלט עבור השכבה האמצעית הראשונה. עבור שכבה 2 של הרשת, ואנחנו גם זוכרים להוסיף את מונח ההטיה. ואז אנחנו עושים עוד 2 שלבים של המסירה קדימה כדי לחשב את (3)a ו- (4)a שהוא בעצם הפלט של פונקצית ההשערה (h(x. אז זה היישום הוקטורי שלנו של המסירה קדימה וזה מאפשר לנו לחשב את ערכי ההפעלה או הקלט עבור כל הנוירונים ברשת העצבים שלנו. עכשיו, כדי לחשב את הנגזרות, אנחנו נשתמש באלגוריתם שנקרא מסירה או הפצה אחורה. כדי להבין בצורה אינטואיטיבית את אלגוריתם המסירה לאחור חשבו שלכל צומת אנחנו נחשב את המונח δ (דלתא) סימן עילי l אינדקס j וזה ייצג איכשהו את השגיאה של צומת j בשכבה l. זכרו כי a עם סימן עילי l אינדקס j זה ערך ההפעלה של היחידה j בשכבה l. ולכן המונח הזה, δ סימן עילי l אינדקס j במובן מסוים משקף את השגיאה שלנו בהפעלת היחידה הזו. זאת אומרת, בכמה אנחנו רוצים שההפעלה של הצומת הזה תהיה שונה. אז לשם הדוגמא אני לוקח את הדוגמא של הרשת העצבית שיש לנו כאן מימין שבה יש ארבע שכבות. אז L שווה ל-4. עבור כל יחידת פלט, אנחנו נחשב את המונח δ. אז, δ עבור היחידה j בשכבה הרביעית שווה פשוט ערך ההפעלה של היחידה שהוא הפלט פחות הערך האמיתי של הדגימה בדוגמת האימון שלנו. אז המונח הזה כאן יכול גם להיכתב כ-(h(x אינדקס j, נכון. אז המונח הזה δ הוא בעצם רק ההפרש בין מה שפלטה ההשערה לבין הערך של y בדוגמת האימון שלנו ואילו y אינדקס j הוא האלמנט ה-j של הוקטור y, וקטור התויות בסדרת האימון שלנו. ודרך אגב, אם תחשבו על δ, a ו-y כעל וקטורים אז אתם בעצם יכולים לכתוב יישום וקטורי של החישוב הזה, שהוא פשוט δ4 מקבל (a(4 מינוס y. כאשר כאן, כל אלה, δ4, a4 ו-y, כל אחד מהם הוא וקטור עם מימד ששווה למספר יחידות הפלט ברשת שלנו. אז עכשיו חישבנו את מונח השגיאה δ4 עבור הרשת שלנו. מה שצריך לעשות עכשיו הוא ללכת אחורה ולחשב את הביטוי ל-δ עבור השכבות הקודמות ברשת שלנו. הנה נוסחה לחישוב δ3: δ3 שווה Θ3 משוחלף כפול δ4. והאופרטור "נקודה כפול", הוא אופרטור המכפלה לפי אלמנט שאנחנו מכירים מ-MATLAB. אז Θ3 משוחלפת כפול δ4, זה וקטור; g תג של z3 גם הוא וקטור ולכן "נקודה כפול" ביניהם הוא הכפל של שני הוקטורים איבר איבר. המונח הזה g תג של z3, באופן רשמי הוא למעשה הנגזרת של פונקצית ההפעלה או הסיגמואיד g בנקודה z3. מי שיודע דיפרנציאלים יכול לנסות לחשב את זה בעצמו ולראות שהוא מצליח לפשט ולהגיע לאותה תשובה שאני מקבל. אבל אני פשוט אגיד לכם מה זה אומר באופן מעשי. מה שעושים כדי לחשב את g תג, את הנגזרות החלקיות האלה הוא פשוט (3)a נקודה כפול (1 פחות (3)a) כש-(a(3 הוא וקטור ההפעלה של השכבה השלישית, 1 הוא הוקטור של אחדים ו-(a(3 הוא שוב וקטור ההפעלה של השכבה השלישית. ואז עושים אותו דבר כדי לחשב את δ2 שזה דבר שניתן לחשב באמצעות נוסחה דומה. אלא שעכשיו האיבר הזה הוא (2)a כך, וכאן אני לא הוכחתי את זה אבל זה אפשרי למעשה, אפשר להוכיח את זה אם יודעים חשבון דיפרנציאלי שהביטוי הזה שווה מתמטית לנגזרת של הפונקציה g, פונקציית ההפעלה, שאני מסמן כאן ב-g תג. וכאן, זה הסוף ואין כאן המונח של δ1, כי השכבה הראשונה תואמת לשכבת הקלט שהוא פשוט ערך התכונה שראינו בקבוצת האימון שלנו, כך שאין שום שגיאה המשויכת אליו. אתם מבינים שאנחנו לא באמת רוצים לנסות לשנות את הערכים האלה. ולכן יש לנו ביטוי שמכיל δ רק לשכבות 2, 3, ו-4 בדוגמה הזו, השם "מסירה אחורה" מגיע מן העובדה שאנחנו מתחילים בחישוב מונח δ עבור שכבת הפלט ואז אנחנו חוזרים שכבה אחורה ומחשבים את מונח δ עבור השכבה השלישית ואז אנחנו חוזרים צעד נוסף אחורה ומחשבים את δ2 וכך, אנחנו כאילו מוסרים אחורה, מעבירים את השגיאות של שכבת הפלט לשכבה 3 ומשם עוד אחורה ומכאן השם מסירה לאחור. עוד נקודה, הגזירה היא מסובכת להפליא, קשה באופן מפתיע, אבל אם פשוט תעשו את מספר הצעדים האלה של חישוב, ניתן להוכיח את הדבר שהוא בכנות הוכחה מתמטית מסובכת. אפשר להוכיח שאם מתעלמים מההסדרה אז מונחי הנגזרות החלקיות שאנחנו מחפשים מתקבלים בדיוק על ידי הביטויים של ערכי ההפעלה ו-δ. זה מתעלם מ-λ (למבדא) או לחילופין אם הערך של λ שווה ל-0. אנו נסדר את הפרט הזה מאוחר יותר ונכניס פנימה את ההסדרה, וכך על ידי ביצוע מסירה לאחור וחשוב של ביטויי ה-δ האלה, אנחנו מצליחים לחשב את הנגזרות החלקיות האלה עבור כל הפרמטרים במהירות גבוהה. אז יש כאן הרבה פרטים. בואו ניקח את כל זה ונארגן את זה כך שנוכל לדבר על איך ליישם את המסירה אחורה כדי לחשב נגזרות ביחס לכל הפרמטרים. ובמקרה שיש לנו סדרת אימון גדולה, לא סדרת אימון של דוגמה אחת, הנה מה שאנחנו עושים. נניח שיש לנו קבוצת אימון של m דוגמאות כמו זו שמוצגת כאן. הדבר הראשון שאנחנו הולכים לעשות הוא להגדיר את Δ (דלתא גדולה) l אינדקס ij. הסמל המשולש הזה? זוהי האות הגדולה היוונית Δ. האות שהיתה לנו בשקופית הקודמת היתה האות הקטנה דלתא (δ). והמשולש הזה, Δ, הוא דלתא גדולה. אנחנו הולכים להגדיר שזה שווה לאפס עבור כל הערכים של l,i,j. בסופו של דבר, נשתמש באות הגדולה דלתא Δ הזו כדי לחשב את הביטויים של הנגזרות החלקיות, הנגזרות החלקיות ביחס לתטא lij של J של Θ. אז כפי שנראה בעוד רגע, הדֶלְתות האלה (Δ) עומדות להיות אוגרים שיצברו אליהם ערכים לאט לאט כדי לחשב את הנגזרות החלקיות האלה. עכשיו אנחנו עומדים לרוץ בלולאה על ערכת האימון שלנו. אז נאמר ש-i שווה 1 עד m וכשעובדים על האיטרציה ה-i, אנחנו עובדים עם דוגמת האימון xi, yi. אז הדבר הראשון שאנחנו הולכים לעשות הוא להגדיר את a1 שהוא ההפעלה של שכבת הקלט, להגדיר אותו להיות שווה ל-xi והוא הקלט עבור דוגמת האימון i שלנו, אז עכשיו אנחנו מבצעים מסירה קדימה כדי לחשב את ההפעלה עבור שכבה 2, שכבה שלוש וכן הלאה עד השכבה הסופית, שכבה L גדולה. אחר כך אנחנו נשתמש בתווית הפלט yi מהדוגמה הספציפית עליה אנחנו מסתכלים כדי לחשב את ביטוי השגיאה δL עבור הפלט הזה. אז δL היא הפלט של פונקצית ההשערה פחות הערך הנצפה של תווית היעד. ואז אנחנו נשתמש באלגוריתם ההפצה או המסירה לאחור כדי לחשב את (δ(L-1 (δ(L-2 וכן הלאה עד δ2 ושוב אין לנו δ1 כי אנחנו לא משייכים מונח שגיאה לשכבת הקלט עצמה. ולבסוף, אנחנו הולכים להשתמש בביטויי ה-Δ כדי לאגור את הנגזרות החלקיות שרשמנו בשקף הקודם. ודרך אגב, אם תסתכלו על הביטוי הזה, אפשר גם אותו לעשות בצורה וקטורית. למעשה, אם תחשבו על Δij כעל מטריצה, שהאינדקסים שלה הם ij. אז אם ΔL היא מטריצה אנחנו יכולים לשכתב את זה Δl מתעדכן להיות Δl פלוס (δ(l+1 כפול (a(l משוחלפת. אז זהו יישום וקטורי של החישוב הזה שעושה עדכון באופן אוטומטי עבור כל הערכים של i ו-j. לבסוף, לאחר ביצוע החישובים בלולאה אנחנו יוצאים מהלולאה ואנו מחשבים את הדברים הבאים. אנו מחשבים את D גדולה כדלקמן, ויש לנו שני מקרים נפרדים עבור j שווה ל-0 ועבור j שונה מ-0. המקרה של j=0 מתאים לביטוי ההטיה, כאשר j=0 מה שחסר לנו הוא מונח ההסדרה. לבסוף, למרות שההוכחה הרשמית היא די מסובכת, מה שאפשר להראות הוא שברגע שחישבתם את ביטויי D האלה, מה שקיבלתם הוא בדיוק הנגזרת החלקית של פונקציית העלות ביחס לכל אחד מהפרמטרים אז עכשיו אפשר להשתמש בהם או בירידה בשיפוע או באחד מהאלגוריתמים המתקדמים. ... אז זהו אלגוריתם המסירה לאחור וכיצד מחשבים נגזרות של פונקציית העלות שלך עבור רשתות עצביות. אני יודע שזה נראה כאילו היו כאן הרבה פרטים והרבה צעדים מחוברים. אבל הן במשימות התכנות שתקבלו תיכף והן מאוחר יותר בסרטונים הבאים, אנו ניתן לכם סיכום של זה כדי שזה יהיה אפשרי לחבר את כל החלקים של האלגוריתם, כך שתדעו בדיוק מה שאתם צריכים ליישם כאשר תרצו ליישם מסירה לאחור כדי לחשב את הנגזרות של פונקציית העלות של הרשת העצבית שלכם ביחס לפרמטרים האלה.