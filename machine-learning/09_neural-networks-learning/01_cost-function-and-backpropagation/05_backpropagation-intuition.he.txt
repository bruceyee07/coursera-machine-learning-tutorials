בסרטון הקודם, דיברנו על אלגוריתם המסירה אחורה. לרבים הרואים אותו בפעם הראשונה, הרושם הראשון שלהם הוא לעתים קרובות שזה אלגוריתם מסובך באמת, ויש בו כל מיני שלבים שונים, ואני לא בטוח איך הם משתלבים זה בזה. וזו מין קופסה שחורה בגלל כל הצעדים המסובכים האלה. אם כך אתה מרגיש לגבי המסירה אחורה, זה ממש בסדר. המסירה אחורה אולי לצערי הוא אלגוריתם פחות נקי מתמטית, או פחות פשוט מתמטית, לעומת רגרסיה ליניארית או רגרסיה לוגיסטית. ואני משתמש במסירה אחורה די בהצלחה במשך שנים רבות. וגם היום אני לפעמים עדיין לא מרגיש שיש לי הבנה טובה מאוד של מה שהוא עושה, או אינטואיציה על מה עושה המסירה לאחור. בשביל אלה מכם שעושים את תרגילי התכנות, הוידאו הזה יוליך אתכם לפחות מכנית דרך השלבים השונים של איך ליישם את המסירה אחורה. כדי שתוכלו בעצמכם לגרום לו לעבוד. ומה שאני רוצה לעשות בסרטון הזה הוא לבחון קצת יותר לעומק את הצעדים המכניים של המסירה אחורה, ולנסות לתת לכם קצת יותר אינטואיציה על הצעדים המכניים שעושה המסירה לאחור בתקווה לשכנע אתכם שזה לפחות אלגוריתם סביר. אם גם אחרי הוידאו הזה עדיין נראית המסירה אחורה כקופסה שחורה מאוד עם צעדים מורכבים מדי וקצת כמו קסם או כישוף, זה בסדר גמור. ולמרות שהשתמשתי במסירה אחורה במשך שנים רבות, לפעמים גם לי זה אלגוריתם קשה להבנה, אבל אני מקווה שהוידאו הזה יעזור קצת. כדי להבין טוב יותר את המסירה לאחור, הבה נבחן מקרוב את המסירה קדימה. הנה רשת עצבית עם שתי יחידות קלט מלבד יחידת ההטיה, ויש כאן שתי יחידות נסתרות בשכבה הזו, ושתי יחידות נסתרות בשכבה הבאה. ואז, בסוף, יחידת פלט אחת. שוב, אנחנו סופרים שתיים, שתיים, שתיים, ולא סופרים את יחידות ההטיה האלה למעלה. כדי להדגים מסירה קדימה, אני אצייר את הרשת טיפה אחרת. בפרט אני אצייר את הרשת העצבית הזאת עם הצמתים מוארכים, כמו האליפסה הזו, כדי שאוכל לכתוב בהם טקסט. בעת ביצוע מסירה קדימה, ייתכן שיש לנו דוגמה מסוימת. נניח איזו דוגמה xi,yi. ואנחנו מאכילים את שכבת הקלט שלנו בערך xi. לדוגמא מאכילים את שתי יחידות הקלט ב-x(i)1 וב-x(i)2. וכאשר אנו מוסרים קדימה אל השכבה הראשונה הנסתרת כאן, מה שאנחנו עושים הוא לחשב z(2)1 ו -z(2)2. אז אלה הם הסכומים המשוקללים של הקלט מיחידות הקלט. ואז אנו מפעילים את פונקצית הסיגמואיד, הפונקציה הלוגיסטית, מפעילים את פונקציית ההפעלה על הערך z. אלה ערכי ההפעלה. מה שנותן לנו את a(2)1 ואת a(2)2. ואז אנחנו מוסרים קדימה שוב כדי להגיע ל- z(3)1. מפעילים את הסיגמואיד או הפונקציה הלוגיסטית, פונקציית ההפעלה כדי לקבל את a(3)1. וכן הלאה, עד שאנחנו מקבלים את z(4)1, מפעילים את פונקצית ההפעלה, מה שנותן לנו את a(4)1, שהוא ערך הפלט הסופי של הרשת העצבית. בואו נמחק את החץ הזה כדי לתת לי עוד מקום. ואם נסתכל על מה באמת עושה החישוב הזה, תוך התמקדות, נניח, ביחידה הנסתרת הזו. אנחנו צריכים לחבר את המשקל הזה המוצג בסגול כאן, המשקל הוא Θ(2)1,0, האינדקס לא חשוב. והמשקל הזה כאן, שאני מדגיש באדום, זה Θ(2)1,1 והמשקל הזה כאן, שאני מצייר בטורקיז, הוא Θ(2)1,2. אז הדרך בה אנו מחשבים את הערך הזה, z(3)1 היא, z(3)1 שווה למשקל הסגול כפול הערך הזה. שהוא Θ(2)1,0 כפול 1. ועוד המשקל האדום כפול הערך הזה, שהוא Θ(2)1,1 כפול a(2)1. ועוד המשקל בטורקיז כפול הערך הזה, דהיינו פלוס Θ(2)1,2 כפול 2(2)a. וזו היא המסירה קדימה. כפי שנראה בהמשך הסרטון הזה, המסירה אחורה עושה תהליך מאוד דומה לזה. אלא שבמקום החישובים שזורמים במסירה קדימה משמאל לימין, החישובים במסירה לאחור זורמים מימין לשמאל של הרשת. תוך שימוש בחישוב דומה מאוד לזה. ואני אסביר תוך שתי שקופיות בדיוק למה אני מתכוון. כדי להבין טוב יותר מה עושה המסירה אחורה, בואו נסתכל על פונקציית העלות. זו אותה פונקציית העלות שהיתה לנו כאשר היתה לנו רק יחידת פלט אחת. אם יש לנו יותר מאשר יחידת פלט אחת, אנחנו צריכים לעשות סכימה על יחידות הפלט עם אינדקס k שם. אבל אם יש רק יחידת פלט אחת אז זו פשוט פונקציה עלות. ואנחנו עושים מסירה לפנים ומסירה לאחור על דוגמת לימוד אחת בכל פעם. אז בואו פשוט נתמקד בדוגמה אחת, (x(i),y(i ונתמקד במקרה של יחידת פלט אחת. אז (y(i כאן הוא פשוט מספר ממשי. ובואו נתעלם כרגע מההסדרה, אז λ שווה 0. וגם המונח האחרון הזה, מונח ההסדרה, פשוט נעלם. עכשיו אם נסתכל בתוך הסיכום, נמצא שהביטוי של העלות המשויך לדוגמת האימון הספציפית, דהיינו העלות המשויכת לדוגמת האימון (x(i),y(i ניתנת על ידי הביטוי הזה. אז העלות המשויכת לדוגמא i נכתבת כך. ופונקציית העלות משחקת תפקיד דומה לריבוע השגיאה. אז במקום להסתכל על הביטוי המסובך הזה, אם תרצו אתם יכולים לחשוב על העלות של דוגמא i כבערך ריבוע ההפרש בין מה שפלטה רשת עצבית לבין הערך של הדוגמה בפועל. בדיוק כמו ברגרסיה לוגיסטית, אנחנו למעשה מעדיפים להשתמש בפונקצית עלות קצת יותר מסובכת שמשתמשת בלוג. אבל למטרות אינטואיציה, אל תהססו לחשוב על פונקציית העלות כמין פונקציית עלות השגיאה בריבוע. אז העלות של דוגמא i מודדת עד כמה טובה הרשת בניבוי התוצאה של דוגמה i. כמה קרוב הפלט לתווית שנצפתה בפועל (y(i עכשיו בואו נראה מה עושה המסירה אחורה. אינטואיציה שימושית אחת היא שהמסירה אחורה מחשבת את ערכי ה"שגיאות" דלתא l)j). ואנחנו יכולים לחשוב על אלה כמו ה"שגיאה" בערך ההפעלה שקיבלנו עבור היחידה j בשכבה ה-l. או בצורה יותר רשמית, אולי רק עבור אלה מכם שיודעים חשבון דיפרנציאלי. מבחינה פורמלית יותר, מונחי ה-δ האלה הם הנגזרות החלקיות ביחס ל- z(l)j, כלומר זהו הסכום המשוקלל של הקלטים שתרמו לחישוב ערכי z. הנגזרות החלקיות ביחס לדברים אלה בפונקציית העלות. אז בצורה מדויקת, פונקציית העלות היא פונקציה של התווית y והערך h של x שהוא הפלט של הרשת העצבית. ואם היינו יכולים להיכנס לתוך הרשת העצבית ופשוט לשנות את הערכים האלה z(l)j קצת, אז זה ישפיע על הערכים אותם פולטת הרשת העצבית, וזה בסופו של דבר יגרום לשינוי פונקציית העלות. ושוב באמת, זה רק לאלה מכם שמרגישים טוב עם נגזרות. לאלה שמרגישים בנוח עם נגזרות חלקיות, אז ביטויי δ האלה הם נגזרות חלקיות של פונקציית העלות, בנקודות של צמתי הביניים האלה שאנחנו מחשבים. ולכן הם מדד טוב לכמה היינו רוצים לשנות את המשקלות של הרשת העצבית כדי להשפיע על ערכי הביניים האלה של החישוב. שזה יגרום להשפיע על הפלט הסופי (h(x של הרשת העצבית, וכך להשפיע על העלות הכוללת. במקרה שהקטע האחרון של האינטואיציה על נגזרות חלקיות, אם זה לא נראה לכם הגיוני. אבל אל תטריחו את עצמכם אם קשה לכם, אנחנו בהחלט יכולים לעשות את זה בלי לדבר על נגזרות חלקיות. אבל בואו נראה בפירוט רב יותר מה עושה בדיוק המסירה אחורה. עבור שכבת הפלט, אנחנו מחשבים בהתחלה את ה-δ הזה, δ(4)1, כ-(y(i כשאנחנו עושים מסירה קדימה ומסירה אחורה על דוגמת האימון הזו, i. זאת אומרת y(i)-a(4)1. אז זו בעצם השגיאה, נכון? זה ההבדל בין הערך בפועל של (y(i פחות הערך החזוי על ידי הפונקציה, ולכן אנחנו מחשבים את δ(4)1 כך. בשלב הבא אנחנו מפיצים או מוסרים את הערכים האלה לאחור. אני אסביר את זה בשנייה, וכך מחשבים את הביטויים של δ עבור השכבה הקודמת. יצא לנו δ(3)1. δ(3)2. ואז אנחנו מוסרים את זה שוב אחורה, ומחשבים את δ(2)1 ו-δ(2)2. החישוב של המסירה אחורה דומה מאוד להפעלת אלגוריתם המסירה קדימה, אבל הוא עושה את זה לאחור. הנה מה שאני מתכוון. בואו נראה איך הגענו לערך הזה של δ(2)2. אז יש לנו δ(2)2. ובדומה למסירה קדימה, הרשו לי לשים תוויות על כמה מהמשקלות. אז המשקל הזה, שאני אצייר בטורקיז, נניח כי המשקל הזה הוא Θ(2)1,2, וזה כאן למטה שאנו מדגישים באדום, זה יהיה נניח Θ(2)2,2. אז אם אנחנו מסתכלים איך מחושב δ(2)2, איך מחושב הצומת הזה, אז מה שאנחנו צריכים לעשות, הוא לקחת את הערך הזה להכפיל אותו במשקל הזה, ולחבר אליו את הערך הזה מוכפל במשקל הזה. אז זה בעצם סכום של ערכי δ האלה משוקללים על ידי המשקל של הקשתות המתאימות. אז באופן מעשי, הרשו לי למלא את זה, δ(2)2 יהיה שווה ל-Θ(2)1,2 הסגולה כפול δ(3)1. ועוד הדבר הזה באדום, זה Θ(2)2,2 כפול δ(3)2. אז זה ממש המשקל האדום כפול הערך הזה, ועוד המשקל הסגול כפול הערך הזה. וכך הגענו לערך הזה של δ. ורק בתור דוגמה נוספת, בואו נסתכל על הערך הזה. איך הגענו לערך הזה? ובכן באופן דומה. אם המשקל הזה, שאני מדגיש בירוק, אם המשקל הזה שווה, למשל, Θ(3)1,2. אז δ(3)2 יהיה שווה למשקל הירוק הזה, Θ(3)1,2 כפול δ(4)1. ודרך אגב, עד כה כתבתי את ערכי δ רק עבור היחידות הנסתרות, אבל לא ליחידות ההטיה. תלוי איך אתה מגדיר את אלגוריתם המסירה אחורה, או בהתאם לאופן שבו אתה מיישם את זה, אתה עשוי ליישם משהו שמחשב ערכי δ גם עבור יחידות ההטיה. יחידות ההטיה תמיד מוציאות את הערך 1, הן פשוט מה שהן, ואין לנו אפשרות לשנות את הערך. וכך, בהתאם ליישום שלך למסירה אחורה, הדרך שבה אני בדרך כלל מיישם את זה. אני בסופו של דבר כן מחשב את ערכי δ האלה, אבל אנחנו פשוט זורקים אותם, אנחנו לא משתמשים בהם. כי הם לא באמת חלק מהחישוב הדרוש כדי לחשב נגזרת. אז אני מקווה שזה נותן לך קצת יותר אינטואיציה על מה עושה המסירה אחורה. במקרה שכל זה עדיין נראה מין כישוף כזה, סוג של קופסה שחורה, אז בסרטון מאוחר יותר,
וידאו של "מחברים את הכל לתמונה אחת", אני אנסה לתת עוד קצת אינטואיציה על מה עושה המסירה אחורה. אבל לצערי זה אלגוריתם שקשה לנסות לראות בעיני רוחנו ולהבין מה הוא באמת עושה. אבל למרבה המזל אני, ואני מניח עוד אנשים רבים משתמשים בו בהצלחה רבה במשך שנים רבות. ואם תיישם את האלגוריתם אתה יכול לקבל אלגוריתם למידה יעיל מאוד. למרות שפעולתו הפנימית או איך הוא עובד יכול להיות קשה יותר לדמיין.