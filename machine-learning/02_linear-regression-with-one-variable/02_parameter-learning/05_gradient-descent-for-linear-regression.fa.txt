در ویدئوهای قبل، در مورد
الگوریتم گرادیان نزولی و همچنین در مورد مدلسازی رگرسیون خطی
و تابع هزینه خطای مجذور صحبت کردیم. در این ویدئو گرادیان نزولی را
با تابع هزینه ترکیب می‌کنیم و الگوریتم رگرسیون خطی را
به دست می‌آوریم تا بتوانیم خطی راست برای
داده‌هایمان به دست آوریم. در ویدئوهای قبلی به این
معادلات رسیدیم. این الگوریتم گرادیان نزولی است
که برایتان آشنا است و این مدل رگرسیون خطی است
به همراه تابع خطی فرضیه و تابع هزینه خطای مجذور. در اینجا می‌خواهیم از گرادیان نزولی برای مینیمم کردن
تابع هزینه خطای مجذور استفاده کنیم. برای استفاده از گرادیان نزولی، یعنی این کادر،
عبارت کلیدی این مشتق جزئی است. باید بدانید این عبارت مشتق جزئی چیست و آن را وارد تعریف تابع هزینه J کنید. که اینجا می‌نویسم. مجموع iها از یک تا m. برای این تابع هزینه‌ی خطای مجذور. کاری که در اینجا کردم تعریف تابع هزینه را نوشتم. و اگر کمی ساده‌تر کنیم
خواهیم داشت: سیگما i از یک تا m،
تتا صفر به اضافه تتا یک در xi
منهای yi، کلا به توان دو. و در اینجا فقط تعریف فرض را گرفتم و اینجا نوشتم. باید بدانیم که این مشتق جزئی برای دو حالت j = 0
و j = 1 چیست. یعنی باید ببینیم این مشتق جزئی برای تتا صفر و تتا یک چیست. و در اینجا جواب‌ها را می‌نویسم. این عبارت اول ساده می‌شود به
یک روی m مجموع عبارت تابع h برحسب xi منهای yi
و برای این عبارت مشتق جزئی می‌نویسیم تتا یک که به این عبارت می‌رسم. مجموع تابع h بر حسب xi
منهای yi ضرب در xi خب. و برای محاسبه مشتق‌های جزئی
از این معادله به یکی از معادله‌های زیرین می‌رسیم. محاسبه این مشتق‌های جزئی
نیازمند حسابان چندمتغیره است. اگر حسابان می‌دانید
می‌توانید خودتان مشتق‌گیری را چک کنید و به همین جوابی می‌رسید
که من اینجا می‌گویم. اما اگر با حسابان آشنایی کمتری دارید
نگران نباشید می‌توانید از همین معادله‌ها استفاده کنید و لازم نیست حسابان بدانید
تا تکالیف را حل کنید. پس در اینجا گرادیان نزولی را اضافه می‌کنیم. پس مسلح به این معادله‌ها و مسلح به مشتق‌ها
که در واقع شیب تابع هزینه J هستند، می‌توانیم به الگوریتم گرادیان نزولی برگردیم. این گرادیان نزولی است برای رگرسیون خطی که تا همگرایی تکرار می‌شود.
تتا صفر و تتا یک به این صورت تغییر می‌کنند که
مقدار قبلی منهای آلفا ضرب‌در عبارت مشتق می‌شود. یعنی این عبارت. این الگوریتم رگرسیون خطی است. اولین عبارت در اینجا، عبارت مشتق جزئی برحسب تتا صفر است، که در اسلاید قبلی به دست آوردیم. و دومین عبارت اینجا،
مشتق جزئی برحسب تتا یک است
که در اسلاید قبلی به دست آوردیم. و به عنوان یادآور سریع،
باید در زمان استفاده از گرادیان نزولی به این نکته توجه کنید که تتا صفر و تتا یک را همزمان تغییر دهید. خب. ببینیم گرادیان نزولی چطور کار می‌کند. یکی از اشکالات گرادیان نزولی این است که ممکن است به بهینه موضعی برسد. اولین بار که گرادیان نزولی را معرفی کردم
این تصویر را نشان دادم که به پایین می‌رفتیم و دیدیم که
بسته به اینکه از کجا شروع کنیم ممکن است به بهینه‌ی موضعی برسیم. مثلا به اینجا یا اینجا برسید. اما تابع هزینه برای رگرسیون خطی همیشه تابعی سهمی مانند
شبیه این است. اصطلاح تخصصی برای این شکل
تابع محدب است. و نمی‌خواهم تعریف دقیق تابع محدب را بگویم، که حروفش
C, O, N, V, E, X
است، همین‌قدر بدانید که تابع محدب تابعی کاسه‌مانند است و این تابع بهینه‌های موضعی ندارد
و فقط یک نقطه‌ی بهینه دارد. و گرادیان نزولی برای این نوع تابع هزینه
که در آن از رگرسیون خطی استفاده می‌کنید
همیشه به نقطه بهینه همگرا می‌شود. چون نقطه‌ی بهینه‌ی دیگری به جز
بهینه‌ی اصلی وجود ندارد. بیایید این الگوریتم را در عمل ببینیم. مثل همیشه، این تابع فرضیه
و این تابع هزینه‌ی J است. و بیایید پارامترهایم را با این مقدار شروع کنم. معمولا پارامترها را با صفر و صفر آغاز می‌کنند. یعنی تتا صفر و تتا یک هر دو برابر 0 اما برای این شکل و این تابع تتا صفر را برابر 900
و تتا یک را برابر منفی 0.1 قرار می‌دهم. پس خواهیم داشت
h(x)=900-0.1x این خط است که از تابع هزینه به دست آمده است.
حال اگر یک قدم در گرادیان نزولی برداریم، از این نقطه در اینجا به آن نقطه دوم می‌رسیم. و متوجه شدید که خط من کمی تغییر کرد. و اگر یک قدم دیگر در گرادیان نزولی بردارم
خط من در سمت چپ تغییر می‌کند. درست؟ و روی تابع هزینه به نقطه جدیدی می‌روم. و اگر باز هم قدمی در گرادیان نزولی بردارم
هزینه کمتر می‌شود. پس پارامترها و همه چیز دیگر
این مسیر را دنبال می‌کنند. و اگر به سمت چپ نگاه کنید
این متناظر با تابع فرضیه‌ای است که بهتر و بهتر با داده‌ها متناسب می‌شود. تا زمانی که نهایتا به مینیمم اصلی می‌رسد
و این نقطه مینیمم متناظر با این تابع فرضیه است
که تناسب خوبی با داده‌ها دارد. و این گرادیان نزولی است
که در عمل دیدیم و تناسب خوبی با
مجموعه داده‌های قیمت‌های خانه دارد. و اکنون می‌توانید از آن استفاده کنید تا اگر دوستتان خانه‌ای
با مساحت ۱۲۵۰ فوت مربع داشت بگویید که مثلا احتمال دارد بتواند ۲۵۰۰۰۰ دلار بفروشد. نکته آخر اینکه اسم این الگوریتم که اکنون دیدیم
گرادیان نزولی انبوه نام دارد. و در یادگیری ماشین ثابت شده که ما همیشه از اسامی خوبی برای
نامیدن الگوریتم‌هایمان استفاده نمی‌کنیم اما واژه گرادیان نزولی انبوه به این معنی است که در هر قدم گرادیان نزولی
از تمامی نمونه‌های آموزشی استفاده می‌کنیم. پس در گرادیان نزولی زمانی که
مشتق‌ها را محاسبه می‌کنیم این مجموع را محاسبه می‌کنیم. پس در هر قدم گرادیان نزولی
چیزی شبیه این را محاسبه می‌کنیم که در آن مجموع خطای m نمونه آموزشی محاسبه می‌شود
بنابراین اصطلاح گرادیان نزولی انبوه به این اشاره دارد که انبوه نمونه‌های آموزشی را
در نظر می‌گیریم. و مجددا می‌گویم که نام زیبایی نیست اما کاربران یادگیری ماشین از آن استفاده می‌کنند. و گاهی مواقع دیگر نسخه‌های گرادیان نزولی نسخه‌های انبوه نیستند و تمامی مجموعه آموزشی را در نظر نمی‌گیرند بلکه هربار مجموعه‌ی کوچکی از
نمونه‌های آموزشی را در نظر می‌گیرند. و در مورد آن نسخه‌ها هم بعدتر صحبت می‌کنیم. اما اکنون از الگوریتمی که دیدیم یعنی
گرادیان نزولی انبوه استفاده می‌کنیم. اکنون می‌دانید که چگونه از گرادیان نزولی
برای رگرسیون خطی استفاده کنید. این هم از رگرسیون خطی با گرادیان نزولی. اگر از قبل با جبر خطی پیشرفته آشنایی دارید، مثلا ممکن است بعضی از شما کلاسش را گذرانده باشید، احتمالا می‌دانید راه‌حلی وجود دارد که به صورت عددی مینیمم تابع هزینه J را مشخص می‌کند
بدون آنکه نیازی به استفاده الگوریتمی تکرارشونده
مانند گرادیان نزولی باشد. بعدتر در این دوره،
در مورد آن روش هم صحبت می‌کنیم که مینیمم تابع هزینه J را محاسبه می‌کند،
بدون آنکه نیازی به انجام مراحل متعدد گرادیان نزولی باشد. نام آن روش شیوه معادلات نرمال است. اما اگر با آن روش آشنایید باید بدانید گرادیان نزولی برای داده‌های بزرگ‌تر
بهتر از متد معادلات نرمال عمل می‌کند. و اکنون که در مورد گرادیان نزولی می‌دانیم،
می‌خواهیم از آن برای موضوعات مختلف و مسائل مختلف یادگیری ماشین استفاده کنیم. تبریک به شما برای یادگیری
نخستین الگوریتم یادگیری ماشین. بعدتر تمرین‌هایی داریم که ازتان می‌خواهیم
از گرادیان نزولی استفاده کنید، و می‌توانید کارکرد الگوریتم را خودتان ببینید. اما پیش از آن می‌خواهم
به شما بگویم که در چند ویدئوی آینده با هم در مورد تعمیم الگوریتم گرادیان نزولی صحبت می‌کنیم
تا آن را قدرتمندتر کنیم. پس در ویدئوی بعد
بیشتر در این خصوص صحبت می‌کنیم.