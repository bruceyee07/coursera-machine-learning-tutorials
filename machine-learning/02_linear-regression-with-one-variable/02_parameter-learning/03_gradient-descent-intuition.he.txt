בסרטון הקודם, נתנו הגדרה מתמטית לירידה במדרון. בואו נצלול בסרט הזה עמוק יותר, בכדי לקבל אינטואיציה טובה יותר על מה עושה האלגוריתם ומדוע הצעדים שמבצע האלגוריתם הם הגיוניים. הנה אלגוריתם הירידה במדרון, כפי שראינו בפעם שעברה, ורק להזכיר, הפרמטר הזה או הגורם α נקרא קצב הלימוד. והוא שולט בגודל הצעד שאנו עושים בכל עדכון של הפרמטר θj. והגורם השני כאן הוא הנגזרת ומה שאני רוצה לעשות בהרצאה זו הוא לתת לכם אינטואיציה לגבי מה עושה כל אחד משני הגורמים האלה, ומדוע כאשר מרכיבים אותם, כל העדכון הגיוני. כדי להעביר את האינטואיציות האלה, מה שאני רוצה לעשות הוא להשתמש בדוגמא קצת יותר פשוטה, בה אנחנו רוצים למזער פונקציה של פרמטר אחד בלבד. נניח שיש לנו פונקצית מחיר, J של פרמטר אחד בלבד, θ₁, כמו שעשינו לפני כמה קטעי הרצאות, כש-θ₁ הוא מספר ממשי. אז יש לנו גרף דו-ממדי, שקצת יותר פשוט לשרטט אותו. בואו ננסה להבין מה תעשה הירידה במדרון לפונקציה הזו. אז בואו נאמר שזו הפונקציה שלי, (J(θ₁. זו הפונקציה שלי. ו-θ₁ הוא מספר ממשי. בסדר? עתה, בואו נשים בשקופית את הירידה במדרון עם θ₁ במיקום הזה. דמיינו שאנחנו מתחילים בנקודה הזו על הפונקציה שלי. מה שהירידה במדרון תעשה הוא לעדכן את θ₁, שייתעדכן ל-θ₁ פחות α כפול נגזרת לפי θ₁ של (J(θ₁, נכון? ואגב, הגורם הזה, הנגזרת, אם אתם תוהים מדוע שיניתי את הסמל הקודם של נגזרת חלקית לזה. אם אינכם יודעים מה ההבדל בין הסימון של נגזרת ונגזרת חלקית, תתעלמו מזה. מבחינה טכנית במתמטיקה קוראים לאחד נגזרת חלקית ולשני נגזרת, בהתאם למספר הפרמטרים בפונקציה J. אבל זה עניין טכני מתמטי. לצורך הרצאה זו חישבו על הסמלים של נגזרת חלקית ושל נגזרת כאילו הם אותו דבר בדיוק. ואל תדאגו לגבי ההבדל ביניהם. אני אנסה להשתמש בסימון מתמטי מדויק, אבל לענייננו שני הסימונים האלה הם בעצם אותו הדבר. אז בואו נראה מה תעשה המשוואה הזאת. עכשיו נחשב את הנגזרת הזו, אינני יודע כמה אתם מכירים נגזרות, אבל בעצם הנגזרת היא פשוט הישר המשיק לאותה נקודה, קו ישר כזה, הקו האדום, שבדיוק נוגע בפונקציה הזו, בואו נראה מה השיפוע של הישר האדום הזה. זהו מה שהנגזרת אומרת, מה השיפוע של הישר הזה שבדיוק משיק לפונקציה. אוקיי, השיפוע של ישר הוא הגובה הזה חלקי האורך האופקי הזה. עכשיו, לישר הזה יש שיפוע חיובי, ולכן הנגזרת היא חיובית. אז העדכון של θ יהיה θ₁ מקבל את θ₁ פחות α כפול איזשהו מספר חיובי. בסדר. α, המשתנה שמגדיר את קצב הלמידה, הוא תמיד מספר חיובי. אז מה שיוצא זה שאנחנו נעדכן את θ₁ לערך θ₁ מינוס משהו חיובי. אז בסופו של דבר θ₁ נע שמאלה. θ₁ פוחת והולך, ואנחנו יכולים לראות שזה הדבר הנכון לעשות כי אנחנו רוצים לנוע בכיוון הזה. אתם רואים, להתקרב למינימום שנמצא שם. אז עד עכשיו, לפי הירידה במדרון אנחנו עושים את הדבר הנכון. בואו נראה עוד דוגמא. אז בואו ניקח את אותה פונקציה J, בואו ננסה להשתמש באותה פונקציה, (J(θ₁. ועכשיו, נניח שאתחלנו את הפרמטר שם משמאל. אז θ₁ הוא כאן. אני מתבונן בנקודה הזו על המשטח. עכשיו הנגזרת לפי θ₁ של (J(θ₁ כשמחשבים אותה בנקודה הזו, אנחנו מסתכלים על השיפוע של הישר הזה, אז הנגזרת היא השיפוע של הקו הזה. אבל הישר הזה הוא יורד, ולכן לישר הזה יש שיפוע שלילי. בסדר. או במילים אחרות זה אומר שלפונקציה הזו יש נגזרת שלילית, זו המשמעות של שיפוע שלילי בנקודה זו. אז החלק הזה קטן או שווה ל-0, וכאשר נעדכן את θ₁, יהיה לנו... θ₁ פחות α כפול מספר שלילי. את התוצאה היא θ₁ פחות מספר שלילי כלומר בעצם θ₁ תגדל, כי חיסור מספר שלילי פירושו תוספת של משהו חיובי ל-θ₁. ופירוש הדבר הוא שאנחנו מגדילים את θ₁ עד שהוא זז הנה, וזה אכן שוב נראה כמו הדבר הנכון לעשות כדי לנסות ולהתקרב למינימום. אז זו כל התיאוריה הזאת או האינטואיציה מאחורי מה שעושה גורם הנגזרת. עכשיו בואו נעיף מבט על α, קצב הלימוד, ונראה מה הוא עושה. אז הנה הירידה במדרון עם המשוואה הזו. בואו נסתכל מה קורה אם α הוא או קטן מדי או גדול מדי. אז קודם כל מה קורה אם α קטן מדי? אז הנה הפונקציה J שלי, (J(θ₁. בואו נתחיל כאן. אם α קטן מדי, אז מה שנעשה הוא להכפיל את הנגזרת במספר קטן, מה שיגרום ש-θ₁ ישתנה בכמות מאד קטנה. אוקיי, אז זה היה הצעד הראשון. מהנקודה החדשה הזה, נצטרך לעשות צעד נוסף. אבל אם α קטן מדי, גם הצעד הבא יהיה קטן. אז אם קצב הלימוד הוא קטן מדי אנחנו פוסעים בצעדי תינוק קטנטנים בדרך למינימום. ונצטרך הרבה שלבים כדי להגיע למינימום ולכן אם α הוא קטן מדי אז הירידה במדרון תהיה מאד איטית כי אנחנו צועדים צעדי תינוק קטנטנים ולכן נצטרך הרבה שלבים לפני שנגיע לאיזה שהוא מקום קרוב למינימום הגלובלי. מצד שני, מה קורה אם α הוא גדול מדי? אז הנה הפונקציה (J(θ₁, ואם α מדי גדול, אז הירידה במדרון עלולה להחטיא את המינימום ועלולה אפילו לא להצליח להתכנס, ואולי היא אפילו תתבדר, הנה מה שאני מתכוון. נניח שהנקודה הנוכחית שלנו היא שם, קרובה למדי למינימום. אז הנגזרת פונה ימינה, אבל אם α הוא גדול מדי, אנחנו נעשה צעד ענק. נעשה צעד ענק כזה. אז אנחנו נעשה צעד ענק, ועכשיו הפונקציה שלנו מתחילה להתבדר. כי היא התחילה עם הערך הזה, ועכשיו, הערכים שלה מתחילים לעלות. עכשיו הנגזרת מצביעה שמאלה, מה שאומר שאני צריך להקטין את θ₁. אבל אם α, קבוע הלמידה, גדול מדי, אנחנו עלולים לעשות צעד ענק שיוצא מכאן ועולה שמאלה הרבה. אז הגענו לכאן, נכון? ואם α קצת גדול מדי, אנחנו יכולים לעשות עוד צעד ענק ולעלות עוד יותר לגובה ולהחטיא את המטרה שוב ושוב, עד שאתם כבר שמים לב שאני בעצם מתרחק יותר ויותר מהמינימום. אז אם α גדול מדי, התהליך עלול לא להתכנס או אפילו להתבדר. עכשיו, יש לי עוד שאלה בשבילכם. זה קצת מסובך וכשלמדתי את זה לראשונה באמת זה לקח לי הרבה זמן להבין את זה. מה אם הפרמטר θ₁ כבר נמצא במינימום מקומי, מה אתה חושב שיעשה הצעד הבא של הירידה במדרון? בואו נניח שאתחלנו את θ₁ למינימום מקומי. נניח שהערך הראשוני של θ₁ הוא כבר באופטימום המקומי או במינימום המקומי. אז מה שקורה הוא שבאופטימום המקומי, הנגזרת תהיה שווה לאפס. אז בשביל המדרון הזה, זו נקודת המשיק, ולכן השיפוע של הקו הזה יהיה שווה לאפס ולכן הנגזרת הזו שווה לאפס. אז שלב העדכון בירידה במדרון תשאיר את θ₁ כמו שהוא כי אנחנו מפחיתים ממנו α כפול אפס. אז מה שזה אומר הוא שאם אתה כבר נמצא במינימום המקומי θ₁ יישאר ללא שינוי בשלב העדכון, θ₁ שווה θ₁. אז אם הפרמטרים שלך כבר במינימום מקומי אז הירידה במדרון לא תעשה דבר לפרמטרים וזה מה שאנחנו רוצים, כי הוא משאיר את הפתרון במינימום המקומי. וזה גם מסביר מדוע הירידה במדרון מתכנסת למינימום המקומי למרות ש-α, קצב הלימוד, קבוע. הנה מה שאני מתכוון, בואו נסתכל בדוגמא. אז הנה פונקצית העלות (J(θ שאותה אני רוצה למזער ובואו נניח שאתחלתי את האלגוריתם שלי, אלגוריתם הירידה במדרון, שם בנקודה בצבע מגנטה. אם אני עושה צעד אחד של ירידה במדרון, אולי זה יביא אותי לנקודה הזו, כי הנגזרת באזור הזה כאן היא די תלולה. נכון? עכשיו, אני בנקודה הירוקה הזו, ואם אני עושה עוד צעד בירידה במדרון, אתם יכולים לראות שהנגזרת, או המדרון, הוא פחות תלול בנקודה הירוקה לעומת הנקודה החיצונית בסגול. ככל שאני מתקרב למינימום, הנגזרת מתקרבת יותר ויותר לאפס, כשאני מתקרב למינימום. אז אחרי צעד אחד במדרון, הנגזרת החדשה שלי קצת יותר קטנה. וכשאעשה את הצעד הבא, הצעד שאעשה יהיה קצת יותר קטן מהקודם. עכשיו יש לנו עוד נקודה, נקודה אדומה, קרוב אפילו יותר למינימום הגלובלי והנגזרת כאן תהיה עוד יותר קטנה מאשר בנקודה הירוקה. אז בצעד הבא בירידה במדרון הנגזרת עוד יותר קטנה ולכן גודל העדכון של θ₁ יהיה עוד יותר קטן, ולכן הצעד יהיה יותר קטן. ותוך כדי התהליך של הירידה במדרון, הצעדים באופן אוטומטי יהפכו לצעדים קטנים יותר. עד שבסופו של דבר הצעדים יהיו קטנים מאוד ובסוף התהליך יתכנס למינימום המקומי. אז לסיכום, בירידה במדרון, ככל שאנו מתקרבים למינימום מקומי, הירידה תתקדם בצעדים קטנים יותר באופן אוטומטי. וזה משום שככל שאנו מתקרבים למינימום המקומי, ההגדרה של המינימום המקומי היא שהנגזרת בו משתווה לאפס. ככל שאנו מתקרבים למינימום מקומי, הביטוי הזה של הנגזרת ילך ויקטן באופן אוטומטי, ולכן הירידה תיעשה בצעדים קטנים יותר. כך שאין צורך להקטין את α כל הזמן. אז זהו אלגוריתם הירידה במדרון, ואפשר להשתמש בו כדי לנסות ולמזער כל פונקציית עלות J, לאו דווקא פונקצית העלות J שהגדרנו עבור רגרסיה ליניארית. בסרטון הבא, אנחנו הולכים לחזור לפונקצית העלות J שבה השתמשנו כפונקצית העלות של רגרסיה ליניארית, פונקצית העלות של ריבועי השגיאות בה השתמשנו קודם. ניקח את הירידה במדרון ואת פונקצית העלות הריבועית הזו ונשלב אותן. וזה ייתן לנו את אלגוריתם הלמידה הראשון שלנו, אלגוריתם הרגרסיה הליניארית.