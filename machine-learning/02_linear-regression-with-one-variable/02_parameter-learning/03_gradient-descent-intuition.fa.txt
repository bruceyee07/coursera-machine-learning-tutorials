در ویدئوی قبل، تعریف ریاضیاتی گرادیان نزولی
را ارائه کردیم. اجازه دهید در این ویدئو
عمیق‌تر به مسأله بپردازیم که این الگوریتم چه کاری انجام می‌دهد و چرا
مراحل گرادیان نزولی درست هستند. این الگوریتن گرادیان نزولی است
که در ویدئوی قبل دیدیم و برای یادآوری، این پارامتر آلفا
نرخ یادگیری نام دارد. و بزرگی قدمی که برمی‌داریم
تا پارامتر تتا j را تغییر دهیم کنترل می‌کند. و این اصطلاح دوم در اینجا
عبارت مشتق است. و می‌خواهم در این ویدئو بگویم
که این دو عبارت چه کاری انجام می‌دهند و چرا
وقتی با هم باشند معنا می‌دهند. به منظور درک این مطالب،
می‌خواهم از مثالی ساده‌تر استفاده کنم، که در آن می‌خواهیم تابعی با
یک پارامتر را مینیمم کنیم. پس فرض کنیم تابع هزینه J فقط
یک پارامتر دارد، تتا یک، مثل کاری که چند ویدئو قبل‌تر کردیم
و تتا یک هم عددی حقیقی است. پس می‌توانیم روی کاغذ
نشان دهیم و این خیلی راحت‌تر است. ببینیم گرادیان نزولی بر روی این تابع
چه کاری انجام می‌دهد. فرض کنیم این تابع J برای
تتا یک است. به این ترتیب. و تتا یک هم عددی حقیقی است. بسیار خب ؟ فرض کنیم در گرادیان نزولی
تتا یک را در اینجا قرار داده‌ایم. یعنی تصور کنید با این نقطه روی تابع
شروع کرده‌ایم. کاری که گرادیان نزولی انجام می‌دهد تغییر تتا است. پس تتا یک به این شکل
تغییر می‌کند: تتا یک منهای آلفا ضرب در مشتق تابع J بر حسب تتا یک. و در حاشیه بگویم،
برای عبارت مشتق، شاید تعجب کردید که چرا
از این علامت مشتق جزئی استفاده نکردم. اگر نمی‌دانید که تفاوت میان علامت
مشتق جزئی و استفاده از d بر d تتا چیست
نگران نباشید. از نظر فنی در ریاضیات
به این مشتق جزئی می‌گوییم و به این مشتق،
و بستگی به تعداد پارامترهای تابع J دارد. اما این موارد فنی ریاضیات است. برای این ویدئو، نمادهای مشتق جزئی و مشتق را
درست یکسان در نظر بگیرید. و نگران نباشید که تفاوت اصلی چیست. من از علامت‌های ریاضی درست استفاده می‌کنم اما برای این ویدئو
این دو واقعا یکسانند. ببینیم این معادله چه می‌کند. می‌خواهیم این مشتق را حساب کنیم.
نمی‌دانم مشتق را قبلا در حسابان دیده‌اید یا نه،
اما کاری که مشتق در این نقطه می‌کند این است که تانژانت این نقطه را می‌گیرد،
مثل این خط قرمز، که با تابع مماس است،
و به شیب این خط نگاه کنیم. این مشتق است. یعنی شیب خطی که بر این نقطه از تابع
مماس است چقدر است. و شیب خط هم که از تقسیم ارتفاع
بر خط افقی به دست می‌آید. شیب این خط مثبت است، پس مشتقش مثبت است. پس تغییر تتا به این صورت است که تتا یک تغییر می‌کند به
تتا یک منهای آلفا ضرب در یک عدد مثبت. Okay. آلفا که نرخ یادگیری است
همیشه عدد مثبت است، بنابراین داریم
تتا یک منهای چیزی. پس در نهایت تتا یک به سمت راست می‌رود. تتا یک کاهش می‌یابد
و می‌بینیم که درست است، چون باید در این مسیر حرکت کنیم. تا به مینیممی که اینجاست
نزدیک‌تر شویم. پس تا اینجا با گرادیان نزولی
کار درست را انجام می‌دهیم. یک مثال دیگر. مجددا از تابع J استفاده می‌کنیم.
پس تابع J را رسم کنیم، J بر حسب تتا یک. و این بار پارامترمان را در
سمت چپ در نظر بگیریم. پس تتا یک اینجاست. که مربوط به این نقطه است. اکنون عبارت مشتق یعنی d به d تتا یک
برای J بر حسب تتا یک برای این نقطه
شیب خط این است، پس عبارت مشتق
شیب این خط است. اما این خط نزولی است،
پس شیبش منفی است. Right. یا می‌توان گفت مشتق این تابع (در این نقطه)
منفی است، یعنی شیب منفی در این نقطه. یعنی کمتر از صفر است،
پس زمانی که تتا را تغییر می‌دهم، یعنی تتا را تغییر می‌دهم به
تتا منهای آلفا ضرب‌در عددی منفی. پس یعنی تتا یک را منهای عددی منفی می‌کنم
که در واقع یعنی تتا را افزایش می‌دهم،
چون منهای عددی منفی می‌شود، پس یعنی چیزی به تتا اضافه می‌شود. پس تتا افزایش می‌یابد و تتا به سمت راست می‌رود
و این چیزی است که می‌خواستم که به مینیمم نزدیک‌تر شوم. پس گفتیم که معنای عبارت مشتق چیست. اکنون بیایید ببینیم نرخ یادگیری آلفا چیست
و چه می‌کند. در اینجا گرادیان نزولی
و معادله‌اش را می‌بینیم. و بیایید ببینیم چه می‌شود اگر
آلفا خیلی کوچک یا خیلی بزرگ باشد. در مثال اول،
ببینیم اگر آلفا خیلی کوچک باشد چه می‌شود؟ پس این تابع J بر حسب تتا است. خب از اینجا شروع کنیم. اگر آلفا خیلی کوچک باشد،
چیزی که اتفاق می‌افتد این است که تتا با مقادیر کم تغییر می‌کند
یعنی هر بار قدم‌ها خیلی کوچک است. خب این یک قدم است. پس از این نقطه جدید
می‌خواهم یک قدم جدید بردارم. اما اگر آلفا خیلی کوچک باشد،
یک قدم خیلی کوچک دیگر برمی‌دارم. پس اگر نرخ یادگیری خیلی کوچک باشد، هر بار این قدم‌های خیلی کوچک را
برای رسیدن به مینیمم برمی‌داریم. و نیاز به قدم‌های زیادی خواهد بود
تا به مینیمم برسیم. اگر آلفا کوچک باشد
گرادیان نزولی خیلی کند می‌شود چون هر بار قدم‌ها خیلی کوچک است پس برای رسیدن به مینیمم
باید قدم‌های زیادی بردارم. خب اگر آلفا خیلی بزرگ باشد چه؟ خب این تابع J بر حسب تتا.
اگر آلفا خیلی بزرگ باشد گرادیان نزولی ممکن است از مینیمم بگذرد و حتی نتواند همگرا شود. یا حتی واگرا شود. فرض کنیم تتا اینجا
نزدیک به مینیمم باشد. در اینجا مشتق به سمت راست می‌رود
اما اگر آلفا خیلی بزرگ باشد قدم بزرگی برمی‌دارم. قدم بزرگی مثل این. پس قدم بزرگی خواهد بود
و اکنون تابع هزینه بدتر شده. چون با این مقدار شروع شد
و اکنون مقدار بدتر شده. اکنون مشتق به سمت چپ اشاره می‌کند
یعنی تتا باید کم شود. اما اگر آلفا خیلی بزرگ باشد، قدم بزرگی از این سمت به
طرف دیگر برمی‌دارم. پس از اینجا سردرمی‌آورم. و اگر آلفا خیلی بزرگ باشد
قدم بعدی هم بزرگ است و هر بار دورتر و دورتر می‌شویم
تا اینکه متوجه می‌شوید که دارم از مینیمم دورتر و دورتر می‌شوم. پس اگر آلفا خیلی بزرگ باشد
همگرا نمی‌شود و حتی واگرا می‌شود. حالا یک سؤال از شما. این سؤال ظریفی است
و اولین بار که این مطالب را می‌آموختم خیلی طول کشید تا فهمیدم. چه می‌شود اگر
پارامتر تتا یک در نقطه مینیمم باشد؟ فکر می‌کنید یک قدم گرادیان نزولی چه می‌کند؟ فرض کنید تتا یک در
مینیمم موضعی باشد. پس فرض کنید این مقدار اولیه
برای تتا یک است. و همین الان در بهینه‌ی موضعی
یا مینیمم موضعی است. در بهینه موضوعی مشتق شما صفر است. پس برای این شیب،
شیب این خط برابر صفر است،
پس عبارت مشتق برابر صفر است، و در تغییر گرادیان نزولی دارید تتا یک منهای آلفا ضرب در صفر. و این بدان معناست که
اگر در نقطه بهینه‌ی موضعی باشید تتا یک بدون تغییر باقی می‌ماند،
چون تتا یک به تتا یک تغییر می‌کند. پس اگر پارامترهایتان
در مینیمم موضعی باشد گرادیان نزولی مطلقا هیچ کاری
برای پارامترتان انجام نمی‌دهد و این همان چیزی است که می‌خواهید
چون شما را در بهینه‌ی موضعی نگه می‌دارد. همچنین توضیح می‌دهد که چرا گرادیان نزولی
به مینیمم موضعی ختم می‌شود حتی زمانی که نرخ یادگیری آلفا ثابت باشد. با یک مثال توضیح می‌دهم. اینجا تابع هزینه J را داریم که می‌خواهیم مینیمم کنیم
و فرض کنیم که الگوریتم را، الگوریتم گرادیان نزولی را
از این نقطه صورتی شروع کنم. اگر یک قدم در گرادیان نزولی بردارم،
ممکن است به این نقطه برسم، چون مشتقم خیلی تند است. درست؟ خب الان در این نقطه سبز رنگم
و اگر یک قدم دیگر در گرادیان نزولی بردارم، می‌بینید که مشتق، یعنی شیب خط در نقطه سبز نسبت به صورتی،
ملایم‌تر است چون همین طور که به مینیمم نزدیک می‌شوم
مشتقم به صفر میل می‌کند. چون همین طور که به مینیمم نزدیک می‌شوم
مشتقم به صفر میل می‌کند. پس بعد از یک قدم نزولی،
مشتقم کوچک‌تر می‌شود. پس در قدم بعدی گرادیان نزولی طبیعتا قدم کوچک‌تری از
این نقطه سبزرنگ برمی‌دارم نسبت به نقطه صورتی رنگ. حالا در نقطه جدید، نقطه قرمز،
به مینیمم نزدیک‌ترم پس مشتق در اینجا
کوچک‌تر از مشتق سبز رنگ است. یک قدم دیگر در گرادیان نزولی برمی‌دارم. اکنون مشتقم کوچک‌تر است
بنابراین بزرگی تغییر تتا یک کوچک‌تر است
پس قدم کوچک‌تر است. و همین طور که گرادیان نزولی
پیش می‌رود قدم‌ها خودبه‌خود کوچک و کوچک‌تر می‌شود. تا زمانی که در نهایت
قدم‌ها خیلی کوچک می‌شوند و در نهایت به مینیمم موضعی می‌رسید. پس خلاصه کنیم.
در گرادیان نزولی زمانی که به مینیمم موضعی نزدیک می‌شویم قدم‌های گرادیان نزولی خودبه‌خود
کوچک‌تر می‌شود. و این به این خاطر است که تعریف مینیمم یعنی زمانی که
مشتق برابر صفر است. پس وقتی به مینیمم موضعی نزدیک می‌شویم
این عبارت مشتق خودبه‌خود کوچک‌تر می‌شود و بنابراین
قدم‌های گرادیان نزولی کوچک می‌شود. به این دلیل نیازی نیست که
آلفا را در طول زمان کاهش دهیم. این الگوریتم گرادیان نزولی است
و می‌توانید از آن برای مینیمم کردن هر تابع هزینه J استفاده کنید
نه فقط تابع هزینه‌ای که برای رگرسیون خطی تعریف کردیم. در ویدئوی بعدی،
می‌خواهیم از تابع J استفاده کنیم و آن را به همان
تابع هزینه‌ی رگرسیون خطی تغییر دهیم، همان تابع هزینه‌ی مجذور
که پیش‌تر به دست آوردیم. و گرادیان نزولی را برای
این تابع هزینه‌ی عالی به کار می‌بریم. و اینگونه به اولین الگوریتم یادگیری می‌رسیم، که همان الگوریتم رگرسیون خطی است.