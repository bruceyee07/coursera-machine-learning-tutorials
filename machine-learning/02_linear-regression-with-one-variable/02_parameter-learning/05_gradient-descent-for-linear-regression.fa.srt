1
00:00:00,520 --> 00:00:04,480
در ویدئوهای قبل، در مورد
الگوریتم گرادیان نزولی

2
00:00:04,480 --> 00:00:09,540
و همچنین در مورد مدلسازی رگرسیون خطی
و تابع هزینه خطای مجذور صحبت کردیم.

3
00:00:09,540 --> 00:00:14,280
در این ویدئو گرادیان نزولی را
با تابع هزینه ترکیب می‌کنیم

4
00:00:14,280 --> 00:00:17,400
و الگوریتم رگرسیون خطی را
به دست می‌آوریم تا بتوانیم

5
00:00:17,400 --> 00:00:18,730
خطی راست برای
داده‌هایمان به دست آوریم.

6
00:00:20,800 --> 00:00:24,950
در ویدئوهای قبلی به این
معادلات رسیدیم.

7
00:00:24,950 --> 00:00:28,920
این الگوریتم گرادیان نزولی است
که برایتان آشنا است

8
00:00:28,920 --> 00:00:34,210
و این مدل رگرسیون خطی است
به همراه تابع خطی فرضیه

9
00:00:34,210 --> 00:00:36,540
و تابع هزینه خطای مجذور.

10
00:00:36,540 --> 00:00:42,312
در اینجا می‌خواهیم از گرادیان نزولی

11
00:00:42,312 --> 00:00:47,820
برای مینیمم کردن
تابع هزینه خطای مجذور استفاده کنیم.

12
00:00:47,820 --> 00:00:51,275
برای استفاده از گرادیان نزولی،

13
00:00:51,275 --> 00:00:59,810
یعنی این کادر،
عبارت کلیدی این مشتق جزئی است.

14
00:00:59,810 --> 00:01:04,060
باید بدانید این عبارت مشتق جزئی چیست

15
00:01:04,060 --> 00:01:07,710
و آن را وارد تعریف تابع هزینه J کنید.

16
00:01:07,710 --> 00:01:11,670
که اینجا می‌نویسم.

17
00:01:13,020 --> 00:01:15,550
مجموع iها از یک تا m.

18
00:01:15,550 --> 00:01:21,400
برای این تابع هزینه‌ی خطای مجذور.

19
00:01:21,400 --> 00:01:23,520
کاری که در اینجا کردم

20
00:01:23,520 --> 00:01:26,190
تعریف تابع هزینه را نوشتم.

21
00:01:27,290 --> 00:01:34,820
و اگر کمی ساده‌تر کنیم
خواهیم داشت:

22
00:01:34,820 --> 00:01:43,280
سیگما i از یک تا m،
تتا صفر به اضافه تتا یک در xi
منهای yi، کلا به توان دو.

23
00:01:43,280 --> 00:01:47,830
و در اینجا فقط تعریف فرض را گرفتم

24
00:01:47,830 --> 00:01:50,782
و اینجا نوشتم.

25
00:01:50,782 --> 00:01:53,190
باید بدانیم که این مشتق جزئی

26
00:01:53,190 --> 00:01:56,570
برای دو حالت j = 0
و j = 1 چیست.

27
00:01:56,570 --> 00:02:00,310
یعنی باید ببینیم این مشتق جزئی

28
00:02:00,310 --> 00:02:04,170
برای تتا صفر و تتا یک چیست.

29
00:02:04,170 --> 00:02:06,940
و در اینجا جواب‌ها را می‌نویسم.

30
00:02:06,940 --> 00:02:12,064
این عبارت اول ساده می‌شود به
یک روی m

31
00:02:12,064 --> 00:02:18,354
مجموع عبارت تابع h برحسب xi

32
00:02:18,354 --> 00:02:24,294
منهای yi
و برای این عبارت مشتق جزئی می‌نویسیم تتا یک

33
00:02:24,294 --> 00:02:27,114
که به این عبارت می‌رسم.

34
00:02:27,114 --> 00:02:34,008
مجموع تابع h بر حسب xi
منهای yi ضرب در xi

35
00:02:34,008 --> 00:02:37,440
خب.

36
00:02:37,440 --> 00:02:41,720
و برای محاسبه مشتق‌های جزئی
از این معادله

37
00:02:41,720 --> 00:02:46,000
به یکی از معادله‌های زیرین می‌رسیم.

38
00:02:46,000 --> 00:02:51,020
محاسبه این مشتق‌های جزئی
نیازمند حسابان چندمتغیره است.

39
00:02:51,020 --> 00:02:54,930
اگر حسابان می‌دانید
می‌توانید خودتان مشتق‌گیری را

40
00:02:54,930 --> 00:02:59,510
چک کنید و به همین جوابی می‌رسید
که من اینجا می‌گویم.

41
00:02:59,510 --> 00:03:04,050
اما اگر با حسابان آشنایی کمتری دارید
نگران نباشید

42
00:03:04,050 --> 00:03:08,100
می‌توانید از همین معادله‌ها استفاده کنید

43
00:03:08,100 --> 00:03:11,350
و لازم نیست حسابان بدانید
تا تکالیف را حل کنید.

44
00:03:11,350 --> 00:03:13,390
پس در اینجا گرادیان نزولی را اضافه می‌کنیم.

45
00:03:14,750 --> 00:03:18,490
پس مسلح به این معادله‌ها

46
00:03:18,490 --> 00:03:22,310
و مسلح به مشتق‌ها
که در واقع شیب تابع هزینه J هستند،

47
00:03:23,310 --> 00:03:27,160
می‌توانیم به الگوریتم گرادیان نزولی برگردیم.

48
00:03:27,160 --> 00:03:28,640
این گرادیان نزولی است برای

49
00:03:28,640 --> 00:03:32,728
رگرسیون خطی که تا همگرایی تکرار می‌شود.
تتا صفر

50
00:03:32,728 --> 00:03:38,380
و تتا یک به این صورت تغییر می‌کنند که
مقدار قبلی منهای آلفا ضرب‌در عبارت مشتق می‌شود.

51
00:03:39,390 --> 00:03:41,070
یعنی این عبارت.

52
00:03:43,080 --> 00:03:46,050
این الگوریتم رگرسیون خطی است.

53
00:03:47,160 --> 00:03:48,628
اولین عبارت در اینجا،

54
00:03:52,529 --> 00:03:56,804
عبارت مشتق جزئی برحسب تتا صفر است،

55
00:03:56,804 --> 00:03:59,790
که در اسلاید قبلی به دست آوردیم.

56
00:03:59,790 --> 00:04:05,730
و دومین عبارت اینجا،
مشتق جزئی

57
00:04:05,730 --> 00:04:11,420
برحسب تتا یک است
که در اسلاید قبلی به دست آوردیم.

58
00:04:11,420 --> 00:04:15,230
و به عنوان یادآور سریع،
باید در زمان استفاده از گرادیان نزولی

59
00:04:15,230 --> 00:04:19,265
به این نکته توجه کنید که

60
00:04:19,265 --> 00:04:22,250
تتا صفر و تتا یک را همزمان تغییر دهید.

61
00:04:24,290 --> 00:04:25,570
خب.

62
00:04:25,570 --> 00:04:28,120
ببینیم گرادیان نزولی چطور کار می‌کند.

63
00:04:28,120 --> 00:04:31,862
یکی از اشکالات گرادیان نزولی این است که ممکن است

64
00:04:31,862 --> 00:04:32,700
به بهینه موضعی برسد.

65
00:04:32,700 --> 00:04:36,780
اولین بار که گرادیان نزولی را معرفی کردم
این تصویر را نشان دادم

66
00:04:36,780 --> 00:04:40,900
که به پایین می‌رفتیم و دیدیم که
بسته به اینکه از کجا شروع کنیم

67
00:04:40,900 --> 00:04:43,014
ممکن است به بهینه‌ی موضعی برسیم.

68
00:04:43,014 --> 00:04:45,480
مثلا به اینجا یا اینجا برسید.

69
00:04:45,480 --> 00:04:50,390
اما تابع هزینه برای

70
00:04:50,390 --> 00:04:55,220
رگرسیون خطی همیشه تابعی سهمی مانند
شبیه این است.

71
00:04:55,220 --> 00:05:00,190
اصطلاح تخصصی برای این شکل
تابع محدب است.

72
00:05:03,230 --> 00:05:07,800
و نمی‌خواهم تعریف دقیق تابع محدب را بگویم،

73
00:05:07,800 --> 00:05:09,490
که حروفش
C, O, N, V, E, X
است،

74
00:05:09,490 --> 00:05:16,620
همین‌قدر بدانید که تابع محدب تابعی کاسه‌مانند است

75
00:05:16,620 --> 00:05:22,295
و این تابع بهینه‌های موضعی ندارد
و فقط یک نقطه‌ی بهینه دارد.

76
00:05:22,295 --> 00:05:26,465
و گرادیان نزولی برای این نوع تابع هزینه
که در آن

77
00:05:26,465 --> 00:05:30,445
از رگرسیون خطی استفاده می‌کنید
همیشه به نقطه بهینه همگرا می‌شود.

78
00:05:30,445 --> 00:05:33,155
چون نقطه‌ی بهینه‌ی دیگری به جز
بهینه‌ی اصلی وجود ندارد.

79
00:05:33,155 --> 00:05:36,615
بیایید این الگوریتم را در عمل ببینیم.

80
00:05:38,250 --> 00:05:45,910
مثل همیشه، این تابع فرضیه
و این تابع هزینه‌ی J است.

81
00:05:45,910 --> 00:05:50,020
و بیایید پارامترهایم را با این مقدار شروع کنم.

82
00:05:50,020 --> 00:05:54,220
معمولا پارامترها را با صفر و صفر آغاز می‌کنند.

83
00:05:54,220 --> 00:05:56,370
یعنی تتا صفر و تتا یک هر دو برابر 0

84
00:05:56,370 --> 00:06:01,354
اما برای این شکل و این تابع

85
00:06:01,354 --> 00:06:07,619
تتا صفر را برابر 900
و تتا یک را برابر منفی 0.1 قرار می‌دهم.

86
00:06:07,619 --> 00:06:12,644
پس خواهیم داشت
h(x)=900-0.1x

87
00:06:12,644 --> 00:06:16,547
این خط است که

88
00:06:16,547 --> 00:06:21,060
از تابع هزینه به دست آمده است.
حال اگر یک قدم در گرادیان نزولی برداریم،

89
00:06:21,060 --> 00:06:26,845
از این نقطه در اینجا

90
00:06:26,845 --> 00:06:31,510
به آن نقطه دوم می‌رسیم.

91
00:06:31,510 --> 00:06:35,450
و متوجه شدید که خط من کمی تغییر کرد.

92
00:06:35,450 --> 00:06:39,780
و اگر یک قدم دیگر در گرادیان نزولی بردارم
خط من در سمت چپ تغییر می‌کند.

93
00:06:41,230 --> 00:06:42,380
درست؟

94
00:06:42,380 --> 00:06:46,370
و روی تابع هزینه به نقطه جدیدی می‌روم.

95
00:06:47,670 --> 00:06:52,760
و اگر باز هم قدمی در گرادیان نزولی بردارم
هزینه کمتر می‌شود.

96
00:06:52,760 --> 00:06:56,190
پس پارامترها و همه چیز دیگر
این مسیر را دنبال می‌کنند.

97
00:06:57,340 --> 00:07:02,430
و اگر به سمت چپ نگاه کنید
این متناظر با تابع فرضیه‌ای است

98
00:07:02,430 --> 00:07:06,520
که بهتر و بهتر با داده‌ها متناسب می‌شود.

99
00:07:08,200 --> 00:07:14,660
تا زمانی که نهایتا به مینیمم اصلی می‌رسد
و این نقطه مینیمم

100
00:07:14,660 --> 00:07:20,090
متناظر با این تابع فرضیه است
که تناسب خوبی با داده‌ها دارد.

101
00:07:21,400 --> 00:07:25,800
و این گرادیان نزولی است
که در عمل دیدیم

102
00:07:25,800 --> 00:07:31,230
و تناسب خوبی با
مجموعه داده‌های قیمت‌های خانه دارد.

103
00:07:31,230 --> 00:07:34,490
و اکنون می‌توانید از آن استفاده کنید

104
00:07:34,490 --> 00:07:38,900
تا اگر دوستتان خانه‌ای
با مساحت ۱۲۵۰ فوت مربع داشت

105
00:07:38,900 --> 00:07:43,350
بگویید که مثلا احتمال دارد

106
00:07:43,350 --> 00:07:48,720
بتواند ۲۵۰۰۰۰ دلار بفروشد.

107
00:07:48,720 --> 00:07:52,620
نکته آخر اینکه اسم این الگوریتم

108
00:07:52,620 --> 00:07:57,510
که اکنون دیدیم
گرادیان نزولی انبوه نام دارد.

109
00:07:57,510 --> 00:08:00,730
و در یادگیری ماشین ثابت شده که ما

110
00:08:00,730 --> 00:08:04,310
همیشه از اسامی خوبی برای
نامیدن الگوریتم‌هایمان استفاده نمی‌کنیم

111
00:08:04,310 --> 00:08:08,880
اما واژه گرادیان نزولی انبوه به این معنی است که

112
00:08:08,880 --> 00:08:13,850
در هر قدم گرادیان نزولی
از تمامی نمونه‌های آموزشی استفاده می‌کنیم.

113
00:08:13,850 --> 00:08:17,760
پس در گرادیان نزولی زمانی که
مشتق‌ها را محاسبه می‌کنیم

114
00:08:17,760 --> 00:08:21,400
این مجموع را محاسبه می‌کنیم.

115
00:08:21,400 --> 00:08:25,660
پس در هر قدم گرادیان نزولی
چیزی شبیه این را محاسبه می‌کنیم که در آن

116
00:08:25,660 --> 00:08:30,620
مجموع خطای m نمونه آموزشی محاسبه می‌شود
بنابراین اصطلاح گرادیان نزولی انبوه

117
00:08:30,620 --> 00:08:34,175
به این اشاره دارد که انبوه نمونه‌های آموزشی را
در نظر می‌گیریم.

118
00:08:34,175 --> 00:08:36,365
و مجددا می‌گویم که نام زیبایی نیست

119
00:08:36,365 --> 00:08:39,585
اما کاربران یادگیری ماشین از آن استفاده می‌کنند.

120
00:08:39,585 --> 00:08:43,715
و گاهی مواقع دیگر نسخه‌های گرادیان نزولی

121
00:08:43,715 --> 00:08:46,247
نسخه‌های انبوه نیستند و

122
00:08:46,247 --> 00:08:48,837
تمامی مجموعه آموزشی را در نظر نمی‌گیرند

123
00:08:48,837 --> 00:08:51,247
بلکه هربار مجموعه‌ی کوچکی از
نمونه‌های آموزشی را در نظر می‌گیرند.

124
00:08:51,247 --> 00:08:55,207
و در مورد آن نسخه‌ها هم بعدتر صحبت می‌کنیم.

125
00:08:55,207 --> 00:08:58,357
اما اکنون از الگوریتمی که دیدیم یعنی
گرادیان نزولی انبوه استفاده می‌کنیم.

126
00:08:58,357 --> 00:09:03,497
اکنون می‌دانید که چگونه از گرادیان نزولی
برای رگرسیون خطی استفاده کنید.

127
00:09:05,980 --> 00:09:09,550
این هم از رگرسیون خطی با گرادیان نزولی.

128
00:09:09,550 --> 00:09:12,260
اگر از قبل با جبر خطی پیشرفته آشنایی دارید،

129
00:09:12,260 --> 00:09:15,510
مثلا ممکن است بعضی از شما کلاسش را گذرانده باشید،

130
00:09:15,510 --> 00:09:19,410
احتمالا می‌دانید راه‌حلی وجود دارد که به صورت عددی

131
00:09:19,410 --> 00:09:22,270
مینیمم تابع هزینه J را مشخص می‌کند
بدون آنکه

132
00:09:22,270 --> 00:09:25,870
نیازی به استفاده الگوریتمی تکرارشونده
مانند گرادیان نزولی باشد.

133
00:09:25,870 --> 00:09:29,730
بعدتر در این دوره،
در مورد آن روش هم صحبت می‌کنیم

134
00:09:29,730 --> 00:09:33,020
که مینیمم تابع هزینه J را محاسبه می‌کند،
بدون آنکه نیازی به انجام مراحل متعدد

135
00:09:33,020 --> 00:09:34,520
گرادیان نزولی باشد.

136
00:09:34,520 --> 00:09:37,020
نام آن روش شیوه معادلات نرمال است.

137
00:09:37,020 --> 00:09:41,000
اما اگر با آن روش آشنایید باید بدانید

138
00:09:41,000 --> 00:09:46,420
گرادیان نزولی برای داده‌های بزرگ‌تر
بهتر از متد معادلات نرمال عمل می‌کند.

139
00:09:46,420 --> 00:09:50,140
و اکنون که در مورد گرادیان نزولی می‌دانیم،
می‌خواهیم از آن برای

140
00:09:50,140 --> 00:09:51,400
موضوعات مختلف

141
00:09:51,400 --> 00:09:53,910
و مسائل مختلف یادگیری ماشین استفاده کنیم.

142
00:09:55,340 --> 00:10:00,430
تبریک به شما برای یادگیری
نخستین الگوریتم یادگیری ماشین.

143
00:10:00,430 --> 00:10:04,990
بعدتر تمرین‌هایی داریم که ازتان می‌خواهیم
از گرادیان نزولی استفاده کنید،

144
00:10:04,990 --> 00:10:07,480
و می‌توانید کارکرد الگوریتم را خودتان ببینید.

145
00:10:07,480 --> 00:10:11,460
اما پیش از آن می‌خواهم
به شما بگویم که در چند ویدئوی آینده

146
00:10:11,460 --> 00:10:14,510
با هم در مورد تعمیم

147
00:10:14,510 --> 00:10:17,900
الگوریتم گرادیان نزولی صحبت می‌کنیم
تا آن را قدرتمندتر کنیم.

148
00:10:17,900 --> 00:10:20,420
پس در ویدئوی بعد
بیشتر در این خصوص صحبت می‌کنیم.