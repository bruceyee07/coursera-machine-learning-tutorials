1
00:00:00,190 --> 00:00:01,190
در ویدئوی قبل،

2
00:00:01,190 --> 00:00:04,960
تعریف ریاضیاتی گرادیان نزولی
را ارائه کردیم.

3
00:00:04,960 --> 00:00:08,830
اجازه دهید در این ویدئو
عمیق‌تر به مسأله بپردازیم که این الگوریتم

4
00:00:08,830 --> 00:00:12,580
چه کاری انجام می‌دهد و چرا
مراحل گرادیان نزولی درست هستند.

5
00:00:15,430 --> 00:00:19,580
این الگوریتن گرادیان نزولی است
که در ویدئوی قبل دیدیم

6
00:00:19,580 --> 00:00:25,800
و برای یادآوری، این پارامتر آلفا
نرخ یادگیری نام دارد.

7
00:00:25,800 --> 00:00:30,620
و بزرگی قدمی که برمی‌داریم
تا پارامتر تتا j را تغییر دهیم کنترل می‌کند.

8
00:00:31,660 --> 00:00:36,406
و این اصطلاح دوم در اینجا
عبارت مشتق است.

9
00:00:39,147 --> 00:00:43,904
و می‌خواهم در این ویدئو بگویم
که این دو عبارت

10
00:00:43,904 --> 00:00:49,281
چه کاری انجام می‌دهند و چرا
وقتی با هم باشند معنا می‌دهند.

11
00:00:49,281 --> 00:00:55,054
به منظور درک این مطالب،
می‌خواهم از مثالی ساده‌تر استفاده کنم،

12
00:00:55,054 --> 00:01:00,403
که در آن می‌خواهیم تابعی با
یک پارامتر را مینیمم کنیم.

13
00:01:00,403 --> 00:01:03,930
پس فرض کنیم تابع هزینه J فقط
یک پارامتر دارد،

14
00:01:03,930 --> 00:01:09,410
تتا یک، مثل کاری که چند ویدئو قبل‌تر کردیم
و تتا یک هم عددی حقیقی است.

15
00:01:09,410 --> 00:01:14,410
پس می‌توانیم روی کاغذ
نشان دهیم و این خیلی راحت‌تر است.

16
00:01:14,410 --> 00:01:17,360
ببینیم گرادیان نزولی بر روی این تابع
چه کاری انجام می‌دهد.

17
00:01:20,850 --> 00:01:26,980
فرض کنیم این تابع J برای
تتا یک است.

18
00:01:26,980 --> 00:01:28,550
به این ترتیب.

19
00:01:28,550 --> 00:01:31,505
و تتا یک هم عددی حقیقی است.

20
00:01:32,635 --> 00:01:33,755
بسیار خب ؟

21
00:01:33,755 --> 00:01:39,605
فرض کنیم در گرادیان نزولی
تتا یک را در اینجا قرار داده‌ایم.

22
00:01:39,605 --> 00:01:43,085
یعنی تصور کنید با این نقطه روی تابع
شروع کرده‌ایم.

23
00:01:44,495 --> 00:01:48,105
کاری که گرادیان نزولی انجام می‌دهد

24
00:01:49,570 --> 00:01:54,719
تغییر تتا است. پس تتا یک به این شکل
تغییر می‌کند: تتا یک منهای آلفا ضرب در

25
00:01:54,719 --> 00:02:01,940
مشتق تابع J بر حسب تتا یک.

26
00:02:01,940 --> 00:02:09,140
و در حاشیه بگویم،
برای عبارت مشتق،

27
00:02:09,140 --> 00:02:13,170
شاید تعجب کردید که چرا
از این علامت مشتق جزئی استفاده نکردم.

28
00:02:13,170 --> 00:02:16,280
اگر نمی‌دانید که تفاوت میان علامت
مشتق جزئی

29
00:02:16,280 --> 00:02:18,490
و استفاده از d بر d تتا چیست
نگران نباشید.

30
00:02:18,490 --> 00:02:22,000
از نظر فنی در ریاضیات
به این مشتق جزئی می‌گوییم

31
00:02:22,000 --> 00:02:27,170
و به این مشتق،
و بستگی به تعداد پارامترهای تابع J دارد.

32
00:02:27,170 --> 00:02:28,690
اما این موارد فنی ریاضیات است.

33
00:02:28,690 --> 00:02:32,000
برای این ویدئو،

34
00:02:32,000 --> 00:02:36,430
نمادهای مشتق جزئی و مشتق را
درست یکسان در نظر بگیرید.

35
00:02:36,430 --> 00:02:38,650
و نگران نباشید که تفاوت اصلی چیست.

36
00:02:38,650 --> 00:02:42,340
من از علامت‌های ریاضی درست استفاده می‌کنم

37
00:02:42,340 --> 00:02:46,550
اما برای این ویدئو
این دو واقعا یکسانند.

38
00:02:46,550 --> 00:02:48,890
ببینیم این معادله چه می‌کند.

39
00:02:48,890 --> 00:02:53,470
می‌خواهیم این مشتق را حساب کنیم.
نمی‌دانم مشتق را

40
00:02:53,470 --> 00:02:57,460
قبلا در حسابان دیده‌اید یا نه،
اما کاری که مشتق در این نقطه می‌کند این است که

41
00:02:57,460 --> 00:03:02,380
تانژانت این نقطه را می‌گیرد،
مثل این خط قرمز،

42
00:03:02,380 --> 00:03:06,480
که با تابع مماس است،
و به شیب این خط نگاه کنیم.

43
00:03:06,480 --> 00:03:07,870
این مشتق است.

44
00:03:07,870 --> 00:03:11,890
یعنی شیب خطی که بر این نقطه از تابع
مماس است چقدر است.

45
00:03:11,890 --> 00:03:17,278
و شیب خط هم که از تقسیم ارتفاع
بر خط افقی به دست می‌آید.

46
00:03:17,278 --> 00:03:22,780
شیب این خط مثبت است،

47
00:03:22,780 --> 00:03:26,690
پس مشتقش مثبت است.

48
00:03:26,690 --> 00:03:30,370
پس تغییر تتا به این صورت است که

49
00:03:30,370 --> 00:03:35,813
تتا یک تغییر می‌کند به
تتا یک منهای آلفا ضرب در یک عدد مثبت.

50
00:03:39,296 --> 00:03:40,271
Okay.

51
00:03:40,271 --> 00:03:43,300
آلفا که نرخ یادگیری است
همیشه عدد مثبت است،

52
00:03:43,300 --> 00:03:47,220
بنابراین داریم
تتا یک منهای چیزی.

53
00:03:47,220 --> 00:03:49,910
پس در نهایت تتا یک به سمت راست می‌رود.

54
00:03:49,910 --> 00:03:53,350
تتا یک کاهش می‌یابد
و می‌بینیم که درست است،

55
00:03:53,350 --> 00:03:55,810
چون باید در این مسیر حرکت کنیم.

56
00:03:55,810 --> 00:03:59,490
تا به مینیممی که اینجاست
نزدیک‌تر شویم.

57
00:04:00,810 --> 00:04:04,060
پس تا اینجا با گرادیان نزولی
کار درست را انجام می‌دهیم.

58
00:04:04,060 --> 00:04:06,120
یک مثال دیگر.

59
00:04:06,120 --> 00:04:10,330
مجددا از تابع J استفاده می‌کنیم.
پس تابع J را رسم کنیم،

60
00:04:10,330 --> 00:04:11,530
J بر حسب تتا یک.

61
00:04:11,530 --> 00:04:16,140
و این بار پارامترمان را در
سمت چپ در نظر بگیریم.

62
00:04:16,140 --> 00:04:17,740
پس تتا یک اینجاست.

63
00:04:17,740 --> 00:04:19,240
که مربوط به این نقطه است.

64
00:04:20,650 --> 00:04:25,680
اکنون عبارت مشتق یعنی d به d تتا یک
برای J بر حسب تتا یک

65
00:04:25,680 --> 00:04:31,420
برای این نقطه
شیب خط این است،

66
00:04:31,420 --> 00:04:34,580
پس عبارت مشتق
شیب این خط است.

67
00:04:34,580 --> 00:04:37,644
اما این خط نزولی است،
پس شیبش منفی است.

68
00:04:41,021 --> 00:04:41,990
Right.

69
00:04:41,990 --> 00:04:45,880
یا می‌توان گفت مشتق این تابع (در این نقطه)
منفی است،

70
00:04:45,880 --> 00:04:48,220
یعنی شیب منفی در این نقطه.

71
00:04:48,220 --> 00:04:54,690
یعنی کمتر از صفر است،
پس زمانی که تتا را تغییر می‌دهم،

72
00:04:54,690 --> 00:04:59,100
یعنی تتا را تغییر می‌دهم به
تتا منهای آلفا ضرب‌در عددی منفی.

73
00:05:02,380 --> 00:05:07,460
پس یعنی تتا یک را منهای عددی منفی می‌کنم
که در واقع یعنی

74
00:05:07,460 --> 00:05:11,210
تتا را افزایش می‌دهم،
چون منهای عددی منفی می‌شود،

75
00:05:11,210 --> 00:05:12,840
پس یعنی چیزی به تتا اضافه می‌شود.

76
00:05:12,840 --> 00:05:16,640
پس تتا افزایش می‌یابد

77
00:05:16,640 --> 00:05:21,110
و تتا به سمت راست می‌رود
و این چیزی است که می‌خواستم

78
00:05:21,110 --> 00:05:24,270
که به مینیمم نزدیک‌تر شوم.

79
00:05:26,430 --> 00:05:31,600
پس گفتیم که معنای عبارت مشتق چیست.

80
00:05:31,600 --> 00:05:36,250
اکنون بیایید ببینیم نرخ یادگیری آلفا چیست
و چه می‌کند.

81
00:05:38,090 --> 00:05:42,330
در اینجا گرادیان نزولی
و معادله‌اش را می‌بینیم.

82
00:05:43,890 --> 00:05:48,440
و بیایید ببینیم چه می‌شود اگر
آلفا خیلی کوچک

83
00:05:48,440 --> 00:05:50,740
یا خیلی بزرگ باشد.

84
00:05:50,740 --> 00:05:54,200
در مثال اول،
ببینیم اگر آلفا خیلی کوچک باشد چه می‌شود؟

85
00:05:54,200 --> 00:05:59,228
پس این تابع J بر حسب تتا است.

86
00:05:59,228 --> 00:06:02,460
خب از اینجا شروع کنیم.

87
00:06:02,460 --> 00:06:06,920
اگر آلفا خیلی کوچک باشد،
چیزی که اتفاق می‌افتد این است که

88
00:06:06,920 --> 00:06:11,220
تتا با مقادیر کم تغییر می‌کند
یعنی هر بار قدم‌ها خیلی کوچک است.

89
00:06:11,220 --> 00:06:13,350
خب این یک قدم است.

90
00:06:13,350 --> 00:06:16,520
پس از این نقطه جدید
می‌خواهم یک قدم جدید بردارم.

91
00:06:16,520 --> 00:06:19,690
اما اگر آلفا خیلی کوچک باشد،
یک قدم خیلی کوچک دیگر برمی‌دارم.

92
00:06:19,690 --> 00:06:26,530
پس اگر نرخ یادگیری خیلی کوچک باشد،

93
00:06:26,530 --> 00:06:31,770
هر بار این قدم‌های خیلی کوچک را
برای رسیدن به مینیمم برمی‌داریم.

94
00:06:31,770 --> 00:06:35,380
و نیاز به قدم‌های زیادی خواهد بود
تا به مینیمم برسیم.

95
00:06:35,380 --> 00:06:38,980
اگر آلفا کوچک باشد
گرادیان نزولی خیلی کند می‌شود چون

96
00:06:38,980 --> 00:06:40,880
هر بار قدم‌ها خیلی کوچک است

97
00:06:40,880 --> 00:06:44,830
پس برای رسیدن به مینیمم
باید قدم‌های زیادی بردارم.

98
00:06:46,750 --> 00:06:49,460
خب اگر آلفا خیلی بزرگ باشد چه؟

99
00:06:49,460 --> 00:06:54,880
خب این تابع J بر حسب تتا.
اگر آلفا خیلی بزرگ باشد

100
00:06:54,880 --> 00:06:59,180
گرادیان نزولی ممکن است از مینیمم بگذرد و حتی نتواند همگرا شود.

101
00:06:59,180 --> 00:07:00,910
یا حتی واگرا شود.

102
00:07:00,910 --> 00:07:04,170
فرض کنیم تتا اینجا
نزدیک به مینیمم باشد.

103
00:07:04,170 --> 00:07:07,430
در اینجا مشتق به سمت راست می‌رود
اما اگر آلفا خیلی بزرگ باشد

104
00:07:07,430 --> 00:07:09,060
قدم بزرگی برمی‌دارم.

105
00:07:09,060 --> 00:07:10,820
قدم بزرگی مثل این.

106
00:07:10,820 --> 00:07:14,980
پس قدم بزرگی خواهد بود
و اکنون تابع هزینه بدتر شده.

107
00:07:14,980 --> 00:07:19,390
چون با این مقدار شروع شد
و اکنون مقدار بدتر شده.

108
00:07:19,390 --> 00:07:22,872
اکنون مشتق به سمت چپ اشاره می‌کند
یعنی تتا باید کم شود.

109
00:07:22,872 --> 00:07:25,070
اما اگر آلفا خیلی بزرگ باشد،

110
00:07:25,070 --> 00:07:27,930
قدم بزرگی از این سمت به
طرف دیگر برمی‌دارم.

111
00:07:27,930 --> 00:07:31,560
پس از اینجا سردرمی‌آورم.

112
00:07:31,560 --> 00:07:35,020
و اگر آلفا خیلی بزرگ باشد
قدم بعدی هم بزرگ است

113
00:07:35,020 --> 00:07:39,950
و هر بار دورتر و دورتر می‌شویم
تا اینکه متوجه می‌شوید

114
00:07:39,950 --> 00:07:44,170
که دارم از مینیمم دورتر و دورتر می‌شوم.

115
00:07:44,170 --> 00:07:49,530
پس اگر آلفا خیلی بزرگ باشد
همگرا نمی‌شود و حتی واگرا می‌شود.

116
00:07:49,530 --> 00:07:52,170
حالا یک سؤال از شما.

117
00:07:52,170 --> 00:07:55,870
این سؤال ظریفی است
و اولین بار که این مطالب را می‌آموختم

118
00:07:55,870 --> 00:07:57,020
خیلی طول کشید تا فهمیدم.

119
00:07:57,020 --> 00:08:00,740
چه می‌شود اگر
پارامتر تتا یک در نقطه مینیمم باشد؟

120
00:08:00,740 --> 00:08:03,420
فکر می‌کنید یک قدم گرادیان نزولی چه می‌کند؟

121
00:08:06,520 --> 00:08:10,260
فرض کنید تتا یک در
مینیمم موضعی باشد.

122
00:08:10,260 --> 00:08:15,580
پس فرض کنید این مقدار اولیه
برای تتا یک است.

123
00:08:15,580 --> 00:08:18,630
و همین الان در بهینه‌ی موضعی
یا مینیمم موضعی است.

124
00:08:19,960 --> 00:08:23,280
در بهینه موضوعی مشتق شما صفر است.

125
00:08:23,280 --> 00:08:29,070
پس برای این شیب،
شیب این خط

126
00:08:29,070 --> 00:08:36,370
برابر صفر است،
پس عبارت مشتق برابر صفر است،

127
00:08:36,370 --> 00:08:38,430
و در تغییر گرادیان نزولی

128
00:08:38,430 --> 00:08:43,970
دارید تتا یک منهای آلفا ضرب در صفر.

129
00:08:43,970 --> 00:08:48,780
و این بدان معناست که
اگر در نقطه بهینه‌ی موضعی باشید

130
00:08:48,780 --> 00:08:54,680
تتا یک بدون تغییر باقی می‌ماند،
چون تتا یک به تتا یک تغییر می‌کند.

131
00:08:54,680 --> 00:08:57,830
پس اگر پارامترهایتان
در مینیمم موضعی باشد

132
00:08:57,830 --> 00:09:00,980
گرادیان نزولی مطلقا هیچ کاری
برای پارامترتان انجام نمی‌دهد

133
00:09:00,980 --> 00:09:04,830
و این همان چیزی است که می‌خواهید
چون شما را در بهینه‌ی موضعی نگه می‌دارد.

134
00:09:05,970 --> 00:09:09,860
همچنین توضیح می‌دهد که چرا گرادیان نزولی
به مینیمم موضعی ختم می‌شود

135
00:09:09,860 --> 00:09:13,110
حتی زمانی که نرخ یادگیری آلفا ثابت باشد.

136
00:09:13,110 --> 00:09:15,570
با یک مثال توضیح می‌دهم.

137
00:09:15,570 --> 00:09:20,570
اینجا تابع هزینه J را داریم

138
00:09:20,570 --> 00:09:24,750
که می‌خواهیم مینیمم کنیم
و فرض کنیم که الگوریتم را،

139
00:09:24,750 --> 00:09:29,040
الگوریتم گرادیان نزولی را
از این نقطه صورتی شروع کنم.

140
00:09:29,040 --> 00:09:33,060
اگر یک قدم در گرادیان نزولی بردارم،
ممکن است به این نقطه برسم،

141
00:09:33,060 --> 00:09:34,770
چون مشتقم خیلی تند است.

142
00:09:34,770 --> 00:09:36,020
درست؟

143
00:09:36,020 --> 00:09:41,130
خب الان در این نقطه سبز رنگم
و اگر یک قدم دیگر در گرادیان نزولی بردارم،

144
00:09:41,130 --> 00:09:45,740
می‌بینید که مشتق، یعنی شیب خط

145
00:09:45,740 --> 00:09:49,470
در نقطه سبز نسبت به صورتی،
ملایم‌تر است

146
00:09:49,470 --> 00:09:54,060
چون همین طور که به مینیمم نزدیک می‌شوم
مشتقم به صفر میل می‌کند.

147
00:09:54,060 --> 00:09:57,570
چون همین طور که به مینیمم نزدیک می‌شوم
مشتقم به صفر میل می‌کند.

148
00:09:57,570 --> 00:10:02,350
پس بعد از یک قدم نزولی،
مشتقم کوچک‌تر می‌شود.

149
00:10:02,350 --> 00:10:04,890
پس در قدم بعدی گرادیان نزولی

150
00:10:04,890 --> 00:10:08,910
طبیعتا قدم کوچک‌تری از
این نقطه سبزرنگ برمی‌دارم

151
00:10:08,910 --> 00:10:11,290
نسبت به نقطه صورتی رنگ.

152
00:10:11,290 --> 00:10:15,030
حالا در نقطه جدید، نقطه قرمز،
به مینیمم نزدیک‌ترم

153
00:10:15,030 --> 00:10:19,390
پس مشتق در اینجا
کوچک‌تر از مشتق سبز رنگ است.

154
00:10:19,390 --> 00:10:21,050
یک قدم دیگر در گرادیان نزولی برمی‌دارم.

155
00:10:22,280 --> 00:10:26,560
اکنون مشتقم کوچک‌تر است
بنابراین بزرگی

156
00:10:26,560 --> 00:10:31,700
تغییر تتا یک کوچک‌تر است
پس قدم کوچک‌تر است.

157
00:10:31,700 --> 00:10:36,630
و همین طور که گرادیان نزولی
پیش می‌رود

158
00:10:36,630 --> 00:10:40,870
قدم‌ها خودبه‌خود کوچک و کوچک‌تر می‌شود.

159
00:10:41,880 --> 00:10:45,230
تا زمانی که در نهایت
قدم‌ها خیلی کوچک می‌شوند

160
00:10:45,230 --> 00:10:48,990
و در نهایت به مینیمم موضعی می‌رسید.

161
00:10:50,270 --> 00:10:55,580
پس خلاصه کنیم.
در گرادیان نزولی زمانی که به مینیمم موضعی نزدیک می‌شویم

162
00:10:55,580 --> 00:10:58,290
قدم‌های گرادیان نزولی خودبه‌خود
کوچک‌تر می‌شود.

163
00:10:58,290 --> 00:11:01,060
و این به این خاطر است که

164
00:11:01,060 --> 00:11:06,110
تعریف مینیمم یعنی زمانی که
مشتق برابر صفر است.

165
00:11:06,110 --> 00:11:10,450
پس وقتی به مینیمم موضعی نزدیک می‌شویم
این عبارت مشتق خودبه‌خود

166
00:11:10,450 --> 00:11:16,720
کوچک‌تر می‌شود و بنابراین
قدم‌های گرادیان نزولی کوچک می‌شود.

167
00:11:16,720 --> 00:11:21,140
به این دلیل نیازی نیست که
آلفا را در طول زمان کاهش دهیم.

168
00:11:22,810 --> 00:11:27,840
این الگوریتم گرادیان نزولی است
و می‌توانید از آن برای مینیمم کردن

169
00:11:27,840 --> 00:11:32,940
هر تابع هزینه J استفاده کنید
نه فقط تابع هزینه‌ای که برای رگرسیون خطی تعریف کردیم.

170
00:11:32,940 --> 00:11:35,720
در ویدئوی بعدی،
می‌خواهیم از تابع J استفاده کنیم

171
00:11:35,720 --> 00:11:39,350
و آن را به همان
تابع هزینه‌ی رگرسیون خطی تغییر دهیم،

172
00:11:39,350 --> 00:11:42,140
همان تابع هزینه‌ی مجذور
که پیش‌تر به دست آوردیم.

173
00:11:42,140 --> 00:11:46,210
و گرادیان نزولی را برای
این تابع هزینه‌ی عالی به کار می‌بریم.

174
00:11:46,210 --> 00:11:48,830
و اینگونه به اولین الگوریتم یادگیری می‌رسیم،

175
00:11:48,830 --> 00:11:50,750
که همان الگوریتم رگرسیون خطی است.