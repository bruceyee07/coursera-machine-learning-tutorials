באלגוריתם PCA אנו לוקחים תכונות n ממדיות ומצמצמים אותם לייצוג תכונות k מימדי. המספר הזה k הוא פרמטר של אלגוריתם PCA. המספר הזה נקרא גם מספר המרכיבים העיקריים, מספר המרכיבים העיקריים שנשמרו. בסרטון הזה אני רוצה לתת לכם הנחיות, ולהסביר לכם איך אנשים חושבים בדרך כלל על בחירת הפרמטר הזה k עבור PCA. כדי לבחור את k, כלומר לבחור את מספר המרכיבים העיקריים, נתחיל בכמה מושגים שימושיים. PCA מנסה למזער את ריבוע שגיאת ההיטל הממוצעת. הוא מנסה למזער את הביטוי הזה שאני כותב כאן, שהוא ההפרש בין הנתונים המקוריים x לבין גרסת ההיטל, קירוב-x, שהגדרנו בסרטון שעבר, או הוא מנסה לצמצם את ריבוע המרחק בין x ובין ההיטל שלו על פני תת-המרחב הזה עם מימד קטן יותר. אז זה ריבוע שגיאת ההיטל הממוצעת. כמו כן, אני אגדיר את הווריאציה הכוללת של הנתונים להיות ממוצע ריבוע האורך של הדוגמאות xi או במילים אחרות הווריאציה הכוללת של הנתונים היא הממוצע בסדרת האימון, של האורך של כל אחת מדוגמאות האימון. מה שזה אומר הוא, "בממוצע, כווקטורים, עד כמה רחוקות דוגמאות האימון שלי מאפס?" עד כמה רחוקות בממוצע דוגמאות האימון שלי מראשית הצירים? כאשר אנו מנסים לבחור k, כלל אצבע נפוץ למדי לבחירת k הוא לבחור את הערך הקטן ביותר של k כך שהיחס בין שני הביטויים האלה קטן מ-0.01. במילים אחרות, דרך נפוצה למדי לחשוב על האופן שבו אנו בוחרים k הוא שאנחנו רוצים שריבוע ממוצע שגיאות ההיטל, דהיינו המרחק הממוצע בין x לבין ההיטל שלו חלקי הוריאציה הכוללת של הנתונים, דהיינו מה גודלם של הנתונים, אנחנו רוצים שהיחס הזה יהיה קטן, נניח, מ-0.01. או קטן מ-1%, וזו דרך לחשוב על זה. זו הדרך שבה רוב האנשים חושבים על בחירת k, במקום לבחור ישירות את k, הדרך שבה רוב האנשים חושבים על זה היא דרך המספר הזה, בין אם הוא 0.01 ובין אם הוא מספר אחר. ואם המספר הוא 0.01, אז דרך אחרת לומר את זה בשימוש בשפה של PCA היא ש-99% מהשונות נשמרת. אני לא באמת רוצה, אל תתאמצו מדי להבין את הביטוי מבחינה טכנית, אבל הביטוי "99% מהשונות נשמרת" פשוט אומר שהגודל הזה בצד שמאל הוא קטן מ-0.01. אז אם אתה משתמש ב-PCA ואתה רוצה לספר למישהו כמה מרכיבים עיקריים שימרת, אז זה יהיה נפוץ יותר לומר משהו כמו "בחרתי k כך ש-99% מהשונות נשמרה". וזה דבר שהוא שימושי לדעת, זה אומר שריבוע שגיאת ההיטל הממוצעת חלקי הווריאציה הכוללת הוא לכל היותר 1%. זה נותן לנו תובנה מסוימת, ואילו אם היית אומר למישהו "שימרתי 100 מרכיבים עיקריים" או "k שווה 100 כשהנתונים הם בעלי ממד אלף" זה יהיה קצת יותר קשה לאנשים להפנים את המשמעות של זה. אז המספר הזה, 0.01, הוא מה שאנשים משתמשים בו לעתים קרובות. ערך נפוץ אחר הוא 0.05, או 5%, ואם זה המספר שלך אז אתה תאמר ש-95% מהשונות נשמרת, ויש עוד מספרים אחרים, אולי 90% מהשונות נשמרת, או אולי נמוך יותר, כמו 85%. אז 90% מתאים נניח ל-0.10, או 10%. אז טווחי ערכים של 90%, 95%, 99%, אולי אפילו נמוך עד כדי 85% מהשונות יהיו טווחים אופייניים למדי לערכים. אולי 95% עד 99% הוא הטווח הנפוץ ביותר של ערכים שמשתמשים בהם. עבור ערכות נתונים רבות אתם תהיו מופתעים, על מנת לשמור על 99% מהשונות, אפשר לעתים קרובות להפחית את ממד הנתונים באופן משמעותי ועדיין לשמור על רוב השונות. כי עבור רוב הנתונים האמיתיים תכונות רבות הם מתואמות מאוד, ולכן מתברר שאפשר לדחוס הרבה את הנתונים ועדיין לשמור על 99% או 95% מהשונות. אז איך מיישמים את זה? אז הנה אלגוריתם אחד שבו אפשר להשתמש. אפשר להתחיל, אם רוצים לבחור את הערך של k, אנו עשויים להתחיל עם k שווה 1. ואז אנחנו מריצים את PCA. אנחנו מחשבים, ומצמצמים, את z1, z2, עד zm. ואז מחשבים את קירוב-x1 וכו' עד קירוב-xm ובודקים אם 99% מהשונות נשמרת. אם כן, אז הכל טוב ואנחנו משתמשים ב-k שווה 1. אבל אם לא אז ננסה בנסיון הבא את k שווה 2. ואז שוב נפעיל את כל התהליך הזה ונבדוק האם האי-שיוויון הזה מתקיים. האם המנה קטנה מ-0.01. ואם לא אז נעשה את זה שוב. ננסה את זה עם k שווה 3, ואז ננסה k שווה 4, וכן הלאה עד שנגיע אולי ל-k שווה 17 ונגלה שאכן 99% מהשונות נשמרת, ואז נשתמש ב-k שווה 17, ברור? זוהי אחת הדרכים לבחור את הערך הקטן ביותר של k, כך ש-99% מהשונות נשמרת. אבל כפי שאפשר לדמיין, התהליך הזה נראה ממש לא יעיל לנסות k=1, k=2, ולעשות את כל החישובים האלה. למזלנו, כאשר מיישמים את PCA למעשה, אז השורה הזו של התוכנית, היא בעצם נותנת לנו ערך שגורם לזה להיות הרבה יותר קל גם לחשב את הדברים האלה. באופן ספציפי כאשר קוראים לפונקצית svd כדי לקבל את המטריצות U, S, ו-V, כאשר קוראים ל-svd על מטריצת השונות Σ, אנחנו מקבלים כפלט גם את המטריצה S, ו-S היא מטריצה ריבועית n על n, שהיא למעשה אלכסונית. זאת אומרת שערכי האלכסון S11, S22, S33 וכן הלאה עד Snn, הם האיברים היחידים במטריצה הזו שאינם אפס, וכל השאר שאיננו על האלכסונים הוא אפס. בסדר? האפסים הגדולים האלה שאני רושם כאן, משמעותם היא שכל מה שאיננו באלכסון של המטריצה, כל הערכים האלה הם אפסים. וניתן להוכיח, למרות שאני לא אוכיח את זה כאן, מתברר שלערך נתון של k, ניתן לחשב את הערך של השבר הזה שהקפתי כאן הרבה יותר בפשטות. והסכום הזה הוא אחת מינוס הסכום מ-i שווה 1 עד k של Sii חלקי הסכום של i שווה 1 עד n של Sii. אז רק כדי לומר את זה במילים, או כדי להראות את ההסבר של זה בעוד צורה, אם k שווה נניח 3, מה שנעשה כדי לחשב את המונה הוא הסכום מ-1 עד 3 של Sii, פשוט נחשב את הסכום של שלושת האיברים הראשונים. אז זה המונה. ולגבי המכנה, הוא פשוט הסכום של כל הערכים האלה באלכסון. ואחת מינוס היחס בין שני המספרים האלה, זה הגודל הזה כאן שהקפתי בכחול. אז מה שאנחנו יכולים לעשות הוא פשוט לבדוק אם זה קטן או שווה ל-0.01. או באופן שקול, אנו יכולים לבדוק אם הסכום מ-i שווה 1 עד k של Sii, חלקי הסכום מ-i שווה 1 עד n של Sii, אם זה גדול או שווה ל-0.99, אם אנחנו רוצים להיות בטוחים ש-99% מהשונות נשמרת. אז מה שאפשר לעשות הוא להגדיל את k לאט לאט, להגדיר k שווה 1, אחר כך k שווה 2, להגדיר k שווה שלוש וכן הלאה, ולבדוק רק את הגודל הזה כדי לראות מהו הערך הקטן ביותר של k שמבטיח ש-99% מהשונות נשמרת. וכשעושים את זה כך, אז צריך לקרוא לפונקציה svd רק פעם אחת. וזה מחזיר לנו את המטריצה S, ולאחר שקיבלנו את המטריצה S, אפשר פשוט להמשיך ולעשות את החישוב הזה על ידי הגדלת הערך של k במונה ולכן לא צריך לקרוא ל-svd שוב ושוב כדי לבדוק את הערכים השונים של k. אז השיטה הזו היא הרבה יותר יעילה, ומאפשרת לנו לבחור את הערך של k ללא צורך להפעיל את PCA שוב ושוב. צריך פשוט להפעיל את svd פעם אחת, ולקבל את המטריצה האלכסונית הזו, עם כל המספרים האלה S₁₁, S₂₂ עד Snn, ואז אפשר פשוט לשנות את k בביטוי הזה במונה כדי למצוא את הערך הקטן ביותר של k, כך ש-99% מהשונות נשמרת. אז לסיכום, הדרך שבה אני משתמש לעתים קרובות, הדרך שבה אני לעיתים קרובות בוחר את k כאשר אני משתמש ב-PCA לדחיסה הוא לקרוא ל-svd פעם אחת עם מטריצת השונות, ואז להשתמש בנוסחה הזו ולבחור את הערך הקטן ביותר של k שמקיים את האי שיוויון הזה. ודרך אגב, גם אם תרצה לבחור ערך אחר של k, גם אם תחליט לבחור את הערך של k ידנית, לדוגמא אם יש לך נתונים בעלי אלפי ממדים ואתה רוצה לבחור דווקא k שווה מאה, אז אם תרצה להסביר לאחרים מה עשית, דרך טובה להסביר להם מה עושה היישום של PCA, היא למעשה לקחת את הנוסחה הזו ולחשב את היחס הזה, שאומר מה אחוז השונות הנשמרת. ואם תשתמש במספר הזה כשתסביר לאחרים, אחרים שמכירים את PCA, אז הם יוכלו להשתמש במספר הזה כדי לקבל הבנה טובה לגבי מידת הקירוב בה מייצגים מאה המימדים שלך את מערך הנתונים המקורי שלך, כי אמרת להם כמה אחוז מהשונות נשמרת. זו באמת מידה של ריבוע שגיאת ההיטל, כי היחס של 0.01, פשוט נותן לבני שיחך תחושה אינטואיטיבית טובה אם היישום שלך של PCA מוצא קירוב טוב של ערכת הנתונים המקוריים שלך. אז אני מקווה שזה נותן לכם תהליך יעיל בבחירת המספר k, לבחור את המימד שאליו לצמצם את הנתונים, ואם תשתמשו ב-PCA על ערכות נתונים בעלות ממד גבוה מאוד, לדוגמא בעלות מימד 1000, לעתים מאוד קרובות, פשוט מכיוון שבתוך מגוון הנתונים סביר שתהיינה תכונות מתואמות מאוד, זו פשוט תכונה של רוב ערכות הנתונים שתראו, אתם תגלו לעתים קרובות ש-PCA יוכל לשמר תשעים ותשעה אחוזים מהשונות או אולי בין תשעים וחמישה לתשעים ותשעה, אחוז מאוד גבוה של השונות, גם בעת דחיסת הנתונים בפקטור גדול מאוד.