בכמה מהסרטונים הקודמים, דיברתי על PCA כאלגוריתם דחיסה שבו אפשר לקחת נתונים עם 1,000 ממדים ולדחוס אותם לוקטור של 100 ממדים או תכונות. או לקחת נתונים תלת מימדיים ולדחוס אותם לייצוג דו מימדי. אז אם זה אלגוריתם דחיסה, צריכה גם להיות דרך לחזור מן הייצוג הדחוס בחזרה לקירוב של הנתונים במימדים המקוריים שלהם. זאת אומרת, אם נתון לנו zi, שעשוי להיות 100 מימדי, איך אנחנו חוזרים לייצוג המקורי xi שהיו לו אולי 1000 מימדים. בסרטון הזה, אני רוצה לתאר כיצד לעשות זאת. באלגוריתם PCA, אנחנו עשויים לקבל דוגמה כזאת, אז אולי זו דוגמא x1, ואולי זו דוגמא x2. ואנחנו לוקחים את הדוגמאות האלה, ועושים היטלים אותם על המישור החד ממדי - הקו - הזה. ועכשיו יש לנו מספר ממשי, נניח z1, לציין את המיקום של הנקודות האלה לאחר שהוטלו על הקו הזה. אז, בהינתן הנקודה z1, איך אנחנו יכולים לחזור ממנה למרחב הדו-ממדי המקורי? ספציפית, בהינתן נקודה z שנמצאת ב-R, האם נוכל למפות אותה בחזרה לאיזו שהיא נקודה x ב-R² שבה היה הערך המקורי של הנתונים? נזכור ש-z שווה Uᵀx מצומצמת, אז אם רוצים ללכת בכיוון ההפוך, המשוואה תהיה קירוב-x שווה ל-U מצומצמת כפול z. ושוב, רק כדי לבדוק את הממדים, U מצומצמת היא מטריצה של n על k, ואילו z הוא וקטור של k על 1. אז כשכפולים אותם מקבלים n על 1, אז קירוב-x יהיה וקטור בעל ממד n. והכוונה של PCA, שאם שגיאת ההיטל הריבועית אינה גדולה מדי, היא שקירוב-x יהיה קרוב למה שהיה הערך המקורי של x, שבו השתמשנו כדי לחשב את z מלכתחילה. בואו נראה תמונה של איך זה נראה, זה נראה כך. מה שמקבלים כתוצאה של התהליך הזה הן נקודות שנמצאות על ההיטל של הדבר הזה, על הקו הירוק. אז ניקח את הדוגמה הקודמת שלנו, אם התחלנו עם הערך הזה של x1, וקיבלנו את הערך הזה של z1, אם נציב את z1 בתוך הנוסחה הזו כדי לקבל את קירוב-x1, אז הנקודה כאן תהיה קירוב-x1, שהיא נקודה ב-R2. ואם נעשה את אותו תהליך על הנקודה הזו, נקבל קירוב-x2. ואלה קירובים סבירים למדי לנתונים המקוריים. אז ככה חוזרים מהייצוג במימד הנמוך שלנו z אל ייצוג לא דחוס של הנתונים. כדי לקבל בחזרה קירוב לנתונים המקוריים x. ואנחנו קוראים לתהליך הזה "שחזור" של הנתונים המקוריים כי אנחנו מנסים לשחזר את הערך המקורי של x מהייצוג הדחוס. אז כשמקבלים ערכת נתונים לא מתויגים, עכשיו אתם צריכים לדעת איך ליישם PCA כדי לקחת תכונות x עם ממד גבוה ולמפות אותם לייצוג עם מימד נמוך z. ומהוידאו הזה בתקווה למדתם ועכשיו אתם גם יודעים איך לקחת את הייצוג הנמוך z ולמפות אותו בחזרה לקירוב של הנתונים בעלי המימד הגבוה המקורי. עכשיו כשאתם יודעים איך ליישם את PCA, מה שאני רוצה לעשות הוא לדבר על הפרטים של איך בעצם להשתמש ב-PCA בצורה טובה. ובסרטון הבא ספציפית, אני רוצה לדבר על בחירת k, בחירת הממד של וקטור הייצוג המופחת z.