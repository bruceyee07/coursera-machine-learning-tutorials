בסרטון קודם אמרתי ש-PCA יכול לשמש לפעמים כדי להאיץ את זמן הריצה של אלגוריתם למידה. בסרטון הזה, אני רוצה להסביר איך לעשות את זה למעשה, וגם לנסות ולתת כמה עצות לגבי איך ליישם PCA. הנה איך אפשר להשתמש ב-PCA כדי לזרז אלגוריתם למידה, והשימוש של האצת הלמידה של אלגוריתם למידה בפיקוח הוא למעשה השימוש הנפוץ ביותר שאני אישית עושה ב-PCA. נניח שיש לכם בעיית למידה מבוקרת, שימו לב שזו בעיית למידה מבוקרת עם קלטים x ותוויות y, ונניח שהדוגמאות xi שלך הן בעלות ממד גבוה מאוד. נניח שהדוגמאות xi הן וקטורי תכונות בעלות ממד 10,000. דוגמה אחת לכך תהיה בעייה בראייה ממוחשבת, שבה יש לך תמונות של 100x100 פיקסלים, אז 100x100 זה 10,000 פיקסלים, ולכן אם xi הם וקטורי תכונה המכילים את עוצמות הערכים של 10,000 הפיקסלים, אז יש לך וקטורי תכונות עם ממד 10,000. אז עם וקטורים בעלי ממד גבוה כזה, הפעלת אלגוריתם למידה יכולה להיות איטית, נכון? פשוט, אם אתה מאכיל וקטורי תכונות עם ממד 10,000 לרגרסיה לוגיסטית, או לרשת עצבית, או מכונת תמך וקטורי או מה שלא יהיה, רק בגלל שיש כל כך הרבה נתונים, 10,000 מספרים, זה יכול לגרום לאלגוריתם הלמידה שלך לרוץ לאט יותר. למרבה המזל עם PCA נוכל לצמצם את הממד של הנתונים האלה וכך לגרום לאלגוריתמים שלנו לפעול ביעילות רבה יותר. והנה השיטה לעשות את זה. קודם כל אנחנו נבדוק את סדרת האימון המתויגת שלנו ונחלץ ממנה את הקלטים, אנחנו פשוט נחלץ את הx-ים ובאופן זמני נתעלם מה-y. אז עכשיו תהיה לנו סדרת אימון בלתי מתויגת x1 עד xm שהיא אולי סדרת נתונים עשרת-אלפים ממדית, דוגמאות בעלות עשרת אלפים ממדים. פשוט נחלץ את וקטורי הקלט x1 עד xm. אחר כך אנחנו נריץ את PCA ונקבל ייצוג של הנתונים עם מימד מופחת, אז במקום וקטורי תכונות של 10,000 ממדים, יהיו לנו עכשיו וקטורים של אולי 1,000 ממדים. אז זה חיסכון של פי 10. זה נותן לנו, אם תרצו, סדרת אימון חדשה. אז בעוד בעבר היתה לנו דוגמה x1, y1, קלט האימון הראשון שלי, כעת הוא מיוצג על ידי z1. וכך יש לנו דוגמה אימון חדשה, שבה z1 מחובר ל-y1. וכן z2 ל-y2, וכן הלאה, עד לזוג (zm, ym). דוגמאות האימון שלנו מיוצגות עכשיו על ידי ייצוג עם מימד הרבה יותר נמוך z1, z2, עד zm. ועכשיו אפשר לקחת את סדרת האימון המצומצמת ולהכניס אותה לאלגוריתם למידה כגון רשת עצבית, או אולי רגרסיה לוגיסטית, שיכולה ללמוד את פונקצית ההשערה ()h, שמקבלת את הקלט z שהוא בעל מימד נמוך וממנו מנסה לעשות תחזיות. אז אם אני משתמש ברגרסיה לוגיסטית למשל, הייתי מאמן השערה שהפלט שלה 1 חלקי 1 ועוד e בחזקת θᵀ- כפול z, שלוקחת כקלט אחד הוקטורים z, ומנסה לעשות חיזוי. ואם אני מקבל דוגמה חדשה, לדוגמה x, אני לוקח את דוגמת הבדיקה x, ממפה אותה באמצעות אותו מיפוי שבנינו ב-PCA כדי לקבל את ה-z המתאים. ואז ה-z הזה מוזן כקלט להשערה הזו, וההשערה הזאת יוצרת חיזוי על הקלט x. עוד הערה אחת, PCA מגדיר מיפוי מ-x ל-z והמיפוי הזה מ-x ל-z צריך להיות מוגדר על ידי הפעלת PCA רק על ערכת האימון. ובפרט, המיפוי הזה ש- PCA לומד, הוא מחשב את מערכת הפרמטרים. הוא מחשב את המישקול והנירמול של התכונות. והוא מחשב גם את המטריצה U המצומצמת. אבל כל הדברים האלה, U מצומצמת, הם פרמטרים שנלמדים על ידי PCA, ואנחנו צריכים להתאים את הפרמטרים שלנו רק לערכת האימון, ולא לערכת האימות הצולב או לערכת המבחן, הדברים האלה, U מצומצמת וכן הלאה, צריך לחשב אותם על ידי הפעלת PCA רק על סדרת האימון. ולאחר שמצאנו U מצומצמת, או לאחר שמצאנו את הפרמטרים עבור מישקול התכונות או נירמול של כל תכונה, מישקול שבו אנחנו מביאים את כל התכונות לאותו קנה-מידה. לאחר שמצאתי את כל הפרמטרים האלה בערכת האימון, אפשר להחיל את אותו מיפוי על דוגמאות אחרות, לדוגמא אלה במערכי האימות הצולב או המבחן, ברור? אז לסיכום, כאשר מפעילים את PCA, צריך להפעיל אותו רק על החלק של סדרת האימון של הנתונים ולא על סדרות האימות הצולב והמבחן. וכך תקבלו את המיפוי מ-x ל-z, ואז אפשר להחיל את המיפוי על סדרת האימות הצולב ועל סדרת המבחן. דרך אגב, בדוגמה הזו דיברתי על צמצום הנתונים מעשרת אלפים ממדים לאלף מימדים, זה לא באמת בלתי מציאותי. ישנן בעיות רבות שבהם אנחנו מצמצמים את ממדי הנתונים פי 5 או אולי פי 10, ואנחנו עדיין שומרים על רוב השונות ואנחנו יכולים לעשות את זה עם רק ירידה קלה מאוד באיכות הסיווג במונחים של דיוק הסיווג, נניח, זה בקושי משפיע על דיוק הסיווג של אלגוריתם הלמידה. ובזכות העבודה עם נתונים בעלי ממד נמוך יותר, אלגוריתם הלמידה שלנו יכול הרבה פעמים לרוץ הרבה יותר מהר. לסיכום, עד כה דיברנו על היישומים הבאים של PCA. הראשון הוא יישום הדחיסה, שבו אנו עשויים להריץ אותו כדי להפחית את צריכת הזיכרון או את שטח הדיסק הדרוש לאחסון הנתונים, וכרגע דיברנו על שימוש ב-PCA כדי לזרז את אלגוריתם הלמידה. למטרות האלה, על מנת לבחור את k, אנחנו בדרך כלל נעשה את זה בהתאם לחישוב אחוז השונות הנשמרת, אז כשעושים את זה בשביל המהירות, בדרך כלל תישמר 99% מהשונות. וזו תהיה בחירה אופיינית מאוד ל-k. כך בוחרים את k עבור יישומים של דחיסה. אבל כשאנחנו עושים את זה בשביל הדמייה או ויזואליזציה, בדרך כלל אנחנו יודעים לעשות גרפים רק של נתונים דו מימדיים או תלת ממדיים, ולכן כשהיישום הוא הדמיה, אנחנו בדרך כלל בוחרים k שווה 2 או k שווה 3, כי אנחנו יודעים לשרטט רק ערכות נתונים של 2D ו-3D. זה מסכם את היישומים העיקריים של PCA, כמו גם כיצד לבחור את הערך של k עבור היישומים השונים האלה. אני רוצה להזכיר שלפעמים יש המשתמשים ב-PCA למטרה שהוא איננו מתאים לה, ולפעמים שומעים שמישהו עושה את זה, בתקווה לא לעתים קרובות מדי. אני רק רוצה להזכיר את זה כך שתדעו שלא לעשות את זה. יש שימוש רע אחד ב-PCA, שהוא לנסות להשתמש בו כדי למנוע התאמת-יתר. הנה ההיגיון של זה. זו לא דרך טובה להשתמש ב-PCA, אבל הנה ההגיון מאחורי זה, שהוא, אם יש לנו xi, אז אולי יש לנו n תכונות, אבל אם נדחס את הנתונים ונשתמש ב-zi במקום זה, זה יקטין את מספר התכונות ל-k, שיכול להיות הרבה יותר קטן. אז אם יש לנו מספר הרבה יותר קטן של תכונות, לדוגמא אם k הוא 1,000 ו-n הוא 10,000, אז אם יש לנו רק 1,000 תכונות, אולי תהיה פחות נטייה להתאמה-יתר מאשר לו השתמשנו ב-10,000 תכונות. אז יש החושבים על PCA כדרך למנוע התאמת-יתר. אבל אני מדגיש שזה לא יישום נכון של PCA ואני ממליץ שלא לעשות את זה. ולא כי השיטה הזו לא פועלת היטב. אם אתם רוצים להשתמש בשיטה הזו כדי להפחית את ממדי הנתונים כדי לנסות למנוע התאמת-יתר, יכול להיות שזה יעבוד בסדר. אבל זו פשוט לא דרך טובה כדי לטפל בהתאמת-יתר, ואם אתם חוששים מהתאמת-יתר, יש דרך טובה יותר לטפל בזה, דהיינו להשתמש בהסדרה במקום להשתמש ב-PCA ולהקטין את מימד הנתונים . והסיבה היא, אם תחשבו על איך PCA עובד, הוא איננו משתמש בתוויות y. הוא מתייחס רק לקלטים xi, ומשתמש בזה כדי למצוא קירוב עם מימד נמוך יותר לנתונים. אז PCA מתעלם מחלק מהמידע. הוא מתעלם או מצמצם את מימדי הנתונים שלך בלי לדעת מה הערכים של y, אז שימוש ב-PCA בדרך הזו הוא כנראה בסדר אם נניח 99 אחוז מהשונות נשמרת, אם אנחנו שומרים את רוב השונות, אבל עדיין אולי זורקים קצת מידע בעל ערך. אבל מתברר שאם נשמור 99% מהשונות או 95% מהשונות או כמה שזה לא יהיה, מתברר שההסדרה בדרך כלל תיתן לנו תוצאות טובות לפחות כמו PCA כשיטה למניעת התאמת-יתר, וההסדרה לעתים קרובות פשוט תעבוד טוב יותר, כי כאשר אנחנו מיישמים רגרסיה ליניארית או רגרסיה לוגיסטית או שיטה אחרת שבה משתמשים בהסדרה, אז בעיית המזעור הזו בעצם יודעת מה הערכים של y, ולכן פחות סביר שתזרוק מידע בעל ערך, בעוד PCA איננו משתמש בתוויות, ולכן סביר יותר שכן יזרוק מידע בעל ערך. אז לסיכום, שימוש ב-PCA הוא טוב אם המוטיבציה העיקרית שלך היא להאיץ את אלגוריתם הלמידה, אבל שימוש ב-PCA כדי למנוע התאמת-יתר איננו שימוש טוב ב-PCA, ושימוש בהסדרה במקום זה הוא הדבר הנכון. שמומלץ על ידי בעלי המקצוע. לבסוף, יש עוד שימוש לרעה של PCA. אני חייב לומר ש-PCA הוא אלגוריתם שימושי מאוד, ואני משתמש בו לעיתים קרובות למטרות דחיסה או הדמיה. אבל מה שאני רואה לפעמים הוא שלפעמים משתמשים ב-PCA בצורה שלא צריכה לקרות. דבר נפוץ למדי שאני רואה, והוא שבזמן עיצוב של מערכת למידה חישובית, המתכננים כותבים לעצמם תכנית כזו: בואו נתכנן מערכת למידה. נשיג נתוני אימון, ואז נפעיל את PCA, ואז נריץ רגרסיה לוגיסטית ונבדוק את התוצאות על נתוני הבדיקה. זאת אומרת שהרבה פעמים מיד בתחילת הפרויקט כשמתכננים את הפרויקט מייד מעלים את האפשרות לעשות את ארבעת השלבים האלה כשאחד מהם הוא PCA. לפני שכותבים תוכנית פרויקט שמשלבת את PCA, כדאי לשאול שאלה טובה מאוד והיא מה יקרה אם פשוט נעשה את כל זה בלי להשתמש ב-PCA. והרבה פעמים מתכנני הפרויקט לא חושבים על הרעיון הזה ומייד בונים תוכנית פרויקט מסובכת הכוללת יישום של PCA וכו'. אבל מה שאני מייעץ למתכננים בדרך כלל לעשות הוא, לפני שאתם מיישמים PCA, הייתי מציע תחילה לעשות מה שאתם באמת רוצים לעשות, לקחת את כל מה שאתם רוצים לעשות ובהתחלה לנסות לעשות את זה עם נתוני הגלם המקוריים שלכם xi, ורק אם זה לא מצליח לעשות מה שניסיתם, רק אז להכניס לתהליך את PCA ולהשתמש ב-zi. אז לפני השימוש ב-PCA, במקום לצמצם את מספר המאפיינים של הנתונים, הייתי שוקל פשוט לוותר על הצעד של PCA, ובמקום זאת לאמן את אלגוריתם הלמידה על הנתונים המקוריים. בוא נשתמש רק בקלט הראשוני xi, ואני ממליץ במקום להכניס את הצעד של PCA לתוך האלגוריתם, פשוט תנסו לעשות את אותו תהליך שרציתם לעשות אבל עם ה-xi הראשוניים. ורק אם יש לכם סיבה להאמין שזה לא יעבוד, לדוגמא אם ברור שאלגוריתם הלמידה ירוץ לאט מדי ויסתיים באיטיות רבה מדי, או אם ברור שדרישות הזיכרון או שטח הדיסק גדולות מדי, מה שגורם לכם לרצות לדחוס את הייצוג שלכם, אבל רק אם השימוש ב-xi לא עובד, רק אם יש לכם ראיות טובות או סיבה טובה להאמין שהשימוש ב-xi לא יעבוד, רק אז ליישם PCA ולשקול שימוש בייצוג הדחוס. כי לפי מה שאני רואה, לפעמים מתחילים עם תוכנית פרויקט שמשלבת PCA, ולפעמים מה שהם עושים יעבוד בסדר גמור אפילו ללא שימוש ב-PCA. אז פשוט תשקלו גם את זה כחלופה, לפני שתבזבזו הרבה זמן בהרצה של PCA, בהחלטה על k וכן הלאה. אז זהו זה בקשר ל-PCA. למרות ההערות והאזהרות האחרונות האלה, PCA הוא אלגוריתם שימושי להפליא כאשר משתמשים בו למטרות המתאימות, ואני השתמשתי ומשתמש ב-PCA לעתים קרובות למדי, אני משתמש בו בעיקר כדי להאיץ את זמן הריצה של אלגוריתמי הלמידה שלי. אבל אני חושב שהשימוש ב-PCA צריך להיות כדי לדחוס נתונים, כדי להפחית את דרישות הזיכרון או שטח הדיסק, או להשתמש בו כדי להציג נתונים. PCA הוא אחד האלגוריתמים הנפוצים ביותר ואחד מאלגוריתמי הלמידה הלא-מושגחת החזקים ביותר. ובעזרת מה שלמדתם בקטעי הוידאו האלה, אני מקווה שתצליחו ליישם PCA וגם להשתמש בו לכל המטרות הללו.