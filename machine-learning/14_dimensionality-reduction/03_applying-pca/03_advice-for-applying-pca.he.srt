1
00:00:00,090 --> 00:00:01,450
בסרטון קודם

2
00:00:01,610 --> 00:00:02,710
אמרתי ש-PCA יכול לשמש

3
00:00:02,840 --> 00:00:05,410
לפעמים כדי להאיץ את זמן הריצה של אלגוריתם למידה.

4
00:00:07,070 --> 00:00:08,140
בסרטון הזה, אני רוצה

5
00:00:08,370 --> 00:00:09,520
להסביר איך

6
00:00:09,820 --> 00:00:11,270
לעשות את זה למעשה, וגם

7
00:00:11,460 --> 00:00:12,900
לנסות ולתת

8
00:00:12,990 --> 00:00:14,550
כמה עצות לגבי איך ליישם PCA.

9
00:00:17,110 --> 00:00:19,630
הנה איך אפשר להשתמש ב-PCA כדי לזרז אלגוריתם למידה,

10
00:00:20,270 --> 00:00:21,940
והשימוש של האצת הלמידה

11
00:00:22,270 --> 00:00:23,630
של אלגוריתם למידה בפיקוח הוא למעשה

12
00:00:23,870 --> 00:00:25,870
השימוש הנפוץ ביותר

13
00:00:26,530 --> 00:00:27,720
שאני אישית עושה ב-PCA.

14
00:00:28,710 --> 00:00:29,640
נניח שיש לכם בעיית

15
00:00:30,300 --> 00:00:31,660
למידה מבוקרת, שימו לב

16
00:00:31,810 --> 00:00:33,380
שזו בעיית למידה מבוקרת עם קלטים

17
00:00:33,690 --> 00:00:35,510
x ותוויות y,

18
00:00:35,810 --> 00:00:37,330
ונניח שהדוגמאות xi

19
00:00:37,830 --> 00:00:39,140
שלך הן בעלות ממד גבוה מאוד.

20
00:00:39,840 --> 00:00:41,670
נניח שהדוגמאות xi

21
00:00:41,800 --> 00:00:44,000
הן וקטורי תכונות בעלות ממד 10,000.

22
00:00:45,510 --> 00:00:46,550
דוגמה אחת לכך תהיה

23
00:00:46,700 --> 00:00:48,130
בעייה בראייה ממוחשבת,

24
00:00:48,540 --> 00:00:50,390
שבה יש לך

25
00:00:50,650 --> 00:00:52,410
תמונות של 100x100 פיקסלים,

26
00:00:52,780 --> 00:00:55,550
אז 100x100 זה 10,000

27
00:00:55,850 --> 00:00:57,520
פיקסלים, ולכן אם xi

28
00:00:57,780 --> 00:00:59,240
הם וקטורי תכונה

29
00:00:59,760 --> 00:01:01,670
המכילים את עוצמות הערכים

30
00:01:02,470 --> 00:01:03,580
של 10,000 הפיקסלים,

31
00:01:04,410 --> 00:01:05,580
אז יש לך וקטורי תכונות עם ממד 10,000.

32
00:01:06,880 --> 00:01:08,530
אז עם וקטורים בעלי

33
00:01:09,300 --> 00:01:10,890
ממד גבוה כזה, הפעלת

34
00:01:11,320 --> 00:01:12,860
אלגוריתם למידה יכולה להיות איטית, נכון?

35
00:01:13,030 --> 00:01:14,300
פשוט, אם אתה מאכיל וקטורי תכונות

36
00:01:14,790 --> 00:01:16,980
עם ממד 10,000 לרגרסיה לוגיסטית,

37
00:01:17,570 --> 00:01:19,780
או לרשת עצבית, או מכונת תמך וקטורי או מה שלא יהיה,

38
00:01:20,450 --> 00:01:22,000
רק בגלל שיש כל כך הרבה נתונים,

39
00:01:22,200 --> 00:01:23,060
10,000 מספרים,

40
00:01:24,130 --> 00:01:25,970
זה יכול לגרום לאלגוריתם הלמידה שלך לרוץ לאט יותר.

41
00:01:27,170 --> 00:01:28,530
למרבה המזל עם PCA

42
00:01:28,680 --> 00:01:29,810
נוכל לצמצם את הממד

43
00:01:30,060 --> 00:01:31,050
של הנתונים האלה וכך

44
00:01:31,180 --> 00:01:32,410
לגרום לאלגוריתמים שלנו

45
00:01:32,920 --> 00:01:34,440
לפעול ביעילות רבה יותר. והנה השיטה

46
00:01:34,590 --> 00:01:35,780
לעשות את זה. קודם כל

47
00:01:35,980 --> 00:01:37,030
אנחנו נבדוק את סדרת האימון

48
00:01:37,400 --> 00:01:39,520
המתויגת שלנו ונחלץ ממנה

49
00:01:39,800 --> 00:01:41,830
את הקלטים, אנחנו פשוט נחלץ את הx-ים

50
00:01:42,730 --> 00:01:45,130
ובאופן זמני נתעלם מה-y.

51
00:01:45,860 --> 00:01:46,750
אז עכשיו תהיה לנו

52
00:01:47,090 --> 00:01:49,150
סדרת אימון בלתי מתויגת x1

53
00:01:49,400 --> 00:01:51,000
עד xm שהיא אולי

54
00:01:51,240 --> 00:01:53,600
סדרת נתונים עשרת-אלפים ממדית,

55
00:01:53,940 --> 00:01:55,800
דוגמאות בעלות עשרת אלפים ממדים.

56
00:01:55,870 --> 00:01:57,230
פשוט נחלץ את וקטורי הקלט

57
00:01:58,370 --> 00:01:58,930
x1 עד xm.

58
00:02:00,650 --> 00:02:01,810
אחר כך אנחנו נריץ את PCA

59
00:02:02,700 --> 00:02:03,740
ונקבל

60
00:02:03,980 --> 00:02:06,100
ייצוג של הנתונים עם מימד מופחת,

61
00:02:06,410 --> 00:02:08,010
אז במקום

62
00:02:08,260 --> 00:02:09,540
וקטורי תכונות של 10,000 ממדים,

63
00:02:09,780 --> 00:02:11,880
יהיו לנו עכשיו וקטורים של אולי 1,000 ממדים.

64
00:02:12,330 --> 00:02:13,500
אז זה חיסכון של פי 10.

65
00:02:15,110 --> 00:02:17,260
זה נותן לנו, אם תרצו, סדרת אימון חדשה.

66
00:02:17,910 --> 00:02:19,430
אז בעוד בעבר

67
00:02:19,620 --> 00:02:21,180
היתה לנו דוגמה x1, y1,

68
00:02:21,490 --> 00:02:24,340
קלט האימון הראשון שלי, כעת הוא מיוצג על ידי z1.

69
00:02:24,580 --> 00:02:25,800
וכך יש לנו

70
00:02:26,050 --> 00:02:27,010
דוגמה אימון חדשה,

71
00:02:28,210 --> 00:02:29,240
שבה z1 מחובר ל-y1.

72
00:02:30,700 --> 00:02:33,170
וכן z2 ל-y2, וכן הלאה, עד לזוג (zm, ym).

73
00:02:33,770 --> 00:02:35,300
דוגמאות האימון שלנו

74
00:02:35,460 --> 00:02:36,980
מיוצגות עכשיו על ידי ייצוג

75
00:02:37,480 --> 00:02:41,040
עם מימד הרבה יותר נמוך z1, z2, עד zm.

76
00:02:41,310 --> 00:02:42,340
ועכשיו אפשר לקחת

77
00:02:43,650 --> 00:02:45,060
את סדרת האימון המצומצמת

78
00:02:45,240 --> 00:02:46,540
ולהכניס אותה לאלגוריתם למידה

79
00:02:46,640 --> 00:02:47,900
כגון רשת עצבית, או אולי

80
00:02:48,280 --> 00:02:49,450
רגרסיה לוגיסטית,

81
00:02:49,750 --> 00:02:51,990
שיכולה ללמוד את פונקצית ההשערה ()h,

82
00:02:52,230 --> 00:02:53,830
שמקבלת את הקלט z שהוא בעל מימד נמוך

83
00:02:54,330 --> 00:02:56,230
וממנו מנסה לעשות תחזיות.

84
00:02:57,890 --> 00:02:59,030
אז אם אני משתמש

85
00:02:59,460 --> 00:03:00,880
ברגרסיה לוגיסטית למשל, הייתי

86
00:03:01,060 --> 00:03:02,760
מאמן השערה שהפלט שלה

87
00:03:03,080 --> 00:03:04,020
1 חלקי 1 ועוד e

88
00:03:04,180 --> 00:03:06,020
בחזקת θᵀ-

89
00:03:07,620 --> 00:03:10,150
כפול z,

90
00:03:10,610 --> 00:03:11,530
שלוקחת כקלט

91
00:03:11,960 --> 00:03:13,660
אחד הוקטורים z, ומנסה לעשות חיזוי.

92
00:03:15,260 --> 00:03:16,310
ואם אני מקבל

93
00:03:16,630 --> 00:03:17,800
דוגמה חדשה,

94
00:03:17,920 --> 00:03:20,060
לדוגמה x,

95
00:03:20,220 --> 00:03:21,340
אני

96
00:03:22,130 --> 00:03:23,730
לוקח את דוגמת הבדיקה x,

97
00:03:24,960 --> 00:03:26,590
ממפה אותה באמצעות אותו מיפוי

98
00:03:26,990 --> 00:03:27,860
שבנינו ב-PCA

99
00:03:28,220 --> 00:03:29,610
כדי לקבל את ה-z המתאים.

100
00:03:30,390 --> 00:03:31,280
ואז ה-z הזה

101
00:03:31,950 --> 00:03:33,740
מוזן כקלט להשערה הזו,

102
00:03:33,910 --> 00:03:35,450
וההשערה הזאת יוצרת

103
00:03:35,750 --> 00:03:36,740
חיזוי על הקלט x.

104
00:03:38,110 --> 00:03:40,090
עוד הערה אחת, PCA

105
00:03:40,510 --> 00:03:42,350
מגדיר מיפוי

106
00:03:42,710 --> 00:03:45,090
מ-x ל-z

107
00:03:45,960 --> 00:03:46,970
והמיפוי הזה מ-x

108
00:03:47,050 --> 00:03:48,280
ל-z צריך להיות מוגדר על ידי

109
00:03:48,580 --> 00:03:50,840
הפעלת PCA רק על ערכת האימון.

110
00:03:51,650 --> 00:03:53,310
ובפרט, המיפוי הזה

111
00:03:53,530 --> 00:03:54,770
ש- PCA לומד,

112
00:03:54,950 --> 00:03:57,650
הוא מחשב את מערכת הפרמטרים.

113
00:03:58,210 --> 00:04:00,500
הוא מחשב את המישקול והנירמול של התכונות.

114
00:04:01,240 --> 00:04:04,040
והוא מחשב גם את המטריצה U המצומצמת.

115
00:04:04,680 --> 00:04:05,510
אבל כל הדברים האלה,

116
00:04:05,670 --> 00:04:06,980
U מצומצמת,

117
00:04:07,120 --> 00:04:08,420
הם פרמטרים שנלמדים

118
00:04:08,670 --> 00:04:09,950
על ידי PCA, ואנחנו צריכים

119
00:04:10,150 --> 00:04:12,270
להתאים את הפרמטרים שלנו רק

120
00:04:12,480 --> 00:04:13,990
לערכת האימון, ולא

121
00:04:14,040 --> 00:04:16,250
לערכת האימות הצולב או לערכת המבחן,

122
00:04:16,370 --> 00:04:17,560
הדברים האלה, U מצומצמת

123
00:04:18,180 --> 00:04:19,460
וכן הלאה, צריך

124
00:04:19,820 --> 00:04:22,430
לחשב אותם על ידי הפעלת PCA רק על סדרת האימון.

125
00:04:23,300 --> 00:04:26,930
ולאחר שמצאנו U מצומצמת, או לאחר שמצאנו את הפרמטרים עבור

126
00:04:27,350 --> 00:04:28,620
מישקול התכונות

127
00:04:29,320 --> 00:04:31,790
או נירמול של כל תכונה,

128
00:04:32,180 --> 00:04:34,500
מישקול שבו אנחנו מביאים את כל התכונות לאותו קנה-מידה.

129
00:04:34,760 --> 00:04:36,010
לאחר שמצאתי את כל הפרמטרים האלה

130
00:04:36,570 --> 00:04:38,010
בערכת האימון, אפשר

131
00:04:38,220 --> 00:04:41,560
להחיל את אותו מיפוי על דוגמאות אחרות,

132
00:04:41,820 --> 00:04:45,020
לדוגמא אלה במערכי האימות הצולב

133
00:04:45,180 --> 00:04:46,680
או המבחן, ברור?

134
00:04:47,150 --> 00:04:48,340
אז לסיכום, כאשר

135
00:04:48,450 --> 00:04:49,790
מפעילים את PCA, צריך

136
00:04:49,900 --> 00:04:51,070
להפעיל אותו רק על

137
00:04:51,220 --> 00:04:52,450
החלק של סדרת האימון

138
00:04:52,490 --> 00:04:55,880
של הנתונים ולא על סדרות האימות הצולב והמבחן.

139
00:04:56,410 --> 00:04:57,620
וכך תקבלו את המיפוי

140
00:04:57,870 --> 00:04:58,770
מ-x ל-z, ואז אפשר

141
00:04:58,950 --> 00:05:00,320
להחיל את המיפוי

142
00:05:00,560 --> 00:05:02,240
על סדרת האימות הצולב

143
00:05:02,290 --> 00:05:03,370
ועל סדרת המבחן. דרך אגב,

144
00:05:03,450 --> 00:05:04,660
בדוגמה הזו דיברתי

145
00:05:05,000 --> 00:05:06,660
על צמצום הנתונים

146
00:05:06,950 --> 00:05:08,510
מעשרת אלפים ממדים

147
00:05:08,740 --> 00:05:10,350
לאלף מימדים, זה לא באמת

148
00:05:10,660 --> 00:05:11,950
בלתי מציאותי. ישנן

149
00:05:12,280 --> 00:05:14,720
בעיות רבות שבהם אנחנו מצמצמים את ממדי הנתונים

150
00:05:17,600 --> 00:05:18,700
פי 5 או אולי פי 10,

151
00:05:18,780 --> 00:05:20,910
ואנחנו עדיין שומרים על רוב השונות ואנחנו יכולים לעשות את זה

152
00:05:21,270 --> 00:05:22,680
עם רק ירידה קלה מאוד באיכות הסיווג

153
00:05:23,900 --> 00:05:25,840
במונחים של דיוק הסיווג, נניח,

154
00:05:26,240 --> 00:05:27,970
זה בקושי משפיע על

155
00:05:28,770 --> 00:05:30,320
דיוק הסיווג של אלגוריתם הלמידה.

156
00:05:31,090 --> 00:05:32,140
ובזכות העבודה עם נתונים

157
00:05:32,590 --> 00:05:33,730
בעלי ממד נמוך יותר, אלגוריתם הלמידה שלנו

158
00:05:34,060 --> 00:05:36,500
יכול הרבה פעמים לרוץ הרבה יותר מהר.

159
00:05:36,910 --> 00:05:38,120
לסיכום, עד כה דיברנו

160
00:05:38,410 --> 00:05:40,920
על היישומים הבאים של PCA.

161
00:05:41,970 --> 00:05:43,780
הראשון הוא יישום הדחיסה,

162
00:05:44,020 --> 00:05:45,140
שבו אנו עשויים להריץ אותו כדי להפחית

163
00:05:45,500 --> 00:05:46,440
את צריכת הזיכרון או את שטח הדיסק

164
00:05:46,590 --> 00:05:47,960
הדרוש לאחסון הנתונים,

165
00:05:48,240 --> 00:05:49,390
וכרגע דיברנו על שימוש

166
00:05:49,460 --> 00:05:51,630
ב-PCA כדי לזרז את אלגוריתם הלמידה.

167
00:05:52,100 --> 00:05:53,870
למטרות האלה, על מנת

168
00:05:54,130 --> 00:05:56,240
לבחור את k, אנחנו בדרך כלל

169
00:05:56,420 --> 00:05:58,770
נעשה את זה בהתאם

170
00:05:59,160 --> 00:06:00,590
לחישוב אחוז

171
00:06:00,810 --> 00:06:03,880
השונות הנשמרת, אז

172
00:06:04,780 --> 00:06:06,320
כשעושים את זה בשביל המהירות,

173
00:06:06,570 --> 00:06:10,050
בדרך כלל תישמר 99% מהשונות.

174
00:06:10,530 --> 00:06:11,690
וזו תהיה בחירה

175
00:06:12,100 --> 00:06:14,270
אופיינית מאוד ל-k.

176
00:06:14,730 --> 00:06:16,640
כך בוחרים את k עבור יישומים של דחיסה.

177
00:06:17,850 --> 00:06:19,590
אבל כשאנחנו עושים את זה בשביל הדמייה או ויזואליזציה,

178
00:06:20,760 --> 00:06:22,100
בדרך כלל אנחנו יודעים

179
00:06:22,230 --> 00:06:23,550
לעשות גרפים רק של נתונים

180
00:06:24,020 --> 00:06:25,520
דו מימדיים או תלת ממדיים,

181
00:06:26,540 --> 00:06:28,650
ולכן כשהיישום הוא הדמיה,

182
00:06:28,830 --> 00:06:29,660
אנחנו בדרך כלל בוחרים k שווה 2

183
00:06:29,710 --> 00:06:31,930
או k שווה 3, כי אנחנו יודעים לשרטט

184
00:06:32,740 --> 00:06:33,500
רק ערכות נתונים של 2D ו-3D.

185
00:06:34,510 --> 00:06:35,720
זה מסכם את היישומים

186
00:06:36,020 --> 00:06:37,230
העיקריים של PCA, כמו גם

187
00:06:37,870 --> 00:06:39,580
כיצד לבחור את

188
00:06:39,670 --> 00:06:41,540
הערך של k עבור היישומים השונים האלה.

189
00:06:42,890 --> 00:06:45,710
אני רוצה להזכיר שלפעמים יש המשתמשים

190
00:06:46,400 --> 00:06:48,100
ב-PCA למטרה שהוא איננו מתאים לה,

191
00:06:48,800 --> 00:06:50,300
ולפעמים שומעים שמישהו

192
00:06:50,580 --> 00:06:51,820
עושה את זה, בתקווה לא לעתים קרובות מדי.

193
00:06:52,230 --> 00:06:54,780
אני רק רוצה להזכיר את זה כך שתדעו שלא לעשות את זה.

194
00:06:55,480 --> 00:06:56,460
יש שימוש רע אחד

195
00:06:56,540 --> 00:06:59,170
ב-PCA, שהוא לנסות להשתמש בו כדי למנוע התאמת-יתר.

196
00:07:00,380 --> 00:07:00,660
הנה ההיגיון של זה.

197
00:07:01,910 --> 00:07:03,080
זו לא דרך טובה

198
00:07:03,730 --> 00:07:04,610
להשתמש ב-PCA,

199
00:07:04,670 --> 00:07:05,630
אבל הנה ההגיון

200
00:07:05,690 --> 00:07:07,080
מאחורי זה, שהוא,

201
00:07:07,350 --> 00:07:09,090
אם יש לנו xi,

202
00:07:09,300 --> 00:07:10,660
אז אולי יש לנו n תכונות,

203
00:07:10,830 --> 00:07:12,640
אבל אם נדחס את הנתונים

204
00:07:12,750 --> 00:07:13,700
ונשתמש ב-zi במקום זה,

205
00:07:14,270 --> 00:07:15,410
זה יקטין את מספר

206
00:07:15,560 --> 00:07:17,050
התכונות ל-k,

207
00:07:17,290 --> 00:07:19,300
שיכול להיות הרבה יותר קטן.

208
00:07:19,410 --> 00:07:21,130
אז אם יש לנו מספר הרבה יותר קטן

209
00:07:21,490 --> 00:07:22,520
של תכונות, לדוגמא אם k

210
00:07:22,770 --> 00:07:25,800
הוא 1,000 ו-n הוא 10,000,

211
00:07:26,090 --> 00:07:27,010
אז אם יש לנו

212
00:07:27,780 --> 00:07:29,390
רק 1,000 תכונות, אולי

213
00:07:29,670 --> 00:07:30,580
תהיה פחות נטייה להתאמה-יתר

214
00:07:31,260 --> 00:07:32,230
מאשר לו השתמשנו

215
00:07:33,280 --> 00:07:34,980
ב-10,000 תכונות.

216
00:07:35,950 --> 00:07:37,160
אז יש החושבים

217
00:07:37,360 --> 00:07:39,360
על PCA כדרך למנוע התאמת-יתר.

218
00:07:39,950 --> 00:07:41,940
אבל אני מדגיש

219
00:07:42,110 --> 00:07:44,000
שזה לא יישום נכון של PCA

220
00:07:44,260 --> 00:07:46,080
ואני ממליץ שלא לעשות את זה.

221
00:07:46,520 --> 00:07:48,430
ולא כי השיטה הזו לא פועלת היטב.

222
00:07:49,000 --> 00:07:49,920
אם אתם רוצים להשתמש

223
00:07:50,330 --> 00:07:51,560
בשיטה הזו כדי להפחית את ממדי

224
00:07:51,890 --> 00:07:52,830
הנתונים כדי לנסות למנוע התאמת-יתר,

225
00:07:53,690 --> 00:07:54,830
יכול להיות שזה יעבוד בסדר.

226
00:07:55,560 --> 00:07:56,720
אבל זו פשוט לא

227
00:07:57,040 --> 00:07:58,340
דרך טובה כדי לטפל

228
00:07:58,680 --> 00:08:00,390
בהתאמת-יתר, ואם אתם

229
00:08:00,510 --> 00:08:01,810
חוששים מהתאמת-יתר,

230
00:08:02,030 --> 00:08:03,420
יש דרך טובה יותר לטפל בזה,

231
00:08:03,800 --> 00:08:05,680
דהיינו להשתמש בהסדרה

232
00:08:05,900 --> 00:08:07,910
במקום להשתמש ב-PCA ולהקטין את מימד הנתונים .

233
00:08:08,670 --> 00:08:10,000
והסיבה היא, אם

234
00:08:11,010 --> 00:08:12,150
תחשבו על איך PCA עובד,

235
00:08:12,900 --> 00:08:13,950
הוא איננו משתמש בתוויות

236
00:08:14,530 --> 00:08:15,680
y. הוא מתייחס רק

237
00:08:16,050 --> 00:08:17,220
לקלטים xi, ומשתמש

238
00:08:17,340 --> 00:08:19,070
בזה כדי למצוא

239
00:08:19,130 --> 00:08:21,150
קירוב עם מימד נמוך יותר לנתונים.

240
00:08:21,390 --> 00:08:22,840
אז PCA

241
00:08:23,190 --> 00:08:25,410
מתעלם מחלק מהמידע.

242
00:08:26,460 --> 00:08:28,040
הוא מתעלם או מצמצם

243
00:08:28,180 --> 00:08:29,680
את מימדי הנתונים שלך

244
00:08:30,110 --> 00:08:31,390
בלי לדעת מה הערכים של y,

245
00:08:32,380 --> 00:08:33,700
אז שימוש

246
00:08:34,250 --> 00:08:35,770
ב-PCA בדרך הזו

247
00:08:35,920 --> 00:08:37,750
הוא כנראה בסדר אם

248
00:08:37,990 --> 00:08:39,190
נניח 99 אחוז

249
00:08:39,410 --> 00:08:40,400
מהשונות נשמרת, אם אנחנו שומרים

250
00:08:40,830 --> 00:08:41,970
את רוב השונות, אבל

251
00:08:42,100 --> 00:08:44,230
עדיין אולי זורקים קצת מידע בעל ערך.

252
00:08:45,010 --> 00:08:45,980
אבל מתברר

253
00:08:46,310 --> 00:08:47,580
שאם נשמור 99%

254
00:08:47,820 --> 00:08:49,260
מהשונות או 95%

255
00:08:49,360 --> 00:08:50,940
מהשונות או כמה שזה לא יהיה,

256
00:08:51,020 --> 00:08:52,310
מתברר שההסדרה

257
00:08:52,720 --> 00:08:54,650
בדרך כלל תיתן לנו

258
00:08:54,790 --> 00:08:56,010
תוצאות טובות לפחות

259
00:08:56,220 --> 00:08:57,880
כמו PCA כשיטה למניעת התאמת-יתר,

260
00:08:58,900 --> 00:09:00,340
וההסדרה לעתים קרובות פשוט

261
00:09:00,590 --> 00:09:02,220
תעבוד טוב יותר, כי כאשר אנחנו

262
00:09:02,350 --> 00:09:03,890
מיישמים רגרסיה ליניארית

263
00:09:04,250 --> 00:09:05,240
או רגרסיה לוגיסטית או שיטה אחרת

264
00:09:05,600 --> 00:09:07,390
שבה משתמשים בהסדרה, אז

265
00:09:08,010 --> 00:09:09,420
בעיית המזעור הזו בעצם יודעת מה

266
00:09:09,480 --> 00:09:10,740
הערכים של y,

267
00:09:10,960 --> 00:09:12,680
ולכן פחות סביר שתזרוק

268
00:09:12,880 --> 00:09:14,330
מידע בעל ערך, בעוד

269
00:09:14,730 --> 00:09:15,790
PCA איננו משתמש

270
00:09:16,060 --> 00:09:17,810
בתוויות, ולכן סביר

271
00:09:17,850 --> 00:09:19,940
יותר שכן יזרוק מידע בעל ערך.

272
00:09:20,230 --> 00:09:21,370
אז לסיכום,

273
00:09:21,620 --> 00:09:22,900
שימוש ב-PCA

274
00:09:23,010 --> 00:09:24,380
הוא טוב אם המוטיבציה העיקרית שלך

275
00:09:24,530 --> 00:09:26,490
היא להאיץ את אלגוריתם הלמידה,

276
00:09:26,790 --> 00:09:28,360
אבל שימוש ב-PCA כדי למנוע התאמת-יתר

277
00:09:28,650 --> 00:09:29,630
איננו שימוש טוב

278
00:09:30,030 --> 00:09:32,270
ב-PCA, ושימוש בהסדרה במקום זה

279
00:09:32,900 --> 00:09:36,190
הוא הדבר הנכון.

280
00:09:36,440 --> 00:09:40,490
שמומלץ על ידי בעלי המקצוע.

281
00:09:41,310 --> 00:09:43,350
לבסוף, יש עוד שימוש לרעה של PCA.

282
00:09:43,750 --> 00:09:45,760
אני חייב לומר ש-PCA הוא אלגוריתם שימושי מאוד,

283
00:09:46,270 --> 00:09:49,170
ואני משתמש בו לעיתים קרובות למטרות דחיסה או הדמיה.

284
00:09:50,230 --> 00:09:51,400
אבל מה שאני רואה

285
00:09:51,570 --> 00:09:53,310
לפעמים הוא שלפעמים

286
00:09:53,710 --> 00:09:56,080
משתמשים ב-PCA בצורה שלא צריכה לקרות.

287
00:09:56,220 --> 00:09:57,940
דבר נפוץ למדי

288
00:09:58,030 --> 00:09:59,140
שאני רואה, והוא שבזמן

289
00:09:59,330 --> 00:10:00,330
עיצוב של מערכת למידה חישובית,

290
00:10:01,010 --> 00:10:02,130
המתכננים כותבים לעצמם

291
00:10:02,200 --> 00:10:04,150
תכנית כזו: בואו נתכנן מערכת למידה.

292
00:10:05,060 --> 00:10:06,080
נשיג נתוני אימון,

293
00:10:06,570 --> 00:10:07,350
ואז

294
00:10:07,400 --> 00:10:08,700
נפעיל את PCA, ואז

295
00:10:08,860 --> 00:10:11,200
נריץ רגרסיה לוגיסטית ונבדוק את התוצאות על נתוני הבדיקה.

296
00:10:11,680 --> 00:10:12,770
זאת אומרת שהרבה פעמים

297
00:10:13,090 --> 00:10:14,360
מיד בתחילת הפרויקט

298
00:10:14,600 --> 00:10:15,600
כשמתכננים את הפרויקט

299
00:10:15,720 --> 00:10:16,980
מייד מעלים את האפשרות

300
00:10:17,310 --> 00:10:18,610
לעשות את ארבעת השלבים האלה כשאחד מהם הוא PCA.

301
00:10:20,210 --> 00:10:21,220
לפני שכותבים תוכנית פרויקט

302
00:10:21,530 --> 00:10:23,350
שמשלבת את PCA,

303
00:10:23,560 --> 00:10:24,860
כדאי לשאול שאלה

304
00:10:25,030 --> 00:10:27,110
טובה מאוד והיא מה יקרה

305
00:10:27,630 --> 00:10:28,560
אם פשוט נעשה את כל זה

306
00:10:29,540 --> 00:10:31,470
בלי להשתמש ב-PCA.

307
00:10:32,170 --> 00:10:33,450
והרבה פעמים מתכנני הפרויקט

308
00:10:33,800 --> 00:10:34,940
לא חושבים על הרעיון הזה

309
00:10:35,440 --> 00:10:37,080
ומייד בונים תוכנית פרויקט מסובכת

310
00:10:37,920 --> 00:10:40,620
הכוללת יישום של PCA וכו'.

311
00:10:40,810 --> 00:10:42,360
אבל מה שאני

312
00:10:43,050 --> 00:10:44,300
מייעץ למתכננים בדרך כלל לעשות הוא,

313
00:10:44,670 --> 00:10:45,980
לפני שאתם מיישמים

314
00:10:46,450 --> 00:10:47,970
PCA, הייתי מציע

315
00:10:48,220 --> 00:10:49,410
תחילה לעשות

316
00:10:49,600 --> 00:10:50,770
מה שאתם באמת רוצים לעשות,

317
00:10:50,850 --> 00:10:52,030
לקחת את כל מה שאתם רוצים לעשות

318
00:10:52,450 --> 00:10:53,650
ובהתחלה לנסות לעשות את זה

319
00:10:53,980 --> 00:10:56,420
עם נתוני הגלם המקוריים שלכם xi,

320
00:10:56,600 --> 00:10:57,860
ורק אם זה לא מצליח

321
00:10:57,960 --> 00:10:59,650
לעשות מה שניסיתם, רק אז להכניס לתהליך את PCA ולהשתמש ב-zi.

322
00:11:01,010 --> 00:11:02,420
אז לפני השימוש ב-PCA,

323
00:11:03,030 --> 00:11:03,930
במקום לצמצם את מספר המאפיינים

324
00:11:04,360 --> 00:11:05,710
של הנתונים, הייתי שוקל

325
00:11:06,640 --> 00:11:08,020
פשוט לוותר על הצעד של PCA,

326
00:11:08,420 --> 00:11:09,690
ובמקום זאת

327
00:11:10,040 --> 00:11:11,460
לאמן את אלגוריתם הלמידה

328
00:11:12,440 --> 00:11:13,560
על הנתונים המקוריים.

329
00:11:14,410 --> 00:11:15,990
בוא נשתמש רק בקלט הראשוני xi,

330
00:11:16,300 --> 00:11:17,770
ואני ממליץ

331
00:11:18,180 --> 00:11:19,550
במקום להכניס את הצעד

332
00:11:19,720 --> 00:11:20,910
של PCA לתוך האלגוריתם, פשוט

333
00:11:21,030 --> 00:11:23,250
תנסו לעשות את אותו תהליך שרציתם לעשות אבל עם ה-xi הראשוניים.

334
00:11:24,090 --> 00:11:25,000
ורק אם יש לכם

335
00:11:25,150 --> 00:11:26,180
סיבה להאמין שזה לא יעבוד,

336
00:11:26,480 --> 00:11:27,590
לדוגמא אם ברור

337
00:11:27,790 --> 00:11:29,470
שאלגוריתם הלמידה ירוץ

338
00:11:29,510 --> 00:11:31,100
לאט מדי ויסתיים באיטיות רבה מדי,

339
00:11:31,280 --> 00:11:32,680
או אם ברור שדרישות הזיכרון

340
00:11:32,910 --> 00:11:34,140
או שטח הדיסק גדולות מדי,

341
00:11:34,430 --> 00:11:35,850
מה שגורם לכם לרצות

342
00:11:36,190 --> 00:11:37,810
לדחוס את הייצוג שלכם, אבל רק

343
00:11:38,000 --> 00:11:39,020
אם השימוש ב-xi לא עובד,

344
00:11:39,360 --> 00:11:40,640
רק אם יש לכם ראיות טובות

345
00:11:40,950 --> 00:11:41,890
או סיבה טובה להאמין שהשימוש

346
00:11:42,380 --> 00:11:43,890
ב-xi לא יעבוד, רק אז ליישם

347
00:11:44,380 --> 00:11:46,730
PCA ולשקול שימוש בייצוג הדחוס.

348
00:11:47,990 --> 00:11:48,830
כי לפי מה שאני רואה,

349
00:11:49,100 --> 00:11:50,380
לפעמים מתחילים עם

350
00:11:50,530 --> 00:11:51,520
תוכנית פרויקט שמשלבת PCA,

351
00:11:52,100 --> 00:11:54,580
ולפעמים מה

352
00:11:54,650 --> 00:11:55,620
שהם עושים

353
00:11:55,820 --> 00:11:57,380
יעבוד בסדר גמור

354
00:11:57,660 --> 00:11:59,520
אפילו ללא שימוש ב-PCA.

355
00:11:59,840 --> 00:12:01,650
אז פשוט תשקלו גם את זה

356
00:12:01,860 --> 00:12:03,130
כחלופה, לפני

357
00:12:03,320 --> 00:12:04,170
שתבזבזו

358
00:12:04,300 --> 00:12:05,570
הרבה זמן בהרצה של PCA,

359
00:12:05,770 --> 00:12:08,100
בהחלטה על k וכן הלאה.

360
00:12:08,250 --> 00:12:09,330
אז זהו זה בקשר ל-PCA.

361
00:12:09,800 --> 00:12:11,000
למרות ההערות והאזהרות

362
00:12:11,080 --> 00:12:12,380
האחרונות האלה, PCA הוא

363
00:12:12,690 --> 00:12:14,060
אלגוריתם שימושי להפליא כאשר

364
00:12:14,150 --> 00:12:15,330
משתמשים בו למטרות המתאימות,

365
00:12:16,070 --> 00:12:17,480
ואני השתמשתי ומשתמש ב-PCA

366
00:12:17,770 --> 00:12:19,330
לעתים קרובות למדי,

367
00:12:19,580 --> 00:12:20,650
אני משתמש בו בעיקר כדי להאיץ

368
00:12:20,850 --> 00:12:22,150
את זמן הריצה של אלגוריתמי הלמידה שלי.

369
00:12:22,880 --> 00:12:24,310
אבל אני חושב

370
00:12:24,400 --> 00:12:25,690
שהשימוש ב-PCA

371
00:12:26,020 --> 00:12:27,300
צריך להיות

372
00:12:27,410 --> 00:12:29,030
כדי לדחוס נתונים, כדי להפחית

373
00:12:29,620 --> 00:12:30,650
את דרישות הזיכרון

374
00:12:30,990 --> 00:12:33,130
או שטח הדיסק, או להשתמש בו כדי להציג נתונים.

375
00:12:34,270 --> 00:12:35,710
PCA הוא אחד

376
00:12:35,750 --> 00:12:36,960
האלגוריתמים הנפוצים ביותר

377
00:12:36,990 --> 00:12:39,420
ואחד מאלגוריתמי הלמידה הלא-מושגחת החזקים ביותר.

378
00:12:40,060 --> 00:12:41,210
ובעזרת מה שלמדתם

379
00:12:41,420 --> 00:12:43,120
בקטעי הוידאו האלה, אני מקווה

380
00:12:43,500 --> 00:12:44,710
שתצליחו ליישם

381
00:12:45,150 --> 00:12:46,280
PCA וגם להשתמש בו

382
00:12:46,500 --> 00:12:47,930
לכל המטרות הללו.