עבור הבעיה של הפחתת ממדיo, ללא ספק האלגוריתם הפופולרי ביותר והנפוץ ביותר הוא משהו שנקרא ניתוח מרכיבים עיקריים, או PCA. בסרטון הזה, אני רוצה להתחיל לדבר על ניסוח הבעיה עבור PCA. במילים אחרות, בואו ננסה לנסח, בדיוק, מה בדיוק אנחנו רוצים ש-PCA יעשה. נניח שיש לנו מערך נתונים כזה. זה אוסף נתונים של דוגמאות x ב-R2, ונניח שאני רוצה להקטין את המימד של הנתונים מדו מימדי לחד מימדי. במילים אחרות, אני רוצה למצוא קו שעליו אני יכול להטיל את הנתונים. משהו שנראה כמו קו טוב להטיל עליו את הנתונים, לדוגמא קו כזה יכול להיות בחירה טובה למדי. והסיבה שאנחנו חושבים שהוא יכול להיות בחירה טובה היא שאם תסתכלו על ההיטלים של הנקודות, לדוגמא את הנקודה הזו אנחנו מטילים הנה, ואת הנקודה הזאת לכאן, לכאן, לכאן, לכאן, מה שאנו מוצאים הוא שהמרחק בין כל נקודה לגירסת ההיטל שלה הוא די קטן. כלומר, הקטעים הכחולים הם קצרים למדי. אז מה ש-PCA עושה בצורה רשמית הוא מנסה למצוא משטח ממימד נמוך יותר, בעצם קו במקרה שלנו, אשר עליו הוא יטיל את הנתונים, כך שסכום הריבועים של הקטעים הקטנים האלה הוא מינימלי. האורך של הקטעים הכחולים נקרא גם לפעמים שגיאת ההיטל. אז PCA מנסה למצוא משטח שעליו הוא יטיל את הנתונים כך ששגיאת ההיטל תהיה מינימלית. דרך אגב, לפני שמריצים את PCA, נהוג לבצע נירמול ומישקול של התכונות, כך שהממוצע של כל תכונה הוא אפס, וכך שיש לכל התכונות טווחים דומים של ערכים. אני כבר עשיתי את זה מראש עבור הדוגמה הזו, אבל אני אחזור לזה מאוחר יותר ואדבר עוד על מישקול ונירמול בהקשר של PCA. אבל בחזרה לדוגמה, בניגוד לקו האדום שציירתי, הנה קו אחר שעליו אני יכול להטיל את הנתונים שלי, הקו הוורוד. וכפי שנראה, הקו הוורוד הוא כיוון הרבה יותר גרוע להטיל עליו את הנתונים, נכון? לו הטלנו את הנתונים לקו הוורוד, היינו מקבלים נקודות כאלה. ושגיאת ההטלה, האורך של הקטעים הכחולים, תהיה עצומה. הנקודות האלה צריכות לנוע המון כדי להגיע להיטל שלהן על הקו הוורוד. ולכן PCA, ניתוח רכיבים עיקריים, יבחר משהו כמו הקו האדום ולא הקו הוורוד כאן. בואו נכתוב את הבעיה של PCA בצורה קצת יותר רשמית. המטרה של PCA, כשאנחנו רוצים לצמצם את הנתונים משני מימדים למימד אחד היא לנסות ולמצוא וקטור u1, שהוא וקטור ב-Rⁿ, דהיינו R² במקרה שלנו, ולמצוא את הכיוון שעליו יש להטיל את הנתונים, כך ששגיאת ההיטל תהיה מינימלית. בדוגמה הזו אנחנו מקווים ש-PCA ימצא את הווקטור הזה, שלו אני רוצה לקרוא (1)u, כך שכאשר נטיל את הנתונים על הקו המוגדר על ידי הארכת הווקטור הזה, נקבל שגיאת היטל קטנה למדי. ההטלה של הנתונים שלנו נראית ככה. ודרך אגב, יש לציין כי זה לא חשוב אם PCA נותן לי (1)u או (1)u-. אם הוא נותן לי וקטור חיובי בכיוון הזה, זה בסדר גמור. ואם הוא נותן לי את הווקטור בכיוון ההפוך, בעצם (1)u-. בואו נצייר את הווקטור ההפוך בכחול, רואים? זה לא חשוב אם PCA נותן לנו (1)u חיובי או (1)u שלילי, כי כל אחד מהווקטורים מגדיר אותו קו אדום שעליו אנחנו מטילים את הנתונים. אז זה המקרה של צמצום נתונים דו מימדיים למימד אחד. במקרה היותר כללי יש לנו נתונים n-ממדיים ואנחנו רוצים לצמצם אותם לk ממדים. במקרה הזה אנחנו רוצים למצוא לא וקטור אחד שעליו להטיל את הנתונים, אלא אנחנו רוצים למצוא k וקטורים, או תת-מרחב k-ממדי בתוך המרחב הn-ממדי ועליו להטיל את הנתונים. בצורה שתמזער את שגיאת ההיטל. אז הנה דוגמה. אם יש לי ענן כזה של נקודות תלת-ממדיות, אז אולי מה שאני רוצה הוא למצוא וקטורים למצוא זוג של וקטורים ואני אקרא להם, ...בואו נצייר אותם באדום... אני רוצה למצוא זוג וקטורים, כמובן מתחילים בראשית, הראשון הוא (1)u, והשני הוא (2)u. ביחד, שני הווקטורים מגדירים מישור, או משטח דו-ממדי, נכון? משטח דו-ממדי שעליו אנחנו נטיל את הנתונים. לאלו מכם המכירים אלגברה ליניארית, או שהם באמת מומחים באלגברה ליניארית, ההגדרה הפורמלית של זה היא שאנחנו נמצא קבוצה של וקטורים (2)u(1), u אולי עד (u(k. ואנחנו נבצע הטלות של הנתונים שלנו על תת-המרחב הווקטורי הנפרש על ידי הקבוצה הזו של k וקטורים. אבל למי שלא מכיר אלגברה ליניארית, תחשבו על זה כעל למצוא k כיוונים במקום כיוון אחד שעליו עושים היטל של הנתונים. אז מציאת משטח k-מימדי במקרה שלנו הוא בדיוק מציאת מישור 2D, כמו שמוצג באיור הזה, שבו אנחנו יכולים להגדיר את המיקום של נקודות במישור באמצעות k כיוונים. ולכן ב-PCA אנחנו רוצים למצוא k וקטורים שעליהם לעשות היטלים של הנתונים. אז באופן יותר רשמי ב-PCA, מה שאנחנו רוצים לעשות הוא למצוא דרך להטיל את הנתונים ולמזער את שגיאת ההיטלים, שהיא המרחק בין הנקודות לבין התחזיות. וכך גם בתמונה התלת-ממדית הזו, בהינתן נקודה אנחנו לוקחים את הנקודה ומטילים אותה על המישור הדו-ממדי הזה. ... אז שגיאת ההיטל היא המרחק בין הנקודה והמיקום של ההיטל שלה על פני המישור הדו-ממדי. אז מה ש-PCA עושה הוא מנסה למצוא קו, או מישור או מה שלא יהיה, כדי להטיל עליו את הנתונים, ולנסות למזער את ההיטל המרובע, ההיטל ב-90 מעלות או שגיאת ההיטל האורתוגונלי. לבסוף, שאלה אחת שלפעמים שואלים אותי הוא על היחס שבין PCA ורגרסיה ליניארית? כי כאשר אני מסביר את PCA, אני לפעמים מצייר דיאגרמות כאלה וזה נראה דומה לרגרסיה ליניארית. מתברר ש-PCA אינו רגרסיה ליניארית, ולמרות דמיון קוסמטי כלשהו, אלה הם למעשה אלגוריתמים שונים לחלוטין. לו עשינו רגרסיה ליניארית, מה שהיינו עושים היה, בצד שמאל היינו מנסים לחזות את הערך של איזה משתנה y על פי וקטור תכונות קלט x. אז ברגרסיה ליניארית אנחנו מתאימים קו ישר כדי למזער את השגיאה הריבועית בין נקודות לקו הישר הזה. מה שאנחנו ממזערים יהיה הגודל של הקווים הכחולים האלה. ושימו לב שאני מצייר את הקווים הכחולים האלה אנכית. כי הקווים הכחולים האלה הם המרחק האנכי בין הנקודה לבין הערך החזוי על ידי ההשערה. לעומת זאת ב- PCA אנחנו מנסים למזער את גודל הקווים הכחולים האלה שמשורטטים בזווית. אלה הם המרחקים האורתוגונליים הקצרים ביותר. המרחק הקצר ביותר בין הנקודה x לבין הקו האדום. וזה נותן תוצאות שונות מאוד בהתאם למערך הנתונים. ובאופן כללי יותר, כאשר עושים רגרסיה ליניארית, יש המשתנה הזה y שמנסים לחזות. כל מה שעושה רגרסיה ליניארית הוא לקחת את הערכים של x ולנסות להשתמש בו כדי לחזות את y. בעוד ב-PCA, אין משתנה מיוחד y שאנחנו מנסים לחזות. במקום זאת, יש לנו רשימה של תכונות, x1, x2, וכן הלאה, עד xn, וכל התכונות האלה מטופלות באופן שווה, ואף אחת מהן איננה מיוחדת. כדוגמה אחת אחרונה, אם יש לי נתונים תלת מימדיים ואני רוצה לצמצם את הנתונים מ-3D ל-2D, נאמר שאני רוצה למצוא שני כיוונים, (1)u ו-(2)u, שעליהם להטיל את הנתונים שלי. אז יש לי שלוש תכונות, x1, x2, x3, וכולן מטופלות בצורה זהה. כל התכונות מטופלות באופן סימטרי ואין שום משתנה מיוחד שאני מנסה לחזות. אז PCA איננו רגרסיה ליניארית, ואף על פי שברמה הקוסמטית הם עשויים להיראות קשורים, אלה הם למעשה אלגוריתמים שונים מאוד. אז אני מקווה שאתם כבר מבינים מה PCA עושה. הוא מנסה למצוא משטח ממימד יותר נמוך ועליו לעשות היטלים של הנתונים בצורה שתמזער את שגיאת ההיטל האורתוגונלי. למזער את המרחק האורתוגונלי בין כל נקודה והמיקום שאליו היא מוטלת. בסרטון הבא, נתחיל לדבר על איך באמת למצוא תת-מרחב כזה ממימד נמוך שעליו אפשר לעשות היטלים.