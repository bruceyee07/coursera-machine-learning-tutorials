בסרטון הזה אני רוצה לספר לכם על אלגוריתם ניתוח מרכיבים עיקריים. ובסוף הסרטון הזה אתם תדעו ליישם PCA בעצמכם ולהשתמש בו כדי לצמצם את מספר המאפיינים של הנתונים שלכם. לפני שמיישמים את PCA, יש שלב מקדים של עיבוד נתונים שתמיד צריך לעשות. כשמקבלים ערכת אימון של m דוגמאות לא מתויגות, חשוב תמיד לבצע נירמול ובהתאם לנתונים שלך, אולי גם מישקול. זה דומה מאוד לנירמול הממוצע ולתהליך שינוי קנה המידה שיש לנו בלמידה מבוקרת. למעשה זה בדיוק אותו תהליך, חוץ מזה שאנחנו עושים את זה עכשיו על נתונים לא מתויגים, x1 עד xm. אז בנירמול של הממוצע אנחנו מחשבים את הממוצע של כל תכונה ואז מחליפים כל תכונה, x, ב-x מינוס הממוצע שלה, מה שגורם לכך שהממוצע של כל תכונה הוא עכשיו אפס. לגבי המישקול, לפעמים לתכונות השונות יש קני מידה שונים מאוד. למשל, אם x1 הוא גודל הבית, ו-x2 הוא מספר חדרי השינה, להשתמש בדוגמה הקודמת שלנו, אז אנחנו חייבים לשנות את קני המידה כך שלכל תכונה יהיה טווח דומה של ערכים. אז בדומה למה שהיה לנו עם למידה בפיקוח, אנחנו לוקחים את (x(i במקום ה-j, המאפיין ה-j, מחסירים את הממוצע, זו הפעולה הראשונה, ואז מחלקים ב-sj. וכאן sj הוא מדד מסוים של טווח הערכים של התכונה j. אז זה יכול להיות המקסימום פחות המינימום, או משהו שיותר נפוץ, סטיית התקן של התכונה j. לאחר שעשינו את עיבוד הנתונים המקדים הזה, הנה מה שעושה אלגוריתם PCA. ראינו בווידאו הקודם ש-PCA מנסה למצוא תת-מרחב מממד קטן יותר ולהטיל עליו את הנתונים, כך שזה ימזער את שגיאת ההיטל הריבועי, את סכום שגיאות ההיטל הריבועיים, ריבוע האורך של הקווים הכחולים. אז מה שרצינו ספציפית לעשות הוא למצוא וקטור, u1, המציין את הכיוון או במקרה של שני ממדים אנחנו רוצים למצוא שני וקטורים, u1 ו-u2 שיגדירו את המישור שעליו נבצע היטלים של הנתונים. אז, כתזכורת מהירה של מה פירושו של צמצום הממדים של נתונים, בדוגמה הזו בצד שמאל קיבלנו דוגמאות xi, שהן ב-R². ואנחנו רוצים למצוא קבוצה של מספרים zi ב-R¹, ובעזרתם לייצג את הנתונים שלנו. זוהי הפחתת מימדים מ-2D ל-1D. אז ספציפית, אם נעשה היטלים על הקו האדום הזה, נצטרך רק מספר אחד כדי לציין את המיקום של הנקודות על הקו. אז למספר הזה אנחנו נקרא z1 או סתם z. z הוא מספר ממשי, אז הוא כמו וקטור עם ממד אחד. אז z1 פשוט מתייחס למרכיב הראשון של המטריצה אחת על אחת או הוקטור החד-ממדי הזה. אז אנחנו צריכים רק מספר אחד כדי לציין את המיקום של נקודה. אם הדוגמה הזו כאן היא דוגמה x1, אז אולי היא תמופה לכאן. ואם הדוגמה הזו היא x2, אולי היא ממופה לכאן, אז הנקודה הזאת כאן תהיה z1 והנקודה הזאת כאן תהיה z2, ובדומה לכך תהיינה לנו נקודות אחרות עבור שאר הנקודות, נניח x3, x4, x5 ממופות ל-z5, z4, z3. אז מה ש-PCA צריך לעשות הוא לתת לנו דרך לחשב שני דברים. הראשון הוא לחשב את הוקטורים האלה, u1, ובמקרה הזה u1 ו-u2. והשני הוא לחשב את המספרים האלה, z. אז בדוגמה משמאל אנחנו מצמצמים נתונים מ-2D ל-1D. ובדוגמה מימין אנחנו מצמצמים נתונים מ-3 ממדים, דהיינו מ-R³, ל-zi, שהם עכשיו דו-ממדיים. אז הווקטורים zi הם עכשיו דו-ממדיים. אז הווקטור הוא z1 z2 ככה, ואנחנו גם צריכים למצוא דרך לחשב את הייצוגים החדשים של הנתונים, z1 ו-z2. אז איך מחשבים את הגדלים האלו? מתברר כי הגזירה המתמטית, ההוכחה המתמטית, של מה הם הערכים הנכונים של u1, u2, z1, z2 וכן הלאה, ההוכחה המתמטית הזו מורכבת מאוד ומחוץ להיקף של הקורס. אבל ברגע שעשית את הגזירה הזו, מתברר שהתהליך של מציאת הערך של u1 שזה מה שצריך, זה לא כל כך קשה, למרות שההוכחה המתמטית שהערך הזה הוא הערך הנכון היא משהו יותר מדי מסובך בשבילנו. אז אני פשוט אתאר את הפרוצדורה המעשית שצריך ליישם כדי לחשב את כל הוקטורים האלה, u1, u2, z וכו'. הנה הנוהל. נניח שאנחנו רוצים להקטין את הנתונים מ-n ממדים ל-k ממדים. מה שאנחנו נעשה הוא קודם כל לחשב משהו שנקרא מטריצת שונוּת או מטריצת שונוּת משותפת (covariance matrix), ומטריצת השונוּת מסומנת בדרך כלל על ידי האות היוונית סיגמא גדולה, Σ. זה קצת מבלבל שסיגמא גדולה נראית בדיוק כמו הסמל של סיכום. אז זו סיגמא שמשמשת לציון מטריצה וכאן היא סימן של סיכום. אני מקווה שבשקפים האלה לא יהיה בלבול לגבי איזו סיגמא היא מטריצה ואיזו היא סמל של סיכום, ואני מקווה שזה יהיה ברור מההקשר מתי אני משתמש בכל אחד מהם. איך מחשבים את המטריצה הזו? נניח שאנחנו רוצים לאחסן אותה במשתנה של אוקטבה שנקרא סיגמא. מה שצריך לעשות הוא לחשב משהו שנקרא הווקטורים העצמיים (eigen vectors) של המטריצה סיגמא. באוקטבה, עושים את זה בשימוש בפקודה הזו, U S V שווה svd של סיגמא. svd, אגב, מייצג פירוק ערך סינגולרי. פירוק ערך סינגולרי הוא דבר הרבה יותר מתקדם,... אלגברה ליניארית הרבה יותר מתקדמת ממה שאתם באמת צריכים לדעת אבל מתברר שכאשר סיגמא היא מטריצת שונוּת יש כמה דרכים לחשב את הווקטורים העצמיים האלה ומי שמומחה באלגברה ליניארית ושמע כבר על וקטורים עצמיים, עשוי לדעת שיש עוד פונקציה באוקטבה שנקראת eig, שאפשר להשתמש בה כדי לחשב את אותו הדבר. וכפי שמתברר הפונקציה svd והפונקציה eig נותנות את אותם וקטורים, אבל svd היא קצת יותר יציבה מבחינה נומרית ולכן אני נוטה להשתמש ב-svd, אם כי יש לי כמה חברים שמשתמשים בפונקציה eig לעשות את זה, אבל כאשר מיישמים את זה על מטריצת השונות Σ, התוצאה היא זהה. הסיבה לכך היא כי מטריצת השונות תמיד מקיימת תכונה מתמטית הנקראת סימטרית חיובית חצי-מוחלטת. אתם ממש לא צריכים לדעת מה זה אומר, אבל svd ו-eig הן פונקציות שונות, שכאשר מריצים אותם על מטריצת שונות, שניתן להוכיח שהיא תמיד מקיימת את המאפיין המתמטי הזה, הפונקציות תחזרנה תשובה זהה. טוב, זה היה כנראה הרבה יותר אלגברה ליניארית ממה שצריך לדעת. ואם לא באמת קלטתם, לא נורא. כל מה שאתם צריכים לדעת הוא שזו היא הפונקציה שאתם צריבים להריץ באוקטבה. ואם אתם מיישמים את זה בשפה אחרת מאשר אוקטבה או MATLAB, אתם צריכים למצוא ספריית אלגברה ליניארית שיכולה לחשב פירוק ערך סינגולרי, ויש הרבה ספריות כאלה כנראה לכל שפות התכנות הגדולות. ואפשר להשתמש בהן כדי לחשב את המטריצות U, S ו-V של המטריצה Σ. אז בואו נוסיף עוד כמה פרטים, מטריצת השונות Σ הזו היא מטריצה n על n. קל לראות את זה כי אם נסתכל על ההגדרה שלה, אז זה וקטור בממד n על 1 וזו היא המשוחלפת שלה שהיא 1 על n, ולכן המכפלה שלהם תהיה מטריצה n על n. 1xn כפול 1xn משוחלף, זו מטריצה nxn וכאשר אנו מחברים כמה כאלה יש לנו עדיין מטריצה nxn. הפונקציה svd מוציאה פלט של שלוש מטריצות, U, S, V. מה שמעניין אותנו הוא המטריצה U. גם U היא מטריצה nxn. וכשנסתכל על העמודות של המטריצה U, נגלה כי העמודות הן בדיוק אותם וקטורים, u1, u2 וכן הלאה. אז U היא מטריצה nxn. ואם אנחנו רוצים להפחית את הנתונים מ-n ממדים ל-k ממדים, מה שאנחנו צריכים לעשות הוא לקחת את k הוקטורים הראשונים של U. זה נותן לנו u1 עד uk שהם k הכיוונים שאליהם אנחנו רוצים לבצע הטלות של הנתונים. אז התהליך הוא: מריצים את פונקצית svd ומקבלים את המטריצה U. נקרא לעמודות האלה u1-un. שוב, לסכם את התיאור של שאר התהליך, מריצים את פונקצית האלגברה הליניארית svd ומקבלים מטריצות U, S, ו-V. נשתמש ב-k העמודות הראשונות של המטריצה U ונקבל u1-uk. אז הדבר השני שאנחנו צריכים הוא לקחת את ערכת הנתונים המקוריים x שהם ב-Rⁿ ולמצוא ייצוג עם ממד נמוך יותר z ב-Rk עבור הנתונים האלה. אז אנחנו עושים את זה על ידי לקיחת k העמודות הראשונות של המטריצה U. בנינו את המטריצה הזו. העמדנו את u1, u2 וכן הלאה עד uk בעמודות. בעצם לקחנו את החלק הזה של המטריצה, k העמודות הראשונות של המטריצה הזו. אז זו מטריצה n על k. אני אתן למטריצה הזאת שם. אני אקרא למטריצה הזו U סימן תחתון "מופחתת", מין גרסה מופחתת של המטריצה U. ואנחנו נשתמש בה כדי להקטין את המימד של הנתונים שלנו. והדרך שבה נחשב את z היא זו: z שווה U מופחתת משוחלפת כפול x. או לחילופין, בואו נרשום מה פירוש משוחלפת. כשמשחלפים את המטריצה U המופחתת, מקבלים את וקטורי העמודה האלה כשורות. אנחנו מקבלים את u1 משוחלף עד uk משוחלף. מכפילים את זה ב-x, וככה מקבלים את הוקטור z. רק לוודא שהממדים הם מה שרצינו, המטריצה הזו כאן היא k על n ו-x הוא n על 1 ולכן המכפלה היא k על 1. ולכן z הוא k-ממדי, הוא וקטור בעל מימד k, וזה בדיוק מה שרצינו. וכמובן הx-ים האלה יכולים להיות דוגמאות ממערך האימון, או מסדרת האימות או מסדרת המבחן. אז למשל דוגמת ההדרכה i, אפשר לכתוב אותה כ-xi, והתוצאה היא zi. לסיכום, הנה אלגוריתם PCA בשקופית אחת. לאחר נירמול שמבטיח שממוצע כל תכונה הוא אפס ולפעמים גם מישקול שחייבים לעשות אם לתכונות יש טווחים שונים מאוד של ערכים. לאחר העיבוד הזה מראש, אנחנו מחשבים את מטריצת השונות Σ, ודרך אגב אם הנתונים נמצאים במטריצה, אם הנתונים נתונים בשורות כמו כאן, אם יש לנו מטריצה X שהיא סדרת האימון שלנו, שבה השורות הן x1 משוחלפת, עד xm משוחלפת, אז לחישוב המטריצה Σ יש יישום וקטורי פשוט. היישום באוקטבה, הוא סיגמא שווה 1 חלקי m, כפול X - שהיא המטריצה כאן - משוחלפת, כפול X, והביטוי הפשוט הזה הוא היישום הוקטורי של חישוב המטריצה Σ. אני לא אוכיח היום שזה נכון, אם תרצו, תוכלו לבחון את זה בעצמכם על ידי כך שתנסו באוקטבה ותוודאו ששני היישומים האלה נותנים אותן תשובות, או אתם יכולים לנסות להוכיח את זה מתמטית. בכל מקרה, זהו יישום וקטורי נכון של איך לחשב את Σ. בשלב הבא, אנו מריצים את פונקצית svd כדי לקבל את U, S, ו-V. ואז אנחנו לוקחים את k העמודות הראשונות של המטריצה U, "מפחיתים" אותה, וזה מגדיר איך אנחנו מחשבים מתוך וקטור התכונות x את הווקטור מופחת הממדים z. ובדומה לK-מרכזים, אנחנו מיישמים את PCA עם וקטורים x שנמצאים ב-Rⁿ. אנחנו לא משתמשים בקונבנציה של x₀=1. אז זהו אלגוריתם PCA. דבר אחד שלא עשיתי הוא לתת הוכחה מתמטית שמה שעשינו באמת נותן היטל k-מימדי של הנתונים, על תת-מרחב k-ממדי שבאמת ממזער את שגיאת ההיטל הריבועי. הוכחה של זה היא מעבר להיקף של הקורס הזה. למרבה המזל אלגוריתם PCA ניתן ליישום בלא יותר מדי שורות קוד. באוקטבה או ב-MATLAB, תקבלו אלגוריתם יעיל מאוד להפחתת מימדים. אז זה היה אלגוריתם PCA. דבר אחד שלא עשיתי הוא לתת הוכחה מתמטית ש-u1 ו-u2 וכן הלאה ו-z וכן הלאה שיוצאים מהתהליך הזה הם באמת הבחירות שימזערו את שגיאת ההיטל בריבוע. נכון, זכרו שאמרנו ש-PCA מנסה למצוא משטח או קו שעליו לעשות היטל של הנתונים כך ששגיאת ההיטל המרובע תמוזער. אז לא הוכחתי כי זה, וההוכחה המתמטית הוכחה של זה היא מעבר להיקף של הקורס הזה. אבל למרבה המזל אלגוריתם PCA ניתן ליישום בלא יותר מדי שורות קוד באוקטבה. ואם תיישמו אותו הוא יעבוד, יעבוד טוב, ואם תיישמו את האלגוריתם הזה, תקבלו אלגוריתם צמצום מימדים יעיל מאוד שעושה את הדבר הנכון של מזעור שגיאת ההיטל המרובע הזו.