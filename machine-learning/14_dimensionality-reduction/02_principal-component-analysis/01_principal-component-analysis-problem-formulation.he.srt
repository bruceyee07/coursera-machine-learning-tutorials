1
00:00:00,110 --> 00:00:04,230
עבור הבעיה של הפחתת ממדיo, ללא ספק האלגוריתם הפופולרי ביותר

2
00:00:04,230 --> 00:00:07,180
והנפוץ ביותר הוא משהו שנקרא

3
00:00:07,180 --> 00:00:10,230
ניתוח מרכיבים עיקריים, או PCA.

4
00:00:10,230 --> 00:00:14,920
בסרטון הזה, אני רוצה להתחיל לדבר על ניסוח הבעיה עבור PCA.

5
00:00:14,920 --> 00:00:17,040
במילים אחרות, בואו ננסה לנסח,

6
00:00:17,040 --> 00:00:20,690
בדיוק, מה בדיוק אנחנו רוצים ש-PCA יעשה.

7
00:00:20,690 --> 00:00:22,220
נניח שיש לנו מערך נתונים כזה.

8
00:00:22,220 --> 00:00:26,520
זה אוסף נתונים של דוגמאות x ב-R2,

9
00:00:26,520 --> 00:00:31,190
ונניח שאני רוצה להקטין את המימד של הנתונים מדו מימדי לחד מימדי.

10
00:00:31,190 --> 00:00:35,190
במילים אחרות, אני רוצה למצוא קו שעליו אני יכול להטיל את הנתונים.

11
00:00:35,190 --> 00:00:38,329
משהו שנראה כמו קו טוב להטיל עליו את הנתונים,

12
00:00:38,329 --> 00:00:41,177
לדוגמא קו כזה יכול להיות בחירה טובה למדי.

13
00:00:41,177 --> 00:00:45,463
והסיבה שאנחנו חושבים שהוא יכול להיות בחירה טובה היא

14
00:00:45,463 --> 00:00:49,342
שאם תסתכלו על ההיטלים של הנקודות, לדוגמא את הנקודה הזו

15
00:00:49,342 --> 00:00:50,656
אנחנו מטילים הנה,

16
00:00:50,656 --> 00:00:55,440
ואת הנקודה הזאת לכאן, לכאן, לכאן, לכאן,

17
00:00:55,440 --> 00:00:59,905
מה שאנו מוצאים הוא שהמרחק בין כל נקודה

18
00:00:59,905 --> 00:01:03,219
לגירסת ההיטל שלה הוא די קטן.

19
00:01:03,219 --> 00:01:08,769
כלומר, הקטעים הכחולים הם קצרים למדי.

20
00:01:08,769 --> 00:01:13,961
אז מה ש-PCA עושה בצורה רשמית הוא מנסה למצוא משטח ממימד נמוך יותר,

21
00:01:13,961 --> 00:01:18,147
בעצם קו במקרה שלנו, אשר עליו הוא יטיל את הנתונים,

22
00:01:18,147 --> 00:01:23,580
כך שסכום הריבועים של הקטעים הקטנים האלה הוא מינימלי.

23
00:01:23,580 --> 00:01:25,605
האורך של הקטעים הכחולים

24
00:01:25,605 --> 00:01:28,850
נקרא גם לפעמים שגיאת ההיטל.

25
00:01:28,850 --> 00:01:33,022
אז PCA מנסה למצוא משטח שעליו הוא יטיל את הנתונים

26
00:01:33,022 --> 00:01:34,810
כך ששגיאת ההיטל תהיה מינימלית.

27
00:01:34,810 --> 00:01:39,820
דרך אגב, לפני שמריצים את PCA, נהוג לבצע

28
00:01:39,820 --> 00:01:44,490
נירמול ומישקול של התכונות,

29
00:01:44,490 --> 00:01:49,160
כך שהממוצע של כל תכונה הוא אפס, וכך שיש לכל התכונות טווחים דומים של ערכים.

30
00:01:49,160 --> 00:01:52,700
אני כבר עשיתי את זה מראש עבור הדוגמה הזו, אבל אני אחזור לזה מאוחר יותר

31
00:01:52,700 --> 00:01:57,385
ואדבר עוד על מישקול ונירמול בהקשר של PCA.

32
00:01:58,540 --> 00:02:03,340
אבל בחזרה לדוגמה, בניגוד לקו האדום שציירתי,

33
00:02:03,340 --> 00:02:06,370
הנה קו אחר שעליו אני יכול להטיל את הנתונים שלי,

34
00:02:06,370 --> 00:02:07,808
הקו הוורוד.

35
00:02:07,808 --> 00:02:12,380
וכפי שנראה, הקו הוורוד הוא כיוון הרבה יותר גרוע

36
00:02:12,380 --> 00:02:14,120
להטיל עליו את הנתונים, נכון?

37
00:02:14,120 --> 00:02:17,070
לו הטלנו את הנתונים לקו הוורוד,

38
00:02:17,070 --> 00:02:19,140
היינו מקבלים נקודות כאלה.

39
00:02:19,140 --> 00:02:24,950
ושגיאת ההטלה, האורך של הקטעים הכחולים, תהיה עצומה.

40
00:02:24,950 --> 00:02:29,650
הנקודות האלה צריכות לנוע המון

41
00:02:29,650 --> 00:02:33,760
כדי להגיע להיטל שלהן על הקו הוורוד.

42
00:02:33,760 --> 00:02:37,430
ולכן PCA, ניתוח רכיבים עיקריים,

43
00:02:37,430 --> 00:02:41,658
יבחר משהו כמו הקו האדום ולא הקו הוורוד כאן.

44
00:02:42,915 --> 00:02:45,791
בואו נכתוב את הבעיה של PCA בצורה קצת יותר רשמית.

45
00:02:45,791 --> 00:02:50,504
המטרה של PCA, כשאנחנו רוצים לצמצם את הנתונים משני מימדים למימד אחד היא

46
00:02:50,504 --> 00:02:55,974
לנסות ולמצוא וקטור u1,

47
00:02:55,974 --> 00:03:00,367
שהוא וקטור ב-Rⁿ, דהיינו R² במקרה שלנו,

48
00:03:00,367 --> 00:03:03,365
ולמצוא את הכיוון שעליו יש להטיל את הנתונים,

49
00:03:03,365 --> 00:03:05,350
כך ששגיאת ההיטל תהיה מינימלית.

50
00:03:05,350 --> 00:03:10,273
בדוגמה הזו אנחנו מקווים ש-PCA ימצא את הווקטור הזה,

51
00:03:10,273 --> 00:03:15,105
שלו אני רוצה לקרוא (1)u, כך שכאשר נטיל את הנתונים

52
00:03:15,105 --> 00:03:19,116
על הקו המוגדר על ידי הארכת הווקטור הזה,

53
00:03:19,116 --> 00:03:23,055
נקבל שגיאת היטל קטנה למדי.

54
00:03:23,055 --> 00:03:25,736
ההטלה של הנתונים שלנו נראית ככה.

55
00:03:25,736 --> 00:03:30,392
ודרך אגב, יש לציין כי זה לא חשוב אם PCA נותן לי

56
00:03:30,392 --> 00:03:32,630
(1)u או (1)u-.

57
00:03:32,630 --> 00:03:35,020
אם הוא נותן לי וקטור חיובי בכיוון הזה, זה בסדר גמור.

58
00:03:35,020 --> 00:03:40,170
ואם הוא נותן לי את הווקטור בכיוון ההפוך,

59
00:03:40,170 --> 00:03:42,190
בעצם (1)u-.

60
00:03:42,190 --> 00:03:43,900
בואו נצייר את הווקטור ההפוך בכחול, רואים?

61
00:03:43,900 --> 00:03:48,010
זה לא חשוב אם PCA נותן לנו (1)u חיובי או (1)u שלילי,

62
00:03:48,010 --> 00:03:52,950
כי כל אחד מהווקטורים מגדיר אותו קו אדום שעליו אנחנו מטילים את הנתונים.

63
00:03:54,110 --> 00:03:58,970
אז זה המקרה של צמצום נתונים דו מימדיים למימד אחד.

64
00:03:58,970 --> 00:04:02,336
במקרה היותר כללי יש לנו נתונים n-ממדיים

65
00:04:02,336 --> 00:04:05,020
ואנחנו רוצים לצמצם אותם לk ממדים.

66
00:04:05,020 --> 00:04:08,880
במקרה הזה אנחנו רוצים למצוא לא וקטור אחד שעליו להטיל את הנתונים,

67
00:04:08,880 --> 00:04:13,340
אלא אנחנו רוצים למצוא k וקטורים, או תת-מרחב k-ממדי בתוך המרחב הn-ממדי ועליו להטיל את הנתונים.

68
00:04:13,340 --> 00:04:16,420
בצורה שתמזער את שגיאת ההיטל.

69
00:04:16,420 --> 00:04:17,540
אז הנה דוגמה.

70
00:04:17,540 --> 00:04:24,910
אם יש לי ענן כזה של נקודות תלת-ממדיות, אז אולי מה שאני רוצה הוא למצוא וקטורים

71
00:04:24,910 --> 00:04:27,030
למצוא זוג של וקטורים

72
00:04:27,030 --> 00:04:29,010
ואני אקרא להם,

73
00:04:29,010 --> 00:04:30,430
...בואו נצייר אותם באדום...

74
00:04:30,430 --> 00:04:33,340
אני רוצה למצוא זוג וקטורים, כמובן מתחילים בראשית,

75
00:04:33,340 --> 00:04:40,421
הראשון הוא (1)u, והשני הוא (2)u.

76
00:04:40,421 --> 00:04:46,776
ביחד, שני הווקטורים מגדירים מישור, או משטח דו-ממדי, נכון?

77
00:04:46,776 --> 00:04:52,160
משטח דו-ממדי שעליו אנחנו נטיל את הנתונים.

78
00:04:52,160 --> 00:04:55,170
לאלו מכם המכירים אלגברה ליניארית,

79
00:04:55,170 --> 00:04:58,850
או שהם באמת מומחים באלגברה ליניארית, ההגדרה הפורמלית של זה היא

80
00:04:58,850 --> 00:05:03,230
שאנחנו נמצא קבוצה של וקטורים (2)u(1), u אולי עד (u(k.

81
00:05:03,230 --> 00:05:05,930
ואנחנו נבצע הטלות של הנתונים שלנו

82
00:05:05,930 --> 00:05:10,570
על תת-המרחב הווקטורי הנפרש על ידי הקבוצה הזו של k וקטורים.

83
00:05:10,570 --> 00:05:14,130
אבל למי שלא מכיר אלגברה ליניארית, תחשבו על זה

84
00:05:14,130 --> 00:05:18,710
כעל למצוא k כיוונים במקום כיוון אחד שעליו עושים היטל של הנתונים.

85
00:05:18,710 --> 00:05:23,490
אז מציאת משטח k-מימדי במקרה שלנו הוא בדיוק מציאת מישור 2D,

86
00:05:23,490 --> 00:05:24,940
כמו שמוצג באיור הזה,

87
00:05:24,940 --> 00:05:30,260
שבו אנחנו יכולים להגדיר את המיקום של נקודות במישור באמצעות k כיוונים.

88
00:05:30,260 --> 00:05:34,660
ולכן ב-PCA אנחנו רוצים למצוא k וקטורים שעליהם לעשות היטלים של הנתונים.

89
00:05:34,660 --> 00:05:39,582
אז באופן יותר רשמי ב-PCA, מה שאנחנו רוצים לעשות הוא למצוא דרך להטיל את הנתונים

90
00:05:39,582 --> 00:05:42,742
ולמזער את שגיאת ההיטלים,

91
00:05:42,742 --> 00:05:46,644
שהיא המרחק בין הנקודות לבין התחזיות.

92
00:05:46,644 --> 00:05:48,870
וכך גם בתמונה התלת-ממדית הזו,

93
00:05:48,870 --> 00:05:54,667
בהינתן נקודה אנחנו לוקחים את הנקודה ומטילים אותה על המישור הדו-ממדי הזה.

94
00:05:55,760 --> 00:05:57,290
...

95
00:05:57,290 --> 00:06:02,120
אז שגיאת ההיטל היא המרחק בין הנקודה

96
00:06:02,120 --> 00:06:05,910
והמיקום של ההיטל שלה על פני המישור הדו-ממדי.

97
00:06:05,910 --> 00:06:09,994
אז מה ש-PCA עושה הוא מנסה למצוא קו, או מישור או מה שלא יהיה,

98
00:06:09,994 --> 00:06:14,428
כדי להטיל עליו את הנתונים, ולנסות למזער את ההיטל המרובע,

99
00:06:14,428 --> 00:06:18,110
ההיטל ב-90 מעלות או שגיאת ההיטל האורתוגונלי.

100
00:06:18,110 --> 00:06:22,400
לבסוף, שאלה אחת שלפעמים שואלים אותי הוא על היחס שבין PCA

101
00:06:22,400 --> 00:06:23,278
ורגרסיה ליניארית?

102
00:06:23,278 --> 00:06:27,520
כי כאשר אני מסביר את PCA, אני לפעמים מצייר דיאגרמות כאלה

103
00:06:27,520 --> 00:06:28,719
וזה נראה דומה לרגרסיה ליניארית.

104
00:06:30,710 --> 00:06:34,000
מתברר ש-PCA אינו רגרסיה ליניארית,

105
00:06:34,000 --> 00:06:38,740
ולמרות דמיון קוסמטי כלשהו, אלה הם למעשה אלגוריתמים שונים לחלוטין.

106
00:06:38,740 --> 00:06:42,811
לו עשינו רגרסיה ליניארית, מה שהיינו עושים היה, בצד שמאל

107
00:06:42,811 --> 00:06:46,590
היינו מנסים לחזות את הערך של איזה משתנה y על פי וקטור תכונות קלט x.

108
00:06:46,590 --> 00:06:51,723
אז ברגרסיה ליניארית אנחנו מתאימים קו ישר

109
00:06:51,723 --> 00:06:56,188
כדי למזער את השגיאה הריבועית בין נקודות לקו הישר הזה.

110
00:06:56,188 --> 00:07:01,240
מה שאנחנו ממזערים יהיה הגודל של הקווים הכחולים האלה.

111
00:07:01,240 --> 00:07:04,390
ושימו לב שאני מצייר את הקווים הכחולים האלה אנכית.

112
00:07:04,390 --> 00:07:07,660
כי הקווים הכחולים האלה הם המרחק האנכי בין הנקודה

113
00:07:07,660 --> 00:07:10,270
לבין הערך החזוי על ידי ההשערה.

114
00:07:10,270 --> 00:07:15,036
לעומת זאת ב- PCA אנחנו מנסים למזער

115
00:07:15,036 --> 00:07:19,729
את גודל הקווים הכחולים האלה שמשורטטים בזווית.

116
00:07:19,729 --> 00:07:22,435
אלה הם המרחקים האורתוגונליים הקצרים ביותר.

117
00:07:22,435 --> 00:07:26,180
המרחק הקצר ביותר בין הנקודה x לבין הקו האדום.

118
00:07:27,370 --> 00:07:33,030
וזה נותן תוצאות שונות מאוד בהתאם למערך הנתונים.

119
00:07:33,030 --> 00:07:37,380
ובאופן כללי יותר, כאשר עושים רגרסיה ליניארית,

120
00:07:37,380 --> 00:07:40,990
יש המשתנה הזה y שמנסים לחזות.

121
00:07:40,990 --> 00:07:44,656
כל מה שעושה רגרסיה ליניארית הוא לקחת את הערכים של x

122
00:07:44,656 --> 00:07:46,266
ולנסות להשתמש בו כדי לחזות את y.

123
00:07:46,266 --> 00:07:48,894
בעוד ב-PCA,

124
00:07:48,894 --> 00:07:52,503
אין משתנה מיוחד y שאנחנו מנסים לחזות.

125
00:07:52,503 --> 00:07:57,390
במקום זאת, יש לנו רשימה של תכונות, x1, x2, וכן הלאה, עד xn,

126
00:07:57,390 --> 00:08:02,005
וכל התכונות האלה מטופלות באופן שווה, ואף אחת מהן איננה מיוחדת.

127
00:08:02,005 --> 00:08:06,515
כדוגמה אחת אחרונה, אם יש לי נתונים תלת מימדיים

128
00:08:06,515 --> 00:08:12,194
ואני רוצה לצמצם את הנתונים מ-3D ל-2D, נאמר שאני רוצה למצוא שני כיוונים,

129
00:08:12,194 --> 00:08:16,271
(1)u ו-(2)u, שעליהם להטיל את הנתונים שלי.

130
00:08:16,271 --> 00:08:20,388
אז יש לי שלוש תכונות, x1, x2, x3,

131
00:08:20,388 --> 00:08:22,501
וכולן מטופלות בצורה זהה.

132
00:08:22,501 --> 00:08:24,878
כל התכונות מטופלות באופן סימטרי

133
00:08:24,878 --> 00:08:28,250
ואין שום משתנה מיוחד שאני מנסה לחזות.

134
00:08:28,250 --> 00:08:31,655
אז PCA איננו רגרסיה ליניארית,

135
00:08:31,655 --> 00:08:36,618
ואף על פי שברמה הקוסמטית הם עשויים להיראות קשורים,

136
00:08:36,618 --> 00:08:40,627
אלה הם למעשה אלגוריתמים שונים מאוד.

137
00:08:40,627 --> 00:08:44,612
אז אני מקווה שאתם כבר מבינים מה PCA עושה.

138
00:08:44,612 --> 00:08:49,286
הוא מנסה למצוא משטח ממימד יותר נמוך ועליו לעשות היטלים של הנתונים

139
00:08:49,286 --> 00:08:52,695
בצורה שתמזער את שגיאת ההיטל האורתוגונלי.

140
00:08:52,695 --> 00:08:55,465
למזער את המרחק האורתוגונלי בין כל נקודה

141
00:08:55,465 --> 00:08:57,845
והמיקום שאליו היא מוטלת.

142
00:08:57,845 --> 00:09:00,415
בסרטון הבא, נתחיל לדבר על איך באמת

143
00:09:00,415 --> 00:09:04,665
למצוא תת-מרחב כזה ממימד נמוך שעליו אפשר לעשות היטלים.