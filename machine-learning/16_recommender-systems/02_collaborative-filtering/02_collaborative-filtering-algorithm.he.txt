בשני הסרטונים האחרונים, דיברנו על הרעיונות של איך, קודם כל, אם יש לנו תכונות עבור סרטים, אפשר להשתמש בהם כדי ללמוד את נתוני הפרמטרים עבור המשתמשים. ושנית, אם יש לנו פרמטרים עבור המשתמשים, אפשר להשתמש בהם כדי ללמוד תכונות עבור סרטים. בסרטון הזה אנחנו ניקח את הרעיונות האלה ונחבר אותם כדי לבנות אלגוריתם סינון שיתופי. אחד הדברים שעליהם עברנו מקודם הוא איך אם יש לנו תכונות עבור הסרטים אז אפשר לפתור את בעיית המזעור הזו ולמצוא את הפרמטרים θ עבור המשתמשים. ואז הראינו גם איך אם נתונים לנו הפרמטרים θ, אפשר גם להשתמש בהם כדי להעריך את התכונות x, וגם את זה אפשר לעשות על ידי פתרון את בעיית המזעור הזו. אז ראינו שאפשר בעצם לעשות איטרציות הלוך ושוב. לאתחל את הפרמטרים אולי באופן אקראי ואז לחשב את θ, ואז לחשב את x, ואז לחשב את θ, ואז לחשב את x. אבל מתברר שיש אלגוריתם יעיל יותר, שבו אין צורך ללכת הלוך ושוב בין הx-ים וה-θ, אלא אפשר לחשב את θ ו-x בו זמנית. והנה הוא. מה שאנחנו נעשה הוא בעצם לקחת את שתי מטרות האופטימיזציה האלה ולחבר אותן למטרה אחת. אז אני אגדיר את מטרת האופטימיזציה החדשה J, שהיא פונקציית עלות, כפונקציה של התכונות x ושל הפרמטרים θ. אלה בעצם שני יעדי האופטימיזציה שהיו לנו מראש, אבל עכשיו הן מאוחדות. אז כדי להסביר את זה, ראשית אני רוצה לציין שהמונח הזה כאן, מונח השגיאה הריבועי הזה, זהה למונח השגיאה הריבועי הזה, והסיכומים נראים קצת שונים, אבל בואו נראה מה בעצם עושים הסיכומים האלה. הסיכום הראשון הוא סכום על כל המשתמשים j ובתוכו סכום על כל הסרטים שדורגו על ידי אותו משתמש. אז זה בעצם סיכום על כל הזוגות (i,j), שמשמעותם סרט i שדורג על ידי משתמש j. סכום מעל j אומר, עבור כל משתמש j, סכום של כל הסרטים שדורגו על ידי המשתמש. והסיכום הזה כאן, עושה את הדברים פשוט בסדר הפוך. הוא אומר עבור כל סרט i, סכום על כל המשתמשים j שדירגו את הסרט, אז שוב הסיכומים האלה הם פשוט סיכומים על כל הזוגות i,j שבהם (r(i,j שווה 1. זה בדיוק סיכום על כל הזוגות של סרט ומשתמש שעבורם יש לנו דירוג. אז שני המונחים האלה כאן הם בדיוק המונח הראשון הזה, רק שכתבתי את הסיכום כאן במפורש, ואני פשוט אומר שזה הסכום של כל הזוגות ij, שבהם (r(i,j שווה 1. אנחנו נגדיר כאן מטרת אופטימיזציה משולבת שאותה אנחנו רוצים למזער על מנת לחשב בו זמנית את x ו-θ. והמונחים האחרים במטרת האופטימיזציה הם זה, שהוא ההסדרה במונחים של θ. ואותו הורדנו הנה למטה, והחלק האחרון הוא הביטוי הזה שהוא ההסדרה של מטרת האופטימיזציה של x ואותו הורדנו לכאן. ולמטרת האופטימיזציה הזו J יש למעשה תכונה מעניינת שאם אנחנו קובעים את x וממזערים ביחס לווקטורים θ, אז אנחנו פותרים בדיוק את הבעיה הזו, ואילו אם נעשה את ההפך ונשמור על θ קבוע וכך נמזער את J רק ביחס ל-x, אז זה נהיה שווה לזה. כי או המונח הזה או המונח הזה הוא קבוע אם אנחנו ממזערים רק את x או בהתאמה רק את θ. אז הנה לנו יעד אופטימיזציה שמאחד את פונקציות העלות גם במונחים של x וגם במונחים של θ. וכדי לנסח בעית אופטימיזציה אחת, מה שנעשה הוא לטפל בפונקצית העלות הזו כפונקציה של התכונות x וגם של הפרמטרים θ של המשתמשים ופשוט נמזער את כל הדבר הזה, כפונקציה גם של הx-ים וגם של θ. ובעצם כל ההבדל בין זה לבין האלגוריתם הקודם שלנו הוא שבמקום לעשות איטרציות הלוך ושוב, כי מקודם דיברנו על מזעור ביחס ל-θ ואז מזעור ביחס x, ואז שוב מזעור ביחס ל-θ, ושוב מזעור ביחס ל-x וכו'. בגרסה החדשה הזו במקום ללכת בצורה טורית בין 2 הקבוצות של פרמטרים x ו-θ, מה שאנחנו עושים הוא למזער ביחס לשתי הקבוצות של פרמטרים בו-זמנית. לבסוף עוד פרט אחד אחרון, כאשר אנו לומדים את התכונות בדרך הזו, בעבר היתה לנו קונוונציה שיש לנו מעין-תכונה x0 שערכה הוא 1 והיא מתאימה להסטה או הטייה. כאשר אנחנו משתמשים בסוג הזה של פורמליזם שבו אנחנו ממש לומדים את התכונות, אנחנו נחסל את הקונוונציה הזאת. אז וקטור התכונות שאותו נלמד, x, יהיה ב-Rn. בעוד שבעבר היו לנו תכונות x ב-Rn+1 כולל את איבר ההטייה. עכשיו כשנפטרנו מ-x0 עכשיו הווקטור הוא ב-Rn. ובדומה לכך מכיוון שגם הפרמטרים θ הם באותו מימד, עכשיו גם θ יהיה ב-Rn כי אם אין x0, אז גם אין צורך בפרמטר דמה θ0. והסיבה שאנחנו מוותרים על הקונוונציה הזאת היא כי אנחנו עכשיו לומדים את כל התכונות, נכון? אז אין שום צורך להכניס מין תכונה כזו שתמיד שווה 1. כי אם האלגוריתם באמת ירצה תכונה שתמיד שווה ל-1, הוא יכול להחליט ללמוד תכונה כזו בעצמו. אם האלגוריתם ירצה, הוא יכול להגדיר תכונה x1 שווה 1. אז אין שום צורך בתכונה של x0 = 1, לאלגוריתם עכשיו יש הגמישות פשוט ללמוד את זה בעצמו. אז בואו נרשום הכל על נקי. הנה אלגוריתם הסינון השיתופי שלנו. בהתחלה אנחנו מאתחלים את x ו-θ לערכים אקראיים קטנים. וזה קצת דומה לאימון של רשת עצבית, שגם שם התחלנו באתחול של כל הפרמטרים של הרשת העצבית לערכים אקראיים קטנים. אחר כך אנחנו ממזערים את פונקציית העלות באמצעות ירידה במדרון או באחד האלגוריתמים המתקדמים לאופטימיזציה. אז אם נעשה נגזרות נמצא שהירידה במדרון משתמשת בנגזרות האלה, המונח הזה כאן הוא הנגזרת החלקית של פונקציית העלות, אני לא הולך לכתוב את זה, ביחס לתכונה x(i)k והמונח הזה כאן גם הוא הנגזרת החלקית של פונקציית העלות ביחס לפרמטר θ שאנו ממזערים. ולתזכורת, בנוסחה הזו, מכיוון שכבר אין לנו את ה-x0 שווה 1 הזה, עכשיו כש-x הוא ב-Rn ו-θ הוא ב-Rn. בפורמליזם החדש הזה, אנחנו עושים הסדרה על כל אחד מהפרמטרים θ, וכל אחד מהפרמטרים xn. אין עוד המקרה המיוחד הזה של θ0, שהיה מוסדר בצורה שונה, או לא מוסדר לעומת הפרמטרים θ1 עד θn. אז עכשיו אין עוד θ0, ולכן בעדכונים האלה, אין לנו ביטוי מיוחד עבור k שווה 0. אז אנחנו משתמשים בירידה הדרגתית למזער את פונקציית המחיר J ביחס לתכונות x וביחס לפרמטרים θ. ועכשיו, בהינתן משתמש, אם למשתמש יש איזשהו וקטור פרמטרים θ, ואם יש סרט עם איזשהם תכונות x שלמדנו, אז אנחנו מנבאים שהמשתמש הזה ייתן לסרט הזה דירוג של θᵀx. או כדי לכתוב את זה בצורה מלאה, אנחנו אומרים שאם המשתמש j עדיין לא דירג את הסרט i, אז אנחנו מנבאים שהמשתמש j ידרג את הסרט i לפי ⁽θ⁽ʲ⁾ᵀx⁽ⁱ אז זה אלגוריתם הסינון השיתופי ואם תיישמו את האלגוריתם הזה תקבלו כנראה אלגוריתם די בסדר, שיצליח בתקווה ללמוד בו זמנית גם תכונות טובות של כל הסרטים, וגם ללמוד פרמטרים עבור כל המשתמשים וניתן לקוות שהוא ייתן תחזיות טובות למדי על איך ידרגו משתמשים שונים סרטים שונים שהם עדיין לא דירגו.