1
00:00:00,240 --> 00:00:01,690
בשני הסרטונים האחרונים,

2
00:00:01,820 --> 00:00:02,990
דיברנו על הרעיונות של

3
00:00:03,140 --> 00:00:04,570
איך, קודם כל, אם

4
00:00:04,780 --> 00:00:06,210
יש לנו תכונות עבור סרטים,

5
00:00:06,920 --> 00:00:08,610
אפשר להשתמש בהם כדי ללמוד את נתוני הפרמטרים עבור המשתמשים.

6
00:00:09,490 --> 00:00:11,400
ושנית, אם יש לנו פרמטרים עבור המשתמשים,

7
00:00:11,920 --> 00:00:13,570
אפשר להשתמש בהם כדי ללמוד תכונות עבור סרטים.

8
00:00:14,480 --> 00:00:15,550
בסרטון הזה אנחנו

9
00:00:15,650 --> 00:00:16,670
ניקח את הרעיונות האלה

10
00:00:16,850 --> 00:00:18,130
ונחבר אותם כדי לבנות

11
00:00:18,280 --> 00:00:20,130
אלגוריתם סינון שיתופי.

12
00:00:21,250 --> 00:00:22,450
אחד הדברים שעליהם עברנו

13
00:00:22,520 --> 00:00:23,640
מקודם הוא איך

14
00:00:23,680 --> 00:00:24,510
אם יש לנו תכונות עבור

15
00:00:24,600 --> 00:00:25,740
הסרטים אז אפשר לפתור

16
00:00:26,070 --> 00:00:27,590
את בעיית המזעור הזו ולמצוא

17
00:00:27,950 --> 00:00:30,010
את הפרמטרים θ עבור המשתמשים.

18
00:00:30,730 --> 00:00:32,260
ואז הראינו גם

19
00:00:32,640 --> 00:00:33,960
איך אם

20
00:00:34,360 --> 00:00:37,440
נתונים לנו הפרמטרים θ,

21
00:00:38,080 --> 00:00:38,990
אפשר גם להשתמש בהם

22
00:00:39,170 --> 00:00:40,800
כדי להעריך את התכונות x,

23
00:00:40,870 --> 00:00:42,980
וגם את זה אפשר לעשות על ידי פתרון את בעיית המזעור הזו.

24
00:00:44,310 --> 00:00:45,720
אז ראינו

25
00:00:45,880 --> 00:00:47,360
שאפשר בעצם לעשות איטרציות הלוך ושוב.

26
00:00:47,870 --> 00:00:50,230
לאתחל את הפרמטרים אולי באופן אקראי

27
00:00:50,510 --> 00:00:51,350
ואז לחשב את θ,

28
00:00:51,780 --> 00:00:52,690
ואז לחשב את x, ואז לחשב את θ,

29
00:00:52,870 --> 00:00:54,330
ואז לחשב את x. אבל

30
00:00:54,420 --> 00:00:55,220
מתברר שיש

31
00:00:55,400 --> 00:00:56,760
אלגוריתם יעיל יותר, שבו

32
00:00:56,980 --> 00:00:57,910
אין צורך ללכת הלוך ושוב

33
00:00:58,110 --> 00:00:59,700
בין הx-ים

34
00:00:59,730 --> 00:01:00,670
וה-θ, אלא אפשר לחשב

35
00:01:01,300 --> 00:01:04,250
את θ ו-x בו זמנית.

36
00:01:05,160 --> 00:01:06,310
והנה הוא. מה שאנחנו נעשה הוא בעצם לקחת

37
00:01:06,600 --> 00:01:08,990
את שתי מטרות האופטימיזציה האלה

38
00:01:09,130 --> 00:01:10,640
ולחבר אותן למטרה אחת.

39
00:01:11,550 --> 00:01:12,590
אז אני אגדיר

40
00:01:12,730 --> 00:01:15,010
את מטרת האופטימיזציה החדשה J,

41
00:01:15,250 --> 00:01:16,540
שהיא פונקציית עלות,

42
00:01:16,640 --> 00:01:17,630
כפונקציה של התכונות

43
00:01:18,050 --> 00:01:19,150
x ושל

44
00:01:19,790 --> 00:01:20,750
הפרמטרים θ.

45
00:01:21,660 --> 00:01:23,050
אלה בעצם שני יעדי האופטימיזציה

46
00:01:23,520 --> 00:01:24,920
שהיו לנו מראש, אבל עכשיו הן מאוחדות.

47
00:01:26,270 --> 00:01:27,760
אז כדי

48
00:01:28,060 --> 00:01:31,140
להסביר את זה, ראשית אני רוצה לציין

49
00:01:31,400 --> 00:01:33,420
שהמונח הזה כאן, מונח

50
00:01:33,820 --> 00:01:35,490
השגיאה הריבועי הזה, זהה

51
00:01:35,920 --> 00:01:39,250
למונח השגיאה הריבועי הזה,

52
00:01:39,760 --> 00:01:40,880
והסיכומים נראים קצת

53
00:01:41,050 --> 00:01:42,940
שונים, אבל בואו נראה מה בעצם עושים הסיכומים האלה.

54
00:01:43,800 --> 00:01:45,090
הסיכום הראשון הוא סכום

55
00:01:45,480 --> 00:01:48,280
על כל המשתמשים j

56
00:01:48,380 --> 00:01:50,590
ובתוכו סכום על כל הסרטים שדורגו על ידי אותו משתמש.

57
00:01:51,890 --> 00:01:53,240
אז זה בעצם סיכום על כל

58
00:01:53,470 --> 00:01:55,950
הזוגות (i,j), שמשמעותם

59
00:01:56,510 --> 00:01:57,830
סרט i שדורג על ידי משתמש j.

60
00:01:58,550 --> 00:01:59,960
סכום מעל j אומר, עבור

61
00:02:00,150 --> 00:02:01,520
כל משתמש j, סכום של

62
00:02:01,740 --> 00:02:03,110
כל הסרטים שדורגו על ידי המשתמש.

63
00:02:04,250 --> 00:02:07,340
והסיכום הזה כאן, עושה את הדברים פשוט בסדר הפוך.

64
00:02:07,630 --> 00:02:08,710
הוא אומר עבור כל סרט

65
00:02:09,050 --> 00:02:11,140
i, סכום על כל

66
00:02:11,340 --> 00:02:12,480
המשתמשים j

67
00:02:12,690 --> 00:02:14,580
שדירגו את הסרט, אז שוב

68
00:02:14,690 --> 00:02:16,100
הסיכומים האלה הם

69
00:02:16,220 --> 00:02:18,150
פשוט סיכומים על כל הזוגות

70
00:02:18,930 --> 00:02:21,150
i,j שבהם

71
00:02:21,440 --> 00:02:24,620
(r(i,j שווה 1.

72
00:02:24,660 --> 00:02:26,580
זה בדיוק סיכום על כל

73
00:02:27,180 --> 00:02:29,810
הזוגות של סרט ומשתמש שעבורם יש לנו דירוג.

74
00:02:30,840 --> 00:02:32,230
אז שני המונחים האלה

75
00:02:32,600 --> 00:02:34,740
כאן הם בדיוק

76
00:02:34,930 --> 00:02:36,460
המונח הראשון הזה,

77
00:02:36,500 --> 00:02:38,310
רק שכתבתי את הסיכום כאן במפורש,

78
00:02:39,310 --> 00:02:40,290
ואני פשוט אומר שזה הסכום

79
00:02:40,580 --> 00:02:42,290
של כל הזוגות ij, שבהם

80
00:02:42,540 --> 00:02:45,060
(r(i,j שווה 1.

81
00:02:45,310 --> 00:02:46,800
אנחנו

82
00:02:46,940 --> 00:02:48,790
נגדיר כאן

83
00:02:49,130 --> 00:02:51,410
מטרת אופטימיזציה משולבת

84
00:02:51,670 --> 00:02:53,290
שאותה אנחנו רוצים למזער על מנת

85
00:02:53,550 --> 00:02:55,700
לחשב בו זמנית את x ו-θ.

86
00:02:56,970 --> 00:02:58,040
והמונחים האחרים

87
00:02:58,070 --> 00:03:00,250
במטרת האופטימיזציה הם זה,

88
00:03:00,570 --> 00:03:02,870
שהוא ההסדרה במונחים של θ.

89
00:03:03,770 --> 00:03:05,830
ואותו הורדנו הנה למטה,

90
00:03:06,290 --> 00:03:08,190
והחלק האחרון הוא

91
00:03:08,900 --> 00:03:10,690
הביטוי הזה שהוא

92
00:03:10,850 --> 00:03:12,970
ההסדרה של מטרת האופטימיזציה של x

93
00:03:13,170 --> 00:03:16,180
ואותו הורדנו לכאן.

94
00:03:16,500 --> 00:03:18,020
ולמטרת האופטימיזציה הזו

95
00:03:18,720 --> 00:03:19,730
J יש למעשה תכונה מעניינת

96
00:03:20,240 --> 00:03:20,950
שאם אנחנו קובעים

97
00:03:21,410 --> 00:03:23,070
את x

98
00:03:23,260 --> 00:03:25,490
וממזערים ביחס לווקטורים θ,

99
00:03:25,670 --> 00:03:27,040
אז אנחנו פותרים בדיוק את הבעיה הזו,

100
00:03:27,840 --> 00:03:28,450
ואילו אם נעשה

101
00:03:28,620 --> 00:03:29,590
את ההפך

102
00:03:29,690 --> 00:03:31,310
ונשמור על θ קבוע וכך נמזער

103
00:03:31,670 --> 00:03:32,650
את J רק ביחס

104
00:03:32,750 --> 00:03:34,920
ל-x, אז זה נהיה שווה לזה.

105
00:03:35,230 --> 00:03:36,780
כי או המונח הזה

106
00:03:37,060 --> 00:03:38,860
או המונח הזה הוא קבוע

107
00:03:38,970 --> 00:03:40,510
אם אנחנו ממזערים רק את x או בהתאמה רק את θ.

108
00:03:40,920 --> 00:03:43,680
אז הנה לנו יעד

109
00:03:44,640 --> 00:03:46,840
אופטימיזציה שמאחד את

110
00:03:47,440 --> 00:03:50,230
פונקציות העלות גם במונחים של x וגם במונחים של θ.

111
00:03:51,620 --> 00:03:53,050
וכדי

112
00:03:53,470 --> 00:03:54,750
לנסח

113
00:03:55,090 --> 00:03:56,130
בעית אופטימיזציה אחת, מה שנעשה

114
00:03:56,280 --> 00:03:57,590
הוא לטפל

115
00:03:58,430 --> 00:03:59,850
בפונקצית העלות הזו

116
00:03:59,880 --> 00:04:00,890
כפונקציה של התכונות

117
00:04:01,410 --> 00:04:02,540
x וגם של הפרמטרים

118
00:04:03,180 --> 00:04:05,020
θ של המשתמשים

119
00:04:05,140 --> 00:04:06,570
ופשוט נמזער את כל הדבר הזה,

120
00:04:06,740 --> 00:04:07,830
כפונקציה גם של

121
00:04:08,120 --> 00:04:10,210
הx-ים וגם של θ.

122
00:04:11,300 --> 00:04:12,400
ובעצם כל ההבדל

123
00:04:12,540 --> 00:04:13,800
בין זה לבין

124
00:04:14,160 --> 00:04:15,650
האלגוריתם הקודם שלנו הוא

125
00:04:15,980 --> 00:04:17,340
שבמקום לעשות איטרציות הלוך ושוב,

126
00:04:17,840 --> 00:04:20,110
כי מקודם דיברנו על מזעור

127
00:04:20,420 --> 00:04:22,130
ביחס ל-θ ואז מזעור ביחס x,

128
00:04:22,260 --> 00:04:23,370
ואז שוב מזעור ביחס ל-θ,

129
00:04:23,900 --> 00:04:25,270
ושוב מזעור ביחס ל-x וכו'.

130
00:04:26,130 --> 00:04:28,090
בגרסה החדשה הזו במקום

131
00:04:28,560 --> 00:04:30,020
ללכת בצורה טורית בין

132
00:04:30,220 --> 00:04:31,880
2 הקבוצות של פרמטרים x ו-θ,

133
00:04:32,180 --> 00:04:32,940
מה שאנחנו עושים הוא

134
00:04:33,230 --> 00:04:34,600
למזער ביחס

135
00:04:34,780 --> 00:04:36,410
לשתי הקבוצות של פרמטרים בו-זמנית.

136
00:04:39,750 --> 00:04:41,290
לבסוף עוד פרט אחד אחרון,

137
00:04:42,030 --> 00:04:44,380
כאשר אנו לומדים את התכונות בדרך הזו,

138
00:04:45,110 --> 00:04:46,410
בעבר היתה לנו

139
00:04:46,840 --> 00:04:49,290
קונוונציה

140
00:04:49,470 --> 00:04:50,540
שיש לנו מעין-תכונה x0

141
00:04:50,740 --> 00:04:52,940
שערכה הוא 1 והיא מתאימה להסטה או הטייה.

142
00:04:54,140 --> 00:04:55,530
כאשר אנחנו משתמשים בסוג

143
00:04:55,760 --> 00:04:57,790
הזה של פורמליזם שבו אנחנו ממש לומדים את התכונות,

144
00:04:58,300 --> 00:05:00,200
אנחנו נחסל את הקונוונציה הזאת.

145
00:05:01,400 --> 00:05:04,220
אז וקטור התכונות שאותו נלמד, x, יהיה ב-Rn.

146
00:05:05,430 --> 00:05:06,650
בעוד שבעבר היו לנו

147
00:05:06,810 --> 00:05:09,770
תכונות x ב-Rn+1 כולל את איבר ההטייה.

148
00:05:10,390 --> 00:05:13,390
עכשיו כשנפטרנו מ-x0 עכשיו הווקטור הוא ב-Rn.

149
00:05:14,880 --> 00:05:16,520
ובדומה לכך מכיוון

150
00:05:16,590 --> 00:05:17,780
שגם הפרמטרים θ הם

151
00:05:17,850 --> 00:05:19,260
באותו מימד, עכשיו גם

152
00:05:19,510 --> 00:05:21,010
θ יהיה ב-Rn

153
00:05:21,540 --> 00:05:23,340
כי אם אין

154
00:05:23,710 --> 00:05:24,580
x0, אז גם אין צורך

155
00:05:25,370 --> 00:05:26,880
בפרמטר דמה θ0.

156
00:05:27,960 --> 00:05:28,880
והסיבה שאנחנו מוותרים

157
00:05:29,160 --> 00:05:30,390
על הקונוונציה הזאת היא כי

158
00:05:31,010 --> 00:05:32,610
אנחנו עכשיו לומדים את כל התכונות, נכון?

159
00:05:32,820 --> 00:05:34,280
אז אין שום צורך

160
00:05:34,420 --> 00:05:36,650
להכניס מין תכונה כזו שתמיד שווה 1.

161
00:05:37,170 --> 00:05:38,310
כי אם האלגוריתם באמת ירצה

162
00:05:38,600 --> 00:05:39,450
תכונה שתמיד שווה

163
00:05:40,060 --> 00:05:41,830
ל-1, הוא יכול להחליט ללמוד תכונה כזו בעצמו.

164
00:05:42,290 --> 00:05:43,430
אם האלגוריתם ירצה,

165
00:05:43,720 --> 00:05:45,330
הוא יכול להגדיר תכונה x1 שווה 1.

166
00:05:45,670 --> 00:05:47,010
אז אין שום צורך

167
00:05:47,260 --> 00:05:48,300
בתכונה של x0 = 1,

168
00:05:48,440 --> 00:05:50,060
לאלגוריתם עכשיו יש

169
00:05:50,340 --> 00:05:55,890
הגמישות פשוט ללמוד את זה בעצמו. אז בואו

170
00:05:56,420 --> 00:05:58,410
נרשום הכל על נקי. הנה

171
00:05:58,780 --> 00:05:59,910
אלגוריתם הסינון השיתופי שלנו.

172
00:06:01,460 --> 00:06:02,330
בהתחלה אנחנו

173
00:06:03,010 --> 00:06:05,580
מאתחלים את x

174
00:06:05,820 --> 00:06:07,290
ו-θ לערכים אקראיים קטנים.

175
00:06:08,450 --> 00:06:09,200
וזה קצת דומה

176
00:06:09,310 --> 00:06:11,700
לאימון של רשת עצבית, שגם שם

177
00:06:11,720 --> 00:06:14,240
התחלנו באתחול של כל הפרמטרים של הרשת העצבית לערכים אקראיים קטנים.

178
00:06:16,640 --> 00:06:17,730
אחר כך אנחנו

179
00:06:17,950 --> 00:06:20,110
ממזערים את פונקציית העלות

180
00:06:20,500 --> 00:06:23,360
באמצעות ירידה במדרון או באחד האלגוריתמים המתקדמים לאופטימיזציה.

181
00:06:24,610 --> 00:06:25,890
אז אם נעשה נגזרות

182
00:06:26,020 --> 00:06:27,460
נמצא שהירידה במדרון

183
00:06:27,590 --> 00:06:29,320
משתמשת בנגזרות האלה,

184
00:06:29,630 --> 00:06:31,160
המונח הזה כאן הוא

185
00:06:31,660 --> 00:06:33,890
הנגזרת החלקית של פונקציית העלות,

186
00:06:35,140 --> 00:06:35,940
אני לא הולך לכתוב את זה,

187
00:06:36,110 --> 00:06:37,860
ביחס לתכונה

188
00:06:38,070 --> 00:06:40,020
x(i)k והמונח הזה כאן

189
00:06:41,020 --> 00:06:42,430
גם הוא

190
00:06:43,030 --> 00:06:44,660
הנגזרת החלקית של

191
00:06:44,730 --> 00:06:46,480
פונקציית העלות ביחס לפרמטר

192
00:06:46,930 --> 00:06:48,950
θ שאנו ממזערים.

193
00:06:50,210 --> 00:06:51,410
ולתזכורת,

194
00:06:51,760 --> 00:06:52,920
בנוסחה הזו, מכיוון

195
00:06:53,130 --> 00:06:54,760
שכבר אין לנו את ה-x0 שווה

196
00:06:54,970 --> 00:06:56,740
1 הזה, עכשיו

197
00:06:57,010 --> 00:07:00,010
כש-x הוא ב-Rn ו-θ הוא ב-Rn.

198
00:07:01,480 --> 00:07:03,100
בפורמליזם החדש הזה, אנחנו עושים הסדרה

199
00:07:03,760 --> 00:07:05,220
על כל אחד מהפרמטרים θ, וכל אחד מהפרמטרים xn.

200
00:07:07,400 --> 00:07:09,060
אין עוד המקרה

201
00:07:09,480 --> 00:07:11,850
המיוחד הזה של θ0, שהיה

202
00:07:12,210 --> 00:07:13,760
מוסדר בצורה שונה, או

203
00:07:13,860 --> 00:07:15,440
לא מוסדר לעומת

204
00:07:15,560 --> 00:07:17,650
הפרמטרים θ1 עד θn.

205
00:07:18,370 --> 00:07:19,710
אז עכשיו אין עוד

206
00:07:20,070 --> 00:07:21,150
θ0, ולכן

207
00:07:21,400 --> 00:07:22,450
בעדכונים האלה, אין לנו

208
00:07:22,700 --> 00:07:24,080
ביטוי מיוחד עבור k שווה 0.

209
00:07:26,070 --> 00:07:27,230
אז אנחנו משתמשים בירידה הדרגתית

210
00:07:27,740 --> 00:07:28,710
למזער את פונקציית המחיר

211
00:07:29,090 --> 00:07:30,260
J ביחס

212
00:07:30,390 --> 00:07:32,000
לתכונות x וביחס לפרמטרים θ.

213
00:07:33,160 --> 00:07:35,050
ועכשיו, בהינתן

214
00:07:35,140 --> 00:07:36,320
משתמש, אם למשתמש

215
00:07:36,570 --> 00:07:38,920
יש איזשהו וקטור פרמטרים θ,

216
00:07:39,410 --> 00:07:40,540
ואם יש סרט עם

217
00:07:40,690 --> 00:07:41,980
איזשהם תכונות x שלמדנו,

218
00:07:42,580 --> 00:07:43,720
אז אנחנו מנבאים

219
00:07:43,970 --> 00:07:44,940
שהמשתמש הזה ייתן

220
00:07:45,030 --> 00:07:46,200
לסרט הזה דירוג

221
00:07:47,010 --> 00:07:48,780
של θᵀx.

222
00:07:48,860 --> 00:07:50,370
או כדי לכתוב את זה בצורה מלאה,

223
00:07:50,640 --> 00:07:52,250
אנחנו אומרים שאם המשתמש

224
00:07:52,630 --> 00:07:53,780
j עדיין לא

225
00:07:54,010 --> 00:07:55,980
דירג את הסרט i, אז

226
00:07:56,170 --> 00:07:57,300
אנחנו מנבאים

227
00:07:58,150 --> 00:07:59,120
שהמשתמש j

228
00:07:59,710 --> 00:08:01,420
ידרג את הסרט i לפי 

229
00:08:02,300 --> 00:08:04,230
⁽θ⁽ʲ⁾ᵀx⁽ⁱ

230
00:08:06,650 --> 00:08:08,010
אז זה אלגוריתם

231
00:08:08,810 --> 00:08:10,170
הסינון השיתופי ואם

232
00:08:10,310 --> 00:08:12,230
תיישמו את האלגוריתם הזה תקבלו כנראה

233
00:08:12,730 --> 00:08:14,080
אלגוריתם די בסדר, שיצליח בתקווה

234
00:08:15,060 --> 00:08:16,770
ללמוד בו זמנית

235
00:08:17,110 --> 00:08:18,460
גם תכונות טובות של כל הסרטים,

236
00:08:18,570 --> 00:08:19,890
וגם ללמוד פרמטרים עבור כל

237
00:08:20,050 --> 00:08:21,290
המשתמשים וניתן לקוות שהוא ייתן

238
00:08:21,440 --> 00:08:23,060
תחזיות טובות למדי על איך

239
00:08:23,290 --> 00:08:25,890
ידרגו משתמשים שונים סרטים שונים שהם עדיין לא דירגו.