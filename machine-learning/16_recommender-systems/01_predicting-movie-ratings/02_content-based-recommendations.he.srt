1
00:00:01,400 --> 00:00:05,490
בסרטון האחרון שוחחנו על בעיית מערכות המלצה שבהן

2
00:00:05,490 --> 00:00:09,830
לדוגמה, יש לנו סדרה של סרטים ויש לנו קבוצה של משתמשים,

3
00:00:09,830 --> 00:00:13,100
שכל אחד מהם דירג איזו תת-קבוצה של סרטים.

4
00:00:13,100 --> 00:00:16,450
הם דירגו את הסרטים מאחד עד חמישה כוכבים או מאפס עד חמישה כוכבים.

5
00:00:16,450 --> 00:00:19,210
ומה שאנחנו רוצים לעשות הוא להסתכל על המשתמשים האלה

6
00:00:19,210 --> 00:00:23,580
ולחזות איך הם היו מדרגים סרטים אחרים שהם עדיין לא דרגו.

7
00:00:23,580 --> 00:00:26,828
בסרטון הזה אני רוצה לדבר על הגישה הראשונה שלנו לבניית

8
00:00:26,828 --> 00:00:27,990
מערכת המלצה.

9
00:00:27,990 --> 00:00:31,480
גישה זו נקראת המלצות מבוססות תוכן.

10
00:00:31,480 --> 00:00:35,560
הנה מערכת הנתונים שלנו, ולפני כן אני רק מזכיר קצת סימונים,

11
00:00:35,560 --> 00:00:40,224
השתמשתי ב-nᵤ כדי לציין את מספר המשתמשים ולכן כאן הוא שווה ל-4,

12
00:00:40,224 --> 00:00:45,179
וב-nₘ לציין את מספר הסרטים, ויש לנו 5 סרטים.

13
00:00:47,280 --> 00:00:51,350
אז איך נוכל לחזות את ערכם של הערכים החסרים האלה?

14
00:00:52,520 --> 00:00:57,649
נניח שיש לנו קבוצה של תכונות עבור כל אחד מהסרטים האלה.

15
00:00:57,649 --> 00:00:59,720
בפרט, נניח

16
00:00:59,720 --> 00:01:04,081
שעבור כל אחד מהסרטים יש לנו שתי תכונות שנציין אותן ב-x₁ ו-x₂.

17
00:01:04,081 --> 00:01:08,426
כאשר x₁ מודד את המידה שבה הסרט הוא סרט רומנטי

18
00:01:08,426 --> 00:01:12,830
ו-x₂ מודד את המידה שבה הסרט הוא סרט פעולה.

19
00:01:12,830 --> 00:01:18,700
אז אם ניקח את הסרט "אהבה סופסוף", אנחנו יודעים שהוא דירוג 0.9 בסולם הרומנטיקה.

20
00:01:18,700 --> 00:01:21,720
זהו סרט רומנטי מאוד, אבל הוא אפס בסולם הפעולה.

21
00:01:21,720 --> 00:01:24,227
אין כמעט שום אקשן בסרט הזה.

22
00:01:24,227 --> 00:01:29,310
ו"רומנטיקה לנצח" הוא 1.0 בסולם הרומנטיקה, ו-0.01 בסולם הפעולה.

23
00:01:29,310 --> 00:01:33,140
אני לא יודע, אולי יש שם איזו תאונת מכוניות זניחה או משהו.

24
00:01:33,140 --> 00:01:35,448
אז יש קצת פעולה.

25
00:01:35,448 --> 00:01:41,439
בואו נדלג הלאה ונעשה את "חרבות נגד קראטה", אולי יש לו דירוג 0 בסולם הרומנטיקה

26
00:01:41,439 --> 00:01:44,888
כי הוא לא רומנטי בכלל אבל הרבה פעולה.

27
00:01:44,888 --> 00:01:46,234
ו"מרדף מכוניות אינסופי",

28
00:01:46,234 --> 00:01:50,740
שוב אולי יש קצת רומנטיקה בסרט הזה אבל בעיקר פעולה.

29
00:01:50,740 --> 00:01:54,570
וחזרה ל"גורים חמודים של אהבה" בעיקר סרט רומנטי בלי שום אקשן.

30
00:01:55,995 --> 00:01:58,335
אז אם יש לנו תכונות כאלה,

31
00:01:58,335 --> 00:02:02,415
אז כל סרט יכול להיות מיוצג בעזרת וקטור תכונות.

32
00:02:02,415 --> 00:02:03,597
בואו ניקח סרט אחד.

33
00:02:03,597 --> 00:02:07,499
אז בואו נקרא לסרטים האלה 1, 2, 3, 4 ו-5.

34
00:02:07,499 --> 00:02:13,280
לסרט הראשון, "אהבה סופסוף", שתי התכונות הן 0.9 ו-0.

35
00:02:13,280 --> 00:02:16,078
אלה התכונות x₁ ו-x₂.

36
00:02:16,078 --> 00:02:21,519
ובואו נוסיף עוד תכונה כרגיל, שהיא תכונת ההיסט או ההֲטָיָה שלנו, x₀=1.

37
00:02:21,519 --> 00:02:26,980
אז לסיכום של זה, יש לנו תכונה (x(1.

38
00:02:26,980 --> 00:02:30,600
ה-1 כאן מציין שזה וקטור התכונות עבור הסרט הראשון שלי,

39
00:02:30,600 --> 00:02:32,700
והערך הראשון בוקטור התכונות הזה שווה ל-1.

40
00:02:32,700 --> 00:02:35,740
ה-1 הראשון כאן הוא ההיסט.

41
00:02:35,740 --> 00:02:40,410
ואז המאפיין הבא שלנו הוא 0.90 ככה.

42
00:02:40,410 --> 00:02:44,412
עבור הסרט "אהבה סופסוף" בנינו את וקטור התכונות (x(1,

43
00:02:44,412 --> 00:02:50,203
עבור הסרט "רומנטיקה לנצח" נבנה לנו וקטור תכונות (x(2, וכן הלאה,

44
00:02:50,203 --> 00:02:56,104
ועבור הסרט "חרבות נגד קראטה" אנחנו נגדיר וקטור תכונות (x(5.

45
00:02:56,104 --> 00:02:59,897
כמו כן, כדי להישאר עקבי עם מערכת הסימונים שהגדרנו מקודם,

46
00:02:59,897 --> 00:03:04,730
אנחנו נגדיר את n להיות מספר התכונות בהתעלם מתכונת ההיסט x₀.

47
00:03:04,730 --> 00:03:08,320
אז n שווה 2 כי יש לנו שתי תכונות x₁ ו-x₂

48
00:03:08,320 --> 00:03:13,860
שמסמנות לנו את מידת הרומנטיקה ואת מידת הפעולה של כל סרט.

49
00:03:13,860 --> 00:03:19,290
עכשיו על מנת לעשות כאן תחזיות, דבר אחד שאפשר לעשות הוא

50
00:03:19,290 --> 00:03:23,180
לטפל בחיזוי הדירוגים של כל משתמש

51
00:03:23,180 --> 00:03:26,200
כבעיית רגרסיה ליניארית נפרדת.

52
00:03:26,200 --> 00:03:29,460
אז באופן ספציפי, נניח שעבור כל משתמש j, אנחנו

53
00:03:29,460 --> 00:03:33,550
נלמד את וקטור הפרמטרים (θ(j, שיהיה במקרה שלנו ב-R3.

54
00:03:33,550 --> 00:03:37,940
או באופן כללי יותר, (θ(j יהיה ב-(R(n+1,

55
00:03:37,940 --> 00:03:41,710
כאשר n הוא מספר התכונות לא כולל תכונת ההיסט.

56
00:03:41,710 --> 00:03:46,360
ואנחנו נעשה תחזית שהמשתמש j ידרג את הסרט i על ידי המכפלה

57
00:03:46,360 --> 00:03:51,860
הפנימית בין וקטור הפרמטרים θ לבין וקטור התכונות (x(i.

58
00:03:51,860 --> 00:03:54,346
אז בואו ניקח דוגמה ספציפית.

59
00:03:54,346 --> 00:04:01,130
בואו ניקח את המשתמש 1, שהוא במקרה המשתמשת אליס.

60
00:04:01,130 --> 00:04:05,730
וקשור לאליס יהיה איזה וקטור פרמטרים (θ(1.

61
00:04:05,730 --> 00:04:07,440
והמשתמש השני שלנו, בוב,

62
00:04:07,440 --> 00:04:10,780
אליו משויך וקטור פרמטרים שונה (θ(2.

63
00:04:10,780 --> 00:04:14,248
קרול תהיה משויכת לווקטור פרמטרים שונה, (θ(3

64
00:04:14,248 --> 00:04:16,460
ואילו דייב משויך לווקטור פרמטרים (θ(4.

65
00:04:17,490 --> 00:04:19,930
אז נניח שאנחנו רוצים לעשות חיזוי על

66
00:04:19,930 --> 00:04:24,170
מה אליס תחשוב על הסרט "גורים חמודים של אהבה".

67
00:04:24,170 --> 00:04:29,102
ובכן לסרט הזה כזכור יש איזשהו וקטור פרמטרים (x(3

68
00:04:29,102 --> 00:04:33,209
ואנחנו יודעים ש-(x(3 שווה ל-1,

69
00:04:33,209 --> 00:04:38,160
שהוא מונח ההיסט שלנו, ואז 0.99 ואז 0.

70
00:04:38,160 --> 00:04:43,370
ונניח לשם הדוגמה הזו, נניח כי איכשהו מצאנו כבר

71
00:04:43,370 --> 00:04:45,510
וקטור פרמטרים (θ(1 עבור אליס.

72
00:04:45,510 --> 00:04:49,230
אנחנו נבין מאוחר יותר איך הצלחנו לבנות את וקטור הפרמטרים הזה.

73
00:04:50,370 --> 00:04:54,130
אבל בואו רק נניח עכשיו שאיזה אלגוריתם למידה לא ידוע

74
00:04:54,130 --> 00:04:59,150
למד את וקטור הפרמטרים (θ(1 והוא שווה ל-0,5,0.

75
00:04:59,150 --> 00:05:06,010
אז החיזוי שלנו עבור הערך הזה יהיה

76
00:05:06,010 --> 00:05:11,470
(θ(1, שהוא וקטור הפרמטרים של אליס, משוחלפת, כפול (x(3

77
00:05:11,470 --> 00:05:16,250
שהוא וקטור התכונות של הסרט "גורים חמודים של אהבה", סרט מספר 3.

78
00:05:16,250 --> 00:05:22,640
אז המכפלה הפנימית בין שני הוקטורים האלה תהיה 5 כפול 0.99,

79
00:05:22,640 --> 00:05:27,390
שזה שווה ל-4.95.

80
00:05:27,390 --> 00:05:32,496
ולכן התחזית שלי לערך הזה תהיה 4.95.

81
00:05:32,496 --> 00:05:36,911
ואולי זה נראה כמו ערך סביר אם אכן

82
00:05:36,911 --> 00:05:38,930
זהו וקטור הפרמטרים שלנו (θ(1.

83
00:05:38,930 --> 00:05:43,794
אז כל מה שאנחנו עושים כאן הוא להחיל עותק שונה של

84
00:05:43,794 --> 00:05:48,506
רגרסיה ליניארית עבור כל אחד מהמשתמשים, ואנחנו אומרים שמה שאליס עושה הוא

85
00:05:48,506 --> 00:05:53,294
שלאליס יש איזה וקטור פרמטרים (θ(1 שהיא ואנחנו משתמשים בו כדי לנבא את

86
00:05:53,294 --> 00:05:58,220
הדירוג שלה כפונקציה של עד כמה הסרט רומנטי ועד כמה מלא אקשן.

87
00:05:58,220 --> 00:06:02,469
וגם בוב וקרול ודייב, לכל אחד מהם יש פונקציה ליניארית שונה

88
00:06:02,469 --> 00:06:07,326
של הרומנטיקה והפעולה, או מידת הרומנטיקה ומידת הפעולה בסרט,

89
00:06:07,326 --> 00:06:11,187
וככה אנחנו מתכוונים לחזות את דירוג הכוכבים שלהם.

90
00:06:14,902 --> 00:06:19,320
באופן רשמי יותר, הנה כך אנחנו יכולים לנסח את הבעיה.

91
00:06:19,320 --> 00:06:23,615
הסימון שלנו הוא ש-(r(i,j שווה ל-1 אם המשתמש דרג את הסרט i

92
00:06:23,615 --> 00:06:28,399
ו-(y(i,j הוא הדירוג של הסרט, אם הדירוג קיים.

93
00:06:29,580 --> 00:06:31,700
כלומר, אם המשתמש הזה אכן דירג את הסרט הזה.

94
00:06:32,850 --> 00:06:37,780
בנוסף, בשקופית הקודמת הגדרנו גם את (θ(j האלה, שהם וקטורי פרמטרים

95
00:06:37,780 --> 00:06:41,640
עבור המשתמש j, ואת (x(i, שהוא וקטור תכונות עבור סרט מסוים i.

96
00:06:41,640 --> 00:06:47,084
עבור כל משתמש וכל סרט, אנו צופים את הדירוג כדלקמן.

97
00:06:47,084 --> 00:06:53,351
תנו לי להציג רק לרגע עוד סימון אחד נוסף (m(j.

98
00:06:53,351 --> 00:06:56,789
אנו נשתמש ב-(m(j כדי לציין את מספר הסרטים שדירג המשתמש j.

99
00:06:56,789 --> 00:06:59,470
אנחנו צריכים את הסימון הזה רק בשביל השקף הזה.

100
00:06:59,470 --> 00:07:04,560
עכשיו כדי ללמוד את וקטור הפרמטרים עבור (θ(j, הנה איך עושים את זה.

101
00:07:04,560 --> 00:07:06,930
זוהי בעצם בעית רגרסיה ליניארית.

102
00:07:06,930 --> 00:07:11,380
אז מה שאנחנו יכולים לעשות הוא פשוט לבחור וקטור פרמטרים (θ(j כך

103
00:07:11,380 --> 00:07:16,080
שהערכים החזויים כאן קרובים ככל האפשר לערכים

104
00:07:16,080 --> 00:07:19,930
שראינו בקבוצת האימון שלנו ולערכים שראינו בנתונים שלנו.

105
00:07:19,930 --> 00:07:22,350
אז בואו נכתוב את זה.

106
00:07:22,350 --> 00:07:25,675
כדי ללמוד את וקטור הפרמטרים (θ(j,

107
00:07:25,675 --> 00:07:30,900
בואו נמזער על וקטור הפרמטרים (θ(j את הסכום,

108
00:07:31,920 --> 00:07:36,300
ואני רוצה לסכם כאן את כל הסרטים שהמשתמש j דרג,

109
00:07:36,300 --> 00:07:39,130
אז אנחנו כותבים את זה כסכום על כל הערכים של i.

110
00:07:39,130 --> 00:07:43,900
שבהם (נקודתיים) (r(i,j שווה 1,

111
00:07:43,900 --> 00:07:48,020
הדרך לקרוא את הדבר הזה היא שהסיכום הזה הוא סיכום

112
00:07:48,020 --> 00:07:51,090
על כל הערכים של i כך ש-(r(i,j שווה 1,

113
00:07:51,090 --> 00:07:55,130
דהיינו, סיכום על כל הסרטים שהמשתמש j דירג,

114
00:07:56,210 --> 00:08:04,500
של הערך של (θ(j)ᵀx(i.

115
00:08:04,500 --> 00:08:11,860
זו התחזית של הדירוג של המשתמש j של הסרט i, פחות (y(i,j.

116
00:08:11,860 --> 00:08:15,160
דהיינו הדירוג שנצפה בפועל, בריבוע.

117
00:08:15,160 --> 00:08:22,230
ואז כל מה שנשאר הוא לחלק את זה במספר הסרטים שהמשתמש j למעשה דירג.

118
00:08:22,230 --> 00:08:24,950
אז בואו נחלק ב-(2m(j.

119
00:08:24,950 --> 00:08:28,260
אז זה בדיוק כמו רגרסיה של סכום ריבועים מינימלי.

120
00:08:28,260 --> 00:08:32,152
זה בדיוק כמו רגרסיה ליניארית, שבה אנחנו רוצים לבחור את וקטור הפרמטרים

121
00:08:32,152 --> 00:08:35,339
(θ(j כדי למזער את הביטוי הזה של סכום ריבועי השגיאה.

122
00:08:36,370 --> 00:08:41,690
ואם תרצו, אפשר גם להוסיף את מונח ההסדרה, פלוס λ

123
00:08:41,690 --> 00:08:48,930
חלקי 2m וזה בעצם (2m(j כי יש לנו (m(j דוגמאות.

124
00:08:48,930 --> 00:08:52,100
המשתמש j דירג את מספר הסרטים הזה, אז זה מספר

125
00:08:52,100 --> 00:08:55,610
הנתונים כדי להתאים בעזרתם את הפרמטרים של (θ(j.

126
00:08:55,610 --> 00:09:00,330
ותרשו לי להוסיף את מונח ההסדרה הרגיל שלי כאן

127
00:09:00,330 --> 00:09:02,970
שהוא ⁽θₖ⁽ʲ בריבוע.

128
00:09:02,970 --> 00:09:09,890
כרגיל, הסכום הזה הוא מ-k שווה 1 עד n, אז כאן, (θ(ʲ יהיה

129
00:09:09,890 --> 00:09:15,360
וקטור n+1 ממדי, כמו שבדוגמה המוקדמת שלנו n היה שווה ל-2.

130
00:09:15,360 --> 00:09:19,468
אבל באופן כללי יותר n הוא מספר התכונות שיש לנו לכל סרט.

131
00:09:19,468 --> 00:09:22,684
וכרגיל, אנחנו לא מפעילים הסדרה על (θ(0.

132
00:09:22,684 --> 00:09:24,660
אנחנו לא מסדירים את תנאי ההסטה.

133
00:09:24,660 --> 00:09:26,132
הסכום הוא מ-k שווה 1 עד n.

134
00:09:27,910 --> 00:09:32,740
אז אם נמזער את זה כפונקציה של (θ(ʲ נקבל פתרון טוב,

135
00:09:32,740 --> 00:09:36,510
נקבל הערכה טובה למדי של וקטור הפרמטרים (θ(ʲ

136
00:09:36,510 --> 00:09:40,800
שאיתו ניתן לעשות תחזיות עבור דירוגי הסרטים של המשתמש j.

137
00:09:40,800 --> 00:09:44,520
עבור מערכות המלצות, אני עומד לשנות קצת את הסימונים האלה.

138
00:09:44,520 --> 00:09:49,590
כדי לפשט את המתמטיקה בהמשך, אני איפטר מהמונח הזה ⁽m⁽ʲ.

139
00:09:49,590 --> 00:09:50,930
זה פשוט קבוע, נכון?

140
00:09:50,930 --> 00:09:54,600
אז אני יכול למחוק אותו מבלי לשנות את הערך של (θ(j שאנחנו

141
00:09:54,600 --> 00:09:56,010
מקבלים מהאופטימיזציה הזו.

142
00:09:56,010 --> 00:09:59,790
אז תדמיינו את כל המשוואה הזאת, אנחנו לוקחים את כל הביטוי הזה

143
00:09:59,790 --> 00:10:02,460
ומכפילים אותו ב-(m(j, וכך מתפטרים מהקבוע.

144
00:10:02,460 --> 00:10:06,680
וכאשר נמזער את זה, אנחנו עדיין אמורים לקבל אותו ערך של (θ(j כמו קודם.

145
00:10:06,680 --> 00:10:10,390
אז כדי לחזור על מה שכתבנו בשקופית הקודמת,

146
00:10:10,390 --> 00:10:12,180
הנה מטרת האופטימיזציה שלנו.

147
00:10:12,180 --> 00:10:14,900
כדי ללמוד את (θ(j שהוא הפרמטרים

148
00:10:14,900 --> 00:10:19,850
עבור המשתמש j, אנחנו נמזער מעל (θ(j את מטרת האופטימיזציה הזו.

149
00:10:19,850 --> 00:10:25,980
זה מונח השגיאה בריבוע הרגיל שלנו וזה מונח ההסדרה שלנו.

150
00:10:25,980 --> 00:10:28,740
עכשיו כמובן בבניית מערכת המלצות,

151
00:10:28,740 --> 00:10:31,370
אנחנו לא רוצים רק ללמוד פרמטרים עבור משתמש יחיד.

152
00:10:31,370 --> 00:10:34,450
אנחנו רוצים ללמוד את הפרמטרים עבור כל המשתמשים שלנו.

153
00:10:34,450 --> 00:10:38,920
יש לנו nᵤ משתמשים ואנחנו רוצים ללמוד את הפרמטרים האלה של כולם.

154
00:10:38,920 --> 00:10:43,158
אז מה שנעשה הוא לקחת את מטרת האופטימיזציה

155
00:10:43,158 --> 00:10:45,582
ופשוט להוסיף עליה עוד סיכום של הסיכומים.

156
00:10:45,582 --> 00:10:50,349
אז הביטוי הזה כאן עם החצי כאן, הוא בדיוק

157
00:10:50,349 --> 00:10:52,571
אותו הדבר כמו מה שהיה לנו למעלה.

158
00:10:52,571 --> 00:10:57,232
אלא שעכשיו, במקום לעשות את זה רק עבור משתמש מסוים j,

159
00:10:57,232 --> 00:11:00,708
אנחנו נסכם את מטרת האופטימיזציה על כל המשתמשים

160
00:11:00,708 --> 00:11:06,491
ואז נמזער את המטרה הכוללת של אופטימיזציה, נמזער את העלות הכוללת.

161
00:11:06,491 --> 00:11:11,423
וכאשר נמזער את זה כפונקציה של (θ(1), θ(2,

162
00:11:11,423 --> 00:11:17,440
עד (θ(nᵤ, נקבל וקטור פרמטרים נפרד עבור כל משתמש.

163
00:11:17,440 --> 00:11:21,380
ואז אפשר להשתמש בהם כדי ליצור תחזיות עבור כל אחד מהמשתמשים,

164
00:11:21,380 --> 00:11:22,739
עבור כל nᵤ המשתמשים שלנו.

165
00:11:24,540 --> 00:11:29,484
אז עכשיו שוב על נקי, זו למעלה היתה מטרת האופטימיזציה שלנו.

166
00:11:29,484 --> 00:11:36,089
וכדי לתת לדבר הזה שם, אני אקרא לזה ((J((θ(1), ..., (θ(nᵤ.

167
00:11:36,089 --> 00:11:40,425
אז הפונקציה J כרגיל היא מטרת האופטימיזציה שלנו, מה שאנחנו מנסים למזער.

168
00:11:41,465 --> 00:11:45,645
בשלב הבא, על מנת לעשות בפועל את המזעור, אם נפתח

169
00:11:45,645 --> 00:11:49,035
את הנוסחה של שלב העדכון בירידה במדרון, אלה הם המשוואות שנקבל.

170
00:11:49,035 --> 00:11:53,245
אז לוקחים את ⁽θₖ⁽ʲ, ומחסרים α,

171
00:11:53,245 --> 00:11:56,865
שהוא שיעור הלמידה, כפול הביטויים האלה מימין.

172
00:11:56,865 --> 00:12:00,830
המקרה שונה במקצת בין אם k שווה 0 ובין אם k לא שווה 0.

173
00:12:00,830 --> 00:12:05,542
כי ביטוי ההסדרה שלנו כאן מסדיר רק את הערכים של ⁽θₖ⁽ʲ

174
00:12:05,542 --> 00:12:09,350
עבור המקרים ש-k לא שווה ל-0, כי אנחנו לא מסדירים את (θ(0, אז

175
00:12:09,350 --> 00:12:14,670
יש עדכונים שונים במקצת למקרים של k שווה ל-0 ושל k שונה מ-0.

176
00:12:14,670 --> 00:12:16,690
והביטוי הזה כאן, למשל,

177
00:12:16,690 --> 00:12:21,120
הוא פשוט הנגזרת החלקית ביחס לפרמטר,

178
00:12:23,260 --> 00:12:28,410
זה של מטרת האופטימיזציה.

179
00:12:28,410 --> 00:12:32,740
נכון, אז זו פשוט ירידה במדרון

180
00:12:32,740 --> 00:12:35,670
ואני כבר חישבתי את הנגזרות והיצבתי אותם כאן.

181
00:12:37,930 --> 00:12:43,190
ואם הירידה במדרון הזו נראית מאד דומה למה שהיה לנו

182
00:12:43,190 --> 00:12:44,130
עבור רגרסיה ליניארית.

183
00:12:44,130 --> 00:12:48,005
הסיבה לכך היא שזה בעצם אותו דבר כמו רגרסיה ליניארית.

184
00:12:48,005 --> 00:12:52,753
ההבדל הקטן היחיד הוא שברגרסיה ליניארית היה לנו

185
00:12:52,753 --> 00:12:57,421
ה-1 חלקי m, או בעצם 1 חלקי (m(j.

186
00:12:57,421 --> 00:13:00,880
אבל קודם, כאשר בנינו את מטרת האופטימיזציה,

187
00:13:00,880 --> 00:13:04,480
נפטרנו מזה, ולכן אין לנו את המונח 1 חלקי m.

188
00:13:04,480 --> 00:13:09,678
אבל חוץ מזה, זה באמת הסכום על דוגמאות האימון של השגיאה

189
00:13:09,678 --> 00:13:13,390
כפול (x(k ועוד מונח ההסדרה,

190
00:13:13,390 --> 00:13:18,060
מה שמונח ההסדרה תורם לנגזרת.

191
00:13:18,060 --> 00:13:21,640
אז אם נשתמש כאן בירידה במדרון נוכל

192
00:13:21,640 --> 00:13:25,130
למזער את פונקצית העלות J וכך ללמוד את כל הפרמטרים.

193
00:13:25,130 --> 00:13:27,420
ואם נשתמש בנוסחאות האלה עבור הנגזרות החלקיות אם נרצה,

194
00:13:27,420 --> 00:13:30,740
אפשר גם להציב אותם באלגוריתם אופטימיזציה מתקדם יותר,

195
00:13:30,740 --> 00:13:33,650
כמו מדרון מוטה או LBFGS או מה שיהיה.

196
00:13:33,650 --> 00:13:36,120
ולהשתמש גם בו כדי לנסות למזער את פונקצית העלות J.

197
00:13:37,380 --> 00:13:41,910
אז אני מקווה שעכשיו אתם יודעים איך אפשר ליישם למעשה גרסה של רגרסיה ליניארית

198
00:13:41,910 --> 00:13:46,350
על מנת לחזות דירוג של סרטים שונים על ידי משתמשים שונים.

199
00:13:46,350 --> 00:13:50,000
האלגוריתם המסוים זה נקרא המלצות מבוססות תוכן, או

200
00:13:50,000 --> 00:13:51,530
גישה מבוססת תוכן,

201
00:13:51,530 --> 00:13:56,038
מכיוון שאנו מניחים שיש לנו תכונות זמינות עבור הסרטים השונים.

202
00:13:56,038 --> 00:14:00,060
יש לנו תכונות שבעזרתם אנחנו מבינים מהו התוכן של הסרטים האלה,

203
00:14:00,060 --> 00:14:03,020
כמה רומנטי הסרט, כמה אקשן יש בסרט.

204
00:14:03,020 --> 00:14:06,500
ואנחנו באמת משתמשים בתכונות של התוכן של הסרטים

205
00:14:06,500 --> 00:14:07,359
כדי לייצר את התחזיות.

206
00:14:08,380 --> 00:14:11,870
אבל עבור סרטים רבים, אין לנו בעצם תכונות כאלה.

207
00:14:11,870 --> 00:14:14,810
או אולי קשה מאוד להשיג תכונות כאלה

208
00:14:14,810 --> 00:14:18,650
עבור כל הסרטים שלנו, או עבור כל הפריטים שאנחנו מנסים למכור.

209
00:14:18,650 --> 00:14:22,610
אז בסרטון הבא, נתחיל לדבר על גישה למערכות המלצה

210
00:14:22,610 --> 00:14:27,360
שאיננה מבוססת תוכן ואינה מניחה שיש איזה מישהו

211
00:14:27,360 --> 00:14:30,720
שנותן לנו את כל התכונות האלו עבור כל הסרטים בקבוצת הנתונים שלנו .