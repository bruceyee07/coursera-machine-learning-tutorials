בסרטון האחרון שוחחנו על בעיית מערכות המלצה שבהן לדוגמה, יש לנו סדרה של סרטים ויש לנו קבוצה של משתמשים, שכל אחד מהם דירג איזו תת-קבוצה של סרטים. הם דירגו את הסרטים מאחד עד חמישה כוכבים או מאפס עד חמישה כוכבים. ומה שאנחנו רוצים לעשות הוא להסתכל על המשתמשים האלה ולחזות איך הם היו מדרגים סרטים אחרים שהם עדיין לא דרגו. בסרטון הזה אני רוצה לדבר על הגישה הראשונה שלנו לבניית מערכת המלצה. גישה זו נקראת המלצות מבוססות תוכן. הנה מערכת הנתונים שלנו, ולפני כן אני רק מזכיר קצת סימונים, השתמשתי ב-nᵤ כדי לציין את מספר המשתמשים ולכן כאן הוא שווה ל-4, וב-nₘ לציין את מספר הסרטים, ויש לנו 5 סרטים. אז איך נוכל לחזות את ערכם של הערכים החסרים האלה? נניח שיש לנו קבוצה של תכונות עבור כל אחד מהסרטים האלה. בפרט, נניח שעבור כל אחד מהסרטים יש לנו שתי תכונות שנציין אותן ב-x₁ ו-x₂. כאשר x₁ מודד את המידה שבה הסרט הוא סרט רומנטי ו-x₂ מודד את המידה שבה הסרט הוא סרט פעולה. אז אם ניקח את הסרט "אהבה סופסוף", אנחנו יודעים שהוא דירוג 0.9 בסולם הרומנטיקה. זהו סרט רומנטי מאוד, אבל הוא אפס בסולם הפעולה. אין כמעט שום אקשן בסרט הזה. ו"רומנטיקה לנצח" הוא 1.0 בסולם הרומנטיקה, ו-0.01 בסולם הפעולה. אני לא יודע, אולי יש שם איזו תאונת מכוניות זניחה או משהו. אז יש קצת פעולה. בואו נדלג הלאה ונעשה את "חרבות נגד קראטה", אולי יש לו דירוג 0 בסולם הרומנטיקה כי הוא לא רומנטי בכלל אבל הרבה פעולה. ו"מרדף מכוניות אינסופי", שוב אולי יש קצת רומנטיקה בסרט הזה אבל בעיקר פעולה. וחזרה ל"גורים חמודים של אהבה" בעיקר סרט רומנטי בלי שום אקשן. אז אם יש לנו תכונות כאלה, אז כל סרט יכול להיות מיוצג בעזרת וקטור תכונות. בואו ניקח סרט אחד. אז בואו נקרא לסרטים האלה 1, 2, 3, 4 ו-5. לסרט הראשון, "אהבה סופסוף", שתי התכונות הן 0.9 ו-0. אלה התכונות x₁ ו-x₂. ובואו נוסיף עוד תכונה כרגיל, שהיא תכונת ההיסט או ההֲטָיָה שלנו, x₀=1. אז לסיכום של זה, יש לנו תכונה (x(1. ה-1 כאן מציין שזה וקטור התכונות עבור הסרט הראשון שלי, והערך הראשון בוקטור התכונות הזה שווה ל-1. ה-1 הראשון כאן הוא ההיסט. ואז המאפיין הבא שלנו הוא 0.90 ככה. עבור הסרט "אהבה סופסוף" בנינו את וקטור התכונות (x(1, עבור הסרט "רומנטיקה לנצח" נבנה לנו וקטור תכונות (x(2, וכן הלאה, ועבור הסרט "חרבות נגד קראטה" אנחנו נגדיר וקטור תכונות (x(5. כמו כן, כדי להישאר עקבי עם מערכת הסימונים שהגדרנו מקודם, אנחנו נגדיר את n להיות מספר התכונות בהתעלם מתכונת ההיסט x₀. אז n שווה 2 כי יש לנו שתי תכונות x₁ ו-x₂ שמסמנות לנו את מידת הרומנטיקה ואת מידת הפעולה של כל סרט. עכשיו על מנת לעשות כאן תחזיות, דבר אחד שאפשר לעשות הוא לטפל בחיזוי הדירוגים של כל משתמש כבעיית רגרסיה ליניארית נפרדת. אז באופן ספציפי, נניח שעבור כל משתמש j, אנחנו נלמד את וקטור הפרמטרים (θ(j, שיהיה במקרה שלנו ב-R3. או באופן כללי יותר, (θ(j יהיה ב-(R(n+1, כאשר n הוא מספר התכונות לא כולל תכונת ההיסט. ואנחנו נעשה תחזית שהמשתמש j ידרג את הסרט i על ידי המכפלה הפנימית בין וקטור הפרמטרים θ לבין וקטור התכונות (x(i. אז בואו ניקח דוגמה ספציפית. בואו ניקח את המשתמש 1, שהוא במקרה המשתמשת אליס. וקשור לאליס יהיה איזה וקטור פרמטרים (θ(1. והמשתמש השני שלנו, בוב, אליו משויך וקטור פרמטרים שונה (θ(2. קרול תהיה משויכת לווקטור פרמטרים שונה, (θ(3 ואילו דייב משויך לווקטור פרמטרים (θ(4. אז נניח שאנחנו רוצים לעשות חיזוי על מה אליס תחשוב על הסרט "גורים חמודים של אהבה". ובכן לסרט הזה כזכור יש איזשהו וקטור פרמטרים (x(3 ואנחנו יודעים ש-(x(3 שווה ל-1, שהוא מונח ההיסט שלנו, ואז 0.99 ואז 0. ונניח לשם הדוגמה הזו, נניח כי איכשהו מצאנו כבר וקטור פרמטרים (θ(1 עבור אליס. אנחנו נבין מאוחר יותר איך הצלחנו לבנות את וקטור הפרמטרים הזה. אבל בואו רק נניח עכשיו שאיזה אלגוריתם למידה לא ידוע למד את וקטור הפרמטרים (θ(1 והוא שווה ל-0,5,0. אז החיזוי שלנו עבור הערך הזה יהיה (θ(1, שהוא וקטור הפרמטרים של אליס, משוחלפת, כפול (x(3 שהוא וקטור התכונות של הסרט "גורים חמודים של אהבה", סרט מספר 3. אז המכפלה הפנימית בין שני הוקטורים האלה תהיה 5 כפול 0.99, שזה שווה ל-4.95. ולכן התחזית שלי לערך הזה תהיה 4.95. ואולי זה נראה כמו ערך סביר אם אכן זהו וקטור הפרמטרים שלנו (θ(1. אז כל מה שאנחנו עושים כאן הוא להחיל עותק שונה של רגרסיה ליניארית עבור כל אחד מהמשתמשים, ואנחנו אומרים שמה שאליס עושה הוא שלאליס יש איזה וקטור פרמטרים (θ(1 שהיא ואנחנו משתמשים בו כדי לנבא את הדירוג שלה כפונקציה של עד כמה הסרט רומנטי ועד כמה מלא אקשן. וגם בוב וקרול ודייב, לכל אחד מהם יש פונקציה ליניארית שונה של הרומנטיקה והפעולה, או מידת הרומנטיקה ומידת הפעולה בסרט, וככה אנחנו מתכוונים לחזות את דירוג הכוכבים שלהם. באופן רשמי יותר, הנה כך אנחנו יכולים לנסח את הבעיה. הסימון שלנו הוא ש-(r(i,j שווה ל-1 אם המשתמש דרג את הסרט i ו-(y(i,j הוא הדירוג של הסרט, אם הדירוג קיים. כלומר, אם המשתמש הזה אכן דירג את הסרט הזה. בנוסף, בשקופית הקודמת הגדרנו גם את (θ(j האלה, שהם וקטורי פרמטרים עבור המשתמש j, ואת (x(i, שהוא וקטור תכונות עבור סרט מסוים i. עבור כל משתמש וכל סרט, אנו צופים את הדירוג כדלקמן. תנו לי להציג רק לרגע עוד סימון אחד נוסף (m(j. אנו נשתמש ב-(m(j כדי לציין את מספר הסרטים שדירג המשתמש j. אנחנו צריכים את הסימון הזה רק בשביל השקף הזה. עכשיו כדי ללמוד את וקטור הפרמטרים עבור (θ(j, הנה איך עושים את זה. זוהי בעצם בעית רגרסיה ליניארית. אז מה שאנחנו יכולים לעשות הוא פשוט לבחור וקטור פרמטרים (θ(j כך שהערכים החזויים כאן קרובים ככל האפשר לערכים שראינו בקבוצת האימון שלנו ולערכים שראינו בנתונים שלנו. אז בואו נכתוב את זה. כדי ללמוד את וקטור הפרמטרים (θ(j, בואו נמזער על וקטור הפרמטרים (θ(j את הסכום, ואני רוצה לסכם כאן את כל הסרטים שהמשתמש j דרג, אז אנחנו כותבים את זה כסכום על כל הערכים של i. שבהם (נקודתיים) (r(i,j שווה 1, הדרך לקרוא את הדבר הזה היא שהסיכום הזה הוא סיכום על כל הערכים של i כך ש-(r(i,j שווה 1, דהיינו, סיכום על כל הסרטים שהמשתמש j דירג, של הערך של (θ(j)ᵀx(i. זו התחזית של הדירוג של המשתמש j של הסרט i, פחות (y(i,j. דהיינו הדירוג שנצפה בפועל, בריבוע. ואז כל מה שנשאר הוא לחלק את זה במספר הסרטים שהמשתמש j למעשה דירג. אז בואו נחלק ב-(2m(j. אז זה בדיוק כמו רגרסיה של סכום ריבועים מינימלי. זה בדיוק כמו רגרסיה ליניארית, שבה אנחנו רוצים לבחור את וקטור הפרמטרים (θ(j כדי למזער את הביטוי הזה של סכום ריבועי השגיאה. ואם תרצו, אפשר גם להוסיף את מונח ההסדרה, פלוס λ חלקי 2m וזה בעצם (2m(j כי יש לנו (m(j דוגמאות. המשתמש j דירג את מספר הסרטים הזה, אז זה מספר הנתונים כדי להתאים בעזרתם את הפרמטרים של (θ(j. ותרשו לי להוסיף את מונח ההסדרה הרגיל שלי כאן שהוא ⁽θₖ⁽ʲ בריבוע. כרגיל, הסכום הזה הוא מ-k שווה 1 עד n, אז כאן, (θ(ʲ יהיה וקטור n+1 ממדי, כמו שבדוגמה המוקדמת שלנו n היה שווה ל-2. אבל באופן כללי יותר n הוא מספר התכונות שיש לנו לכל סרט. וכרגיל, אנחנו לא מפעילים הסדרה על (θ(0. אנחנו לא מסדירים את תנאי ההסטה. הסכום הוא מ-k שווה 1 עד n. אז אם נמזער את זה כפונקציה של (θ(ʲ נקבל פתרון טוב, נקבל הערכה טובה למדי של וקטור הפרמטרים (θ(ʲ שאיתו ניתן לעשות תחזיות עבור דירוגי הסרטים של המשתמש j. עבור מערכות המלצות, אני עומד לשנות קצת את הסימונים האלה. כדי לפשט את המתמטיקה בהמשך, אני איפטר מהמונח הזה ⁽m⁽ʲ. זה פשוט קבוע, נכון? אז אני יכול למחוק אותו מבלי לשנות את הערך של (θ(j שאנחנו מקבלים מהאופטימיזציה הזו. אז תדמיינו את כל המשוואה הזאת, אנחנו לוקחים את כל הביטוי הזה ומכפילים אותו ב-(m(j, וכך מתפטרים מהקבוע. וכאשר נמזער את זה, אנחנו עדיין אמורים לקבל אותו ערך של (θ(j כמו קודם. אז כדי לחזור על מה שכתבנו בשקופית הקודמת, הנה מטרת האופטימיזציה שלנו. כדי ללמוד את (θ(j שהוא הפרמטרים עבור המשתמש j, אנחנו נמזער מעל (θ(j את מטרת האופטימיזציה הזו. זה מונח השגיאה בריבוע הרגיל שלנו וזה מונח ההסדרה שלנו. עכשיו כמובן בבניית מערכת המלצות, אנחנו לא רוצים רק ללמוד פרמטרים עבור משתמש יחיד. אנחנו רוצים ללמוד את הפרמטרים עבור כל המשתמשים שלנו. יש לנו nᵤ משתמשים ואנחנו רוצים ללמוד את הפרמטרים האלה של כולם. אז מה שנעשה הוא לקחת את מטרת האופטימיזציה ופשוט להוסיף עליה עוד סיכום של הסיכומים. אז הביטוי הזה כאן עם החצי כאן, הוא בדיוק אותו הדבר כמו מה שהיה לנו למעלה. אלא שעכשיו, במקום לעשות את זה רק עבור משתמש מסוים j, אנחנו נסכם את מטרת האופטימיזציה על כל המשתמשים ואז נמזער את המטרה הכוללת של אופטימיזציה, נמזער את העלות הכוללת. וכאשר נמזער את זה כפונקציה של (θ(1), θ(2, עד (θ(nᵤ, נקבל וקטור פרמטרים נפרד עבור כל משתמש. ואז אפשר להשתמש בהם כדי ליצור תחזיות עבור כל אחד מהמשתמשים, עבור כל nᵤ המשתמשים שלנו. אז עכשיו שוב על נקי, זו למעלה היתה מטרת האופטימיזציה שלנו. וכדי לתת לדבר הזה שם, אני אקרא לזה ((J((θ(1), ..., (θ(nᵤ. אז הפונקציה J כרגיל היא מטרת האופטימיזציה שלנו, מה שאנחנו מנסים למזער. בשלב הבא, על מנת לעשות בפועל את המזעור, אם נפתח את הנוסחה של שלב העדכון בירידה במדרון, אלה הם המשוואות שנקבל. אז לוקחים את ⁽θₖ⁽ʲ, ומחסרים α, שהוא שיעור הלמידה, כפול הביטויים האלה מימין. המקרה שונה במקצת בין אם k שווה 0 ובין אם k לא שווה 0. כי ביטוי ההסדרה שלנו כאן מסדיר רק את הערכים של ⁽θₖ⁽ʲ עבור המקרים ש-k לא שווה ל-0, כי אנחנו לא מסדירים את (θ(0, אז יש עדכונים שונים במקצת למקרים של k שווה ל-0 ושל k שונה מ-0. והביטוי הזה כאן, למשל, הוא פשוט הנגזרת החלקית ביחס לפרמטר, זה של מטרת האופטימיזציה. נכון, אז זו פשוט ירידה במדרון ואני כבר חישבתי את הנגזרות והיצבתי אותם כאן. ואם הירידה במדרון הזו נראית מאד דומה למה שהיה לנו עבור רגרסיה ליניארית. הסיבה לכך היא שזה בעצם אותו דבר כמו רגרסיה ליניארית. ההבדל הקטן היחיד הוא שברגרסיה ליניארית היה לנו ה-1 חלקי m, או בעצם 1 חלקי (m(j. אבל קודם, כאשר בנינו את מטרת האופטימיזציה, נפטרנו מזה, ולכן אין לנו את המונח 1 חלקי m. אבל חוץ מזה, זה באמת הסכום על דוגמאות האימון של השגיאה כפול (x(k ועוד מונח ההסדרה, מה שמונח ההסדרה תורם לנגזרת. אז אם נשתמש כאן בירידה במדרון נוכל למזער את פונקצית העלות J וכך ללמוד את כל הפרמטרים. ואם נשתמש בנוסחאות האלה עבור הנגזרות החלקיות אם נרצה, אפשר גם להציב אותם באלגוריתם אופטימיזציה מתקדם יותר, כמו מדרון מוטה או LBFGS או מה שיהיה. ולהשתמש גם בו כדי לנסות למזער את פונקצית העלות J. אז אני מקווה שעכשיו אתם יודעים איך אפשר ליישם למעשה גרסה של רגרסיה ליניארית על מנת לחזות דירוג של סרטים שונים על ידי משתמשים שונים. האלגוריתם המסוים זה נקרא המלצות מבוססות תוכן, או גישה מבוססת תוכן, מכיוון שאנו מניחים שיש לנו תכונות זמינות עבור הסרטים השונים. יש לנו תכונות שבעזרתם אנחנו מבינים מהו התוכן של הסרטים האלה, כמה רומנטי הסרט, כמה אקשן יש בסרט. ואנחנו באמת משתמשים בתכונות של התוכן של הסרטים כדי לייצר את התחזיות. אבל עבור סרטים רבים, אין לנו בעצם תכונות כאלה. או אולי קשה מאוד להשיג תכונות כאלה עבור כל הסרטים שלנו, או עבור כל הפריטים שאנחנו מנסים למכור. אז בסרטון הבא, נתחיל לדבר על גישה למערכות המלצה שאיננה מבוססת תוכן ואינה מניחה שיש איזה מישהו שנותן לנו את כל התכונות האלו עבור כל הסרטים בקבוצת הנתונים שלנו .