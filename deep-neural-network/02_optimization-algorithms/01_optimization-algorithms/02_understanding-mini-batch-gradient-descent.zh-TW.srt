1
00:00:00,320 --> 00:00:04,590
在上一部影片，你看到了如何利用小批次梯度下降法

2
00:00:04,590 --> 00:00:08,370
就讓你開始有進度，開始踏出梯度下降的步驟，即使

3
00:00:08,370 --> 00:00:11,960
你才第一次處理你的訓練資料、才進行到一半。

4
00:00:11,960 --> 00:00:16,282
在這部影片，你會學到更多實作梯度下降的細節

5
00:00:16,282 --> 00:00:19,896
更能了解這在做什麼，以及為什麼這樣有用。

6
00:00:19,896 --> 00:00:24,481
在批次梯度下降法，每次的迭代 (iteration)
你都會跑過整個訓練集

7
00:00:24,481 --> 00:00:29,380
你能期待成本值在每一次的迭代都會往下降

8
00:00:30,660 --> 00:00:33,390
所以如果我們畫出成本函數 J

9
00:00:33,390 --> 00:00:37,500
每次迭代時的一個函數，那他在每個迭代都會下降。

10
00:00:37,500 --> 00:00:40,730
如果他在某次迭代往上升，那麼一定有地方錯了

11
00:00:40,730 --> 00:00:43,250
也許是你的學習率太大了。

12
00:00:43,250 --> 00:00:48,090
但是在小批次梯度下降法，如果你把成本函數的歷程畫出來

13
00:00:48,090 --> 00:00:51,660
他不見得每次迭代都會減少。

14
00:00:51,660 --> 00:00:56,822
特別是，每次迭代的運算，你處理的是

15
00:00:56,822 --> 00:01:01,425
某批 X{t}, Y{t}

16
00:01:01,425 --> 00:01:05,888
所以如果你畫出成本函數 J{t}

17
00:01:05,888 --> 00:01:11,490
他只是用 X{t}, Y{t} 來計算的

18
00:01:11,490 --> 00:01:17,170
那麼每次的迭代，你如同在訓練不一樣的訓練集

19
00:01:17,170 --> 00:01:19,380
或者說不同一批的小量資料

20
00:01:19,380 --> 00:01:20,770
所以當你畫成本函數 J

21
00:01:20,770 --> 00:01:23,310
你更有可能看到像這樣的東西

22
00:01:23,310 --> 00:01:27,479
他應該有往下的趨勢，但是會有一點噪音、浮動。

23
00:01:30,554 --> 00:01:35,692
所以當你用小批次梯度下降法訓練的時候，
如果你畫出 J{t}

24
00:01:35,692 --> 00:01:40,670
經過多個 epoch 後，你可以期待看到這樣的曲線。

25
00:01:40,670 --> 00:01:44,284
所以如果沒有每次迭代都下降，也沒有關係，

26
00:01:44,284 --> 00:01:46,783
但他應該會有往下的趨勢，

27
00:01:46,783 --> 00:01:51,281
而會有一點噪音的原因是，有可能 X{1}

28
00:01:51,281 --> 00:01:56,527
Y{1} 是一批比較容易的資料，所以你的成本值會低一些，

29
00:01:56,527 --> 00:02:02,057
但接下來有可能運氣不好，X{2}, Y{2} 是不好解決的一批資料

30
00:02:02,057 --> 00:02:04,356
甚至有可能有標錯的資料

31
00:02:04,356 --> 00:02:06,780
所以成本值會比較高一些

32
00:02:06,780 --> 00:02:09,511
所以這是為什麼你會看到這些振動

33
00:02:09,511 --> 00:02:13,277
當你在跑小批次梯度下降、畫出成本的時候。

34
00:02:13,277 --> 00:02:18,070
現在呢，有一個你必須要挑的參數，
是一批小量資料的筆數

35
00:02:18,070 --> 00:02:22,894
我們說過 m 是整個訓練集的大小，
考慮某個極端情況，

36
00:02:26,544 --> 00:02:31,056
如果小批資料的大小等於 m，這就變成批次梯度下降法

37
00:02:36,056 --> 00:02:41,068
在這極端的情況下，你只會有一批資料 X{1},

38
00:02:41,068 --> 00:02:45,720
Y{1} 而這小批資料等同於你的一整個訓練資料

39
00:02:45,720 --> 00:02:49,830
所以把小批資料筆數設成 m，你會得到批次梯度下降。

40
00:02:49,830 --> 00:02:57,669
另一種極端的情況，是小批資料的大小等於 1

41
00:02:59,752 --> 00:03:03,238
這會給你一個演算法叫「隨機梯度下降法」
(stochastic gradient descent)

42
00:03:07,385 --> 00:03:16,076
這時每一筆資料自己是一個小批資料

43
00:03:18,429 --> 00:03:24,172
所以在這種情況，你首先看第一小批資料 X{1}, Y{1}

44
00:03:24,172 --> 00:03:29,682
但是當你的小批資料大小為 1 的時候，
其實就是你的第一筆訓練資料

45
00:03:29,682 --> 00:03:34,620
你拿第一筆訓練資料來執行梯度下降

46
00:03:34,620 --> 00:03:39,810
然後接下來，你拿第二批小量資料，而這其實就是

47
00:03:39,810 --> 00:03:43,280
你的第二筆訓練資料，然後拿他做梯度下降

48
00:03:43,280 --> 00:03:45,170
然後你拿第三筆訓練資料，依此類推

49
00:03:45,170 --> 00:03:47,940
也就是一次只看一筆訓練資料。

50
00:03:50,100 --> 00:03:55,840
那麼，讓我們看看在這兩種極端的情況，
最佳化我們的成本函數會怎麼發生什麼事

51
00:03:55,840 --> 00:03:59,795
假設這個是我們想優化的成本函數的等值線

52
00:03:59,795 --> 00:04:01,067
所以最小值在這邊，

53
00:04:01,067 --> 00:04:05,825
那麼，批次梯度下降法可能在某個地方開始

54
00:04:05,825 --> 00:04:12,320
而且能夠踏出比較穩定 (low noise)，比較大的步伐

55
00:04:12,320 --> 00:04:15,600
如同行軍，不斷地往最小值邁進。

56
00:04:15,600 --> 00:04:19,290
相對的，隨機梯度下降法 (stochastic gradient descent)

57
00:04:19,290 --> 00:04:22,430
假設他在另一個點開始

58
00:04:22,430 --> 00:04:26,180
那麼在每一次的迭代 (iteration)，
你只拿一筆訓練資料做梯度下降

59
00:04:26,180 --> 00:04:30,080
所以大部分的時候你會往最小值前進

60
00:04:30,080 --> 00:04:33,865
但是有時你最走錯方向，萬一那筆資料

61
00:04:33,865 --> 00:04:36,303
剛好把你指到爛的方向

62
00:04:36,303 --> 00:04:40,530
所以隨機梯度下降法可能會非常有噪音、亂跑

63
00:04:40,530 --> 00:04:45,070
平均來說，他會把你帶到一個好的方向

64
00:04:45,070 --> 00:04:47,116
但偶爾他也會走錯方向。

65
00:04:47,116 --> 00:04:50,190
由於隨機梯度下降法不會收斂

66
00:04:50,190 --> 00:04:54,760
他總是會在最小值附近震盪、流浪

67
00:04:54,760 --> 00:04:58,006
但不會走到最小值以後就定在那邊。

68
00:04:58,006 --> 00:05:03,320
實際上，你選的小批資料大小會介在

69
00:05:07,976 --> 00:05:15,100
1 到 m 之間；
如果選 1 或 m 會太小或太大

70
00:05:15,100 --> 00:05:16,199
這是為什麼呢

71
00:05:16,199 --> 00:05:23,844
如果你用批次梯度下降法

72
00:05:23,844 --> 00:05:27,386
也就是小批資料筆數為 m

73
00:05:30,878 --> 00:05:35,190
那麼你每一回合都在處理巨大的訓練集

74
00:05:35,190 --> 00:05:40,101
所以主要的缺點是他會花太久的時間

75
00:05:40,101 --> 00:05:43,860
在每次的迭代上 — 假設你有很大的訓練集。

76
00:05:43,860 --> 00:05:46,792
如果你的訓練集很小，那批次梯度下降法不會有問題。

77
00:05:46,792 --> 00:05:51,200
相反地，如果你用隨機梯度下降法，

78
00:05:54,076 --> 00:05:58,967
那麼，每次處理一筆資料你都會有進展，這很好

79
00:05:58,967 --> 00:06:02,030
這其實不會是問題，

80
00:06:02,030 --> 00:06:04,290
而那噪音、不穩定，可以用

81
00:06:04,290 --> 00:06:07,378
比較小的學習率 (learning rate) 來改善減緩之。

82
00:06:07,378 --> 00:06:12,160
然而，隨機梯度下降法有一個很大的缺點

83
00:06:12,160 --> 00:06:17,050
你幾乎無法享受到向量化 (vectorization) 帶來的加速

84
00:06:18,370 --> 00:06:22,050
因為在這裡，你一次只處理單一一筆資料

85
00:06:22,050 --> 00:06:26,130
這樣一筆一筆資料處理會很沒有效率。

86
00:06:26,130 --> 00:06:32,380
所以實務上最有效的是介在兩者中間

87
00:06:36,687 --> 00:06:40,360
你的小批資料筆數不會太多或太少

88
00:06:44,439 --> 00:06:48,630
實務上這讓你訓練得最快

89
00:06:51,405 --> 00:06:54,860
而且你可以注意到這樣會有兩個好處：

90
00:06:54,860 --> 00:06:58,174
第一，你的確享受到很多的向量化

91
00:06:58,174 --> 00:07:02,667
拿前部影片為例子，如果你的小批資料大小

92
00:07:02,667 --> 00:07:07,669
為 1000 筆，那你可以對 1000 筆資料做向量化

93
00:07:07,669 --> 00:07:12,110
這會比一次一筆還快非常多。

94
00:07:13,670 --> 00:07:16,710
第二，你還是可以有所進展

95
00:07:22,210 --> 00:07:27,710
不用等到整個訓練集處理完畢。

96
00:07:32,430 --> 00:07:36,719
再拿前部影片的數字為例，每個 epoch、每一次

97
00:07:36,719 --> 00:07:40,640
算過你的訓練集，能給你 5000 步的梯度下降

98
00:07:41,840 --> 00:07:46,370
所以實務上，某個中庸的小批資料大小會最有效。

99
00:07:46,370 --> 00:07:49,380
假設小批次梯度下降法從這裡開始

100
00:07:49,380 --> 00:07:53,670
也許第一次迭代是這樣、兩次後是這樣、三、四...

101
00:07:53,670 --> 00:07:58,521
他不會保證每次都往最小值走，不過

102
00:07:58,521 --> 00:08:03,383
和隨機梯度下降法相比，他會更穩定往最小值的方向

103
00:08:03,383 --> 00:08:08,320
他並不會每次都收斂或遊蕩在非常小的區域

104
00:08:08,320 --> 00:08:11,550
如果這對你是個問題，你總是可以慢慢把學習率變小

105
00:08:11,550 --> 00:08:13,410
我們之後會講到學習率衰減 (learning rate decay)

106
00:08:13,410 --> 00:08:15,960
在之後的影片談如何減少學習率。

107
00:08:15,960 --> 00:08:20,020
那麼，如果小批資料大小不應該是 m 也不應該是 1

108
00:08:20,020 --> 00:08:23,410
而應該是中間某個值，那你該怎麼選呢？

109
00:08:23,410 --> 00:08:24,826
這邊有些準則

110
00:08:24,826 --> 00:08:33,470
第一，如果你有的訓練資料很少，就用批次梯度下降法吧

111
00:08:36,655 --> 00:08:41,023
如果你的訓練集很小，就沒有必要用小批次梯度下降

112
00:08:41,023 --> 00:08:43,670
你處理一整個訓練集的時候就夠快了

113
00:08:43,670 --> 00:08:45,619
所以你可以用批次梯度下降法

114
00:08:45,619 --> 00:08:50,281
「小」的訓練集是多小呢？我大概會說可能小於2000筆吧

115
00:08:50,281 --> 00:08:54,480
只要用批次梯度下降就好了

116
00:08:54,480 --> 00:09:00,391
否則，如果你的訓練集更大，典型的小批資料筆數會是

117
00:09:03,336 --> 00:09:09,437
64 到 512 中間的任何值，還滿常見的

118
00:09:09,437 --> 00:09:14,130
而由於電腦記憶體的配置和存取方式的關係

119
00:09:14,130 --> 00:09:19,460
如果你的小批資料大小是 2 的次方，程式有時會跑得快一點

120
00:09:19,460 --> 00:09:24,108
所以64是2的6次方，然後2的7次方、2的8次方、

121
00:09:24,108 --> 00:09:30,080
2的9次方，所以通常我會把小批資料大小做成 2 的次方

122
00:09:30,080 --> 00:09:33,900
我知道在前一部影片，我用 1000 做為大小

123
00:09:33,900 --> 00:09:37,990
如果你真要這樣的話，我會建議你用 1024

124
00:09:37,990 --> 00:09:39,870
也就是 2 的 10 次方

125
00:09:39,870 --> 00:09:46,176
你的確可以看到用 1024 當小批次的大小，有點罕見就是

126
00:09:46,176 --> 00:09:50,681
在這一個範圍的小批次大小，會比較常見。

127
00:09:50,681 --> 00:09:54,980
最後一個建議是，確保你的小批次資料

128
00:09:57,260 --> 00:10:05,309
你的 X{t}, Y{t} 能塞得進去 CPU/GPU 的記憶體

129
00:10:08,563 --> 00:10:10,863
而這取決於你的應用、

130
00:10:10,863 --> 00:10:12,800
和你一筆訓練資料多大有關，

131
00:10:12,800 --> 00:10:17,430
如果你碰到某一批小資料是CPU/GPU記憶體

132
00:10:17,430 --> 00:10:20,640
所塞不下的，無論拿來計算的資料是什麼，

133
00:10:20,640 --> 00:10:24,336
那麼你會發現效能突然隕落

134
00:10:24,336 --> 00:10:25,809
會忽然變很差。

135
00:10:25,809 --> 00:10:30,273
那麼，我希望這能給一點概念，
大家常用的小批資料筆數的範圍

136
00:10:30,273 --> 00:10:31,790
是怎麼樣的。

137
00:10:31,790 --> 00:10:35,970
實際上，小批資料的大小顯然是一個超參數
(hyperparameter)

138
00:10:35,970 --> 00:10:40,840
你大概會想要尋找一下，了解哪一個大小

139
00:10:40,840 --> 00:10:43,960
能讓你最有效率地降低成本值 J。

140
00:10:43,960 --> 00:10:47,065
所以我會做的是，試試幾個不同的值

141
00:10:47,065 --> 00:10:51,727
嘗試幾個 2 的不同次方，然後看看能否挑到一個值

142
00:10:51,727 --> 00:10:56,470
盡可能讓你的下降優化演算法有效率。

143
00:10:56,470 --> 00:10:59,940
總之，希望我能給你一些準則，

144
00:10:59,940 --> 00:11:03,405
怎麼開始找這個超參數。

145
00:11:03,405 --> 00:11:07,012
你現在知道了怎麼實作小批次梯度下降法
讓你的演算法

146
00:11:07,012 --> 00:11:10,378
跑得更快 — 尤其是在訓練很大的訓練集的時候。

147
00:11:10,378 --> 00:11:12,936
不過呢，其實還有某些演算法，

148
00:11:12,936 --> 00:11:15,805
比梯度下降法或小批次梯度下降法更有效率。

149
00:11:15,805 --> 00:11:18,215
讓我們在接下來的幾部影片看看吧