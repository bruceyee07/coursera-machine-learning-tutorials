1
00:00:00,380 --> 00:00:03,140
有一种方法或许能学习算法运行更快

2
00:00:03,140 --> 00:00:06,240
那就是渐渐地减小学习率

3
00:00:06,240 --> 00:00:08,520
我们称之为学习率衰减

4
00:00:08,520 --> 00:00:10,650
让我们看看如何实现这个方法

5
00:00:10,650 --> 00:00:13,710
我们用一个例子说明为什么你可能想要

6
00:00:13,710 --> 00:00:15,150
用到学习率衰减

7
00:00:15,150 --> 00:00:18,260
当你使用适量的小样本进行

8
00:00:18,260 --> 00:00:20,070
小批次梯度下降法(mini-batch gradient descent)时

9
00:00:20,070 --> 00:00:24,210
也许一个批次只有64个 或128个样本

10
00:00:24,210 --> 00:00:28,210
当你迭代时 步长(steps)会有些浮动

11
00:00:28,210 --> 00:00:33,940
它会逐步向最小值靠近 但不会完全收敛到这点

12
00:00:33,940 --> 00:00:38,040
所以你的算法会在最小值周围浮动 但是

13
00:00:38,040 --> 00:00:43,390
却永远不会真正收敛 因为你的学习率α取了固定值

14
00:00:43,390 --> 00:00:46,660
且不同的批次(mini-batches)也可能产生些噪声

15
00:00:46,660 --> 00:00:52,650
但是如果你慢慢地降低你的学习率α

16
00:00:52,650 --> 00:00:56,410
那么在初始阶段 因为学习率α取值还比较大

17
00:00:56,410 --> 00:00:59,270
学习速度仍然可以比较快

18
00:00:59,270 --> 00:01:05,940
但随着学习率降低α变小 步长也会渐渐变小

19
00:01:05,940 --> 00:01:11,160
所以最终将围绕着离极小值点更近的区域摆动

20
00:01:11,160 --> 00:01:15,398
即使继续训练下去也不会漂游远离

21
00:01:15,398 --> 00:01:20,200
逐渐降低学习率α背后的思考是

22
00:01:20,200 --> 00:01:25,170
在学习的初始步骤中 你可以采取大得多的步长

23
00:01:25,170 --> 00:01:29,060
但随着学习开始收敛于一点时

24
00:01:29,060 --> 00:01:33,070
较低的学习率可以允许你采取更小的步长

25
00:01:33,070 --> 00:01:36,650
那怎么实现率衰减(learning rate decay)呢

26
00:01:36,650 --> 00:01:40,640
还记得一个迭代就是将算法

27
00:01:42,512 --> 00:01:45,430
在所有数据上过一遍 对吧

28
00:01:45,430 --> 00:01:49,053
所以如果你有一个像这样的训练集

29
00:01:49,053 --> 00:01:53,866
也许你把它分解成不同的小批次

30
00:01:53,866 --> 00:02:00,446
算法在训练集上通过第一遍 称为第一次迭代

31
00:02:00,446 --> 00:02:05,613
通过第二遍 称为第二次迭代 如此类推

32
00:02:05,613 --> 00:02:10,628
所以你这样设定学习率α

33
00:02:10,628 --> 00:02:15,464
先计算 1 除以 1 加上某个参数 我把它称为衰减率

34
00:02:18,112 --> 00:02:22,490
再把这个和乘以迭代数 作为分母

35
00:02:22,490 --> 00:02:26,890
然后将这个商再乘以初始学习率 α0

36
00:02:26,890 --> 00:02:30,730
这里的衰减率是另一个超参数

37
00:02:30,730 --> 00:02:32,340
也可能需要你再调整

38
00:02:32,340 --> 00:02:33,910
举一个具体的例子

39
00:02:35,070 --> 00:02:39,659
如果算法在数据上通过了若干次 即是若干次迭代

40
00:02:39,659 --> 00:02:46,211
假定α0=0.2 衰减率=1

41
00:02:46,211 --> 00:02:50,267
那么在第一次迭代中

42
00:02:50,267 --> 00:02:55,268
α = 1/(1+1)*α0

43
00:02:55,268 --> 00:02:59,785
所以你的学习率是0.1

44
00:02:59,785 --> 00:03:04,289
就是把衰减率设为1 把迭代数设为1

45
00:03:04,289 --> 00:03:05,755
然后代入这个公式得到的

46
00:03:05,755 --> 00:03:10,613
在第二次迭代中 你的学习率将递减为0.67

47
00:03:10,613 --> 00:03:15,924
第三次迭代为0.5 第四次迭代为0.4 如此类推

48
00:03:15,924 --> 00:03:18,150
你可以自己去求出更后面的值

49
00:03:18,150 --> 00:03:23,200
然后随着你的迭代函数 你的学习率

50
00:03:23,200 --> 00:03:29,930
也会根据以上那个式子逐渐递减

51
00:03:29,930 --> 00:03:33,860
所以如果你想使用学习率衰减 你可以尝试

52
00:03:33,860 --> 00:03:38,830
不同的超参数组合 包括α0

53
00:03:38,830 --> 00:03:41,550
以及这个衰减率的超参数

54
00:03:41,550 --> 00:03:44,710
然后去尝试寻找一个效果好的数值

55
00:03:44,710 --> 00:03:47,188
除了这个学习率衰减的公式之外

56
00:03:47,188 --> 00:03:49,314
人们还会用一些其他的方式

57
00:03:49,314 --> 00:03:52,097
例如 这是指数衰减

58
00:03:52,097 --> 00:03:58,009
α在这里取一个小于1的值

59
00:03:58,009 --> 00:04:04,513
例如0.95的迭代数次方 再乘以α0

60
00:04:04,513 --> 00:04:10,500
所以这将以指速级速度 使学习率快速衰减

61
00:04:10,500 --> 00:04:15,788
还有一些别的公式 比如说

62
00:04:15,788 --> 00:04:21,805
α = 某个常数 / (迭代次数平方根) * α0

63
00:04:21,805 --> 00:04:26,627
或是常数k 另一个超参数

64
00:04:26,627 --> 00:04:32,956
除以 小批次数t的平方根 与α0 的乘积

65
00:04:32,956 --> 00:04:37,627
有时你也会看到有些人让学习率

66
00:04:37,627 --> 00:04:38,821
以离散的阶梯衰减

67
00:04:38,821 --> 00:04:42,798
开始时以某个初始学习率开始

68
00:04:42,798 --> 00:04:45,960
过一会儿减少一半

69
00:04:45,960 --> 00:04:47,320
然后再一半

70
00:04:47,320 --> 00:04:48,970
然后再一半

71
00:04:48,970 --> 00:04:52,793
于是这样成了一个离散的阶梯

72
00:04:55,954 --> 00:05:01,395
那我们已经讨论过如何使用一些公式来决定

73
00:05:01,395 --> 00:05:05,210
学习率α 如何随着时间推移而发生变化

74
00:05:05,210 --> 00:05:08,900
有时候人们会做的另一件事就是是手动衰减

75
00:05:08,900 --> 00:05:11,980
那就是如果你在一段时间 而单单这个模型

76
00:05:11,980 --> 00:05:16,070
就需要几个小时或甚至几天来训练的话

77
00:05:16,070 --> 00:05:17,090
那有些人的做法就是

78
00:05:17,090 --> 00:05:21,638
先对模型的训练过程进行多天的观察

79
00:05:21,638 --> 00:05:25,180
然后说 看起来学习率像是减慢了

80
00:05:25,180 --> 00:05:27,180
我就要略微降低α

81
00:05:27,180 --> 00:05:30,242
这种手动控制α的方法当然也是可行的

82
00:05:30,242 --> 00:05:33,710
就一小时一小时 一天一天地 手工去调整α

83
00:05:33,710 --> 00:05:37,140
虽然只有在训练少量的模型时 这方法才可行

84
00:05:37,140 --> 00:05:39,100
但有时人们也是会用这个方法的

85
00:05:39,100 --> 00:05:43,580
那现在你有更多的选择如何去控制阿尔法学习率

86
00:05:43,580 --> 00:05:46,630
如果你现在在想 哇 这有那么多的超参数

87
00:05:46,630 --> 00:05:49,320
那我该如何在这么多不同的选项当中做出选择?

88
00:05:49,320 --> 00:05:51,190
我想说 现在不用担心这个

89
00:05:51,190 --> 00:05:56,550
下周 我们将对系统性地选择超参数 进行更多讨论

90
00:05:56,550 --> 00:06:00,500
对我来说 学习率衰减通常位于

91
00:06:00,500 --> 00:06:02,080
我尝试的事情中比较靠后的位置

92
00:06:02,080 --> 00:06:05,670
设置一个固定数值的阿尔法 还要使它优化得良好

93
00:06:05,670 --> 00:06:07,080
对结果是会有巨大的影响的

94
00:06:07,080 --> 00:06:09,050
学习率衰减的确是有帮助的

95
00:06:09,050 --> 00:06:11,050
有时它可以真正帮助加速训练

96
00:06:11,050 --> 00:06:15,720
但是它还是在我尝试的办法中比较靠后的一顶

97
00:06:15,720 --> 00:06:18,543
但下个星期 我们谈到超参数优化时

98
00:06:18,543 --> 00:06:21,978
你会看到更多系统性来地安排所有的超参数的方式

99
00:06:21,978 --> 00:06:24,422
以及如何有效地在当中搜索

100
00:06:24,422 --> 00:06:27,790
那这些就是学习率衰减的内容

101
00:06:27,790 --> 00:06:31,420
在那最后 我还会讨论一下神经网络里的

102
00:06:31,420 --> 00:06:33,390
局部最优解和鞍点

103
00:06:33,390 --> 00:06:36,210
这样你能够对于不同的优化算法所适用的问题

104
00:06:36,210 --> 00:06:39,970
能够有更好的感觉

105
00:06:39,970 --> 00:06:41,840
在你要训练神经网络时这会有用

106
00:06:41,840 --> 00:06:43,570
我们下一节再见