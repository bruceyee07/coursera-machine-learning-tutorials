1
00:00:00,390 --> 00:00:04,770
有個演算法叫「動量法」 (momentum)，或叫
動量梯度下降法 (gradient descent with momentum)

2
00:00:04,770 --> 00:00:09,600
他幾乎都比標準的梯度下降法還要快

3
00:00:09,600 --> 00:00:14,100
一句話來解釋：基本概念是計算梯度的指數加權平均

4
00:00:14,100 --> 00:00:18,850
然後改用那一個梯度來更新你的權重。

5
00:00:18,850 --> 00:00:22,010
在這部影片，就讓我們把這一句話抽絲剝繭

6
00:00:22,010 --> 00:00:23,848
看看實際上如何實作之。

7
00:00:23,848 --> 00:00:28,619
如同往常一樣，假設你想要最佳化一個成本函數

8
00:00:28,619 --> 00:00:30,510
其等值線像這樣

9
00:00:30,510 --> 00:00:34,350
而紅點表示極小值的地方

10
00:00:34,350 --> 00:00:39,307
假設你在這邊開始梯度下降，如果你跑一步

11
00:00:39,307 --> 00:00:44,670
無論是批次或小批次的梯度下降，可能會跑到那裡

12
00:00:44,670 --> 00:00:47,370
不過呢，你現在跑到橢圓的另一側了

13
00:00:47,370 --> 00:00:51,810
如果你再跑一步梯度下降，可能會這樣

14
00:00:51,810 --> 00:00:55,590
然後再一步，又一步，依此類推

15
00:00:55,590 --> 00:01:00,460
你可以觀察到這梯度下降法會跑很多步，對吧

16
00:01:00,460 --> 00:01:07,190
慢慢地往最小值搖擺過去

17
00:01:07,190 --> 00:01:11,206
這樣的上下震盪減緩了梯度的下降

18
00:01:11,206 --> 00:01:14,500
也讓你無法用比較大的學習率

19
00:01:14,500 --> 00:01:19,226
特別是，如果你真要用很大的學習率，最後有可能

20
00:01:19,226 --> 00:01:21,533
會射得太遠，最後像這樣發散

21
00:01:21,533 --> 00:01:25,826
為了不要有過大的震盪，這強迫你

22
00:01:25,826 --> 00:01:29,650
不能用太大的學習率。

23
00:01:29,650 --> 00:01:34,120
另一個看這問題的角度是，從垂直軸的方向看

24
00:01:34,120 --> 00:01:38,990
你希望你學習得比較慢，因為你不想要那些震盪

25
00:01:38,990 --> 00:01:43,701
但是在水平軸上，你希望學習快一些

26
00:01:45,552 --> 00:01:48,831
因為你想要積極地由左跑到右

27
00:01:48,831 --> 00:01:51,910
往最小值、往紅點的方向。

28
00:01:51,910 --> 00:01:55,621
那麼如果你把「動量」加入梯度下降法的實作，就可以這樣：

29
00:01:58,542 --> 00:02:03,611
在每回合，或更具體說，在第 t 個迭代

30
00:02:03,611 --> 00:02:11,562
你照例計算導數 dW, db

31
00:02:11,562 --> 00:02:15,834
我在此省略上標方括號 [l]

32
00:02:15,834 --> 00:02:19,940
你在目前的小批資料上計算 dW, db

33
00:02:19,940 --> 00:02:21,550
如果你用的是批次梯度下降法，

34
00:02:21,550 --> 00:02:24,200
那這小批資料其實就是整個資料

35
00:02:24,200 --> 00:02:26,670
這方法在批次梯度下降也有用

36
00:02:26,670 --> 00:02:29,580
所以如果你的小批資料就是一整個訓練集

37
00:02:29,580 --> 00:02:31,560
這也是沒問題的。

38
00:02:31,560 --> 00:02:36,453
然後呢，你要做的是

39
00:02:36,453 --> 00:02:41,346
計算 V_dW，讓他變成 beta * V_dW 加上

40
00:02:41,346 --> 00:02:45,779
(1-beta) * dW

41
00:02:45,779 --> 00:02:50,808
這個很像先前的計算，也就是

42
00:02:50,808 --> 00:02:55,960
v_theta = beta * v_theta + (1-beta) * theta_t

43
00:02:57,130 --> 00:03:02,453
因此，這就是在計算對W的導數的移動平均

44
00:03:02,453 --> 00:03:07,754
同樣地，你計算 v_db 等於這個

45
00:03:07,754 --> 00:03:13,980
... + (1-beta) 乘 db

46
00:03:13,980 --> 00:03:18,810
然後，你更新權重 W，讓他變成

47
00:03:18,810 --> 00:03:23,850
W 減掉學習率乘上，這邊不用導數 dW，

48
00:03:23,850 --> 00:03:28,240
你用的是 V_dW 來更新

49
00:03:28,240 --> 00:03:35,630
同樣地，b 更新為 b - alpha * v_db

50
00:03:35,630 --> 00:03:39,570
所以這邊做的，是讓梯度下降的步伐更平順

51
00:03:41,230 --> 00:03:45,760
舉個例子，假設最近幾個導數算出來是這個

52
00:03:45,760 --> 00:03:47,298
這個、這個、這個

53
00:03:48,330 --> 00:03:52,354
如果你把這些梯度平均，你會發現垂直方向的震盪

54
00:03:52,354 --> 00:03:55,472
大概會平均掉，變比較靠近零

55
00:03:55,472 --> 00:04:00,301
所以，在你想慢慢學的垂直方向，

56
00:04:00,301 --> 00:04:05,390
這會把正數和負數平均掉，會靠近零。

57
00:04:05,390 --> 00:04:07,740
而對於水平方向

58
00:04:07,740 --> 00:04:11,160
從水平來看，每個導數都往右指

59
00:04:11,160 --> 00:04:14,340
所以水平方向上的平均還是滿大

60
00:04:14,340 --> 00:04:18,200
這就是為什麼這個演算法經過幾步以後

61
00:04:18,200 --> 00:04:22,930
你會發現動量梯度下降法的步伐

62
00:04:22,930 --> 00:04:28,100
最後在垂直方向的震盪會變得很小

63
00:04:28,100 --> 00:04:33,700
但是在水平方向上會很快、很直接地移動

64
00:04:33,700 --> 00:04:37,480
所以這讓你的演算法走更直接的路徑

65
00:04:37,480 --> 00:04:42,990
或說是抑制震盪，一路邁向最小值。

66
00:04:42,990 --> 00:04:47,304
有個關於這動量法的直觀看法 — 有些人能接受，

67
00:04:47,304 --> 00:04:53,040
但不適合每個人 — 就是你試著
在這個碗型的函數找到最小值

68
00:04:53,040 --> 00:04:55,440
這其實是一個碗的等值線

69
00:04:55,440 --> 00:04:57,840
我猜我不大會畫畫

70
00:04:57,840 --> 00:05:02,470
我們想在這種碗狀的函數取極小值，那麼，

71
00:05:02,470 --> 00:05:06,625
這個導數項，你可以想成他提供「加速度」

72
00:05:06,625 --> 00:05:11,071
給一顆往下滾的球

73
00:05:11,071 --> 00:05:19,151
而這個動量項，可以看作代表「速度」，

74
00:05:20,812 --> 00:05:24,749
所以想像一下，你有一個碗，然後你拿一顆球，

75
00:05:24,749 --> 00:05:28,854
導數賦予球「加速度」

76
00:05:28,854 --> 00:05:32,440
當這球往下滾的時候

77
00:05:32,440 --> 00:05:36,980
因為加速度，他會越滾越快。

78
00:05:36,980 --> 00:05:42,390
然後 beta，因為他略比 1 小，他扮演著

79
00:05:42,390 --> 00:05:46,690
「摩擦力」，讓你的球不會無限加速

80
00:05:46,690 --> 00:05:50,380
所以不像正常的梯度下降法

81
00:05:50,380 --> 00:05:54,120
所踏的每一步和之前的步伐沒有關係

82
00:05:54,120 --> 00:05:56,460
現在，你的球往下滾的時候

83
00:05:56,460 --> 00:06:01,610
會獲得「動量」；他能這碗內加速往下，所以獲得動量。

84
00:06:01,610 --> 00:06:05,640
我發現這個球在碗裡往下滾的比喻，好像

85
00:06:05,640 --> 00:06:07,770
對喜歡物理概念的人有用

86
00:06:07,770 --> 00:06:12,160
不過不是所有人都能接受，所以如果球在碗滾

87
00:06:12,160 --> 00:06:15,000
這個比喻不能說服你，也不用太擔心。

88
00:06:15,000 --> 00:06:18,280
最後呢，讓我們看看實作的細節

89
00:06:18,280 --> 00:06:21,300
這邊是演算法，所以你現在有兩個

90
00:06:22,300 --> 00:06:27,100
超參數：學習率 alpha，還有這個 beta

91
00:06:27,100 --> 00:06:30,080
beta 控制你的指數加權平均

92
00:06:30,080 --> 00:06:33,073
常見的 beta 值為 0.9

93
00:06:33,073 --> 00:06:35,730
也就是最近 10 天的氣溫平均

94
00:06:35,730 --> 00:06:39,930
所以這個對最近 10 次迭代的梯度做平均，

95
00:06:39,930 --> 00:06:42,768
實務上，beta=0.9 會做得很好

96
00:06:42,768 --> 00:06:45,420
歡迎隨意嘗試不同的值

97
00:06:45,420 --> 00:06:50,120
做超參數的搜尋，不過 0.9 似乎是滿通用穩定的值。

98
00:06:50,120 --> 00:06:51,932
那麼，關於「偏差矯正」呢？ (bias correction)

99
00:06:51,932 --> 00:06:58,170
所以你要不要拿 V_dW 或 v_db，除以 1-beta^t 呢？

100
00:06:58,170 --> 00:07:02,380
實務上，大家通常不會這樣做，因為過了10回合

101
00:07:02,380 --> 00:07:06,530
你的移動平均已經暖身完畢，估計不再有偏差

102
00:07:06,530 --> 00:07:11,357
所以實務上，我沒看過大家會因為沒做矯正而在意

103
00:07:11,357 --> 00:07:14,663
當實作動量梯度下降法的時候。

104
00:07:14,663 --> 00:07:18,785
而當然，程序上會把 V_dW 初始化設成 0

105
00:07:18,785 --> 00:07:23,546
注意到這是零矩陣，和 dW 有著相同維度

106
00:07:23,546 --> 00:07:26,810
也就是和 W 有相同維度

107
00:07:26,810 --> 00:07:30,620
而 v_db 也是初始化成元素為 0 的向量

108
00:07:30,620 --> 00:07:35,400
和 db 有相同維度，所以和 b 維度相同。

109
00:07:35,400 --> 00:07:40,050
最後我想提一下，如果你去讀動量法的文獻

110
00:07:40,050 --> 00:07:45,590
你常常會看到這一項被省略

111
00:07:45,590 --> 00:07:48,854
1-beta 這一項拿掉

112
00:07:48,854 --> 00:07:57,080
所以你最後會有 V_dW = beta V_dW + dW

113
00:07:57,080 --> 00:08:02,127
這紫色版本的最終效果就是，V_dW 最後

114
00:08:02,127 --> 00:08:07,300
照 1-beta 這比例縮放，嚴格講是放大 (1-beta) 分之一

115
00:08:07,300 --> 00:08:11,230
所以做梯度下降更新時，alpha

116
00:08:11,230 --> 00:08:16,220
要因為 1/(1-beta) 做相對應的縮放

117
00:08:16,220 --> 00:08:18,800
實務上，兩種版本都好

118
00:08:18,800 --> 00:08:23,740
受影響的只有學習率 alpha 的最佳值

119
00:08:23,740 --> 00:08:28,350
不過我覺得這一條式子比較不直觀

120
00:08:28,350 --> 00:08:33,610
因為有個影響：如果你要調整超參數 beta 的話

121
00:08:33,610 --> 00:08:37,770
你也會影響 V_dW 和 v_db 的大小尺度

122
00:08:37,770 --> 00:08:42,710
所以你或許也需要重新調整學習率 alpha。

123
00:08:42,710 --> 00:08:46,970
因此，我個人偏好左邊這個公式

124
00:08:46,970 --> 00:08:49,600
而不是把 1-beta 拿掉。

125
00:08:49,600 --> 00:08:52,450
因此，我傾向於用左邊的公式

126
00:08:52,450 --> 00:08:55,140
有 1-beta、這個印刷的版本。

127
00:08:55,140 --> 00:09:00,280
不過呢，對於這兩個版本，
超參數 beta=0.9 都是很常見的選擇

128
00:09:00,280 --> 00:09:03,500
只不過 alpha, 學習率, 在不同的版本

129
00:09:03,500 --> 00:09:04,880
會調整成不同的值。

130
00:09:04,880 --> 00:09:07,500
那麼，這就是動量梯次下降法。

131
00:09:07,500 --> 00:09:11,120
和正常版、沒有動量的梯度下降相比，

132
00:09:11,120 --> 00:09:13,740
這個方法幾乎都會表現更好。

133
00:09:13,740 --> 00:09:17,020
不過呢，我們還能做其他事情
讓你的演算法學得更快

134
00:09:17,020 --> 00:09:19,920
讓我們在後面的影片繼續探討之