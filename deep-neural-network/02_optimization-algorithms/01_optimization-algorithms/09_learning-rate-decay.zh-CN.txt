有一种方法或许能学习算法运行更快 那就是渐渐地减小学习率 我们称之为学习率衰减 让我们看看如何实现这个方法 我们用一个例子说明为什么你可能想要 用到学习率衰减 当你使用适量的小样本进行 小批次梯度下降法(mini-batch gradient descent)时 也许一个批次只有64个 或128个样本 当你迭代时 步长(steps)会有些浮动 它会逐步向最小值靠近 但不会完全收敛到这点 所以你的算法会在最小值周围浮动 但是 却永远不会真正收敛 因为你的学习率α取了固定值 且不同的批次(mini-batches)也可能产生些噪声 但是如果你慢慢地降低你的学习率α 那么在初始阶段 因为学习率α取值还比较大 学习速度仍然可以比较快 但随着学习率降低α变小 步长也会渐渐变小 所以最终将围绕着离极小值点更近的区域摆动 即使继续训练下去也不会漂游远离 逐渐降低学习率α背后的思考是 在学习的初始步骤中 你可以采取大得多的步长 但随着学习开始收敛于一点时 较低的学习率可以允许你采取更小的步长 那怎么实现率衰减(learning rate decay)呢 还记得一个迭代就是将算法 在所有数据上过一遍 对吧 所以如果你有一个像这样的训练集 也许你把它分解成不同的小批次 算法在训练集上通过第一遍 称为第一次迭代 通过第二遍 称为第二次迭代 如此类推 所以你这样设定学习率α 先计算 1 除以 1 加上某个参数 我把它称为衰减率 再把这个和乘以迭代数 作为分母 然后将这个商再乘以初始学习率 α0 这里的衰减率是另一个超参数 也可能需要你再调整 举一个具体的例子 如果算法在数据上通过了若干次 即是若干次迭代 假定α0=0.2 衰减率=1 那么在第一次迭代中 α = 1/(1+1)*α0 所以你的学习率是0.1 就是把衰减率设为1 把迭代数设为1 然后代入这个公式得到的 在第二次迭代中 你的学习率将递减为0.67 第三次迭代为0.5 第四次迭代为0.4 如此类推 你可以自己去求出更后面的值 然后随着你的迭代函数 你的学习率 也会根据以上那个式子逐渐递减 所以如果你想使用学习率衰减 你可以尝试 不同的超参数组合 包括α0 以及这个衰减率的超参数 然后去尝试寻找一个效果好的数值 除了这个学习率衰减的公式之外 人们还会用一些其他的方式 例如 这是指数衰减 α在这里取一个小于1的值 例如0.95的迭代数次方 再乘以α0 所以这将以指速级速度 使学习率快速衰减 还有一些别的公式 比如说 α = 某个常数 / (迭代次数平方根) * α0 或是常数k 另一个超参数 除以 小批次数t的平方根 与α0 的乘积 有时你也会看到有些人让学习率 以离散的阶梯衰减 开始时以某个初始学习率开始 过一会儿减少一半 然后再一半 然后再一半 于是这样成了一个离散的阶梯 那我们已经讨论过如何使用一些公式来决定 学习率α 如何随着时间推移而发生变化 有时候人们会做的另一件事就是是手动衰减 那就是如果你在一段时间 而单单这个模型 就需要几个小时或甚至几天来训练的话 那有些人的做法就是 先对模型的训练过程进行多天的观察 然后说 看起来学习率像是减慢了 我就要略微降低α 这种手动控制α的方法当然也是可行的 就一小时一小时 一天一天地 手工去调整α 虽然只有在训练少量的模型时 这方法才可行 但有时人们也是会用这个方法的 那现在你有更多的选择如何去控制阿尔法学习率 如果你现在在想 哇 这有那么多的超参数 那我该如何在这么多不同的选项当中做出选择? 我想说 现在不用担心这个 下周 我们将对系统性地选择超参数 进行更多讨论 对我来说 学习率衰减通常位于 我尝试的事情中比较靠后的位置 设置一个固定数值的阿尔法 还要使它优化得良好 对结果是会有巨大的影响的 学习率衰减的确是有帮助的 有时它可以真正帮助加速训练 但是它还是在我尝试的办法中比较靠后的一顶 但下个星期 我们谈到超参数优化时 你会看到更多系统性来地安排所有的超参数的方式 以及如何有效地在当中搜索 那这些就是学习率衰减的内容 在那最后 我还会讨论一下神经网络里的 局部最优解和鞍点 这样你能够对于不同的优化算法所适用的问题 能够有更好的感觉 在你要训练神经网络时这会有用 我们下一节再见