1
00:00:00,000 --> 00:00:02,340
在深度學習的歷史中

2
00:00:02,340 --> 00:00:05,700
很多研究者，包括一些有名的研究者

3
00:00:05,700 --> 00:00:07,790
有時會發明最佳化演算法

4
00:00:07,790 --> 00:00:09,825
而且告訴大家他們在一些問題上很有效

5
00:00:09,825 --> 00:00:13,440
但是那些優化演算法之後被發現

6
00:00:13,440 --> 00:00:18,130
不太能廣泛運用，在眾多種你想訓練的神經網路上，
不一定都很有效。

7
00:00:18,130 --> 00:00:21,360
所以隨著時間，我想深度學習的社群

8
00:00:21,360 --> 00:00:25,597
面對新的最佳化演算法，開始會抱持一定的懷疑

9
00:00:25,597 --> 00:00:29,350
很多人覺得動量梯度下降法真的很有用

10
00:00:29,350 --> 00:00:32,720
很難再提出比這個更有效的了

11
00:00:32,720 --> 00:00:36,070
所以 RMSprop，和 Adam 最佳化演算法

12
00:00:36,070 --> 00:00:37,730
也就是這部影片會談到的

13
00:00:37,730 --> 00:00:41,460
是少數經得起考驗的演算法

14
00:00:41,460 --> 00:00:47,250
而且在很多類的深度學習架構上
都已被證明表現得很好

15
00:00:47,250 --> 00:00:50,150
所以，我會毫不猶豫推薦你試試這個演算法

16
00:00:50,150 --> 00:00:54,625
因為已經有很多人試過了，看起來在很多問題上都很有效。

17
00:00:54,625 --> 00:00:57,720
Adam 最佳化演算法基本上，是拿

18
00:00:57,720 --> 00:01:01,250
動量法和 RMSprop，把他們兩個摻在一起

19
00:01:01,250 --> 00:01:03,105
那麼，讓我們看看他的運作。

20
00:01:03,105 --> 00:01:05,695
要實作Adam，你會初始一些變數

21
00:01:05,695 --> 00:01:15,877
V_dW = 0, S_dW = 0, 類似地 v_db = 0, s_db = 0

22
00:01:15,877 --> 00:01:19,810
然後在第 t 個迭代

23
00:01:19,810 --> 00:01:30,170
你會用現在的小批資料算出導數：dW, db

24
00:01:30,170 --> 00:01:33,775
通常這會用在小批次梯度下降法

25
00:01:33,775 --> 00:01:41,480
然後你算動量的指數加權平均，所以 V_dW = ß

26
00:01:41,480 --> 00:01:46,410
不過我現在要叫他 ß_1，以便和

27
00:01:46,410 --> 00:01:52,660
RMSprop部份的超參數 ß_2 分別

28
00:01:52,660 --> 00:01:58,180
所以，這個跟我們之前實作的

29
00:01:58,180 --> 00:02:03,788
和動量法一模一樣，只是現在叫這超參數 ß_1 而非 ß

30
00:02:03,788 --> 00:02:14,312
然後類似地，你會有 v_db 像這樣...... (1-ß_1) * db

31
00:02:14,312 --> 00:02:18,685
然後你進行和 RMSprop 很像的更新

32
00:02:18,685 --> 00:02:26,630
你有另一個超參數 ß_2 ...... 加 (1-ß_2) 乘以 dW 的平方

33
00:02:26,630 --> 00:02:33,325
再次強調，這邊的「平方」是對於 dW 的每個元素的。

34
00:02:33,325 --> 00:02:44,005
然後 s_db 等於這個加上 (1-ß_2) * db
(筆誤, 應要有平方)

35
00:02:44,005 --> 00:02:49,145
所以這邊的更新像動量法，用超參數 ß_1

36
00:02:49,145 --> 00:02:55,318
而這邊的更新像RMSprop，用超參數 ß_2

37
00:02:55,318 --> 00:02:58,599
在典型的 Adam 實作裡

38
00:02:58,599 --> 00:03:01,255
你真的會實作偏差矯正 (bias correction)

39
00:03:01,255 --> 00:03:04,215
所以你會有 V corrected

40
00:03:04,215 --> 00:03:06,705
"corrected" 意思是矯正過後

41
00:03:06,705 --> 00:03:16,244
... dW = V_dW 除以 1 - ß_1 的 t 次方
假設在做 t 個迭代

42
00:03:16,244 --> 00:03:25,040
同樣地，v_db corrected 等於 v_db / (1 - ß_1^t)

43
00:03:25,040 --> 00:03:30,756
然後類似地，你對於 S 系列也做偏差矯正

44
00:03:30,756 --> 00:03:37,405
所以這是 S_dW 除以 (1 - ß_2^t)，

45
00:03:37,405 --> 00:03:48,700
s_db corrected = s_db / (1 - ß_2^t)

46
00:03:48,700 --> 00:03:50,660
最後呢，你進行更新

47
00:03:50,660 --> 00:03:55,060
所以 W 被更新為 W 減掉 alpha 乘上

48
00:03:55,060 --> 00:03:59,870
— 如果你只做動量法，你會用 V_dW

49
00:03:59,870 --> 00:04:03,408
或者是 V_dW corrected

50
00:04:03,408 --> 00:04:06,615
但是呢，我們現在加入 RMSprop 的部份

51
00:04:06,615 --> 00:04:13,390
所以我們也會除以 S_dW corrected + epsilon 的平方根

52
00:04:13,390 --> 00:04:18,232
同樣地，b 用類似的式子更新

53
00:04:18,232 --> 00:04:24,070
...... v_db corrected 除以

54
00:04:24,070 --> 00:04:28,595
s_db corrected + epsilon 開根號

55
00:04:28,595 --> 00:04:33,070
因此，這個演算法結合了動量法

56
00:04:33,070 --> 00:04:37,572
和 RMSprop 兩者的效果。

57
00:04:37,572 --> 00:04:41,740
這一個演算法很常用，也被證明在

58
00:04:41,740 --> 00:04:46,640
很多不同的神經網路、眾多類型的架構上都非常有效。

59
00:04:46,640 --> 00:04:49,805
那麼，這個演算法有一些超參數

60
00:04:49,805 --> 00:04:57,330
學習率這個超參數, alpha, 仍然很重要，需要調教

61
00:04:57,330 --> 00:05:01,675
所以你必須試各種大小的值，看那個有效。

62
00:05:01,675 --> 00:05:06,090
ß_1 常見的、預設的選擇是 0.9

63
00:05:06,090 --> 00:05:08,065
所以這代表移動平均、

64
00:05:08,065 --> 00:05:12,220
dW 的加權平均；這是動量部份的項。

65
00:05:12,220 --> 00:05:15,455
對於超參數 ß_2

66
00:05:15,455 --> 00:05:16,950
Adam 論文的作者

67
00:05:16,950 --> 00:05:20,014
發明 Adam 法的人建議 0.999

68
00:05:20,014 --> 00:05:26,485
這是拿來算 dW^2 和 db^2 的移動加權平均。

69
00:05:26,485 --> 00:05:31,030
然後是 epsilon，其實 epsilon 的選擇關係不大

70
00:05:31,030 --> 00:05:34,755
不過 Adam 論文的作者建議 10 的負8次方

71
00:05:34,755 --> 00:05:38,230
不過這一個超參數，你真的不用

72
00:05:38,230 --> 00:05:42,555
太在意，這不會影響表現很多。

73
00:05:42,555 --> 00:05:44,280
不過呢，當實作 Adam

74
00:05:44,280 --> 00:05:47,030
大家通常都只使用預設值

75
00:05:47,030 --> 00:05:49,960
也就是 ß_1, ß_2 和 epsilon

76
00:05:49,960 --> 00:05:52,300
我覺得沒有人真的在調 epsilon

77
00:05:52,300 --> 00:05:56,335
然後嘗試不同大小的 alpha，看哪個最有效

78
00:05:56,335 --> 00:05:59,140
你也可以調整 ß_1 和 ß_2，不過

79
00:05:59,140 --> 00:06:02,440
我認識的人裡面這樣做的不多。

80
00:06:02,440 --> 00:06:06,100
那麼，"Adam" 這個字是怎麼來的呢？

81
00:06:06,100 --> 00:06:15,267
Adam 代表 ADAptive Moment estimation
(適應性矩估計)

82
00:06:15,267 --> 00:06:18,175
ß_1 在算導數的平均

83
00:06:18,175 --> 00:06:19,780
這個叫 the first moment (一階矩)

84
00:06:19,780 --> 00:06:21,975
而 ß_2 是拿來算

85
00:06:21,975 --> 00:06:25,830
平方項的指數加權平均，這叫 the second moment (二階矩)

86
00:06:25,830 --> 00:06:29,380
所以這引出了這名字 adaptive moment estimation

87
00:06:29,380 --> 00:06:32,875
不過大家就叫他 Adam 最佳化演算法

88
00:06:32,875 --> 00:06:37,800
題外話，我有一個老友夥伴，名字叫 Adam Coates

89
00:06:37,800 --> 00:06:40,425
據我所知，這演算法和他一點關係都沒有

90
00:06:40,425 --> 00:06:43,525
除了...我猜他有時候會用這個方法吧

91
00:06:43,525 --> 00:06:45,847
不過我偶爾會被問這個問題

92
00:06:45,847 --> 00:06:47,945
萬一你也在納悶的話。

93
00:06:47,945 --> 00:06:51,187
那麼，這就是 Adam 最佳化演算法

94
00:06:51,187 --> 00:06:54,435
用了他，我想你訓練神經網路能變得更飛快

95
00:06:54,435 --> 00:06:56,055
但在我們結束這周之前

96
00:06:56,055 --> 00:06:58,950
讓我們繼續談談超參數的調整

97
00:06:58,950 --> 00:07:01,465
還有體會更多的感覺，關於

98
00:07:01,465 --> 00:07:04,230
神經網路的優化問題看起來會是怎麼樣的。

99
00:07:04,230 --> 00:07:07,260
在下一部影片，我們來談談 learning rate decay (學習率衰減)