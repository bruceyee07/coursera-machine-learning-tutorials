1
00:00:00,380 --> 00:00:03,140
학습 알고리즘을 가속화시키는데
도움이 될 수 있는 것 중 하나는

2
00:00:03,140 --> 00:00:06,240
러닝 레이트를 시간이 가면서
천천히 줄여나가는 것입니다.

3
00:00:06,240 --> 00:00:08,520
이것을 learning rate decay
(학습 속도 감쇠법)라고 합니다.

4
00:00:08,520 --> 00:00:10,650
이것을 어떻게 구현할 수 있는지 한번 보겠습니다.

5
00:00:10,650 --> 00:00:13,710
왜 학습 속도 감쇠법(learning rate decay)를 써 볼 필요가 있는지

6
00:00:13,710 --> 00:00:15,150
한가지 예제부터 보겠습니다.

7
00:00:15,150 --> 00:00:18,260
여러분이 미니 배치 기울기 강하를 사용한다고 가정해보죠.

8
00:00:18,260 --> 00:00:20,070
비교적 작은 미니 배치로 말이죠.

9
00:00:20,070 --> 00:00:24,210
미니 배치가 64개나 128개 데이터가 있다고 해보죠.

10
00:00:24,210 --> 00:00:28,210
여러분이 학습을 반복하면
이 단계들이 조금 지저분 할 수 있습니다.

11
00:00:28,210 --> 00:00:33,940
여기선 최소값으로 가는 경향은 있지만,
정확히 수렴하진 않을 것입니다.

12
00:00:33,940 --> 00:00:38,040
그러면, 여러분의 알고리즘은 계속 주위를 돌면서

13
00:00:38,040 --> 00:00:43,390
절대로 수렴하지 않을 것입니다.
알파를 어떤 값으로 고정했기 때문이고,

14
00:00:43,390 --> 00:00:46,660
또, 미니 배치들에 노이즈도 있기 때문입니다.

15
00:00:46,660 --> 00:00:52,650
만약 여러분이 학습 속도 알파α 
천천이 줄여 나간다고 한다면,

16
00:00:52,650 --> 00:00:56,410
초기 구간에서는
학습 속도 알파가 아직 큰 값일 때에,

17
00:00:56,410 --> 00:00:59,270
비교적 빠른 학습이 가능할 것입니다.

18
00:00:59,270 --> 00:01:05,940
그러다, 알파 값이 점점 작아지면서,
스텝도 점차 느려지고 작아질 것입니다.

19
00:01:05,940 --> 00:01:11,160
그럼 결국엔 이 최소값 부근의
매우 좁은 범위에서 왔다갔다 하겠죠.

20
00:01:11,160 --> 00:01:15,398
아까 전처럼 학습을 계속해도
멀리 크게 왔다갔다 하는 대신에 말이죠.

21
00:01:15,398 --> 00:01:20,200
그러므로 알파 값을 천천히 줄이는
방법의 배경을 직관해 보면

22
00:01:20,200 --> 00:01:25,170
학습 초기 단계에는 큰 스텝으로 움직이게 할 수 있고,

23
00:01:25,170 --> 00:01:29,060
학습이 점점 수렴해 갈 수록

24
00:01:29,060 --> 00:01:33,070
학습 속도를 느려지게 해서
점점 작은 스텝으로 움직이게 합니다.

25
00:01:33,070 --> 00:01:36,650
학습 속도 감쇠법
(learning rate decay)의
구현은 이렇습니다.

26
00:01:36,650 --> 00:01:40,640
전에 기억이 나시겠지만,
1 에폭(epoch)은 한번의 패스(pass),

27
00:01:42,512 --> 00:01:45,430
데이터들로을학습 루프를
죽 한번 돈 것입니다. 그렇죠?

28
00:01:45,430 --> 00:01:49,053
그럼, 이런 트레이닝 세트가 있다고 하면,

29
00:01:49,053 --> 00:01:53,866
미니 배치들로 쪼갤 수 있겠죠.

30
00:01:53,866 --> 00:02:00,446
첫번째 패스를 트레이닝 세트를 죽 실행하면
첫번째 에폭(epoch)이라고 하고,

31
00:02:00,446 --> 00:02:05,613
두번째 패스를 둘 째 에폭(epoch)이라고 하고,
이런 식으로 이어집니다.

32
00:02:05,613 --> 00:02:10,628
한가지 해 보면
러닝 속도 알파는

33
00:02:10,628 --> 00:02:15,464
1 / 1 + 감쇠비(decay rate)라는 파라미터에

34
00:02:18,112 --> 00:02:22,490
곱하기 에폭 값(epoch number)을 하고,

35
00:02:22,490 --> 00:02:26,890
여기에 러닝 속도 α의 초기값 α0을 곱해 줍니다.

36
00:02:26,890 --> 00:02:30,730
여기 감쇠비(decay rate)가
또 하나의 하이퍼 파라미터가 생겼죠.

37
00:02:30,730 --> 00:02:32,340
그래서, 이것도 튜닝할 필요가 있겠죠.

38
00:02:32,340 --> 00:02:33,910
구체적인 사례를 보겠습니다.

39
00:02:35,070 --> 00:02:39,659
여러 에폭(epoch)을 진행해 보면 즉,
데이터에 여러번 패스를 돌리면

40
00:02:39,659 --> 00:02:46,211
α0 는 0.2 이고, 감쇠비(decay-rate)는 1이면,

41
00:02:46,211 --> 00:02:50,267
그럼 첫번 에폭(epoch)에서

42
00:02:50,267 --> 00:02:55,268
α는 1/ (1 + 1 * α0)입니다.

43
00:02:55,268 --> 00:02:59,785
학습 속도는 0.1이 되겠죠.

44
00:02:59,785 --> 00:03:04,289
이 공식에 넣어보면 나오고요.
감쇠비(decay-rate)는 1이고,

45
00:03:04,289 --> 00:03:05,755
에폭 값(epoch-num)은 1일 때 말이죠.

46
00:03:05,755 --> 00:03:10,613
두번째 에폭(epoch)에서는
러닝 속도가 0.67로 감쇠되었습니다.

47
00:03:10,613 --> 00:03:15,924
세번째에서는 0.5,
네번째에서는 0.4. 등등.

48
00:03:15,924 --> 00:03:18,150
이 값들을 좀 더 구해보시길 바라고요.

49
00:03:18,150 --> 00:03:23,200
에폭 값(epoch number)의 함수로

50
00:03:23,200 --> 00:03:29,930
학습 속도는 점진적으로 작아지는 것이 감이 오죠.
위에 식에 따라서 말이죠.

51
00:03:29,930 --> 00:03:33,860
여러분이 학습속도감쇠법
(learning rate decay)을
사용하시려면요,

52
00:03:33,860 --> 00:03:38,830
다양한 값으로 이 하이퍼 파라미터 α0을 바꿔 보시고,

53
00:03:38,830 --> 00:03:41,550
이 감쇠비(decay rate) 값도 바꿔보시고요.

54
00:03:41,550 --> 00:03:44,710
그렇게 잘 동작하는 값을 찾아보세요.

55
00:03:44,710 --> 00:03:47,188
이 학습속도감쇠법
(learning rate decay)의 수식 외에도

56
00:03:47,188 --> 00:03:49,314
사람들이 쓰는 몇가지 또 다른 방법들이 있습니다.

57
00:03:49,314 --> 00:03:52,097
예를 들어, 이것을 지수감쇠(exponential decay)라 하는데요.

58
00:03:52,097 --> 00:03:58,009
알파 값은 1보다 작은 값인데,

59
00:03:58,009 --> 00:04:04,513
예를 들어, 0.95에 epoch-num 제곱,
곱하기 α0 합니다.

60
00:04:04,513 --> 00:04:10,500
그러면 이것은 기하급수적으로 빠르게
학습속도가 줄어들 것입니다.

61
00:04:10,500 --> 00:04:15,788
사람들이 쓰는 또 다른 공식은

62
00:04:15,788 --> 00:04:21,805
α = 어떤 상수 k / 루트 에폭값에 곱하기 α0.

63
00:04:21,805 --> 00:04:26,627
또는, 
또 하나의 하이퍼 파라미터죠. 어떤 상수 k에

64
00:04:26,627 --> 00:04:32,956
나누기 루트 미니 배치의 수 t 
곱하기 α0 입니다.

65
00:04:32,956 --> 00:04:37,627
그리고 종종 사람들이 사용하는 것 중에,
학습속도를 감소시키는데

66
00:04:37,627 --> 00:04:38,821
단계적으로 하는 방법도 있습니다.

67
00:04:38,821 --> 00:04:42,798
몇 스텝 동안은 특정 러닝속도로 하다가

68
00:04:42,798 --> 00:04:45,960
속도를 반으로 줄이고

69
00:04:45,960 --> 00:04:47,320
한동안 하다가 다시 반으로 줄이고,

70
00:04:47,320 --> 00:04:48,970
다시 조금 후에 반으로 줄이고,

71
00:04:48,970 --> 00:04:52,793
이렇게 불연속 계단식으로 말이죠.

72
00:04:55,954 --> 00:05:01,395
이제까지 몇가지 공식들을 얘기했죠. 수식을 이용해서

73
00:05:01,395 --> 00:05:05,210
학습 속도 알파를 시간에 따라 어떻게 변하게 할지.

74
00:05:05,210 --> 00:05:08,900
가끔 사람들이 하는 한 가지 방법은
수작업식 감쇠(manual decay)입니다.

75
00:05:08,900 --> 00:05:11,980
만약 한번에 한가지 모델로 트레이닝을 한다면,

76
00:05:11,980 --> 00:05:16,070
그 모델이 트레이닝 시키는데 몇 시간
또는 심지어 많은 날들이 소요된다면

77
00:05:16,070 --> 00:05:17,090
어떤 사람들은 이런 경우에 뭘 하냐면,

78
00:05:17,090 --> 00:05:21,638
그 많은 날들을, 그 모델이 학습 되는 것을
그저 보고 있을 것입니다.

79
00:05:21,638 --> 00:05:25,180
그러다 수동 방식인데
"학습 속도가 좀 줄면 좋겠네",

80
00:05:25,180 --> 00:05:27,180
"알파 값을 좀 줄여봐야지" 하죠.

81
00:05:27,180 --> 00:05:30,242
물론, 이 방법도 됩니다.
이렇게 알파값을 수동으로

82
00:05:30,242 --> 00:05:33,710
직접 손으로 시간마다 또는 날마다
튜닝해 주면요.

83
00:05:33,710 --> 00:05:37,140
이 방법은 트레이닝 모델 개수가
적을 때만 가능하지만

84
00:05:37,140 --> 00:05:39,100
종종 그렇게 하고 있습니다.

85
00:05:39,100 --> 00:05:43,580
이제 학습속도α 를 조정하는 방법에
좀 다양 옵션이 생겼습니다.

86
00:05:43,580 --> 00:05:46,630
여러분이 "하이퍼 파라미터들이 엄청나게 많네"
라고 생각이 든다면,

87
00:05:46,630 --> 00:05:49,320
"어떻게 이 많은 옵션 중에 좋은 것을 고르지?"

88
00:05:49,320 --> 00:05:51,190
일단, 저는 걱정하지 마시라고 말씀드리고요.

89
00:05:51,190 --> 00:05:56,550
다음 주에는 하이퍼 파라미터들을
체계젹으로 고르는 방법을
이야기 해보겠습니다.

90
00:05:56,550 --> 00:06:00,500
저라면 학습속도감쇠법(learning rate decay)은

91
00:06:00,500 --> 00:06:02,080
제가 해 볼 것 중에 더 나중으로 놓을 것입니다.

92
00:06:02,080 --> 00:06:05,670
알파를 정하는 것이,
일단 어떤 값으로 알파를 시작해서,

93
00:06:05,670 --> 00:06:07,080
잘 튜닝하는 것이 큰 효과가 있고요.

94
00:06:07,080 --> 00:06:09,050
학습속도감쇠법(learning rate decay)도
도움이 되기는 합니다.

95
00:06:09,050 --> 00:06:11,050
가끔 트레이닝의 속도를 정말로 올려주기도 하지만,

96
00:06:11,050 --> 00:06:15,720
개인적으로 시도할 것 중에서는
가장 먼저 할 일은 아닙니다.

97
00:06:15,720 --> 00:06:18,543
다음주에는 하이퍼 파라미터 튜닝에 관한 이야기를 하면서

98
00:06:18,543 --> 00:06:21,978
더 체계적 방법으로
이 모든 하이퍼 파라미터들을 다뤄 보겠습니다.

99
00:06:21,978 --> 00:06:24,422
또한,효율적으로 하이퍼 파라미터들을
찾는 방법도 같이 배우겠습니다.

100
00:06:24,422 --> 00:06:27,790
여기까지가 학습속도감쇠법(learning rate decay) 이었고요.

101
00:06:27,790 --> 00:06:31,420
마지막으로 국소 최적(local optima)에 관한 내용과

102
00:06:31,420 --> 00:06:33,390
신경망의 안장점(saddle point)에 대해 이야기를 하려는데요,

103
00:06:33,390 --> 00:06:36,210
여러분이 풀려는 최적화 문제들이나

104
00:06:36,210 --> 00:06:39,970
최적화 알고리즘들의 유형들에
좀 더 나은 직관이 생길 것입니다.

105
00:06:39,970 --> 00:06:41,840
신경망을 트레이닝시킬 때 생길 수 있는 이런 유형들을

106
00:06:41,840 --> 00:06:43,570
그럼, 다음 강좌에서 그것을 같이 알아 보죠.