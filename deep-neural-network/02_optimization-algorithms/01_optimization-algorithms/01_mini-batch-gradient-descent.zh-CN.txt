欢迎回来 本周我们要学习 加快神经网络训练速度的优化算法 我之前说过机器学习的应用<br />是一个高度依赖经验的 不断重复的过程 你需要训练很多模型才能找到一个确实好用的 所以能够快速的训练模型的确是个优势 令情况更艰难的是 在大数据领域中深度学习表现得并不算完美 我们能够训练基于大量数据的神经网络 而用大量数据训练就会很慢 所以你会发现快速的优化算法 好的优化算法 的确能大幅提高你和你的团队的效率 那么让我们从小批量梯度下降算法<br />(mini-batch gradient descent)开始 我们之前学过 矢量化(vectorization)可以 让你有效地计算所有m个样例 而不需要一个具体的for循环<br />就能处理整个训练集 这就是为什么我们要将所有的训练样例集中到 这些巨型的矩阵X中去 就是x1 x2直到xm的m个训练样例 Y也做类似处理 y1 y2 y3直到ym 所以X为nx*m维矩阵,Y为1*m维矩阵 矢量化运算能够相对快地处理M个样例 如果M非常大 速度依然会慢 例如 如果M是5百万或者5千万或者更大 对你的整个训练集运用梯度下降法 你必须 先处理你的整个训练集 才能在梯度下降中往前一小步 然后再处理一次 整个5百万的训练集 才能再往前一小步 所以实际上算法是可以加快的 如果你让梯度下降<br />在处理完整个巨型的5百万训练集之前 就开始有所成效 具体来说 你可以这样做 首先将你的训练集拆分成更小的 微小的训练集 即小批量训练集(mini-batch) 比如说每一个微型训练集只有1000个训练样例 也就是说 取x1至x1000作为第一个微训练集 也叫做小批量训练集 然后取接下来的1000个样例 x1001至x2000这1000个样例 依次继续 我要引入一个新的符号 把这些表示为X{1} 这些表示为 x{2} 现在 如果你总共有5百万个训练样例 每个小批量样例有1000个样例 则你有5000个这样的小批量样例<br />因为5000乘1000是5百万 即总共有5000个小批量样例 所以最后一个为X{5000} Y也作类似处理 做相应的拆分处理 这个命名为Y{1}<br />接下来这个包含y1001至y2000 命名为Y{2}<br />依次拆分 最后得到Y{5000} 第T个小批量样例 包括X,T和Y。T 即对应1000个训练样例的输入输出对 在进行下一步之前<br />先再明确一下我用的标记符号 我们之前用小括号上标X(i)表示训练集中的 第i个样例 用上标中括号[l] 索引神经网络的不同层 所以z[l]表示 神经网络第L层的z值 这里我们引入大括号{t}<br />代表不同的小批量样例 那么你就有了X{t} Y{t}<br />为了检验你对这些的理解 请问X{t}和Y{t}的维分别是多少 X的维度是nx*m 如果X{1}代表1000个样例<br />或者说1000个样例的x值 那么它的维度应该是nx*1000<br />X{2}的也是nx*1000 依此类推 即所有都是nx*1000维的 而这些应该是1*1000维的 要解释这个算法的名字 批量梯度下降(batch gradient descent) 可以先参考之前学习过的梯度下降法 该算法同时处理整个训练集 这个名字的由来就是 它同时处理整个训练集批次 我知道这不是一个特牛的名字<br />但是它就是这样叫的 小批量梯度下降 相对的 是指下一页将要介绍到的算法 该算法每次只处理一个小批量样例X{t} Y{t} 而不是一次处理完整个训练集X Y 我们来看看小批量梯度下降是怎么做的 在训练集上运行小批量梯度下降法的时候 t=1到5000都要运行一遍<br />因为我们有5000个子集 每个子集1000个样例 for循环里要做的基本上就是 用(X{t},Y{t})做一次梯度下降 就好像你有一个规模为1000的训练集 而你只是要实现你已熟知的算法 只不过现在在你的m为1000的子训练集上 而不是为全部1000个样例写一个for循环 也就是说用矢量化的方法同时处理1000个样例 让我们先写出来 首先对输入值运用前向传播(Forward Prop) 也就是对X{t} 计算z[1]等于W(1) 之前这里只有X 对吧 但是现在你不是在处理整个训练集 只是处理第一个小批量训练集 所以当你处理第T个小批量训练集时<br />对应的X为X{t} 然后有A[1]=g[1](Z[1]) 这个Z是大写的 因为这里要表达一个矢量含义<br />依次继续 直到A[l]=g[l](Z[l]) 得到你的预测值 你应该已经注意到了 这些都是矢量化运算 只不过这个矢量化的算法 每次只处理1000个样例而不是5百万个 然后你要计算代价函数J 这里要除以1000 1000是你的子训练集的规模 对i=1到l的(^y(i),y(i))的损失值求和 说明一下 这个i是指 子训练集(X{t},Y{t})中的样例 如果你要正则化 可以加入这个正则化项 将2移至分母然后乘以 l次求和的<br />弗罗贝尼乌斯范数(Frobenius norm)的平方 因为这只是一个小型训练集的代价函数值 我用上标大括号{t}标识J 你应该已经注意到我们正在做的与 之前的梯度下降法的实现一模一样 只不过之前是训练X Y 而不是X{t}和Y{t} 接下来运用反向传播(Back Prop) 以计算J{t}的梯度 仍然只使用X{t}和Y{t} 然后再更新权重W 实际上是W[l] 减αdW[l] b也做类似操作 这是小批量梯度下降算法处理训练集一轮的过程 刚刚我写的代码<br />也叫做训练集的一次遍历(epoch) 遍历是指过一遍训练集 只不过在批量梯度下降法中 对训练集的一轮处理只能得到一步梯度逼近 而小批量梯度下降法中对训练集的一轮处理 也就是一次遍历 可以得到5000步梯度逼近 当然你会一如既往地想对 训练集进行多轮遍历 你可以用另一个for循环或者while循环实现 所以你不断地训练训练集 并希望它收敛在某个近似收敛值 当你有一个大型训练集时 小批量梯度下降法比梯度下降法要快得多 这几乎是每个从事深度学习的人 在处理一个大型数据集时会采用的算法 下一节我们将继续深入小型梯度下降算法 来进一步理解它在做什么和它为什么会奏效