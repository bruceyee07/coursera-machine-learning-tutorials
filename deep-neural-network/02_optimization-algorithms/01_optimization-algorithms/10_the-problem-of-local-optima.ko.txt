딥러닝 분야 초기에 사람들은 최적화 알고리즘이 좋지 않은 국소 최적 값에 걸리는 것에 대해 매우 불안해 했습니다. 하지만 점차 딥러닝에 대한 이론적인 부분이 발달하면서 국소 최적에 대한 이해가 날로 변하고 있습니다. 딥러닝 분야에서 국소 최적에 대한 문제와 최적화 문제에 대해 현재 시점에서 어떻게 생각하고 있는지 보여드리겠습니다. 이 그림은 사람들이 국소 최적에 대해 걱정을 했을 때 생각했던 그림입니다. 여러분은 아마 특정 세트의 파라미터를 최적화하고 싶을 수 있습니다. 이것을 W1 과 W2라고 부릅니다. 표면의 높이는 비용함수입니다. 이 그림에서, 이런 다양한 지점에서 국소 최적값이 많은 것처럼 보입니다. 그리고 기울기 강하나 또는 다른 유형의 알고리즘이 전역최적값에 도달하기 보다는 국소 최적값에 속하는 경우가 쉽게 발생할 것입니다. 알고보면 2차원에서 이런 그림을 그리는 경우, 이런 그래프에서 다수의 국소 최적값을 쉽게 만들 수 있다는 것을 발견할 수 있습니다. 이런 저차원의 그래프에서 직관을 얻을 수 있는데요. 하지만 사실 이것이 올바르다고 할 수 없습니다. 신경망 네트워크를 새로 만들게되면, 기울기가 0인 지점이 항상 이런 지점처럼 국소 최적값인 것은 아닙니다. 대신에, 비용함수에서 기울기가 0인 대부분의 지점들은 안장점입니다. 다시 말씀드리면, 기울기가 0인 경우 그리고 파라미터 값 W1과 W2이고 높이가 비용함수 J의 값인 경우 말이죠. 비공식적으로는, 고차원의 공간에서 정의되는 함수에서 만약 기울기가 0인 경우, 방향에 따라서 convex light 함수이거나 concave light 함수일 수 있습니다. 만약에 고차원인 경우, 예를 들어, 20,000 차원의 공간의 경우 국소 최적값이 존재하기 위해서는 20,000가지의 방향은 이렇게 생겨야 할 것입니다. 이렇게 될 확률은 아마 매우 작을 것인데요. 아마 2의 마이너스 20,000승으로 말이죠. 반대로 커브가 이렇게 위 방향으로 구부러지는 경우가 더 많을 것입니다. 또는 이렇게 커브 함수가 아래로 구부러지는 경우도 발생할 것입니다. 모든 함수가 위로 올라가는 것이 아니라요. 그렇기 때문에 고차원의 공간에서는 오른쪽에서 보이는 그림과 같이 안장점을 접할 확률이 매우 높습니다. 국초 최적값 대신에 말이죠. 이 부분이 안장점이라고 불리는 이유는 모양을 상상해 보시면, 아마 이부분이 말 위에 올리는 안장과 비슷하게 생겼죠. 맞죠? 이 부분이 말이라고 볼 수 있겠네요. 여기가 말의 머리부분입니다. 여기는 눈이구요. 말을 잘 그리진 않았는데요. 대략적인 아이디어는 이렇습니다. 그럼 말 타는 사람은 여기 안장에 앉겠죠. 그렇기 때문에 여기 이 지점이, 기울기가 0인 지점이, 안장점이라고 불리는 것입니다. 아마 안장에서 여기 이 지점이 바로 앉는 부분이라고 하겠죠. 이 부분은 기울기가 0인 지점이기도 하구요. 그렇게해서 오늘 강좌를 통해 저희는 딥러닝의 역사에 대해 배웠는데요 저차원의 공간에 대한 내용을 많이 다루었습니다. 여기 이 왼쪽부분에서 그릴 수 있는 것과 같이 말이죠. 이 공간에는 고차원 공간에서 운영하는 알고리즘이 이동하지 않습니다. 20,000개의 매개 변수가 있기 때문에 20,000 차원의 벡터에서 정의되는 J함수가 있고 이 함수를 통해 국소 최적값을 보기보다는 안장점을 볼 확률이 더 높습니다. 국소 최적값이 문제가 되지 않는다면, 어떤 것이 문제가 될까요? plateau 가 러닝속도를 저하시킬 수 있는데요, plateau는 함수 기울기의 값이 0에 근접한 긴 범위를 이야기 합니다. 이 지점에 있다고 하면 기울기 강하가 표면을 따라 밑으로 이동할 것입니다. 그러면 기울기가 0이거나 0에 근접한 값이기 때문에, 표면이 이렇게 완만해 질 것입니다. 시간이 굉장히 많이 소요될 수 있습니다. 여기 정체구간에 도달하는데 말이죠 그리고 왼쪽, 오른쪽 무작위의 이동을 통해서 조금 더 잘 표시하기위해 펜 색깔을 바꿔보겠습니다. 알고리즘이 그 이후, 정체구간인 plateau에서 빠져나오게 됩니다. 이 길다란 슬로프를 따라 오면, 이 지점까지 오다가 여기서 plateau 지점을 나옵니다. 이 비디오에서 중요한 부분은 첫번째로, 비교적 신경망이 꽤 큰 네트워크를 트레이닝 하는 이상, 또한 파라미터가 많은 경우, 국소 최적값에 갇힐 확률은 작습니다. 그리고 J 비용함수는 비교적 고차원의 공간에서 정의됩니다. 두번째로, plateau가 문제이긴 한데요, 러닝의 속도를 늦추게 하는 요소이기도 한데요, 여기가 바로 momentum 도는 RmsProp 또는 Adam 과 같은 알고리즘이 도움을 줄 수 있는 부분입니다. 이런 plateau와 같은 시나리오 경우, 가장 정교한 observation 알고리즘인 Adam과 같은 알고리즘이 plateau를 빠져나오는 속도를 높혀줄 수 있습니다. plateau 구간을 지나서 완전히 빠져나오는데 도움을 준다는 것입니다. 여러분의 네트워크가 워낙 고차원의 공간에서 최적화 문제를 풀기 때문에 제가 생각하기에 사람들은 이런 공간이 어떻게 생겼는지 아직 잘 모를 수 있습니다. 또한 저희가 이런 공간을 이해하는 방식도 진화되고 있습니다. 이번 강의가 여러분이 직관적으로 최적화 알고리즘에 대해 이해하는데 도움이 되고 앞으로 직면하고 있는 과제를 이해하는데
도움이 되셨으면 좋겠습니다. 이번주의 마지막 강의에 도달하신 것에 축하드립니다. 이번주 퀴즈 학습을 보시길 바라구요, 또 연습학습도 진행하시기 바랍니다. 여러분이 이번주 아이디어를 바탕으로 연습 학습을 잘 진행하길 바랍니다. 다음주 비디오 강의에서 만나겠습니다.