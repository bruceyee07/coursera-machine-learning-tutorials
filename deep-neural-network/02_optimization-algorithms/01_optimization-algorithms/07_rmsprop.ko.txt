여러분은 모멘텀을 이용해서
기울기 강하를 빠르게 실행하는 법을 배웠는데요. 또 다른 RMSprop라는 알고리즘이 있습니다. 이것은 root mean square propagation의 약자인데요,
이 방법을 이용해 기울기 강하의 속도를 증가시킬 수 있습니다. 어떻게 작동하는지 보겠습니다. 이전 예제를 다시 한번 보겠습니다.
기울기 강하를 도입하는 경우엔, 세로로 아주 큰 변동이 일어날 수 있습니다. 가로줄로 나아가려고 할때 말이죠. 이 예제와 관련하여 직관적인 부분을 제공하겠습니다. 세로축이 파라미터 v이고,
가로축이 파라미터 w라고 하겠습니다. w1 과 w1일 수도 있겠죠. 중앙 파라미터 는 b와 w로 지정할 수도 있겠죠. 여러분이 b의 방향으로 학습 속도를 늦추려고 하거나, 세로방향으로 늦추는 것이겠죠. 그리고 가로방향으로는
속도를 늘리거나, 적어도 늦추지는 않습니다. RMSprop 알고리즘이 하는 일은
이와 같습니다. iteration업무 t에서 예전과 같이,
derivative dW를 계산할 것입니다. 현재 미니 배치에 대해 db도 말이죠. 이 기하급수적 가중평균은 유지하려고 했는데요, VdW대신에 저는 SdW 표기를 사용하겠습니다. 그러면 SdW는 베타 곱하기 이전 값들 더하기 1 빼기 베타 곱하기 dW 제곱입니다. 가끔씩은 dW 제곱으로 말이죠. 명료성을 위해, 여기 제곱은
element-wise 제곱입니다. 이것이 하는 역할은, 실제로 기하 급수적으로 가중된 미분의 제곱 평균을 유지하는 것입니다. 비슷하게, Sdb = 베타 Sdb 더하기 1 빼기 베타, db 제곱이 있는데요, 그리고,
여기 제곱은 마찬가지로 element-wise operation입니다. 다음으로, RMSprop은 다음과 같이
파라미터를 업데이트 시킵니다. W는 W 빼기 러닝속도로 업데이트 될 것이고, 이전에는 알파 곱하기 dW였죠.
이제는 dW 나누기 SdW의 루트입니다. 그리고 b는 b 빼기 러닝속도
곱하기 기울기만 적용하는 대신,
이 값은 Sdb로 나뉩니다. 이제 이게 어떻게 작동하는지 직관적인 부분을
다루어 보도록 하겠습니다. 기억하시겠지만, 가로 방향으로 또는, 이번 예제에서는, W방향으로
러닝이 빨리 이동하기 바랍니다. 세로 방향으로는 또는,
이 예제의 경우에는 b의 방향으로는, 세로축에서의 변동은 늦추고 싶습니다. 그럼 여기에서 있는 항, SdW와 Sdb는 우리가 하고 싶은 것은,
SdW는 비교적 작은 값으로, 그렇게해서 비교적 작은 값으로
나눌 수 있게 하고, Sdb는 비교적 크게됩니다.
여기서 yt를 비교적 큰 값으로 나누어서 세로축의 업데이트를
늦추기 위해서 말이죠. 실제로 derivative를 보면, 가로방향에서보다 세로방향으로 더 큽니다. 그러면 기울기는 b방향으로 매우 큽니다. 맞죠? 이런 derivative로, db의 값은 매우 크고,
dw의 값은 비교적 작은데요, 함수가 세로 방향으로 더 기울기가 크게 나오는데요, b방향보다, w방향보다,
가로 방향보다 말이죠. 그러므로 db의 제곱은 비교적 큰 값이 될 것입니다. 또 Sdb가 상대적으로 클텐데요,
반면에 이 값고 비교하면 dW는 더 작을 것입니다. 또는 dW 제곱은 더 작거나,
SdW가 더 작을 것입니다. 이것의 순수효과는 세로방향의 업데이트는 훨씬 더 큰 숫자로 나누어지기 때문에,
이것이 변동폭을 조금 더 무디게 만드는데 도와줍니다. 반면에 가로방향의 업데이트는
더 작은 숫자로 나누어집니다. 그렇기 때문에 RMSprop을
사용함으로써 생기는 순수영향은 업데이트가 이렇게 생기게 되는 것입니다. 이것은 세로방향의 업데이트이고요, 가로방향은 계속 진행하시면 됩니다. 이것의 효과 1가지는
그러므로 여러분이 더 큰 러닝속도 알파를 이용해도 됩니다. 그리하여
세로 방향으로 갈리지 않으면서
더 빠른 러닝을 진행할 수 있죠. 명료성을 위해서,
세로와 가로 방향을 각각 w와 b로
이렇게 나타냈는데요, 실제로는 아주 고차원의 파라미터
공간에 있는 것인데요, 그렇기 때문에 변동을 무디게 하려고 하는 세러방향 다이멘션은
w1, w2, w17의 파라미터 세트의 합입니다. 그리고 가로 다이멘션는
w3, w4 등등 이죠. 맞죠. 그러므로, w 와 p 가있다라는 거는 일러스트레이션뿐입니다. 실제로 dw는 굉장히 고차원의 파라미터 벡터입니다. Db 또한 고차원의 파라미터 벡터입니다. 하지만 직관적인 부분은,
이런 변동이 있는 부분의 다이멘션에서는 이렇게 더 큰 합을 산출하게 됩니다. 여기 derivative의 제곱의 가중평균값 말이죠. 그렇게하여 변동을 무디게 합니다. 이것이 RMSprop인데요, 이것은 root mean squared prop의 약자입니다. 여기서는 derivative을 제곱하고, 여기서는
마지막부분에서 루트를 적용하기 때문입니다. 마지막으로, 몇가지 알고리즘에 대한 상세내용에 대해
다음으로 넘어가기 전에 말씀드리겠습니다. 다음 비디오에서는, 저희는 RMSprop과 모멘텀을 합칠 것입니다. 그렇기 때문에, 모멘텀에서 썼던
하이퍼 파라미터 베타를 쓰는 대신에, 여기 하이퍼 파라미터 베타2라고 부를 것입니다.
충돌하게 하지 않기 위해서요. 모멘텀과 RMSprop 2개 동시에 쓰이는
하이퍼 파라미터 입니다. 또, 알고리즘이 0으로 나뉘지 않게 하기 위한 방법입니다. 만약 루트 Sdw가 0가 매우 가까우면 어떨까요? 그러면 여기 이 부분이 폭발적인 값을 가질 것입니다. 숫자적으로 안정적이게 하기 위해서,
실제로는 여기 이것을 도입할때, 분모에 아주 작은 앱실론의 값을 더합니다. 어떤 앱실론이 쓰여도 사실 상관은 없습니다. 10의 -8승이 합리적인 기본값인데요,
이 값은 조금 더 안정적인 수치를 제공합니다. 숫자의 반올림이나 다른 이유에서 말이죠. 여러분이 너무 작은 숫자로 나누지 않게 하기 위한
방편입니다. 이것이 RMSprop인데요,
모멘텀과 비슷하게, 변동을 무디게하는 효과가 있습니다. 기울기 강하, 미니 배치 기울기 강하 에서 말이죠. 여러분이 조금 더 큰 러닝속도 알파 값을 사용할 수 있게하고, 여러분의 알고리즘 러닝속도를 높혀줍니다. 자 이제 여러분은 RMSprop을 도입하는 방법을 알게되었는데요, 이제 러닝 알고리즘의 속도를 높히는 방법을 배웠습니다. RMSprop의 한가지 흥미로운 점은 이것이 논문으로 통해 처음에 발표된 것이 아니라 몇년 전에 Jeff Hinton이라는 분을 통해
Courera 코스에서 발표되었다는 점입니다. 학술적인 리서치 분야에서 보급 역할을 하는 것이
Coursera의 의도는 아니였지만 이 경우엔, 잘 된 경우라고 볼 수도 있겠네요. 그래서 Coursera 코스를 통해
RMSprop이 광범위하게 쓰이는 계기가 되었고 사람들에게 알려지고 뜨기 시작했습니다. 저희는 모멘텀에 대해서 이야기 했습니다. RMSprop에 대해서도 이야기했죠. 이것들을 같이 합치면 더 좋은 최적 알고리즘이 나오는데요, 이 내용은 다음 비디오에서 이야기하겠습니다.