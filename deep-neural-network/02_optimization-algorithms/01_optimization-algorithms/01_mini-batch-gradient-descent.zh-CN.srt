1
00:00:00,000 --> 00:00:01,700
欢迎回来

2
00:00:01,700 --> 00:00:04,625
本周我们要学习

3
00:00:04,625 --> 00:00:08,280
加快神经网络训练速度的优化算法

4
00:00:08,280 --> 00:00:12,630
我之前说过机器学习的应用<br />是一个高度依赖经验的

5
00:00:12,630 --> 00:00:14,320
不断重复的过程

6
00:00:14,320 --> 00:00:18,295
你需要训练很多模型才能找到一个确实好用的

7
00:00:18,295 --> 00:00:21,210
所以能够快速的训练模型的确是个优势

8
00:00:21,210 --> 00:00:23,280
令情况更艰难的是

9
00:00:23,280 --> 00:00:26,640
在大数据领域中深度学习表现得并不算完美

10
00:00:26,640 --> 00:00:29,310
我们能够训练基于大量数据的神经网络

11
00:00:29,310 --> 00:00:33,025
而用大量数据训练就会很慢

12
00:00:33,025 --> 00:00:36,820
所以你会发现快速的优化算法

13
00:00:36,820 --> 00:00:39,030
好的优化算法

14
00:00:39,030 --> 00:00:41,865
的确能大幅提高你和你的团队的效率

15
00:00:41,865 --> 00:00:45,939
那么让我们从小批量梯度下降算法<br />(mini-batch gradient descent)开始

16
00:00:45,939 --> 00:00:48,750
我们之前学过 矢量化(vectorization)可以

17
00:00:48,750 --> 00:00:51,720
让你有效地计算所有m个样例

18
00:00:51,720 --> 00:00:56,949
而不需要一个具体的for循环<br />就能处理整个训练集

19
00:00:56,949 --> 00:01:00,540
这就是为什么我们要将所有的训练样例集中到

20
00:01:00,540 --> 00:01:04,480
这些巨型的矩阵X中去

21
00:01:04,480 --> 00:01:12,945
就是x1 x2直到xm的m个训练样例

22
00:01:12,945 --> 00:01:15,055
Y也做类似处理 y1 y2

23
00:01:15,055 --> 00:01:22,635
y3直到ym

24
00:01:22,635 --> 00:01:30,355
所以X为nx*m维矩阵,Y为1*m维矩阵

25
00:01:30,355 --> 00:01:33,810
矢量化运算能够相对快地处理M个样例

26
00:01:33,810 --> 00:01:37,885
如果M非常大 速度依然会慢

27
00:01:37,885 --> 00:01:44,085
例如 如果M是5百万或者5千万或者更大

28
00:01:44,085 --> 00:01:48,010
对你的整个训练集运用梯度下降法

29
00:01:48,010 --> 00:01:49,530
你必须

30
00:01:49,530 --> 00:01:51,675
先处理你的整个训练集

31
00:01:51,675 --> 00:01:54,610
才能在梯度下降中往前一小步

32
00:01:54,610 --> 00:01:56,960
然后再处理一次

33
00:01:56,960 --> 00:01:58,680
整个5百万的训练集

34
00:01:58,680 --> 00:02:00,665
才能再往前一小步

35
00:02:00,665 --> 00:02:04,950
所以实际上算法是可以加快的

36
00:02:04,950 --> 00:02:10,260
如果你让梯度下降<br />在处理完整个巨型的5百万训练集之前

37
00:02:10,260 --> 00:02:14,255
就开始有所成效

38
00:02:14,255 --> 00:02:16,620
具体来说 你可以这样做

39
00:02:16,620 --> 00:02:19,750
首先将你的训练集拆分成更小的

40
00:02:19,750 --> 00:02:27,390
微小的训练集 即小批量训练集(mini-batch)

41
00:02:27,390 --> 00:02:35,553
比如说每一个微型训练集只有1000个训练样例

42
00:02:35,553 --> 00:02:42,320
也就是说 取x1至x1000作为第一个微训练集

43
00:02:42,320 --> 00:02:43,910
也叫做小批量训练集

44
00:02:43,910 --> 00:02:47,630
然后取接下来的1000个样例

45
00:02:47,630 --> 00:02:56,650
x1001至x2000这1000个样例 依次继续

46
00:02:56,650 --> 00:02:59,375
我要引入一个新的符号

47
00:02:59,375 --> 00:03:03,965
把这些表示为X{1}

48
00:03:03,965 --> 00:03:06,507
这些表示为

49
00:03:06,507 --> 00:03:11,940
x{2}

50
00:03:11,940 --> 00:03:15,160
现在 如果你总共有5百万个训练样例

51
00:03:15,160 --> 00:03:18,370
每个小批量样例有1000个样例

52
00:03:18,370 --> 00:03:24,460
则你有5000个这样的小批量样例<br />因为5000乘1000是5百万

53
00:03:24,460 --> 00:03:31,670
即总共有5000个小批量样例

54
00:03:31,670 --> 00:03:33,400
所以最后一个为X{5000}

55
00:03:33,400 --> 00:03:37,180
Y也作类似处理

56
00:03:37,180 --> 00:03:41,811
做相应的拆分处理

57
00:03:41,811 --> 00:03:50,805
这个命名为Y{1}<br />接下来这个包含y1001至y2000

58
00:03:50,805 --> 00:04:00,965
命名为Y{2}<br />依次拆分 最后得到Y{5000}

59
00:04:00,965 --> 00:04:08,500
第T个小批量样例

60
00:04:08,500 --> 00:04:12,770
包括X,T和Y。T

61
00:04:12,770 --> 00:04:18,220
即对应1000个训练样例的输入输出对

62
00:04:18,220 --> 00:04:22,295
在进行下一步之前<br />先再明确一下我用的标记符号

63
00:04:22,295 --> 00:04:27,465
我们之前用小括号上标X(i)表示训练集中的

64
00:04:27,465 --> 00:04:29,180
第i个样例

65
00:04:29,180 --> 00:04:31,630
用上标中括号[l]

66
00:04:31,630 --> 00:04:34,980
索引神经网络的不同层

67
00:04:34,980 --> 00:04:39,078
所以z[l]表示

68
00:04:39,078 --> 00:04:42,800
神经网络第L层的z值

69
00:04:42,800 --> 00:04:48,020
这里我们引入大括号{t}<br />代表不同的小批量样例

70
00:04:48,020 --> 00:04:53,960
那么你就有了X{t} Y{t}<br />为了检验你对这些的理解

71
00:04:53,960 --> 00:05:01,460
请问X{t}和Y{t}的维分别是多少

72
00:05:01,460 --> 00:05:04,880
X的维度是nx*m

73
00:05:04,880 --> 00:05:10,040
如果X{1}代表1000个样例<br />或者说1000个样例的x值

74
00:05:10,040 --> 00:05:19,260
那么它的维度应该是nx*1000<br />X{2}的也是nx*1000 依此类推

75
00:05:19,260 --> 00:05:22,940
即所有都是nx*1000维的

76
00:05:22,940 --> 00:05:29,200
而这些应该是1*1000维的

77
00:05:29,870 --> 00:05:34,563
要解释这个算法的名字

78
00:05:34,563 --> 00:05:37,130
批量梯度下降(batch gradient descent)

79
00:05:37,130 --> 00:05:40,250
可以先参考之前学习过的梯度下降法

80
00:05:40,250 --> 00:05:43,340
该算法同时处理整个训练集

81
00:05:43,340 --> 00:05:46,348
这个名字的由来就是

82
00:05:46,348 --> 00:05:49,545
它同时处理整个训练集批次

83
00:05:49,545 --> 00:05:53,100
我知道这不是一个特牛的名字<br />但是它就是这样叫的

84
00:05:53,100 --> 00:05:55,526
小批量梯度下降 相对的

85
00:05:55,526 --> 00:05:58,994
是指下一页将要介绍到的算法

86
00:05:58,994 --> 00:06:02,910
该算法每次只处理一个小批量样例X{t} Y{t}

87
00:06:02,910 --> 00:06:09,270
而不是一次处理完整个训练集X Y

88
00:06:09,270 --> 00:06:12,020
我们来看看小批量梯度下降是怎么做的

89
00:06:12,020 --> 00:06:17,765
在训练集上运行小批量梯度下降法的时候

90
00:06:17,765 --> 00:06:24,730
t=1到5000都要运行一遍<br />因为我们有5000个子集 每个子集1000个样例

91
00:06:24,730 --> 00:06:29,600
for循环里要做的基本上就是

92
00:06:29,600 --> 00:06:38,157
用(X{t},Y{t})做一次梯度下降

93
00:06:38,157 --> 00:06:48,340
就好像你有一个规模为1000的训练集

94
00:06:48,340 --> 00:06:51,130
而你只是要实现你已熟知的算法

95
00:06:51,130 --> 00:06:54,370
只不过现在在你的m为1000的子训练集上

96
00:06:54,370 --> 00:07:00,910
而不是为全部1000个样例写一个for循环

97
00:07:00,910 --> 00:07:06,595
也就是说用矢量化的方法同时处理1000个样例

98
00:07:06,595 --> 00:07:08,910
让我们先写出来

99
00:07:08,910 --> 00:07:15,710
首先对输入值运用前向传播(Forward Prop)

100
00:07:15,710 --> 00:07:24,315
也就是对X{t} 计算z[1]等于W(1)

101
00:07:24,315 --> 00:07:27,655
之前这里只有X 对吧

102
00:07:27,655 --> 00:07:30,040
但是现在你不是在处理整个训练集

103
00:07:30,040 --> 00:07:32,140
只是处理第一个小批量训练集

104
00:07:32,140 --> 00:07:36,065
所以当你处理第T个小批量训练集时<br />对应的X为X{t}

105
00:07:36,065 --> 00:07:45,420
然后有A[1]=g[1](Z[1])

106
00:07:45,420 --> 00:07:48,394
这个Z是大写的

107
00:07:48,394 --> 00:07:57,585
因为这里要表达一个矢量含义<br />依次继续 直到A[l]=g[l](Z[l])

108
00:07:57,585 --> 00:08:03,935
得到你的预测值

109
00:08:03,935 --> 00:08:09,005
你应该已经注意到了 这些都是矢量化运算

110
00:08:09,005 --> 00:08:14,125
只不过这个矢量化的算法

111
00:08:14,125 --> 00:08:18,840
每次只处理1000个样例而不是5百万个

112
00:08:18,840 --> 00:08:25,500
然后你要计算代价函数J 这里要除以1000

113
00:08:25,500 --> 00:08:32,895
1000是你的子训练集的规模

114
00:08:32,895 --> 00:08:38,580
对i=1到l的(^y(i),y(i))的损失值求和

115
00:08:38,580 --> 00:08:45,490
说明一下 这个i是指

116
00:08:45,490 --> 00:08:53,300
子训练集(X{t},Y{t})中的样例

117
00:08:53,300 --> 00:08:55,344
如果你要正则化

118
00:08:55,344 --> 00:08:59,295
可以加入这个正则化项

119
00:08:59,295 --> 00:09:03,345
将2移至分母然后乘以

120
00:09:03,345 --> 00:09:07,980
l次求和的<br />弗罗贝尼乌斯范数(Frobenius norm)的平方

121
00:09:07,980 --> 00:09:12,625
因为这只是一个小型训练集的代价函数值

122
00:09:12,625 --> 00:09:18,983
我用上标大括号{t}标识J

123
00:09:18,983 --> 00:09:23,925
你应该已经注意到我们正在做的与

124
00:09:23,925 --> 00:09:29,040
之前的梯度下降法的实现一模一样

125
00:09:29,040 --> 00:09:31,680
只不过之前是训练X Y 而不是X{t}和Y{t}

126
00:09:31,680 --> 00:09:36,470
接下来运用反向传播(Back Prop)

127
00:09:36,470 --> 00:09:44,285
以计算J{t}的梯度

128
00:09:44,285 --> 00:09:54,120
仍然只使用X{t}和Y{t} 然后再更新权重W

129
00:09:54,120 --> 00:09:59,410
实际上是W[l]

130
00:09:59,410 --> 00:10:08,124
减αdW[l] b也做类似操作

131
00:10:08,124 --> 00:10:17,620
这是小批量梯度下降算法处理训练集一轮的过程

132
00:10:17,620 --> 00:10:25,420
刚刚我写的代码<br />也叫做训练集的一次遍历(epoch)

133
00:10:25,420 --> 00:10:34,022
遍历是指过一遍训练集

134
00:10:34,022 --> 00:10:38,440
只不过在批量梯度下降法中

135
00:10:38,440 --> 00:10:44,420
对训练集的一轮处理只能得到一步梯度逼近

136
00:10:44,420 --> 00:10:48,475
而小批量梯度下降法中对训练集的一轮处理

137
00:10:48,475 --> 00:10:52,890
也就是一次遍历 可以得到5000步梯度逼近

138
00:10:52,890 --> 00:10:55,040
当然你会一如既往地想对

139
00:10:55,040 --> 00:10:58,430
训练集进行多轮遍历

140
00:10:58,430 --> 00:11:02,730
你可以用另一个for循环或者while循环实现

141
00:11:02,730 --> 00:11:05,180
所以你不断地训练训练集

142
00:11:05,180 --> 00:11:08,909
并希望它收敛在某个近似收敛值

143
00:11:08,909 --> 00:11:10,620
当你有一个大型训练集时

144
00:11:10,620 --> 00:11:15,330
小批量梯度下降法比梯度下降法要快得多

145
00:11:15,330 --> 00:11:17,540
这几乎是每个从事深度学习的人

146
00:11:17,540 --> 00:11:20,205
在处理一个大型数据集时会采用的算法

147
00:11:20,205 --> 00:11:24,230
下一节我们将继续深入小型梯度下降算法

148
00:11:24,230 --> 00:11:28,650
来进一步理解它在做什么和它为什么会奏效