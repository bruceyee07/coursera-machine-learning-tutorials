1
00:00:00,000 --> 00:00:02,340
Ao longo da história da aprendizagem profunda,

2
00:00:02,340 --> 00:00:05,700
muitos pesquisadores, incluindo alguns bem renomados,

3
00:00:05,700 --> 00:00:07,790
às vezes, propuseram algoritmos de otimização,

4
00:00:07,790 --> 00:00:09,825
e demonstraram que eles funcionavam bem em algumas ocasiões.

5
00:00:09,825 --> 00:00:13,440
Mas esses algoritmos de otimização posteriormente mostraram

6
00:00:13,440 --> 00:00:18,130
não se adequarem tão bem
à vasta gama de redes neurais que você poderia querer treinar.

7
00:00:18,130 --> 00:00:21,360
Então ao longo do tempo,
eu acho que a comunidade de aprendizagem profunda realmente desenvolveu

8
00:00:21,360 --> 00:00:25,597
um certo ceticismo em relação a algoritmos de otimização novos.

9
00:00:25,597 --> 00:00:29,350
E muitas pessoas sentiram
que gradiente descendente com impulso realmente funciona bem,

10
00:00:29,350 --> 00:00:32,720
então era difícil propor coisas que funcionassem muito melhor.

11
00:00:32,720 --> 00:00:36,070
Assim, a propagação da raiz das médias quadradas (RMS prop)
e o algoritmo de otimização Adam,

12
00:00:36,070 --> 00:00:37,730
sobre o qual falaremos neste vídeo,

13
00:00:37,730 --> 00:00:41,460
é um desses algoritmos raros que realmente se destacou,

14
00:00:41,460 --> 00:00:47,250
e tem demonstrado funcionar bem
em meio a uma ampla gama de arquiteturas de aprendizagem profunda.

15
00:00:47,250 --> 00:00:50,150
Então, esse é um dos algoritmos
que eu não hesitaria em recomendar que testassem,

16
00:00:50,150 --> 00:00:54,625
porque muitas pessoas o usaram
e viram que ele funciona bem em várias ocasiões.

17
00:00:54,625 --> 00:00:57,720
E o algoritmo de otimização Adam,
basicamente, pega

18
00:00:57,720 --> 00:01:01,250
o impulso e a 
propagação da raiz das médias quadradas e os une.

19
00:01:01,250 --> 00:01:03,105
Vamos ver como isso funciona!

20
00:01:03,105 --> 00:01:05,695
Para implementar o Adam, você pode inicializar:

21
00:01:05,695 --> 00:01:15,877
Vdw=0, Sdw=0 e da mesma forma, 
Vdb=0, Sdb=0.

22
00:01:15,877 --> 00:01:19,810
E então na iteração t,

23
00:01:19,810 --> 00:01:30,170
você calcula as derivadas: 
calcula dw, db, usando mini-batch.

24
00:01:30,170 --> 00:01:33,775
Assim, normalmente, você o faz com 
gradiente descendente de mini-batch.

25
00:01:33,775 --> 00:01:41,480
E assim você faz a média móvel exponencialmente ponderada 
do impulso. Então Vdw=ß.

26
00:01:41,480 --> 00:01:46,410
Mas agora, eu o chamarei ß₁
para distingui-lo deste hiperparâmetro

27
00:01:46,410 --> 00:01:52,660
ß₂ que nós usaremos para a 
propagação RMS disto.

28
00:01:52,660 --> 00:01:58,180
Então, isto é exatamente
o que nós tínhamos quando estávamos implementando

29
00:01:58,180 --> 00:02:03,788
o impulso, com a exceção de que agora
é chamado de hiperparâmetro ß₁, ao invés de ß.

30
00:02:03,788 --> 00:02:14,312
E da mesma forma, você tem Vdb, como sendo: 
1-ß₁ vezes db.

31
00:02:14,312 --> 00:02:18,685
E então você faz a atualização da propagação RMS também.

32
00:02:18,685 --> 00:02:26,630
Assim agora, Sdw = você tem um hiperparâmetro 
ß₂ Sdw₂ + (1 - ß₂ ) dw².

33
00:02:26,630 --> 00:02:33,325
E novamente, a potência ali
é a potência do produto de Hadamard das suas derivadas dw.

34
00:02:33,325 --> 00:02:44,005
E assim, Sdb = ß₂ Sdb₂ + (1 - ß₂ ) db²

35
00:02:44,005 --> 00:02:49,145
Logo, esta é a atualização do impulso com o hiperparâmetro

36
00:02:49,145 --> 00:02:55,318
ß₁ e esta é a atualização da propagação RMS
 com o hiperparâmetro ß₂.

37
00:02:55,318 --> 00:02:58,599
Na implementação do Adam,

38
00:02:58,599 --> 00:03:01,255
você realmente implementa 
correção do viés.

39
00:03:01,255 --> 00:03:04,215
Assim, você terá V corrigido.

40
00:03:04,215 --> 00:03:06,705
Corrigido significa 
"após correção de enviesamento".

41
00:03:06,705 --> 00:03:16,244
Vdw=Vdw / (1 - ß₁ ^ t) , 
se você tiver feito 't' iterações.

42
00:03:16,244 --> 00:03:25,040
E da mesma forma, Vdb corrigido
é igual a Vdb / (1 - ß1 elevado a t).

43
00:03:25,040 --> 00:03:30,756
E então similarmente,
você implementa esta correção de enviesamento no S também.

44
00:03:30,756 --> 00:03:37,405
Logo, isso é 
Sdw dividido por um menos ß₂ elevado a t

45
00:03:37,405 --> 00:03:48,700
e Sdb corrigido igual a 
Sdb dividido por um menos ß₂ elevado a t.

46
00:03:48,700 --> 00:03:50,660
Por fim, você executa a atualização.

47
00:03:50,660 --> 00:03:55,060
Então atualizamos W, como sendo
 W menos alfa vezes...

48
00:03:55,060 --> 00:03:59,870
se você estivesse implementando apenas o impulso,

49
00:03:59,870 --> 00:04:03,408
você usaria Vdw, ou talvez Vdw corrigido.

50
00:04:03,408 --> 00:04:06,615
Mas agora, nós adicionamos
 a porção da propagação da raiz disto.

51
00:04:06,615 --> 00:04:13,390
                              Logo, nós também vamos dividir
por raízes quadradas de Sdw corrigido mais épsilon.

52
00:04:13,390 --> 00:04:18,232
E da mesma forma, atualizamos 'b' de uma forma parecida,

53
00:04:18,232 --> 00:04:24,070
Vdb corrigido divido por raiz quadrada de Sdb corrigido,

54
00:04:24,070 --> 00:04:28,595
mais épsilon.

55
00:04:28,595 --> 00:04:33,070
E então, este algoritmo combina o efeito do gradiente descendente

56
00:04:33,070 --> 00:04:37,572
com impulso junto com o 
gradiente descendente com a propagação da raiz.

57
00:04:37,572 --> 00:04:41,740
E isto é um algoritmo de aprendizagem muito usado,

58
00:04:41,740 --> 00:04:46,640
que provou ser muito eficaz para muitas redes neurais
diferentes de uma ampla variedade de arquiteturas.

59
00:04:46,640 --> 00:04:49,805
Assim, este algoritmo tem vários hiperparâmetros.

60
00:04:49,805 --> 00:04:57,330
O hiperparâmetro alfa (taxa de aprendizagem) ainda é importante
e geralmente precisa ser adaptado.

61
00:04:57,330 --> 00:05:01,675
Então você tem apenas que experimentar
alguns valores de alfa e checar qual deles funciona.

62
00:05:01,675 --> 00:05:06,090
Uma opção comum, a realmente padrão, para ß₁ é 0.9.

63
00:05:06,090 --> 00:05:08,065
Então esta é uma média móvel,

64
00:05:08,065 --> 00:05:12,220
a média ponderada de dw, certo?
Esta é a luz dinâmica do impulso.

65
00:05:12,220 --> 00:05:15,455
O hiperparâmetro para ß₂,

66
00:05:15,455 --> 00:05:16,950
os autores do artigo do Adam,

67
00:05:16,950 --> 00:05:20,014
inventores do algoritmo de Adam, recomendam 0.999.

68
00:05:20,014 --> 00:05:26,485
Novamente, isto calcula a média móvel
exponencialmente ponderada de dw², assim como db².

69
00:05:26,485 --> 00:05:31,030
E então épsilon, a escolha de épsilon não importa muito.

70
00:05:31,030 --> 00:05:34,755
Porém os autores do artigo Adam recomendaram 10 elevado a menos 8.

71
00:05:34,755 --> 00:05:38,230
Mas este parâmetro, você realmente

72
00:05:38,230 --> 00:05:42,555
não precisa estabelecer, e isso não afeta muito o desempenho.

73
00:05:42,555 --> 00:05:44,280
Mas, ao implementar Adam,

74
00:05:44,280 --> 00:05:47,030
o que as pessoas geralmente fazem é apenas usar o valor padrão.

75
00:05:47,030 --> 00:05:49,960
Logo, ß₁ e ß₂, assim como épsilon.

76
00:05:49,960 --> 00:05:52,300
Eu não acho que alguém realmente ajuste épsilon.

77
00:05:52,300 --> 00:05:56,335
Então, tente uma variedade de valores de alfa 
e veja o que funciona melhor.

78
00:05:56,335 --> 00:05:59,140
Você também poderia ajustar ß₁ e ß₂,

79
00:05:59,140 --> 00:06:02,440
mas isso não é feito com frequência
entre os praticantes que conheço.

80
00:06:02,440 --> 00:06:06,100
Então, de vem o termo 'Adam'?

81
00:06:06,100 --> 00:06:15,267
Adam significa Estimativa do Momento Adaptável.

82
00:06:15,267 --> 00:06:18,175
Assim, ß₁ calcula a média das derivadas.

83
00:06:18,175 --> 00:06:19,780
Isto é chamado de primeiro momento.

84
00:06:19,780 --> 00:06:21,975
E ß₂ é usado para se calcular

85
00:06:21,975 --> 00:06:25,830
a média móvel exponencialmente
ponderada das potências, e isto é chamado de segundo momento.

86
00:06:25,830 --> 00:06:29,380
Então isso dá origem ao nome 
Estimativa do Momento Adaptável.

87
00:06:29,380 --> 00:06:32,875
Mas todos o chamam apenas de 
algoritmo de otimização Adam.

88
00:06:32,875 --> 00:06:37,800
E, a propósito, um dos meus mais antigos amigos
e colaboradores é chamado Adam Coates.

89
00:06:37,800 --> 00:06:40,425
Até onde sei, este algoritmo não tem nada a ver com ele,

90
00:06:40,425 --> 00:06:43,525
à parte do fato de ele o utilizar, às vezes.

91
00:06:43,525 --> 00:06:45,847
Mas algumas vezes, me perguntam sobre isso,

92
00:06:45,847 --> 00:06:47,945
então agora, você já sabe.

93
00:06:47,945 --> 00:06:51,187
Bom, isso é tudo sobre o algoritmo de otimização Adam.

94
00:06:51,187 --> 00:06:54,435
Com isso, eu acho que você será capaz de treinar suas redes neurais muito mais rápido.

95
00:06:54,435 --> 00:06:56,055
Mas antes de concluirmos esta semana,

96
00:06:56,055 --> 00:06:58,950
continuaremos falando sobre ajustes de hiperparâmetro,

97
00:06:58,950 --> 00:07:01,465
assim como sobre entender

98
00:07:01,465 --> 00:07:04,230
como um problema de otimização para redes neurais se parece.

99
00:07:04,230 --> 00:07:07,260
No próximo vídeo, falaremos sobre 
decaimento da taxa de aprendizagem.
[Tradução: Diogo dos Santos farias | Revisão: Carlos Lage]