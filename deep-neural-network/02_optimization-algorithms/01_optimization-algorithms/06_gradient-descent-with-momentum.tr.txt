Momentum veya Momentumlu dereceli alçalma adında standart dereceli alçalma algoritmasından daha hızlı çalışan bir algoritma vardır. Bunu tek cümleyle, buradaki temel fikir derecelerinizin üstel ağırlıklı ortalamasını hesaplamak ve bu dereceyi ağırlıklarınızı güncelleştirmenin yerine kullanmak olarak açıklayabiliriz. Bu videoda, bu cümleyi açalım ve bunu nasıl uygulayacağımızı görelim. Örneğin bu şekilde hatlara sahip bir maliyet fonksiyonunu optimize etmeye çalıştığımızı farz edelim. Buradaki kırmızı nokta minimumun konumunu belirtir. Belki burada bir dereceli alçalma başlatırsın ve dereceli alçalmanın ya da oraya giden bir alçalmanın bir yinelemesini alırsınız. Ama şimdi elipsin diğer tarafındasınız ve eğer bir adım daha atarsanız bunu bitirebilirsiniz. Ve ardından bir adım daha, bir adım daha ve dereceli alçalmanın birçok adım attığını göreceksiniz, değil mi? Yavaşça minimuma doğru salınım yapar. Bu yukarı aşağı salınımlar dereceli alçalmayı yavaşlatır ve sizin daha yüksek öğrenme oranı kullanmanızı önler. Özellikle, eğer daha yüksek bir öğrenme oranı kullanacak olsaydınız çekimi bitirir ve bu şekilde sapardınız. Böylece salınımın büyümesini önleme ihtiyacı sizi çok büyük olmayan bir öğrenme oranı kullanmanıza zorlar. Bu probleme başka bir açıdan bakarsak, dikey eksende öğrenmenin biraz daha yavaş olmasını istersiniz, çünkü bu salınımları istemezsiniz. Ama yatay eksende öğrenmenin daha hızlı olmasını istersiniz. Doğru çünkü öğrenmenin soldan sağa, minimuma doğru yani kırmızı noktaya doğru hareket etmesini istersiniz. İşte momentumlu dereceli alçalmayı uygularsanız yapabilecekleriniz. Her yinelemede veya özellikle yineleme sırasında dw,db gibi klasik türevleri hesaplarsınız. Ben bendeki üssü köşeli parantezi atlayacağım ama siz şu anki mini-batch'da dw ve db yi hesaplayacaksınız. Ve eğer dereceli alçalma kullanıyorsanız şu anki mini-batch sizin toplu yığınınız olacaktır. Ve bu dereceli alçalmayla gayet iyi çalışır. Böylece eğer şu anki mini-batch'ınız sizin tüm eğitim setiniz ise bu gayet iyi çalışacaktır. Daha sonra yapmanız gereken Vdw=ßVdw + (1 - ß)dw işlemini hesaplamaktır. Bu işlem daha önce hesapladığımız θ=ßVθ+1-ßθt işlemine benzemektedir. Evet bu aldığınız w'nun türevinin ortalama hareketini hesaplamaktır. Ve daha sonra aynı şekilde vdb=ßVdb+(1-ß)db işlemini hesaplarsınız. Daha sonra, dW'yi güncellemek yerine (W-öğrenme oranları çarpımı) olarak güncellenmiş W'yu kullanarak ağırlıklarınızı güncellersiniz. Türevle beraber, vdW ile güncellersiniz. Buna benzer olarak b de (b-α.vdb) olarak güncellenir. Yani bu dereceli alçalma basamaklarını yumuşatır. Örneğin, hesapladığınız türevlerden birkaçının bu, bu bu ve bu olduğunu farz edelim. Eğer bu gradyanların ortalamasını alırsanız, salınımların ortalama olarak sıfıra yakın yerlere yöneldiğini görürsünüz. İşte dikey eksende, öğrenmenin yavaşlamasını istediğiniz eksende, bu ortalama pozitif ve negatif sayılar çıkacaktır, böylece ortalama sıfıra yakın olacaktır. Buna karşılık yatay eksende, tüm türevler doğrudan yatay ekseni gösterir, böylece yatay eksendeki ortalama hala daha büyük kalır. İşte neden bu algoritmayı kullandığımızı birkaç yineleme ile en sonunda momentumlu dereceli alçalmayı dikey eksende daha küçük salınım adımları atarken buluyorsunuz. Ama bu yatay eksende hızlı hareket etmeye daha yatkın. Bu da algoritmanızın daha fazla kolay yol almasını veya minimum yolundaki salınımları en aza indirmesine izin verir. Bazı insanlarda çalışan bu momentum için sezgi, eğer kase şeklindeki fonksiyonunuzu minimize etmeye çalışıyorsanız bazı insanlarda çalışmayacaktır, değil mi? Bunlar gerçekten kasenin çizgileri. Sanırım çizim işinde pek iyi değilim. Onlar kase şeklindeki bu fonksiyonu minimize etmeye çalışır sonra şu türev terimlerinin aşağıya doğru yuvarlanan bir topun hız kazanmasını sağladığını düşünebilirsiniz. Ve bu momentum terimlerinin sürati temsil ettiğini düşünebilirsiniz. Şimdi bir kaseye sahip olduğunuzu düşünün, bir top alıyorsunuz ve türev bu küçük topa sanki aşağıya yuvarlanıyormuş gibi bir ivme kazandırıyor, değil mi? Böylece ivmeden dolayı top daha daha hızlı yuvarlanıyor. Ve veri -çünkü bu sayı birden çok ufak küçük- bir dizi sürtünme oluşturur ve bu da topunuzun sınırsız olarak hızlanmasını engeller. Ancak dereceli alçalmadan ziyade her bir adımı önceki adımlardan bağımsız olarak almak yeterlidir. Şimdi, küçük topunuz aşağıya yuvarlanıp momentum kazanabilir, ama bu kaseyi hızlandırabilir bu nedenle de momentum kazanabilir. Bunu kase içinde yuvarlanan bir topa benzetiyorum,bu fizik yasalarından hoşlanan bazı insanlar için çalışıyormuş gibi görünüyor. Ancak herkes için çalışmaz, işte eğer bu kasede aşağı yuvarlanan top benzetmesi sizde çalışmazsa, bunun için endişelenmeyin. Son olarak, bunu nasıl uygulayacağımız hakkında detayları konuşalım. İşte algoritma ve bu yüzden şimdi öğrenme oranının ağırlıklı ortalamanızı kontrol eden alfa ile birlikte beta adında iki hiper parametresine sahipsiniz. Beta'nın en yaygın değeri 0.9 dur. Son on günün sıcaklık ortalamasını hesaplıyoruz. Yani bu son on yinelemenin derecelerinin ortalamasıdır. Ve pratikte Beta'nın 0.9 değerine eşit olması gayet iyi çalışır. Diğer değerleri denemekte ve başka hiper parametreleri aramakta serbestsiniz, ama 0.9 biraz güçlü bir değer gibi görünüyor. Peki ya ön yargıları düzeltmeye ne dersiniz? Yani vdW ve vdb yi almak ve (1-B^t) ye bölmek istiyorsunuz. Pratikte insanlar bunu genelde kullanmaz çünkü 10 yinelemeden sonra hareketli ortalamanız ısınacak ve artık ön yargı tahmini olmayacaktır. Bu yüzden gerçekten insanların momentumlu dereceli alçalma uygularken ön yargı düzeltmesi ile bir sorununun olduğunu görmüyorum. Ve tabii ki bu işlem vdW=0 işlemini başlatacaktır. Bunun dW ile aynı boyutta ve aynı zamanda W ile de aynı boyutlarda olan bir sıfırlar matrisi olduğunu unutmayın. Ve Vdb de bir sıfırlar vektörüne başlatılmıştır. Yani, db ile aynı boyutlarda, aynı zamanda b ile de aynı boyuta sahiptir. Son olarak, eğer momentumlu dereceli alçalma ile ilgili şeyler okumuşsanız, genellikle bu terimin atlandığını, aynı zamanda (1-ß) nın da atlandığını görmüş olduğunuzdan bahsetmek istiyorum. Yani vdW, (ßvdW+dW)'ya eşittir. Ve şu morla gösterilen sürümü kullanmanın net etkisi, vdW'nin 1-ß katsayısı ya da gerçekten (1-ß)'dan 1 kat daha fazla ölçeklenmesiyle sonuçlanmasıdır. Ve böylece şu dereceli alçalma güncellemelerini yaptığınız zaman, alfa sadece (1-ß) üzeri 1'e karşılık gelen bir değerle değiştir. Uygulamada, ikisi de gayet iyi çalışacaktır, bu sadece alfa'nın en iyi öğrenme değerini etkileyecektir. Ama bu özel formülasyonun daha az sezgisel olduğunu düşünüyorum. Çünkü bunun bir etkisi de şudur, eğer Beta hiper parametresinin ayarını sonlandırırsanız, bu da vdW ve vdb'nin ölçeklenmesini etkiler. Ve böylece öğrenme oranını, belki de alfayı yeniden hesaplama ihtiyacınız biter. Şahsen ben (1-ß) terimini çıkarmak yerine solda yazdığım formülasyonu kullanmayı tercih ederim. Ama, bu yüzden soldaki basılı formülü (1-ß) terimiyle kullanmayı tercih ediyorum. Fakat Beta eşittir 0.9 kullanan iki versiyon da yaygın bir hiper parametre seçimidir. Sadece alfa'da öğrenme oranının bu iki farklı versiyon için farklı şekilde ayarlanması gerekecektir. İşte momentumlu dereceli alçalma bunun içindir. Bu hemen hemen her zaman anlaşılması kolay olan momentumsuz dereceli alçalma algoritmasından daha hızlı çalışacaktır. Ama hala daha öğrenme algoritmalarını hızlandırmak için yapmamız gereken başka şeyler vardır. Bunun hakkında sonraki videolarda konuşmaya devam edelim.