1
00:00:00,000 --> 00:00:01,710
在深度学习的早期阶段

2
00:00:01,710 --> 00:00:04,380
人们常常担心优化算法

3
00:00:04,380 --> 00:00:07,415
会陷入糟糕的局部最优(Local Optima)之中

4
00:00:07,415 --> 00:00:09,660
但随着深度学习理论的发展

5
00:00:09,660 --> 00:00:13,285
我们对局部最优的理解也在改变

6
00:00:13,285 --> 00:00:16,855
我来向大家介绍一下<br />现在我们是如何看待局部最优

7
00:00:16,855 --> 00:00:21,279
以及深度学习中的优化问题

8
00:00:21,279 --> 00:00:25,695
当人们担心局部最优的时候<br />他们脑海中浮现的往往是这幅图

9
00:00:25,695 --> 00:00:28,786
假设你正在优化某些参数

10
00:00:28,786 --> 00:00:30,580
比如W_1和W_2

11
00:00:30,580 --> 00:00:33,913
这个曲面的高度表示代价函数

12
00:00:33,913 --> 00:00:38,655
在这幅图中 看上去到处都是局部最优

13
00:00:38,655 --> 00:00:41,010
对于梯度下降

14
00:00:41,010 --> 00:00:43,622
或其他的算法

15
00:00:43,622 --> 00:00:47,226
都很容易陷入局部最优 而找不到全局最优

16
00:00:47,226 --> 00:00:51,945
如果你画的是一张二维的图

17
00:00:51,945 --> 00:00:56,637
确实容易像这样有很多局部最优

18
00:00:56,637 --> 00:01:00,285
而人们的直觉往往被这些低维图像所引导

19
00:01:00,285 --> 00:01:02,730
但实际上 这种直觉上的感受并不准确

20
00:01:02,730 --> 00:01:04,878
如果你是在训练一个神经网络

21
00:01:04,878 --> 00:01:09,965
代价函数中大部分梯度为零的点<br />实际上并不是像这样的局部最优

22
00:01:09,965 --> 00:01:15,330
而是鞍点(Saddle Point)

23
00:01:15,330 --> 00:01:17,845
这个点就是一个梯度为零的点

24
00:01:17,845 --> 00:01:19,826
这两个坐标轴是W_1 W_2

25
00:01:19,826 --> 00:01:25,150
高度代表代价函数J的值

26
00:01:25,150 --> 00:01:28,523
从经验上来说 对于一个高维空间的函数

27
00:01:28,523 --> 00:01:30,075
如果梯度为零

28
00:01:30,075 --> 00:01:32,835
则在每个方向上

29
00:01:32,835 --> 00:01:36,810
它可能是凸函数 或者是凹函数

30
00:01:36,810 --> 00:01:38,660
假设在一个

31
00:01:38,660 --> 00:01:40,785
2万维的空间中

32
00:01:40,785 --> 00:01:42,510
如果一个点要成为局部最优

33
00:01:42,510 --> 00:01:45,795
则需要在所有的2万个方向上都像这样

34
00:01:45,795 --> 00:01:49,274
因此这件事发生的概率非常低

35
00:01:49,274 --> 00:01:51,564
大概2的负2万次方

36
00:01:51,564 --> 00:01:57,945
你更有可能遇到的情况是<br />某些方向的曲线像这样向上弯曲

37
00:01:57,945 --> 00:02:01,140
同时另一些方向的曲线则向下弯曲

38
00:02:01,140 --> 00:02:04,720
并非所有曲线都向上弯曲

39
00:02:04,720 --> 00:02:07,430
这就是为什么在高维空间中

40
00:02:07,430 --> 00:02:10,270
你更有可能碰到一个像右图这样的鞍点

41
00:02:10,270 --> 00:02:13,575
而不是局部最优

42
00:02:13,575 --> 00:02:16,305
至于为什么把这样的曲面称为鞍点

43
00:02:16,305 --> 00:02:17,545
你可以想象一下

44
00:02:17,545 --> 00:02:21,060
这像是放在马身上的鞍 对吧

45
00:02:21,060 --> 00:02:23,165
比方说这是一匹马

46
00:02:23,165 --> 00:02:24,540
这是马的头

47
00:02:24,540 --> 00:02:28,390
这是马的眼睛

48
00:02:28,390 --> 00:02:33,235
马画的不太好 但你们可以感受一下

49
00:02:33,235 --> 00:02:34,530
然后你是那个骑马的人

50
00:02:34,530 --> 00:02:38,462
会坐在这个马鞍上

51
00:02:38,462 --> 00:02:41,585
这就是

52
00:02:41,585 --> 00:02:43,445
这个导数为零的点

53
00:02:43,445 --> 00:02:47,480
叫做鞍点的原因

54
00:02:47,480 --> 00:02:50,370
我猜你坐在马鞍上的那一点

55
00:02:50,370 --> 00:02:53,480
导数应该正好是零

56
00:02:53,480 --> 00:02:56,310
这就是我们在深度学习历史中学到的一课

57
00:02:56,310 --> 00:02:59,790
就是我们在低维空间里的大部分直观感受

58
00:02:59,790 --> 00:03:01,235
比如左边这幅图

59
00:03:01,235 --> 00:03:03,120
事实上并不适用于

60
00:03:03,120 --> 00:03:07,695
我们深度学习算法所应用的<br />那些高维空间中

61
00:03:07,695 --> 00:03:10,860
因为如果你有2万个参数

62
00:03:10,860 --> 00:03:14,399
那么J作为一个2万维向量的函数

63
00:03:14,399 --> 00:03:17,964
你更有可能遇到鞍点 而不是局部最优

64
00:03:17,964 --> 00:03:20,265
如果局部最优不是问题

65
00:03:20,265 --> 00:03:22,002
那么问题是什么呢

66
00:03:22,002 --> 00:03:26,155
真正会降低学习速度的<br />实际上是停滞区(Plateaus)

67
00:03:26,155 --> 00:03:31,635
停滞区指的是 导数长时间接近于零的一段区域

68
00:03:31,635 --> 00:03:33,915
如果你在这里

69
00:03:33,915 --> 00:03:38,230
那么梯度下降会沿着这个曲面向下移动

70
00:03:38,230 --> 00:03:41,250
然而因为梯度为零或接近于零

71
00:03:41,250 --> 00:03:42,829
曲面很平

72
00:03:42,829 --> 00:03:45,300
你会花费很长的时间

73
00:03:45,300 --> 00:03:51,555
缓慢地在停滞区里找到这个点

74
00:03:51,555 --> 00:03:53,820
然后因为左侧或右侧的随机扰动

75
00:03:53,820 --> 00:03:57,870
为了能更清晰地展示 我换一个颜色

76
00:03:57,870 --> 00:04:00,745
你的算法终于能够离开这个停滞区

77
00:04:00,745 --> 00:04:04,620
它一直沿着这个很长的坡往下走

78
00:04:04,620 --> 00:04:09,130
直到抵达此处 离开这个停滞区

79
00:04:09,130 --> 00:04:11,720
本次视频的要点是

80
00:04:11,720 --> 00:04:13,740
首先 实际上你不太可能<br />陷入糟糕的局部最优点

81
00:04:13,740 --> 00:04:17,150
只要你训练的是一个较大的神经网络

82
00:04:17,150 --> 00:04:18,555
有很多参数

83
00:04:18,555 --> 00:04:23,185
代价函数J定义在一个相对高维的空间上

84
00:04:23,185 --> 00:04:28,750
其次 停滞区是个问题<br />它会让学习过程变得相当慢

85
00:04:28,750 --> 00:04:31,826
这也是像动量(Momentum)算法<br />或RmsProp算法

86
00:04:31,826 --> 00:04:35,985
或Adam算法能改善你的学习算法的地方

87
00:04:35,985 --> 00:04:40,855
这些场景下 更复杂的算法 比如Adam算法

88
00:04:40,855 --> 00:04:43,570
可以加快沿停滞区向下移动

89
00:04:43,570 --> 00:04:46,720
然后离开停滞区的速度

90
00:04:46,720 --> 00:04:49,270
因为神经网络是在

91
00:04:49,270 --> 00:04:53,055
非常高维的空间上解决优化问题

92
00:04:53,055 --> 00:04:57,445
说实话 我觉得没有人能对<br />这类空间长什么样有非常好的直觉

93
00:04:57,445 --> 00:04:59,910
我们对它们的理解还在不断发展

94
00:04:59,910 --> 00:05:02,785
但我希望这节课能够让你们

95
00:05:02,785 --> 00:05:06,660
对优化算法所面对的挑战 有一些更直观的感受

96
00:05:06,660 --> 00:05:11,100
至此 恭喜你们学完了这周最后的内容

97
00:05:11,100 --> 00:05:15,275
请做一下这周的小测验和编程练习

98
00:05:15,275 --> 00:05:18,310
希望你们能在本周的编程练习中学以致用

99
00:05:18,310 --> 00:05:23,000
期待在下周的视频中再和你们见面