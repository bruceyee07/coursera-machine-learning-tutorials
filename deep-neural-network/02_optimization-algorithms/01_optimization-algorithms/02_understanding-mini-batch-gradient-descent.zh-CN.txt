在上一节中 你学习了如何使用小批量梯度下降 以及在仅部分处理训练集数据时 运行梯度下降算法 在这一节中 你将进一步学习如何使用梯度下降 并进一步学习它究竟在做什么以及为什么有效 在批量梯度下降算法中
每一次迭代你将遍历整个训练集 并希望代价函数的值随之不断减小 如果我们用 J 来表示代价函数 那么它应该随着迭代单调递减 如果某一次迭代它的值增加了
那么一定是哪里错了 也许是你的学习率太大 而在小批量梯度下降中
同样画图就会发现 并不是每一次迭代代价函数的值都会变小 从细节来看 每次迭代 都是对X{t} Y{t}的处理 所以对通过它们计算出来的代价函数值 J{t} 进行画图 这就好像每次迭代你都使用不同的训练集 也就是使用不同的小块 (mini-batch) 所以如果你对代价函数 J 画图 你就会看到类似这样的 它的趋势是向下的 但是也会有许多噪声 如果使用小批量梯度下降算法 经过几轮训练后 对 J{t} 作图结果很可能就像这样 它并不一定每次迭代都会下降 但是整体趋势必须是向下的 而它之所以有噪声 可能和计算代价函数时使用的
那个批次X{t} Y{t}有关 让你的代价函数的值或大或小 也可能这个批次里含有一些标签标错的数据 导致代价函数有一些高 等等 这就是为什么在使用小批量梯度下降时 得到的代价函数图像有这样的震动 你必须定义的一个参数是mini-batch的大小 如果m是训练集的大小 
一个极端的情况是 mini-batch的大小就等于m
这样其实就是批量梯度下降 在这种情况下你的mini-batch
只有一个X{1}和Y{1} 而它就等于你的整个训练集 所以如果把mini-batch的大小设置成m
你就得到了批量梯度下降 另一极端情况是把mini-batch的大小设为1 就会得到一种叫随机梯度下降的算法 这里每一条数据就是一个mini-batch 在这种情况下 先看第一个mini-batch
X{1} Y{1} 因为mini-batch的大小是1
所以其实就是训练数据的第一个样本 你对此运行梯度下降算法 然后 第二个mini-batch 其实就是第二个样本
对它们再运行梯度下降算法 然后再用第三个样本做类似的工作 每次都只使用一组训练数据 下面让我们看一下这两种方法
在优化代价函数时有什么不同 这是你想要最小化的代价函数的等高线图 你的最小值在这里 批量梯度下降算法可能从这里开始 它的噪声相对小些 每一步相对大些 并且最终可以达到最小值 而相对的 随机梯度下降算法 让我们选一个不同的点 假使从这里开始 这时对于每一次迭代你就在一个样本上做梯度下降 大多数时候你可以达到全局最小值 但是有时候也可能因为某组数据不太好 把你指向一个错误的方向 因此随机梯度算法的噪声会非常大 一般来说它会沿着正确的方向 但是有事也会指向错误的方向 而且随机梯度下降算法
最后也不会收敛到一个点 它一般会在最低点附近摆动 但是不会达到并且停在那里 实际上 mini-batch的大小一般会在这2个极端之间 一个在1和m之间的值
因为1和m都太小或太大了 以下是原因 如果你使用批量梯度下降算法 你的mini-batch大小就是m 那么你将在每一次迭代中遍历整个训练集 这样做最大的缺点是
如果你的训练集非常大 就将在每一次迭代上花费太长的时间 如果你的训练集比较小
那它还是一个不错的选择 相反 如果你使用随机梯度下降算法 使用一个样本来更新梯度 这没有问题 而且可以通过选择比较小的学习率 来减少噪声 但随机梯度下降有一个很大的缺点是 你失去了可以利用向量加速运算的机会 因为这里你每次只处理一个训练数据 这是非常没有效率的 所以更好的做法是选择一个中间值 让mini-batch的大小既不太大也不太小 这样你的训练才能最快 这么做有两个好处 第一你可以使用向量的方式运算 在上一节的例子中
如果你的mini-batch大小是1000 你就可以用一个向量同时处理这1000个样本 这就比一个个的处理要快许多 第二你可以不用等待整个训练集 都遍历完一遍才运行梯度下降 还是用前一个视频中的数字 每遍历一遍训练集
可以执行5000次梯度下降算法 所以在实践中这样选择mini-batch的大小是最好的 如果使用小批量梯度下降算法 我们从这里开始 可能第一次迭代是这样 第二次 第三次 第四次 它并不能保证总是可以达到最小值 但是相比随机梯度下降 它的噪声会更小 而且它不会总在最小值附近摆动 如果有什么问题 你可以缓慢的减小学习率 我们会在下一节中介绍学习率衰减 以及如何降低学习率 如果mini-batch的大小既不能是m也不能是1 而是在它们之间的一个值 那么该如何选择呢？ 这里有一些准则 第一 如果你的训练集较小
就使用批量梯度下降算法 这时候没有理由使用小批量梯度下降 因为你可以快速的处理整个训练集 所以使用批量梯度下降算法是没问题的 较小的定义我觉得是小于2000 这时使用批量梯度下降是非常适合的 否则 如果你有个更大的训练集 一般选择64到512作为mini-batch的大小 这是因为计算机内存的布局和访问方式 所以把mini-batch的大小设置为2的幂数
你的代码会运行的快一些 就像64是2的6次方 2的7次方 2的8次方 2的9次方 所以一般我会把mini-batch的大小
设置成2的幂数 我在上一节中我的mini-batch大小是1000 但我建议你就使用1024 2的10次方 但是1024这个值还是比较罕见的 而这些值更常用些 最后一个提醒是确保你的mini-batch 你所有的X{t} Y{t}
是可以放进你CPU/GPU 内存的 当然这和你的配置 以及一个训练样本的大小都有关系 但是如果你使用的mini-batch超过了 CPU/GPU 内存的容量 不管你怎么做 你都会发现 结果会突然变得很糟 我希望以上说的能让你对
mini-batch大小的标准范围 有更好的了解 当然mini-batch的大小也是一个超参数 可能你要做一个快速的搜索去确定哪一个值 可以让代价函数J下降的最快 所以我的做法是尝试几个不同的值 尝试几个不同的2的幂数
然后看能否找到那个 让你的梯度下降算法尽可能效率的值 希望这能给你一些指导在 对这个超参的选择上 现在你知道了如何使用小批量梯度下降算法 并且在大训练集时让你的算法跑的更快 但是事实上还有一些比梯度下降 或者小批量梯度下降更高效的算法 我们将在下面几讲介绍它们