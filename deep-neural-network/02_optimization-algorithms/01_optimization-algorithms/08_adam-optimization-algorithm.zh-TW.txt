在深度學習的歷史中 很多研究者，包括一些有名的研究者 有時會發明最佳化演算法 而且告訴大家他們在一些問題上很有效 但是那些優化演算法之後被發現 不太能廣泛運用，在眾多種你想訓練的神經網路上，
不一定都很有效。 所以隨著時間，我想深度學習的社群 面對新的最佳化演算法，開始會抱持一定的懷疑 很多人覺得動量梯度下降法真的很有用 很難再提出比這個更有效的了 所以 RMSprop，和 Adam 最佳化演算法 也就是這部影片會談到的 是少數經得起考驗的演算法 而且在很多類的深度學習架構上
都已被證明表現得很好 所以，我會毫不猶豫推薦你試試這個演算法 因為已經有很多人試過了，看起來在很多問題上都很有效。 Adam 最佳化演算法基本上，是拿 動量法和 RMSprop，把他們兩個摻在一起 那麼，讓我們看看他的運作。 要實作Adam，你會初始一些變數 V_dW = 0, S_dW = 0, 類似地 v_db = 0, s_db = 0 然後在第 t 個迭代 你會用現在的小批資料算出導數：dW, db 通常這會用在小批次梯度下降法 然後你算動量的指數加權平均，所以 V_dW = ß 不過我現在要叫他 ß_1，以便和 RMSprop部份的超參數 ß_2 分別 所以，這個跟我們之前實作的 和動量法一模一樣，只是現在叫這超參數 ß_1 而非 ß 然後類似地，你會有 v_db 像這樣...... (1-ß_1) * db 然後你進行和 RMSprop 很像的更新 你有另一個超參數 ß_2 ...... 加 (1-ß_2) 乘以 dW 的平方 再次強調，這邊的「平方」是對於 dW 的每個元素的。 然後 s_db 等於這個加上 (1-ß_2) * db
(筆誤, 應要有平方) 所以這邊的更新像動量法，用超參數 ß_1 而這邊的更新像RMSprop，用超參數 ß_2 在典型的 Adam 實作裡 你真的會實作偏差矯正 (bias correction) 所以你會有 V corrected "corrected" 意思是矯正過後 ... dW = V_dW 除以 1 - ß_1 的 t 次方
假設在做 t 個迭代 同樣地，v_db corrected 等於 v_db / (1 - ß_1^t) 然後類似地，你對於 S 系列也做偏差矯正 所以這是 S_dW 除以 (1 - ß_2^t)， s_db corrected = s_db / (1 - ß_2^t) 最後呢，你進行更新 所以 W 被更新為 W 減掉 alpha 乘上 — 如果你只做動量法，你會用 V_dW 或者是 V_dW corrected 但是呢，我們現在加入 RMSprop 的部份 所以我們也會除以 S_dW corrected + epsilon 的平方根 同樣地，b 用類似的式子更新 ...... v_db corrected 除以 s_db corrected + epsilon 開根號 因此，這個演算法結合了動量法 和 RMSprop 兩者的效果。 這一個演算法很常用，也被證明在 很多不同的神經網路、眾多類型的架構上都非常有效。 那麼，這個演算法有一些超參數 學習率這個超參數, alpha, 仍然很重要，需要調教 所以你必須試各種大小的值，看那個有效。 ß_1 常見的、預設的選擇是 0.9 所以這代表移動平均、 dW 的加權平均；這是動量部份的項。 對於超參數 ß_2 Adam 論文的作者 發明 Adam 法的人建議 0.999 這是拿來算 dW^2 和 db^2 的移動加權平均。 然後是 epsilon，其實 epsilon 的選擇關係不大 不過 Adam 論文的作者建議 10 的負8次方 不過這一個超參數，你真的不用 太在意，這不會影響表現很多。 不過呢，當實作 Adam 大家通常都只使用預設值 也就是 ß_1, ß_2 和 epsilon 我覺得沒有人真的在調 epsilon 然後嘗試不同大小的 alpha，看哪個最有效 你也可以調整 ß_1 和 ß_2，不過 我認識的人裡面這樣做的不多。 那麼，"Adam" 這個字是怎麼來的呢？ Adam 代表 ADAptive Moment estimation
(適應性矩估計) ß_1 在算導數的平均 這個叫 the first moment (一階矩) 而 ß_2 是拿來算 平方項的指數加權平均，這叫 the second moment (二階矩) 所以這引出了這名字 adaptive moment estimation 不過大家就叫他 Adam 最佳化演算法 題外話，我有一個老友夥伴，名字叫 Adam Coates 據我所知，這演算法和他一點關係都沒有 除了...我猜他有時候會用這個方法吧 不過我偶爾會被問這個問題 萬一你也在納悶的話。 那麼，這就是 Adam 最佳化演算法 用了他，我想你訓練神經網路能變得更飛快 但在我們結束這周之前 讓我們繼續談談超參數的調整 還有體會更多的感覺，關於 神經網路的優化問題看起來會是怎麼樣的。 在下一部影片，我們來談談 learning rate decay (學習率衰減)