1
00:00:00,000 --> 00:00:01,700
嗨,歡迎回來.

2
00:00:01,700 --> 00:00:04,625
在這禮拜，你會學到一些最佳化的演算法

3
00:00:04,625 --> 00:00:08,280
這能讓你訓練神經網路時做得更快

4
00:00:08,280 --> 00:00:12,630
你聽我說過，運用機器學習是個很吃經驗的過程

5
00:00:12,630 --> 00:00:14,320
需要反覆嘗試

6
00:00:14,320 --> 00:00:18,295
你需要訓練出很多個模型來找出很厲害的那個

7
00:00:18,295 --> 00:00:21,210
因此，能很快訓練出模型，這真的很重要。

8
00:00:21,210 --> 00:00:23,280
而這當中困難之處在於

9
00:00:23,280 --> 00:00:26,640
深度學習通常在資料很多的時候表現最好

10
00:00:26,640 --> 00:00:29,310
當你能拿大量資料來訓練你的神經網路的時候。

11
00:00:29,310 --> 00:00:33,025
然而，訓練大量資料就是慢。

12
00:00:33,025 --> 00:00:36,820
所以你會發現，擁有迅速的最佳化演算法，

13
00:00:36,820 --> 00:00:39,030
擁有很好的最佳化演算法，真的能夠

14
00:00:39,030 --> 00:00:41,865
增加你和你的團隊的效率。

15
00:00:41,865 --> 00:00:45,939
那麼，就讓我們從「小批次梯度下降法」來開始吧
(mini-batch gradient descent)

16
00:00:45,939 --> 00:00:48,750
在前面你學到向量化 (vectorization) 能讓你

17
00:00:48,750 --> 00:00:51,720
計算所有 m 筆資料時更有效率

18
00:00:51,720 --> 00:00:56,949
能讓你處理過整個訓練集，不用明確地寫for迴圈

19
00:00:56,949 --> 00:01:00,540
這就是為什麼我們會把訓練資料疊起來

20
00:01:00,540 --> 00:01:04,480
變成一個超大矩陣 X

21
00:01:04,480 --> 00:01:12,945
x(1), x(2), x(3)，最後到 x(m)，如果你有m筆資料

22
00:01:12,945 --> 00:01:15,055
Y也是類似情況，y(1),

23
00:01:15,055 --> 00:01:22,635
y(2), y(3), ... 到 y(m)

24
00:01:22,635 --> 00:01:30,355
所以 X 的維度是 n_x 乘 m，而這個是 1 乘 m。
向量化能

25
00:01:30,355 --> 00:01:33,810
讓你處理全部m筆資料時比較快一點

26
00:01:33,810 --> 00:01:37,885
而如果 m 很大的時候，這還是很慢

27
00:01:37,885 --> 00:01:44,085
例如，如果 m 是五百萬或五千萬、或甚至更大呢

28
00:01:44,085 --> 00:01:48,010
拿整個訓練集做梯度下降演算法時

29
00:01:48,010 --> 00:01:49,530
你要做的是 

30
00:01:49,530 --> 00:01:51,675
你必須要處理過所有的訓練資料

31
00:01:51,675 --> 00:01:54,610
才能踏出梯度下降的一小步

32
00:01:54,610 --> 00:01:56,960
然後你必須再處理過整個訓練集

33
00:01:56,960 --> 00:01:58,680
例如五百萬筆資料

34
00:01:58,680 --> 00:02:00,665
才能夠再踏出梯度下降的另一小步。

35
00:02:00,665 --> 00:02:04,950
其實，你有比較快的方法
— 如果你讓梯度下降

36
00:02:04,950 --> 00:02:10,260
在處理完你一整個資料前就先有一些進度

37
00:02:10,260 --> 00:02:14,255
在處理完那巨大的五百萬筆資料之前。

38
00:02:14,255 --> 00:02:16,620
特別來說，你可以這樣做

39
00:02:16,620 --> 00:02:19,750
假設你把訓練集分割成比較小的，

40
00:02:19,750 --> 00:02:27,390
一堆小小的資料集，我們稱這些小小的資料集叫「小批資料」
(mini-batches)

41
00:02:27,390 --> 00:02:35,553
而且假設你這些小小的資料集，每一個只有一千筆資料

42
00:02:35,553 --> 00:02:42,320
所以你把 x(1) 到 x(1000) 看做你的第一個小小資料集

43
00:02:42,320 --> 00:02:43,910
也就是小批資料 (mini-batch)

44
00:02:43,910 --> 00:02:47,630
然後，你拿接下來的一千筆資料

45
00:02:47,630 --> 00:02:56,650
x(1001) 到 x(2000) 這是一千筆，然後再下一批，依此類推

46
00:02:56,650 --> 00:02:59,375
我要用一個新的符號，我要

47
00:02:59,375 --> 00:03:03,965
稱這個為 X 上標、大括號

48
00:03:03,965 --> 00:03:06,507
1，然後稱這個為

49
00:03:06,507 --> 00:03:11,940
X 上標大括號2。

50
00:03:11,940 --> 00:03:15,160
那麼，如果你總共有五百萬筆訓練資料

51
00:03:15,160 --> 00:03:18,370
而每一小批資料都有一千筆

52
00:03:18,370 --> 00:03:24,460
那你就會有5000批的小批資料，因為5000乘以一千是五百萬

53
00:03:24,460 --> 00:03:31,670
那總共你會有5000個這些小批資料

54
00:03:31,670 --> 00:03:33,400
所以結尾是 X｛5000｝，

55
00:03:33,400 --> 00:03:37,180
然後呢，你對 Y 做類似的事情

56
00:03:37,180 --> 00:03:41,811
你也把相對應的訓練資料的 Y 分開

57
00:03:41,811 --> 00:03:50,805
所以叫這 Y｛1｝，然後這是 y(1001) 到 y(2000)

58
00:03:50,805 --> 00:04:00,965
這些變成 Y｛2｝，依此類推，直到最後你有 Y｛5000｝。

59
00:04:00,965 --> 00:04:08,500
所以呢，第 t 個小批資料會包括

60
00:04:08,500 --> 00:04:12,770
X｛t｝和 Y｛t｝

61
00:04:12,770 --> 00:04:18,220
也就是一千筆訓練資料，亦即其相對應的輸入輸出對

62
00:04:18,220 --> 00:04:22,295
在進到下一步前，只是想確認一下符號很清楚

63
00:04:22,295 --> 00:04:27,465
我們之前用上標小括號 i 來表示訓練資料的索引，
所以 x(i)

64
00:04:27,465 --> 00:04:29,180
是第 i 個訓練資料。

65
00:04:29,180 --> 00:04:31,630
我們用上標中括號

66
00:04:31,630 --> 00:04:34,980
[l] 來代表神經網路內，不同層的索引

67
00:04:34,980 --> 00:04:39,078
所以 z[l] 是

68
00:04:39,078 --> 00:04:42,800
神經網路第 l 層的 z 值。
而今天我們介紹了

69
00:04:42,800 --> 00:04:48,020
大括號 t 來代表不同小批資料的索引，

70
00:04:48,020 --> 00:04:53,960
所以你有 X｛t｝、Y｛t｝。
來看看你是否真的理解：

71
00:04:53,960 --> 00:05:01,460
X｛t｝和 Y｛t｝的維度各是多少？

72
00:05:01,460 --> 00:05:04,880
嗯，X 是 n_x乘m，所以

73
00:05:04,880 --> 00:05:10,040
如果 X｛1｝是一千筆資料，或說那千筆資料的值

74
00:05:10,040 --> 00:05:19,260
那他的維度應該是 n_x乘1000，而
X｛2｝也會是n_x乘1000，依此類推

75
00:05:19,260 --> 00:05:22,940
所以這些全部的維度都應該是 n_x 乘 1000

76
00:05:22,940 --> 00:05:29,200
而這些維度都要是 1 乘 1000。

77
00:05:29,870 --> 00:05:34,563
要解釋這演算法的名字的話

78
00:05:34,563 --> 00:05:37,130
批次梯度下降法 (batch gradient descent)，也就是

79
00:05:37,130 --> 00:05:40,250
我們之前不斷提到的梯度下降演算法，

80
00:05:40,250 --> 00:05:43,340
也就是你同時處理整個訓練集。

81
00:05:43,340 --> 00:05:46,348
這名稱是從這樣來的：

82
00:05:46,348 --> 00:05:49,545
同時處理「整批」訓練資料。

83
00:05:49,545 --> 00:05:53,100
我不確定這命名好不好，不過這就是這樣叫的。

84
00:05:53,100 --> 00:05:55,526
相對地，小批次梯度下降 (mini-batch gradient descent)

85
00:05:55,526 --> 00:05:58,994
也就是下一張投影片會講的演算法

86
00:05:58,994 --> 00:06:02,910
你一次處理「一小批」(mini batch)
的資料 X｛t｝

87
00:06:02,910 --> 00:06:09,270
Y｛t｝，而不是同時處理整個訓練集 X, Y。

88
00:06:09,270 --> 00:06:12,020
那麼，就讓我們瞧瞧小批次梯度下降法怎麼做吧

89
00:06:12,020 --> 00:06:17,765
要在你的訓練資料上執行小批次梯度下降，
你會跑過 t =

90
00:06:17,765 --> 00:06:24,730
1 到 5000，因為我們有 5000 批小量的資料，每批1000筆

91
00:06:24,730 --> 00:06:29,600
在這 for 迴圈你要做的，基本上是實作一步的

92
00:06:29,600 --> 00:06:38,157
梯度下降，且是用 X｛t｝, Y｛t｝

93
00:06:38,157 --> 00:06:48,340
就彷彿你只有1000筆訓練資料一樣

94
00:06:48,340 --> 00:06:51,130
也彷彿要你實作已經很熟的那個

95
00:06:51,130 --> 00:06:54,370
演算法，不過只在這很小的訓練集上

96
00:06:54,370 --> 00:07:00,910
大小是 m=1000。
其中並不用真的 for 迴圈跑過所有1000筆資料

97
00:07:00,910 --> 00:07:06,595
而是用向量化 (vectorization) 的方法
同時處理全部這1000筆。

98
00:07:06,595 --> 00:07:08,910
讓我們寫下來：首先

99
00:07:08,910 --> 00:07:15,710
你對輸入資料做正向傳播

100
00:07:15,710 --> 00:07:24,315
所以只有 X｛t｝；要這樣的方法是讓 Z[1] 等於 W[1]

101
00:07:24,315 --> 00:07:27,655
在之前，我們只會用 X 對吧

102
00:07:27,655 --> 00:07:30,040
不過現在，你不是處理整個訓練集

103
00:07:30,040 --> 00:07:32,140
你只是處理第一小批的資料，

104
00:07:32,140 --> 00:07:36,065
所以處理第 t 批小量資料時，這裡變成 X｛t｝

105
00:07:36,065 --> 00:07:45,420
然後你有 A[1] = g[1](Z[1])

106
00:07:45,420 --> 00:07:48,394
— 這邊是大寫 Z，因為實際上

107
00:07:48,394 --> 00:07:57,585
用的是向量化的作法 — 依此類推，直到 A[L]

108
00:07:57,585 --> 00:08:03,935
是 g[L](Z[L])，而這就是你的預測值

109
00:08:03,935 --> 00:08:09,005
注意到，你這裡應該要用向量化的實作方式

110
00:08:09,005 --> 00:08:14,125
只是這邊的向量化一次只處理

111
00:08:14,125 --> 00:08:18,840
1000筆資料而不是五百萬筆資料。

112
00:08:18,840 --> 00:08:25,500
接下來，你計算成本函數 J，在這邊我會寫成

113
00:08:25,500 --> 00:08:32,895
一千分之一：因為1000是小批訓練集的大小

114
00:08:32,895 --> 00:08:38,580
然後 i 從 1 到 l 把損失函數

115
00:08:38,580 --> 00:08:45,490
y hat (i), y(i) 加起來。
說明一下這邊的記號

116
00:08:45,490 --> 00:08:53,300
是代表從 X｛t｝, Y｛t｝小批資料來的。

117
00:08:53,300 --> 00:08:55,344
如果你要用正則化

118
00:08:55,344 --> 00:08:59,295
你也可以有這一項正則化的項

119
00:08:59,295 --> 00:09:03,345
— 分母這邊有個2 — 乘以對 l 相加

120
00:09:03,345 --> 00:09:07,980
權重矩陣的 Frobenius norm (弗比尼斯範數)，然後平方。

121
00:09:07,980 --> 00:09:12,625
因為這其實只是一小批資料的成本

122
00:09:12,625 --> 00:09:18,983
我要把這成本 J 弄個上標、大括號 t。

123
00:09:18,983 --> 00:09:23,925
你可以注意到，我們這邊所做的都和

124
00:09:23,925 --> 00:09:29,040
之前實作梯度下降法的時候一模一樣
— 除了不是做在 X, Y，

125
00:09:29,040 --> 00:09:31,680
而是做在 X｛t｝, Y｛t｝。

126
00:09:31,680 --> 00:09:36,470
接下來，你實作反向傳播

127
00:09:36,470 --> 00:09:44,285
計算 J｛t｝的梯度

128
00:09:44,285 --> 00:09:54,120
你這邊只看 X｛t｝, Y｛t｝，然後你更新權重

129
00:09:54,120 --> 00:09:59,410
每個 W[l] 更新成 W[l]

130
00:09:59,410 --> 00:10:08,124
減掉 alpha dW[l]；而 b 也類似。

131
00:10:08,124 --> 00:10:17,620
所以這邊的流程，
就是用小批次梯度下降把整個訓練集處理過一次

132
00:10:17,620 --> 00:10:25,420
這邊寫的程式碼也稱為「做一個 "epoch" 的訓練」

133
00:10:25,420 --> 00:10:34,022
"epoch" 的意思是把訓練集掃過一遍

134
00:10:34,022 --> 00:10:38,440
在批次梯度下降法

135
00:10:38,440 --> 00:10:44,420
把訓練集掃過一遍只能讓你走一步的梯度下降

136
00:10:44,420 --> 00:10:48,475
而用小批次梯度下降法，把訓練集掃過一遍

137
00:10:48,475 --> 00:10:52,890
也就是一個 "epoch"，能讓你走5000步的梯度下降。

138
00:10:52,890 --> 00:10:55,040
當然，你會希望

139
00:10:55,040 --> 00:10:58,430
掃過訓練集很多遍，通常都會這樣，

140
00:10:58,430 --> 00:11:02,730
你大概會在外面需要另一個 for 迴圈或 while 迴圈

141
00:11:02,730 --> 00:11:05,180
所以你不斷地掃過訓練資料

142
00:11:05,180 --> 00:11:08,909
直到收斂或近似收斂為止。

143
00:11:08,909 --> 00:11:10,620
當你有非常多的訓練資料

144
00:11:10,620 --> 00:11:15,330
小批次梯度下降跑得比批次梯度下降還快得多

145
00:11:15,330 --> 00:11:17,540
大概每個做深度學習的人

146
00:11:17,540 --> 00:11:20,205
在訓練很大的資料集時都會使用之

147
00:11:20,205 --> 00:11:24,230
在下部影片，讓我們更深入探討小批次梯度下降法

148
00:11:24,230 --> 00:11:28,650
讓你更清楚知道它在做什麼，還有為什麼能表現這麼好