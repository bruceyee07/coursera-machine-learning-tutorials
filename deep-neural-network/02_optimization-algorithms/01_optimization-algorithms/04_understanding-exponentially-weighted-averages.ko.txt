이전 비디오에서는 지수 가중 평균에 대하여 이야기했었는데요, 여러분이 신경망을 트레이닝하는데 사용한 여러 최적화 알고리즘의 주요 요소가 될 것입니다. 그러므로 이번 비디오에서는, 이런 알고리즘이 하는 내용에 대한 직관적인 부분에 대해서 조금 자세히 살펴보도록 하겠습니다. 기억하시겠지만 이것이 지수 가중 평균의 중요 공식입니다. 여기서 베타의 값이 0.9인 경우 빨간색 선이 나오는데요, 만약에 그 값이 1에서 가까울 경우, 만약 0.98이면, 초록색의 선이 나옵니다. 만약 그 값이 더 작을 경우, 0.5같은 경우엔, 여기와 같은 노란색 선이 됩니다. 조금 더 수학적인 부분을 자세히 보겠습니다. 일별 평균 기온을 어떻게 산출하는지 한번 이해해보도록 하겠습니다. 여기는 아까봤던 그 공식인데요, 베타의 값을 0.9로 설정하고, 이 값에 상응하는 몇개의 식을 더 적어보겠습니다. 만약에 T의 값이 0에서 1로 그리고 2에서 3으로 T가 늘어날때 의 값을 분석하는 반면에 여기서는 T의 값이 감소할때 의 경우를 적었습니다. 여기 첫번째 식을 보겠습니다. V100이 실제로는 어떤 의미인지 보겠습니다. V100은 어떻게 되냐면, 여기 2개의 항을 서로 맞바꿔 볼텐데요, 여기는 0.1 곱하기 쎄타 100에 더하기 0.9 곱하기 전날의 기온을 해줍니다. 그러면 V99는 또 어떻게 되죠? 여기 식에 이 식을 대입해보겠습니다. 이 값은 그냥 0.1 곱하기 쎄타 99, 그리고 다시 2개의 항을 서로 맞바꿨습니다. 더하기 0.9 곱하기 V98입니다. 그러면 또 V98은 또 어떻게 되죠? 자, 여기서 구할 수 있는데요, 이곳에다가 대입시키면 됩니다. 0.1 곱하기 쎄타 98 더하기 0.9 곱하기 V97 등등 말이죠. 그리고 여기 항들을 모두 곱해버리면 V100 은 0.1 곱하기 쎄타 100 더하기 자, 이제 쎄타99의 계수를 한번 보겠습니다. 그 값은 0.1 곱하기 0.9 곱하기 쎄타99 그리고 이제 쎄타 98의 계수를 보겠습니다. 0.1 한번 곱하고 0.9 곱하기 0.9 식들을 전개를 하면, 이 값은 0.1 곱하기 0.9 의 제곱 곱하기 쎄타 98 이것을 계속 전개해서 괄호를 풀면 이 값은 0.1 곱하기 0.9의 3승 쎄타 97 더하기 0.1 곱하기 0.9의 4승 곱하기 쎄타 96 더하기... 이런 방법으로 합을 구하면 되고, 쎄타100에 대한 가중평균치 인데요, 그 해의 100일간의 온도를 계산한 V100 까지 반영하여 현재 시점의 기온을 뜻합니다. 이것은 쎄타 100과 쎄타 99 쎄타 98, 쎄타 97, 쎄타 96 등등의 합을 나타내는 값입니다. 이것을 그림으로 그리는 방법 중 한가지는, 몇일간의 기온이 있다고 가정해보겠습니다. 이것은 쎄타이고, 이것은 T이고요, 그러면 쎄타100은 어떤 값을 띌 것이고, 쎄타99도 어떤 값이 있을 것이고, 쎄타 98도 그럴 것입니다. 그러면 T는 100, 99, 98, 등등 이렇게 값을 가질 것이고, 그렇죠, 이렇게 몇일의 기온이 있을 것입니다. 그러면 이제 기하급수로 감소하는 함수 하나를 그려볼텐데요. 0.1에서 시작해서 0.9까지 곱하기 0.1 에서 0.9제곱 곱하기 0.1 에서 등등 이렇게 말이죠. 그렇게해서 이렇게 기하급수적으로 감소하는 함수를 되었죠. 여러분이 V100을 계산하는 방법은, 여기 이 두 함수 간에 각 항 별로 곱하고나서 합하면 됩니다. 그러면 여기 이 값에다 쎄타 100 곱하기 0.1 을 하고, 여기 이 값에는 쎄타 99 곱하기 0.1 곱하기 0.9, 이게 2번째 항이고, 계속 이렇게 합니다. 즉, 여기에서처럼, 일별 기온을 갖고 기하급수적으로 감소하는 함수에 곱한다음, 값을 더하는 것입니다. 그러면 그것이 V100이 됩니다. 알고보면, 나중에 제가 설명을 드리겠지만, 여기에 있는 모든 계수들을 더하면 1이 되거나 1과 매우 근접한 값을 갖게됩니다. 편이보정(bias correction)이라는 값과 이것에 대한 내용은 다음 비디오에서 다루겠지만, 이런 점 때문에 지수 가중 평균이라고 합니다. 마지막으로, 여러분은 궁금해할 수 있는데요, 몇일간의 기록으로 기온이 평균값을 갖는지 말이죠. 0.9의 10승을 해 보면, 그 값이 0.35정도 되는데요, 이 값은 대략 1 나누기 e로, 자연로그의 밑(base), e 분의 1이 됩니다. 조금 더 일반적으로,1-앱실론으로 써 보면, 이번 예제에서는 앱실론은 0.1일 것입니다. 그러므로 이 값이 0.9이면, (1-앱실론)의 (1/앱실론) 승이면 이 값은 1/e 정도 됩니다. 그 값은 0.34 에서 0.35 정도 되구요. 다시 말해서, 여기 이 높이가 3분의 1정도로 감소하는데 약 10일 정도가 소요됩니다. 대략 피크의 1/e 이죠. 이렇기 때문에, 베타가 0.9인 경우, 이것은 마치 여러분이 직전 10일 간의 기온만 집중하여 지수 가중 평균을 구하는 것과 같은데요, 약 10일 정도 이후 가중치가 현재 날짜 가중치 3분의 1보다 약간 더 감소하기 때문에 그렇습니다. 반면에 베타의 값이 0.98이였다고 하면, 그러면, 0.98의 몇승을 해야 이 값이 매우 작은 값이 될까요? 알고보니 0.98의 50승이면 대략 1/e와 비슷한 값이 됩니다. 그렇기 때문에 가중치는 첫 50일에는 1/e 보다 큰 값이 되었다가, 그 후엔, 꽤 빠른 속도로 감소할 것입니다. 그래서 직관적으로, 이것은 고정불변의 법칙 같이 봐도 되는데, 이 값이 50일간의 대략적인 평균치라고 보면 됩니다. 그 이유는, 이 예제에서 여기 왼쪽의 표기법을 쓰자면, 마치 앱실론의 값이 0.02인 것과 같습니다. 즉, 앱실론 분의 1이 50인 것이죠. 이렇게해서 이 공식이 나오는 것입니다. 1 빼기 베타 분의 1로 평균을 구하는 것이 되죠. 여기서 앱실론은 1-베타를 대체합니다. 이것이 말해주는 것인 어떠한 상수까지, 대략적으로 이것이 평균이 되어야하는 평균기온 일 수를 나타냅니다. 하지만 이것은 그냥 대충 들여다 본 것이고, 정식 수학적 표현은 아닙니다. 자 그럼 마지막으로 이것을 어떻게 구현하는지 알아보겠습니다. 기억하시겠지만, V0은 0으로 초기화하고 시작하는데요, 그 다음에 첫째날의 V1을 계산합니다. 그 다음에 V2 등등 말이죠. 자 이제 알고리즘을 설명드리자면, 여기 V0, V1, V2 등을 각각의 변수로 적은게 유용했죠. 실제로 이것을 구현하는 경우엔, 이렇게 합니다.
V를 초기화한 값을 0이라 하고, 첫번째 날은, V의 값을 베타 곱하기 V 더하기 1 빼기 베타 곱하기 쎄타 1로 설정할 것입니다. 그 다음날에는, V를 업데이트하여 베타 곱하기 V 더하기 1빼기 베타, 곱하기 쎄타2를 할 것입니다. 이렇게 계속 이어지겠죠. 어떤 것은 이런 V 아래첨자 쎄타로 표기할텐데요, 쎄타인 파라미터에 대해 지수 가중 평균을 구하고 있다는 의미를 나타냅니다. 이것을 다시 for loop 형식으로 말씀드리자면, V 쎄타를 0으로 설정하고, 그 다음에, 반복적으로, 매일, 다음 쎄타 T를 가지고,
그 다음 V 쎄타는 베타 곱하기 이전 V 쎄타 더하기 1 빼기 베타 곱하기 현재 값의 V 쎄타 값으로 갱신 합니다. 그래서 지수 가중 평균을 구하는 것의 장점 중 하나는, 아주 적은 양의 메모리가 이용된다는 것입니다. 단 하나의 값만 메모리에 보관하면 됩니다. 그리고, 가장 마지막 구한 값에 이 공식을 써서
계속 덮어 쓰면 됩니다. 이런 장점, 효율성인데, 기본적으로 한줄 코드면 되고, 지수 가중 평균을 계산해서 값 하나만 메모리에 저장하면 됩니다. 물론 가장 좋은 방법은 아닙니다. 가장 정확한 평균 계산 방식은 아닌데요, 구간(윈도우)을 이동하여 계산하려면, 명백하게 합을 구해야 하는데,
최근 10일간의 값이나 지난 50일 간의 기온을 더하고
10이나 50으로 나누면 대개 더 좋은 추정치를 얻게 됩니다. 하지만 이것의 단점은 이렇게 명백히 그 동안의 기온들을 보관하고 지난 10일간의 수치를 더하려면 더 많은 메모리가 필요하고, 구현도 더 복잡하고, 연산 부담도 커 집니다. 예제들을 다음 몇개의 비디오에서 보겠습니다만, 많은 양의 변수들의 평균값 계산이 필요합니다. 이 지수 가중 평균은 매우 효율적인데,
연산과 메모리 효율면에서 좋고,
머신러닝에서 많이 쓰는 이유입니다. 단지 한줄 코드면 된다는 점 역시,
또 하나의 장점이라고 말할 필요도 없죠. 자 이제 여러분은 지수 가중 평균을 어떻게 구현하는지 배웠습니다. 이제 한가지 여러분이 아시면 좋을만한 것이 있는데요. 바로 bias correction입니다. 이것은 다음 비디오를 통해서 볼텐데요, 그 다음에는 이것들을 사용해서 간단한 기울기 강하 방법보다 더 나은 최적화 알고리즘을 만들 수 있을 것입니다.