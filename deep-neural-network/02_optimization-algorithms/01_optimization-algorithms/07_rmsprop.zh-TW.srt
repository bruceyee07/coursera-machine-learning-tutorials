1
00:00:00,470 --> 00:00:03,955
你見過了利用動量來加快梯度下降

2
00:00:03,955 --> 00:00:06,230
有另外一個演算法叫 RMSprop

3
00:00:06,230 --> 00:00:10,490
代表 Root-Mean-Square prop (方均根傳播)，
這也能加快梯度的下降

4
00:00:10,490 --> 00:00:11,800
我們來看看其運作

5
00:00:11,800 --> 00:00:16,313
還記得之前的例子，如果你用梯度下降法

6
00:00:16,313 --> 00:00:20,252
你可能會在垂直方向有強烈的震盪

7
00:00:20,252 --> 00:00:24,569
而水平方向還仍在努力取得進展

8
00:00:24,569 --> 00:00:29,214
以這例子，為了提供比較直觀的看法，我們假設

9
00:00:29,214 --> 00:00:34,733
垂直軸代表參數 b，水平軸代表參數 W

10
00:00:34,733 --> 00:00:39,614
— 其實他可以是w1, w2 或是其他的參數；這邊叫

11
00:00:39,614 --> 00:00:42,090
b 和 w 只是為了直覺方便。

12
00:00:42,090 --> 00:00:46,690
所以，你想讓 b 這個方向學得慢一些

13
00:00:46,690 --> 00:00:48,400
或說是垂直方向。

14
00:00:48,400 --> 00:00:54,830
而在水平方向上加快學習，至少不要慢下來。

15
00:00:54,830 --> 00:00:59,411
為了達成這個，下面就是 RMSprop 演算法所做的

16
00:00:59,411 --> 00:01:07,237
在第 t 個迭代，照例地會在小批資料上

17
00:01:07,237 --> 00:01:11,387
計算導數 dW, db

18
00:01:15,464 --> 00:01:19,400
和之前的指數加權平均類似

19
00:01:19,400 --> 00:01:22,890
只不過不是用 V_dW，這裡我用新的符號 S_dW

20
00:01:22,890 --> 00:01:28,954
所以 S_dW 等於 beta 乘以前一個值

21
00:01:28,954 --> 00:01:34,181
加上 (1-beta) 乘以 dW 的平方

22
00:01:34,181 --> 00:01:41,130
— 有時會寫成 **2 代表次方，不過我這邊就這樣寫。

23
00:01:41,130 --> 00:01:48,530
釐清一下，這個「平方」是逐元素的平方運算

24
00:01:48,530 --> 00:01:52,170
所以這邊所作的，其實是記錄導數的平方

25
00:01:52,170 --> 00:01:56,230
的指數加權平均。

26
00:01:56,230 --> 00:02:04,368
類似地，我們有 s_db = beta * s_db + (1-beta) * db^2

27
00:02:04,368 --> 00:02:08,031
再次強調，這邊的「平方」是對每個元素的。

28
00:02:08,031 --> 00:02:13,330
接下來，RMSprop 用下面的方法更新參數：

29
00:02:13,330 --> 00:02:17,875
W 被更新為 W 減掉學習率

30
00:02:17,875 --> 00:02:22,580
雖然之前 alpha 乘的是 dW，現在的是

31
00:02:22,580 --> 00:02:27,596
dW 除以 S_dW 的平方根。

32
00:02:27,596 --> 00:02:33,322
而 b 被更新為 b 減學習率乘以

33
00:02:33,322 --> 00:02:38,080
不是只有梯度，這個也會被 s_db (的平方根) 除。

34
00:02:39,600 --> 00:02:42,970
那麼，我們來多了解一下運作概念

35
00:02:42,970 --> 00:02:45,750
還記得在水平的方向

36
00:02:45,750 --> 00:02:50,380
或在這個例子是 W 的方向，我們想要學習變很快

37
00:02:50,380 --> 00:02:54,819
而在垂直的方向，在這個例子是 b 的方向，

38
00:02:54,819 --> 00:02:59,137
我們想要慢慢來，或者是抑制垂直方向上的擺動

39
00:02:59,137 --> 00:03:01,737
所以這兩項 S_dW 和 s_db，

40
00:03:01,737 --> 00:03:06,729
我們希望 S_dW 相對較小

41
00:03:06,729 --> 00:03:11,836
以至於我們在這邊除的是比較小的數字

42
00:03:11,836 --> 00:03:16,851
而 s_db 會相對比較大，以至於在這裡除的是

43
00:03:16,851 --> 00:03:21,226
比較大的數字，以減緩垂直維度的更新。

44
00:03:21,226 --> 00:03:25,518
而的確，如果你看這些導數，他們

45
00:03:25,518 --> 00:03:30,340
在垂直方向比較大，在水平方向比較小

46
00:03:30,340 --> 00:03:33,720
在 b 方向上，這坡度是很陡峭的對吧

47
00:03:33,720 --> 00:03:40,790
所以像這樣的導數，db是非常大的，dW相對比較小。

48
00:03:40,790 --> 00:03:45,350
因為這一個函數的斜度在垂直方向上很陡

49
00:03:45,350 --> 00:03:50,870
也就是b方向，遠比W方向也就是水平方向，還斜的多

50
00:03:50,870 --> 00:03:53,008
所以，db平方會比較大

51
00:03:53,008 --> 00:03:58,010
也所以 s_db 會比較大；相較之下 dW 比較小

52
00:03:58,010 --> 00:04:02,080
dW 平方比較小，所以 S_dW 會比較小。

53
00:04:02,080 --> 00:04:06,600
因此最終的效果是，你在垂直方向上的更新

54
00:04:06,600 --> 00:04:11,230
會除以一個比較大的數字，所以有助於抑制震盪，

55
00:04:11,230 --> 00:04:15,440
而在水平方向上的更新會除以比較小的數字。

56
00:04:15,440 --> 00:04:19,470
所以使用 RMSprop 最終的效果會是，你的更新

57
00:04:19,470 --> 00:04:20,750
最後看起來像這樣

58
00:04:22,750 --> 00:04:27,587
你在垂直方向的更新被抑制，

59
00:04:27,587 --> 00:04:32,370
但是在水平方向你能繼續前進。

60
00:04:32,370 --> 00:04:36,890
而這個效果讓你因此也能用比較大的學習率 alpha

61
00:04:36,890 --> 00:04:41,540
能更快的學習，而不會在垂直方向發散。

62
00:04:41,540 --> 00:04:45,223
那麼只是釐清一下，我一直把垂直和水平方向

63
00:04:45,223 --> 00:04:48,348
稱為 b 和 W，這只是為了示範，

64
00:04:48,348 --> 00:04:53,188
實際上，你在一個很高維度的參數空間裡

65
00:04:53,188 --> 00:04:57,383
所以或許你想抑制的垂直的維度

66
00:04:57,383 --> 00:05:01,757
是某些參數 w1, w2, w17，

67
00:05:01,757 --> 00:05:07,223
而水平維度可能是 w3, w4, 等等等

68
00:05:07,223 --> 00:05:11,150
所以分成 w 和 b 只是為了示範

69
00:05:11,150 --> 00:05:15,330
實務上 dW 是非常高維的參數向量

70
00:05:15,330 --> 00:05:18,620
db 也是非常高維的參數向量

71
00:05:18,620 --> 00:05:22,830
不過概念是，在那些你會有震盪的維度

72
00:05:22,830 --> 00:05:26,570
你最後會算出比較大的總和

73
00:05:26,570 --> 00:05:29,406
或者是導數的平方的加權平均，

74
00:05:29,406 --> 00:05:33,080
所以那些有震盪的方向終會漸緩。

75
00:05:33,080 --> 00:05:39,680
那麼這就是 RMSprop，
他代表 root mean square (方均根) prop，因為

76
00:05:39,680 --> 00:05:44,110
這裡你把導數「平方」，而(平均)後取「平方根」

77
00:05:44,110 --> 00:05:48,560
在繼續其他的之前，這演算法還有一兩個細節

78
00:05:49,870 --> 00:05:55,420
在下部影片，實際上我們會把 RMSprop 和動量法合併在一起

79
00:05:55,420 --> 00:06:00,540
所以對於這個超參數，與其於用動量法寫過的 beta，

80
00:06:00,540 --> 00:06:05,188
我要叫這個超參數 beta_2，避免衝突

81
00:06:05,188 --> 00:06:09,350
不要讓動量法和 RMSprop 超參數名字同樣。

82
00:06:09,350 --> 00:06:13,540
還有為了確保你的演算法不會除以 0

83
00:06:13,540 --> 00:06:17,910
你知道的，萬一 S_dW 的平方根非常靠近 0

84
00:06:17,910 --> 00:06:19,730
那就爆掉了

85
00:06:19,730 --> 00:06:24,320
只是為了確保運算上數值的穩定性，
在實務上你寫程式的時候

86
00:06:24,320 --> 00:06:28,200
你在分母加一個非常非常小的 epsilon

87
00:06:28,200 --> 00:06:30,760
epsilon 實際上是多少不大重要，

88
00:06:30,760 --> 00:06:34,948
用 10 的 -8 次方會是個合理的預設值，
不過這只是確保

89
00:06:34,948 --> 00:06:39,202
比較好的數值穩定性，避免四捨五入還是其他原因，

90
00:06:39,202 --> 00:06:43,030
讓你最後不要除以一個非常非常小的數字。

91
00:06:43,030 --> 00:06:47,870
那麼，這就是 RMSprop；和動量法類似，這個有

92
00:06:47,870 --> 00:06:52,910
舒緩震盪的效果，在梯度下降或是小批次梯度下降，

93
00:06:52,910 --> 00:06:56,510
讓你有機會使用更大的學習率 alpha，

94
00:06:56,510 --> 00:07:01,920
也一定能讓你的演算法學得更快。

95
00:07:01,920 --> 00:07:05,350
那麼你知道怎麼實作 RMSprop 了，這會是另一個

96
00:07:05,350 --> 00:07:07,920
加快你的學習演算法的方式。

97
00:07:07,920 --> 00:07:09,554
一個關於 RMSprop 有趣的事

98
00:07:09,554 --> 00:07:13,572
他一開始其實不是在學術論文裡刊登

99
00:07:13,572 --> 00:07:17,947
而是多年前在 Geoffrey Hinton 的一門 Coursera 課程中提出

100
00:07:17,947 --> 00:07:22,108
我猜 Coursera 本意並不是想當一個

101
00:07:22,108 --> 00:07:26,214
最新學術研究的傳播平台，不過就這例子還挺不錯的

102
00:07:26,214 --> 00:07:30,126
而確實是由 Coursera 課程開始，RMSprop 開始廣為人知

103
00:07:30,126 --> 00:07:31,790
開始起飛。

104
00:07:31,790 --> 00:07:32,970
我們談了動量法 (momentum)

105
00:07:32,970 --> 00:07:34,330
我們談了 RMSprop

106
00:07:34,330 --> 00:07:37,970
其實如果你把兩個加在一起，你會得到更好的

107
00:07:37,970 --> 00:07:39,530
最佳化演算法

108
00:07:39,530 --> 00:07:41,040
讓我們在下部影片來談談