嗨,歡迎回來. 在這禮拜，你會學到一些最佳化的演算法 這能讓你訓練神經網路時做得更快 你聽我說過，運用機器學習是個很吃經驗的過程 需要反覆嘗試 你需要訓練出很多個模型來找出很厲害的那個 因此，能很快訓練出模型，這真的很重要。 而這當中困難之處在於 深度學習通常在資料很多的時候表現最好 當你能拿大量資料來訓練你的神經網路的時候。 然而，訓練大量資料就是慢。 所以你會發現，擁有迅速的最佳化演算法， 擁有很好的最佳化演算法，真的能夠 增加你和你的團隊的效率。 那麼，就讓我們從「小批次梯度下降法」來開始吧
(mini-batch gradient descent) 在前面你學到向量化 (vectorization) 能讓你 計算所有 m 筆資料時更有效率 能讓你處理過整個訓練集，不用明確地寫for迴圈 這就是為什麼我們會把訓練資料疊起來 變成一個超大矩陣 X x(1), x(2), x(3)，最後到 x(m)，如果你有m筆資料 Y也是類似情況，y(1), y(2), y(3), ... 到 y(m) 所以 X 的維度是 n_x 乘 m，而這個是 1 乘 m。
向量化能 讓你處理全部m筆資料時比較快一點 而如果 m 很大的時候，這還是很慢 例如，如果 m 是五百萬或五千萬、或甚至更大呢 拿整個訓練集做梯度下降演算法時 你要做的是 你必須要處理過所有的訓練資料 才能踏出梯度下降的一小步 然後你必須再處理過整個訓練集 例如五百萬筆資料 才能夠再踏出梯度下降的另一小步。 其實，你有比較快的方法
— 如果你讓梯度下降 在處理完你一整個資料前就先有一些進度 在處理完那巨大的五百萬筆資料之前。 特別來說，你可以這樣做 假設你把訓練集分割成比較小的， 一堆小小的資料集，我們稱這些小小的資料集叫「小批資料」
(mini-batches) 而且假設你這些小小的資料集，每一個只有一千筆資料 所以你把 x(1) 到 x(1000) 看做你的第一個小小資料集 也就是小批資料 (mini-batch) 然後，你拿接下來的一千筆資料 x(1001) 到 x(2000) 這是一千筆，然後再下一批，依此類推 我要用一個新的符號，我要 稱這個為 X 上標、大括號 1，然後稱這個為 X 上標大括號2。 那麼，如果你總共有五百萬筆訓練資料 而每一小批資料都有一千筆 那你就會有5000批的小批資料，因為5000乘以一千是五百萬 那總共你會有5000個這些小批資料 所以結尾是 X｛5000｝， 然後呢，你對 Y 做類似的事情 你也把相對應的訓練資料的 Y 分開 所以叫這 Y｛1｝，然後這是 y(1001) 到 y(2000) 這些變成 Y｛2｝，依此類推，直到最後你有 Y｛5000｝。 所以呢，第 t 個小批資料會包括 X｛t｝和 Y｛t｝ 也就是一千筆訓練資料，亦即其相對應的輸入輸出對 在進到下一步前，只是想確認一下符號很清楚 我們之前用上標小括號 i 來表示訓練資料的索引，
所以 x(i) 是第 i 個訓練資料。 我們用上標中括號 [l] 來代表神經網路內，不同層的索引 所以 z[l] 是 神經網路第 l 層的 z 值。
而今天我們介紹了 大括號 t 來代表不同小批資料的索引， 所以你有 X｛t｝、Y｛t｝。
來看看你是否真的理解： X｛t｝和 Y｛t｝的維度各是多少？ 嗯，X 是 n_x乘m，所以 如果 X｛1｝是一千筆資料，或說那千筆資料的值 那他的維度應該是 n_x乘1000，而
X｛2｝也會是n_x乘1000，依此類推 所以這些全部的維度都應該是 n_x 乘 1000 而這些維度都要是 1 乘 1000。 要解釋這演算法的名字的話 批次梯度下降法 (batch gradient descent)，也就是 我們之前不斷提到的梯度下降演算法， 也就是你同時處理整個訓練集。 這名稱是從這樣來的： 同時處理「整批」訓練資料。 我不確定這命名好不好，不過這就是這樣叫的。 相對地，小批次梯度下降 (mini-batch gradient descent) 也就是下一張投影片會講的演算法 你一次處理「一小批」(mini batch)
的資料 X｛t｝ Y｛t｝，而不是同時處理整個訓練集 X, Y。 那麼，就讓我們瞧瞧小批次梯度下降法怎麼做吧 要在你的訓練資料上執行小批次梯度下降，
你會跑過 t = 1 到 5000，因為我們有 5000 批小量的資料，每批1000筆 在這 for 迴圈你要做的，基本上是實作一步的 梯度下降，且是用 X｛t｝, Y｛t｝ 就彷彿你只有1000筆訓練資料一樣 也彷彿要你實作已經很熟的那個 演算法，不過只在這很小的訓練集上 大小是 m=1000。
其中並不用真的 for 迴圈跑過所有1000筆資料 而是用向量化 (vectorization) 的方法
同時處理全部這1000筆。 讓我們寫下來：首先 你對輸入資料做正向傳播 所以只有 X｛t｝；要這樣的方法是讓 Z[1] 等於 W[1] 在之前，我們只會用 X 對吧 不過現在，你不是處理整個訓練集 你只是處理第一小批的資料， 所以處理第 t 批小量資料時，這裡變成 X｛t｝ 然後你有 A[1] = g[1](Z[1]) — 這邊是大寫 Z，因為實際上 用的是向量化的作法 — 依此類推，直到 A[L] 是 g[L](Z[L])，而這就是你的預測值 注意到，你這裡應該要用向量化的實作方式 只是這邊的向量化一次只處理 1000筆資料而不是五百萬筆資料。 接下來，你計算成本函數 J，在這邊我會寫成 一千分之一：因為1000是小批訓練集的大小 然後 i 從 1 到 l 把損失函數 y hat (i), y(i) 加起來。
說明一下這邊的記號 是代表從 X｛t｝, Y｛t｝小批資料來的。 如果你要用正則化 你也可以有這一項正則化的項 — 分母這邊有個2 — 乘以對 l 相加 權重矩陣的 Frobenius norm (弗比尼斯範數)，然後平方。 因為這其實只是一小批資料的成本 我要把這成本 J 弄個上標、大括號 t。 你可以注意到，我們這邊所做的都和 之前實作梯度下降法的時候一模一樣
— 除了不是做在 X, Y， 而是做在 X｛t｝, Y｛t｝。 接下來，你實作反向傳播 計算 J｛t｝的梯度 你這邊只看 X｛t｝, Y｛t｝，然後你更新權重 每個 W[l] 更新成 W[l] 減掉 alpha dW[l]；而 b 也類似。 所以這邊的流程，
就是用小批次梯度下降把整個訓練集處理過一次 這邊寫的程式碼也稱為「做一個 "epoch" 的訓練」 "epoch" 的意思是把訓練集掃過一遍 在批次梯度下降法 把訓練集掃過一遍只能讓你走一步的梯度下降 而用小批次梯度下降法，把訓練集掃過一遍 也就是一個 "epoch"，能讓你走5000步的梯度下降。 當然，你會希望 掃過訓練集很多遍，通常都會這樣， 你大概會在外面需要另一個 for 迴圈或 while 迴圈 所以你不斷地掃過訓練資料 直到收斂或近似收斂為止。 當你有非常多的訓練資料 小批次梯度下降跑得比批次梯度下降還快得多 大概每個做深度學習的人 在訓練很大的資料集時都會使用之 在下部影片，讓我們更深入探討小批次梯度下降法 讓你更清楚知道它在做什麼，還有為什麼能表現這麼好