Derin öğrenme tarihi süresince çok iyi bilinen bazır araştırmacılarında içinde bulunduğu birçok araştırmacı bazen  optimizasyon algoritmalarını önerdi ve birkaç problemde iyi çalışığını gösterdi. Ancak bu optimizasyon algoritmaları daha sonra  eğitmek isteyebileceğiniz geniş yelpazede ki yapay sinir ağları için çok iyi bir genelleme yapamadığını gösterdi. Zamanla derin öğrenme topluluğunun yeni algoritmalara karşı aslında bir miktar kuşku geliştirdiğini düşünüyorum. Ve birçok insan (alçalan eğimin) momentum ile birlikte çok iyi çalıştığını düşünmektedir. Daha iyi çalışan şeyler teklif etmekte zor. Yani, RmsProp ve Adam  optimizasyon algoritması bu videoda konuşacaklarımız. gerçekten karşı koyabilmiş olan nadir algoritmalardan biri ve geniş yapay sinir ağ mimarilerinde iyi çalıştığı gösterildi. Size tereddüt etmeden denemenizi önerdiğim algortimalardan biridir çünkü birçok kişi bunu denedi ve farklı problemleri çok iyi şekilde çözdüğünü gördü ve Adam optimizasyon algoritması temel olarak momentumu ve Rmsprop'u  alır ve bunları bir araya getirir Hadi nasıl çalıştığını görelim. Adam'ı uygulamak için şunları başlatırsın : Vdw=0, Sdw=0 ve benzer olarak Vdb, Sdb=0. ve sonra T üzerinde yinelenmede , türevleri hesaplarsınız : mevcut mini-batch kullanarak dw ve db hesaplayın. Yani genellikle, bunu mini-batch gradyan alçalması ile yaparsınız. Ve sonra momentumun üstel ağırlıklı ortalamasını alırsınız. Yani Vdw = ß. Ama şimdi ß1'e  gidiyorum onu hiper parametresinden ayırt etmek için ß2'yi bunun rmsprop oranı için kullanacağız. Yani, bu tam olarak uygulamada sahip olduğumuz momentumun ß yerine hiper parametresi ß1 olarak adlandırıldığı şeydir. ve benzer şekilde , VBD aşağıdaki gibidir : ( 1 - ß1 )x db. Ve sonra Rmsprop güncellemesini de yapın. Şimdi, farklı bir hiper parametreye sahipsiniz : ß2 + (1 - ß2 )dw² Ve yeniden, kareleştirme içinde senin dw türev ürünlerinin karesini alan y unsuru bulunmakta. Ve Sdb = (1 - ß2 ) çarpı db'dir. Şimdi bu momentum hiper parametre ile güncellenen ß1 gibidir, ve Rmsprop ise hiper parametre ile güncellenen  ß2. gibidir. Adam'ın tipik uygulamasında önyargı düzeltmesini uygularsınız. Yani, düzeltilmiş V'ye sahip olacaksınız. Düzeltilme anlamı önryargı düzeltmesinden sonra demektir. Eğer T yinelemelerini tamamladıysanız , Dw= Vdw /(1 - ß1 üzeri t) dür. Ve benzer şekilde , Vdb (düzeltilmiş) = Vdb / (1 - (  ß1 üzeri t ) ) Ve sonra benzer şekilde önyargı düzeltmesini S üzerinde uygularsınız. Yani, Sdw = Sdw / ( 1 - ( ß2 üzeri t ) ) ve Sdb (düzeltilmiş) = Sdb / ( 1 - ( ß2 üzeri t ) ) Son olarak güncellemeyi gerçekleştirebilirsiniz. Yani W = W - alfa olarak güncellenir Eğer sadece momentumu uyguluyorsan vdw kullanacaksın vw veya belki düzeltildi. Ama şimdi, biz bu Rmsprop kısmını ekliyoruz Bu yüzden Sdw (düzeltilmiş artı) epsilon'un karekökleri ile de bölünecek. Ve benzer olarak, B benzer bir formül olarak güncellenir. Vdb (düzeltilmiş) , düzeltilmiş Sdb + Epsilonun karaköküne bölünür . Ve böylece, bu algoritma gradyan alçalma etkisini birleştirir. gradyan alçalma , momentum ve rmsprop ile birlikte Ve bu yaygın olarak kullanılan kanıtlanmış öğrenme algoritmasıdır çok çeşitli farklı yapay sinir ağ mimarileri için etkilidir. Yani ,bu algoritma bir dizi hiper parametreye sahiptir. Hiper parametre alpha ile öğrenme hala önemli ve genellikle ayarlanması gerekiyor. Yani bir dizi denemek ve nasıl çalıştığını görmek zorundasınız. ß1 için varsayılan yaygın seçim gerçek olarak 0.9 dur. Bu bir hareketli ortalama, Dw hareketli ortalaması bir hafif momentum terimidir. ß2 için hiper parametre, Adam raporunun yazarları, Adam algoritmasının mucitleri 0.999 önerir Yine bu dw2 hareketli ortalamasının yanı sıra db karelerinin hesaplanmasıdır. Ve Epsilon, seçimi çok önemli değil. Fakat Adam raponun yazarları 10 üzeri eksi 8 önerirler. Fakat bu parametrede sen gerçekten ihtiyacın yok bunu ayarlamana ve bu performansı hiç etkilemez. Fakat Adam uyguladığın zaman insanların genellikle yaptığı sadece varsayılanı kullanmak. Yani epsilon yanı sıra ß1 ve ß2 Kimsenin gerçekten Epsilon'u seçtiğini sanmıyorum. Ve sonra, en iyisi için bir dizi Alpha değer, belirleyin. Ayrıca  ß1 ve ß2 ayarlayabilirsin fakat bu sıklıkla bildiğim uygulayıcılar arasında değil. Peki, Adam terimi nereden geliyor ? Adam uyarlamalı moment tahminini temsil eder. ß1 türevlerin ortalamalarını hesaplıyor. Buna ilk an denilir. ß2 ise hesaplamaları için ²s lerin üssel ortalamalarını alır ve buna ikinci an denir. Ve bu  uyarlanabilir moment tahminine yol açar. Fakat herkes bunu Adam yetkilendirme algoritması olarak adlandırır. Ve bu arada, eski arkadaşlarımdan ve işbirlikçilerimden biri Adam Coates'i çağrıştırıyor. Bildiğim kadarıyla , bu algoritmanın onunla hiçbir bağlantısı yok, Bazen onu kullandığını düşündüğüm gerçeğinin dışında. Fakat bazen o soruyu sorarım, sadece merak ettiğiniz noktada. Yani, Adam optimizasyon algoritması bu kadar. Bununla birlikte, sinir ağlarınızı çok daha hızlı bir şekilde eğitebileceksiniz. fakat bu haftaya başlamadan önce hiper parametresi ayarlaması hakkında konuşmaya devam edelim hem de ne olduğu hakkında biraz sezgi elde edelim : sinir ağları için optimizasyon problemleri nasıl görünüyor. Bir sonraki videoda, öğrenme hızı azalması hakkında konuşacağız.