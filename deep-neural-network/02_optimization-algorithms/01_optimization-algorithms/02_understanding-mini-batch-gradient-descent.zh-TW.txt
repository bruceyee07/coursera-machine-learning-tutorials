在上一部影片，你看到了如何利用小批次梯度下降法 就讓你開始有進度，開始踏出梯度下降的步驟，即使 你才第一次處理你的訓練資料、才進行到一半。 在這部影片，你會學到更多實作梯度下降的細節 更能了解這在做什麼，以及為什麼這樣有用。 在批次梯度下降法，每次的迭代 (iteration)
你都會跑過整個訓練集 你能期待成本值在每一次的迭代都會往下降 所以如果我們畫出成本函數 J 每次迭代時的一個函數，那他在每個迭代都會下降。 如果他在某次迭代往上升，那麼一定有地方錯了 也許是你的學習率太大了。 但是在小批次梯度下降法，如果你把成本函數的歷程畫出來 他不見得每次迭代都會減少。 特別是，每次迭代的運算，你處理的是 某批 X{t}, Y{t} 所以如果你畫出成本函數 J{t} 他只是用 X{t}, Y{t} 來計算的 那麼每次的迭代，你如同在訓練不一樣的訓練集 或者說不同一批的小量資料 所以當你畫成本函數 J 你更有可能看到像這樣的東西 他應該有往下的趨勢，但是會有一點噪音、浮動。 所以當你用小批次梯度下降法訓練的時候，
如果你畫出 J{t} 經過多個 epoch 後，你可以期待看到這樣的曲線。 所以如果沒有每次迭代都下降，也沒有關係， 但他應該會有往下的趨勢， 而會有一點噪音的原因是，有可能 X{1} Y{1} 是一批比較容易的資料，所以你的成本值會低一些， 但接下來有可能運氣不好，X{2}, Y{2} 是不好解決的一批資料 甚至有可能有標錯的資料 所以成本值會比較高一些 所以這是為什麼你會看到這些振動 當你在跑小批次梯度下降、畫出成本的時候。 現在呢，有一個你必須要挑的參數，
是一批小量資料的筆數 我們說過 m 是整個訓練集的大小，
考慮某個極端情況， 如果小批資料的大小等於 m，這就變成批次梯度下降法 在這極端的情況下，你只會有一批資料 X{1}, Y{1} 而這小批資料等同於你的一整個訓練資料 所以把小批資料筆數設成 m，你會得到批次梯度下降。 另一種極端的情況，是小批資料的大小等於 1 這會給你一個演算法叫「隨機梯度下降法」
(stochastic gradient descent) 這時每一筆資料自己是一個小批資料 所以在這種情況，你首先看第一小批資料 X{1}, Y{1} 但是當你的小批資料大小為 1 的時候，
其實就是你的第一筆訓練資料 你拿第一筆訓練資料來執行梯度下降 然後接下來，你拿第二批小量資料，而這其實就是 你的第二筆訓練資料，然後拿他做梯度下降 然後你拿第三筆訓練資料，依此類推 也就是一次只看一筆訓練資料。 那麼，讓我們看看在這兩種極端的情況，
最佳化我們的成本函數會怎麼發生什麼事 假設這個是我們想優化的成本函數的等值線 所以最小值在這邊， 那麼，批次梯度下降法可能在某個地方開始 而且能夠踏出比較穩定 (low noise)，比較大的步伐 如同行軍，不斷地往最小值邁進。 相對的，隨機梯度下降法 (stochastic gradient descent) 假設他在另一個點開始 那麼在每一次的迭代 (iteration)，
你只拿一筆訓練資料做梯度下降 所以大部分的時候你會往最小值前進 但是有時你最走錯方向，萬一那筆資料 剛好把你指到爛的方向 所以隨機梯度下降法可能會非常有噪音、亂跑 平均來說，他會把你帶到一個好的方向 但偶爾他也會走錯方向。 由於隨機梯度下降法不會收斂 他總是會在最小值附近震盪、流浪 但不會走到最小值以後就定在那邊。 實際上，你選的小批資料大小會介在 1 到 m 之間；
如果選 1 或 m 會太小或太大 這是為什麼呢 如果你用批次梯度下降法 也就是小批資料筆數為 m 那麼你每一回合都在處理巨大的訓練集 所以主要的缺點是他會花太久的時間 在每次的迭代上 — 假設你有很大的訓練集。 如果你的訓練集很小，那批次梯度下降法不會有問題。 相反地，如果你用隨機梯度下降法， 那麼，每次處理一筆資料你都會有進展，這很好 這其實不會是問題， 而那噪音、不穩定，可以用 比較小的學習率 (learning rate) 來改善減緩之。 然而，隨機梯度下降法有一個很大的缺點 你幾乎無法享受到向量化 (vectorization) 帶來的加速 因為在這裡，你一次只處理單一一筆資料 這樣一筆一筆資料處理會很沒有效率。 所以實務上最有效的是介在兩者中間 你的小批資料筆數不會太多或太少 實務上這讓你訓練得最快 而且你可以注意到這樣會有兩個好處： 第一，你的確享受到很多的向量化 拿前部影片為例子，如果你的小批資料大小 為 1000 筆，那你可以對 1000 筆資料做向量化 這會比一次一筆還快非常多。 第二，你還是可以有所進展 不用等到整個訓練集處理完畢。 再拿前部影片的數字為例，每個 epoch、每一次 算過你的訓練集，能給你 5000 步的梯度下降 所以實務上，某個中庸的小批資料大小會最有效。 假設小批次梯度下降法從這裡開始 也許第一次迭代是這樣、兩次後是這樣、三、四... 他不會保證每次都往最小值走，不過 和隨機梯度下降法相比，他會更穩定往最小值的方向 他並不會每次都收斂或遊蕩在非常小的區域 如果這對你是個問題，你總是可以慢慢把學習率變小 我們之後會講到學習率衰減 (learning rate decay) 在之後的影片談如何減少學習率。 那麼，如果小批資料大小不應該是 m 也不應該是 1 而應該是中間某個值，那你該怎麼選呢？ 這邊有些準則 第一，如果你有的訓練資料很少，就用批次梯度下降法吧 如果你的訓練集很小，就沒有必要用小批次梯度下降 你處理一整個訓練集的時候就夠快了 所以你可以用批次梯度下降法 「小」的訓練集是多小呢？我大概會說可能小於2000筆吧 只要用批次梯度下降就好了 否則，如果你的訓練集更大，典型的小批資料筆數會是 64 到 512 中間的任何值，還滿常見的 而由於電腦記憶體的配置和存取方式的關係 如果你的小批資料大小是 2 的次方，程式有時會跑得快一點 所以64是2的6次方，然後2的7次方、2的8次方、 2的9次方，所以通常我會把小批資料大小做成 2 的次方 我知道在前一部影片，我用 1000 做為大小 如果你真要這樣的話，我會建議你用 1024 也就是 2 的 10 次方 你的確可以看到用 1024 當小批次的大小，有點罕見就是 在這一個範圍的小批次大小，會比較常見。 最後一個建議是，確保你的小批次資料 你的 X{t}, Y{t} 能塞得進去 CPU/GPU 的記憶體 而這取決於你的應用、 和你一筆訓練資料多大有關， 如果你碰到某一批小資料是CPU/GPU記憶體 所塞不下的，無論拿來計算的資料是什麼， 那麼你會發現效能突然隕落 會忽然變很差。 那麼，我希望這能給一點概念，
大家常用的小批資料筆數的範圍 是怎麼樣的。 實際上，小批資料的大小顯然是一個超參數
(hyperparameter) 你大概會想要尋找一下，了解哪一個大小 能讓你最有效率地降低成本值 J。 所以我會做的是，試試幾個不同的值 嘗試幾個 2 的不同次方，然後看看能否挑到一個值 盡可能讓你的下降優化演算法有效率。 總之，希望我能給你一些準則， 怎麼開始找這個超參數。 你現在知道了怎麼實作小批次梯度下降法
讓你的演算法 跑得更快 — 尤其是在訓練很大的訓練集的時候。 不過呢，其實還有某些演算法， 比梯度下降法或小批次梯度下降法更有效率。 讓我們在接下來的幾部影片看看吧