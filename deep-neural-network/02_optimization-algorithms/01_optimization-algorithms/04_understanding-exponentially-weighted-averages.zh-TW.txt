在上一部影片，我們談到了指數加權平均 它其實很關鍵 在一些訓練神經網路的最佳化演算法當中。 所以在這部影片，我想深入探討 這方法的道理及背後所做之事。 回想一下，這是實作指數加權平均的主要公式 如果 beta = 0.9，你會得到這條紅線 如果很靠近 1 如果 beta 是 0.98，你就有這條綠色的 而比較小的話 例如 0.5，你得到這條黃線 讓我們做更多觀察，來了解 這是怎麼算出每日氣溫的平均。 又看到這公式啦 這次我們假設 beta 為 0.9，然後列出一些這公式的式子 雖然實作的時候，你的 t 是從零到一、二、三 ... t 是往上增加。但是分析的時候 我讓 t 遞減、寫下來，依此類推。 那麼我們拿第一個式子 了解 V_100 到底是什麼 所以 V_100 會是 讓我把這兩項交換 會是 0.1 乘 theta_100 加上 0.9 乘上前一天算出的值 可是，V_99 又是什麼呢？ 我們就把這個式子代進去 所以這個會是 0.1 乘 theta_99 — 我一樣把這兩項交換 — 加上 0.9 乘以 V_98。 那 V_98 又是什麼？ 你看，他在這邊 所以你可以代入這邊 0.1 乘 theta_98 加上 0.9 乘 V_97，依此類推。 如果你把全部的項乘開 你可以得到 V_100 是 0.1 乘以 theta_100、加上 我們來看看 theta_99 的係數 他會是 0.1 乘 0.9 乘以 theta_99 然後來看看 theta_98 的係數 這邊有個 0.1、然後乘 0.9 再乘 0.9 所以把運算展開 這個會是 0.1 乘以 0.9的平方，乘上 theta_98。 如果你持續展開下去 你發現這變成 0.1 乘 0.9的三次方 theta_97，加上 0.1 乘 0.9 的四次方 乘以 theta_96 ...。 那麼，這其實是一個經過加權的總和，
也就是加權平均，對於 theta_100 — theta_100 是當天的氣溫，如果你從 想算出第一百天，也就是 V_100 的角度來看 — 所以是 theta_100 theta_99, theta_98, theta_97, theta_96... 等等的加權總和。 如果要用畫的來表示，可以這樣 假設我們有某些日子的溫度 這邊是 theta、這邊是 t；theta_100 是某個值 然後 theta_99 是某個值 theta_98 所以這是 t=100 99, 98, 等等等 所以你有一些日子的氣溫 那麼我們有的，是一個呈指數衰減的函數 從 0.1 開始 到 0.9 乘以 0.1 到 0.9 平方乘以 0.1，依此類推 所以你有這個指數衰減的函數 那你要算 V_100 的方法 就是把這兩個函數之間相對應的元素乘起來，再加總 所以拿這個值 theta_100 乘以 0.1 然後這個值 theta_99，乘以0.1乘0.9 也就是第二項，以此類推 其實就是拿這個每日溫度 乘以這個指數衰減函數，然後加總起來 這就會是你的 V_100 實際上 我們後面會講到細節 這邊所有的係數 加起來會是 1，或說是非常靠近 1 在下部影片我們會談到這個偏差矯正的細節 不過總之因為這樣，這的確是這指數加權的平均。 最後呢，你可能有疑問 這究竟是對多少天的氣溫做平均 那麼其實，0.9的10次方 大概是 0.35，也就是 e 分之 1， 1 除以自然對數的底 更一般地說，如果你有 1 - epsilon — 在這例子 epsilon 會是 0.1 所以這是0.9 — 那麼 (1-epsilon) 的 epsilon分之一次方 這大約會是 e分之一 大約是 0.34, 0.35 換句話說 大概要過10天，這個的高度 會衰減至約 1/3，其實是 1/e 因為這樣 我們會說當 beta=0.9 時 這如同於你在計算 專注在最後 10 天氣溫的指數加權平均 因為在 10 天之後，這邊的加權衰減至 比當天的加權的三分之一還少。 相對地，如果 beta 是 0.98 那麼 0.98 的多少次方會變得那樣子小呢？ 原來 0.98 的 50 次方大約會等於 e分之一。所以其權重 在前五十天會很大，比 1/e 還大， 然後會快速地衰減 所以概念上是這樣的 你可以把這看成是以50天的氣溫做平均 因為在這個例子 看左邊這一項 如同 epsilon=0.02 所以 1/epsilon 是 50 題外話，這就是我們之前說的公式的由來 說我們是對 1/(1-beta) 天做平均 在這裡 epsilon 相當於 1-beta。 這告訴了你，在某個定值以上， 大概有多少日子的氣溫，是你認為足以當作平均的。 不過呢這只是大略地描述其思路 並不是正式的數學說明。 最後，來談實際上你要怎麼實作 還記得我們從 V_0 為 0 開始 然後計算第一天的 V_1 V_2, 等等等 那麼，為了解釋演算法， 把 V_0, V_1, V_2 寫成不同的變數，解釋比較方便。 不過實際寫程式的時候 你會這麼做：你把變數 V 初始成 0， 然後在第一天 你會定義 V 為 beta * V + (1-beta) * theta_1 然後隔天，你更新 V 的值 變成 beta * V + (1-beta) theta_2, 依此類推 有時候標示上會用 V 下標 theta 來表示 V 在計算參數 theta 的指數加權平均 那麼，我用 for 迴圈的方式把這個再寫一次 你設 V_theta 為 0 然後每天重複做 拿下一個 theta_t，然後 更新 V_theta 為 beta 乘以舊的 V_theta 加 (1-beta) 乘以這一次的 theta_t。 所以這種指數加權平均算法的優點 在於他佔非常少的記憶體 你只要儲存這單一個數字在記憶體內 然後你一直用這式子算出最新的值，然後覆蓋之 因為「效率」這一個理由 你只需要一行程式碼 而且記憶體 只需要記一個數字，就可以算出指數加權平均 這其實不是最佳 最準的算法來計算平均 如果你以移動範圍 (moving window) 來計算 也就是你明確地算出過去十天的總和 或過去五十天氣溫的總和，然後除以10或50 這通常會是個較好的估計 但其缺點是 必須要一直記得最近的氣溫 和近十天的總和，這花更多的記憶體 實作起來更複雜，計算也比較費力 我們會在後面的影片看到幾個例子，所以那些 要算很多變數的平均的情況 利用這個會很有效率 — 從計算 和記憶體來看都是 — 所以在機器學習中很常用到 更不用說這只需要一行程式，也算是另一個優點。 所以現在你已經知道要怎麼寫出指數加權平均了 還有一個技術上的細節， 值得一聽，叫偏差矯正 (bias correction) 我們下部影片會看到。然後接下來 你會用這整個技巧來建構 比正常的梯度下降法還要更厲害的最佳化演算法