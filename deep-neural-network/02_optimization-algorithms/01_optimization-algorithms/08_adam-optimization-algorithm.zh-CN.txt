在深度学习的发展过程中 很多研究者 其中也包括一些非常著名的研究者 有时候会提出优化算法 在少数问题上 这些算法很有效 但随后有人指出 这些算法并不能被广泛应用 在各种不同类型的神经网络上 因此 深度学习领域逐渐出现了一些 质疑新型优化算法的声音 有些人尝试把梯度下降与动量算法结合起来 结果非常有效 而至今也并没有出现效果比它好很多的优化算法 所以 这个视频将会提到的 是关于RMSProp和Adam优化算法 属于其中极少数真正有效的算法 适用于很多不同的深度学习网络结构 所以 我一定要推荐你试试这个算法 因为很多人尝试后 发现它在很多问题上效果很好 Adam优化算法本质上是将 动量算法和RMSprop结合起来 现在 我们一起来学习下Adam算法 首先你需要进行初始化 Vdw=0, Sdw=0, 以及Vdb=0, Sdb=0 接着 在第t次迭代过程中 你需要用到一些微积分的知识<br />用当前的mini-batch来计算dW和db 通常我们会使用mini-batch梯度下降算法 然后计算动量指数加权移动平均值 所以Vdw=ß 但是这里我标记为ß1 以此来区别于下面我们会提到的 RMSprop算法的超参数β2 这个其实就是我们在使用动量算法时 的公式 但是我们现在用β1来代替β 可以类推至 Vdb=β1*Vdb+(1-β1)db 然后用同样方式更新RMSprop中的参数 我们用一个与之前不同的超参数β2 加上(1-β2)*dw^2 强调一下 这里的平方指的是对dW矩阵中每个元素进行平方 Sdb=β2*Sdb+(1-β2)db^2 所以这个公式和动量很相似 只是用超参数β1来更新 这里就像是用超参数β2来更新了RMSprop 一般在构建Adam算法的过程中 需要进行偏差修正 我们标记为Vdw上标Corrected Corrected代表偏差修正 它等于VdW/(1-β1^t) t代表迭代次数 类似的 Vdb偏差修正等于Vdb/(1-β1^t) 然后同样在S上进行偏差修正 我们写成SdW/(1-β2^t) Sdb偏差修正等于Sdb/(1-β2^t) 最后 执行更新 W的更新是使用W减去 α乘以 如果你只使用了动量算法 那么只要VdW就好了 或者偏差修正的VdW就好了 但是现在我们把RMSprop的算法引入 所以还要除以 (偏差修正SdW的平方根)+ε 然后以此类推更新参数b 偏差修正Vdb除以 (偏差修正Sdb的平方根)+ε 这就是Adam算法 结合了动量和RMSprop梯度下降 的各种优势 被广泛使用 且已经被证明在很多不同种类的 神经网络构架中都是十分有效的 这个算法中有几个超参数 但是超参数α依然很重要且通常需要调整 你可以尝试不同的值来比较效果 通常我们会将β1的默认值设置为0.9 这是涉及到动量算法的dW的移动平均 或者说加权平均 至于超参数β2 Adam算法论文的作者 Adam模型的创建者推荐使用0.999 这是关于dW平方与db平方的移动加权平均计算 另外对于ε 其实如何选择影响都不大 但是Adam论文的作者推荐使用10的-8次方作为默认值 对于这个参数 并不需要 真的去深入研究 因为这并不会影响算法的性能 在使用Adam算法的时候 业内通常对β1 β2以及ε 都直接使用默认值 我认为不会有人去管ε 然后尝试一下不同的学习率α来看看哪个最效果最好 你也可以调整β1和β2 但是这种做法在我认识的从业者中用得不多 至于'Adam'这个名字的由来 Adam代表自适应矩估计<br />(Adaptive Moment Estimation) β1代表这个导数的平均值 被称为第一阶的矩 β2被用于计算平方数的 指数加权平均 也被称为第二阶矩 以上就是Adam这个命名的由来 但是几乎所有人都用Adam算法这个简称 顺便一提 我有一个多年的朋友兼合作者叫做Adam Coates 但是据我所知 他只是碰巧和算法重名 当然有可能偶尔也会用一下这个算法 有时我会经常会被问及他和这个算法关系 所以如果你也有这个疑问的话 希望不要被名字所迷惑 以上就是Adam优化算法的知识 通过这节课的学习 我想你能把神经网络的训练变得更高效 在我们结束这周的课程前 我们还会说一下怎样调整超参数 以及让大家更直观地理解 到底如何进行神经网络优化 下期视频我们会讨论学习率衰减的知识<br />翻译 | 审阅：Cousera Global Translator Community