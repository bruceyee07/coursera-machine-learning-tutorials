1
00:00:00,000 --> 00:00:01,700
안녕하세요, 환영합니다.

2
00:00:01,700 --> 00:00:04,625
여러분의 신경망 네트워크를 훨씬 더 빨리 트레이닝 시킬 수 있도록

3
00:00:04,625 --> 00:00:08,280
도와주는 최적화 알고리즘에 대해 배우도록 하겠습니다.

4
00:00:08,280 --> 00:00:12,630
머신 러닝을 적용한다는 것은 매우 경험에 의거한 절차와 

5
00:00:12,630 --> 00:00:14,320
매우 반복적인 업무를 통반한다고 제가 말한 것을 이전에 들으셨을텐데요, 

6
00:00:14,320 --> 00:00:18,295
여러 모델들을 그냥 무조건 트레이닝시키고 가장 작 작동하는 것을 찾아야 합니다.

7
00:00:18,295 --> 00:00:21,210
그렇기 때문에 재빨리 모델들을 트레이닝 시키는 것이 매우 중요합니다.

8
00:00:21,210 --> 00:00:23,280
한가지 더 어렵게 만들 수 있는 부분은, 

9
00:00:23,280 --> 00:00:26,640
딥러닝이 큰 데이터의 경우 가장 잘 작동하는 경향이 있다는 것입니다.

10
00:00:26,640 --> 00:00:29,310
저희는 신경망을 큰 데이터세트에서 

11
00:00:29,310 --> 00:00:33,025
트레이닝을 시킬 수 있는데요, 이런 경우 속도가 매우 느리겠죠. 

12
00:00:33,025 --> 00:00:36,820
이런 경우 빠른 최적화 알고리즘과, 

13
00:00:36,820 --> 00:00:39,030
그리고 양질의 최적화 알고리즘을 갖는 것이

14
00:00:39,030 --> 00:00:41,865
여러분의 팀 효율성을 높힐 수 있다는 것입니다. 속도를 높혀주기 때문이죠.

15
00:00:41,865 --> 00:00:45,939
그러면 이제 미니 배치 기울기 강하에 대한 이야기로 본강을 시작해보록 하겠습니다.

16
00:00:45,939 --> 00:00:48,750
이전에 배우셨지만, 벡터화는 모든 m 예시에 대해서

17
00:00:48,750 --> 00:00:51,720
효율적으로 산출을 할 수 있게 해줍니다.

18
00:00:51,720 --> 00:00:56,949
특정한 공식 없이도 모든 전체 트레이닝세트를 처리할 수 있게 말이죠.

19
00:00:56,949 --> 00:01:00,540
그렇기 때문에 저희는 트레이닝 예시를 가지고 

20
00:01:00,540 --> 00:01:04,480
방대한 매트릭스 캡슐 X들에 쌓아올리는 것입니다. 

21
00:01:04,480 --> 00:01:12,945
X1, X2, X3, 그리고 xm 트레이닝 샘플까지 올라갑니다. 

22
00:01:12,945 --> 00:01:15,055
여기 Y값에 대해서도 비슷하게 말이죠. 이 값은 Y1, 

23
00:01:15,055 --> 00:01:22,635
Y2, Y3, 그리고 YM 까지 말이죠. 

24
00:01:22,635 --> 00:01:30,355
X의 다이멘션은 nx x m이였죠. 그리고 이것은 1 x m 이였습니다. 

25
00:01:30,355 --> 00:01:33,810
벡터화는 M 예시들을 꽤 빨리

26
00:01:33,810 --> 00:01:37,885
처리할 수 있게 해줍니다. M이 아주 큰 값일 경우에 말이죠. 아직 느릴 수는 있죠.

27
00:01:37,885 --> 00:01:44,085
예를 들어, M이 5백만이거나 5천만이거나 더 큰 값인 경우, 

28
00:01:44,085 --> 00:01:48,010
전체 트레이닝세트에 기울기 강하를 도입하면, 

29
00:01:48,010 --> 00:01:49,530
여러분이 해야 하는 것은, 

30
00:01:49,530 --> 00:01:51,675
전체 트레이닝 세트를 

31
00:01:51,675 --> 00:01:54,610
기울기 강하에 대한 조금한 첫 단계를 시작하기 전에 
모든 것을 처리해야 합니다.

32
00:01:54,610 --> 00:01:56,960
그리고나서 전체 5백만개의 트레이닝 세트를 

33
00:01:56,960 --> 00:01:58,680
다시 처리해야 합니다. 

34
00:01:58,680 --> 00:02:00,665
기울기 강하에 대한 또 하나의 추가 절차를 밟기 전에 말이죠.

35
00:02:00,665 --> 00:02:04,950
여러분이 만약에 전체 아주 큰 5백만개 예시가 있는 트레이닝 세트를 

36
00:02:04,950 --> 00:02:10,260
트레이닝 시키기도 전에 기울기 강하가 조금 진전이 있게 하면,

37
00:02:10,260 --> 00:02:14,255
훨씬 더 빠른 알고리즘을 구축할 수도 있습니다.

38
00:02:14,255 --> 00:02:16,620
특히, 여러분이 할 수 있는 것은

39
00:02:16,620 --> 00:02:19,750
만약에 여러분이 트레이닝 세트를 조금 더 조금하게 나눈다고 해보겠습니다.

40
00:02:19,750 --> 00:02:27,390
이런 아주 작은 트레이닝 세트로 나눠서 아기 트레이닝 세트를 미니 배치들이라고 부르겠습니다.

41
00:02:27,390 --> 00:02:35,553
그러고 난 뒤, 이런 라기 트레이닝 세트가 각각 천 개의 예시를 가지고 있다고 해보겠습니다. 

42
00:02:35,553 --> 00:02:42,320
그러면 x1에서 x1000까지 있고, 이것을 첫번째 아기 트레이닝 세트라고 하겠습니다.

43
00:02:42,320 --> 00:02:43,910
또는 미니 배치라고도 하죠.

44
00:02:43,910 --> 00:02:47,630
그 다음에 또 추가로 1000개의 예시를 갖습니다.

45
00:02:47,630 --> 00:02:56,650
x1001에서 x2000까지 1000개의 예시를 갖고, 그렇게 다음도 이어서 올 수 있겠죠. 

46
00:02:56,650 --> 00:02:59,375
이제 저는 새로운 표기법을 보여드릴 텐데요,

47
00:02:59,375 --> 00:03:03,965
여기는 x 위첨자 

48
00:03:03,965 --> 00:03:06,507
중괄호 1의 값이라고 부르고, 

49
00:03:06,507 --> 00:03:11,940
그리고 여기는 x 위첨자 중괄호 2라고 부를 것입니다.

50
00:03:11,940 --> 00:03:15,160
자 그럼 여기서 총 5백만개의 트레이닝 샘플이 있고, 

51
00:03:15,160 --> 00:03:18,370
여기 각각의 미니 배치들이 1000개의 예시가 있으면, 

52
00:03:18,370 --> 00:03:24,460
여기 이것은 5000개가 있다는 것입니다. 왜냐면, 5000 곱하기 1000이 5백만개이기 때문입니다.

53
00:03:24,460 --> 00:03:31,670
그럼 총 여기 민 배치들은 5000개가 되는 것입니다.

54
00:03:31,670 --> 00:03:33,400
그러면 최종적으로 x 위첨자 중괄호 

55
00:03:33,400 --> 00:03:37,180
5000이 되는 것입니다. Y에 대해서도 비슷하게 진행합니다.

56
00:03:37,180 --> 00:03:41,811
Y에 대한 트레이닝 데이터도 적합하게 나눠줍니다.

57
00:03:41,811 --> 00:03:50,805
이것을 Y1이라고 부르고, 그러면 이것은 Y1001 에서 Y2000까지가 되겠죠. 

58
00:03:50,805 --> 00:04:00,965
이것은 Y2라고 하고, 등등 Y5000이 될 때까지 이어집니다.

59
00:04:00,965 --> 00:04:08,500
그러면 T까지의 미니 배치는 X, 

60
00:04:08,500 --> 00:04:12,770
T, Y, T로 이루어져 있을 것입니다.

61
00:04:12,770 --> 00:04:18,220
이것은 입력 값과 결과값 쌍으로 이루어진 1000개의 트레이닝 샘플입니다. 

62
00:04:18,220 --> 00:04:22,295
다음으로 넘어가기 전에, 여기 표기법이 명백하도록 하겠습니다.

63
00:04:22,295 --> 00:04:27,465
이전에는 위첨자 소괄호를 이용하여 트레이닝 세트를 인덱싱 했는데요, 

64
00:04:27,465 --> 00:04:29,180
x i는 i번째 트레이닝예시인데요, 

65
00:04:29,180 --> 00:04:31,630
그리고 위첨자 대괄호 L을 이용해서

66
00:04:31,630 --> 00:04:34,980
신경망의 층을 나타냅니다.

67
00:04:34,980 --> 00:04:39,078
그러므로 ZL은 Z 값에서 

68
00:04:39,078 --> 00:04:42,800
L 개의 신경망 네트워크 층을 갖고 있음을 뜻하는데요, 

69
00:04:42,800 --> 00:04:48,020
그리고 여기서는 중괄호 t를 이용해서 mini bacthes들을 인덱싱 합니다.

70
00:04:48,020 --> 00:04:53,960
그러면 XT와 YT 가 있는데요, 여러분이 이것들에 대한 이해도를 체크하기 위해서,

71
00:04:53,960 --> 00:05:01,460
XT 와 YT의 다이멘션이 어떤지 확인할 수 있는데요,

72
00:05:01,460 --> 00:05:04,880
x는 nX x M입니다. 

73
00:05:04,880 --> 00:05:10,040
그러므로 만약 X1이 1000개의 트레이닝 샘플이거나, 1000개의 예시에 대한 값인 경우,

74
00:05:10,040 --> 00:05:19,260
여기 다이멘션 은 nx, 1000이여야하고, X2는 또한 nx, 1000이여야 합니다. 

75
00:05:19,260 --> 00:05:22,940
그렇기 때문에 여기 모든 다이멘션은 nx, 1000이여야 합니다.

76
00:05:22,940 --> 00:05:29,200
그리고 여기는 1,1000의 다이멘션을 가져야 합니다.

77
00:05:29,870 --> 00:05:34,563
여기 알고리즘의 이름을 설명드리자면, 

78
00:05:34,563 --> 00:05:37,130
배치 기울기 강하는 

79
00:05:37,130 --> 00:05:40,250
이전에 다뤘었던 기울기 강하 알고리즘을 뜻하는데요,

80
00:05:40,250 --> 00:05:43,340
전체 트레이닝 세트를 한번에 처리하는 경우를 뜻합니다.

81
00:05:43,340 --> 00:05:46,348
이름은 어디서 유래되었냐면, 

82
00:05:46,348 --> 00:05:49,545
전체 트레이닝 샘플 배치를 한번에 처리한다는데에서 유래되었습니다.

83
00:05:49,545 --> 00:05:53,100
훌륭한 이름이라고 보긴 힘들지만 단순히 그냥 이렇게 불립니다.

84
00:05:53,100 --> 00:05:55,526
반대로 미니 배치 기울기 강하는 

85
00:05:55,526 --> 00:05:58,994
다음 슬라이드에서 이야기할 알고리즘을 뜻하는데요, 

86
00:05:58,994 --> 00:06:02,910
싱글 미니 배치인 XT, YT를 한번에 처리하는 경우를 뜻합니다.

87
00:06:02,910 --> 00:06:09,270
YX 트레이닝세트를 한번에 처리하는 것이 아니고 말이죠.

88
00:06:09,270 --> 00:06:12,020
그러면 미니 배치 기울기 강하가 어떻게 작동하는지 한번 보겠습니다.

89
00:06:12,020 --> 00:06:17,765
미니 배치 기울기 강하를 트레이닝 세트에서 실행하기 위해서는, 

90
00:06:17,765 --> 00:06:24,730
t=1 에서 5000까지 실행합니다. 각각 1000개씩 가지고 있는 미니 배치가 5000개 있었죠.

91
00:06:24,730 --> 00:06:29,600
for loop 내부에서는 무엇을 할 것이냐면, xt, yt를 이용해서 

92
00:06:29,600 --> 00:06:38,157
기울기 강하의 한 단계를 도입할 것입니다.

93
00:06:38,157 --> 00:06:48,340
이것은 마치 트레이닝 세트의 크기가 1000개의 예시가 있는 것과 비슷합니다.

94
00:06:48,340 --> 00:06:51,130
그리고 여러분이 이미 익숙한 알고리즘을 도입하려고 하는 

95
00:06:51,130 --> 00:06:54,370
것과 비슷합니다. 여기 작은 트레이닝 세트인 M값이

96
00:06:54,370 --> 00:07:00,910
1000인 경우에서 말이죠. 여러분이 별개의 for loop을 1000개의 예시에 

97
00:07:00,910 --> 00:07:06,595
갖게 하는 것이 아닌, 벡터화를 통해 1000개의 예시를 한번에 처리하는 것과 같습니다.

98
00:07:06,595 --> 00:07:08,910
한번 먼저 적어볼까요?

99
00:07:08,910 --> 00:07:15,710
첫째로, 입력 값에 대한 전 방향전파 을 도입합니다.

100
00:07:15,710 --> 00:07:24,315
Xt의 값에 말이죠. 이것은 Z1=W1이 되게하여 도입합니다.

101
00:07:24,315 --> 00:07:27,655
이전에는 여기가 그냥 X였죠?

102
00:07:27,655 --> 00:07:30,040
하지만 이제 여러분은 전체 트레이닝 세트를 처리하는 것이 아니라, 

103
00:07:30,040 --> 00:07:32,140
첫번째 미니 배치만 처리하는 것입니다.

104
00:07:32,140 --> 00:07:36,065
그렇기 때문에 여기가 xT가 되죠. 미니 배치 T를

105
00:07:36,065 --> 00:07:45,420
처리하는 경우에 말이죠. 그러면 G1의 Z1의 값을 갖게 되겠죠. 

106
00:07:45,420 --> 00:07:48,394
여기는 대문자 Z죠, 

107
00:07:48,394 --> 00:07:57,585
벡터화 도입이기 때문입니다. 이어서 AL값이 될 때까지 진행하는데요, 

108
00:07:57,585 --> 00:08:03,935
여기는 GL의 ZL입니다. 그리고 이 값이 예측 값이 되는 것입니다. 

109
00:08:03,935 --> 00:08:09,005
그리고 여기서 아시겠지만, 벡터화 도입을 이용하셔야 합니다.

110
00:08:09,005 --> 00:08:14,125
여기서 벡터화 도입이 5백만개의 예시 대신에

111
00:08:14,125 --> 00:08:18,840
한번에 1000개의 예시를 처리하기 때문입니다.

112
00:08:18,840 --> 00:08:25,500
다음으로는, J 비용함수를 산출합니다. 여기서는 이 함수를

113
00:08:25,500 --> 00:08:32,895
1 나누기 1000으로 적을 것인데요, 그 이유는 1000이 여러분의 작은 트레이닝 세트의 
크기이기 때문입니다.

114
00:08:32,895 --> 00:08:38,580
1에서 L까지의 합으로 y hat i, yi의 loss의 함수입니다.

115
00:08:38,580 --> 00:08:45,490
명료성을 위해서 여기 

116
00:08:45,490 --> 00:08:53,300
표기법은 미니 배치 XT, YT의 예시를 뜻합니다.

117
00:08:53,300 --> 00:08:55,344
그리고 여러분이 일반화를 사용하는 경우엔, 

118
00:08:55,344 --> 00:08:59,295
여기 이런 일반화 항을 쓸 수 있습니다.

119
00:08:59,295 --> 00:09:03,345
2는 분모가 되겠구요, 곱하기 l의 합

120
00:09:03,345 --> 00:09:07,980
L의 매트릭스 제곱값의 Frobenius 방식입니다.

121
00:09:07,980 --> 00:09:12,625
이 값은 하나의 미니 배치에 대한 비용함수이기 때문에, 

122
00:09:12,625 --> 00:09:18,983
이것을 J비용함수 위첨자 중괄호 T로 인덱싱할 것인데요, 

123
00:09:18,983 --> 00:09:23,925
이전에 기울기 강하에서 도입했던 방식과 XY에서 하는 것을

124
00:09:23,925 --> 00:09:29,040
XT, YT에서 하는 것을 제외하고는 모든 것이

125
00:09:29,040 --> 00:09:31,680
동일합니다.

126
00:09:31,680 --> 00:09:36,470
다음으로는, 후 방향전파을 도입하고, 

127
00:09:36,470 --> 00:09:44,285
JT에 대해서 gradient를 산출하기 위해서 말이죠.

128
00:09:44,285 --> 00:09:54,120
여기서는 아직 XT 와 YT만 사용하는데요, 그 다음으로

129
00:09:54,120 --> 00:09:59,410
비중인 W를 업데이트 합니다. 모든 WL은

130
00:09:59,410 --> 00:10:08,124
WL 빼기 알파 곱하기 D 곱하기 WL로 업데이트 되는데요, B도 비슷하게 됩니다.

131
00:10:08,124 --> 00:10:17,620
그래서 이것은 미니 배치 기울기 강하를 이용해서 한번에 크게 트레이닝 세트를 통과하는 개념으로 생각하면 됩니다.

132
00:10:17,620 --> 00:10:25,420
여기 제가 쓴 코드는 1 epoch 트레이닝을 한다고 표현하기도 하는데요, 

133
00:10:25,420 --> 00:10:34,022
epoch이라는 용어는 트레이닝 세트로 1번 통과한다는 뜻입니다. 

134
00:10:34,022 --> 00:10:38,440
반면에 배치 기울기 강하에서는

135
00:10:38,440 --> 00:10:44,420
한번 트레이닝을 통과하는 것이 오로지 한번의 기울기 강하 절차를 밟게 해줍니다.

136
00:10:44,420 --> 00:10:48,475
미니 배치 기울기 강하를 통해, 트레이닝 세트로 한번의 통과를 통해, 

137
00:10:48,475 --> 00:10:52,890
즉, 1epoch 이죠, 5000 단계의 기울기 강하를 진행하게 해줍니다.

138
00:10:52,890 --> 00:10:55,040
당연히, 복수로

139
00:10:55,040 --> 00:10:58,430
트레이닝 세트를 통과하는 것을 주로 원하겠죠.

140
00:10:58,430 --> 00:11:02,730
또 하나의 for loop를 갖거나 while loop를 갖어야 할 것입니다.

141
00:11:02,730 --> 00:11:05,180
그리하여 바라건대 지속적으로 트레이닝 세트를 

142
00:11:05,180 --> 00:11:08,909
통과하여 수렴하는 지점을 갖도록 합니다.

143
00:11:08,909 --> 00:11:10,620
큰 사이즈의 트레이닝 세트가 있는 경우,

144
00:11:10,620 --> 00:11:15,330
미니 배치 기울기 강하가 배치 기울기 강하보다 훨씬 더 빨리 운영 돕니다.

145
00:11:15,330 --> 00:11:17,540
그리고 이것이 큰 데이터 세트에서 트레이닝하는 경우,

146
00:11:17,540 --> 00:11:20,205
딥러닝에서 거의 모든 사람이 쓰는 것입니다.

147
00:11:20,205 --> 00:11:24,230
다음 비디오에서는, 미니 배치 기울기 강하에 대해 조금 더 자세히 알아보도록 하겠습니다.

148
00:11:24,230 --> 00:11:28,650
여러분의 이해도를 높히고 이것의 역할이 어떤지, 또 왜 잘 작동하는지.