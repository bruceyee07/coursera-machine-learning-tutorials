1
00:00:00,470 --> 00:00:03,955
你已经学习了如何用动量来加速梯度下降

2
00:00:03,955 --> 00:00:06,230
还有一个叫做RMSprop的算法

3
00:00:06,230 --> 00:00:10,490
全称为均方根传递(Root Mean Square prop)<br />它也可以加速梯度下降

4
00:00:10,490 --> 00:00:11,800
我们来看看它是如何工作的

5
00:00:11,800 --> 00:00:16,313
回忆一下之前的例子 在实现梯度下降时

6
00:00:16,313 --> 00:00:20,252
可能会在垂直方向上出现巨大的振荡

7
00:00:20,252 --> 00:00:24,569
即使它试图在水平方向上前进

8
00:00:24,569 --> 00:00:29,214
为了说明这个例子 我们假设

9
00:00:29,214 --> 00:00:34,733
纵轴代表参数b 横轴代表参数W

10
00:00:34,733 --> 00:00:39,614
当然这里也可以是W1和W2等其他参数

11
00:00:39,614 --> 00:00:42,090
我们使用b和W是为了便于理解

12
00:00:42,090 --> 00:00:46,690
你希望减慢b方向的学习

13
00:00:46,690 --> 00:00:48,400
也就是垂直方向

14
00:00:48,400 --> 00:00:54,830
同时加速或至少不减慢水平方向的学习

15
00:00:54,830 --> 00:00:59,411
这就是RMSprop算法要做的

16
00:00:59,411 --> 00:01:07,237
在第t次迭代中 它会像往常一样计算

17
00:01:07,237 --> 00:01:11,387
当前小批量的倒数dW和db

18
00:01:15,464 --> 00:01:19,400
我会保留这个指数加权平均数

19
00:01:19,400 --> 00:01:22,890
这次使用新符号S_dW 取代之前的V_dW

20
00:01:22,890 --> 00:01:28,954
S_dW就等于beta乘以之前的值

21
00:01:28,954 --> 00:01:34,181
再加上(1-beta)*dW^2

22
00:01:34,181 --> 00:01:41,130
有时我们使用dW**2的形式来表示幂 这里写成dW^2

23
00:01:41,130 --> 00:01:48,530
需要澄清的是 这个平方操作是逐元素的平方操作

24
00:01:48,530 --> 00:01:52,170
所以这一步实际上是保存了导数平方的

25
00:01:52,170 --> 00:01:56,230
指数加权平均数

26
00:01:56,230 --> 00:02:04,368
类似地 S_db=beta*S_db+(1-beta)*db^2

27
00:02:04,368 --> 00:02:08,031
同样 这里的平方是逐元素的运算操作

28
00:02:08,031 --> 00:02:13,330
之后 RMSprop用以下方法更新参数

29
00:02:13,330 --> 00:02:17,875
W更新为W减去学习率

30
00:02:17,875 --> 00:02:22,580
之前我们使用的是alpha乘以dW

31
00:02:22,580 --> 00:02:27,596
但现在使用是dW除以S_dW的平方根

32
00:02:27,596 --> 00:02:33,322
而b更新为b减去学习率乘以

33
00:02:33,322 --> 00:02:38,080
梯度除以S_db的平方根 而不是仅使用梯度

34
00:02:39,600 --> 00:02:42,970
现在我们来理解一下它的工作原理

35
00:02:42,970 --> 00:02:45,750
记得在水平方向上

36
00:02:45,750 --> 00:02:50,380
即例子中W的方向上 我们希望学习速率较快

37
00:02:50,380 --> 00:02:54,819
而在垂直方向上 即例子中b的方向上

38
00:02:54,819 --> 00:02:59,137
我们希望降低垂直方向上的振荡

39
00:02:59,137 --> 00:03:01,737
对于S_dW和S_db这两项

40
00:03:01,737 --> 00:03:06,729
我们希望S_dW相对较小

41
00:03:06,729 --> 00:03:11,836
因此这里除以的是一个较小的数

42
00:03:11,836 --> 00:03:16,851
而S_db相对较大 因此这里除以的是一个较大的数

43
00:03:16,851 --> 00:03:21,226
这样就可以减缓垂直方向上的更新

44
00:03:21,226 --> 00:03:25,518
实际上 如果你看一下导数

45
00:03:25,518 --> 00:03:30,340
就会发现垂直方向上的倒数要比水平方向上的更大

46
00:03:30,340 --> 00:03:33,720
所以在b方向上的斜率很大

47
00:03:33,720 --> 00:03:40,790
对于这样的导数 db很大 而dW相对较小

48
00:03:40,790 --> 00:03:45,350
因为函数在垂直方向 即b方向的斜率

49
00:03:45,350 --> 00:03:50,870
要比w方向 也就是比水平方向更陡

50
00:03:50,870 --> 00:03:53,008
所以 db的平方会相对较大

51
00:03:53,008 --> 00:03:58,010
因此S_db会相对较大 相比之下dW会比较小

52
00:03:58,010 --> 00:04:02,080
或者说dW的平方会较小 所以S_dW会较小

53
00:04:02,080 --> 00:04:06,600
结果是 垂直方向上的更新量

54
00:04:06,600 --> 00:04:11,230
会除以一个较大的数 这有助于减弱振荡

55
00:04:11,230 --> 00:04:15,440
而水平方向上的更新量会除以一个较小的数

56
00:04:15,440 --> 00:04:19,470
使用RMSprop的效果就是使你的更新

57
00:04:19,470 --> 00:04:20,750
会更像这样

58
00:04:22,750 --> 00:04:27,587
在垂直方向上的振荡更小

59
00:04:27,587 --> 00:04:32,370
而在水平方向可以一直走下去

60
00:04:32,370 --> 00:04:36,890
另一个收效是 你可以使用更大的学习率alpha

61
00:04:36,890 --> 00:04:41,540
学习得更快 而不用担心在垂直方向上发散

62
00:04:41,540 --> 00:04:45,223
为了清楚起见 我一直称垂直方向

63
00:04:45,223 --> 00:04:48,348
和水平方向分别为b和W 只是为了方便演示

64
00:04:48,348 --> 00:04:53,188
实践中 你通常会面对非常高维的参数空间

65
00:04:53,188 --> 00:04:57,383
你试图降低振荡的垂直方向

66
00:04:57,383 --> 00:05:01,757
可能是参数W_1 W_2 W_17的集合

67
00:05:01,757 --> 00:05:07,223
而水平维度可能是W_3 W_4等等

68
00:05:07,223 --> 00:05:11,150
所以 这里把W和b分开只是为了便于演示

69
00:05:11,150 --> 00:05:15,330
实践中 dW是一个非常高维度的参数向量

70
00:05:15,330 --> 00:05:18,620
db也是一个非常高维度的参数向量

71
00:05:18,620 --> 00:05:22,830
直观理解是 在出现振荡的维度里

72
00:05:22,830 --> 00:05:26,570
你会计算得到一个更大的和值

73
00:05:26,570 --> 00:05:29,406
即导数平方的加权平均

74
00:05:29,406 --> 00:05:33,080
最后抑制了这些出现振荡的方向

75
00:05:33,080 --> 00:05:39,680
所以这就是RMSprop 全称是均方根传递

76
00:05:39,680 --> 00:05:44,110
因为你对导数求了平方 而且最后取了平方根

77
00:05:44,110 --> 00:05:48,560
最后 在进入下一个话题前<br />我再讲下这个算法的一些细节

78
00:05:49,870 --> 00:05:55,420
下一个视频中 我们将结合RMSprop和动量

79
00:05:55,420 --> 00:06:00,540
由于我们之前在动量中已经使用了超参数beta<br />这里就不再使用beta

80
00:06:00,540 --> 00:06:05,188
我将这个超参数命名为beta_2

81
00:06:05,188 --> 00:06:09,350
以避免在动量和RMSprop中使用相同名称的超参数

82
00:06:09,350 --> 00:06:13,540
同时 为了确保你的算法不会除以零

83
00:06:13,540 --> 00:06:17,910
如果S_dW的平方根非常接近0

84
00:06:17,910 --> 00:06:19,730
这一项就会非常大

85
00:06:19,730 --> 00:06:24,320
为了确保数值稳定性 当你实现这个算法时

86
00:06:24,320 --> 00:06:28,200
你要给分母加上一个非常非常小的epsilon

87
00:06:28,200 --> 00:06:30,760
epsilon的值取多少并不重要

88
00:06:30,760 --> 00:06:34,948
10的-8次方是一个合理的默认值 但这只能

89
00:06:34,948 --> 00:06:39,202
轻微提高数值稳定性<br />无论由于数值上的舍入还是什么别的原因

90
00:06:39,202 --> 00:06:43,030
你都不会除以一个过小的数

91
00:06:43,030 --> 00:06:47,870
所以这就是RMSprop 它和动量一样能够

92
00:06:47,870 --> 00:06:52,910
降低梯度下降和小批量梯度下降中的振荡

93
00:06:52,910 --> 00:06:56,510
并且令你能使用更大的学习率alpha

94
00:06:56,510 --> 00:07:01,920
从而提高你的算法的学习速度

95
00:07:01,920 --> 00:07:05,350
现在你学会了如何实现RMSprop

96
00:07:05,350 --> 00:07:07,920
这是另一种能够加速学习算法的方法

97
00:07:07,920 --> 00:07:09,554
说一个关于RMSprop的趣事

98
00:07:09,554 --> 00:07:13,572
它一开始并不是在学术研究论文中被提出的

99
00:07:13,572 --> 00:07:17,947
而是Geoffrey Hinton在很多年前教的<br />一个Coursera课程中提出的

100
00:07:17,947 --> 00:07:22,108
我猜Cousera最初并没有意图成为一个宣传

101
00:07:22,108 --> 00:07:26,214
新兴学术研究的平台 但它在这方面做得很棒

102
00:07:26,214 --> 00:07:30,126
通过Coursera课程 RMSprop才变得广为人知

103
00:07:30,126 --> 00:07:31,790
并迅速发展起来

104
00:07:31,790 --> 00:07:32,970
我们讲了动量

105
00:07:32,970 --> 00:07:34,330
我们也讲了RMSprop

106
00:07:34,330 --> 00:07:37,970
事实上 如果把二者结合起来 你会得到一个更好的

107
00:07:37,970 --> 00:07:39,530
优化算法

108
00:07:39,530 --> 00:07:41,040
我们将在下一个视频中讨论这一点