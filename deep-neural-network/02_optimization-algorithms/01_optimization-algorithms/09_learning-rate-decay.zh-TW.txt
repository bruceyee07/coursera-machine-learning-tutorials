要加快學習演算法，
其中的一個可能方式 是隨著時間慢慢降低你的學習率 我們稱之為學習率衰減 (learning rate decay) 我們來看看怎麼做 先從一個例子看起，示範為何你會想做 學習率衰減 假設你在做小批次梯度下降法 用合理、少量的的小批資料 可能一批資料只有 64 或 128 筆 在你進行時，你的步伐會有點雜亂、有噪音 大勢上他會往最小值前進，但並不會剛好收斂 你的演算法最終可能會在周圍徘徊 並不會真的收斂 — 因為你用了固定大小的 alpha 而不同批的小資料本來就會有些許噪音 然而，如果你把學習率 alpha 慢慢降低的話 在一開始的時候，你的學習率 alpha 還很大 所以你還可以滿快地學習 而當 alpha 逐漸變小，你的步伐會越來越慢、越短 所以最後你會在極小值附近，更小的區域裡震盪 不會發生就算一直訓練還跑出去的狀況 所以慢慢減少 alpha，這背後的概念是 可能一開始在訓練的時候，你經得起踏出很大的步伐 但隨著學習逐漸要收斂 使用比較慢的學習率讓你有比較精細的步伐 那麼，你可以這樣實作學習率衰減 還記得一個 epoch 表示 掃過一遍資料對吧 所以如果你有這樣的資料集 可能分成各個小批資料 那麼第一次處理過一遍這個資料集，這樣叫第一個 epoch 第二遍處理叫做第二個 epoch，依此類推 所以你可以做的是，把你的學習率 alpha 設為 1 除以 1 加某個參數，我就稱之為衰減率 (decay rate) 然後乘以 epoch 數 然後這個會乘上某個初始的學習率 alpha_0 注意到這邊的衰減率，他會是一個超參數 所以你可能需要調整 這邊是個實際的例子 假設你正在進行多個 epoch、掃過你的資料若干遍 如果 alpha_0 為 0.2，衰減率為 1 那麼在第一個 epoch 時 alpha 會等於 1 / (1 + 1) 乘上 alpha_0 所以你的學習率會是 0.1 這是從這個式子算出來的 — 當衰減率等於 1 而且 epoch 號碼為 1 的時候。 在第二次 epoch，你的學習率衰減至 0.67 (筆誤, 0.067) 第三次 0.5  (0.05), 第四次 0.4 (筆誤, 0.04) 等等 你可以自己再算算看後面的值 讓你有個感覺，你的學習率是epoch數的函數，它會 逐漸減少，對吧，根據上面這個式子 因此，如果你想使用學習率衰減，你可以做的事是 嘗試各種不同超參數的值：超參數 alpha_0、 以及這個衰減率 (decay rate) 超參數 試著找到成效良好的值。 學習率衰減除了這一個公式以外 還有其他幾種大家會用的 舉個例子，這個稱為指數衰減 alpha 等於把某個小於 1 的數字， 例如 0.95，的 epoch數次方，再乘上 alpha_0。 所以這個會呈指數速度減少你的學習率 還有其他大家會用的公式像是 alpha 等於 某個常數除以epoch數的平方根，乘上 alpha_0。 或是某個常數 k —這也是另一個超參數— 除以小批資料次數 t 的平方根，乘上 alpha_0。 有時候，你也會看到有人使用離散階梯型下降的 學習率， 因此在某些數量的步驟，你有某個學習率， 然後過了一些時候，你把它減半， 然後再過一會，減半 再過一會，減半 所以這是個離散狀的樓梯 那麼到目前為止，我們談到利用數學公式 來控制 alpha 學習率要怎麼隨著時間改變 而有時候一些人會採取「手動衰減」 假設你一次只訓練一個模型，而且 如果你的模型要花幾小時甚至幾天來訓練 那麼有些人會 在費時數日的訓練期間，監控他的模型 然後人工觀察，喔看起來這學習趨緩了 我要把 alpha 減低一點 當然這樣手動控制 alpha 也有效 做手工藝來調整 alpha，每一小時或每一天 這只在你訓練很少量的模型的時候才有用，不過 有時候也有人會這麼做。 那麼你有更多的選項來控制學習率 alpha 現在你可能會想，哇塞，這有好多超參數啊 在眾多選擇中，我該怎麼挑啊？ 我會說，現在無需擔心 在下個禮拜，我們會談到更多如何有系統地挑選超參數 對我而言，我會說學習率衰減通常會在我的 試驗清單的後面 設定 alpha，光是一個固定不變的 alpha 值，好好地調校 就會有超大影響 學習率衰減的確有幫助 有時他真的可以幫助加快訓練，但是 他在我想嘗試的事情中，順位有點後面 不過下個禮拜，當我們談到超參數的調校時 你會看到更有系統的方式來組織這些超參數 還有如何有效率地在其中搜尋 那麼，這就是學習率衰減 最後，我還想談一談局部極值 (local optima)，以及 鞍點 (saddle points)，在神經網路內 讓你有更好的概念了解 你的優化演算法所解決的問題類型 當你在訓練這些神經網路的時候 我們下一段影片見