1
00:00:00,000 --> 00:00:01,700
Merhaba, tekrar hoş geldiniz.

2
00:00:01,700 --> 00:00:04,625
Bu hafta, eniyileme algoritmalarını öğreneceksiniz,

3
00:00:04,625 --> 00:00:08,280
bu algoritmalar sinir ağlarınızı çok daha hızlı eğitmenizi sağlayacak

4
00:00:08,280 --> 00:00:12,630
Daha önce söylemiş olduğum üzere makine öğrenmesi ziyadesiyle deneysel bir işlemdir,

5
00:00:12,630 --> 00:00:14,320
ziyadesiyle yinelemeli bir işlemdir.

6
00:00:14,320 --> 00:00:18,295
Bu süreçte en iyi sonucu verecek modeli bulmak için çok fazla model eğitmeniz gerekir.

7
00:00:18,295 --> 00:00:21,210
Bu sebeple, modelleri hızlı bir şekilde eğitmek gerçekten çok yardımcı olur.

8
00:00:21,210 --> 00:00:23,280
Eğitimi zorlaştıran sebeplerden biri de şudur;

9
00:00:23,280 --> 00:00:26,640
Derin Öğrenme, en iyi sonucu büyük veri rejimlerinde vermeye meyillidir.

10
00:00:26,640 --> 00:00:29,310
Sinir ağlarını, devasa bir veri kümesi üzerinde eğitebiliyoruz,

11
00:00:29,310 --> 00:00:33,025
ve geniş bir veri kümesi üzerinde çalışmak oldukça yavaştır.

12
00:00:33,025 --> 00:00:36,820
Yani anlayacağınız şu ki, hızlı eniyileme algoritmalarına sahip olmak,

13
00:00:36,820 --> 00:00:39,030
doğru eniyileme algoritmalarına sahip olmak gerçekten,

14
00:00:39,030 --> 00:00:41,865
sizin ve ekibinizin verimine hız kazandırabilir.

15
00:00:41,865 --> 00:00:45,939
Hadi o zaman "mini-batch gradient descent" hakkında konuşmaya başlayalım.

16
00:00:45,939 --> 00:00:48,750
Daha önce öğrendiğiniz üzere, vektörleştirmenin sağladığı şey

17
00:00:48,750 --> 00:00:51,720
m sayıdaki bütün örnekler üzerinde etkin bir hesap yapmanızdır.

18
00:00:51,720 --> 00:00:56,949
bu durum bütün eğitim kümesi üzerinde belirli bir formül olmadan işlem yapmanızı sağlar.

19
00:00:56,949 --> 00:01:00,540
Bu sebeple bütün eğitim örneklerimizi alacağız ve birleştirip,

20
00:01:00,540 --> 00:01:04,480
büyük harfle X adındaki büyük matriksimizin içine ekleyeceğiz.

21
00:01:04,480 --> 00:01:12,945
x(1) , x(2) , x(3) ... ve nihayetinde x(m)'e kadar gider, m sayıdaki eğitim örneği.

22
00:01:12,945 --> 00:01:15,055
ve benzer şekilde Y için y(1),

23
00:01:15,055 --> 00:01:22,635
y(2), y(3) ve aynı şekilde y(m)'e kadar.

24
00:01:22,635 --> 00:01:30,355
Dolayısıyla, X'in boyutları, nx'e m ve bu da 1'e m olur... Vektörleşme

25
00:01:30,355 --> 00:01:33,810
bütün m örneklerini, hızlıca, göreceli olarak hızlıca işlemenizi sağlar,

26
00:01:33,810 --> 00:01:37,885
ama eğer m çok büyükse, bu durumda yine de yavaş olabilir.

27
00:01:37,885 --> 00:01:44,085
Örnek olarak, eğer m 5 milyon veya 50 milyon hatta daha da büyük olsa ne olur?

28
00:01:44,085 --> 00:01:48,010
Gradyan inişinin tüm eğitim setinize uygulanması ile

29
00:01:48,010 --> 00:01:49,530
yapmanız gereken

30
00:01:49,530 --> 00:01:51,675
gradyan inişinin bir küçük adımını atmadan önce

31
00:01:51,675 --> 00:01:54,610
tüm eğitim setinizi işlemektir.

32
00:01:54,610 --> 00:01:56,960
Ve sonra, gradyan inişinde bir küçük adım daha atmadan önce

33
00:01:56,960 --> 00:01:58,680
beş milyon eğitim örneğinden oluşan

34
00:01:58,680 --> 00:02:00,665
tüm eğitim setinizi tekrar işlemeniz gerekmektedir.

35
00:02:00,665 --> 00:02:04,950
Yani ortaya çıkıyor ki, eğer 5 milyon örneklik dev eğitim setinizin, 

36
00:02:04,950 --> 00:02:10,260
tüm setinizin işlemeyi bitirmeden önce gradyan inişinin bir miktar

37
00:02:10,260 --> 00:02:14,255
ilerlemesine müsaade ederseniz daha hızlı bir algoritma elde edebilirsiniz.

38
00:02:14,255 --> 00:02:16,620
Özellikle yapabileceğiniz şudur.

39
00:02:16,620 --> 00:02:19,750
Diyelim ki, eğitim setiniz daha küçük, küçük bebek eğitim setlerine

40
00:02:19,750 --> 00:02:27,390
böldünüz ve bu bebek eğitim setlerine de mini-toptanlar deniliyor.

41
00:02:27,390 --> 00:02:35,553
Ve diyelim ki her bir bebek eğitim setinizde sadece 1.000 örnek var.

42
00:02:35,553 --> 00:02:42,320
Yani, X1den X1.000 e kadar alıyorsunuz ve bunu ilk bebek eğitim seti olarak adlandırıyorsunuz

43
00:02:42,320 --> 00:02:43,910
ve aynı zamanda mini-toptan olarak adlandırıyorsunuz.

44
00:02:43,910 --> 00:02:47,630
Ve sonra bir sonraki 1.000 örneği eve getiriyorsunuz.

45
00:02:47,630 --> 00:02:56,650
X1.001den X2.000e ve sonra X1.000 örnek ve bir sonraki ve böyle.

46
00:02:56,650 --> 00:02:59,375
Yeni bir notasyon tanıtacam ve bunu

47
00:02:59,375 --> 00:03:03,965
X üzeri süslü parantezde 1 olarak adlandıracağım

48
00:03:03,965 --> 00:03:06,507
ve bunu X üzeri

49
00:03:06,507 --> 00:03:11,940
süslü parantezde 2.

50
00:03:11,940 --> 00:03:15,160
Şimdi, eğer toplamda 5 milyon eğitim örneği varsa

51
00:03:15,160 --> 00:03:18,370
ve bu mini toptanların her birinde bin örnek varsa,

52
00:03:18,370 --> 00:03:24,460
bu bunlardan 5.000 tane olduğu anlamına gelir çünkü biliyorsunuz 5.000 tane 1.000 5 milyona eşittir.

53
00:03:24,460 --> 00:03:31,670
Totalde bu mini toptanlardan 5.000 tane olur.

54
00:03:31,670 --> 00:03:33,400
Yani, X üzeri süslü parantezde 5.000 ile sonlanır 

55
00:03:33,400 --> 00:03:37,180
ve sonra benzer şekilde aynısını Y için de yapıyorsunuz.

56
00:03:37,180 --> 00:03:41,811
Eğitim verinizi bu doğrultuda Y için de böleceksiniz.

57
00:03:41,811 --> 00:03:50,805
Yani, bunu Y1 olarak adlandırın ve sonra bu Y1.001 den Y2.000e.

58
00:03:50,805 --> 00:04:00,965
Bu Y2 olarak adlandırılır ve böyle Y5.000iniz oluncaya kadar devam.

59
00:04:00,965 --> 00:04:08,500
Şimdi, T sayılı mini toptanınız X, T ve Y,Tden

60
00:04:08,500 --> 00:04:12,770
oluşur. Ve

61
00:04:12,770 --> 00:04:18,220
bu karşılık gelen girdi ve çıktı çiftleri ile bin tane eğitim örneğidir.

62
00:04:18,220 --> 00:04:22,295
Devam etmeden önce, sırf notasyonumun anlaşılır olduğundan emin olmak için,

63
00:04:22,295 --> 00:04:27,465
daha eğitim setinde dizin için üste süslü parantezde I kullanmıştık yani X I

64
00:04:27,465 --> 00:04:29,180
I eğitim örneğidir.

65
00:04:29,180 --> 00:04:31,630
Sinir ağının farklı katmanlarının

66
00:04:31,630 --> 00:04:34,980
dizini için üste köşeli parantezde L kullanıyoruz.

67
00:04:34,980 --> 00:04:39,078
Yani, ZL sinir ağındaki L katmanının

68
00:04:39,078 --> 00:04:42,800
Z değerinden geliyor ve burada farklı mini 

69
00:04:42,800 --> 00:04:48,020
toptanları dizinlemek için süslü parantezi tanıtıyoruz.

70
00:04:48,020 --> 00:04:53,960
Yani, XT ve YT var ve bunları anladığınızı kontrol etmek için,

71
00:04:53,960 --> 00:05:01,460
XT ve YTnin boyutu nedir?

72
00:05:01,460 --> 00:05:04,880
Pekala, X Mye Mdir. Yani,

73
00:05:04,880 --> 00:05:10,040
eğer X1 bin eğitim örneği ise veya X in eğitim örneği değerindeyse,

74
00:05:10,040 --> 00:05:19,260
o zaman bu boyut MXe 1.000 olmalı ve X2 Xe 1.000 olmalı ve bunun gibi devam eder.

75
00:05:19,260 --> 00:05:22,940
Yani, tüm bunların MXe 1.000lik boyutu olmalı ve

76
00:05:22,940 --> 00:05:29,200
bunların boyutu 1e 1.000 olmalı.

77
00:05:29,870 --> 00:05:34,563
Bu algoritmanın adını açıklamak için,

78
00:05:34,563 --> 00:05:37,130
toptan gradyan iniş, önceden konuştuğumuz

79
00:05:37,130 --> 00:05:40,250
tüm eğitim setini aynı anda işlediğiniz gradyan

80
00:05:40,250 --> 00:05:43,340
iniş algoritmasına söylenmektedir.

81
00:05:43,340 --> 00:05:46,348
Ve ismi tüm eğitim örneği toptanınızın aynı anda

82
00:05:46,348 --> 00:05:49,545
işlenmesinden gelmektedir.

83
00:05:49,545 --> 00:05:53,100
Bunun harika bir isim olmadığını biliyorum ancak bu şekilde isimlendiriliyor.

84
00:05:53,100 --> 00:05:55,526
Buna karşın mini toptan gradyan inişi,

85
00:05:55,526 --> 00:05:58,994
bir sonraki slaytta bahsedeceğimiz ve tüm XY eğitim setini aynı anda işlemek yerine

86
00:05:58,994 --> 00:06:02,910
XT ve YT mini toptanlarını işleyeceğiniz

87
00:06:02,910 --> 00:06:09,270
algoritmayı ifade etmektedir.

88
00:06:09,270 --> 00:06:12,020
Şimdi, haydi mini-toptan gradyan inişin nasıl çalıştığına bakalım.

89
00:06:12,020 --> 00:06:17,765
Mini-toptan gradyan inişi eğitim setlerinizde çalıştırmak için, T eşittir

90
00:06:17,765 --> 00:06:24,730
1den 5.000e kadar çalıştırıyorsunuz çünkü her biri 1.000 yüksekliğinde olan 5.000 tane mini-toptanımız var.

91
00:06:24,730 --> 00:06:29,600
for döngüsünün içinde yapmanız gereken basitçe XT virgül YTyi

92
00:06:29,600 --> 00:06:38,157
kullanarak gradyan inişin bir adımını uygulamaktır.

93
00:06:38,157 --> 00:06:48,340
Yani sanki 1.000 örneklik bir eğitim setiniz varmış ve

94
00:06:48,340 --> 00:06:51,130
tüm bu bildiklerinizi uygulamanız gerekiyormuş

95
00:06:51,130 --> 00:06:54,370
ama sadece 1.000lik tüm örnekler için belirlenmiş for döngüsü

96
00:06:54,370 --> 00:07:00,910
olmak yerine sadece M eşittir 1.000lik bu küçük eğitim setine uygulayacakmış gibi.

97
00:07:00,910 --> 00:07:06,595
1.000 örneğin hepsini aşağı yukarı aynı anda işlemek için vektörizasyon kullanmış oluyorsunuz.

98
00:07:06,595 --> 00:07:08,910
Önce bunu bir yazalım,

99
00:07:08,910 --> 00:07:15,710
girdilerdeki bir dayanak için uyguluyorsunuz.

100
00:07:15,710 --> 00:07:24,315
yani, sadece XT üzerine ve bunu Z1 eşittir W1i uygulayarak yapıyorsunuz.

101
00:07:24,315 --> 00:07:27,655
Daha önce burada sadece X olurdu, değil mi?

102
00:07:27,655 --> 00:07:30,040
ama şimdi tüm eğitim setini işliyorsunuz,

103
00:07:30,040 --> 00:07:32,140
sadece mini-toptan Tyi işlerken XT olabilmesi için

104
00:07:32,140 --> 00:07:36,065
önce mini toptanı işliyorsunuz.

105
00:07:36,065 --> 00:07:45,420
Daha sonra A1 eşittir Z1in G1i olacaktır,

106
00:07:45,420 --> 00:07:48,394
bu aslında bir vektörleme ifadesi olduğu

107
00:07:48,394 --> 00:07:57,585
için büyük Z ve AL ile sonuçlanana kadar devam,

108
00:07:57,585 --> 00:08:03,935
cevap ZLnin GLsi ve bu sizin öngörünüz.

109
00:08:03,935 --> 00:08:09,005
ve fark ediyorsunuzdur ki burada vektörize bir uygulama kullanmanız gerekmektedir.

110
00:08:09,005 --> 00:08:14,125
Sadece bu vektörize uygulama 5 milyon örnek yerine 

111
00:08:14,125 --> 00:08:18,840
her seferinde 1.000 örneği işliyor.

112
00:08:18,840 --> 00:08:25,500
Sonra bedel fonksiyonu Jyi hesaplıyorsunuz ki bunu

113
00:08:25,500 --> 00:08:32,895
küçük eğitim setinizin boyutu olduğu için bir üzeri 1.000 olarak yazıyorum.

114
00:08:32,895 --> 00:08:38,580
I eşittir birden L'ye kadarın toplamı aslında

115
00:08:38,580 --> 00:08:45,490
YInın bedelidir ve anlaşılabilirlik adına,

116
00:08:45,490 --> 00:08:53,300
bu işaret XT ve YT örneklerini ifade etmektedir.

117
00:08:53,300 --> 00:08:55,344
Ve eğer düzenlileştirme kullanıyorsanız,

118
00:08:55,344 --> 00:08:59,295
ayrıca bu düzenlileştirme teriminiz de olabilir.

119
00:08:59,295 --> 00:09:03,345
Bunu paydaya geçirin çarpı Lnin toplamı,

120
00:09:03,345 --> 00:09:07,980
bunu kare yapıyoruz.

121
00:09:07,980 --> 00:09:12,625
Bu aslında sadece bir mini-toptanın bedeli olduğu için,

122
00:09:12,625 --> 00:09:18,983
bunu bedel J üzeri süslü parantezde T olarak dizinleyeceğim.

123
00:09:18,983 --> 00:09:23,925
Dikkat edin ki, yaptığımız herşey daha önce gradyan iniş uyguladığımızda

124
00:09:23,925 --> 00:09:29,040
yaptığımız ile aynı, sadece bunu XY üzerine yapmak yerine

125
00:09:29,040 --> 00:09:31,680
XT YT üzerinde yapıyorsunuz.

126
00:09:31,680 --> 00:09:36,470
Sonra bu desteği JTye göre gradyanları

127
00:09:36,470 --> 00:09:44,285
hesaplamak için uyguluyorsunuz,

128
00:09:44,285 --> 00:09:54,120
hala sadece XT YT kullanıyorsunuz ve sonra W ağırlıklarını güncellediğinizde,

129
00:09:54,120 --> 00:09:59,410
okuyun, WL, WL eksi alfa d(WL)

130
00:09:59,410 --> 00:10:08,124
olarak güncellenir ve benzer şekilde B için.

131
00:10:08,124 --> 00:10:17,620
Bu mini-toptan gradyan inişi kullanarak eğitim setinizden bir geçiştir.

132
00:10:17,620 --> 00:10:25,420
Buraya yazdığım kod aynı zamanda eğitimin bir dönemi olarak adlandırılır ve

133
00:10:25,420 --> 00:10:34,022
dönem eğitim setinden bir geçiş anlamına gelen bir kelimedir.

134
00:10:34,022 --> 00:10:38,440
Halbuki, toptan gradyan iniş ile,

135
00:10:38,440 --> 00:10:44,420
eğitimden bir tek geçiş sadece bir gradyan iniş adımı atmaya izin verir.

136
00:10:44,420 --> 00:10:48,475
Mini-toptan gradyan iniş ile, eğitim setinden tek bir geçiş,

137
00:10:48,475 --> 00:10:52,890
yani bir dönem, 5.000 gradyan iniş adımı atmanıza izin veriyor.

138
00:10:52,890 --> 00:10:55,040
Şimdi elbette genelde isteyeceğiniz gibi,

139
00:10:55,040 --> 00:10:58,430
eğitim setinden birden fazla geçiş yapmak isteyeceksiniz,

140
00:10:58,430 --> 00:11:02,730
burada bir başka while döngüsü için bir başka for döngüsü isteyebilirsiniz.

141
00:11:02,730 --> 00:11:05,180
yani, yakınsayana veya yaklaşık olarak yakınsayana

142
00:11:05,180 --> 00:11:08,909
kadar eğitim setinden geçiş yapmaya devam edeceksiniz.

143
00:11:08,909 --> 00:11:10,620
Kayıp bir eğitim setiniz olduğunda,

144
00:11:10,620 --> 00:11:15,330
mini-toptan gradyan iniş toptan gradyan inişten çok daha hızlı çalışmaktadır ve

145
00:11:15,330 --> 00:11:17,540
bu aşağı yukarı Derin Öğrenmedeki herkesin büyük bir veri setini

146
00:11:17,540 --> 00:11:20,205
eğitirken kullanacağı şeydir.

147
00:11:20,205 --> 00:11:24,230
Bir sonraki videoda, ne yaptığını veya neden böyle çalıştığını anlamak

148
00:11:24,230 --> 00:11:28,650
için mini toptan gradyan inişe daha derinlemesine bakalım.