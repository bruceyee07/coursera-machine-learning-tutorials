1
00:00:00,320 --> 00:00:04,590
이전 비디오에서는 
미니 배치 기울기 강하를 이용해서

2
00:00:04,590 --> 00:00:08,370
진전있게하고, 처음에도 트레이닝 세트 중간에서

3
00:00:08,370 --> 00:00:11,960
기울기 강하 step를 가질 수 있도록 하는 방법을 보았는데요, 

4
00:00:11,960 --> 00:00:16,282
이번 비디오에서는, 기울기 강하를 도입하는 방법과

5
00:00:16,282 --> 00:00:19,896
이것이 하는 역할과 왜 잘 작동하는지에 대한 
이해도를 높히는 시간을 갖겠습니다.

6
00:00:19,896 --> 00:00:24,481
전체트레이닝세트의 수행업무마다 배치 기울기 강하에서는

7
00:00:24,481 --> 00:00:29,380
비용이 반복수행을 통해서 계속 내려갈 것을 기대할 것입니다.

8
00:00:30,660 --> 00:00:33,390
그러면 j 비용함수를 

9
00:00:33,390 --> 00:00:37,500
다른 반복 업무의 함수로 가지면, 
반복을 할 때마다 비용이 줄어야 할 것입니다.

10
00:00:37,500 --> 00:00:40,730
만약에 한번이라고 iteration절차에서 올라가는 경우엔, 
이상이 있는 것입니다.

11
00:00:40,730 --> 00:00:43,250
아마 러닝속도가 너무 클 수도 있겠죠. 

12
00:00:43,250 --> 00:00:48,090
하지만 미니 배치 기울기 강하 경우에, 
여러분이 만약 진행절차를 비용함수에 그리면

13
00:00:48,090 --> 00:00:51,660
반복할 때마다 감소할 수도 있습니다.

14
00:00:51,660 --> 00:00:56,822
특히, 반복업무마다 여러분은

15
00:00:56,822 --> 00:01:01,425
X{t}, Y{t}를 처리하는 것인데요, 

16
00:01:01,425 --> 00:01:05,888
그렇기 때문에 J{t}비용함수를 그리면,

17
00:01:05,888 --> 00:01:11,490
이것은 X{t}, Y{t}를 이용해서 계산이 되는데요, 

18
00:01:11,490 --> 00:01:17,170
그러면 반복수행을 할때마다 다른 트레이닝세트에서 
트레이닝을 하는 것과 마찬가지입니다.

19
00:01:17,170 --> 00:01:19,380
또는 다른 미니 배치에서 트레이닝 하는 것과 마차가지인 것이죠. 

20
00:01:19,380 --> 00:01:20,770
그러면 J 비용함수를 그리는데요, 

21
00:01:20,770 --> 00:01:23,310
이렇게 생긴 것을 볼 것입니다.

22
00:01:23,310 --> 00:01:27,479
그러면 이렇게 밑으로 내려가는 경향을 보일텐데요, 
동시에 조금 더 noisy할 것입니다.

23
00:01:30,554 --> 00:01:35,692
만약 J{t}를 그리면, 미니 배치 기울기 강하를 
복수의 epoch에서 트레이닝 시킬 때 말이죠, 

24
00:01:35,692 --> 00:01:40,670
그런 경우, 이렇게 생긴 커브를 볼 것입니다.

25
00:01:40,670 --> 00:01:44,284
그렇기 때문에 iteration 마다 내려가지 않아도 괜찮습니다.

26
00:01:44,284 --> 00:01:46,783
하지만 이렇게 내려가는 모양을 보일텐데요, 

27
00:01:46,783 --> 00:01:51,281
조금 더 noisy한 이유는 X{1},

28
00:01:51,281 --> 00:01:56,527
Y{1}이 조금 더 쉬운 미니 배치여서 비용이 조금 더 낮기 때문에 
그럴 수 있는데요, 

29
00:01:56,527 --> 00:02:02,057
하지만 우연으로 X{2},Y{2}가 어려운 미니 배치일 수도 있죠. 

30
00:02:02,057 --> 00:02:04,356
mislabel된 예시도 있을 수도 있겠죠, 

31
00:02:04,356 --> 00:02:06,780
이런 경우 비용이 조금 더 높겠습니다.

32
00:02:06,780 --> 00:02:09,511
그렇기 때문에 비용을 그리면서 이와 같은 변동폭이 생기는 것인데요, 

33
00:02:09,511 --> 00:02:13,277
미니 배치 기울기 강하를 실행할때 말이죠. 

34
00:02:13,277 --> 00:02:18,070
여러분이 골라야하는 parameter중에 하나는 
바로 미니 배치의 크기입니다.

35
00:02:18,070 --> 00:02:22,894
미니 배치 사이즈 가 m 인 경우, m은 one extreme에서의 
트레이닝세트 크기였습니다. 

36
00:02:26,544 --> 00:02:31,056
그럴 경우, 배치 기울기 강하가 남게 되는데요, 

37
00:02:36,056 --> 00:02:41,068
여기 extreme에서는 그냥 한개의 미니 배치
X{1},Y{1} 이 남을텐데요, 

38
00:02:41,068 --> 00:02:45,720
이 미니 배치는 전체 트레이닝 세트와 동일합니다.

39
00:02:45,720 --> 00:02:49,830
그렇기 때문에 미니 배치 사이즈를 m으로 설정하는 것은
단순히 배치 기울기 강하를 줍니다.

40
00:02:49,830 --> 00:02:57,669
다른 extreme은 미니 배치 사이즈가 1일 경우일 것입니다.

41
00:02:59,752 --> 00:03:03,238
이 경우 stochastic 기울기 강하라는 알고리즘을 주는데요. 

42
00:03:07,385 --> 00:03:16,076
여기서 각각의 예시는 그들의 미니 배치 입니다.

43
00:03:18,429 --> 00:03:24,172
이럴 경우에는, 첫번째 미니 배치를 봅니다.
즉, X{1}, Y{1}인데요, 

44
00:03:24,172 --> 00:03:29,682
그렇지만 미니 배치 사이즈가 1인 경우, 
첫번째 트레이닝 예시 밖에 없는데요, 

45
00:03:29,682 --> 00:03:34,620
즉 첫번째 트레이닝 샘플로
기울기 강하를 가져야 합니다.

46
00:03:34,620 --> 00:03:39,810
다음으로 두번째 미니 배치를 봅니다. 

47
00:03:39,810 --> 00:03:43,280
두번째는 그냥 두번째 트레이닝 샘플인데요, 
이 값에 기울기 강하 step를 적용합니다.

48
00:03:43,280 --> 00:03:45,170
그리고 이어서 세번째 트레이닝 예시도 그렇게 합니다.

49
00:03:45,170 --> 00:03:47,940
하나의 트레이닝 샘플씩 보는 것입니다.

50
00:03:50,100 --> 00:03:55,840
비용함수의 최적화에 있어 이런 2개의 extreme이 
어떻게 하는지 한번 보겠습니다.

51
00:03:55,840 --> 00:03:59,795
이것이 바로 여러분이 최소화 시키려하는
비용함수의 곡선이라고 하면, 

52
00:03:59,795 --> 00:04:01,067
여기가 그 최소값이 됩니다.

53
00:04:01,067 --> 00:04:05,825
그러면 배치 기울기 강하 는 여기쯤에서 

54
00:04:05,825 --> 00:04:12,320
시작할텐데요, 이 경우, 낮은 noise와
큰 step를 가질 수 있을 것입니다.

55
00:04:12,320 --> 00:04:15,600
그러면 계속 최소값을 향할 수 있을 것입니다.

56
00:04:15,600 --> 00:04:19,290
stochastic 기울기 강하 와는 반대로, 

57
00:04:19,290 --> 00:04:22,430
다른 점을 한번 지정해보겠습니다.

58
00:04:22,430 --> 00:04:26,180
그러면 한번의 iteration마다 한개의 트레이닝 예시로

59
00:04:26,180 --> 00:04:30,080
gradient descent를 하는 것인데요, 
그렇기 때문에 거의 항상 global minimum 에 도달하는데요

60
00:04:30,080 --> 00:04:33,865
가끔씩은 다른 방향으로 향해서, 

61
00:04:33,865 --> 00:04:36,303
한가지의 옛시가 나쁜 방향으로 가면, 

62
00:04:36,303 --> 00:04:40,530
stochastic gradient descent 같은 경우에,
굉장히 noisy 해질 수 있습니다.

63
00:04:40,530 --> 00:04:45,070
그리고 평균적으로 
좋은 방향으로 인도하겠지만,

64
00:04:45,070 --> 00:04:47,116
가끔씩은 틀린 방향으로 갈 것입니다.

65
00:04:47,116 --> 00:04:50,190
stochastic 기울기 강하는 절대 수렴하지 않을 것이기 때문에, 

66
00:04:50,190 --> 00:04:54,760
항상 어떻게 최소값 범위를 계속 동요하고 왔다갔다할 것입니다.

67
00:04:54,760 --> 00:04:58,006
하지만 이것은 절대로 최소값에 도달해 거기 멈춰있진 않을 것입니다.

68
00:04:58,006 --> 00:05:03,320
실제로는, 여러분이 사용할 미니 배치 사이즈는 여기 값 사이에 있을 것입니다.

69
00:05:07,976 --> 00:05:15,100
1과 m 사이와, 1과 m사이죠. 각각 2개 작은 값, 2개의 큰 값입니다. 이유는 이렇습니다.

70
00:05:15,100 --> 00:05:16,199
그리고 이유를 설명드리겠습니다.

71
00:05:16,199 --> 00:05:23,844
여러분이 기울기 강하를 사용하는 경우, 

72
00:05:23,844 --> 00:05:27,386
이것은 미니 배치 사이즈가 m 인 경우인데요, 

73
00:05:30,878 --> 00:05:35,190
반복 업무마다 아주 거대한 트레이닝 세트를 처리하는 것입니다.

74
00:05:35,190 --> 00:05:40,101
이것의 주요 단점은, iteration마다 너무 

75
00:05:40,101 --> 00:05:43,860
긴 시간이 소요된다는 것입니다.
아주 긴 트레이닝 세트가 있다는 가정하에 말이죠. 

76
00:05:43,860 --> 00:05:46,792
만약에 작은 트레이닝 세트가 있는 경우엔, 
배치 기울기 강하는 괜찮습니다.

77
00:05:46,792 --> 00:05:51,200
반대로 가면, 
stochastic 기울기 강하를 쓰는 경우엔, 

78
00:05:54,076 --> 00:05:58,967
하나의 예시만 처리해도 진전이 있다는 점이

79
00:05:58,967 --> 00:06:02,030
좋습니다. 이것은 문제가 되지 않습니다.

80
00:06:02,030 --> 00:06:04,290
그리고 noisiness는 더 작은 값의 러닝속도를 사용하여

81
00:06:04,290 --> 00:06:07,378
개선시키거나 감소시킬 수 있습니다.

82
00:06:07,378 --> 00:06:12,160
하지만 stochastic 기울기 강하의 큰 단점은 

83
00:06:12,160 --> 00:06:17,050
벡터화에서 거의 대부분의 속도를 잃는다는 것입니다.

84
00:06:18,370 --> 00:06:22,050
그 이유는, 여기서는 트레이닝 샘플을
하나씩 처리하는데요, 

85
00:06:22,050 --> 00:06:26,130
각각의 예시를 처리하는 방식은
매우 비효율적일 것입니다.

86
00:06:26,130 --> 00:06:32,380
그러므로 실제로 가장 잘 작동하는 것은 
미니 배치 사이즈의 사이즈가

87
00:06:36,687 --> 00:06:40,360
너무 크지 않거나 작지 않은 경우입니다.

88
00:06:44,439 --> 00:06:48,630
실제로 이런 경우에 가장 빠른 러닝을 제공합니다.

89
00:06:51,405 --> 00:06:54,860
여러분도 아시겠지만 이렇게 되면 2가지 부분에서 좋습니다.

90
00:06:54,860 --> 00:06:58,174
하나는 많은 벡터화를 갖게 된다는 것입니다.

91
00:06:58,174 --> 00:07:02,667
이번 비디오에서 사용했던 예시를 기억하면, 

92
00:07:02,667 --> 00:07:07,669
여러분의 미니 배치 사이즈가 1000개의 예시였으면, 
1000개의 예시를 두고 벡터화가 가능할 것입니다.

93
00:07:07,669 --> 00:07:12,110
이 경우, 예시를 하나씩 처리하는 것보다 훨씬 빠를 것입니다.

94
00:07:13,670 --> 00:07:16,710
둘째로, 전체 트레이닝 세트를 처리하기 전까지

95
00:07:22,210 --> 00:07:27,710
기다릴 필요없이 진전이 있게 만들 수 있습니다.

96
00:07:32,430 --> 00:07:36,719
다시 한번 이전 비디오에서 다뤘던 수치를 
사용하면, 각각의 트레이닝 세트의 

97
00:07:36,719 --> 00:07:40,640
Epocho는 5000개의 기울기 강하 step를 밟게 해줍니다.

98
00:07:41,840 --> 00:07:46,370
그렇기 때문에 실제로는 가장 잘 작동하는
중간의 미니 배치 사이즈가 있을 것입니다.

99
00:07:46,370 --> 00:07:49,380
그러므로 미니 배치 기울기 강하에 대해서는
여기서 시작하겠습니다.

100
00:07:49,380 --> 00:07:53,670
한번의 iteration이 이렇게 만들 수 있고, 
2개의 iteration, 3개, 4개 

101
00:07:53,670 --> 00:07:58,521
그리고 최소화 값을 향할 것이라는 것은 보장할 수 없습니다.

102
00:07:58,521 --> 00:08:03,383
하지만 조금 더 균등하게 최소값을 향해 
consequent descent보다 가는 경향이 있습니다.

103
00:08:03,383 --> 00:08:08,320
그리고 항상 정확하게 변환되거나 작은 범위에서 변동하는 것이 아닙니다.

104
00:08:08,320 --> 00:08:11,550
이것이 문제라고 하면 언제든지 
러닝속도를 늦출 수 있습니다.

105
00:08:11,550 --> 00:08:13,410
learning rate decay에 대해선
나중의 비디오에서 

106
00:08:13,410 --> 00:08:15,960
러닝속도를 줄이는 방법과 같이 알아보겠습니다.

107
00:08:15,960 --> 00:08:20,020
그러면 만약 미니 배치 사이즈가 
m이 아니고, 1이 아니며, 

108
00:08:20,020 --> 00:08:23,410
그 사이 값을 가지려면, 
어떤 것을 고를 수 있을까요?

109
00:08:23,410 --> 00:08:24,826
여기 그 가이드라인이 조금 있습니다.

110
00:08:24,826 --> 00:08:33,470
첫번째로, 작은 트레이닝 세트일 경우,
그냥 배치 기울기 강하를 이용하십시오. 

111
00:08:36,655 --> 00:08:41,023
작은 트레이닝 세트를 갖고 계시는 경우, 
미니 배치 기울기 강하를 쓰는 의미가 없습니다.

112
00:08:41,023 --> 00:08:43,670
전체 트레이닝 세트를 꽤 빨리 처리할 수 있기 때문입니다.

113
00:08:43,670 --> 00:08:45,619
그러므로 그냥 배치 기울기 강하를 이용하면 되겠죠. 

114
00:08:45,619 --> 00:08:50,281
작은 트레이닝 세트가 뜻하는 것은, 제가 생각하기에
2000보다 작을 때를 말하는 것입니다.

115
00:08:50,281 --> 00:08:54,480
이런 경우, 배치 기울기 강하를 쓰는 것이 괜찮습니다.

116
00:08:54,480 --> 00:09:00,391
아니면, 조금 더 큰 트레이닝 세트의 경우, 
전형적인 미니 배치 사이즈는

117
00:09:03,336 --> 00:09:09,437
64에서 512가 대표적인데요, 

118
00:09:09,437 --> 00:09:14,130
컴퓨터 메모리가 배치되어 있는 방식과 
접속되는 방법에 따라, 

119
00:09:14,130 --> 00:09:19,460
가끔씩 그 코드가 2의 지수값을 가질때 
더 빨리 실행됩니다.

120
00:09:19,460 --> 00:09:24,108
64는 그러면 2의 6승이구여, 
2의 7승, 2의 8승,

121
00:09:24,108 --> 00:09:30,080
2의 9승, 저는 그래서 자주
미니 배치 사이즈를 어떤값의 2승으로 도입합니다.

122
00:09:30,080 --> 00:09:33,900
이번 비디오에서는 제가 
미니 배치 사이즈 1000을 사용했는데요, 

123
00:09:33,900 --> 00:09:37,990
만약 정말로 그렇게 하고 싶은 경우엔, 
저는 여러분께서 1024를 쓰시는 것을 추천 드립니다.

124
00:09:37,990 --> 00:09:39,870
이 값은 2의 10승이죠. 

125
00:09:39,870 --> 00:09:46,176
조금 흔하진 않지만, 미니 배치 사이즈가 1024인게 있긴 합니다.

126
00:09:46,176 --> 00:09:50,681
여기 이 범위의 미니 배치 사이즈가 조금 더 흔하긴 합니다.

127
00:09:50,681 --> 00:09:54,980
마지막 팁으로는,

128
00:09:57,260 --> 00:10:05,309
모든 X{t}, Y{t} 값이 CPU/GPU 메모리에 들어가게 하도록 하는 것입니다.

129
00:10:08,563 --> 00:10:10,863
이것은 전적으로 여러분의 어플에 달려있는데요, 

130
00:10:10,863 --> 00:10:12,800
또 트레이닝 샘플이 얼마나 큰지에 따라 변할 수 있습니다.

131
00:10:12,800 --> 00:10:17,430
하지만 여러분이 CPU 또는 GPU에 들어가지 않는 미니 배치를 

132
00:10:17,430 --> 00:10:20,640
처리하게 되는 경우, process와 
데이터를 쓰는 경우에 

133
00:10:20,640 --> 00:10:24,336
성능이 저하되는 것을 보시면서 

134
00:10:24,336 --> 00:10:25,809
더 악화되는 것을 볼 것입니다.

135
00:10:25,809 --> 00:10:30,273
바라건대 이 내용이 사람들이 사용하는 

136
00:10:30,273 --> 00:10:31,790
전형적인 미니 배치 사이즈의 범위에 대해 여러분이 이해하셨으면 좋겠습니다.

137
00:10:31,790 --> 00:10:35,970
실제로는 당연히 미니 배치 사이즈가 
또 다른 하나의 하이퍼 파라미터 인데요,  

138
00:10:35,970 --> 00:10:40,840
여러분이 빠른 검색을 통해 어떤 것이 

139
00:10:40,840 --> 00:10:43,960
j 비용함수를 줄이는데 충분한지 
알아볼 수 있습니다.

140
00:10:43,960 --> 00:10:47,065
저는 개인적으로 여러가지 값들을 시도해볼 것입니다.

141
00:10:47,065 --> 00:10:51,727
2의 지수값을 몇개 시도해보고 그 다음에 
기울기 강하 최적화 알고리즘에 

142
00:10:51,727 --> 00:10:56,470
가장 효율적으로 만들어 줄 수 있는 한가지의 
값을 고를 것입니다.

143
00:10:56,470 --> 00:10:59,940
바라건대 오늘 내용이 가이드라인들이 되어

144
00:10:59,940 --> 00:11:03,405
하이퍼 파라미터 서치에 대한 이해도를 높혀줬길 바랍니다.

145
00:11:03,405 --> 00:11:07,012
이제 여러분은 미니 배치 기울기 강하를 도입하는 방법과

146
00:11:07,012 --> 00:11:10,378
알고리즘을 더 빨리 운영시키는 방법, 특히 큰 트레이닝 세트에서의 경우에 
대한 트레이닝 방법을 배웠습니다. 

147
00:11:10,378 --> 00:11:12,936
기울기 강하 또는 미니 배치 기울기 강하보다

148
00:11:12,936 --> 00:11:15,805
훨씬 더 효율적인 알고리즘이 존재합니다.

149
00:11:15,805 --> 00:11:18,215
다음 비디오로 시작하여 해당 내용을 다뤄보도록 하겠습니다.