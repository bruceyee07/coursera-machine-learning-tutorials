在深度學習早期的時代 大家常常擔心最佳化演算法 會卡在很爛的局部極值 (local optima) 不過隨著深度學習理論的進步 我們對局部極值的理解也隨之更改 就讓我來說明現今在深度學習，我們是 怎麼看待局部極值，還有最佳化時碰到的問題 以前大家擔心局部極值問題時，心裡想的是這種圖 或許你想找到最佳的一組參數 叫 w1, w2 然後這曲面的高度是成本函數 這張圖看起來有很多局部極小值散佈各地 很容易就發生梯度下降法 或其他演算法卡在某個局部極小值 而找不到全域極值 (global optimum) 的情況 其實呢，如果你嘗試在兩個維度的變數上畫圖 很容易就會畫出有很多局部極值的圖 我們的直覺曾經被這種低維度的圖所主導 但這個直覺其實並不正確 事實證明你在訓練神經網路的時候 大部分梯度為0的點，並不是這樣的局部極值 相反地，成本函數上大部分梯度為0的點是鞍點
(saddle points) 例如這個點的梯度就是 0 同樣這軸是 w1、 w2、高是成本函數的值。 不嚴謹來說，對於一個高維度空間的函數 如果其梯度為零 那麼從任一個方向看，他要嘛是 凸函數 (convex)，要嘛是凹函數 (concave) 假設你在 一個 20000 維的空間 一個點為了要是局部極值， 所有 20000 個方向都要看起來像這樣 所以這種發生機率可能非常小 也許是 2 的 -20000 次方。 所以相反地，你更可能看到某些方向像這樣往上彎 也有些方向那個函數會 像這樣往下彎，而不會所有方向都往上彎 這就是為什麼在非常高維度的空間 你通常會更有可能跑到一個鞍點，像右邊這樣， 而不會是局部極值。 至於為什麼這叫鞍點 (saddle point) 你可以想像一下 這看起來像是放在馬上的馬鞍對吧 那...也許這是匹馬 這個是馬頭 有隻眼睛 嗯...畫得不是很好不過你知道的啦 然後你是個騎師 坐在馬鞍這邊 這，就是這個點 導數為零的地方 這個點叫「鞍點」的原因 我猜在馬鞍上，你真的會坐在這個點上面 而那個地方剛好導數為0。 所以，在深度學習歷史上學到的 其中一個教訓是，很多我們在低維度空間的直覺 例如我們左邊畫的 他們不見得能套用到 非常高維度的空間，也就是學習演算法運算的地方 因為如果你有 20000 個參數 那麼 J 就是一個 20000維向量的函數 那你看到的絕大部分會是鞍點，不是局部最佳值。 如果局部極值不是問題 那問題在哪呢？ 結果發現 "plateau" (平頂/高原) 會減緩學習速度 plateau 是一種區域，在這區域的導數很長一段都接近零 所以假設你在這 那梯度下降法會沿著表面往下走 而因為梯度為 0，或很接近 0 這表面很平 所以你會花很長的一段時間 在 plateau 上緩慢地往這個點前進 然後因為隨機的左右微擾 所以最後（我換個筆的顏色比較清楚） 你的演算法找到了離開 plateau 的路線 他在這超級長的緩坡往下走，後來 才找到這邊，然後才能離開這個 plateau。 因此這影片的重點在：第一 你其實不大可能卡在 某個很爛的局部極值 — 只要你訓練的神經網路夠大 也就是有很多參數 而成本函數 J 是定義在比較高維的空間上 然而第二，"plateau" 會是一個問題；
他會讓學習過程非常緩慢 而這就是像動量法或 RMSprop 或 Adam 這種演算法發揮威力、幫助學習之處 在這種情況下，比較精密的最佳化演算法，
例如 Adam， 能真的幫你加速 在 plateau 裡往下跑然後離開。 那麼，因為神經網路解決的 是非常高維空間的優化問題，所以說實話 我覺得沒有人能很直觀地知道
這種空間實際上長怎樣 我們對它們的理解還在不斷發展。 不過，我希望能給你更好的概念 有關最佳化演算法會面臨的挑戰。 那麼，恭喜你，已經來到本週最後的內容了 請看一下這星期的測驗，還有程式練習 我希望你會喜歡在程式作業中練習這些概念 也很期待在下星期的影片再看到您