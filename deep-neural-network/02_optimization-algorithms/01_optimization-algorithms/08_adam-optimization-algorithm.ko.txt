딥러닝의 역사를 보면, 아주 유명한 리서치 연구원을 포함하여 수 많은 리서치 연구원들은 최적화 알고리즘을 간혹 제안하여 해당 알고리즘이 문제해결에 잘 쓰이는 것을 보여주었습니다. 이런 최적화 알고리즘은 여러분이 트레이닝 하고 싶은 넓은 범위의 신경만 네트워크를 잘 일반화시키지 못하는 것으로 보여지게 됩니다. 그렇게해서 점차 시간이 지나면서 딥러닝 커뮤니티는 전반적으로 최적화 알고리즘에 대해 어느정도 회의적인 시각을 갖게된 것 같습니다. 또한 많은 사람들이 기울기 강하 와 모멘텀이 아주 잘 작동한다고 생각해서 이보다 더 잘 작동하는 것을 제안하는 것이 쉽지 않았습니다. 이번 비디오에서는 RmsProp과 Adam 최적화 알고리즘에 대해 이야기 해볼텐데요 이 알고리즘은 드물게 잘 작동하는 알고리즘이였는데요, 넓은 범위의 딥러닝 구조에서도 굉장히 잘 작동하는 것으로 보여졌습니다. 이 알고리즘은 제가 망설이지 않고 여러분께 시도해보라고 추천드리는 알고리즘인데요 그 이유는 많은 사람들이 시도하였고 또 잘 작동하는 것을 직접 보았습니다. Adam 알고리즘은 기본적으로 모멘텀과 RmsProp을 같이 합치는 것이라고 생각하면 됩니다. 이것이 어떻게 되는지 한번 보겠습니다. Adam을 도입하기 위해서는 초기화를 진행할텐데요. Vdw는 0, Sdw도 0, 그리고 비슷하게 Vdb와 Sdb도 0으로 지정합니다. 그리고 반복 루프 t회째에 대해서 미분을 계산할텐데요.
dw와 db를 현재 미니 배치를 이용해 계산하고 보통, 미니 배치 기울기 강하를 이용하면 되고요. 그리고 모멘텀, 지수 가중 평균을 쓸텐데요.
Vdw = ß인데, 이제부터는 이 값을 ß1이라고 쓰고,
RmsProp의 하이퍼 파라미터 ß는 ß2로 구분해서 사용하겠습니다. 자, 이것은 모멘텀을 구현할 때 했던 것과 완전히 동일한데요 유일한 차이는 하이퍼 파라미터 ß 대신에 ß1이라고 부른 점입니다. 유사하게 Vdb 는 이렇게, 1 빼기 ß1 곱하기 db죠. 그리고 RmsProp도 업데이트 하는데요. 이제 하이퍼 파라미터는 ß2죠. 여기는 플러스 1 빼기 ß2 dw² 여기서 제곱은 미분 dw를 원소별로 제곱하는 것이고요. 그리고 Sdb는 이것 더하기 1빼기 ß2 곱하기 db입니다. 이 하이퍼 파라미터 ß1으로 모멘텀과 같은 식으로 업데이트를 하고, 하이퍼 파라미터 ß2 는 RmsProp과 같은 식의 업데이트죠. 일반적으로 Adam 구현에는, 편향보정도 같이 합니다. 그러므로 v corrected 를 사용할텐데, Corrected(보정된)는 편향 보정된 것을 뜻 합니다. dw는 vdw 나누기 1 빼기 ß1 의 t승, t회째 반복 루프를 진행한 경우에 말이죠. 비슷하게, vdb corrected는 vdb 나누기 1 빼기 ß1 의 t승입니다. 그리고 또 비슷하게, S에도 편향보정을 적용해서 즉, sdw 나누기 1 빼기 ß2 의 T승, 그리고 sdb corrected는 sdb 나누기 1 빼기 ß2 의 T승입니다. 마지막으로 업데이트를 진행합니다. 그러면 W는 W 빼기 알파 곱하기, 만약 모멘텀만 사용하는 경우 Vdw를 사용하거나, Vdw corrected가 될 수도 있겠죠. 이제 RmsProp에 해당하는 부분도 추가하는데요. 그래서, Sdw corrected + 앱실론의
루트로 나눠줍니다. 마찬가지로, 비슷한 공식으로 b 또한 업데이트 될텐데요. Vdb corrected 나누기 루트 S corrected db 더하기 앱실론입니다. 이 알고리즘은 기울기 강하에 모멘텀 효과와 함께 RmsProp을 같이 결합한 것입니다. 이게 흔히 자주 쓰이는 학습 알고리즘인데요.
아주 다양한 신경망 네트워크 광범위한 구조들에 매우 효과가 있음이 증명되었습니다. 이 알고리즘은 몇개의 하이퍼 파라미터들이 있는데요. 러닝 속도인 하이퍼 파라미터 알파α는 여전히 중요하고, 보통 튜닝이 필요합니다. 그러므로 여러분이 다양한 범위의 값을 시도해서 어떤 값이 적합한지 찾아야 합니다. ß1의 가장 흔한 설정 값은 0.9입니다. 이것은 이동 평균값인데요. dw의 가중평균값인데요, 이것은 모멘텀 같은 성분이고, 하이퍼 파라미터 ß2는, Adam 논문의 저자, Adam 알고리즘 발명자는 0.999를 권장하였습니다. 이것은 dw²과 또, db²의 이동평균
(지수 가중 평균)들을 계산하는 것입니다. 그리고, 앱실론ε 값 선택은 그리 중요하지 않습니다. Adam 논문의 저자들은 10의 마이너스 8승을 권장합니다만, 이 파라미터는 정말로 굳이 바꿀 필요가 없고, 성능에도 거의 영향이 없기 때문이죠. Adam을 구현할 때는, 사람들은 보통 기본값을 많이 사용합니다. 즉, ß1, 그리고 ß2 또한 ε도 쓰죠. 제 생각에 실제로 앱실론ε을 튜닝하는 사람은 없습니다. 그리고나서, 여러가지 범위의 알파α 값으로 바꿔보며,
어떤게 가장 잘 동작하는지 해 보세요. 물론, ß1과 ß2도 튜닝을 할 수 있습니다만, 전문적으로 일하시는 분들 중엔
자주 하는 않는 것으로 알고 있습니다. 그러면, Adam의 뜻은 어디서 유래된 것일까요? Adam은 "적응 모멘트 추정"
(Adaptive Moment Estimation)을 뜻합니다. ß1은 미분의 평균값을 산출하는데요. 이것을 1차 모멘트라고 합니다. 그리고 ß2는 dW²의 지수이동평균에 사용되는데, 이것은 2차 모멘트라고 합니다. 이런 이유에서 adaptive moment estimation이라는 이름이 나오게 됐습니다. 하지만 대부분의 사람들은 그냥 "아담 최적화 알고리즘"이라고 부릅니다. 아, 그리고 저와 오랜전부터 친구이고 같이 일한 Adam Coates인데요. 제가 아는 한, 이 알고리즘은 이 친구와는 연관된 것이 없습니다. 이 친구가 가끔 이 알고리즘을 쓴다는 점 이외에는요. 이 질문을 가끔씩 주위에서 하셔서요. 혹시나 여러분이 궁금해하실까봐 말씀드렸습니다. Adam 최적화 알고리즘에 대한 부분은 이게 전부인데요. 이런 내용을 바탕으로 이제 여러분께서는 더 빨리 신경망을 트레이닝 시키실 수 있을 것입니다. 이번주 강의를 마치기에 앞서, 하이퍼 파라미터 튜닝에 대해 이야기 해보겠습니다. 또한 신경망과 관련된 최적화 문제가 어떻게 생긴 것인지 직관적인 부분도 다루어 보도록 하겠습니다. 다음 비디오에서는 learning rate decay에 대한 내용을 다루어보겠습니다.