在深度学习的早期阶段 人们常常担心优化算法 会陷入糟糕的局部最优(Local Optima)之中 但随着深度学习理论的发展 我们对局部最优的理解也在改变 我来向大家介绍一下<br />现在我们是如何看待局部最优 以及深度学习中的优化问题 当人们担心局部最优的时候<br />他们脑海中浮现的往往是这幅图 假设你正在优化某些参数 比如W_1和W_2 这个曲面的高度表示代价函数 在这幅图中 看上去到处都是局部最优 对于梯度下降 或其他的算法 都很容易陷入局部最优 而找不到全局最优 如果你画的是一张二维的图 确实容易像这样有很多局部最优 而人们的直觉往往被这些低维图像所引导 但实际上 这种直觉上的感受并不准确 如果你是在训练一个神经网络 代价函数中大部分梯度为零的点<br />实际上并不是像这样的局部最优 而是鞍点(Saddle Point) 这个点就是一个梯度为零的点 这两个坐标轴是W_1 W_2 高度代表代价函数J的值 从经验上来说 对于一个高维空间的函数 如果梯度为零 则在每个方向上 它可能是凸函数 或者是凹函数 假设在一个 2万维的空间中 如果一个点要成为局部最优 则需要在所有的2万个方向上都像这样 因此这件事发生的概率非常低 大概2的负2万次方 你更有可能遇到的情况是<br />某些方向的曲线像这样向上弯曲 同时另一些方向的曲线则向下弯曲 并非所有曲线都向上弯曲 这就是为什么在高维空间中 你更有可能碰到一个像右图这样的鞍点 而不是局部最优 至于为什么把这样的曲面称为鞍点 你可以想象一下 这像是放在马身上的鞍 对吧 比方说这是一匹马 这是马的头 这是马的眼睛 马画的不太好 但你们可以感受一下 然后你是那个骑马的人 会坐在这个马鞍上 这就是 这个导数为零的点 叫做鞍点的原因 我猜你坐在马鞍上的那一点 导数应该正好是零 这就是我们在深度学习历史中学到的一课 就是我们在低维空间里的大部分直观感受 比如左边这幅图 事实上并不适用于 我们深度学习算法所应用的<br />那些高维空间中 因为如果你有2万个参数 那么J作为一个2万维向量的函数 你更有可能遇到鞍点 而不是局部最优 如果局部最优不是问题 那么问题是什么呢 真正会降低学习速度的<br />实际上是停滞区(Plateaus) 停滞区指的是 导数长时间接近于零的一段区域 如果你在这里 那么梯度下降会沿着这个曲面向下移动 然而因为梯度为零或接近于零 曲面很平 你会花费很长的时间 缓慢地在停滞区里找到这个点 然后因为左侧或右侧的随机扰动 为了能更清晰地展示 我换一个颜色 你的算法终于能够离开这个停滞区 它一直沿着这个很长的坡往下走 直到抵达此处 离开这个停滞区 本次视频的要点是 首先 实际上你不太可能<br />陷入糟糕的局部最优点 只要你训练的是一个较大的神经网络 有很多参数 代价函数J定义在一个相对高维的空间上 其次 停滞区是个问题<br />它会让学习过程变得相当慢 这也是像动量(Momentum)算法<br />或RmsProp算法 或Adam算法能改善你的学习算法的地方 这些场景下 更复杂的算法 比如Adam算法 可以加快沿停滞区向下移动 然后离开停滞区的速度 因为神经网络是在 非常高维的空间上解决优化问题 说实话 我觉得没有人能对<br />这类空间长什么样有非常好的直觉 我们对它们的理解还在不断发展 但我希望这节课能够让你们 对优化算法所面对的挑战 有一些更直观的感受 至此 恭喜你们学完了这周最后的内容 请做一下这周的小测验和编程练习 希望你们能在本周的编程练习中学以致用 期待在下周的视频中再和你们见面