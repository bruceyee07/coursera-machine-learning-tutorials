학습 알고리즘을 가속화시키는데
도움이 될 수 있는 것 중 하나는 러닝 레이트를 시간이 가면서
천천히 줄여나가는 것입니다. 이것을 learning rate decay
(학습 속도 감쇠법)라고 합니다. 이것을 어떻게 구현할 수 있는지 한번 보겠습니다. 왜 학습 속도 감쇠법(learning rate decay)를 써 볼 필요가 있는지 한가지 예제부터 보겠습니다. 여러분이 미니 배치 기울기 강하를 사용한다고 가정해보죠. 비교적 작은 미니 배치로 말이죠. 미니 배치가 64개나 128개 데이터가 있다고 해보죠. 여러분이 학습을 반복하면
이 단계들이 조금 지저분 할 수 있습니다. 여기선 최소값으로 가는 경향은 있지만,
정확히 수렴하진 않을 것입니다. 그러면, 여러분의 알고리즘은 계속 주위를 돌면서 절대로 수렴하지 않을 것입니다.
알파를 어떤 값으로 고정했기 때문이고, 또, 미니 배치들에 노이즈도 있기 때문입니다. 만약 여러분이 학습 속도 알파α 
천천이 줄여 나간다고 한다면, 초기 구간에서는
학습 속도 알파가 아직 큰 값일 때에, 비교적 빠른 학습이 가능할 것입니다. 그러다, 알파 값이 점점 작아지면서,
스텝도 점차 느려지고 작아질 것입니다. 그럼 결국엔 이 최소값 부근의
매우 좁은 범위에서 왔다갔다 하겠죠. 아까 전처럼 학습을 계속해도
멀리 크게 왔다갔다 하는 대신에 말이죠. 그러므로 알파 값을 천천히 줄이는
방법의 배경을 직관해 보면 학습 초기 단계에는 큰 스텝으로 움직이게 할 수 있고, 학습이 점점 수렴해 갈 수록 학습 속도를 느려지게 해서
점점 작은 스텝으로 움직이게 합니다. 학습 속도 감쇠법
(learning rate decay)의
구현은 이렇습니다. 전에 기억이 나시겠지만,
1 에폭(epoch)은 한번의 패스(pass), 데이터들로을학습 루프를
죽 한번 돈 것입니다. 그렇죠? 그럼, 이런 트레이닝 세트가 있다고 하면, 미니 배치들로 쪼갤 수 있겠죠. 첫번째 패스를 트레이닝 세트를 죽 실행하면
첫번째 에폭(epoch)이라고 하고, 두번째 패스를 둘 째 에폭(epoch)이라고 하고,
이런 식으로 이어집니다. 한가지 해 보면
러닝 속도 알파는 1 / 1 + 감쇠비(decay rate)라는 파라미터에 곱하기 에폭 값(epoch number)을 하고, 여기에 러닝 속도 α의 초기값 α0을 곱해 줍니다. 여기 감쇠비(decay rate)가
또 하나의 하이퍼 파라미터가 생겼죠. 그래서, 이것도 튜닝할 필요가 있겠죠. 구체적인 사례를 보겠습니다. 여러 에폭(epoch)을 진행해 보면 즉,
데이터에 여러번 패스를 돌리면 α0 는 0.2 이고, 감쇠비(decay-rate)는 1이면, 그럼 첫번 에폭(epoch)에서 α는 1/ (1 + 1 * α0)입니다. 학습 속도는 0.1이 되겠죠. 이 공식에 넣어보면 나오고요.
감쇠비(decay-rate)는 1이고, 에폭 값(epoch-num)은 1일 때 말이죠. 두번째 에폭(epoch)에서는
러닝 속도가 0.67로 감쇠되었습니다. 세번째에서는 0.5,
네번째에서는 0.4. 등등. 이 값들을 좀 더 구해보시길 바라고요. 에폭 값(epoch number)의 함수로 학습 속도는 점진적으로 작아지는 것이 감이 오죠.
위에 식에 따라서 말이죠. 여러분이 학습속도감쇠법
(learning rate decay)을
사용하시려면요, 다양한 값으로 이 하이퍼 파라미터 α0을 바꿔 보시고, 이 감쇠비(decay rate) 값도 바꿔보시고요. 그렇게 잘 동작하는 값을 찾아보세요. 이 학습속도감쇠법
(learning rate decay)의 수식 외에도 사람들이 쓰는 몇가지 또 다른 방법들이 있습니다. 예를 들어, 이것을 지수감쇠(exponential decay)라 하는데요. 알파 값은 1보다 작은 값인데, 예를 들어, 0.95에 epoch-num 제곱,
곱하기 α0 합니다. 그러면 이것은 기하급수적으로 빠르게
학습속도가 줄어들 것입니다. 사람들이 쓰는 또 다른 공식은 α = 어떤 상수 k / 루트 에폭값에 곱하기 α0. 또는, 
또 하나의 하이퍼 파라미터죠. 어떤 상수 k에 나누기 루트 미니 배치의 수 t 
곱하기 α0 입니다. 그리고 종종 사람들이 사용하는 것 중에,
학습속도를 감소시키는데 단계적으로 하는 방법도 있습니다. 몇 스텝 동안은 특정 러닝속도로 하다가 속도를 반으로 줄이고 한동안 하다가 다시 반으로 줄이고, 다시 조금 후에 반으로 줄이고, 이렇게 불연속 계단식으로 말이죠. 이제까지 몇가지 공식들을 얘기했죠. 수식을 이용해서 학습 속도 알파를 시간에 따라 어떻게 변하게 할지. 가끔 사람들이 하는 한 가지 방법은
수작업식 감쇠(manual decay)입니다. 만약 한번에 한가지 모델로 트레이닝을 한다면, 그 모델이 트레이닝 시키는데 몇 시간
또는 심지어 많은 날들이 소요된다면 어떤 사람들은 이런 경우에 뭘 하냐면, 그 많은 날들을, 그 모델이 학습 되는 것을
그저 보고 있을 것입니다. 그러다 수동 방식인데
"학습 속도가 좀 줄면 좋겠네", "알파 값을 좀 줄여봐야지" 하죠. 물론, 이 방법도 됩니다.
이렇게 알파값을 수동으로 직접 손으로 시간마다 또는 날마다
튜닝해 주면요. 이 방법은 트레이닝 모델 개수가
적을 때만 가능하지만 종종 그렇게 하고 있습니다. 이제 학습속도α 를 조정하는 방법에
좀 다양 옵션이 생겼습니다. 여러분이 "하이퍼 파라미터들이 엄청나게 많네"
라고 생각이 든다면, "어떻게 이 많은 옵션 중에 좋은 것을 고르지?" 일단, 저는 걱정하지 마시라고 말씀드리고요. 다음 주에는 하이퍼 파라미터들을
체계젹으로 고르는 방법을
이야기 해보겠습니다. 저라면 학습속도감쇠법(learning rate decay)은 제가 해 볼 것 중에 더 나중으로 놓을 것입니다. 알파를 정하는 것이,
일단 어떤 값으로 알파를 시작해서, 잘 튜닝하는 것이 큰 효과가 있고요. 학습속도감쇠법(learning rate decay)도
도움이 되기는 합니다. 가끔 트레이닝의 속도를 정말로 올려주기도 하지만, 개인적으로 시도할 것 중에서는
가장 먼저 할 일은 아닙니다. 다음주에는 하이퍼 파라미터 튜닝에 관한 이야기를 하면서 더 체계적 방법으로
이 모든 하이퍼 파라미터들을 다뤄 보겠습니다. 또한,효율적으로 하이퍼 파라미터들을
찾는 방법도 같이 배우겠습니다. 여기까지가 학습속도감쇠법(learning rate decay) 이었고요. 마지막으로 국소 최적(local optima)에 관한 내용과 신경망의 안장점(saddle point)에 대해 이야기를 하려는데요, 여러분이 풀려는 최적화 문제들이나 최적화 알고리즘들의 유형들에
좀 더 나은 직관이 생길 것입니다. 신경망을 트레이닝시킬 때 생길 수 있는 이런 유형들을 그럼, 다음 강좌에서 그것을 같이 알아 보죠.