有一种算法叫做动量(Momentum)
或者叫动量梯度下降算法 它几乎总会比标准的梯度下降算法更快 一言以蔽之 算法的主要思想是
计算梯度的指数加权平均 然后使用这个梯度来更新权重 在本视频中 我们会详细解释这句话 并教你如何实现它 举例来说 假设你想要优化一个代价函数 如等高线图所示 红色的点表示最小值的位置 假设从这里开始执行梯度下降<br />经过梯度下降的一次迭代后 无论是批量或小批量下降 结果可能会朝向这里 现在到了椭圆的另一边 如果你再进行一步梯度下降 可能就到了这里 一步又一步 以此类推 你会发现梯度下降算法会计算很多步 向着最小值缓慢地振荡前进 这种上下的振荡会减慢梯度下降的速度 同时也让你无法使用较大的学习率 如果你使用的学习率很大 可能会超调 像这样发散出去 因此为了避免振荡过大 你只能使用比较小的学习率 换一个角度来看这个问题 在纵轴上 你希望学习慢一点 因为你不希望有这些振荡 但是在横轴上 你希望加快学习速度 因为你希望快速地从左向右 朝最小值移动 也就是那个红点 下面是实现动量梯度下降的步骤 对每一次迭代 具体来说 在第t次迭代中 你需要计算导数dW db 这里我省略了上角标[l] 计算当前的小批量的dW db 如果你使用的是批量梯度下降算法 则当前的小批量就是你的整个批量 这同样适用于批量梯度下降 如果当前的小批量是整个训练集 也同样适用 然后要做的是计算 v_dW=beta*v_dW+(1-beta)*dW v_dW=beta*v_dW+(1-beta)*dW 这和我们之前计算过的 v_theta=beta*v_theta+(1-beta)*theta_t 很像 也就是计算W导数的滑动平均值 然后对v_db做同样的计算 v_db=beta*v_db+(1-beta)*db 然后更新权重 新的权重是 W-alpha*v_dW 使用v_dW更新权重 而不是dW 同样地 b=b-alpha*v_db 这样做可以让梯度下降的每一步变得平滑 举例来说 假设你计算的前几个倒数分别是 这样 这样 这样 这样 如果把这些梯度平均一下 你会发现这些震荡 在纵轴上的平均值趋近于0 所以 在垂直方向上 你会希望减慢速度 正数和负数在计算平均时相互抵消了 平均值接近于0 然而在水平方向上 所有导数都指向水平方向的右边 所以水平方向的平均值仍然较大 因此在数次迭代之后 你会发现动量梯度下降算法的每一步 在垂直方向上的振荡非常小 且在水平方向上运动得更快 这会让你的算法选择更加直接的路径 或者说减弱了前往最小值的路径上的振荡 对动量有这样一种直观的解释 可能有助于部分人理解 就是如果你想最小化一个碗型函数 这是一个碗的等高线图 我可能不太擅长画画 在最小化这个碗型函数时 你可以把这两个导数项看成 一个球滚下坡时的加速度 把这些动量项看成是球的速度 所以想象一下 你有一个碗和一个球 导数项给球了一个加速度 然后球就向下滚 因为有加速度 所以它滚得越来越快 因为beta是一个略小于1的数 可以把它看作摩擦力 让球不至于无限加速下去 与梯度下降中 每一步都独立于之前步骤所不同的是 现在你的球可以向下滚并获得动量 沿碗向下加速并获得动量 我觉得这个球沿碗滚下的比喻 对那些喜欢物理的人比较容易理解 但并不是每个人都能接受 这个球沿碗滚下的比喻 如果你不能理解 也不用在意 最后 让我们进一步学习如何实现它 算法如下 现在你有2个超参数 学习率alpha和这个参数beta beta控制指数加权平均 beta最常用的取值是0.9 就像我们之前计算最近10天气温的平均值 这里就是计算前10次迭代的梯度的平均值 在实践中 使用beta=0.9效果很好 你也可以尝试不同的值 做一些超参数搜索 但是0.9是非常稳健的参数值 至于偏差修正 是否需要让v_dW或v_db除以1-beta^t呢 实际上 通常人们不会这么做 因为在10次迭代之后 你的滑动平均值已经就绪 不再是一个偏差估计 所以实际上 在实现梯度下降或动量梯度下降时 我没见过有人会做偏差修正 当然这里需要把v_dW初始化为0 注意这是一个零矩阵 维数和dW相同 也就是和W具有相同的维数 v_db也要初始化为一个零向量 维数和db相同 也就是和b有相同的维数 最后我想提一点 当你在阅读关于 动量梯度下降的文献时会发现 (1-beta)这一项常常被省略 这样会得到 v_dW=beta*v_dW+dW 使用紫色版本的结果是 v_dW被缩小了(1-beta)倍 相当于乘以1/(1-beta) 所以在进行梯度下降的更新时 alpha也要根据1/(1-beta)进行调整 实际上 这两种写法都可以 只会影响学习率alpha的最佳值 但是我觉得这个公式不够直观 其中一个影响是 如果你调整了超参数beta的值 就会同时影响v_dW和v_db的缩放 然后你可能又需要重新调整学习率alpha 所以我个人更倾向于使用写在左侧的这些公式 而不是省去(1-beta)项 我倾向于使用左侧的公式 即有(1-beta)项的公式 不过对于这两种版本 将beta设置为0.9都是普遍的选择 只是这两个版本对学习率alpha的调整会 有所不同 以上就是动量梯度下降算法 它几乎总是优于 不使用动量的梯度下降算法 还有一些方法也可以加速你的学习算法 我们会在接下来的几个视频中进行讨论