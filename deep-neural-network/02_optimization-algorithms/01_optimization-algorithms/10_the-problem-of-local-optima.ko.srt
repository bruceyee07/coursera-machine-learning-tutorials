1
00:00:00,000 --> 00:00:01,710
딥러닝 분야 초기에 사람들은

2
00:00:01,710 --> 00:00:04,380
최적화 알고리즘이 좋지 않은 국소 최적 값에 걸리는

3
00:00:04,380 --> 00:00:07,415
것에 대해 매우 불안해 했습니다.

4
00:00:07,415 --> 00:00:09,660
하지만 점차 딥러닝에 대한 이론적인 부분이 발달하면서

5
00:00:09,660 --> 00:00:13,285
국소 최적에 대한 이해가 날로 변하고 있습니다.

6
00:00:13,285 --> 00:00:16,855
딥러닝 분야에서 국소 최적에 대한 문제와 최적화 문제에

7
00:00:16,855 --> 00:00:21,279
대해 현재 시점에서 어떻게 생각하고 있는지 보여드리겠습니다.

8
00:00:21,279 --> 00:00:25,695
이 그림은 사람들이 국소 최적에 대해 걱정을 했을 때 생각했던 그림입니다.

9
00:00:25,695 --> 00:00:28,786
여러분은 아마 특정 세트의 파라미터를 최적화하고 싶을 수 있습니다.

10
00:00:28,786 --> 00:00:30,580
이것을 W1 과 W2라고 부릅니다.

11
00:00:30,580 --> 00:00:33,913
표면의 높이는 비용함수입니다.

12
00:00:33,913 --> 00:00:38,655
이 그림에서, 이런 다양한 지점에서 국소 최적값이 많은 것처럼 보입니다.

13
00:00:38,655 --> 00:00:41,010
그리고 기울기 강하나

14
00:00:41,010 --> 00:00:43,622
또는 다른 유형의 알고리즘이 전역최적값에 도달하기 보다는

15
00:00:43,622 --> 00:00:47,226
국소 최적값에 속하는 경우가 쉽게 발생할 것입니다.

16
00:00:47,226 --> 00:00:51,945
알고보면 2차원에서 이런 그림을 그리는 경우,

17
00:00:51,945 --> 00:00:56,637
이런 그래프에서 다수의 국소 최적값을 쉽게 만들 수 있다는 것을 발견할 수 있습니다.

18
00:00:56,637 --> 00:01:00,285
이런 저차원의 그래프에서 직관을 얻을 수 있는데요.

19
00:01:00,285 --> 00:01:02,730
하지만 사실 이것이 올바르다고 할 수 없습니다.

20
00:01:02,730 --> 00:01:04,878
신경망 네트워크를 새로 만들게되면,

21
00:01:04,878 --> 00:01:09,965
기울기가 0인 지점이 항상 이런 지점처럼 국소 최적값인 것은 아닙니다.

22
00:01:09,965 --> 00:01:15,330
대신에, 비용함수에서 기울기가 0인 대부분의 지점들은 안장점입니다.

23
00:01:15,330 --> 00:01:17,845
다시 말씀드리면, 기울기가 0인 경우

24
00:01:17,845 --> 00:01:19,826
그리고 파라미터 값 W1과

25
00:01:19,826 --> 00:01:25,150
W2이고 높이가 비용함수 J의 값인 경우 말이죠.

26
00:01:25,150 --> 00:01:28,523
비공식적으로는, 고차원의 공간에서 정의되는 함수에서

27
00:01:28,523 --> 00:01:30,075
만약 기울기가 0인 경우,

28
00:01:30,075 --> 00:01:32,835
방향에 따라서

29
00:01:32,835 --> 00:01:36,810
convex light 함수이거나 concave light 함수일 수 있습니다.

30
00:01:36,810 --> 00:01:38,660
만약에 고차원인 경우,

31
00:01:38,660 --> 00:01:40,785
예를 들어, 20,000 차원의 공간의 경우

32
00:01:40,785 --> 00:01:42,510
국소 최적값이 존재하기 위해서는

33
00:01:42,510 --> 00:01:45,795
20,000가지의 방향은 이렇게 생겨야 할 것입니다.

34
00:01:45,795 --> 00:01:49,274
이렇게 될 확률은 아마 매우 작을 것인데요.

35
00:01:49,274 --> 00:01:51,564
아마 2의 마이너스 20,000승으로 말이죠.

36
00:01:51,564 --> 00:01:57,945
반대로 커브가 이렇게 위 방향으로 구부러지는 경우가 더 많을 것입니다.

37
00:01:57,945 --> 00:02:01,140
또는 이렇게 커브 함수가 아래로 구부러지는 경우도 발생할 것입니다.

38
00:02:01,140 --> 00:02:04,720
모든 함수가 위로 올라가는 것이 아니라요.

39
00:02:04,720 --> 00:02:07,430
그렇기 때문에 고차원의 공간에서는

40
00:02:07,430 --> 00:02:10,270
오른쪽에서 보이는 그림과 같이 안장점을 접할 확률이 매우 높습니다.

41
00:02:10,270 --> 00:02:13,575
국초 최적값 대신에 말이죠.

42
00:02:13,575 --> 00:02:16,305
이 부분이 안장점이라고 불리는 이유는

43
00:02:16,305 --> 00:02:17,545
모양을 상상해 보시면,

44
00:02:17,545 --> 00:02:21,060
아마 이부분이 말 위에 올리는 안장과 비슷하게 생겼죠. 맞죠?

45
00:02:21,060 --> 00:02:23,165
이 부분이 말이라고 볼 수 있겠네요.

46
00:02:23,165 --> 00:02:24,540
여기가 말의 머리부분입니다.

47
00:02:24,540 --> 00:02:28,390
여기는 눈이구요.

48
00:02:28,390 --> 00:02:33,235
말을 잘 그리진 않았는데요. 대략적인 아이디어는 이렇습니다.

49
00:02:33,235 --> 00:02:34,530
그럼 말 타는 사람은

50
00:02:34,530 --> 00:02:38,462
여기 안장에 앉겠죠.

51
00:02:38,462 --> 00:02:41,585
그렇기 때문에 여기 이 지점이,

52
00:02:41,585 --> 00:02:43,445
기울기가 0인 지점이,

53
00:02:43,445 --> 00:02:47,480
안장점이라고 불리는 것입니다.

54
00:02:47,480 --> 00:02:50,370
아마 안장에서 여기 이 지점이 바로 앉는 부분이라고 하겠죠.

55
00:02:50,370 --> 00:02:53,480
이 부분은 기울기가 0인 지점이기도 하구요.

56
00:02:53,480 --> 00:02:56,310
그렇게해서 오늘 강좌를 통해 저희는 딥러닝의 역사에 대해 배웠는데요

57
00:02:56,310 --> 00:02:59,790
저차원의 공간에 대한 내용을 많이 다루었습니다.

58
00:02:59,790 --> 00:03:01,235
여기 이 왼쪽부분에서 그릴 수 있는 것과 같이 말이죠.

59
00:03:01,235 --> 00:03:03,120
이 공간에는 고차원 공간에서 운영하는 알고리즘이

60
00:03:03,120 --> 00:03:07,695
이동하지 않습니다.

61
00:03:07,695 --> 00:03:10,860
20,000개의 매개 변수가 있기 때문에

62
00:03:10,860 --> 00:03:14,399
20,000 차원의 벡터에서 정의되는 J함수가 있고

63
00:03:14,399 --> 00:03:17,964
이 함수를 통해 국소 최적값을 보기보다는 안장점을 볼 확률이 더 높습니다.

64
00:03:17,964 --> 00:03:20,265
국소 최적값이 문제가 되지 않는다면,

65
00:03:20,265 --> 00:03:22,002
어떤 것이 문제가 될까요?

66
00:03:22,002 --> 00:03:26,155
plateau 가 러닝속도를 저하시킬 수 있는데요,

67
00:03:26,155 --> 00:03:31,635
plateau는 함수 기울기의 값이 0에 근접한 긴 범위를 이야기 합니다.

68
00:03:31,635 --> 00:03:33,915
이 지점에 있다고 하면

69
00:03:33,915 --> 00:03:38,230
기울기 강하가 표면을 따라 밑으로 이동할 것입니다.

70
00:03:38,230 --> 00:03:41,250
그러면 기울기가 0이거나 0에 근접한 값이기 때문에,

71
00:03:41,250 --> 00:03:42,829
표면이 이렇게 완만해 질 것입니다.

72
00:03:42,829 --> 00:03:45,300
시간이 굉장히 많이 소요될 수 있습니다.

73
00:03:45,300 --> 00:03:51,555
여기 정체구간에 도달하는데 말이죠

74
00:03:51,555 --> 00:03:53,820
그리고 왼쪽, 오른쪽 무작위의 이동을 통해서

75
00:03:53,820 --> 00:03:57,870
조금 더 잘 표시하기위해 펜 색깔을 바꿔보겠습니다.

76
00:03:57,870 --> 00:04:00,745
알고리즘이 그 이후, 정체구간인 plateau에서 빠져나오게 됩니다.

77
00:04:00,745 --> 00:04:04,620
이 길다란 슬로프를 따라 오면, 이 지점까지 오다가

78
00:04:04,620 --> 00:04:09,130
여기서 plateau 지점을 나옵니다.

79
00:04:09,130 --> 00:04:11,720
이 비디오에서 중요한 부분은 첫번째로,

80
00:04:11,720 --> 00:04:13,740
비교적 신경망이 꽤 큰 네트워크를 트레이닝 하는 이상,

81
00:04:13,740 --> 00:04:17,150
또한 파라미터가 많은 경우, 국소 최적값에

82
00:04:17,150 --> 00:04:18,555
갇힐 확률은 작습니다.

83
00:04:18,555 --> 00:04:23,185
그리고 J 비용함수는 비교적 고차원의 공간에서 정의됩니다.

84
00:04:23,185 --> 00:04:28,750
두번째로, plateau가 문제이긴 한데요, 러닝의 속도를 늦추게 하는 요소이기도 한데요,

85
00:04:28,750 --> 00:04:31,826
여기가 바로 momentum 도는 RmsProp 또는

86
00:04:31,826 --> 00:04:35,985
Adam 과 같은 알고리즘이 도움을 줄 수 있는 부분입니다.

87
00:04:35,985 --> 00:04:40,855
이런 plateau와 같은 시나리오 경우, 가장 정교한 observation 알고리즘인

88
00:04:40,855 --> 00:04:43,570
Adam과 같은 알고리즘이 plateau를 빠져나오는 속도를

89
00:04:43,570 --> 00:04:46,720
높혀줄 수 있습니다. plateau 구간을 지나서 완전히 빠져나오는데 도움을 준다는 것입니다.

90
00:04:46,720 --> 00:04:49,270
여러분의 네트워크가

91
00:04:49,270 --> 00:04:53,055
워낙 고차원의 공간에서 최적화 문제를 풀기 때문에

92
00:04:53,055 --> 00:04:57,445
제가 생각하기에 사람들은 이런 공간이 어떻게 생겼는지 아직 잘 모를 수 있습니다.

93
00:04:57,445 --> 00:04:59,910
또한 저희가 이런 공간을 이해하는 방식도 진화되고 있습니다.

94
00:04:59,910 --> 00:05:02,785
이번 강의가 여러분이 직관적으로 최적화 알고리즘에 대해

95
00:05:02,785 --> 00:05:06,660
이해하는데 도움이 되고 앞으로 직면하고 있는 과제를 이해하는데
도움이 되셨으면 좋겠습니다.

96
00:05:06,660 --> 00:05:11,100
이번주의 마지막 강의에 도달하신 것에 축하드립니다.

97
00:05:11,100 --> 00:05:15,275
이번주 퀴즈 학습을 보시길 바라구요, 또 연습학습도 진행하시기 바랍니다.

98
00:05:15,275 --> 00:05:18,310
여러분이 이번주 아이디어를 바탕으로 연습 학습을 잘 진행하길 바랍니다.

99
00:05:18,310 --> 00:05:23,000
다음주 비디오 강의에서 만나겠습니다.