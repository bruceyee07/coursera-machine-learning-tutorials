1
00:00:00,000 --> 00:00:01,710
在深度學習早期的時代

2
00:00:01,710 --> 00:00:04,380
大家常常擔心最佳化演算法

3
00:00:04,380 --> 00:00:07,415
會卡在很爛的局部極值 (local optima)

4
00:00:07,415 --> 00:00:09,660
不過隨著深度學習理論的進步

5
00:00:09,660 --> 00:00:13,285
我們對局部極值的理解也隨之更改

6
00:00:13,285 --> 00:00:16,855
就讓我來說明現今在深度學習，我們是

7
00:00:16,855 --> 00:00:21,279
怎麼看待局部極值，還有最佳化時碰到的問題

8
00:00:21,279 --> 00:00:25,695
以前大家擔心局部極值問題時，心裡想的是這種圖

9
00:00:25,695 --> 00:00:28,786
或許你想找到最佳的一組參數

10
00:00:28,786 --> 00:00:30,580
叫 w1, w2

11
00:00:30,580 --> 00:00:33,913
然後這曲面的高度是成本函數

12
00:00:33,913 --> 00:00:38,655
這張圖看起來有很多局部極小值散佈各地

13
00:00:38,655 --> 00:00:41,010
很容易就發生梯度下降法

14
00:00:41,010 --> 00:00:43,622
或其他演算法卡在某個局部極小值

15
00:00:43,622 --> 00:00:47,226
而找不到全域極值 (global optimum) 的情況

16
00:00:47,226 --> 00:00:51,945
其實呢，如果你嘗試在兩個維度的變數上畫圖

17
00:00:51,945 --> 00:00:56,637
很容易就會畫出有很多局部極值的圖

18
00:00:56,637 --> 00:01:00,285
我們的直覺曾經被這種低維度的圖所主導

19
00:01:00,285 --> 00:01:02,730
但這個直覺其實並不正確

20
00:01:02,730 --> 00:01:04,878
事實證明你在訓練神經網路的時候

21
00:01:04,878 --> 00:01:09,965
大部分梯度為0的點，並不是這樣的局部極值

22
00:01:09,965 --> 00:01:15,330
相反地，成本函數上大部分梯度為0的點是鞍點
(saddle points)

23
00:01:15,330 --> 00:01:17,845
例如這個點的梯度就是 0

24
00:01:17,845 --> 00:01:19,826
同樣這軸是 w1、

25
00:01:19,826 --> 00:01:25,150
w2、高是成本函數的值。

26
00:01:25,150 --> 00:01:28,523
不嚴謹來說，對於一個高維度空間的函數

27
00:01:28,523 --> 00:01:30,075
如果其梯度為零

28
00:01:30,075 --> 00:01:32,835
那麼從任一個方向看，他要嘛是

29
00:01:32,835 --> 00:01:36,810
凸函數 (convex)，要嘛是凹函數 (concave)

30
00:01:36,810 --> 00:01:38,660
假設你在

31
00:01:38,660 --> 00:01:40,785
一個 20000 維的空間

32
00:01:40,785 --> 00:01:42,510
一個點為了要是局部極值，

33
00:01:42,510 --> 00:01:45,795
所有 20000 個方向都要看起來像這樣

34
00:01:45,795 --> 00:01:49,274
所以這種發生機率可能非常小

35
00:01:49,274 --> 00:01:51,564
也許是 2 的 -20000 次方。

36
00:01:51,564 --> 00:01:57,945
所以相反地，你更可能看到某些方向像這樣往上彎

37
00:01:57,945 --> 00:02:01,140
也有些方向那個函數會

38
00:02:01,140 --> 00:02:04,720
像這樣往下彎，而不會所有方向都往上彎

39
00:02:04,720 --> 00:02:07,430
這就是為什麼在非常高維度的空間

40
00:02:07,430 --> 00:02:10,270
你通常會更有可能跑到一個鞍點，像右邊這樣，

41
00:02:10,270 --> 00:02:13,575
而不會是局部極值。

42
00:02:13,575 --> 00:02:16,305
至於為什麼這叫鞍點 (saddle point)

43
00:02:16,305 --> 00:02:17,545
你可以想像一下

44
00:02:17,545 --> 00:02:21,060
這看起來像是放在馬上的馬鞍對吧

45
00:02:21,060 --> 00:02:23,165
那...也許這是匹馬

46
00:02:23,165 --> 00:02:24,540
這個是馬頭

47
00:02:24,540 --> 00:02:28,390
有隻眼睛

48
00:02:28,390 --> 00:02:33,235
嗯...畫得不是很好不過你知道的啦

49
00:02:33,235 --> 00:02:34,530
然後你是個騎師

50
00:02:34,530 --> 00:02:38,462
坐在馬鞍這邊

51
00:02:38,462 --> 00:02:41,585
這，就是這個點

52
00:02:41,585 --> 00:02:43,445
導數為零的地方

53
00:02:43,445 --> 00:02:47,480
這個點叫「鞍點」的原因

54
00:02:47,480 --> 00:02:50,370
我猜在馬鞍上，你真的會坐在這個點上面

55
00:02:50,370 --> 00:02:53,480
而那個地方剛好導數為0。

56
00:02:53,480 --> 00:02:56,310
所以，在深度學習歷史上學到的 

57
00:02:56,310 --> 00:02:59,790
其中一個教訓是，很多我們在低維度空間的直覺

58
00:02:59,790 --> 00:03:01,235
例如我們左邊畫的

59
00:03:01,235 --> 00:03:03,120
他們不見得能套用到

60
00:03:03,120 --> 00:03:07,695
非常高維度的空間，也就是學習演算法運算的地方

61
00:03:07,695 --> 00:03:10,860
因為如果你有 20000 個參數

62
00:03:10,860 --> 00:03:14,399
那麼 J 就是一個 20000維向量的函數

63
00:03:14,399 --> 00:03:17,964
那你看到的絕大部分會是鞍點，不是局部最佳值。

64
00:03:17,964 --> 00:03:20,265
如果局部極值不是問題

65
00:03:20,265 --> 00:03:22,002
那問題在哪呢？

66
00:03:22,002 --> 00:03:26,155
結果發現 "plateau" (平頂/高原) 會減緩學習速度

67
00:03:26,155 --> 00:03:31,635
plateau 是一種區域，在這區域的導數很長一段都接近零

68
00:03:31,635 --> 00:03:33,915
所以假設你在這

69
00:03:33,915 --> 00:03:38,230
那梯度下降法會沿著表面往下走

70
00:03:38,230 --> 00:03:41,250
而因為梯度為 0，或很接近 0

71
00:03:41,250 --> 00:03:42,829
這表面很平

72
00:03:42,829 --> 00:03:45,300
所以你會花很長的一段時間

73
00:03:45,300 --> 00:03:51,555
在 plateau 上緩慢地往這個點前進

74
00:03:51,555 --> 00:03:53,820
然後因為隨機的左右微擾

75
00:03:53,820 --> 00:03:57,870
所以最後（我換個筆的顏色比較清楚）

76
00:03:57,870 --> 00:04:00,745
你的演算法找到了離開 plateau 的路線

77
00:04:00,745 --> 00:04:04,620
他在這超級長的緩坡往下走，後來

78
00:04:04,620 --> 00:04:09,130
才找到這邊，然後才能離開這個 plateau。

79
00:04:09,130 --> 00:04:11,720
因此這影片的重點在：第一

80
00:04:11,720 --> 00:04:13,740
你其實不大可能卡在

81
00:04:13,740 --> 00:04:17,150
某個很爛的局部極值 — 只要你訓練的神經網路夠大

82
00:04:17,150 --> 00:04:18,555
也就是有很多參數

83
00:04:18,555 --> 00:04:23,185
而成本函數 J 是定義在比較高維的空間上

84
00:04:23,185 --> 00:04:28,750
然而第二，"plateau" 會是一個問題；
他會讓學習過程非常緩慢

85
00:04:28,750 --> 00:04:31,826
而這就是像動量法或 RMSprop 或

86
00:04:31,826 --> 00:04:35,985
Adam 這種演算法發揮威力、幫助學習之處

87
00:04:35,985 --> 00:04:40,855
在這種情況下，比較精密的最佳化演算法，
例如 Adam，

88
00:04:40,855 --> 00:04:43,570
能真的幫你加速

89
00:04:43,570 --> 00:04:46,720
在 plateau 裡往下跑然後離開。

90
00:04:46,720 --> 00:04:49,270
那麼，因為神經網路解決的

91
00:04:49,270 --> 00:04:53,055
是非常高維空間的優化問題，所以說實話

92
00:04:53,055 --> 00:04:57,445
我覺得沒有人能很直觀地知道
這種空間實際上長怎樣

93
00:04:57,445 --> 00:04:59,910
我們對它們的理解還在不斷發展。

94
00:04:59,910 --> 00:05:02,785
不過，我希望能給你更好的概念

95
00:05:02,785 --> 00:05:06,660
有關最佳化演算法會面臨的挑戰。

96
00:05:06,660 --> 00:05:11,100
那麼，恭喜你，已經來到本週最後的內容了

97
00:05:11,100 --> 00:05:15,275
請看一下這星期的測驗，還有程式練習

98
00:05:15,275 --> 00:05:18,310
我希望你會喜歡在程式作業中練習這些概念

99
00:05:18,310 --> 00:05:23,000
也很期待在下星期的影片再看到您