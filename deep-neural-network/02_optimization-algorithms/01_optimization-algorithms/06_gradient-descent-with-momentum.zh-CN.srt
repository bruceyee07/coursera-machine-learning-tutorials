1
00:00:00,390 --> 00:00:04,770
有一种算法叫做动量(Momentum)
或者叫动量梯度下降算法

2
00:00:04,770 --> 00:00:09,600
它几乎总会比标准的梯度下降算法更快

3
00:00:09,600 --> 00:00:14,100
一言以蔽之 算法的主要思想是
计算梯度的指数加权平均

4
00:00:14,100 --> 00:00:18,850
然后使用这个梯度来更新权重

5
00:00:18,850 --> 00:00:22,010
在本视频中 我们会详细解释这句话

6
00:00:22,010 --> 00:00:23,848
并教你如何实现它

7
00:00:23,848 --> 00:00:28,619
举例来说 假设你想要优化一个代价函数

8
00:00:28,619 --> 00:00:30,510
如等高线图所示

9
00:00:30,510 --> 00:00:34,350
红色的点表示最小值的位置

10
00:00:34,350 --> 00:00:39,307
假设从这里开始执行梯度下降<br />经过梯度下降的一次迭代后

11
00:00:39,307 --> 00:00:44,670
无论是批量或小批量下降 结果可能会朝向这里

12
00:00:44,670 --> 00:00:47,370
现在到了椭圆的另一边

13
00:00:47,370 --> 00:00:51,810
如果你再进行一步梯度下降 可能就到了这里

14
00:00:51,810 --> 00:00:55,590
一步又一步 以此类推

15
00:00:55,590 --> 00:01:00,460
你会发现梯度下降算法会计算很多步

16
00:01:00,460 --> 00:01:07,190
向着最小值缓慢地振荡前进

17
00:01:07,190 --> 00:01:11,206
这种上下的振荡会减慢梯度下降的速度

18
00:01:11,206 --> 00:01:14,500
同时也让你无法使用较大的学习率

19
00:01:14,500 --> 00:01:19,226
如果你使用的学习率很大 可能会超调

20
00:01:19,226 --> 00:01:21,533
像这样发散出去

21
00:01:21,533 --> 00:01:25,826
因此为了避免振荡过大

22
00:01:25,826 --> 00:01:29,650
你只能使用比较小的学习率

23
00:01:29,650 --> 00:01:34,120
换一个角度来看这个问题 在纵轴上

24
00:01:34,120 --> 00:01:38,990
你希望学习慢一点 因为你不希望有这些振荡

25
00:01:38,990 --> 00:01:43,701
但是在横轴上 你希望加快学习速度

26
00:01:45,552 --> 00:01:48,831
因为你希望快速地从左向右

27
00:01:48,831 --> 00:01:51,910
朝最小值移动 也就是那个红点

28
00:01:51,910 --> 00:01:55,621
下面是实现动量梯度下降的步骤

29
00:01:58,542 --> 00:02:03,611
对每一次迭代 具体来说 在第t次迭代中

30
00:02:03,611 --> 00:02:11,562
你需要计算导数dW db

31
00:02:11,562 --> 00:02:15,834
这里我省略了上角标[l]

32
00:02:15,834 --> 00:02:19,940
计算当前的小批量的dW db

33
00:02:19,940 --> 00:02:21,550
如果你使用的是批量梯度下降算法

34
00:02:21,550 --> 00:02:24,200
则当前的小批量就是你的整个批量

35
00:02:24,200 --> 00:02:26,670
这同样适用于批量梯度下降

36
00:02:26,670 --> 00:02:29,580
如果当前的小批量是整个训练集

37
00:02:29,580 --> 00:02:31,560
也同样适用

38
00:02:31,560 --> 00:02:36,453
然后要做的是计算

39
00:02:36,453 --> 00:02:41,346
v_dW=beta*v_dW+(1-beta)*dW

40
00:02:41,346 --> 00:02:45,779
v_dW=beta*v_dW+(1-beta)*dW

41
00:02:45,779 --> 00:02:50,808
这和我们之前计算过的

42
00:02:50,808 --> 00:02:55,960
v_theta=beta*v_theta+(1-beta)*theta_t 很像

43
00:02:57,130 --> 00:03:02,453
也就是计算W导数的滑动平均值

44
00:03:02,453 --> 00:03:07,754
然后对v_db做同样的计算

45
00:03:07,754 --> 00:03:13,980
v_db=beta*v_db+(1-beta)*db

46
00:03:13,980 --> 00:03:18,810
然后更新权重 新的权重是

47
00:03:18,810 --> 00:03:23,850
W-alpha*v_dW

48
00:03:23,850 --> 00:03:28,240
使用v_dW更新权重 而不是dW

49
00:03:28,240 --> 00:03:35,630
同样地 b=b-alpha*v_db

50
00:03:35,630 --> 00:03:39,570
这样做可以让梯度下降的每一步变得平滑

51
00:03:41,230 --> 00:03:45,760
举例来说 假设你计算的前几个倒数分别是

52
00:03:45,760 --> 00:03:47,298
这样 这样 这样 这样

53
00:03:48,330 --> 00:03:52,354
如果把这些梯度平均一下 你会发现这些震荡

54
00:03:52,354 --> 00:03:55,472
在纵轴上的平均值趋近于0

55
00:03:55,472 --> 00:04:00,301
所以 在垂直方向上 你会希望减慢速度

56
00:04:00,301 --> 00:04:05,390
正数和负数在计算平均时相互抵消了 平均值接近于0

57
00:04:05,390 --> 00:04:07,740
然而在水平方向上

58
00:04:07,740 --> 00:04:11,160
所有导数都指向水平方向的右边

59
00:04:11,160 --> 00:04:14,340
所以水平方向的平均值仍然较大

60
00:04:14,340 --> 00:04:18,200
因此在数次迭代之后

61
00:04:18,200 --> 00:04:22,930
你会发现动量梯度下降算法的每一步

62
00:04:22,930 --> 00:04:28,100
在垂直方向上的振荡非常小

63
00:04:28,100 --> 00:04:33,700
且在水平方向上运动得更快

64
00:04:33,700 --> 00:04:37,480
这会让你的算法选择更加直接的路径

65
00:04:37,480 --> 00:04:42,990
或者说减弱了前往最小值的路径上的振荡

66
00:04:42,990 --> 00:04:47,304
对动量有这样一种直观的解释 可能有助于部分人理解

67
00:04:47,304 --> 00:04:53,040
就是如果你想最小化一个碗型函数

68
00:04:53,040 --> 00:04:55,440
这是一个碗的等高线图

69
00:04:55,440 --> 00:04:57,840
我可能不太擅长画画

70
00:04:57,840 --> 00:05:02,470
在最小化这个碗型函数时

71
00:05:02,470 --> 00:05:06,625
你可以把这两个导数项看成

72
00:05:06,625 --> 00:05:11,071
一个球滚下坡时的加速度

73
00:05:11,071 --> 00:05:19,151
把这些动量项看成是球的速度

74
00:05:20,812 --> 00:05:24,749
所以想象一下 你有一个碗和一个球

75
00:05:24,749 --> 00:05:28,854
导数项给球了一个加速度

76
00:05:28,854 --> 00:05:32,440
然后球就向下滚

77
00:05:32,440 --> 00:05:36,980
因为有加速度 所以它滚得越来越快

78
00:05:36,980 --> 00:05:42,390
因为beta是一个略小于1的数

79
00:05:42,390 --> 00:05:46,690
可以把它看作摩擦力 让球不至于无限加速下去

80
00:05:46,690 --> 00:05:50,380
与梯度下降中

81
00:05:50,380 --> 00:05:54,120
每一步都独立于之前步骤所不同的是

82
00:05:54,120 --> 00:05:56,460
现在你的球可以向下滚并获得动量

83
00:05:56,460 --> 00:06:01,610
沿碗向下加速并获得动量

84
00:06:01,610 --> 00:06:05,640
我觉得这个球沿碗滚下的比喻

85
00:06:05,640 --> 00:06:07,770
对那些喜欢物理的人比较容易理解

86
00:06:07,770 --> 00:06:12,160
但并不是每个人都能接受 这个球沿碗滚下的比喻

87
00:06:12,160 --> 00:06:15,000
如果你不能理解 也不用在意

88
00:06:15,000 --> 00:06:18,280
最后 让我们进一步学习如何实现它

89
00:06:18,280 --> 00:06:21,300
算法如下 现在你有2个超参数

90
00:06:22,300 --> 00:06:27,100
学习率alpha和这个参数beta

91
00:06:27,100 --> 00:06:30,080
beta控制指数加权平均

92
00:06:30,080 --> 00:06:33,073
beta最常用的取值是0.9

93
00:06:33,073 --> 00:06:35,730
就像我们之前计算最近10天气温的平均值

94
00:06:35,730 --> 00:06:39,930
这里就是计算前10次迭代的梯度的平均值

95
00:06:39,930 --> 00:06:42,768
在实践中 使用beta=0.9效果很好

96
00:06:42,768 --> 00:06:45,420
你也可以尝试不同的值

97
00:06:45,420 --> 00:06:50,120
做一些超参数搜索 但是0.9是非常稳健的参数值

98
00:06:50,120 --> 00:06:51,932
至于偏差修正

99
00:06:51,932 --> 00:06:58,170
是否需要让v_dW或v_db除以1-beta^t呢

100
00:06:58,170 --> 00:07:02,380
实际上 通常人们不会这么做 因为在10次迭代之后

101
00:07:02,380 --> 00:07:06,530
你的滑动平均值已经就绪 不再是一个偏差估计

102
00:07:06,530 --> 00:07:11,357
所以实际上 在实现梯度下降或动量梯度下降时

103
00:07:11,357 --> 00:07:14,663
我没见过有人会做偏差修正

104
00:07:14,663 --> 00:07:18,785
当然这里需要把v_dW初始化为0

105
00:07:18,785 --> 00:07:23,546
注意这是一个零矩阵 维数和dW相同

106
00:07:23,546 --> 00:07:26,810
也就是和W具有相同的维数

107
00:07:26,810 --> 00:07:30,620
v_db也要初始化为一个零向量

108
00:07:30,620 --> 00:07:35,400
维数和db相同 也就是和b有相同的维数

109
00:07:35,400 --> 00:07:40,050
最后我想提一点 当你在阅读关于

110
00:07:40,050 --> 00:07:45,590
动量梯度下降的文献时会发现

111
00:07:45,590 --> 00:07:48,854
(1-beta)这一项常常被省略

112
00:07:48,854 --> 00:07:57,080
这样会得到 v_dW=beta*v_dW+dW

113
00:07:57,080 --> 00:08:02,127
使用紫色版本的结果是

114
00:08:02,127 --> 00:08:07,300
v_dW被缩小了(1-beta)倍 相当于乘以1/(1-beta)

115
00:08:07,300 --> 00:08:11,230
所以在进行梯度下降的更新时

116
00:08:11,230 --> 00:08:16,220
alpha也要根据1/(1-beta)进行调整

117
00:08:16,220 --> 00:08:18,800
实际上 这两种写法都可以

118
00:08:18,800 --> 00:08:23,740
只会影响学习率alpha的最佳值

119
00:08:23,740 --> 00:08:28,350
但是我觉得这个公式不够直观

120
00:08:28,350 --> 00:08:33,610
其中一个影响是 如果你调整了超参数beta的值

121
00:08:33,610 --> 00:08:37,770
就会同时影响v_dW和v_db的缩放

122
00:08:37,770 --> 00:08:42,710
然后你可能又需要重新调整学习率alpha

123
00:08:42,710 --> 00:08:46,970
所以我个人更倾向于使用写在左侧的这些公式

124
00:08:46,970 --> 00:08:49,600
而不是省去(1-beta)项

125
00:08:49,600 --> 00:08:52,450
我倾向于使用左侧的公式

126
00:08:52,450 --> 00:08:55,140
即有(1-beta)项的公式

127
00:08:55,140 --> 00:09:00,280
不过对于这两种版本 将beta设置为0.9都是普遍的选择

128
00:09:00,280 --> 00:09:03,500
只是这两个版本对学习率alpha的调整会

129
00:09:03,500 --> 00:09:04,880
有所不同

130
00:09:04,880 --> 00:09:07,500
以上就是动量梯度下降算法

131
00:09:07,500 --> 00:09:11,120
它几乎总是优于

132
00:09:11,120 --> 00:09:13,740
不使用动量的梯度下降算法

133
00:09:13,740 --> 00:09:17,020
还有一些方法也可以加速你的学习算法

134
00:09:17,020 --> 00:09:19,920
我们会在接下来的几个视频中进行讨论