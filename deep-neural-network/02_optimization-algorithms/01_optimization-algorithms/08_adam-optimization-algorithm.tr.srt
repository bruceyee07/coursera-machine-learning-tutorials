1
00:00:00,000 --> 00:00:02,340
Derin öğrenme tarihi süresince

2
00:00:02,340 --> 00:00:05,700
çok iyi bilinen bazır araştırmacılarında içinde bulunduğu birçok araştırmacı

3
00:00:05,700 --> 00:00:07,790
bazen  optimizasyon algoritmalarını önerdi

4
00:00:07,790 --> 00:00:09,825
ve birkaç problemde iyi çalışığını gösterdi.

5
00:00:09,825 --> 00:00:13,440
Ancak bu optimizasyon algoritmaları daha sonra  eğitmek isteyebileceğiniz

6
00:00:13,440 --> 00:00:18,130
geniş yelpazede ki yapay sinir ağları için çok iyi bir genelleme yapamadığını gösterdi.

7
00:00:18,130 --> 00:00:21,360
Zamanla derin öğrenme topluluğunun yeni algoritmalara karşı aslında

8
00:00:21,360 --> 00:00:25,597
bir miktar kuşku geliştirdiğini düşünüyorum.

9
00:00:25,597 --> 00:00:29,350
Ve birçok insan (alçalan eğimin) momentum ile birlikte çok iyi çalıştığını düşünmektedir.

10
00:00:29,350 --> 00:00:32,720
Daha iyi çalışan şeyler teklif etmekte zor.

11
00:00:32,720 --> 00:00:36,070
Yani, RmsProp ve Adam  optimizasyon algoritması 

12
00:00:36,070 --> 00:00:37,730
bu videoda konuşacaklarımız.

13
00:00:37,730 --> 00:00:41,460
gerçekten karşı koyabilmiş olan nadir algoritmalardan biri

14
00:00:41,460 --> 00:00:47,250
ve geniş yapay sinir ağ mimarilerinde iyi çalıştığı gösterildi.

15
00:00:47,250 --> 00:00:50,150
Size tereddüt etmeden denemenizi önerdiğim algortimalardan biridir

16
00:00:50,150 --> 00:00:54,625
çünkü birçok kişi bunu denedi ve farklı problemleri çok iyi şekilde çözdüğünü gördü

17
00:00:54,625 --> 00:00:57,720
ve Adam optimizasyon algoritması temel olarak

18
00:00:57,720 --> 00:01:01,250
momentumu ve Rmsprop'u  alır ve bunları bir araya getirir

19
00:01:01,250 --> 00:01:03,105
Hadi nasıl çalıştığını görelim.

20
00:01:03,105 --> 00:01:05,695
Adam'ı uygulamak için şunları başlatırsın :

21
00:01:05,695 --> 00:01:15,877
Vdw=0, Sdw=0 ve benzer olarak Vdb, Sdb=0.

22
00:01:15,877 --> 00:01:19,810
ve sonra T üzerinde yinelenmede ,

23
00:01:19,810 --> 00:01:30,170
türevleri hesaplarsınız : mevcut mini-batch kullanarak dw ve db hesaplayın.

24
00:01:30,170 --> 00:01:33,775
Yani genellikle, bunu mini-batch gradyan alçalması ile yaparsınız.

25
00:01:33,775 --> 00:01:41,480
Ve sonra momentumun üstel ağırlıklı ortalamasını alırsınız. Yani Vdw = ß.

26
00:01:41,480 --> 00:01:46,410
Ama şimdi ß1'e  gidiyorum onu hiper parametresinden ayırt etmek için  

27
00:01:46,410 --> 00:01:52,660
ß2'yi bunun rmsprop oranı için kullanacağız.

28
00:01:52,660 --> 00:01:58,180
Yani, bu tam olarak uygulamada sahip olduğumuz

29
00:01:58,180 --> 00:02:03,788
momentumun ß yerine hiper parametresi ß1 olarak adlandırıldığı şeydir.

30
00:02:03,788 --> 00:02:14,312
ve benzer şekilde , VBD aşağıdaki gibidir : ( 1 - ß1 )x db.

31
00:02:14,312 --> 00:02:18,685
Ve sonra Rmsprop güncellemesini de yapın.

32
00:02:18,685 --> 00:02:26,630
Şimdi, farklı bir hiper parametreye sahipsiniz : ß2 + (1 - ß2 )dw²

33
00:02:26,630 --> 00:02:33,325
Ve yeniden, kareleştirme içinde senin dw türev ürünlerinin karesini alan y unsuru bulunmakta.

34
00:02:33,325 --> 00:02:44,005
Ve Sdb = (1 - ß2 ) çarpı db'dir.

35
00:02:44,005 --> 00:02:49,145
Şimdi bu momentum hiper parametre ile güncellenen ß1 gibidir,

36
00:02:49,145 --> 00:02:55,318
ve Rmsprop ise hiper parametre ile güncellenen  ß2. gibidir.

37
00:02:55,318 --> 00:02:58,599
Adam'ın tipik uygulamasında

38
00:02:58,599 --> 00:03:01,255
önyargı düzeltmesini uygularsınız.

39
00:03:01,255 --> 00:03:04,215
Yani, düzeltilmiş V'ye sahip olacaksınız.

40
00:03:04,215 --> 00:03:06,705
Düzeltilme anlamı önryargı düzeltmesinden sonra demektir.

41
00:03:06,705 --> 00:03:16,244
Eğer T yinelemelerini tamamladıysanız , Dw= Vdw /(1 - ß1 üzeri t) dür.

42
00:03:16,244 --> 00:03:25,040
Ve benzer şekilde , Vdb (düzeltilmiş) = Vdb / (1 - (  ß1 üzeri t ) )

43
00:03:25,040 --> 00:03:30,756
Ve sonra benzer şekilde önyargı düzeltmesini S üzerinde uygularsınız.

44
00:03:30,756 --> 00:03:37,405
Yani, Sdw = Sdw / ( 1 - ( ß2 üzeri t ) ) ve

45
00:03:37,405 --> 00:03:48,700
Sdb (düzeltilmiş) = Sdb / ( 1 - ( ß2 üzeri t ) ) 

46
00:03:48,700 --> 00:03:50,660
Son olarak güncellemeyi gerçekleştirebilirsiniz.

47
00:03:50,660 --> 00:03:55,060
Yani W = W - alfa olarak güncellenir

48
00:03:55,060 --> 00:03:59,870
Eğer sadece momentumu uyguluyorsan vdw kullanacaksın

49
00:03:59,870 --> 00:04:03,408
vw veya belki düzeltildi.

50
00:04:03,408 --> 00:04:06,615
Ama şimdi, biz bu Rmsprop kısmını ekliyoruz

51
00:04:06,615 --> 00:04:13,390
Bu yüzden Sdw (düzeltilmiş artı) epsilon'un karekökleri ile de bölünecek.

52
00:04:13,390 --> 00:04:18,232
Ve benzer olarak, B benzer bir formül olarak güncellenir.

53
00:04:18,232 --> 00:04:24,070
Vdb (düzeltilmiş) , düzeltilmiş Sdb + Epsilonun

54
00:04:24,070 --> 00:04:28,595
karaköküne bölünür .

55
00:04:28,595 --> 00:04:33,070
Ve böylece, bu algoritma gradyan alçalma etkisini birleştirir.

56
00:04:33,070 --> 00:04:37,572
gradyan alçalma , momentum ve rmsprop ile birlikte

57
00:04:37,572 --> 00:04:41,740
Ve bu yaygın olarak kullanılan kanıtlanmış öğrenme algoritmasıdır 

58
00:04:41,740 --> 00:04:46,640
çok çeşitli farklı yapay sinir ağ mimarileri için etkilidir.

59
00:04:46,640 --> 00:04:49,805
Yani ,bu algoritma bir dizi hiper parametreye sahiptir.

60
00:04:49,805 --> 00:04:57,330
Hiper parametre alpha ile öğrenme hala önemli ve genellikle ayarlanması gerekiyor.

61
00:04:57,330 --> 00:05:01,675
Yani bir dizi denemek ve nasıl çalıştığını görmek zorundasınız.

62
00:05:01,675 --> 00:05:06,090
ß1 için varsayılan yaygın seçim gerçek olarak 0.9 dur.

63
00:05:06,090 --> 00:05:08,065
Bu bir hareketli ortalama,

64
00:05:08,065 --> 00:05:12,220
Dw hareketli ortalaması bir hafif momentum terimidir.

65
00:05:12,220 --> 00:05:15,455
ß2 için hiper parametre,

66
00:05:15,455 --> 00:05:16,950
Adam raporunun yazarları,

67
00:05:16,950 --> 00:05:20,014
Adam algoritmasının mucitleri 0.999 önerir

68
00:05:20,014 --> 00:05:26,485
Yine bu dw2 hareketli ortalamasının yanı sıra db karelerinin hesaplanmasıdır.

69
00:05:26,485 --> 00:05:31,030
Ve Epsilon, seçimi çok önemli değil.

70
00:05:31,030 --> 00:05:34,755
Fakat Adam raponun yazarları 10 üzeri eksi 8 önerirler.

71
00:05:34,755 --> 00:05:38,230
Fakat bu parametrede sen gerçekten ihtiyacın yok

72
00:05:38,230 --> 00:05:42,555
bunu ayarlamana ve bu performansı hiç etkilemez.

73
00:05:42,555 --> 00:05:44,280
Fakat Adam uyguladığın zaman

74
00:05:44,280 --> 00:05:47,030
insanların genellikle yaptığı sadece varsayılanı kullanmak.

75
00:05:47,030 --> 00:05:49,960
Yani epsilon yanı sıra ß1 ve ß2

76
00:05:49,960 --> 00:05:52,300
Kimsenin gerçekten Epsilon'u seçtiğini sanmıyorum.

77
00:05:52,300 --> 00:05:56,335
Ve sonra, en iyisi için bir dizi Alpha değer, belirleyin.

78
00:05:56,335 --> 00:05:59,140
Ayrıca  ß1 ve ß2 ayarlayabilirsin fakat bu

79
00:05:59,140 --> 00:06:02,440
sıklıkla bildiğim uygulayıcılar arasında değil.

80
00:06:02,440 --> 00:06:06,100
Peki, Adam terimi nereden geliyor ?

81
00:06:06,100 --> 00:06:15,267
Adam uyarlamalı moment tahminini temsil eder.

82
00:06:15,267 --> 00:06:18,175
ß1 türevlerin ortalamalarını hesaplıyor.

83
00:06:18,175 --> 00:06:19,780
Buna ilk an denilir.

84
00:06:19,780 --> 00:06:21,975
 ß2 ise hesaplamaları için

85
00:06:21,975 --> 00:06:25,830
²s lerin üssel ortalamalarını alır ve buna ikinci an denir.

86
00:06:25,830 --> 00:06:29,380
Ve bu  uyarlanabilir moment tahminine yol açar.

87
00:06:29,380 --> 00:06:32,875
Fakat herkes bunu Adam yetkilendirme algoritması olarak adlandırır.

88
00:06:32,875 --> 00:06:37,800
Ve bu arada, eski arkadaşlarımdan ve işbirlikçilerimden biri Adam Coates'i çağrıştırıyor.

89
00:06:37,800 --> 00:06:40,425
Bildiğim kadarıyla , bu algoritmanın onunla hiçbir bağlantısı yok,

90
00:06:40,425 --> 00:06:43,525
Bazen onu kullandığını düşündüğüm gerçeğinin dışında.

91
00:06:43,525 --> 00:06:45,847
Fakat bazen o soruyu sorarım,

92
00:06:45,847 --> 00:06:47,945
sadece merak ettiğiniz noktada.

93
00:06:47,945 --> 00:06:51,187
 Yani, Adam optimizasyon algoritması bu kadar.

94
00:06:51,187 --> 00:06:54,435
Bununla birlikte, sinir ağlarınızı çok daha hızlı bir şekilde eğitebileceksiniz.

95
00:06:54,435 --> 00:06:56,055
fakat bu haftaya başlamadan önce

96
00:06:56,055 --> 00:06:58,950
hiper parametresi ayarlaması hakkında konuşmaya devam edelim

97
00:06:58,950 --> 00:07:01,465
hem de ne olduğu hakkında biraz sezgi elde edelim :

98
00:07:01,465 --> 00:07:04,230
sinir ağları için optimizasyon problemleri nasıl görünüyor.

99
00:07:04,230 --> 00:07:07,260
Bir sonraki videoda, öğrenme hızı azalması hakkında konuşacağız.