Merhaba, tekrar hoş geldiniz. Bu hafta, eniyileme algoritmalarını öğreneceksiniz, bu algoritmalar sinir ağlarınızı çok daha hızlı eğitmenizi sağlayacak Daha önce söylemiş olduğum üzere makine öğrenmesi ziyadesiyle deneysel bir işlemdir, ziyadesiyle yinelemeli bir işlemdir. Bu süreçte en iyi sonucu verecek modeli bulmak için çok fazla model eğitmeniz gerekir. Bu sebeple, modelleri hızlı bir şekilde eğitmek gerçekten çok yardımcı olur. Eğitimi zorlaştıran sebeplerden biri de şudur; Derin Öğrenme, en iyi sonucu büyük veri rejimlerinde vermeye meyillidir. Sinir ağlarını, devasa bir veri kümesi üzerinde eğitebiliyoruz, ve geniş bir veri kümesi üzerinde çalışmak oldukça yavaştır. Yani anlayacağınız şu ki, hızlı eniyileme algoritmalarına sahip olmak, doğru eniyileme algoritmalarına sahip olmak gerçekten, sizin ve ekibinizin verimine hız kazandırabilir. Hadi o zaman "mini-batch gradient descent" hakkında konuşmaya başlayalım. Daha önce öğrendiğiniz üzere, vektörleştirmenin sağladığı şey m sayıdaki bütün örnekler üzerinde etkin bir hesap yapmanızdır. bu durum bütün eğitim kümesi üzerinde belirli bir formül olmadan işlem yapmanızı sağlar. Bu sebeple bütün eğitim örneklerimizi alacağız ve birleştirip, büyük harfle X adındaki büyük matriksimizin içine ekleyeceğiz. x(1) , x(2) , x(3) ... ve nihayetinde x(m)'e kadar gider, m sayıdaki eğitim örneği. ve benzer şekilde Y için y(1), y(2), y(3) ve aynı şekilde y(m)'e kadar. Dolayısıyla, X'in boyutları, nx'e m ve bu da 1'e m olur... Vektörleşme bütün m örneklerini, hızlıca, göreceli olarak hızlıca işlemenizi sağlar, ama eğer m çok büyükse, bu durumda yine de yavaş olabilir. Örnek olarak, eğer m 5 milyon veya 50 milyon hatta daha da büyük olsa ne olur? Gradyan inişinin tüm eğitim setinize uygulanması ile yapmanız gereken gradyan inişinin bir küçük adımını atmadan önce tüm eğitim setinizi işlemektir. Ve sonra, gradyan inişinde bir küçük adım daha atmadan önce beş milyon eğitim örneğinden oluşan tüm eğitim setinizi tekrar işlemeniz gerekmektedir. Yani ortaya çıkıyor ki, eğer 5 milyon örneklik dev eğitim setinizin, tüm setinizin işlemeyi bitirmeden önce gradyan inişinin bir miktar ilerlemesine müsaade ederseniz daha hızlı bir algoritma elde edebilirsiniz. Özellikle yapabileceğiniz şudur. Diyelim ki, eğitim setiniz daha küçük, küçük bebek eğitim setlerine böldünüz ve bu bebek eğitim setlerine de mini-toptanlar deniliyor. Ve diyelim ki her bir bebek eğitim setinizde sadece 1.000 örnek var. Yani, X1den X1.000 e kadar alıyorsunuz ve bunu ilk bebek eğitim seti olarak adlandırıyorsunuz ve aynı zamanda mini-toptan olarak adlandırıyorsunuz. Ve sonra bir sonraki 1.000 örneği eve getiriyorsunuz. X1.001den X2.000e ve sonra X1.000 örnek ve bir sonraki ve böyle. Yeni bir notasyon tanıtacam ve bunu X üzeri süslü parantezde 1 olarak adlandıracağım ve bunu X üzeri süslü parantezde 2. Şimdi, eğer toplamda 5 milyon eğitim örneği varsa ve bu mini toptanların her birinde bin örnek varsa, bu bunlardan 5.000 tane olduğu anlamına gelir çünkü biliyorsunuz 5.000 tane 1.000 5 milyona eşittir. Totalde bu mini toptanlardan 5.000 tane olur. Yani, X üzeri süslü parantezde 5.000 ile sonlanır ve sonra benzer şekilde aynısını Y için de yapıyorsunuz. Eğitim verinizi bu doğrultuda Y için de böleceksiniz. Yani, bunu Y1 olarak adlandırın ve sonra bu Y1.001 den Y2.000e. Bu Y2 olarak adlandırılır ve böyle Y5.000iniz oluncaya kadar devam. Şimdi, T sayılı mini toptanınız X, T ve Y,Tden oluşur. Ve bu karşılık gelen girdi ve çıktı çiftleri ile bin tane eğitim örneğidir. Devam etmeden önce, sırf notasyonumun anlaşılır olduğundan emin olmak için, daha eğitim setinde dizin için üste süslü parantezde I kullanmıştık yani X I I eğitim örneğidir. Sinir ağının farklı katmanlarının dizini için üste köşeli parantezde L kullanıyoruz. Yani, ZL sinir ağındaki L katmanının Z değerinden geliyor ve burada farklı mini toptanları dizinlemek için süslü parantezi tanıtıyoruz. Yani, XT ve YT var ve bunları anladığınızı kontrol etmek için, XT ve YTnin boyutu nedir? Pekala, X Mye Mdir. Yani, eğer X1 bin eğitim örneği ise veya X in eğitim örneği değerindeyse, o zaman bu boyut MXe 1.000 olmalı ve X2 Xe 1.000 olmalı ve bunun gibi devam eder. Yani, tüm bunların MXe 1.000lik boyutu olmalı ve bunların boyutu 1e 1.000 olmalı. Bu algoritmanın adını açıklamak için, toptan gradyan iniş, önceden konuştuğumuz tüm eğitim setini aynı anda işlediğiniz gradyan iniş algoritmasına söylenmektedir. Ve ismi tüm eğitim örneği toptanınızın aynı anda işlenmesinden gelmektedir. Bunun harika bir isim olmadığını biliyorum ancak bu şekilde isimlendiriliyor. Buna karşın mini toptan gradyan inişi, bir sonraki slaytta bahsedeceğimiz ve tüm XY eğitim setini aynı anda işlemek yerine XT ve YT mini toptanlarını işleyeceğiniz algoritmayı ifade etmektedir. Şimdi, haydi mini-toptan gradyan inişin nasıl çalıştığına bakalım. Mini-toptan gradyan inişi eğitim setlerinizde çalıştırmak için, T eşittir 1den 5.000e kadar çalıştırıyorsunuz çünkü her biri 1.000 yüksekliğinde olan 5.000 tane mini-toptanımız var. for döngüsünün içinde yapmanız gereken basitçe XT virgül YTyi kullanarak gradyan inişin bir adımını uygulamaktır. Yani sanki 1.000 örneklik bir eğitim setiniz varmış ve tüm bu bildiklerinizi uygulamanız gerekiyormuş ama sadece 1.000lik tüm örnekler için belirlenmiş for döngüsü olmak yerine sadece M eşittir 1.000lik bu küçük eğitim setine uygulayacakmış gibi. 1.000 örneğin hepsini aşağı yukarı aynı anda işlemek için vektörizasyon kullanmış oluyorsunuz. Önce bunu bir yazalım, girdilerdeki bir dayanak için uyguluyorsunuz. yani, sadece XT üzerine ve bunu Z1 eşittir W1i uygulayarak yapıyorsunuz. Daha önce burada sadece X olurdu, değil mi? ama şimdi tüm eğitim setini işliyorsunuz, sadece mini-toptan Tyi işlerken XT olabilmesi için önce mini toptanı işliyorsunuz. Daha sonra A1 eşittir Z1in G1i olacaktır, bu aslında bir vektörleme ifadesi olduğu için büyük Z ve AL ile sonuçlanana kadar devam, cevap ZLnin GLsi ve bu sizin öngörünüz. ve fark ediyorsunuzdur ki burada vektörize bir uygulama kullanmanız gerekmektedir. Sadece bu vektörize uygulama 5 milyon örnek yerine her seferinde 1.000 örneği işliyor. Sonra bedel fonksiyonu Jyi hesaplıyorsunuz ki bunu küçük eğitim setinizin boyutu olduğu için bir üzeri 1.000 olarak yazıyorum. I eşittir birden L'ye kadarın toplamı aslında YInın bedelidir ve anlaşılabilirlik adına, bu işaret XT ve YT örneklerini ifade etmektedir. Ve eğer düzenlileştirme kullanıyorsanız, ayrıca bu düzenlileştirme teriminiz de olabilir. Bunu paydaya geçirin çarpı Lnin toplamı, bunu kare yapıyoruz. Bu aslında sadece bir mini-toptanın bedeli olduğu için, bunu bedel J üzeri süslü parantezde T olarak dizinleyeceğim. Dikkat edin ki, yaptığımız herşey daha önce gradyan iniş uyguladığımızda yaptığımız ile aynı, sadece bunu XY üzerine yapmak yerine XT YT üzerinde yapıyorsunuz. Sonra bu desteği JTye göre gradyanları hesaplamak için uyguluyorsunuz, hala sadece XT YT kullanıyorsunuz ve sonra W ağırlıklarını güncellediğinizde, okuyun, WL, WL eksi alfa d(WL) olarak güncellenir ve benzer şekilde B için. Bu mini-toptan gradyan inişi kullanarak eğitim setinizden bir geçiştir. Buraya yazdığım kod aynı zamanda eğitimin bir dönemi olarak adlandırılır ve dönem eğitim setinden bir geçiş anlamına gelen bir kelimedir. Halbuki, toptan gradyan iniş ile, eğitimden bir tek geçiş sadece bir gradyan iniş adımı atmaya izin verir. Mini-toptan gradyan iniş ile, eğitim setinden tek bir geçiş, yani bir dönem, 5.000 gradyan iniş adımı atmanıza izin veriyor. Şimdi elbette genelde isteyeceğiniz gibi, eğitim setinden birden fazla geçiş yapmak isteyeceksiniz, burada bir başka while döngüsü için bir başka for döngüsü isteyebilirsiniz. yani, yakınsayana veya yaklaşık olarak yakınsayana kadar eğitim setinden geçiş yapmaya devam edeceksiniz. Kayıp bir eğitim setiniz olduğunda, mini-toptan gradyan iniş toptan gradyan inişten çok daha hızlı çalışmaktadır ve bu aşağı yukarı Derin Öğrenmedeki herkesin büyük bir veri setini eğitirken kullanacağı şeydir. Bir sonraki videoda, ne yaptığını veya neden böyle çalıştığını anlamak için mini toptan gradyan inişe daha derinlemesine bakalım.