Önceki videoda,ilerlemek için ve eğim azalması(gradient descent) adımları hakkında konuşmak için, ilk defa eğitim setiniz işlenirken aşamanın yarısında olsanız bile, küçük yığın eğim azalmasını nasıl kullanacağınızı gördünüz. Bu videoda, eğim azalmasının nasıl uygulanacağı hakkında daha fazla detay öğrenecek ve ne yaptığı ve nasıl çalıştığıyla ilgili daha fazla şey anlayacaksınız. Toplu eğim azalması ile birlikte, her yinelemede bütün eğitim setinin üzerinde geçiyorsunuz ve bedel(cost)'in aşağı düşmesini her bir yinelemenin sonunda sağlıyorsunuz. Dolayısıyla bedel fonksiyonu j'ye sahip olduğumuzu düşünürsek, bu fonksiyonun her bir yinelemede azalma göstermesi gerekiyor. ve eğer bir kez olsun yukarı yönlü hareket ederse bu bir şeyin ters gittiğini gösterir. belki de öğrenme hızı çok fazladır Küçük toplu eğim azalması(mini batch gradient descent)'nda ise, eğer bedel fonksiyonunuzun gelişimini çizerseniz, bu durumda ise her yinelemede bedel fonksiyonunuz azalmayabilir, Özellikle, her yinelemede, X(t), Y(t) yi işlerken, eğer bedel fonksiyonu J(t)'yi çizerseniz, -ki bu da X(t) ve Y(t) kullanarak çiziliyor Bu durumda her yinelemede farklı bir eğitim setinde ya da gerçekten farklı bir küçük yığında eğitim sağlıyorsunuz gibi olacaktır. Dolayısıyla bedel fonksiyonu j'yi çizerseniz, buna benzer bir şekil görme ihtimaliniz yüksektir. Aşağı yönlü bir trend olacaktır fakat yinede bir miktar parazitli(noise) olacaktır. Eğer J(t)'yi birkaç dönem(epoch) boyunca, küçük yığın eğim azalması yönteminin uygulanması sırasında çizecek olursanız buna benzer şekilde bir eğri ile karşılaşırsınız. Dolayısıyla her yinelemede aşağıya gitmemesinde sorun yoktur fakat aşağı yönlü bir trend olmalıdır. Bir miktar parazitli olmasının sebebi, belki X(1), Y(1), göreceli olarak daha kolay bir küçük yığındır dolayısıyla bedel daha düşük olabilir, fakat belki şansa bağlı olarak X(2), Y(2) daha zor bir küçük yığındır. Belki içerisinde yanlış etiketlenmiş örnekler bile barındırıyor olabilir, bu durumda bedel daha yüksek olacaktır vs. Dolayısıyla küçük yığın eğim azalması uygularken bunların sebebiyle bu salınımlar gerçekleşmektedir. Şimdi, seçmeniz gereken parametrelerden biri ise küçük yığınınızın büyüklüğü. Diyelim ki m eğitim seti büyüklüğü olsun, bir uçta eğer küçük yığınınızın boyutunu m olarak belirlerseniz elinizde olan şey toplu eğim azalaması olur, bu uçta sadece bir küçük yığına sahip olmuş olursunuz X(1), Y(1) ve bu küçük yığın sizin bütün eğitim setinize eşit olmuş olur. Dolayısıyla küçük yığın büyüklüğünü m olarak seçerseniz bu durumda direk toplu yığın azalması yöntemini uygulamış olursunuz. Diğer uç ise küçük yığın büyüklüğünü 1 e eşitlediğiniz durumdur. bu size rastgele(stochastic) eğim azalması denilen algoritmayı verir ve burada her örnek kendi küçük yığınına sahiptir dolayısıyla bu durumda yapacağınız şey ilk küçük yığına bakmaktır eğer küçük yığın büyüklüğünü 1 olarak seçerseniz X(1), Y(1) eğitim setinizin ilk örneği olacaktır ve eğim azalması adımınızı eğitim setinizin ilk örneğiyle atmış olursunuz. ve daha sonra ikinci küçük yığına baktığınızda bu da eğitim setinizin ikinci örneği olmuş olur ve diğer eğim azalması adımını bununla atmış olursunuz üçüncü eğim azalması adımında da aynısını uygularsınız ve bu şekilde her seferinde bir eğitim örneğine bakarak devam edersiniz. Şimdi bu iki uç örneğin bedel fonksiyonumuzun iyilenmesinde(optimize edilmesinde) nasıl bir etkiye sahip olduğuna bakalım. Eğer azaltmak istediğiniz bedel fonksiyonunuzun sınırları bu şekilde ise, -dolayısıyla en düşük değeriniz burada bu durumda toplu eğim azalması herhangi bir yerde başlayabilir ve göreceli olarak düşük parazitle ve geniş adımlarla ilerleyebilir -minimuma gidişi izleyin. Tam tersi durumda yani rastgele(stochastic) eğim azalması durumunda, eğer burada başlarsanız, (-başka bir tane başlangıç noktası seçelim) her yinelemede eğim azalmasını sadece bir eğitim seti örneğiyle yapıyorsunuz çoğunlukla doğru yönde ilerlersiniz fakat bazen o eğitim örneği sizi yanlış yöne yönlenecek şekilde denk gelebilir dolayısıyla rastgele eğim azalması çok fazla miktarda parazitli olabilir ve genelde sizi doğru yolda ilerletecektir fakat bazen sizi yanlış yolda da ilerletebilir ve rastgele eğim azalması asla aynı noktada sabit kalmayacaktır her zaman minimum bölgesinde salınım yapacak ve dolaşacaktır fakat hiçbir zaman minimuma gidip orada sabit kalmayacaktır. Pratikte, kullanacağınız küçük yığın büyüklüğü iki uç arasında bir yerde olacaktır 1 ile m arasında bir yerde,ve 1 ve m sırasıyla çok küçük ve çok büyük değerler olarak karşımıza çıkıyor. Şimdi nedenine bakalım, eğer toplu eğim azalmasını kullanırsanız, dolayısıyla bu durumda büyüklüğü m'e eşitleyeceksiniz bu durumda her bir yineleme(iteration)'de çok büyük miktarda eğitim setini işliyor olacaksınız dolayısıyla uzun bir eğitim setine sahip olacağınızı varsayarsak bunun en büyük dezavantajı her yinelemede çok fazla zaman alması olacaktır eğer eğitim setiniz küçükse toplu eğim azalması uygun tercih olacaktır eğer zıt tarafa bakacak olursak yani rastgele eğim azalması kullanırsanız, bu durumda sadece bir eğitim örneğiyle ilerleme sağlayacaksınız bu çok büyük bir problem değil çünkü bu durumda oluşacak olan parazit(noise), daha düşük bir öğrenme hızı kullanılarak düzeltilebilir veya azaltılabilir fakat rastgele eğim azalması kullanmanın büyük dezavantajı ise yöneyleme(vectorization) ile kazandığınız hızın neredeyse tamanını kaybetmeniz olacaktır çünkü burada tek seferde sadece bir eğitim örneği kullanıyorsunuz, dolayısıyla her bir örneği işleme şekliniz yüksek miktarda verimsiz olacaktır dolayısıyla uygulamada en iyi çalışan yöntem bu ikisinin arasında bir değerdir, -küçük yığın büyüklüğünün çok fazla ya da çok az olmadığı bir değer ve bu size uygulamada en hızlı öğrenmeyi verecektir bu şekilde bir seçimin iki iyi özelliğe sahip olduğu farkedebilirsiniz birincisi çok miktarda yöneyleme(vectorization)'ye sahip olacaksınız dolayısıyla önceki videoda kullandığımız örnekte,eğer küçük yığın büyüklüğü 1000 ise bu durumda 1000 örnek üzerinde yöneyleme(vectorize) yapabilirsiniz ki bu örnekleri bir bir işlemekten çok daha hızlı olacaktır ve ikinci olarak bütün eğitim setini bitirmeyi beklemeye gerek kalmadan ilerleme sağlayabilirsiniz dolayısıyla tekrardan önceki videodadan örnek verecek olursak, her dönem(epoch) içerisinde 5000 eğim azalması adımı atabileceksiniz dolayısıyla uygulamada ikisi arasındaki bir küçük yığın büyüklüğü değeri en iyi değer olacaktır ve örnek verecek olursak küçük yığın eğim azalması ile buradan başlarsak, belki bir yinemele bunu iki bunu yapacaktır üç dört diye gidecektir. ve yine burada da her zaman minimuma gideceğinin garantisi yoktur fakat minimuma doğru daha kararlı bir şekilde yol alacaktır ve yine aynı şekilde her zaman bir noktada toplanacağının veya salınım yapmayacağının garantisi yoktur eğer bu bir sorun olursa öğrenme hınızı azaltarak bu problemi çözebilirsiniz öğrenme hızı sönümü(learning rate decay) hakkında ya da öğrenme hızını nasıl azaltacağımız konusunda daha sonraki videolarda konuşacağız. Küçük yığın büyüklüğünün m veya 1 olmaması gerektiğini konuştuk ve arada bir yerde olması gerekiyor dedik fakat bunu nasıl seçeceğiz? birkaç prensip verelim; birincisi, eğer küçük bir eğitim setiniz varsa toplu eğim azalması yöntemini kullanın eğer küçük bir eğitim setiniz varsa küçük toplu öğrenme yöntemini kullanmanızın faydası yoktur bütün eğitim setinizi hızlı bir şekilde işleyebilirsiniz dolayısıyla bu durumda toplu eğim azalmasını kullanmalısınız küçük bir eğitim setinden kastım, belki 2000'den daha az eğitim seti olabilir bu durumda toplu eğim azalmasını kullanmak gayet yerinde olacaktır eğer büyük bir eğitim setiniz varsa, kullanacağınız küçük toplu eğim azalmasında büyüklük olarak 64'ten 512 ye olan değerler alışılageldiktir ve bilgisayar belleklerinin düzenlenişi ve erişiminden dolayı bazen küçük toplu eğim azalmasında kullanacağınız büyüklüklerin 2'nin katları olması önemli kodunuzun daha hızlı çalışmasına sebebiyet verebilir örneğin 64 2 üzeri 6 ya denk geliyor,bunun dışında 2 üzeri 8,2 üzeri 9 gibi değerler de kullanılabilir dolayısıyla büyüklükleri 2'nin katları şeklinde düzenliyorum önceki videoda büyüklüğü 1000 yaptığımın farkındayım, eğer gerçekten 1000 yapmak istiyorsanız size 1024 yapmanızı öneririm -ki bu da 2 üzeri 10'a denk geliyor ve 1024 büyüklüğünü biraz daha nadir göreceksiniz yazdığım değerler biraz daha yaygın olarak kullanılmakta son önerim ise küçük toplu eğim azalmasında kullanacağınız büyüklüklerin, yani bütün X(t), Y(t)'lerin CPU/GPU belleğine uyduğundan emin olmaktır ve bu uyum uygulamaya ve tek bir eğitim örneğinizin ne kadar büyük olduğuna bağlı olarak değişir fakat eğer CPU/GPU belleğine uymayan bir büyüklük seçerseniz bu durumda performansın aniden hızlı bir biçimde düştüğünü göreceksiniz ve aniden çok daha kötü hale geldiğini göreceksiniz. Böylelikle, umarım bu size insanların küçük toplu eğim azalmasında kullandığı standart bir aralıkla ilgili fikir vermiş olur. Uygulamada bu büyüklük aslında üzerinde hızlı bir araştırma yapabileceğiniz ve hangi değerin bedel fonksiyonu j'yi düşürmede daha verimli olduğuyla ilgili kullanılabilecek başka bir yüksek parametredir. Böyle bir durumda benim yapacağım şey birkaç farklı değeri denemek olacaktır 2'nin katı birkaç farklı değeri deneyin ve eğim azalması iyileme algoritmasını olabildiğince verimli hale getiren bir değer seçin. Umut ediyorum ki bunlar size yüksek parametre aramaya başlamada ana prensipler olarak yardım eder. Artık küçük toplu eğim azalmasını nasıl uygulayacağınızı ve özellikle büyük bir eğitim setinde eğitim yaparken algoritmanızı nasıl hızlı çalıştırabileceğinizi biliyorsunuz fakat gerçekte olan şu ki eğim azalmasından veya küçük toplu eğim azalmasındaan daha verimli algoritmalar mevcut Hadi onları da sıradaki videolarda konuşalım.