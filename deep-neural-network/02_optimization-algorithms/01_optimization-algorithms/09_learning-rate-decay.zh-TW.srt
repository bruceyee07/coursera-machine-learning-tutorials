1
00:00:00,380 --> 00:00:03,140
要加快學習演算法，
其中的一個可能方式

2
00:00:03,140 --> 00:00:06,240
是隨著時間慢慢降低你的學習率

3
00:00:06,240 --> 00:00:08,520
我們稱之為學習率衰減 (learning rate decay)

4
00:00:08,520 --> 00:00:10,650
我們來看看怎麼做

5
00:00:10,650 --> 00:00:13,710
先從一個例子看起，示範為何你會想做

6
00:00:13,710 --> 00:00:15,150
學習率衰減

7
00:00:15,150 --> 00:00:18,260
假設你在做小批次梯度下降法

8
00:00:18,260 --> 00:00:20,070
用合理、少量的的小批資料

9
00:00:20,070 --> 00:00:24,210
可能一批資料只有 64 或 128 筆

10
00:00:24,210 --> 00:00:28,210
在你進行時，你的步伐會有點雜亂、有噪音

11
00:00:28,210 --> 00:00:33,940
大勢上他會往最小值前進，但並不會剛好收斂

12
00:00:33,940 --> 00:00:38,040
你的演算法最終可能會在周圍徘徊

13
00:00:38,040 --> 00:00:43,390
並不會真的收斂 — 因為你用了固定大小的 alpha

14
00:00:43,390 --> 00:00:46,660
而不同批的小資料本來就會有些許噪音

15
00:00:46,660 --> 00:00:52,650
然而，如果你把學習率 alpha 慢慢降低的話

16
00:00:52,650 --> 00:00:56,410
在一開始的時候，你的學習率 alpha 還很大

17
00:00:56,410 --> 00:00:59,270
所以你還可以滿快地學習

18
00:00:59,270 --> 00:01:05,940
而當 alpha 逐漸變小，你的步伐會越來越慢、越短

19
00:01:05,940 --> 00:01:11,160
所以最後你會在極小值附近，更小的區域裡震盪

20
00:01:11,160 --> 00:01:15,398
不會發生就算一直訓練還跑出去的狀況

21
00:01:15,398 --> 00:01:20,200
所以慢慢減少 alpha，這背後的概念是

22
00:01:20,200 --> 00:01:25,170
可能一開始在訓練的時候，你經得起踏出很大的步伐

23
00:01:25,170 --> 00:01:29,060
但隨著學習逐漸要收斂

24
00:01:29,060 --> 00:01:33,070
使用比較慢的學習率讓你有比較精細的步伐

25
00:01:33,070 --> 00:01:36,650
那麼，你可以這樣實作學習率衰減

26
00:01:36,650 --> 00:01:40,640
還記得一個 epoch 表示

27
00:01:42,512 --> 00:01:45,430
掃過一遍資料對吧

28
00:01:45,430 --> 00:01:49,053
所以如果你有這樣的資料集

29
00:01:49,053 --> 00:01:53,866
可能分成各個小批資料

30
00:01:53,866 --> 00:02:00,446
那麼第一次處理過一遍這個資料集，這樣叫第一個 epoch

31
00:02:00,446 --> 00:02:05,613
第二遍處理叫做第二個 epoch，依此類推

32
00:02:05,613 --> 00:02:10,628
所以你可以做的是，把你的學習率 alpha 設為

33
00:02:10,628 --> 00:02:15,464
1 除以 1 加某個參數，我就稱之為衰減率 (decay rate)

34
00:02:18,112 --> 00:02:22,490
然後乘以 epoch 數

35
00:02:22,490 --> 00:02:26,890
然後這個會乘上某個初始的學習率 alpha_0

36
00:02:26,890 --> 00:02:30,730
注意到這邊的衰減率，他會是一個超參數

37
00:02:30,730 --> 00:02:32,340
所以你可能需要調整

38
00:02:32,340 --> 00:02:33,910
這邊是個實際的例子

39
00:02:35,070 --> 00:02:39,659
假設你正在進行多個 epoch、掃過你的資料若干遍

40
00:02:39,659 --> 00:02:46,211
如果 alpha_0 為 0.2，衰減率為 1

41
00:02:46,211 --> 00:02:50,267
那麼在第一個 epoch 時

42
00:02:50,267 --> 00:02:55,268
alpha 會等於 1 / (1 + 1) 乘上 alpha_0

43
00:02:55,268 --> 00:02:59,785
所以你的學習率會是 0.1

44
00:02:59,785 --> 00:03:04,289
這是從這個式子算出來的 — 當衰減率等於 1

45
00:03:04,289 --> 00:03:05,755
而且 epoch 號碼為 1 的時候。

46
00:03:05,755 --> 00:03:10,613
在第二次 epoch，你的學習率衰減至 0.67 (筆誤, 0.067)

47
00:03:10,613 --> 00:03:15,924
第三次 0.5  (0.05), 第四次 0.4 (筆誤, 0.04) 等等

48
00:03:15,924 --> 00:03:18,150
你可以自己再算算看後面的值

49
00:03:18,150 --> 00:03:23,200
讓你有個感覺，你的學習率是epoch數的函數，它會

50
00:03:23,200 --> 00:03:29,930
逐漸減少，對吧，根據上面這個式子

51
00:03:29,930 --> 00:03:33,860
因此，如果你想使用學習率衰減，你可以做的事是

52
00:03:33,860 --> 00:03:38,830
嘗試各種不同超參數的值：超參數 alpha_0、

53
00:03:38,830 --> 00:03:41,550
以及這個衰減率 (decay rate) 超參數

54
00:03:41,550 --> 00:03:44,710
試著找到成效良好的值。

55
00:03:44,710 --> 00:03:47,188
學習率衰減除了這一個公式以外

56
00:03:47,188 --> 00:03:49,314
還有其他幾種大家會用的

57
00:03:49,314 --> 00:03:52,097
舉個例子，這個稱為指數衰減

58
00:03:52,097 --> 00:03:58,009
alpha 等於把某個小於 1 的數字，

59
00:03:58,009 --> 00:04:04,513
例如 0.95，的 epoch數次方，再乘上 alpha_0。

60
00:04:04,513 --> 00:04:10,500
所以這個會呈指數速度減少你的學習率

61
00:04:10,500 --> 00:04:15,788
還有其他大家會用的公式像是 alpha 等於

62
00:04:15,788 --> 00:04:21,805
某個常數除以epoch數的平方根，乘上 alpha_0。

63
00:04:21,805 --> 00:04:26,627
或是某個常數 k —這也是另一個超參數—

64
00:04:26,627 --> 00:04:32,956
除以小批資料次數 t 的平方根，乘上 alpha_0。

65
00:04:32,956 --> 00:04:37,627
有時候，你也會看到有人使用離散階梯型下降的

66
00:04:37,627 --> 00:04:38,821
學習率，

67
00:04:38,821 --> 00:04:42,798
因此在某些數量的步驟，你有某個學習率，

68
00:04:42,798 --> 00:04:45,960
然後過了一些時候，你把它減半，

69
00:04:45,960 --> 00:04:47,320
然後再過一會，減半

70
00:04:47,320 --> 00:04:48,970
再過一會，減半

71
00:04:48,970 --> 00:04:52,793
所以這是個離散狀的樓梯

72
00:04:55,954 --> 00:05:01,395
那麼到目前為止，我們談到利用數學公式

73
00:05:01,395 --> 00:05:05,210
來控制 alpha 學習率要怎麼隨著時間改變

74
00:05:05,210 --> 00:05:08,900
而有時候一些人會採取「手動衰減」

75
00:05:08,900 --> 00:05:11,980
假設你一次只訓練一個模型，而且

76
00:05:11,980 --> 00:05:16,070
如果你的模型要花幾小時甚至幾天來訓練

77
00:05:16,070 --> 00:05:17,090
那麼有些人會

78
00:05:17,090 --> 00:05:21,638
在費時數日的訓練期間，監控他的模型

79
00:05:21,638 --> 00:05:25,180
然後人工觀察，喔看起來這學習趨緩了

80
00:05:25,180 --> 00:05:27,180
我要把 alpha 減低一點

81
00:05:27,180 --> 00:05:30,242
當然這樣手動控制 alpha 也有效

82
00:05:30,242 --> 00:05:33,710
做手工藝來調整 alpha，每一小時或每一天

83
00:05:33,710 --> 00:05:37,140
這只在你訓練很少量的模型的時候才有用，不過

84
00:05:37,140 --> 00:05:39,100
有時候也有人會這麼做。

85
00:05:39,100 --> 00:05:43,580
那麼你有更多的選項來控制學習率 alpha

86
00:05:43,580 --> 00:05:46,630
現在你可能會想，哇塞，這有好多超參數啊

87
00:05:46,630 --> 00:05:49,320
在眾多選擇中，我該怎麼挑啊？

88
00:05:49,320 --> 00:05:51,190
我會說，現在無需擔心

89
00:05:51,190 --> 00:05:56,550
在下個禮拜，我們會談到更多如何有系統地挑選超參數

90
00:05:56,550 --> 00:06:00,500
對我而言，我會說學習率衰減通常會在我的

91
00:06:00,500 --> 00:06:02,080
試驗清單的後面

92
00:06:02,080 --> 00:06:05,670
設定 alpha，光是一個固定不變的 alpha 值，好好地調校

93
00:06:05,670 --> 00:06:07,080
就會有超大影響

94
00:06:07,080 --> 00:06:09,050
學習率衰減的確有幫助

95
00:06:09,050 --> 00:06:11,050
有時他真的可以幫助加快訓練，但是

96
00:06:11,050 --> 00:06:15,720
他在我想嘗試的事情中，順位有點後面

97
00:06:15,720 --> 00:06:18,543
不過下個禮拜，當我們談到超參數的調校時

98
00:06:18,543 --> 00:06:21,978
你會看到更有系統的方式來組織這些超參數

99
00:06:21,978 --> 00:06:24,422
還有如何有效率地在其中搜尋

100
00:06:24,422 --> 00:06:27,790
那麼，這就是學習率衰減

101
00:06:27,790 --> 00:06:31,420
最後，我還想談一談局部極值 (local optima)，以及

102
00:06:31,420 --> 00:06:33,390
鞍點 (saddle points)，在神經網路內

103
00:06:33,390 --> 00:06:36,210
讓你有更好的概念了解

104
00:06:36,210 --> 00:06:39,970
你的優化演算法所解決的問題類型

105
00:06:39,970 --> 00:06:41,840
當你在訓練這些神經網路的時候

106
00:06:41,840 --> 00:06:43,570
我們下一段影片見