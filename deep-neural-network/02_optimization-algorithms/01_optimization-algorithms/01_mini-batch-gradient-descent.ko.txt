안녕하세요, 환영합니다. 여러분의 신경망 네트워크를 훨씬 더 빨리 트레이닝 시킬 수 있도록 도와주는 최적화 알고리즘에 대해 배우도록 하겠습니다. 머신 러닝을 적용한다는 것은 매우 경험에 의거한 절차와 매우 반복적인 업무를 통반한다고 제가 말한 것을 이전에 들으셨을텐데요, 여러 모델들을 그냥 무조건 트레이닝시키고 가장 작 작동하는 것을 찾아야 합니다. 그렇기 때문에 재빨리 모델들을 트레이닝 시키는 것이 매우 중요합니다. 한가지 더 어렵게 만들 수 있는 부분은, 딥러닝이 큰 데이터의 경우 가장 잘 작동하는 경향이 있다는 것입니다. 저희는 신경망을 큰 데이터세트에서 트레이닝을 시킬 수 있는데요, 이런 경우 속도가 매우 느리겠죠. 이런 경우 빠른 최적화 알고리즘과, 그리고 양질의 최적화 알고리즘을 갖는 것이 여러분의 팀 효율성을 높힐 수 있다는 것입니다. 속도를 높혀주기 때문이죠. 그러면 이제 미니 배치 기울기 강하에 대한 이야기로 본강을 시작해보록 하겠습니다. 이전에 배우셨지만, 벡터화는 모든 m 예시에 대해서 효율적으로 산출을 할 수 있게 해줍니다. 특정한 공식 없이도 모든 전체 트레이닝세트를 처리할 수 있게 말이죠. 그렇기 때문에 저희는 트레이닝 예시를 가지고 방대한 매트릭스 캡슐 X들에 쌓아올리는 것입니다. X1, X2, X3, 그리고 xm 트레이닝 샘플까지 올라갑니다. 여기 Y값에 대해서도 비슷하게 말이죠. 이 값은 Y1, Y2, Y3, 그리고 YM 까지 말이죠. X의 다이멘션은 nx x m이였죠. 그리고 이것은 1 x m 이였습니다. 벡터화는 M 예시들을 꽤 빨리 처리할 수 있게 해줍니다. M이 아주 큰 값일 경우에 말이죠. 아직 느릴 수는 있죠. 예를 들어, M이 5백만이거나 5천만이거나 더 큰 값인 경우, 전체 트레이닝세트에 기울기 강하를 도입하면, 여러분이 해야 하는 것은, 전체 트레이닝 세트를 기울기 강하에 대한 조금한 첫 단계를 시작하기 전에 
모든 것을 처리해야 합니다. 그리고나서 전체 5백만개의 트레이닝 세트를 다시 처리해야 합니다. 기울기 강하에 대한 또 하나의 추가 절차를 밟기 전에 말이죠. 여러분이 만약에 전체 아주 큰 5백만개 예시가 있는 트레이닝 세트를 트레이닝 시키기도 전에 기울기 강하가 조금 진전이 있게 하면, 훨씬 더 빠른 알고리즘을 구축할 수도 있습니다. 특히, 여러분이 할 수 있는 것은 만약에 여러분이 트레이닝 세트를 조금 더 조금하게 나눈다고 해보겠습니다. 이런 아주 작은 트레이닝 세트로 나눠서 아기 트레이닝 세트를 미니 배치들이라고 부르겠습니다. 그러고 난 뒤, 이런 라기 트레이닝 세트가 각각 천 개의 예시를 가지고 있다고 해보겠습니다. 그러면 x1에서 x1000까지 있고, 이것을 첫번째 아기 트레이닝 세트라고 하겠습니다. 또는 미니 배치라고도 하죠. 그 다음에 또 추가로 1000개의 예시를 갖습니다. x1001에서 x2000까지 1000개의 예시를 갖고, 그렇게 다음도 이어서 올 수 있겠죠. 이제 저는 새로운 표기법을 보여드릴 텐데요, 여기는 x 위첨자 중괄호 1의 값이라고 부르고, 그리고 여기는 x 위첨자 중괄호 2라고 부를 것입니다. 자 그럼 여기서 총 5백만개의 트레이닝 샘플이 있고, 여기 각각의 미니 배치들이 1000개의 예시가 있으면, 여기 이것은 5000개가 있다는 것입니다. 왜냐면, 5000 곱하기 1000이 5백만개이기 때문입니다. 그럼 총 여기 민 배치들은 5000개가 되는 것입니다. 그러면 최종적으로 x 위첨자 중괄호 5000이 되는 것입니다. Y에 대해서도 비슷하게 진행합니다. Y에 대한 트레이닝 데이터도 적합하게 나눠줍니다. 이것을 Y1이라고 부르고, 그러면 이것은 Y1001 에서 Y2000까지가 되겠죠. 이것은 Y2라고 하고, 등등 Y5000이 될 때까지 이어집니다. 그러면 T까지의 미니 배치는 X, T, Y, T로 이루어져 있을 것입니다. 이것은 입력 값과 결과값 쌍으로 이루어진 1000개의 트레이닝 샘플입니다. 다음으로 넘어가기 전에, 여기 표기법이 명백하도록 하겠습니다. 이전에는 위첨자 소괄호를 이용하여 트레이닝 세트를 인덱싱 했는데요, x i는 i번째 트레이닝예시인데요, 그리고 위첨자 대괄호 L을 이용해서 신경망의 층을 나타냅니다. 그러므로 ZL은 Z 값에서 L 개의 신경망 네트워크 층을 갖고 있음을 뜻하는데요, 그리고 여기서는 중괄호 t를 이용해서 mini bacthes들을 인덱싱 합니다. 그러면 XT와 YT 가 있는데요, 여러분이 이것들에 대한 이해도를 체크하기 위해서, XT 와 YT의 다이멘션이 어떤지 확인할 수 있는데요, x는 nX x M입니다. 그러므로 만약 X1이 1000개의 트레이닝 샘플이거나, 1000개의 예시에 대한 값인 경우, 여기 다이멘션 은 nx, 1000이여야하고, X2는 또한 nx, 1000이여야 합니다. 그렇기 때문에 여기 모든 다이멘션은 nx, 1000이여야 합니다. 그리고 여기는 1,1000의 다이멘션을 가져야 합니다. 여기 알고리즘의 이름을 설명드리자면, 배치 기울기 강하는 이전에 다뤘었던 기울기 강하 알고리즘을 뜻하는데요, 전체 트레이닝 세트를 한번에 처리하는 경우를 뜻합니다. 이름은 어디서 유래되었냐면, 전체 트레이닝 샘플 배치를 한번에 처리한다는데에서 유래되었습니다. 훌륭한 이름이라고 보긴 힘들지만 단순히 그냥 이렇게 불립니다. 반대로 미니 배치 기울기 강하는 다음 슬라이드에서 이야기할 알고리즘을 뜻하는데요, 싱글 미니 배치인 XT, YT를 한번에 처리하는 경우를 뜻합니다. YX 트레이닝세트를 한번에 처리하는 것이 아니고 말이죠. 그러면 미니 배치 기울기 강하가 어떻게 작동하는지 한번 보겠습니다. 미니 배치 기울기 강하를 트레이닝 세트에서 실행하기 위해서는, t=1 에서 5000까지 실행합니다. 각각 1000개씩 가지고 있는 미니 배치가 5000개 있었죠. for loop 내부에서는 무엇을 할 것이냐면, xt, yt를 이용해서 기울기 강하의 한 단계를 도입할 것입니다. 이것은 마치 트레이닝 세트의 크기가 1000개의 예시가 있는 것과 비슷합니다. 그리고 여러분이 이미 익숙한 알고리즘을 도입하려고 하는 것과 비슷합니다. 여기 작은 트레이닝 세트인 M값이 1000인 경우에서 말이죠. 여러분이 별개의 for loop을 1000개의 예시에 갖게 하는 것이 아닌, 벡터화를 통해 1000개의 예시를 한번에 처리하는 것과 같습니다. 한번 먼저 적어볼까요? 첫째로, 입력 값에 대한 전 방향전파 을 도입합니다. Xt의 값에 말이죠. 이것은 Z1=W1이 되게하여 도입합니다. 이전에는 여기가 그냥 X였죠? 하지만 이제 여러분은 전체 트레이닝 세트를 처리하는 것이 아니라, 첫번째 미니 배치만 처리하는 것입니다. 그렇기 때문에 여기가 xT가 되죠. 미니 배치 T를 처리하는 경우에 말이죠. 그러면 G1의 Z1의 값을 갖게 되겠죠. 여기는 대문자 Z죠, 벡터화 도입이기 때문입니다. 이어서 AL값이 될 때까지 진행하는데요, 여기는 GL의 ZL입니다. 그리고 이 값이 예측 값이 되는 것입니다. 그리고 여기서 아시겠지만, 벡터화 도입을 이용하셔야 합니다. 여기서 벡터화 도입이 5백만개의 예시 대신에 한번에 1000개의 예시를 처리하기 때문입니다. 다음으로는, J 비용함수를 산출합니다. 여기서는 이 함수를 1 나누기 1000으로 적을 것인데요, 그 이유는 1000이 여러분의 작은 트레이닝 세트의 
크기이기 때문입니다. 1에서 L까지의 합으로 y hat i, yi의 loss의 함수입니다. 명료성을 위해서 여기 표기법은 미니 배치 XT, YT의 예시를 뜻합니다. 그리고 여러분이 일반화를 사용하는 경우엔, 여기 이런 일반화 항을 쓸 수 있습니다. 2는 분모가 되겠구요, 곱하기 l의 합 L의 매트릭스 제곱값의 Frobenius 방식입니다. 이 값은 하나의 미니 배치에 대한 비용함수이기 때문에, 이것을 J비용함수 위첨자 중괄호 T로 인덱싱할 것인데요, 이전에 기울기 강하에서 도입했던 방식과 XY에서 하는 것을 XT, YT에서 하는 것을 제외하고는 모든 것이 동일합니다. 다음으로는, 후 방향전파을 도입하고, JT에 대해서 gradient를 산출하기 위해서 말이죠. 여기서는 아직 XT 와 YT만 사용하는데요, 그 다음으로 비중인 W를 업데이트 합니다. 모든 WL은 WL 빼기 알파 곱하기 D 곱하기 WL로 업데이트 되는데요, B도 비슷하게 됩니다. 그래서 이것은 미니 배치 기울기 강하를 이용해서 한번에 크게 트레이닝 세트를 통과하는 개념으로 생각하면 됩니다. 여기 제가 쓴 코드는 1 epoch 트레이닝을 한다고 표현하기도 하는데요, epoch이라는 용어는 트레이닝 세트로 1번 통과한다는 뜻입니다. 반면에 배치 기울기 강하에서는 한번 트레이닝을 통과하는 것이 오로지 한번의 기울기 강하 절차를 밟게 해줍니다. 미니 배치 기울기 강하를 통해, 트레이닝 세트로 한번의 통과를 통해, 즉, 1epoch 이죠, 5000 단계의 기울기 강하를 진행하게 해줍니다. 당연히, 복수로 트레이닝 세트를 통과하는 것을 주로 원하겠죠. 또 하나의 for loop를 갖거나 while loop를 갖어야 할 것입니다. 그리하여 바라건대 지속적으로 트레이닝 세트를 통과하여 수렴하는 지점을 갖도록 합니다. 큰 사이즈의 트레이닝 세트가 있는 경우, 미니 배치 기울기 강하가 배치 기울기 강하보다 훨씬 더 빨리 운영 돕니다. 그리고 이것이 큰 데이터 세트에서 트레이닝하는 경우, 딥러닝에서 거의 모든 사람이 쓰는 것입니다. 다음 비디오에서는, 미니 배치 기울기 강하에 대해 조금 더 자세히 알아보도록 하겠습니다. 여러분의 이해도를 높히고 이것의 역할이 어떤지, 또 왜 잘 작동하는지.