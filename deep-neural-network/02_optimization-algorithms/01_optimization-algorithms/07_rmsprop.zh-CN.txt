你已经学习了如何用动量来加速梯度下降 还有一个叫做RMSprop的算法 全称为均方根传递(Root Mean Square prop)<br />它也可以加速梯度下降 我们来看看它是如何工作的 回忆一下之前的例子 在实现梯度下降时 可能会在垂直方向上出现巨大的振荡 即使它试图在水平方向上前进 为了说明这个例子 我们假设 纵轴代表参数b 横轴代表参数W 当然这里也可以是W1和W2等其他参数 我们使用b和W是为了便于理解 你希望减慢b方向的学习 也就是垂直方向 同时加速或至少不减慢水平方向的学习 这就是RMSprop算法要做的 在第t次迭代中 它会像往常一样计算 当前小批量的倒数dW和db 我会保留这个指数加权平均数 这次使用新符号S_dW 取代之前的V_dW S_dW就等于beta乘以之前的值 再加上(1-beta)*dW^2 有时我们使用dW**2的形式来表示幂 这里写成dW^2 需要澄清的是 这个平方操作是逐元素的平方操作 所以这一步实际上是保存了导数平方的 指数加权平均数 类似地 S_db=beta*S_db+(1-beta)*db^2 同样 这里的平方是逐元素的运算操作 之后 RMSprop用以下方法更新参数 W更新为W减去学习率 之前我们使用的是alpha乘以dW 但现在使用是dW除以S_dW的平方根 而b更新为b减去学习率乘以 梯度除以S_db的平方根 而不是仅使用梯度 现在我们来理解一下它的工作原理 记得在水平方向上 即例子中W的方向上 我们希望学习速率较快 而在垂直方向上 即例子中b的方向上 我们希望降低垂直方向上的振荡 对于S_dW和S_db这两项 我们希望S_dW相对较小 因此这里除以的是一个较小的数 而S_db相对较大 因此这里除以的是一个较大的数 这样就可以减缓垂直方向上的更新 实际上 如果你看一下导数 就会发现垂直方向上的倒数要比水平方向上的更大 所以在b方向上的斜率很大 对于这样的导数 db很大 而dW相对较小 因为函数在垂直方向 即b方向的斜率 要比w方向 也就是比水平方向更陡 所以 db的平方会相对较大 因此S_db会相对较大 相比之下dW会比较小 或者说dW的平方会较小 所以S_dW会较小 结果是 垂直方向上的更新量 会除以一个较大的数 这有助于减弱振荡 而水平方向上的更新量会除以一个较小的数 使用RMSprop的效果就是使你的更新 会更像这样 在垂直方向上的振荡更小 而在水平方向可以一直走下去 另一个收效是 你可以使用更大的学习率alpha 学习得更快 而不用担心在垂直方向上发散 为了清楚起见 我一直称垂直方向 和水平方向分别为b和W 只是为了方便演示 实践中 你通常会面对非常高维的参数空间 你试图降低振荡的垂直方向 可能是参数W_1 W_2 W_17的集合 而水平维度可能是W_3 W_4等等 所以 这里把W和b分开只是为了便于演示 实践中 dW是一个非常高维度的参数向量 db也是一个非常高维度的参数向量 直观理解是 在出现振荡的维度里 你会计算得到一个更大的和值 即导数平方的加权平均 最后抑制了这些出现振荡的方向 所以这就是RMSprop 全称是均方根传递 因为你对导数求了平方 而且最后取了平方根 最后 在进入下一个话题前<br />我再讲下这个算法的一些细节 下一个视频中 我们将结合RMSprop和动量 由于我们之前在动量中已经使用了超参数beta<br />这里就不再使用beta 我将这个超参数命名为beta_2 以避免在动量和RMSprop中使用相同名称的超参数 同时 为了确保你的算法不会除以零 如果S_dW的平方根非常接近0 这一项就会非常大 为了确保数值稳定性 当你实现这个算法时 你要给分母加上一个非常非常小的epsilon epsilon的值取多少并不重要 10的-8次方是一个合理的默认值 但这只能 轻微提高数值稳定性<br />无论由于数值上的舍入还是什么别的原因 你都不会除以一个过小的数 所以这就是RMSprop 它和动量一样能够 降低梯度下降和小批量梯度下降中的振荡 并且令你能使用更大的学习率alpha 从而提高你的算法的学习速度 现在你学会了如何实现RMSprop 这是另一种能够加速学习算法的方法 说一个关于RMSprop的趣事 它一开始并不是在学术研究论文中被提出的 而是Geoffrey Hinton在很多年前教的<br />一个Coursera课程中提出的 我猜Cousera最初并没有意图成为一个宣传 新兴学术研究的平台 但它在这方面做得很棒 通过Coursera课程 RMSprop才变得广为人知 并迅速发展起来 我们讲了动量 我们也讲了RMSprop 事实上 如果把二者结合起来 你会得到一个更好的 优化算法 我们将在下一个视频中讨论这一点