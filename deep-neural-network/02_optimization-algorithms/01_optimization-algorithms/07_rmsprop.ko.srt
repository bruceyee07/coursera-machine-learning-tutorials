1
00:00:00,470 --> 00:00:03,955
여러분은 모멘텀을 이용해서
기울기 강하를 빠르게 실행하는 법을 배웠는데요.

2
00:00:03,955 --> 00:00:06,230
또 다른 RMSprop라는 알고리즘이 있습니다.

3
00:00:06,230 --> 00:00:10,490
이것은 root mean square propagation의 약자인데요,
이 방법을 이용해 기울기 강하의 속도를 증가시킬 수 있습니다.

4
00:00:10,490 --> 00:00:11,800
어떻게 작동하는지 보겠습니다.

5
00:00:11,800 --> 00:00:16,313
이전 예제를 다시 한번 보겠습니다.
기울기 강하를 도입하는 경우엔,

6
00:00:16,313 --> 00:00:20,252
세로로 아주 큰 변동이 일어날 수 있습니다.

7
00:00:20,252 --> 00:00:24,569
가로줄로 나아가려고 할때 말이죠.

8
00:00:24,569 --> 00:00:29,214
이 예제와 관련하여 직관적인 부분을 제공하겠습니다.

9
00:00:29,214 --> 00:00:34,733
세로축이 파라미터 v이고,
가로축이 파라미터 w라고 하겠습니다.

10
00:00:34,733 --> 00:00:39,614
w1 과 w1일 수도 있겠죠. 중앙 파라미터 는 b와 w로

11
00:00:39,614 --> 00:00:42,090
지정할 수도 있겠죠.

12
00:00:42,090 --> 00:00:46,690
여러분이 b의 방향으로 학습 속도를 늦추려고 하거나,

13
00:00:46,690 --> 00:00:48,400
세로방향으로 늦추는 것이겠죠.

14
00:00:48,400 --> 00:00:54,830
그리고 가로방향으로는
속도를 늘리거나, 적어도 늦추지는 않습니다.

15
00:00:54,830 --> 00:00:59,411
RMSprop 알고리즘이 하는 일은
이와 같습니다.

16
00:00:59,411 --> 00:01:07,237
iteration업무 t에서 예전과 같이,
derivative dW를 계산할 것입니다.

17
00:01:07,237 --> 00:01:11,387
현재 미니 배치에 대해 db도 말이죠.

18
00:01:15,464 --> 00:01:19,400
이 기하급수적 가중평균은 유지하려고 했는데요,

19
00:01:19,400 --> 00:01:22,890
VdW대신에 저는 SdW 표기를 사용하겠습니다.

20
00:01:22,890 --> 00:01:28,954
그러면 SdW는 베타 곱하기 이전 값들 더하기

21
00:01:28,954 --> 00:01:34,181
1 빼기 베타 곱하기 dW 제곱입니다.

22
00:01:34,181 --> 00:01:41,130
가끔씩은 dW 제곱으로 말이죠.

23
00:01:41,130 --> 00:01:48,530
명료성을 위해, 여기 제곱은
element-wise 제곱입니다.

24
00:01:48,530 --> 00:01:52,170
이것이 하는 역할은, 실제로 기하 급수적으로 가중된 미분의

25
00:01:52,170 --> 00:01:56,230
제곱 평균을 유지하는 것입니다.

26
00:01:56,230 --> 00:02:04,368
비슷하게, Sdb = 베타 Sdb 더하기 1 빼기 베타, db 제곱이 있는데요,

27
00:02:04,368 --> 00:02:08,031
그리고,
여기 제곱은 마찬가지로 element-wise operation입니다.

28
00:02:08,031 --> 00:02:13,330
다음으로, RMSprop은 다음과 같이
파라미터를 업데이트 시킵니다.

29
00:02:13,330 --> 00:02:17,875
W는 W 빼기 러닝속도로 업데이트 될 것이고,

30
00:02:17,875 --> 00:02:22,580
이전에는 알파 곱하기 dW였죠.
이제는

31
00:02:22,580 --> 00:02:27,596
dW 나누기 SdW의 루트입니다.

32
00:02:27,596 --> 00:02:33,322
그리고 b는 b 빼기 러닝속도
곱하기

33
00:02:33,322 --> 00:02:38,080
기울기만 적용하는 대신,
이 값은 Sdb로 나뉩니다.

34
00:02:39,600 --> 00:02:42,970
이제 이게 어떻게 작동하는지 직관적인 부분을
다루어 보도록 하겠습니다.

35
00:02:42,970 --> 00:02:45,750
기억하시겠지만, 가로 방향으로 또는,

36
00:02:45,750 --> 00:02:50,380
이번 예제에서는, W방향으로
러닝이 빨리 이동하기 바랍니다.

37
00:02:50,380 --> 00:02:54,819
세로 방향으로는 또는,
이 예제의 경우에는 b의 방향으로는,

38
00:02:54,819 --> 00:02:59,137
세로축에서의 변동은 늦추고 싶습니다.

39
00:02:59,137 --> 00:03:01,737
그럼 여기에서 있는 항, SdW와 Sdb는

40
00:03:01,737 --> 00:03:06,729
우리가 하고 싶은 것은,
SdW는 비교적 작은 값으로,

41
00:03:06,729 --> 00:03:11,836
그렇게해서 비교적 작은 값으로
나눌 수 있게 하고,

42
00:03:11,836 --> 00:03:16,851
Sdb는 비교적 크게됩니다.
여기서 yt를 비교적

43
00:03:16,851 --> 00:03:21,226
큰 값으로 나누어서 세로축의 업데이트를
늦추기 위해서 말이죠.

44
00:03:21,226 --> 00:03:25,518
실제로 derivative를 보면,

45
00:03:25,518 --> 00:03:30,340
가로방향에서보다 세로방향으로 더 큽니다.

46
00:03:30,340 --> 00:03:33,720
그러면 기울기는 b방향으로 매우 큽니다. 맞죠?

47
00:03:33,720 --> 00:03:40,790
이런 derivative로, db의 값은 매우 크고,
dw의 값은 비교적 작은데요,

48
00:03:40,790 --> 00:03:45,350
함수가 세로 방향으로 더 기울기가 크게 나오는데요,

49
00:03:45,350 --> 00:03:50,870
b방향보다, w방향보다,
가로 방향보다 말이죠.

50
00:03:50,870 --> 00:03:53,008
그러므로 db의 제곱은 비교적 큰 값이 될 것입니다.

51
00:03:53,008 --> 00:03:58,010
또 Sdb가 상대적으로 클텐데요,
반면에 이 값고 비교하면 dW는 더 작을 것입니다.

52
00:03:58,010 --> 00:04:02,080
또는 dW 제곱은 더 작거나,
SdW가 더 작을 것입니다.

53
00:04:02,080 --> 00:04:06,600
이것의 순수효과는 세로방향의 업데이트는

54
00:04:06,600 --> 00:04:11,230
훨씬 더 큰 숫자로 나누어지기 때문에,
이것이 변동폭을 조금 더 무디게 만드는데 도와줍니다.

55
00:04:11,230 --> 00:04:15,440
반면에 가로방향의 업데이트는
더 작은 숫자로 나누어집니다.

56
00:04:15,440 --> 00:04:19,470
그렇기 때문에 RMSprop을
사용함으로써 생기는 순수영향은

57
00:04:19,470 --> 00:04:20,750
업데이트가 이렇게 생기게 되는 것입니다.

58
00:04:22,750 --> 00:04:27,587
이것은 세로방향의 업데이트이고요,

59
00:04:27,587 --> 00:04:32,370
가로방향은 계속 진행하시면 됩니다.

60
00:04:32,370 --> 00:04:36,890
이것의 효과 1가지는
그러므로 여러분이 더 큰 러닝속도

61
00:04:36,890 --> 00:04:41,540
알파를 이용해도 됩니다. 그리하여
세로 방향으로 갈리지 않으면서
더 빠른 러닝을 진행할 수 있죠.

62
00:04:41,540 --> 00:04:45,223
명료성을 위해서,
세로와

63
00:04:45,223 --> 00:04:48,348
가로 방향을 각각 w와 b로
이렇게 나타냈는데요,

64
00:04:48,348 --> 00:04:53,188
실제로는 아주 고차원의 파라미터
공간에 있는 것인데요,

65
00:04:53,188 --> 00:04:57,383
그렇기 때문에 변동을

66
00:04:57,383 --> 00:05:01,757
무디게 하려고 하는 세러방향 다이멘션은
w1, w2, w17의 파라미터 세트의 합입니다.

67
00:05:01,757 --> 00:05:07,223
그리고 가로 다이멘션는
w3, w4 등등 이죠. 맞죠.

68
00:05:07,223 --> 00:05:11,150
그러므로, w 와 p 가있다라는 거는 일러스트레이션뿐입니다.

69
00:05:11,150 --> 00:05:15,330
실제로 dw는 굉장히 고차원의 파라미터 벡터입니다.

70
00:05:15,330 --> 00:05:18,620
Db 또한 고차원의 파라미터 벡터입니다.

71
00:05:18,620 --> 00:05:22,830
하지만 직관적인 부분은,
이런 변동이 있는 부분의 다이멘션에서는

72
00:05:22,830 --> 00:05:26,570
이렇게 더 큰 합을 산출하게 됩니다.

73
00:05:26,570 --> 00:05:29,406
여기 derivative의 제곱의 가중평균값 말이죠.

74
00:05:29,406 --> 00:05:33,080
그렇게하여 변동을 무디게 합니다.

75
00:05:33,080 --> 00:05:39,680
이것이 RMSprop인데요, 이것은 root mean squared prop의 약자입니다.

76
00:05:39,680 --> 00:05:44,110
여기서는 derivative을 제곱하고, 여기서는
마지막부분에서 루트를 적용하기 때문입니다.

77
00:05:44,110 --> 00:05:48,560
마지막으로, 몇가지 알고리즘에 대한 상세내용에 대해
다음으로 넘어가기 전에 말씀드리겠습니다.

78
00:05:49,870 --> 00:05:55,420
다음 비디오에서는, 저희는 RMSprop과 모멘텀을 합칠 것입니다.

79
00:05:55,420 --> 00:06:00,540
그렇기 때문에, 모멘텀에서 썼던
하이퍼 파라미터 베타를 쓰는 대신에,

80
00:06:00,540 --> 00:06:05,188
여기 하이퍼 파라미터 베타2라고 부를 것입니다.
충돌하게 하지 않기 위해서요.

81
00:06:05,188 --> 00:06:09,350
모멘텀과 RMSprop 2개 동시에 쓰이는
하이퍼 파라미터 입니다.

82
00:06:09,350 --> 00:06:13,540
또, 알고리즘이 0으로 나뉘지 않게 하기 위한 방법입니다.

83
00:06:13,540 --> 00:06:17,910
만약 루트 Sdw가 0가 매우 가까우면 어떨까요?

84
00:06:17,910 --> 00:06:19,730
그러면 여기 이 부분이 폭발적인 값을 가질 것입니다.

85
00:06:19,730 --> 00:06:24,320
숫자적으로 안정적이게 하기 위해서,
실제로는 여기 이것을 도입할때,

86
00:06:24,320 --> 00:06:28,200
분모에 아주 작은 앱실론의 값을 더합니다.

87
00:06:28,200 --> 00:06:30,760
어떤 앱실론이 쓰여도 사실 상관은 없습니다.

88
00:06:30,760 --> 00:06:34,948
10의 -8승이 합리적인 기본값인데요,
이 값은 조금 더 안정적인 수치를

89
00:06:34,948 --> 00:06:39,202
제공합니다. 숫자의 반올림이나 다른 이유에서 말이죠.

90
00:06:39,202 --> 00:06:43,030
여러분이 너무 작은 숫자로 나누지 않게 하기 위한
방편입니다.

91
00:06:43,030 --> 00:06:47,870
이것이 RMSprop인데요,
모멘텀과 비슷하게,

92
00:06:47,870 --> 00:06:52,910
변동을 무디게하는 효과가 있습니다. 기울기 강하, 미니 배치 기울기 강하 에서 말이죠.

93
00:06:52,910 --> 00:06:56,510
여러분이 조금 더 큰 러닝속도 알파 값을 사용할 수 있게하고,

94
00:06:56,510 --> 00:07:01,920
여러분의 알고리즘 러닝속도를 높혀줍니다.

95
00:07:01,920 --> 00:07:05,350
자 이제 여러분은 RMSprop을 도입하는 방법을 알게되었는데요,

96
00:07:05,350 --> 00:07:07,920
이제 러닝 알고리즘의 속도를 높히는 방법을 배웠습니다.

97
00:07:07,920 --> 00:07:09,554
RMSprop의 한가지 흥미로운 점은

98
00:07:09,554 --> 00:07:13,572
이것이 논문으로 통해 처음에 발표된 것이 아니라

99
00:07:13,572 --> 00:07:17,947
몇년 전에 Jeff Hinton이라는 분을 통해
Courera 코스에서 발표되었다는 점입니다.

100
00:07:17,947 --> 00:07:22,108
학술적인 리서치 분야에서 보급 역할을 하는 것이
Coursera의 의도는 아니였지만

101
00:07:22,108 --> 00:07:26,214
이 경우엔, 잘 된 경우라고 볼 수도 있겠네요.

102
00:07:26,214 --> 00:07:30,126
그래서 Coursera 코스를 통해
RMSprop이 광범위하게

103
00:07:30,126 --> 00:07:31,790
쓰이는 계기가 되었고 사람들에게 알려지고 뜨기 시작했습니다.

104
00:07:31,790 --> 00:07:32,970
저희는 모멘텀에 대해서 이야기 했습니다.

105
00:07:32,970 --> 00:07:34,330
RMSprop에 대해서도 이야기했죠.

106
00:07:34,330 --> 00:07:37,970
이것들을 같이 합치면 더 좋은

107
00:07:37,970 --> 00:07:39,530
최적 알고리즘이 나오는데요,

108
00:07:39,530 --> 00:07:41,040
이 내용은 다음 비디오에서 이야기하겠습니다.