1
00:00:00,000 --> 00:00:02,340
딥러닝의 역사를 보면,

2
00:00:02,340 --> 00:00:05,700
아주 유명한 리서치 연구원을 포함하여 수 많은 리서치 연구원들은

3
00:00:05,700 --> 00:00:07,790
최적화 알고리즘을 간혹 제안하여

4
00:00:07,790 --> 00:00:09,825
해당 알고리즘이 문제해결에 잘 쓰이는 것을 보여주었습니다.

5
00:00:09,825 --> 00:00:13,440
이런 최적화 알고리즘은 여러분이 트레이닝 하고 싶은 넓은 범위의 신경만 네트워크를

6
00:00:13,440 --> 00:00:18,130
잘 일반화시키지 못하는 것으로 보여지게 됩니다.

7
00:00:18,130 --> 00:00:21,360
그렇게해서 점차 시간이 지나면서 딥러닝 커뮤니티는 전반적으로 최적화 알고리즘에 대해

8
00:00:21,360 --> 00:00:25,597
어느정도 회의적인 시각을 갖게된 것 같습니다.

9
00:00:25,597 --> 00:00:29,350
또한 많은 사람들이 기울기 강하 와 모멘텀이 아주 잘 작동한다고 생각해서

10
00:00:29,350 --> 00:00:32,720
이보다 더 잘 작동하는 것을 제안하는 것이 쉽지 않았습니다.

11
00:00:32,720 --> 00:00:36,070
이번 비디오에서는 RmsProp과 Adam 최적화

12
00:00:36,070 --> 00:00:37,730
알고리즘에 대해 이야기 해볼텐데요

13
00:00:37,730 --> 00:00:41,460
이 알고리즘은 드물게 잘 작동하는 알고리즘이였는데요,

14
00:00:41,460 --> 00:00:47,250
넓은 범위의 딥러닝 구조에서도 굉장히 잘 작동하는 것으로 보여졌습니다.

15
00:00:47,250 --> 00:00:50,150
이 알고리즘은 제가 망설이지 않고 여러분께 시도해보라고 추천드리는 알고리즘인데요

16
00:00:50,150 --> 00:00:54,625
그 이유는 많은 사람들이 시도하였고 또 잘 작동하는 것을 직접 보았습니다.

17
00:00:54,625 --> 00:00:57,720
Adam 알고리즘은 기본적으로 모멘텀과 RmsProp을

18
00:00:57,720 --> 00:01:01,250
같이 합치는 것이라고 생각하면 됩니다.

19
00:01:01,250 --> 00:01:03,105
이것이 어떻게 되는지 한번 보겠습니다.

20
00:01:03,105 --> 00:01:05,695
Adam을 도입하기 위해서는 초기화를 진행할텐데요.

21
00:01:05,695 --> 00:01:15,877
Vdw는 0, Sdw도 0, 그리고 비슷하게 Vdb와 Sdb도 0으로 지정합니다.

22
00:01:15,877 --> 00:01:19,810
그리고 반복 루프 t회째에 대해서

23
00:01:19,810 --> 00:01:30,170
미분을 계산할텐데요.
dw와 db를 현재 미니 배치를 이용해 계산하고

24
00:01:30,170 --> 00:01:33,775
보통, 미니 배치 기울기 강하를 이용하면 되고요.

25
00:01:33,775 --> 00:01:41,480
그리고 모멘텀, 지수 가중 평균을 쓸텐데요.
Vdw = ß인데,

26
00:01:41,480 --> 00:01:46,410
이제부터는 이 값을 ß1이라고 쓰고,
RmsProp의 하이퍼 파라미터 ß는

27
00:01:46,410 --> 00:01:52,660
ß2로 구분해서 사용하겠습니다.

28
00:01:52,660 --> 00:01:58,180
자, 이것은 모멘텀을 구현할 때 했던 것과 완전히 동일한데요

29
00:01:58,180 --> 00:02:03,788
유일한 차이는 하이퍼 파라미터 ß 대신에 ß1이라고 부른 점입니다.

30
00:02:03,788 --> 00:02:14,312
유사하게 Vdb 는 이렇게, 1 빼기 ß1 곱하기 db죠.

31
00:02:14,312 --> 00:02:18,685
그리고 RmsProp도 업데이트 하는데요.

32
00:02:18,685 --> 00:02:26,630
이제 하이퍼 파라미터는 ß2죠. 여기는 플러스 1 빼기 ß2 dw²

33
00:02:26,630 --> 00:02:33,325
여기서 제곱은 미분 dw를 원소별로 제곱하는 것이고요.

34
00:02:33,325 --> 00:02:44,005
그리고 Sdb는 이것 더하기 1빼기 ß2 곱하기 db입니다.

35
00:02:44,005 --> 00:02:49,145
이 하이퍼 파라미터 ß1으로 모멘텀과 같은 식으로 업데이트를 하고,

36
00:02:49,145 --> 00:02:55,318
하이퍼 파라미터 ß2 는 RmsProp과 같은 식의 업데이트죠.

37
00:02:55,318 --> 00:02:58,599
일반적으로 Adam 구현에는,

38
00:02:58,599 --> 00:03:01,255
편향보정도 같이 합니다.

39
00:03:01,255 --> 00:03:04,215
그러므로 v corrected 를 사용할텐데,

40
00:03:04,215 --> 00:03:06,705
Corrected(보정된)는 편향 보정된 것을 뜻 합니다.

41
00:03:06,705 --> 00:03:16,244
dw는 vdw 나누기 1 빼기 ß1 의 t승, t회째 반복 루프를 진행한 경우에 말이죠.

42
00:03:16,244 --> 00:03:25,040
비슷하게, vdb corrected는 vdb 나누기 1 빼기 ß1 의 t승입니다.

43
00:03:25,040 --> 00:03:30,756
그리고 또 비슷하게, S에도 편향보정을 적용해서

44
00:03:30,756 --> 00:03:37,405
즉, sdw 나누기 1 빼기 ß2 의 T승, 그리고 sdb corrected는

45
00:03:37,405 --> 00:03:48,700
sdb 나누기 1 빼기 ß2 의 T승입니다.

46
00:03:48,700 --> 00:03:50,660
마지막으로 업데이트를 진행합니다.

47
00:03:50,660 --> 00:03:55,060
그러면 W는 W 빼기 알파 곱하기,

48
00:03:55,060 --> 00:03:59,870
만약 모멘텀만 사용하는 경우 Vdw를 사용하거나,

49
00:03:59,870 --> 00:04:03,408
Vdw corrected가 될 수도 있겠죠.

50
00:04:03,408 --> 00:04:06,615
이제 RmsProp에 해당하는 부분도 추가하는데요.

51
00:04:06,615 --> 00:04:13,390
그래서, Sdw corrected + 앱실론의
루트로 나눠줍니다.

52
00:04:13,390 --> 00:04:18,232
마찬가지로, 비슷한 공식으로 b 또한 업데이트 될텐데요.

53
00:04:18,232 --> 00:04:24,070
Vdb corrected 나누기 루트 S corrected

54
00:04:24,070 --> 00:04:28,595
db 더하기 앱실론입니다.

55
00:04:28,595 --> 00:04:33,070
이 알고리즘은 기울기 강하에 모멘텀 효과와 함께 

56
00:04:33,070 --> 00:04:37,572
RmsProp을 같이 결합한 것입니다.

57
00:04:37,572 --> 00:04:41,740
이게 흔히 자주 쓰이는 학습 알고리즘인데요.
아주 다양한 신경망 네트워크

58
00:04:41,740 --> 00:04:46,640
광범위한 구조들에 매우 효과가 있음이 증명되었습니다.

59
00:04:46,640 --> 00:04:49,805
이 알고리즘은 몇개의 하이퍼 파라미터들이 있는데요.

60
00:04:49,805 --> 00:04:57,330
러닝 속도인 하이퍼 파라미터 알파α는 여전히 중요하고, 보통 튜닝이 필요합니다.

61
00:04:57,330 --> 00:05:01,675
그러므로 여러분이 다양한 범위의 값을 시도해서 어떤 값이 적합한지 찾아야 합니다.

62
00:05:01,675 --> 00:05:06,090
ß1의 가장 흔한 설정 값은 0.9입니다.

63
00:05:06,090 --> 00:05:08,065
이것은 이동 평균값인데요.

64
00:05:08,065 --> 00:05:12,220
dw의 가중평균값인데요, 이것은 모멘텀 같은 성분이고,

65
00:05:12,220 --> 00:05:15,455
하이퍼 파라미터 ß2는,

66
00:05:15,455 --> 00:05:16,950
Adam 논문의 저자,

67
00:05:16,950 --> 00:05:20,014
Adam 알고리즘 발명자는 0.999를 권장하였습니다.

68
00:05:20,014 --> 00:05:26,485
이것은 dw²과 또, db²의 이동평균
(지수 가중 평균)들을 계산하는 것입니다.

69
00:05:26,485 --> 00:05:31,030
그리고, 앱실론ε 값 선택은 그리 중요하지 않습니다.

70
00:05:31,030 --> 00:05:34,755
Adam 논문의 저자들은 10의 마이너스 8승을 권장합니다만,

71
00:05:34,755 --> 00:05:38,230
이 파라미터는 정말로

72
00:05:38,230 --> 00:05:42,555
굳이 바꿀 필요가 없고, 성능에도 거의 영향이 없기 때문이죠.

73
00:05:42,555 --> 00:05:44,280
Adam을 구현할 때는,

74
00:05:44,280 --> 00:05:47,030
사람들은 보통 기본값을 많이 사용합니다.

75
00:05:47,030 --> 00:05:49,960
즉, ß1, 그리고 ß2 또한 ε도 쓰죠.

76
00:05:49,960 --> 00:05:52,300
제 생각에 실제로 앱실론ε을 튜닝하는 사람은 없습니다.

77
00:05:52,300 --> 00:05:56,335
그리고나서, 여러가지 범위의 알파α 값으로 바꿔보며,
어떤게 가장 잘 동작하는지 해 보세요.

78
00:05:56,335 --> 00:05:59,140
물론, ß1과 ß2도 튜닝을 할 수 있습니다만,

79
00:05:59,140 --> 00:06:02,440
전문적으로 일하시는 분들 중엔
자주 하는 않는 것으로 알고 있습니다.

80
00:06:02,440 --> 00:06:06,100
그러면, Adam의 뜻은 어디서 유래된 것일까요?

81
00:06:06,100 --> 00:06:15,267
Adam은 "적응 모멘트 추정"
(Adaptive Moment Estimation)을 뜻합니다.

82
00:06:15,267 --> 00:06:18,175
ß1은 미분의 평균값을 산출하는데요.

83
00:06:18,175 --> 00:06:19,780
이것을 1차 모멘트라고 합니다.

84
00:06:19,780 --> 00:06:21,975
그리고 ß2는

85
00:06:21,975 --> 00:06:25,830
dW²의 지수이동평균에 사용되는데, 이것은 2차 모멘트라고 합니다.

86
00:06:25,830 --> 00:06:29,380
이런 이유에서 adaptive moment estimation이라는 이름이 나오게 됐습니다.

87
00:06:29,380 --> 00:06:32,875
하지만 대부분의 사람들은 그냥 "아담 최적화 알고리즘"이라고 부릅니다.

88
00:06:32,875 --> 00:06:37,800
아, 그리고 저와 오랜전부터 친구이고 같이 일한 Adam Coates인데요.

89
00:06:37,800 --> 00:06:40,425
제가 아는 한, 이 알고리즘은 이 친구와는 연관된 것이 없습니다.

90
00:06:40,425 --> 00:06:43,525
이 친구가 가끔 이 알고리즘을 쓴다는 점 이외에는요.

91
00:06:43,525 --> 00:06:45,847
이 질문을 가끔씩 주위에서 하셔서요.

92
00:06:45,847 --> 00:06:47,945
혹시나 여러분이 궁금해하실까봐 말씀드렸습니다.

93
00:06:47,945 --> 00:06:51,187
Adam 최적화 알고리즘에 대한 부분은 이게 전부인데요.

94
00:06:51,187 --> 00:06:54,435
이런 내용을 바탕으로 이제 여러분께서는 더 빨리 신경망을 트레이닝 시키실 수 있을 것입니다.

95
00:06:54,435 --> 00:06:56,055
이번주 강의를 마치기에 앞서,

96
00:06:56,055 --> 00:06:58,950
하이퍼 파라미터 튜닝에 대해 이야기 해보겠습니다.

97
00:06:58,950 --> 00:07:01,465
또한 신경망과 관련된 최적화 문제가 어떻게 생긴 것인지

98
00:07:01,465 --> 00:07:04,230
직관적인 부분도 다루어 보도록 하겠습니다.

99
00:07:04,230 --> 00:07:07,260
다음 비디오에서는 learning rate decay에 대한 내용을 다루어보겠습니다.