1
00:00:00,320 --> 00:00:04,590
在上一节中 你学习了如何使用小批量梯度下降

2
00:00:04,590 --> 00:00:08,370
以及在仅部分处理训练集数据时

3
00:00:08,370 --> 00:00:11,960
运行梯度下降算法

4
00:00:11,960 --> 00:00:16,282
在这一节中 你将进一步学习如何使用梯度下降

5
00:00:16,282 --> 00:00:19,896
并进一步学习它究竟在做什么以及为什么有效

6
00:00:19,896 --> 00:00:24,481
在批量梯度下降算法中
每一次迭代你将遍历整个训练集

7
00:00:24,481 --> 00:00:29,380
并希望代价函数的值随之不断减小

8
00:00:30,660 --> 00:00:33,390
如果我们用 J 来表示代价函数

9
00:00:33,390 --> 00:00:37,500
那么它应该随着迭代单调递减

10
00:00:37,500 --> 00:00:40,730
如果某一次迭代它的值增加了
那么一定是哪里错了

11
00:00:40,730 --> 00:00:43,250
也许是你的学习率太大

12
00:00:43,250 --> 00:00:48,090
而在小批量梯度下降中
同样画图就会发现

13
00:00:48,090 --> 00:00:51,660
并不是每一次迭代代价函数的值都会变小

14
00:00:51,660 --> 00:00:56,822
从细节来看 每次迭代

15
00:00:56,822 --> 00:01:01,425
都是对X{t} Y{t}的处理

16
00:01:01,425 --> 00:01:05,888
所以对通过它们计算出来的代价函数值 J{t}

17
00:01:05,888 --> 00:01:11,490
进行画图

18
00:01:11,490 --> 00:01:17,170
这就好像每次迭代你都使用不同的训练集

19
00:01:17,170 --> 00:01:19,380
也就是使用不同的小块 (mini-batch)

20
00:01:19,380 --> 00:01:20,770
所以如果你对代价函数 J 画图

21
00:01:20,770 --> 00:01:23,310
你就会看到类似这样的

22
00:01:23,310 --> 00:01:27,479
它的趋势是向下的 但是也会有许多噪声

23
00:01:30,554 --> 00:01:35,692
如果使用小批量梯度下降算法 经过几轮训练后

24
00:01:35,692 --> 00:01:40,670
对 J{t} 作图结果很可能就像这样

25
00:01:40,670 --> 00:01:44,284
它并不一定每次迭代都会下降

26
00:01:44,284 --> 00:01:46,783
但是整体趋势必须是向下的

27
00:01:46,783 --> 00:01:51,281
而它之所以有噪声

28
00:01:51,281 --> 00:01:56,527
可能和计算代价函数时使用的
那个批次X{t} Y{t}有关

29
00:01:56,527 --> 00:02:02,057
让你的代价函数的值或大或小

30
00:02:02,057 --> 00:02:04,356
也可能这个批次里含有一些标签标错的数据

31
00:02:04,356 --> 00:02:06,780
导致代价函数有一些高 等等

32
00:02:06,780 --> 00:02:09,511
这就是为什么在使用小批量梯度下降时

33
00:02:09,511 --> 00:02:13,277
得到的代价函数图像有这样的震动

34
00:02:13,277 --> 00:02:18,070
你必须定义的一个参数是mini-batch的大小

35
00:02:18,070 --> 00:02:22,894
如果m是训练集的大小 
一个极端的情况是

36
00:02:26,544 --> 00:02:31,056
mini-batch的大小就等于m
这样其实就是批量梯度下降

37
00:02:36,056 --> 00:02:41,068
在这种情况下你的mini-batch
只有一个X{1}和Y{1}

38
00:02:41,068 --> 00:02:45,720
而它就等于你的整个训练集

39
00:02:45,720 --> 00:02:49,830
所以如果把mini-batch的大小设置成m
你就得到了批量梯度下降

40
00:02:49,830 --> 00:02:57,669
另一极端情况是把mini-batch的大小设为1

41
00:02:59,752 --> 00:03:03,238
就会得到一种叫随机梯度下降的算法

42
00:03:07,385 --> 00:03:16,076
这里每一条数据就是一个mini-batch

43
00:03:18,429 --> 00:03:24,172
在这种情况下 先看第一个mini-batch
X{1} Y{1}

44
00:03:24,172 --> 00:03:29,682
因为mini-batch的大小是1
所以其实就是训练数据的第一个样本

45
00:03:29,682 --> 00:03:34,620
你对此运行梯度下降算法

46
00:03:34,620 --> 00:03:39,810
然后 第二个mini-batch

47
00:03:39,810 --> 00:03:43,280
其实就是第二个样本
对它们再运行梯度下降算法

48
00:03:43,280 --> 00:03:45,170
然后再用第三个样本做类似的工作

49
00:03:45,170 --> 00:03:47,940
每次都只使用一组训练数据

50
00:03:50,100 --> 00:03:55,840
下面让我们看一下这两种方法
在优化代价函数时有什么不同

51
00:03:55,840 --> 00:03:59,795
这是你想要最小化的代价函数的等高线图

52
00:03:59,795 --> 00:04:01,067
你的最小值在这里

53
00:04:01,067 --> 00:04:05,825
批量梯度下降算法可能从这里开始

54
00:04:05,825 --> 00:04:12,320
它的噪声相对小些 每一步相对大些

55
00:04:12,320 --> 00:04:15,600
并且最终可以达到最小值

56
00:04:15,600 --> 00:04:19,290
而相对的 随机梯度下降算法

57
00:04:19,290 --> 00:04:22,430
让我们选一个不同的点 假使从这里开始

58
00:04:22,430 --> 00:04:26,180
这时对于每一次迭代你就在一个样本上做梯度下降

59
00:04:26,180 --> 00:04:30,080
大多数时候你可以达到全局最小值

60
00:04:30,080 --> 00:04:33,865
但是有时候也可能因为某组数据不太好

61
00:04:33,865 --> 00:04:36,303
把你指向一个错误的方向

62
00:04:36,303 --> 00:04:40,530
因此随机梯度算法的噪声会非常大

63
00:04:40,530 --> 00:04:45,070
一般来说它会沿着正确的方向

64
00:04:45,070 --> 00:04:47,116
但是有事也会指向错误的方向

65
00:04:47,116 --> 00:04:50,190
而且随机梯度下降算法
最后也不会收敛到一个点

66
00:04:50,190 --> 00:04:54,760
它一般会在最低点附近摆动

67
00:04:54,760 --> 00:04:58,006
但是不会达到并且停在那里

68
00:04:58,006 --> 00:05:03,320
实际上 mini-batch的大小一般会在这2个极端之间

69
00:05:07,976 --> 00:05:15,100
一个在1和m之间的值
因为1和m都太小或太大了

70
00:05:15,100 --> 00:05:16,199
以下是原因

71
00:05:16,199 --> 00:05:23,844
如果你使用批量梯度下降算法

72
00:05:23,844 --> 00:05:27,386
你的mini-batch大小就是m

73
00:05:30,878 --> 00:05:35,190
那么你将在每一次迭代中遍历整个训练集

74
00:05:35,190 --> 00:05:40,101
这样做最大的缺点是
如果你的训练集非常大

75
00:05:40,101 --> 00:05:43,860
就将在每一次迭代上花费太长的时间

76
00:05:43,860 --> 00:05:46,792
如果你的训练集比较小
那它还是一个不错的选择

77
00:05:46,792 --> 00:05:51,200
相反 如果你使用随机梯度下降算法

78
00:05:54,076 --> 00:05:58,967
使用一个样本来更新梯度

79
00:05:58,967 --> 00:06:02,030
这没有问题

80
00:06:02,030 --> 00:06:04,290
而且可以通过选择比较小的学习率

81
00:06:04,290 --> 00:06:07,378
来减少噪声

82
00:06:07,378 --> 00:06:12,160
但随机梯度下降有一个很大的缺点是

83
00:06:12,160 --> 00:06:17,050
你失去了可以利用向量加速运算的机会

84
00:06:18,370 --> 00:06:22,050
因为这里你每次只处理一个训练数据

85
00:06:22,050 --> 00:06:26,130
这是非常没有效率的

86
00:06:26,130 --> 00:06:32,380
所以更好的做法是选择一个中间值

87
00:06:36,687 --> 00:06:40,360
让mini-batch的大小既不太大也不太小

88
00:06:44,439 --> 00:06:48,630
这样你的训练才能最快

89
00:06:51,405 --> 00:06:54,860
这么做有两个好处

90
00:06:54,860 --> 00:06:58,174
第一你可以使用向量的方式运算

91
00:06:58,174 --> 00:07:02,667
在上一节的例子中
如果你的mini-batch大小是1000

92
00:07:02,667 --> 00:07:07,669
你就可以用一个向量同时处理这1000个样本

93
00:07:07,669 --> 00:07:12,110
这就比一个个的处理要快许多

94
00:07:13,670 --> 00:07:16,710
第二你可以不用等待整个训练集

95
00:07:22,210 --> 00:07:27,710
都遍历完一遍才运行梯度下降

96
00:07:32,430 --> 00:07:36,719
还是用前一个视频中的数字

97
00:07:36,719 --> 00:07:40,640
每遍历一遍训练集
可以执行5000次梯度下降算法

98
00:07:41,840 --> 00:07:46,370
所以在实践中这样选择mini-batch的大小是最好的

99
00:07:46,370 --> 00:07:49,380
如果使用小批量梯度下降算法 我们从这里开始

100
00:07:49,380 --> 00:07:53,670
可能第一次迭代是这样 第二次 第三次 第四次

101
00:07:53,670 --> 00:07:58,521
它并不能保证总是可以达到最小值

102
00:07:58,521 --> 00:08:03,383
但是相比随机梯度下降 它的噪声会更小

103
00:08:03,383 --> 00:08:08,320
而且它不会总在最小值附近摆动

104
00:08:08,320 --> 00:08:11,550
如果有什么问题 你可以缓慢的减小学习率

105
00:08:11,550 --> 00:08:13,410
我们会在下一节中介绍学习率衰减

106
00:08:13,410 --> 00:08:15,960
以及如何降低学习率

107
00:08:15,960 --> 00:08:20,020
如果mini-batch的大小既不能是m也不能是1

108
00:08:20,020 --> 00:08:23,410
而是在它们之间的一个值 那么该如何选择呢？

109
00:08:23,410 --> 00:08:24,826
这里有一些准则

110
00:08:24,826 --> 00:08:33,470
第一 如果你的训练集较小
就使用批量梯度下降算法

111
00:08:36,655 --> 00:08:41,023
这时候没有理由使用小批量梯度下降

112
00:08:41,023 --> 00:08:43,670
因为你可以快速的处理整个训练集

113
00:08:43,670 --> 00:08:45,619
所以使用批量梯度下降算法是没问题的

114
00:08:45,619 --> 00:08:50,281
较小的定义我觉得是小于2000

115
00:08:50,281 --> 00:08:54,480
这时使用批量梯度下降是非常适合的

116
00:08:54,480 --> 00:09:00,391
否则 如果你有个更大的训练集

117
00:09:03,336 --> 00:09:09,437
一般选择64到512作为mini-batch的大小

118
00:09:09,437 --> 00:09:14,130
这是因为计算机内存的布局和访问方式

119
00:09:14,130 --> 00:09:19,460
所以把mini-batch的大小设置为2的幂数
你的代码会运行的快一些

120
00:09:19,460 --> 00:09:24,108
就像64是2的6次方 2的7次方 2的8次方

121
00:09:24,108 --> 00:09:30,080
2的9次方 所以一般我会把mini-batch的大小
设置成2的幂数

122
00:09:30,080 --> 00:09:33,900
我在上一节中我的mini-batch大小是1000

123
00:09:33,900 --> 00:09:37,990
但我建议你就使用1024

124
00:09:37,990 --> 00:09:39,870
2的10次方

125
00:09:39,870 --> 00:09:46,176
但是1024这个值还是比较罕见的

126
00:09:46,176 --> 00:09:50,681
而这些值更常用些

127
00:09:50,681 --> 00:09:54,980
最后一个提醒是确保你的mini-batch

128
00:09:57,260 --> 00:10:05,309
你所有的X{t} Y{t}
是可以放进你CPU/GPU 内存的

129
00:10:08,563 --> 00:10:10,863
当然这和你的配置

130
00:10:10,863 --> 00:10:12,800
以及一个训练样本的大小都有关系

131
00:10:12,800 --> 00:10:17,430
但是如果你使用的mini-batch超过了

132
00:10:17,430 --> 00:10:20,640
CPU/GPU 内存的容量 不管你怎么做

133
00:10:20,640 --> 00:10:24,336
你都会发现

134
00:10:24,336 --> 00:10:25,809
结果会突然变得很糟

135
00:10:25,809 --> 00:10:30,273
我希望以上说的能让你对
mini-batch大小的标准范围

136
00:10:30,273 --> 00:10:31,790
有更好的了解

137
00:10:31,790 --> 00:10:35,970
当然mini-batch的大小也是一个超参数

138
00:10:35,970 --> 00:10:40,840
可能你要做一个快速的搜索去确定哪一个值

139
00:10:40,840 --> 00:10:43,960
可以让代价函数J下降的最快

140
00:10:43,960 --> 00:10:47,065
所以我的做法是尝试几个不同的值

141
00:10:47,065 --> 00:10:51,727
尝试几个不同的2的幂数
然后看能否找到那个

142
00:10:51,727 --> 00:10:56,470
让你的梯度下降算法尽可能效率的值

143
00:10:56,470 --> 00:10:59,940
希望这能给你一些指导在

144
00:10:59,940 --> 00:11:03,405
对这个超参的选择上

145
00:11:03,405 --> 00:11:07,012
现在你知道了如何使用小批量梯度下降算法

146
00:11:07,012 --> 00:11:10,378
并且在大训练集时让你的算法跑的更快

147
00:11:10,378 --> 00:11:12,936
但是事实上还有一些比梯度下降

148
00:11:12,936 --> 00:11:15,805
或者小批量梯度下降更高效的算法

149
00:11:15,805 --> 00:11:18,215
我们将在下面几讲介绍它们