1
00:00:00,390 --> 00:00:04,770
모멘텀 또는 기울기 하강와 모멘텀 이라고 하는
알고리즘이 있는데요,

2
00:00:04,770 --> 00:00:09,600
일반적인 기울기 하강 알고리즘보다 거의 항상
더 빨리 작동합니다.

3
00:00:09,600 --> 00:00:14,100
한 문장으로 말하자면, 기본 아이디어는
기울기의 기하급수적 가중 평균치를 산출하는 것인데요

4
00:00:14,100 --> 00:00:18,850
그 다음에, 이 기울기를 이용해서 weights를
업데이트 하는 것입니다.

5
00:00:18,850 --> 00:00:22,010
이번 비디오에서는, 한 문장으로 표현한 이 설명을
풀어서 해석하고

6
00:00:22,010 --> 00:00:23,848
어떻게 도입하는지 한번 보겠습니다.

7
00:00:23,848 --> 00:00:28,619
하나의 예로, 이런 곡선을 가진

8
00:00:28,619 --> 00:00:30,510
비용함수를 최적화시키려고 한다고 해봅시다.

9
00:00:30,510 --> 00:00:34,350
빨간점은 최소점의 위치를 나타냅니다.

10
00:00:34,350 --> 00:00:39,307
아마 여기서 기울기 강하를 시작할 수도 있는데요,
1번의 iteration을 기울기 강하나

11
00:00:39,307 --> 00:00:44,670
미니 배치 기울기 강하 의 경우
이곳으로 향하게 됩니다.

12
00:00:44,670 --> 00:00:47,370
하지만 이제는 타원에서 반대편에 있게 되는데요,

13
00:00:47,370 --> 00:00:51,810
그리고 또 다른 기울기 강하의 단계를 거치면
이렇게 하게 됩니다.

14
00:00:51,810 --> 00:00:55,590
또 추가 단계, 그 다음 단계 등등 말이죠.

15
00:00:55,590 --> 00:01:00,460
이렇게 해서 기울기 강하가 여러차례를 거치게 되는 것을 보실텐데요. 그쵸?

16
00:01:00,460 --> 00:01:07,190
천천히 최소값을 향해 왔다갔다하면서 접근하게 됩니다.

17
00:01:07,190 --> 00:01:11,206
그리고 이러한 왔다갔다하는 변동폭이 기울기 강하를
느리게 합니다.

18
00:01:11,206 --> 00:01:14,500
그리하여 더 큰 러닝속도를 쓰지 않게 하고 말이죠.

19
00:01:14,500 --> 00:01:19,226
특히, 만약 훨씬 더 큰 러닝속도를 쓰는 경우엔,

20
00:01:19,226 --> 00:01:21,533
결과적으로 오버슈팅을하거나 갈라지게 할 수 있습니다.

21
00:01:21,533 --> 00:01:25,826
그렇기 때문에 변동이 너무 크게 왔다갔다 하지 않게 하려는 것은

22
00:01:25,826 --> 00:01:29,650
러닝 속도를 너무 큰 값을 사용하지 않게 강요할 것입니다.

23
00:01:29,650 --> 00:01:34,120
이 문제를 보는 또 다른 시각은 바로 세로축에서 입니다.

24
00:01:34,120 --> 00:01:38,990
이 경웨 러닝속도가 조금 늦길 바랄텐데요,
이 변동이 싫기 때문이죠.

25
00:01:38,990 --> 00:01:43,701
하지만 가로축의 경우에는 빠른 러닝을 원할 것입니다.

26
00:01:45,552 --> 00:01:48,831
그 이유는 더 공격적으로 왼쪽에서 오른쪽으로 이동하길 바라기 때문입니다.

27
00:01:48,831 --> 00:01:51,910
저기 빨간색 점인 최소값을 향해 말이죠.

28
00:01:51,910 --> 00:01:55,621
그러면 여러분이 기울기 강하 와 모멘텀을 도입할때
할 수 있는 것들입니다.

29
00:01:58,542 --> 00:02:03,611
iteration 마다, 더 상세하게,

30
00:02:03,611 --> 00:02:11,562
iteration t마다, 기본적인 derivative dw, db를 산출할 것입니다.

31
00:02:11,562 --> 00:02:15,834
저는 위첨자 대괄호 L은 생략하겠습니다.

32
00:02:15,834 --> 00:02:19,940
결국 현재의 미니 배치 에서 dw와 db를 계산하는 것입니다.

33
00:02:19,940 --> 00:02:21,550
그리고 여러분이 배치 기울기 강하를 쓰는 경우,

34
00:02:21,550 --> 00:02:24,200
현재 미니 배치를 그냥 전체 배치 일 것입니다.

35
00:02:24,200 --> 00:02:26,670
그리고 이것은 배치 기울기 강하의 경우 잘 작동합니다.

36
00:02:26,670 --> 00:02:29,580
그러므로 지금 현재 여러분의 미니 배치 가
전체 트레이닝세트인 경우,

37
00:02:29,580 --> 00:02:31,560
이것도 잘 작동합니다.

38
00:02:31,560 --> 00:02:36,453
그러면 그 다음으로는

39
00:02:36,453 --> 00:02:41,346
vdW를 베터 vdw 더하기 1

40
00:02:41,346 --> 00:02:45,779
빼기 베타 dW를 산출합니다.

41
00:02:45,779 --> 00:02:50,808
이것은 이전에 계산했던

42
00:02:50,808 --> 00:02:55,960
쎄타 = 베타 v 쎄타 더하기 1 빼기
베타 쎄타 t입니다.

43
00:02:57,130 --> 00:03:02,453
즉, w의 derivative에 대해 가중 평균치를 구하는 것인데요,

44
00:03:02,453 --> 00:03:07,754
다음으로, 비슷하게 vdb =

45
00:03:07,754 --> 00:03:13,980
이것 더하기 1 빼기 베타 곱하기 db로 산출합니다.

46
00:03:13,980 --> 00:03:18,810
그러고나서 weight를 업데이트할텐데요,

47
00:03:18,810 --> 00:03:23,850
w는 w 빼기 러닝속도 곱하기 ,
dW로 업데이트 하는 대신에, derivative와 말이죠,

48
00:03:23,850 --> 00:03:28,240
대신에 dvW로 업데이트 합니다.

49
00:03:28,240 --> 00:03:35,630
그리고 비슷하게 b는 b 빼기 알파 곱하기 vdb로 합니다.

50
00:03:35,630 --> 00:03:39,570
그러면 이것이 하는 것은
기울기 강하의 절차를 스무스하게 해주는 것입니다.

51
00:03:41,230 --> 00:03:45,760
예를 들어, 마지막 몇개의 derivative에서
산출한게 이렇게 됐다고 해봅시다.

52
00:03:45,760 --> 00:03:47,298
이거, 이거, 이거, 이거입니다.

53
00:03:48,330 --> 00:03:52,354
이 gradient의 평균을 구하면,
세로축의 방향으로

54
00:03:52,354 --> 00:03:55,472
변동은 거의 0과 가깝게 나올 것입니다.

55
00:03:55,472 --> 00:04:00,301
그러므로 세로 방향으로
더 느리게 하고 싶을텐데요,

56
00:04:00,301 --> 00:04:05,390
이것이 양수와 음수를 밸런스시켜
평균이 거의 0과 가깝게 될 것입니다.

57
00:04:05,390 --> 00:04:07,740
반면에 가로축 방향에서는

58
00:04:07,740 --> 00:04:11,160
모든 derivative가 가로 방량의 오른쪽을
가르키는데요,

59
00:04:11,160 --> 00:04:14,340
그러므로 가로방향에서의 평균은
아직 꽤 큰 값일 것입니다.

60
00:04:14,340 --> 00:04:18,200
그렇기 때문에 여기 몇개의 iteration을 따른 알고리즘에서

61
00:04:18,200 --> 00:04:22,930
기울기 강하 와 모멘텀 이 결과적으로

62
00:04:22,930 --> 00:04:28,100
세로방향에서 훨씬 더 작은 변동을 나타내는 것을 찾을 수 있습니다.

63
00:04:28,100 --> 00:04:33,700
하지만 단순히 더 빨리 가로 방향으로 움직이는 것에 초점이 맞춰져 있죠.

64
00:04:33,700 --> 00:04:37,480
그러므로 이것이 여러분의 알고리즘이
더 단순한 여정을 밟도록 해줍니다.

65
00:04:37,480 --> 00:04:42,990
또는 최소값의 방향에서 변동이 무뎌지게 해주거나 말이죠.

66
00:04:42,990 --> 00:04:47,304
간혹 몇몇 분들에 대해서 한가지 직관적인 부분이

67
00:04:47,304 --> 00:04:53,040
이해가 되는 설명이 있는데요, 여러분이 만약에
밥그릇 모양의 함수를 최소화한다고 생각하면,

68
00:04:53,040 --> 00:04:55,440
이것은 실제로 밥그릇의 곡선입니다.

69
00:04:55,440 --> 00:04:57,840
저는 그림은 잘 못 그리는 것 같아요.

70
00:04:57,840 --> 00:05:02,470
그럼 그릇 모양의 함수를 최소화 시키는데요,

71
00:05:02,470 --> 00:05:06,625
이런 derivative 값은

72
00:05:06,625 --> 00:05:11,071
아랫막길로 내려가고 있는 공에 악셀을 제공한다고 생각하면 됩니다.

73
00:05:11,071 --> 00:05:19,151
여기 모멘텀 용어들은 속돌를 대표한다고 생각하면 됩니다.

74
00:05:20,812 --> 00:05:24,749
여러분이 그릇이 있다고 가정하고,
공을 갖고

75
00:05:24,749 --> 00:05:28,854
악셀의 일부를 이 공에 부여하는 개념인데요,

76
00:05:28,854 --> 00:05:32,440
이 공이 내릿막길에서 내려오고 있다고 생각해보겠습니다.

77
00:05:32,440 --> 00:05:36,980
그러면 더욱더 악셀에 따라 빨리 구르겠죠.

78
00:05:36,980 --> 00:05:42,390
그리고 베타는 1보다 약간 작은 값이 되기 때문에,

79
00:05:42,390 --> 00:05:46,690
마찰력이 약간 적용되어 공이 제한없이 속도가 붙는 것을
막아줍니다.

80
00:05:46,690 --> 00:05:50,380
그러므로 기울기 강하가

81
00:05:50,380 --> 00:05:54,120
이전의 단계를 각각 개인별로 취하는 것이 아닌

82
00:05:54,120 --> 00:05:56,460
이제는 이 작은 공이 아랫막길에서
모멘텀을 얻어 굴러 내려갈 수

83
00:05:56,460 --> 00:06:01,610
있습니다. 여기 이 그릇과 같이 생긴데서
모멘텀을 얻어서 말이죠.

84
00:06:01,610 --> 00:06:05,640
여기 이렇게 그릇에서 내려간다는 비유가

85
00:06:05,640 --> 00:06:07,770
물리의 직관을 이용하는 사람에게는 잘 이해된다고 생각합니다.

86
00:06:07,770 --> 00:06:12,160
하지만 모든사람이 이해를 쉽게 할 수 있는 것은 아니겠죠.

87
00:06:12,160 --> 00:06:15,000
이 부분이 잘 이해가 되지 않으시더라고 걱정하지 마십시요.

88
00:06:15,000 --> 00:06:18,280
마지막으로 이것을 어떻게 도입하는지 보겠습니다.

89
00:06:18,280 --> 00:06:21,300
이것이 알고리즘인데요. 

90
00:06:22,300 --> 00:06:27,100
이제 러닝속도 알파와 여기 parameter 베타와 같이
2개의 하이퍼 파라미터가 있습니다.

91
00:06:27,100 --> 00:06:30,080
이것들은 여러분의 기하급수적 가중평균을 조정하죠. 

92
00:06:30,080 --> 00:06:33,073
베타의 가장 흔한 값은 0.9입니다.

93
00:06:33,073 --> 00:06:35,730
그리고 저희는 지금 10일간의 평균기온을 산출하는 것인데요.

94
00:06:35,730 --> 00:06:39,930
지난 10개의 기울기 강하의 평균치를 구하는 것입니다.

95
00:06:39,930 --> 00:06:42,768
실제로는 베타가 0.9인 것이 잘 작동합니다.

96
00:06:42,768 --> 00:06:45,420
여러분은 얼마든지 다른 값도 시도해보셔도 되는데요,

97
00:06:45,420 --> 00:06:50,120
그렇게 하여 하이퍼 파라미터 서치를 해보세요,
그렇지만 0.9의 베타값이 꽤 잘 작동하는 값입니다.

98
00:06:50,120 --> 00:06:51,932
그럼, 바이어스 보정도 해 볼까요, 그렇죠?

99
00:06:51,932 --> 00:06:58,170
vdW 와 vdb 를 갖고, 나누기 1 빼기 베타의 t승을 해줍니다.

100
00:06:58,170 --> 00:07:02,380
실제로 사람들은 이것을 잘 사용 안 하는데요,

101
00:07:02,380 --> 00:07:06,530
그 이유는 단지 10개의 iteration 이류,
가중평균치는 이미 워밍업이 되어서 더 이상 bias estimate이 더 이상
아니기 때문입니다.

102
00:07:06,530 --> 00:07:11,357
그렇기 때문에 실제로 많은 사람들이
바이어스 보정에 대해 신경쓰는 것을 보지 못합니다.

103
00:07:11,357 --> 00:07:14,663
기울기 강하나 모멘텀을 도입할 때 말이죠.

104
00:07:14,663 --> 00:07:18,785
그리고 당연히 이 절차는 vdW를 0으로 초기화시켜주죠.

105
00:07:18,785 --> 00:07:23,546
아시겠지만 여기서 매트릭스는 0으로 이루어진 매트릭스로
dW와 동일한 다이메션을 갖습니다.

106
00:07:23,546 --> 00:07:26,810
dW는 W와 또 동일한 차원으로 이루어져 있고요.

107
00:07:26,810 --> 00:07:30,620
그리고 Vdb 또한 0의 벡터로 초기회되죠.

108
00:07:30,620 --> 00:07:35,400
그러면 db와 같은 차원,
결과적으로 b와 같은 차원을 갖게 됩니다.

109
00:07:35,400 --> 00:07:40,050
마지막으로 여러분이

110
00:07:40,050 --> 00:07:45,590
기울기 강하 와 모멘텀에 대한 학술을 읽으시면,
이 항이 생략되는 것을 보실텐데요

111
00:07:45,590 --> 00:07:48,854
1 빼기 베타 부분말이죠.

112
00:07:48,854 --> 00:07:57,080
그렇게해서 vdW = 베타 vdw 더하기 dW로 남게됩니다.

113
00:07:57,080 --> 00:08:02,127
여기 보라색 버전을 쓰면서 나타나는 최종 효과는
vdW가

114
00:08:02,127 --> 00:08:07,300
1 빼기 1베타로 스케일 된다는 것입니다.
또는, 더 정확히 얘기하면 1 나누기 1-베타로 말입니다.

115
00:08:07,300 --> 00:08:11,230
그러므로 여러분이 기울기 강하 갱신을 할때
알파의 값은

116
00:08:11,230 --> 00:08:16,220
그에 상응하는 1 나누기 1 빼기 베타로 변해야 합니다.

117
00:08:16,220 --> 00:08:18,800
실제로 이 2개 모두 잘 작동할텐데요.

118
00:08:18,800 --> 00:08:23,740
러닝속도 알파가 가장 최적의 값이
어떤것이지에 대해 영향을 줍니다.

119
00:08:23,740 --> 00:08:28,350
저는 개인적으로 이 공식이 조금 덜 직관적으로 이해가 되는데요.

120
00:08:28,350 --> 00:08:33,610
그 이유는 이것의 영향이 바로
하이퍼 파라미터 베타를 튜닝하는 경우,

121
00:08:33,610 --> 00:08:37,770
이것이 vdW 와 vdb의 스케일링에도 영향을 주기 때문입니다.

122
00:08:37,770 --> 00:08:42,710
그러면 결과적으로 러닝속도 알파를 어쩌면
가시 튜닝해야할 수도 있습니다.

123
00:08:42,710 --> 00:08:46,970
그렇기 때문에 저는 개인적으로
여기 왼쪽에 적은 공식을 선호합니다.

124
00:08:46,970 --> 00:08:49,600
1 빼기 베타를 생략하는 것보다 말이죠.

125
00:08:49,600 --> 00:08:52,450
저는 그래서 왼쪽의 공식을 이용하는 편인데요,

126
00:08:52,450 --> 00:08:55,140
여기 1 빼시 베타가 포함된 것 말이죠.

127
00:08:55,140 --> 00:09:00,280
하지만 베타의 값이 0.9인 2개 모두
하이퍼 파라미터로써 흔한 초이스입니다.

128
00:09:00,280 --> 00:09:03,500
단지 러닝속도가 이 2가지 버전에서 다른 방식으로

129
00:09:03,500 --> 00:09:04,880
튜닝되야 한다는 점이 있는 것뿐이죠.

130
00:09:04,880 --> 00:09:07,500
이것이 그럼 기울기 강하 와 모멘텀에 대한
내용의 전부인데요,

131
00:09:07,500 --> 00:09:11,120
이것이 거의 항상

132
00:09:11,120 --> 00:09:13,740
모멘텀이 없는 그냥 기울기 강하 알고리즘보다
더 잘 작동할 것입니다.

133
00:09:13,740 --> 00:09:17,020
하지만 여러분의 러닝 알고리즘을 빨라지게
만드는 다른 방법이 또 있는데요,

134
00:09:17,020 --> 00:09:19,920
다음 이어지는 비디오에서 이런 내용을 다루겠습니다.