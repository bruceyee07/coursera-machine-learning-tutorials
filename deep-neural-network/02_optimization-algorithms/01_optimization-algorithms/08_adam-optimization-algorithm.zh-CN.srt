1
00:00:00,000 --> 00:00:02,340
在深度学习的发展过程中

2
00:00:02,340 --> 00:00:05,700
很多研究者 其中也包括一些非常著名的研究者

3
00:00:05,700 --> 00:00:07,790
有时候会提出优化算法

4
00:00:07,790 --> 00:00:09,825
在少数问题上 这些算法很有效

5
00:00:09,825 --> 00:00:13,440
但随后有人指出 这些算法并不能被广泛应用

6
00:00:13,440 --> 00:00:18,130
在各种不同类型的神经网络上

7
00:00:18,130 --> 00:00:21,360
因此 深度学习领域逐渐出现了一些

8
00:00:21,360 --> 00:00:25,597
质疑新型优化算法的声音

9
00:00:25,597 --> 00:00:29,350
有些人尝试把梯度下降与动量算法结合起来 结果非常有效

10
00:00:29,350 --> 00:00:32,720
而至今也并没有出现效果比它好很多的优化算法

11
00:00:32,720 --> 00:00:36,070
所以 这个视频将会提到的

12
00:00:36,070 --> 00:00:37,730
是关于RMSProp和Adam优化算法

13
00:00:37,730 --> 00:00:41,460
属于其中极少数真正有效的算法

14
00:00:41,460 --> 00:00:47,250
适用于很多不同的深度学习网络结构

15
00:00:47,250 --> 00:00:50,150
所以 我一定要推荐你试试这个算法

16
00:00:50,150 --> 00:00:54,625
因为很多人尝试后 发现它在很多问题上效果很好

17
00:00:54,625 --> 00:00:57,720
Adam优化算法本质上是将

18
00:00:57,720 --> 00:01:01,250
动量算法和RMSprop结合起来

19
00:01:01,250 --> 00:01:03,105
现在 我们一起来学习下Adam算法

20
00:01:03,105 --> 00:01:05,695
首先你需要进行初始化

21
00:01:05,695 --> 00:01:15,877
Vdw=0, Sdw=0, 以及Vdb=0, Sdb=0

22
00:01:15,877 --> 00:01:19,810
接着 在第t次迭代过程中

23
00:01:19,810 --> 00:01:30,170
你需要用到一些微积分的知识<br />用当前的mini-batch来计算dW和db

24
00:01:30,170 --> 00:01:33,775
通常我们会使用mini-batch梯度下降算法

25
00:01:33,775 --> 00:01:41,480
然后计算动量指数加权移动平均值 所以Vdw=ß

26
00:01:41,480 --> 00:01:46,410
但是这里我标记为ß1 以此来区别于下面我们会提到的

27
00:01:46,410 --> 00:01:52,660
RMSprop算法的超参数β2

28
00:01:52,660 --> 00:01:58,180
这个其实就是我们在使用动量算法时

29
00:01:58,180 --> 00:02:03,788
的公式 但是我们现在用β1来代替β

30
00:02:03,788 --> 00:02:14,312
可以类推至 Vdb=β1*Vdb+(1-β1)db

31
00:02:14,312 --> 00:02:18,685
然后用同样方式更新RMSprop中的参数

32
00:02:18,685 --> 00:02:26,630
我们用一个与之前不同的超参数β2 加上(1-β2)*dw^2

33
00:02:26,630 --> 00:02:33,325
强调一下 这里的平方指的是对dW矩阵中每个元素进行平方

34
00:02:33,325 --> 00:02:44,005
Sdb=β2*Sdb+(1-β2)db^2

35
00:02:44,005 --> 00:02:49,145
所以这个公式和动量很相似 只是用超参数β1来更新

36
00:02:49,145 --> 00:02:55,318
这里就像是用超参数β2来更新了RMSprop

37
00:02:55,318 --> 00:02:58,599
一般在构建Adam算法的过程中

38
00:02:58,599 --> 00:03:01,255
需要进行偏差修正

39
00:03:01,255 --> 00:03:04,215
我们标记为Vdw上标Corrected

40
00:03:04,215 --> 00:03:06,705
Corrected代表偏差修正

41
00:03:06,705 --> 00:03:16,244
它等于VdW/(1-β1^t) t代表迭代次数

42
00:03:16,244 --> 00:03:25,040
类似的 Vdb偏差修正等于Vdb/(1-β1^t)

43
00:03:25,040 --> 00:03:30,756
然后同样在S上进行偏差修正

44
00:03:30,756 --> 00:03:37,405
我们写成SdW/(1-β2^t)

45
00:03:37,405 --> 00:03:48,700
Sdb偏差修正等于Sdb/(1-β2^t)

46
00:03:48,700 --> 00:03:50,660
最后 执行更新

47
00:03:50,660 --> 00:03:55,060
W的更新是使用W减去 α乘以

48
00:03:55,060 --> 00:03:59,870
如果你只使用了动量算法 那么只要VdW就好了

49
00:03:59,870 --> 00:04:03,408
或者偏差修正的VdW就好了

50
00:04:03,408 --> 00:04:06,615
但是现在我们把RMSprop的算法引入

51
00:04:06,615 --> 00:04:13,390
所以还要除以 (偏差修正SdW的平方根)+ε

52
00:04:13,390 --> 00:04:18,232
然后以此类推更新参数b

53
00:04:18,232 --> 00:04:24,070
偏差修正Vdb除以

54
00:04:24,070 --> 00:04:28,595
(偏差修正Sdb的平方根)+ε

55
00:04:28,595 --> 00:04:33,070
这就是Adam算法 结合了动量和RMSprop梯度下降

56
00:04:33,070 --> 00:04:37,572
的各种优势 被广泛使用

57
00:04:37,572 --> 00:04:41,740
且已经被证明在很多不同种类的

58
00:04:41,740 --> 00:04:46,640
神经网络构架中都是十分有效的

59
00:04:46,640 --> 00:04:49,805
这个算法中有几个超参数

60
00:04:49,805 --> 00:04:57,330
但是超参数α依然很重要且通常需要调整

61
00:04:57,330 --> 00:05:01,675
你可以尝试不同的值来比较效果

62
00:05:01,675 --> 00:05:06,090
通常我们会将β1的默认值设置为0.9

63
00:05:06,090 --> 00:05:08,065
这是涉及到动量算法的dW的移动平均

64
00:05:08,065 --> 00:05:12,220
或者说加权平均

65
00:05:12,220 --> 00:05:15,455
至于超参数β2

66
00:05:15,455 --> 00:05:16,950
Adam算法论文的作者

67
00:05:16,950 --> 00:05:20,014
Adam模型的创建者推荐使用0.999

68
00:05:20,014 --> 00:05:26,485
这是关于dW平方与db平方的移动加权平均计算

69
00:05:26,485 --> 00:05:31,030
另外对于ε 其实如何选择影响都不大

70
00:05:31,030 --> 00:05:34,755
但是Adam论文的作者推荐使用10的-8次方作为默认值

71
00:05:34,755 --> 00:05:38,230
对于这个参数 并不需要

72
00:05:38,230 --> 00:05:42,555
真的去深入研究 因为这并不会影响算法的性能

73
00:05:42,555 --> 00:05:44,280
在使用Adam算法的时候

74
00:05:44,280 --> 00:05:47,030
业内通常对β1 β2以及ε

75
00:05:47,030 --> 00:05:49,960
都直接使用默认值

76
00:05:49,960 --> 00:05:52,300
我认为不会有人去管ε

77
00:05:52,300 --> 00:05:56,335
然后尝试一下不同的学习率α来看看哪个最效果最好

78
00:05:56,335 --> 00:05:59,140
你也可以调整β1和β2

79
00:05:59,140 --> 00:06:02,440
但是这种做法在我认识的从业者中用得不多

80
00:06:02,440 --> 00:06:06,100
至于'Adam'这个名字的由来

81
00:06:06,100 --> 00:06:15,267
Adam代表自适应矩估计<br />(Adaptive Moment Estimation)

82
00:06:15,267 --> 00:06:18,175
β1代表这个导数的平均值

83
00:06:18,175 --> 00:06:19,780
被称为第一阶的矩

84
00:06:19,780 --> 00:06:21,975
β2被用于计算平方数的

85
00:06:21,975 --> 00:06:25,830
指数加权平均 也被称为第二阶矩

86
00:06:25,830 --> 00:06:29,380
以上就是Adam这个命名的由来

87
00:06:29,380 --> 00:06:32,875
但是几乎所有人都用Adam算法这个简称

88
00:06:32,875 --> 00:06:37,800
顺便一提 我有一个多年的朋友兼合作者叫做Adam Coates

89
00:06:37,800 --> 00:06:40,425
但是据我所知 他只是碰巧和算法重名

90
00:06:40,425 --> 00:06:43,525
当然有可能偶尔也会用一下这个算法

91
00:06:43,525 --> 00:06:45,847
有时我会经常会被问及他和这个算法关系

92
00:06:45,847 --> 00:06:47,945
所以如果你也有这个疑问的话 希望不要被名字所迷惑

93
00:06:47,945 --> 00:06:51,187
以上就是Adam优化算法的知识

94
00:06:51,187 --> 00:06:54,435
通过这节课的学习 我想你能把神经网络的训练变得更高效

95
00:06:54,435 --> 00:06:56,055
在我们结束这周的课程前

96
00:06:56,055 --> 00:06:58,950
我们还会说一下怎样调整超参数

97
00:06:58,950 --> 00:07:01,465
以及让大家更直观地理解

98
00:07:01,465 --> 00:07:04,230
到底如何进行神经网络优化

99
00:07:04,230 --> 00:07:07,260
下期视频我们会讨论学习率衰减的知识<br />翻译 | 审阅：Cousera Global Translator Community