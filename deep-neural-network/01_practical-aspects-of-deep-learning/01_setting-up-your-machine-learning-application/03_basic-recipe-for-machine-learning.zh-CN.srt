1
00:00:00,000 --> 00:00:01,470
在上一堂课中

2
00:00:01,470 --> 00:00:04,778
你看到了如何通过观察训练集和开发集上的误差

3
00:00:04,778 --> 00:00:09,280
来诊断你的算法中是否有偏差问题 方差问题 或二者皆有

4
00:00:09,280 --> 00:00:11,880
你能发现这个信息会让你能够让你有条理得多

5
00:00:11,880 --> 00:00:15,030
就是根据大家所说的

6
00:00:15,030 --> 00:00:18,165
机器学习的基本原则

7
00:00:18,165 --> 00:00:21,510
就能让你在改进算法性能时更有条理 我们来看一下

8
00:00:21,510 --> 00:00:22,900
当训练一个神经网络时

9
00:00:22,900 --> 00:00:24,975
这是一个我会遵循的基本原则

10
00:00:24,975 --> 00:00:26,920
当训练好了最初的神经网络时

11
00:00:26,920 --> 00:00:28,185
我会首先问

12
00:00:28,185 --> 00:00:30,570
这个算法是否有高偏差?

13
00:00:30,570 --> 00:00:33,709
要判断是否存在高偏差

14
00:00:33,709 --> 00:00:35,820
你实际上就是要看

15
00:00:35,820 --> 00:00:40,260
模型在训练集的数据上的表现

16
00:00:40,260 --> 00:00:43,260
如果说模型有高偏差

17
00:00:43,260 --> 00:00:45,735
即是模型甚至连训练集都不能良好拟合

18
00:00:45,735 --> 00:00:49,695
你能尝试的一种办法是挑选一个新的网络

19
00:00:49,695 --> 00:00:52,680
比如带有更多隐藏层或更多隐藏单元的

20
00:00:52,680 --> 00:00:54,825
或是延长训练时间

21
00:00:54,825 --> 00:00:58,953
让梯度下降法运行更长时间 或换用一些更高级的优化算法

22
00:00:58,953 --> 00:01:00,795
我们将在这门课程的后面提到它。

23
00:01:00,795 --> 00:01:03,030
另一个可以尝试的办法

24
00:01:03,030 --> 00:01:06,285
这个办法可能有效 也可能无效

25
00:01:06,285 --> 00:01:10,680
但之后总归会看到 因为神经网络的结构有许多种

26
00:01:10,680 --> 00:01:15,450
所以你能够找到一种更加适合当前问题的结构

27
00:01:15,450 --> 00:01:17,760
我把它写在括号里 是因为它是

28
00:01:17,760 --> 00:01:19,380
一种需要你亲自尝试的方法

29
00:01:19,380 --> 00:01:20,925
也许最终你能使它有效 也许不能

30
00:01:20,925 --> 00:01:24,170
相比起来 使用更大的网络几乎总是有效

31
00:01:24,170 --> 00:01:26,761
而延长训练时间 虽然并不永远有用

32
00:01:26,761 --> 00:01:28,450
但是当然也不会造成坏处

33
00:01:28,450 --> 00:01:29,793
所以当训练一个学习算法时

34
00:01:29,793 --> 00:01:34,100
我会尝试这些办法 直到我把偏差问题消除

35
00:01:34,100 --> 00:01:39,945
所以我尝试完 会回到这里 然后再重复尝试

36
00:01:39,945 --> 00:01:42,460
直到至少能良好地符合训练集

37
00:01:42,460 --> 00:01:44,760
通常如果你的网络够大

38
00:01:44,760 --> 00:01:49,470
你应当通常就能够将训练集拟合好

39
00:01:49,470 --> 00:01:54,150
只要这个学习问题是人类能完成的 对吧

40
00:01:54,150 --> 00:01:55,787
如果图片非常模糊

41
00:01:55,787 --> 00:01:57,300
也许就不可能拟合

42
00:01:57,300 --> 00:01:59,531
但如果至少人类能够良好地完成这个任务

43
00:01:59,531 --> 00:02:01,540
如果你认为贝叶斯误差不是太大的话

44
00:02:01,540 --> 00:02:04,244
那只要训练一个足够大的网络 就应当能够

45
00:02:04,244 --> 00:02:07,275
或许应当能够在训练集上取得良好的表现

46
00:02:07,275 --> 00:02:09,970
也就是把训练集拟合 或是过拟合

47
00:02:09,970 --> 00:02:14,743
当把偏差减小到可以接受的范围之后 就再问

48
00:02:14,743 --> 00:02:17,040
这个算法是否有高方差?

49
00:02:17,040 --> 00:02:21,410
要判断这一点 我会看模型在开发集上的表现

50
00:02:21,410 --> 00:02:24,310
看模型是否具备一般化的能力 如在训练集上性能良好

51
00:02:24,310 --> 00:02:28,595
当一般化到开发集上时 仍然性能较好?

52
00:02:28,595 --> 00:02:30,915
如果你有比较高的方差

53
00:02:30,915 --> 00:02:34,015
解决高方差问题的最好方法是取得更多数据

54
00:02:34,015 --> 00:02:35,199
当然前提是你能获取得到

55
00:02:35,199 --> 00:02:36,875
这个办法才有用

56
00:02:36,875 --> 00:02:40,490
但有时你无法获得更多数据

57
00:02:40,490 --> 00:02:43,300
你还可以尝试正则化

58
00:02:43,300 --> 00:02:45,078
这是我们下一节课中会讨论的

59
00:02:45,078 --> 00:02:46,630
用它可以减少过拟合

60
00:02:46,630 --> 00:02:50,930
还有一种 也需要你亲自尝试的方法

61
00:02:50,930 --> 00:02:54,310
就是如果你能找到更合适的神经网络结构

62
00:02:54,310 --> 00:02:57,335
有时也能够在缓解方差问题的同时

63
00:02:57,335 --> 00:03:00,785
也缓解偏差问题 但具体怎么做呢?

64
00:03:00,785 --> 00:03:04,045
这里倒是不太容易总结出完全系统性的规律

65
00:03:04,045 --> 00:03:06,175
所以我也尝试这些办法 完后也回到初始点

66
00:03:06,175 --> 00:03:11,785
直到找到一种低偏差 低方差的网络

67
00:03:11,785 --> 00:03:14,594
然后问题就到此解决了

68
00:03:14,594 --> 00:03:16,390
这里有几点要注意的是

69
00:03:16,390 --> 00:03:19,736
首先 依据你问题的不同 在高偏差和高方差时

70
00:03:19,736 --> 00:03:24,405
你应当尝试的办法有可能很不一样

71
00:03:24,405 --> 00:03:26,860
所以我通常用训练/开发集判断

72
00:03:26,860 --> 00:03:29,920
问题是在高偏差 还是高方差

73
00:03:29,920 --> 00:03:33,920
然后根据这个来选择一些应当尝试的办法

74
00:03:33,920 --> 00:03:37,270
举例来说 如果你有高偏差问题

75
00:03:37,270 --> 00:03:40,300
就算取得更多训练数据也无济于事

76
00:03:40,300 --> 00:03:44,140
至少也不是最有效率的办法

77
00:03:44,140 --> 00:03:47,770
所以明确认识到 是更像高偏差问题 或是高方差问题

78
00:03:47,770 --> 00:03:52,563
或是二者皆备 就能帮助你选择最有用的办法

79
00:03:52,563 --> 00:03:56,725
另外 在早些时代的机器学习中

80
00:03:56,725 --> 00:04:02,465
曾经有许多关于偏差和方差之间的取舍的讨论

81
00:04:02,465 --> 00:04:04,604
这讨论存在的原因是

82
00:04:04,604 --> 00:04:06,385
对于很多你能尝试的办法来说

83
00:04:06,385 --> 00:04:09,340
你只能在增大偏差的同时减小方差

84
00:04:09,340 --> 00:04:11,920
或减小偏差的同时增大方差

85
00:04:11,920 --> 00:04:15,400
但是深度学习之前的时代中

86
00:04:15,400 --> 00:04:17,165
我们能用的工具不是很多

87
00:04:17,165 --> 00:04:19,755
我们没有太多那种能够单独减小偏差

88
00:04:19,755 --> 00:04:24,380
或单独减小方差 而不顾此失彼的工具

89
00:04:24,380 --> 00:04:28,750
但在当前这个深度学习和大数据的时代

90
00:04:28,750 --> 00:04:31,705
只要你能不断扩大所训练的网络的规模

91
00:04:31,705 --> 00:04:34,200
只要你能不断获得更多数据

92
00:04:34,200 --> 00:04:36,360
虽然这两点都不是永远成立的

93
00:04:36,360 --> 00:04:37,950
但如果这两点是可能的

94
00:04:37,950 --> 00:04:40,590
那扩大网络几乎总是能够

95
00:04:40,590 --> 00:04:43,625
减小偏差而不增大方差

96
00:04:43,625 --> 00:04:46,157
只要你用恰当的方式正则化的话

97
00:04:46,157 --> 00:04:48,810
而获得更多数据几乎总是能够

98
00:04:48,810 --> 00:04:52,370
减小方差而不增大偏差

99
00:04:52,370 --> 00:04:54,195
所以归根结底

100
00:04:54,195 --> 00:04:55,845
有了这两步以后

101
00:04:55,845 --> 00:04:57,405
再加上能够选取不同的网络来训练

102
00:04:57,405 --> 00:04:58,560
以及获取更多数据的能力

103
00:04:58,560 --> 00:05:03,375
我们就有了能够且只单独削减偏差

104
00:05:03,375 --> 00:05:05,700
或者能够并且单独削减方差

105
00:05:05,700 --> 00:05:09,655
同时不会过多影响另一个指标的能力

106
00:05:09,655 --> 00:05:12,240
我认为这就是诸多原因中的一个

107
00:05:12,240 --> 00:05:16,348
它能够解释为何深度学习在监督学习中如此有用

108
00:05:16,348 --> 00:05:18,840
以及为何在深度学习中 偏差与方差的权衡要不明显得多

109
00:05:18,840 --> 00:05:21,345
这样你就不需小心地平衡两者

110
00:05:21,345 --> 00:05:25,053
而是因为有了更多选择 可以单独削减偏差

111
00:05:25,053 --> 00:05:30,315
或单独削减方差 而不会同时增加方差或偏差

112
00:05:30,315 --> 00:05:33,698
而且事实上当你有了一个良好地正则化的网络时

113
00:05:33,698 --> 00:05:36,795
我们将在下一节课中讨论正则化

114
00:05:36,795 --> 00:05:40,110
训练一个更大的网络几乎从来没有坏处

115
00:05:40,110 --> 00:05:44,634
当训练的神经网络太大时主要的代价只是计算时间

116
00:05:44,634 --> 00:05:46,490
只要你采取正则化就行

117
00:05:46,490 --> 00:05:49,440
我希望这个视频能给你一些基本的概念

118
00:05:49,440 --> 00:05:53,255
知道如何有条理地诊断机器学习中的偏差与方差问题

119
00:05:53,255 --> 00:05:57,325
然后采取正确的办法来在问题中取得进展

120
00:05:57,325 --> 00:06:01,367
我在视频中多次提到过正则化的概念

121
00:06:01,367 --> 00:06:03,825
它是用于减小方差的一个很有用的办法

122
00:06:03,825 --> 00:06:07,130
在正则化中存在一点点偏差与方差间的权衡

123
00:06:07,130 --> 00:06:09,045
它可能会使偏差增加一点点

124
00:06:09,045 --> 00:06:13,090
虽然在你的网络足够巨大时 增加得通常不会很多

125
00:06:13,090 --> 00:06:16,735
所以让我们在下一个视频中深入讨论一下

126
00:06:16,735 --> 00:06:21,000
如何对神经网络进行正规化
GTC字幕组翻译