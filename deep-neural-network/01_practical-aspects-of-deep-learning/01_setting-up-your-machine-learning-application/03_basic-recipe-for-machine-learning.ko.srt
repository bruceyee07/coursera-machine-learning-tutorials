1
00:00:00,000 --> 00:00:01,470
이전 강의에서 여러분에게

2
00:00:01,470 --> 00:00:04,778
여러분은 training error와 depth error를 봄으로써

3
00:00:04,778 --> 00:00:09,280
편향이나 변동의 문제 또는 두 가지를 모두 진단할 수 있다는 사실을 배웠습니다.

4
00:00:09,280 --> 00:00:11,880
이러한 특정 정보가 생각보다 더 많은

5
00:00:11,880 --> 00:00:15,030
시스템적인 부분을 알 수 있게 하는데요,

6
00:00:15,030 --> 00:00:18,165
이런 것을 머신 러닝의 기본 레시피라고 하며,

7
00:00:18,165 --> 00:00:21,510
시스템적인 어프로치를 통해 알고리즘의 성능을 향상하는데 기여합니다.
한번 볼까요?

8
00:00:21,510 --> 00:00:22,900
신경망을 교육시키는 경우에,

9
00:00:22,900 --> 00:00:24,975
이와 같은 기본 레시피를 사용하도록 하겠습니다.

10
00:00:24,975 --> 00:00:26,920
초기모델을 교육한 이후 시점에,

11
00:00:26,920 --> 00:00:28,185
먼저 질문 드릴 것입니다.

12
00:00:28,185 --> 00:00:30,570
여러분의 알고리즘은 큰 편향을 띄고 있습니까?

13
00:00:30,570 --> 00:00:33,709
그러므로, 여러분들이 직접 편향이 큰지 여부를 평가해보십시오.

14
00:00:33,709 --> 00:00:35,820
여러분들이 집중적으로 보셔야 할 부분은,

15
00:00:35,820 --> 00:00:40,260
트레이닝 세트와 트레이닝 데이터의 성능입니다.

16
00:00:40,260 --> 00:00:43,260
만약 큰 편향을 띄고 있다면,

17
00:00:43,260 --> 00:00:45,735
또는 트레이닝 세트에 잘 맞지 않는다고 한다면,

18
00:00:45,735 --> 00:00:49,695
시도할 만한 것은 네트워크를 고르시는 것입니다.

19
00:00:49,695 --> 00:00:52,680
예를 들어, 숨겨진 층이나 유닛을 말이죠.

20
00:00:52,680 --> 00:00:54,825
또는 더 장기간 교육을 진행하는 방법도 있습니다.

21
00:00:54,825 --> 00:00:58,953
아니시면 더 길게 운행을 하는 방법이나, 조금 더 최적화된 알고리즘을
이용하는 방법도 있습니다.

22
00:00:58,953 --> 00:01:00,795
이런 내용은 조금 후에 더 다루도록 하겠습니다.

23
00:01:00,795 --> 00:01:03,030
또 다른 방법으로는,

24
00:01:03,030 --> 00:01:06,285
이런 류의 방법이 있을 텐데요, 될 수도 있고 안 될 수도 있을 것 같아요.

25
00:01:06,285 --> 00:01:10,680
나중에 보시겠지만 구조가 상이한 신경망이 다양하기 때문에 

26
00:01:10,680 --> 00:01:15,450
문제에 더욱 적합한 새로운 신경망 구조를 찾는 방법도 있습니다.

27
00:01:15,450 --> 00:01:17,760
괄호 안에 해당 내용 넣어두겠습니다.

28
00:01:17,760 --> 00:01:19,380
여러분이 꼭 시도해봐야 하는 것 중에 하나이기 때문이죠.

29
00:01:19,380 --> 00:01:20,925
여러분이 정상적으로 작동 시킬 수도 있고 안 될 수도 있겠죠.

30
00:01:20,925 --> 00:01:24,170
반면에, 더 큰 네트워크를 갖추는 것은 거의 항상 도움이 됩니다.

31
00:01:24,170 --> 00:01:26,761
교육시간을 늘리는 것은 항상 도움이 되는 것은 아니지만

32
00:01:26,761 --> 00:01:28,450
시도해서 좋지 않을 건 없을 것 같습니다.

33
00:01:28,450 --> 00:01:29,793
교육 알고리즘을 트레이닝 할 때는,

34
00:01:29,793 --> 00:01:34,100
편향을 제거하기 전까지는 저는 개인적으로 이러한 방법들을 시도해 볼 것 같습니다. 

35
00:01:34,100 --> 00:01:39,945
구체적으로, 특정 방법을 시도하셨으면 다시 돌아가서 구현시켜 보는 거죠.
이렇게 해서 잘 맞을 때까지 하는 겁니다.

36
00:01:39,945 --> 00:01:42,460
적어도 트레이닝 세트가 잘 맞을 때까지는 말이죠.

37
00:01:42,460 --> 00:01:44,760
주로 어느 정도 크기의 네트워크를 가지고 계시면

38
00:01:44,760 --> 00:01:49,470
여러분께서는 보통 웬만해서는 트레이닝 데이터를 잘 맞추실 수 있을 것입니다.

39
00:01:49,470 --> 00:01:54,150
적어도 누군가가 풀 수 있는 문제라면 말이죠. 아시겠죠?

40
00:01:54,150 --> 00:01:55,787
이미지가 매우 흐릿하시면

41
00:01:55,787 --> 00:01:57,300
맞추시기가 불가능할 수도 있습니다.

42
00:01:57,300 --> 00:01:59,531
그러나 적어도 인간이 특정 작업을 수행할 수 있고,

43
00:01:59,531 --> 00:02:01,540
베이스 오류가 너무 크지 않다고 하면,

44
00:02:01,540 --> 00:02:04,244
어느 정도 크기가 있는 네트워크는 트레이닝 작업을 통해

45
00:02:04,244 --> 00:02:07,275
적어도 트레이닝 세트에서는 잘 하실 겁니다.

46
00:02:07,275 --> 00:02:09,970
트레이닝 세트가 딱 적정선에 맞춰지거나 또는 여유있게 맞춰지기 위해서 말이죠.

47
00:02:09,970 --> 00:02:14,743
편향을 허용할 수 있는 양으로 줄이셨으면,

48
00:02:14,743 --> 00:02:17,040
변동의 문제가 있는지 스스로 물어보십시오.

49
00:02:17,040 --> 00:02:21,410
이런 평가를 하기 위해서 저 같으면 dev set 성능을 확인해 볼 것입니다.

50
00:02:21,410 --> 00:02:24,310
꽤 훌륭한 트레이닝 세트의 성능에서부터 시작하여

51
00:02:24,310 --> 00:02:28,595
꽤 훌륭한 dev 성능까지 일반화를 시킬 수 있습니까?

52
00:02:28,595 --> 00:02:30,915
큰 변동폭이 상존하는 경우에는, 

53
00:02:30,915 --> 00:02:34,015
더 많은 데이터를 수집하는 것이 문제를 해결하는 가장 좋은 방법입니다.

54
00:02:34,015 --> 00:02:35,199
데이터를 더 수집할 수 있으면

55
00:02:35,199 --> 00:02:36,875
쉽게 말해 더 좋다고 할 수 있겠습니다.

56
00:02:36,875 --> 00:02:40,490
하지만 항상 데이터를 구할 수 있는 것은 아니죠,

57
00:02:40,490 --> 00:02:43,300
아니면 일반화를 시도하는 방법도 있습니다.

58
00:02:43,300 --> 00:02:45,078
이 내용은 다음 비디오에서 다룰 텐데요,

59
00:02:45,078 --> 00:02:46,630
오버피팅을 줄이는 방법을 써보도록 하겠습니다.

60
00:02:46,630 --> 00:02:50,930
이유를 막론하고 가끔씩은 시도해봐야 하는 경우도 있죠.

61
00:02:50,930 --> 00:02:54,310
하지만 더 적합한 신경망 구조를 찾으시는 경우엔

62
00:02:54,310 --> 00:02:57,335
새로운 구조를 사용하시는 것이 변동을 줄이는데 좋을 수 있습니다.

63
00:02:57,335 --> 00:03:00,785
편향도 함께 줄일 수도 있죠. 하지만 어떻게 하면 그렇게 할 수 있을까요?

64
00:03:00,785 --> 00:03:04,045
완전히 시스템적인 접근을 찾는 것은 생각보다 어렵습니다. 

65
00:03:04,045 --> 00:03:06,175
그렇기 때문에 저는 이러한 여러 가지 방법을 시도해보고 다시 체크하러 돌아가봅니다.

66
00:03:06,175 --> 00:03:11,785
줄어든 편향 및 변동을 찾을 때까지 말이죠.

67
00:03:11,785 --> 00:03:14,594
기 시점에 도달했으면 완성했다고 볼 수 있습니다.

68
00:03:14,594 --> 00:03:16,390
몇 가지 주목해야 할 점이 있습니다.

69
00:03:16,390 --> 00:03:19,736
첫 번째로는 큰 편향이나 큰 변동이 존재하는지 여부에 따라서

70
00:03:19,736 --> 00:03:24,405
시도할 수 있는 방법이 상당히 다르다는 점입니다.

71
00:03:24,405 --> 00:03:26,860
그러므로 저는 보통 트레이닝 dev set를 이용해서 

72
00:03:26,860 --> 00:03:29,920
편향이나 변동이 있는 경우, 진단법을 찾을 것 같아요.

73
00:03:29,920 --> 00:03:33,920
그런 이후, 후속 방편을 찾아서 다음 방법을 모색하는 방식으로 접근할 것 같습니다.

74
00:03:33,920 --> 00:03:37,270
예를 들어, 큰 편향이 문제가 된다고 하면,

75
00:03:37,270 --> 00:03:40,300
트레이닝 데이터를 더 수집하는 것은 도움이 되지 않을 것입니다.

76
00:03:40,300 --> 00:03:44,140
적어도 가장 효율적인 방법이라고 볼 순 없겠죠.

77
00:03:44,140 --> 00:03:47,770
그러므로 얼마나 편향의 문제 또는 변동의 문제가 큰지 여부를 확인하는 것이

78
00:03:47,770 --> 00:03:52,563
적합한 시도 방법을 찾는데 도움이 될 것입니다. 

79
00:03:52,563 --> 00:03:56,725
둘째로, 머신러닝 초기에는

80
00:03:56,725 --> 00:04:02,465
bias variance tradeoff 라는 것에 대한 많은 논의가 있었습니다.

81
00:04:02,465 --> 00:04:04,604
이유인 즉 슨,

82
00:04:04,604 --> 00:04:06,385
시도할 수 있는 여러 가지 방법 중에,

83
00:04:06,385 --> 00:04:09,340
편향을 늘리면서 동시에 변동을 줄이거나,

84
00:04:09,340 --> 00:04:11,920
편향을 줄임과 동시에 변동을 늘릴 수 있기 때문이죠.

85
00:04:11,920 --> 00:04:15,400
딥러닝의 초창기에는,

86
00:04:15,400 --> 00:04:17,165
적용할 수 있는 툴이 많이 없었는데요,

87
00:04:17,165 --> 00:04:19,755
한가지 요소의 희생 없이 편향이나 변동을 

88
00:04:19,755 --> 00:04:24,380
줄일 수 있는 방법이 딱히 없었습니다.

89
00:04:24,380 --> 00:04:28,750
하지만 현재 딥러닝은, 빅 데이터 시대에 맞게

90
00:04:28,750 --> 00:04:31,705
더 큰 네트워크를 지속적으로 교육시킬 수 있는 한,

91
00:04:31,705 --> 00:04:34,200
데이터를 지속적으로 수집할 수 있는 한,

92
00:04:34,200 --> 00:04:36,360
물론 항상 이런 것은 아니지만,

93
00:04:36,360 --> 00:04:37,950
이런 조건이 부합하는 경우,

94
00:04:37,950 --> 00:04:40,590
큰 네트워크를 구성하면,

95
00:04:40,590 --> 00:04:43,625
편향은 변동의 영향 없이 거의 항상 줄어든다고 볼 수 있습니다. 

96
00:04:43,625 --> 00:04:46,157
적절히 일반화 한다면 말이죠.

97
00:04:46,157 --> 00:04:48,810
더 많은 데이터를 수집하는 것 또한

98
00:04:48,810 --> 00:04:52,370
편향의 변화 없이 거의 항상 변동을 줄일 수 있습니다.

99
00:04:52,370 --> 00:04:54,195
실제로 어떤 일이 벌어진 것이냐면, 

100
00:04:54,195 --> 00:04:55,845
이러한 두 가지의 절차를 통해

101
00:04:55,845 --> 00:04:57,405
교육시키는 능력, 네트워크를 고르는 능력,

102
00:04:57,405 --> 00:04:58,560
데이터를 추가 수집하는 방법을 통해

103
00:04:58,560 --> 00:05:03,375
우리는 편향을 줄이고 또 줄이고,

104
00:05:03,375 --> 00:05:05,700
변동을 줄이고 또 줄일 수 있는 것입니다.

105
00:05:05,700 --> 00:05:09,655
다른 요소에 악영향을 주지 않고 말이죠.

106
00:05:09,655 --> 00:05:12,240
제 생각에는 이러한 이유가 바로 딥러닝이

107
00:05:12,240 --> 00:05:16,348
지도학습에서 굉장히 유용하게 쓰이는 가장 큰 이유 중에 하나라고 생각됩니다.

108
00:05:16,348 --> 00:05:18,840
절충이 줄면서,

109
00:05:18,840 --> 00:05:21,345
편향과 변동의 양을 조심스럽게 밸런스 맞춰야 하는 경우가 줄어든 셈이죠.

110
00:05:21,345 --> 00:05:25,053
그러나 가끔씩은 편향을 줄이거나 

111
00:05:25,053 --> 00:05:30,315
변동을 줄이는데 다른 한가지 요소를 올릴 필요 없는 경우가
이 방법 외에 몇 가지 더 있을 수 있습니다.

112
00:05:30,315 --> 00:05:33,698
regularized network가 형성되는 경우에 말이죠.

113
00:05:33,698 --> 00:05:36,795
다음 비디오에서부터 일반화에 대해 이야기 해보도록 하겠습니다. 

114
00:05:36,795 --> 00:05:40,110
더 큰 네트워크를 트레이닝 하는 것은 더 좋으면 좋지 나쁘다고 하기 어렵습니다.

115
00:05:40,110 --> 00:05:44,634
신경망을 훈련시키는 과정의 주요 과다 비용은 산출 시간이라고 볼 수 있습니다.

116
00:05:44,634 --> 00:05:46,490
물론, 일반화가 진행되고 있다는 가정하에서 입니다.

117
00:05:46,490 --> 00:05:49,440
저는 오늘 내용이 머신 러닝 기본 구조에 대해,

118
00:05:49,440 --> 00:05:53,255
머신러닝의 문제들을 정리하고 편향과 변동을 진단하는 방법에 대해
잘 다뤄졌기를 바랍니다.

119
00:05:53,255 --> 00:05:57,325
여러분이 문제를 다루는데 있어, 이런 내용을 적용하여 본인에게 적합한 구현 방법을
찾으시길 희망합니다.

120
00:05:57,325 --> 00:06:01,367
제가 비디오에서 몇 번 언급했던 것 중 하나가 바로 일반화인데요,

121
00:06:01,367 --> 00:06:03,825
변동을 줄이기 위한 매우 유용한 기술입니다.

122
00:06:03,825 --> 00:06:07,130
일반화를 사용하는 경우에는 편향과 변동 사이 약간의 절충이 있으나,

123
00:06:07,130 --> 00:06:09,045
편향이 약간 증가할 수 있지만,

124
00:06:09,045 --> 00:06:13,090
방대한 크기의 네트워크로 작동하는 이상 그리 편향이 그리 큰 수치는 아닐 것입니다.

125
00:06:13,090 --> 00:06:16,735
일단 다음 비디오에서 더 상세히 이야기 하도록 하겠습니다.

126
00:06:16,735 --> 00:06:21,000
다음 비디오에선 일반화는 더욱 자세히 이해하고 신경망에 적용하는 방법을 배워보겠습니다.