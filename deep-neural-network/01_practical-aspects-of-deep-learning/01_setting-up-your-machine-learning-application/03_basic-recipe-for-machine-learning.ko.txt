이전 강의에서 여러분에게 여러분은 training error와 depth error를 봄으로써 편향이나 변동의 문제 또는 두 가지를 모두 진단할 수 있다는 사실을 배웠습니다. 이러한 특정 정보가 생각보다 더 많은 시스템적인 부분을 알 수 있게 하는데요, 이런 것을 머신 러닝의 기본 레시피라고 하며, 시스템적인 어프로치를 통해 알고리즘의 성능을 향상하는데 기여합니다.
한번 볼까요? 신경망을 교육시키는 경우에, 이와 같은 기본 레시피를 사용하도록 하겠습니다. 초기모델을 교육한 이후 시점에, 먼저 질문 드릴 것입니다. 여러분의 알고리즘은 큰 편향을 띄고 있습니까? 그러므로, 여러분들이 직접 편향이 큰지 여부를 평가해보십시오. 여러분들이 집중적으로 보셔야 할 부분은, 트레이닝 세트와 트레이닝 데이터의 성능입니다. 만약 큰 편향을 띄고 있다면, 또는 트레이닝 세트에 잘 맞지 않는다고 한다면, 시도할 만한 것은 네트워크를 고르시는 것입니다. 예를 들어, 숨겨진 층이나 유닛을 말이죠. 또는 더 장기간 교육을 진행하는 방법도 있습니다. 아니시면 더 길게 운행을 하는 방법이나, 조금 더 최적화된 알고리즘을
이용하는 방법도 있습니다. 이런 내용은 조금 후에 더 다루도록 하겠습니다. 또 다른 방법으로는, 이런 류의 방법이 있을 텐데요, 될 수도 있고 안 될 수도 있을 것 같아요. 나중에 보시겠지만 구조가 상이한 신경망이 다양하기 때문에 문제에 더욱 적합한 새로운 신경망 구조를 찾는 방법도 있습니다. 괄호 안에 해당 내용 넣어두겠습니다. 여러분이 꼭 시도해봐야 하는 것 중에 하나이기 때문이죠. 여러분이 정상적으로 작동 시킬 수도 있고 안 될 수도 있겠죠. 반면에, 더 큰 네트워크를 갖추는 것은 거의 항상 도움이 됩니다. 교육시간을 늘리는 것은 항상 도움이 되는 것은 아니지만 시도해서 좋지 않을 건 없을 것 같습니다. 교육 알고리즘을 트레이닝 할 때는, 편향을 제거하기 전까지는 저는 개인적으로 이러한 방법들을 시도해 볼 것 같습니다. 구체적으로, 특정 방법을 시도하셨으면 다시 돌아가서 구현시켜 보는 거죠.
이렇게 해서 잘 맞을 때까지 하는 겁니다. 적어도 트레이닝 세트가 잘 맞을 때까지는 말이죠. 주로 어느 정도 크기의 네트워크를 가지고 계시면 여러분께서는 보통 웬만해서는 트레이닝 데이터를 잘 맞추실 수 있을 것입니다. 적어도 누군가가 풀 수 있는 문제라면 말이죠. 아시겠죠? 이미지가 매우 흐릿하시면 맞추시기가 불가능할 수도 있습니다. 그러나 적어도 인간이 특정 작업을 수행할 수 있고, 베이스 오류가 너무 크지 않다고 하면, 어느 정도 크기가 있는 네트워크는 트레이닝 작업을 통해 적어도 트레이닝 세트에서는 잘 하실 겁니다. 트레이닝 세트가 딱 적정선에 맞춰지거나 또는 여유있게 맞춰지기 위해서 말이죠. 편향을 허용할 수 있는 양으로 줄이셨으면, 변동의 문제가 있는지 스스로 물어보십시오. 이런 평가를 하기 위해서 저 같으면 dev set 성능을 확인해 볼 것입니다. 꽤 훌륭한 트레이닝 세트의 성능에서부터 시작하여 꽤 훌륭한 dev 성능까지 일반화를 시킬 수 있습니까? 큰 변동폭이 상존하는 경우에는, 더 많은 데이터를 수집하는 것이 문제를 해결하는 가장 좋은 방법입니다. 데이터를 더 수집할 수 있으면 쉽게 말해 더 좋다고 할 수 있겠습니다. 하지만 항상 데이터를 구할 수 있는 것은 아니죠, 아니면 일반화를 시도하는 방법도 있습니다. 이 내용은 다음 비디오에서 다룰 텐데요, 오버피팅을 줄이는 방법을 써보도록 하겠습니다. 이유를 막론하고 가끔씩은 시도해봐야 하는 경우도 있죠. 하지만 더 적합한 신경망 구조를 찾으시는 경우엔 새로운 구조를 사용하시는 것이 변동을 줄이는데 좋을 수 있습니다. 편향도 함께 줄일 수도 있죠. 하지만 어떻게 하면 그렇게 할 수 있을까요? 완전히 시스템적인 접근을 찾는 것은 생각보다 어렵습니다. 그렇기 때문에 저는 이러한 여러 가지 방법을 시도해보고 다시 체크하러 돌아가봅니다. 줄어든 편향 및 변동을 찾을 때까지 말이죠. 기 시점에 도달했으면 완성했다고 볼 수 있습니다. 몇 가지 주목해야 할 점이 있습니다. 첫 번째로는 큰 편향이나 큰 변동이 존재하는지 여부에 따라서 시도할 수 있는 방법이 상당히 다르다는 점입니다. 그러므로 저는 보통 트레이닝 dev set를 이용해서 편향이나 변동이 있는 경우, 진단법을 찾을 것 같아요. 그런 이후, 후속 방편을 찾아서 다음 방법을 모색하는 방식으로 접근할 것 같습니다. 예를 들어, 큰 편향이 문제가 된다고 하면, 트레이닝 데이터를 더 수집하는 것은 도움이 되지 않을 것입니다. 적어도 가장 효율적인 방법이라고 볼 순 없겠죠. 그러므로 얼마나 편향의 문제 또는 변동의 문제가 큰지 여부를 확인하는 것이 적합한 시도 방법을 찾는데 도움이 될 것입니다. 둘째로, 머신러닝 초기에는 bias variance tradeoff 라는 것에 대한 많은 논의가 있었습니다. 이유인 즉 슨, 시도할 수 있는 여러 가지 방법 중에, 편향을 늘리면서 동시에 변동을 줄이거나, 편향을 줄임과 동시에 변동을 늘릴 수 있기 때문이죠. 딥러닝의 초창기에는, 적용할 수 있는 툴이 많이 없었는데요, 한가지 요소의 희생 없이 편향이나 변동을 줄일 수 있는 방법이 딱히 없었습니다. 하지만 현재 딥러닝은, 빅 데이터 시대에 맞게 더 큰 네트워크를 지속적으로 교육시킬 수 있는 한, 데이터를 지속적으로 수집할 수 있는 한, 물론 항상 이런 것은 아니지만, 이런 조건이 부합하는 경우, 큰 네트워크를 구성하면, 편향은 변동의 영향 없이 거의 항상 줄어든다고 볼 수 있습니다. 적절히 일반화 한다면 말이죠. 더 많은 데이터를 수집하는 것 또한 편향의 변화 없이 거의 항상 변동을 줄일 수 있습니다. 실제로 어떤 일이 벌어진 것이냐면, 이러한 두 가지의 절차를 통해 교육시키는 능력, 네트워크를 고르는 능력, 데이터를 추가 수집하는 방법을 통해 우리는 편향을 줄이고 또 줄이고, 변동을 줄이고 또 줄일 수 있는 것입니다. 다른 요소에 악영향을 주지 않고 말이죠. 제 생각에는 이러한 이유가 바로 딥러닝이 지도학습에서 굉장히 유용하게 쓰이는 가장 큰 이유 중에 하나라고 생각됩니다. 절충이 줄면서, 편향과 변동의 양을 조심스럽게 밸런스 맞춰야 하는 경우가 줄어든 셈이죠. 그러나 가끔씩은 편향을 줄이거나 변동을 줄이는데 다른 한가지 요소를 올릴 필요 없는 경우가
이 방법 외에 몇 가지 더 있을 수 있습니다. regularized network가 형성되는 경우에 말이죠. 다음 비디오에서부터 일반화에 대해 이야기 해보도록 하겠습니다. 더 큰 네트워크를 트레이닝 하는 것은 더 좋으면 좋지 나쁘다고 하기 어렵습니다. 신경망을 훈련시키는 과정의 주요 과다 비용은 산출 시간이라고 볼 수 있습니다. 물론, 일반화가 진행되고 있다는 가정하에서 입니다. 저는 오늘 내용이 머신 러닝 기본 구조에 대해, 머신러닝의 문제들을 정리하고 편향과 변동을 진단하는 방법에 대해
잘 다뤄졌기를 바랍니다. 여러분이 문제를 다루는데 있어, 이런 내용을 적용하여 본인에게 적합한 구현 방법을
찾으시길 희망합니다. 제가 비디오에서 몇 번 언급했던 것 중 하나가 바로 일반화인데요, 변동을 줄이기 위한 매우 유용한 기술입니다. 일반화를 사용하는 경우에는 편향과 변동 사이 약간의 절충이 있으나, 편향이 약간 증가할 수 있지만, 방대한 크기의 네트워크로 작동하는 이상 그리 편향이 그리 큰 수치는 아닐 것입니다. 일단 다음 비디오에서 더 상세히 이야기 하도록 하겠습니다. 다음 비디오에선 일반화는 더욱 자세히 이해하고 신경망에 적용하는 방법을 배워보겠습니다.