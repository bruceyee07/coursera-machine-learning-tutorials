在上一堂课中 你看到了如何通过观察训练集和开发集上的误差 来诊断你的算法中是否有偏差问题 方差问题 或二者皆有 你能发现这个信息会让你能够让你有条理得多 就是根据大家所说的 机器学习的基本原则 就能让你在改进算法性能时更有条理 我们来看一下 当训练一个神经网络时 这是一个我会遵循的基本原则 当训练好了最初的神经网络时 我会首先问 这个算法是否有高偏差? 要判断是否存在高偏差 你实际上就是要看 模型在训练集的数据上的表现 如果说模型有高偏差 即是模型甚至连训练集都不能良好拟合 你能尝试的一种办法是挑选一个新的网络 比如带有更多隐藏层或更多隐藏单元的 或是延长训练时间 让梯度下降法运行更长时间 或换用一些更高级的优化算法 我们将在这门课程的后面提到它。 另一个可以尝试的办法 这个办法可能有效 也可能无效 但之后总归会看到 因为神经网络的结构有许多种 所以你能够找到一种更加适合当前问题的结构 我把它写在括号里 是因为它是 一种需要你亲自尝试的方法 也许最终你能使它有效 也许不能 相比起来 使用更大的网络几乎总是有效 而延长训练时间 虽然并不永远有用 但是当然也不会造成坏处 所以当训练一个学习算法时 我会尝试这些办法 直到我把偏差问题消除 所以我尝试完 会回到这里 然后再重复尝试 直到至少能良好地符合训练集 通常如果你的网络够大 你应当通常就能够将训练集拟合好 只要这个学习问题是人类能完成的 对吧 如果图片非常模糊 也许就不可能拟合 但如果至少人类能够良好地完成这个任务 如果你认为贝叶斯误差不是太大的话 那只要训练一个足够大的网络 就应当能够 或许应当能够在训练集上取得良好的表现 也就是把训练集拟合 或是过拟合 当把偏差减小到可以接受的范围之后 就再问 这个算法是否有高方差? 要判断这一点 我会看模型在开发集上的表现 看模型是否具备一般化的能力 如在训练集上性能良好 当一般化到开发集上时 仍然性能较好? 如果你有比较高的方差 解决高方差问题的最好方法是取得更多数据 当然前提是你能获取得到 这个办法才有用 但有时你无法获得更多数据 你还可以尝试正则化 这是我们下一节课中会讨论的 用它可以减少过拟合 还有一种 也需要你亲自尝试的方法 就是如果你能找到更合适的神经网络结构 有时也能够在缓解方差问题的同时 也缓解偏差问题 但具体怎么做呢? 这里倒是不太容易总结出完全系统性的规律 所以我也尝试这些办法 完后也回到初始点 直到找到一种低偏差 低方差的网络 然后问题就到此解决了 这里有几点要注意的是 首先 依据你问题的不同 在高偏差和高方差时 你应当尝试的办法有可能很不一样 所以我通常用训练/开发集判断 问题是在高偏差 还是高方差 然后根据这个来选择一些应当尝试的办法 举例来说 如果你有高偏差问题 就算取得更多训练数据也无济于事 至少也不是最有效率的办法 所以明确认识到 是更像高偏差问题 或是高方差问题 或是二者皆备 就能帮助你选择最有用的办法 另外 在早些时代的机器学习中 曾经有许多关于偏差和方差之间的取舍的讨论 这讨论存在的原因是 对于很多你能尝试的办法来说 你只能在增大偏差的同时减小方差 或减小偏差的同时增大方差 但是深度学习之前的时代中 我们能用的工具不是很多 我们没有太多那种能够单独减小偏差 或单独减小方差 而不顾此失彼的工具 但在当前这个深度学习和大数据的时代 只要你能不断扩大所训练的网络的规模 只要你能不断获得更多数据 虽然这两点都不是永远成立的 但如果这两点是可能的 那扩大网络几乎总是能够 减小偏差而不增大方差 只要你用恰当的方式正则化的话 而获得更多数据几乎总是能够 减小方差而不增大偏差 所以归根结底 有了这两步以后 再加上能够选取不同的网络来训练 以及获取更多数据的能力 我们就有了能够且只单独削减偏差 或者能够并且单独削减方差 同时不会过多影响另一个指标的能力 我认为这就是诸多原因中的一个 它能够解释为何深度学习在监督学习中如此有用 以及为何在深度学习中 偏差与方差的权衡要不明显得多 这样你就不需小心地平衡两者 而是因为有了更多选择 可以单独削减偏差 或单独削减方差 而不会同时增加方差或偏差 而且事实上当你有了一个良好地正则化的网络时 我们将在下一节课中讨论正则化 训练一个更大的网络几乎从来没有坏处 当训练的神经网络太大时主要的代价只是计算时间 只要你采取正则化就行 我希望这个视频能给你一些基本的概念 知道如何有条理地诊断机器学习中的偏差与方差问题 然后采取正确的办法来在问题中取得进展 我在视频中多次提到过正则化的概念 它是用于减小方差的一个很有用的办法 在正则化中存在一点点偏差与方差间的权衡 它可能会使偏差增加一点点 虽然在你的网络足够巨大时 增加得通常不会很多 所以让我们在下一个视频中深入讨论一下 如何对神经网络进行正规化
GTC字幕组翻译