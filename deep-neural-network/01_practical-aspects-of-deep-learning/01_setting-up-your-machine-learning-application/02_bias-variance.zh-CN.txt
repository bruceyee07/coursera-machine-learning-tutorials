我注意到，几乎所有真正优秀的机器学习参与者 对偏差（偏离度）和方差（集中度）的处理都是非常有经验的 偏差和方差的处理往往非常容易入门，但是非常难以精通 即使你认为你已经看完了偏差和方差的基础理论 也常常会发现总会有一些出乎你意料的东西 在深度学习领域 另一个现象是有关 偏差-方差困境（也有叫做偏差-方差权衡）的讨论很少 你可能之前已经听说过这个被称为“偏差-方差困境”的东西 但是在深度学习领域，这不仅仅是两者间的权衡问题 我们仍然讨论偏差 也仍然讨论方差 但是我们对偏差-方差困境讨论的不多 让我们一起来看看为什么这么说 大家应该看到了屏幕上这些训练集 如果你用一条直线来区分样本数据 那么用逻辑回归可能画出图上的这条直线 这和训练数据的拟合度并不高 这样的分类我们称之为高偏差 或者换一种说法，这是欠拟合 相对的 如果你使用一个极为复杂的分类器 例如深度神经网络 或者含有所有隐藏神经元的神经网络 或许你可以像图上画的这样完美区分训练数据 但那看上去也并不是一个非常好的分类算法 这个高方差的分类器，我们也称之为过拟合 在这两种分类器之间 我们应该能找到一种不那么复杂的 但是能正确分类的算法，像图上这样 这看起来对数据的区分非常合理 我们认为这才是完美匹配的算法，结果居于欠拟合与过拟合之间 像图上的这个有两个特征值 的2D样例 我们把横轴标为x1，纵轴标为x2，你可以绘制数据，把偏差和方差可视化 在高维度问题中 你无法将数据绘制在图上，并可视化决策边界 然而，对应这样的情况也会有几个不同的方法 让我们来探讨一下，试着去理解偏差和方差的含义 让我们接着讲猫的图片分类的例子 这里有一个正确的样本和一个错误的样本 理解偏差和方差的两个关键数据是 训练集误差和开发集误差 为了便于讨论 我们假设你已经在图片中识别出了猫 这几乎每个人都能完美做到的，对不？ 所以我们可以理解为，你的训练集误差是1%，而对于开发集误差 为了便于讨论 我们假设是11% 在这个例子里 你的模型对训练集处理得非常好 但是相对来说，开发集处理得就有些不尽如人意 所以这可能是在处理训练集时过拟合了 模型在某种程度上对于交叉验证集合 （开发集）泛化性不够好 如果你得到的结果和这个例子一样 我们将它定义成高方差 通过观察训练集误差和开发集误差 你将能判断出你的模型算法是否有高方差的问题 现在，让我们再来看第二个例子，假设你得到了训练集和开发集的误差 有了与第一个例子不一样的结果 假设训练集的误差是15% 我把这个训练集误差写在第一行 假设你的开发集误差是16% 在这种情况下，我们假设人工识别误差是0% 因为人可以直接看到这些图片，并判断出这是否是一只猫 所以看上去，这个算法在训练集上的表现并不尽如人意 如果它并未将训练集数据处理得很好 这就是欠拟合 我们认为这个算法是高偏差的 但相比之下，这个算法应用在开发集时还处于一个可接受的水准 和应用在训练集时的表现相比，误差只多了1% 所以这个算法的问题是高偏差 因为它并不能在训练集上对样本进行很好的识别 这种情况和我们在上一页PPT中最左边的图极为相似 好了，我们来讲下一个例子 假设你的算法在训练集上的误差是15% 这是相当高的偏差 但当你将该算法应用在开发集时，情况变得更糟 可能有30%的误差 在这种情况下，我可以判断出这个算法是高偏差的 它并没有将训练集处理好，并且还是高方差的 这是一种非常、十分、极其糟糕的算法 让我们看看最后一个例子 假设你有0.5%的训练集误差 以及1%的开发集误差 这是让大家都喜闻乐见的算法 对猫的分类仅有1%的误差 所以这个算法是低偏差和低方差的 有一点要注意，我简单地提一下 我们会在之后的视频教程中详细讨论 这种分析方法的前提基于真人识别的 误差为0%的假设 一般来说，我们称之为理想误差 或者有时我们叫它贝叶斯误差 贝叶斯误差接近0% 在这期视频中我就不展开来说这个问题了 但是如果理想误差或贝叶斯误差比较高，假设 是15%，那么如果我们继续来看这个分类算法 15%误差实际上对训练集来说是一个近乎完美的结果了 你不应该认为它是高偏差的、低方差的算法 所以如果没有好的分类器存在， 如何来分析偏差和方差呢？ 假设 如果你的图像真的很模糊 哪怕是真人或者任何系统都不能进行分类 那贝叶斯误差就会非常高 然后会有一些不同的手段来改变分析方法 但是我们现在先不讨论这个问题 而我们可以发现，通过观察 训练集的误差，至少可以知道你的算法是否可以 很好的拟合训练集数据 然后总结出是否属于高偏差问题 然后通过观察同一个算法 在开发集上的误差了多少 可以知道这个算法是否有高方差问题 这样你就能判断训练集上的算法是否在开发集上同样适用 这会让你意识到方差问题 上述结果都基于贝叶斯误差非常低并且 你的训练集和开发集都来自与同一个分布 如果不满足这些假设 那么你需要做一个更复杂的分析 我们将在之后几期视频教程中讨论 好了，在上一张PPT中 我们展示了高偏差 和高方差的形态 我猜你已经意识到一个优秀的分类器看上去是什么样的了 高偏差和高方差的算法看上去如何呢？ 这是一种无比糟糕的情况 你应该还记得，我们说过一个像这样的 线性分类器是高偏差的 因为欠拟合（它没有很好的拟合数据） 所以可以说，由于这几乎是个线性分类算法（直线）因此欠拟合了 我们用紫色来标一下 但是如果在某些情况下你的分类算法
做了些诡异的事情（像我现在画的这样） 那么实际上这部分在部分数据上过拟合了 所以我在紫色部分标示的分类器 同时具有高偏差和高方差的问题 从图上看，这里的偏差很高 因为这几乎是一个线性分类器 而它无法拟合分类 你看，就像图上这个曲线形状的类别 但是这个分类算法中间，又变得十分扭曲 虽然某种程度上来说 它正确地区分了这个叉样本 和这个圈样本。过拟合了这两个孤立的样本 我们称这个分类算法是高偏差的，因为它几乎是直线 但你需要的可能是一个曲线函数或二次函数 同时它也是高方差的 因为它在中间用极为扭曲的算法，对两个孤立的 甚至可能是错误的样本进行了拟合 看上去这比较像人为的 这个样例在二维中看上去不太自然 但是当你有相当高维度的输入特征 你可能在获取特征的过程中 在一些地方遇到了高偏差的问题，一些地方遇到了高方差的问题 所以很可能最终得到了在高维度特征输入情况下 产生了这样不自然的分类 让我们总结一下，这节课我们学习了如何通过观察算法 在训练集和开发集的误差来诊断 它是否有高偏差或者高方差的问题 或许两者都有，或许都没有 基于算法遇到高偏差或高方差问题的不同情况 我们可以尝试不同的方法来进行改进 在下一个视频教程中，我想要给你介绍一种方法 我称之为机器学习的基本准则 那会让你在遇到高偏差或者高方差问题时 能够更系统地去尝试改进你的算法 让我们继续下一期教程