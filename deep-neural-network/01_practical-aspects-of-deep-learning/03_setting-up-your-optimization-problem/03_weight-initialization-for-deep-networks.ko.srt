1
00:00:00,530 --> 00:00:04,590
이전 비디오를 통해서 매우 깊은 심층신경망에서는 

2
00:00:04,590 --> 00:00:08,330
vanishing 과 exploding gradient의 문제점이 있을 수 있다는 것을 배웠는데요. 

3
00:00:08,330 --> 00:00:11,040
부분적으로 해결할 수 있는 것은, 

4
00:00:11,040 --> 00:00:13,455
완전히 해결해주지는 않지만, 

5
00:00:13,455 --> 00:00:18,915
여러분의 신경망에서 random initialization을 선정하는데 있어 조금 더 괜찮거나, 조심스러운 선택을 하는 것입니다. 

6
00:00:18,915 --> 00:00:23,220
이것을 이해하기 위해, 하나의 신경세포를 초기화하는 방법에 대한 예시로 시작해 보도록 하죠, 

7
00:00:23,220 --> 00:00:27,505
우리는 이것을 깊은 네트워크로 일반화 할 것입니다.

8
00:00:27,505 --> 00:00:30,040
단 하나의 신경세포로 된 예를 살펴보고, 

9
00:00:30,040 --> 00:00:33,140
그리고 나서 나중에 깊은 그물에 대해 이야기 하겠습니다.

10
00:00:33,140 --> 00:00:39,060
하나의 신경세포를 입력값으로 넣는데요 x1에서부터 x4까지 말입니다. 그리고 

11
00:00:39,060 --> 00:00:42,465
a=g(z) 가 있고 y로 마무리합니다.

12
00:00:42,465 --> 00:00:46,830
그리고 나중에 더 깊은 네트워크에서는 이 입력값들은 

13
00:00:46,830 --> 00:00:51,780
a(l)이라는 층이 될 것입니다. 하지만 지금은 x라고 그냥 하겠습니다.

14
00:00:51,780 --> 00:00:59,070
그러면 z는 w1x1 + w2x2 +... + WnXn까지 가는데요, 

15
00:00:59,070 --> 00:01:08,570
b는 일단 0으로 설정하겠습니다. b는 일단 무시하겠습니다.

16
00:01:08,570 --> 00:01:12,390
그러면 z가 폭발적인 값을 갖지 않고 또 

17
00:01:12,390 --> 00:01:16,960
너무 작지 않게 하기 위해선 n이 크면 클수록

18
00:01:16,960 --> 00:01:22,070
Wi는 더 작게 원할 것입니다. 맞죠?

19
00:01:22,070 --> 00:01:25,910
z는 wixi값의 합이기 때문에 그렇죠, 

20
00:01:25,910 --> 00:01:30,680
그리고 이런 항들을 더하는 경우, 각각의 항이 작은 값을 가져야 좋습니다.

21
00:01:30,680 --> 00:01:41,045
한가지 합리적인 방법은 wi의 편차를 1 나누기 n으로 하는 것입니다. 

22
00:01:41,045 --> 00:01:45,495
여기서 n은 뉴런으로 들어가는 입력 특성의 개수입니다. 

23
00:01:45,495 --> 00:01:51,441
실제로 할 수 있는 것은, 비중인 W 매트릭스를 설정합니다. 특정 층에 대해서 말이죠. 

24
00:01:51,441 --> 00:01:58,045
이것은 np.random.randn로 합니다.

25
00:01:58,045 --> 00:02:02,205
그 후로, 이것이 생긴 매트릭스의 모양이 어떻던 간에, 여기 넣고요

26
00:02:02,205 --> 00:02:06,810
이것을 곱하기 루트 1나누기 

27
00:02:06,810 --> 00:02:12,890
l 층의 뉴런에 들어가는 입력 특성의 개수입니다.

28
00:02:12,890 --> 00:02:14,605
이것은 n(l-1)인데요. 

29
00:02:14,605 --> 00:02:20,700
그 이유는 그것이 l층에 들어가는 각각의 유닛별 숫자입니다. 

30
00:02:20,700 --> 00:02:23,340
만약에 ReLu activation 함수를 쓰는 경우에는 

31
00:02:23,340 --> 00:02:28,830
1/n을 쓰는 것 대신에, 

32
00:02:28,830 --> 00:02:32,105
편차를 2/n으로 설정하는 것이 조금 더 잘 작동합니다.

33
00:02:32,105 --> 00:02:35,580
특히 relu activation 함수를 쓰는 경우에는, 초기화의 경우 볼 수 있는데요, 

34
00:02:35,580 --> 00:02:42,425
공식이 gl(z)는 ReLu(z)인 경우이죠.

35
00:02:42,425 --> 00:02:45,030
여러분이 얼마나 random variable에 익숙한지에 따라서, 

36
00:02:45,030 --> 00:02:46,590
알고 보니, 

37
00:02:46,590 --> 00:02:50,835
Gaussian random variable 과 이것의 루트를 곱하는 것이

38
00:02:50,835 --> 00:02:54,330
편차가 이 값이 되게 해줍니다.

39
00:02:54,330 --> 00:02:59,485
2 나누기 n 말이죠. 그리고 제가 n에서 n 위첨자 l-1로 간 이유는, 

40
00:02:59,485 --> 00:03:02,610
여기 예제에서의 n개의 입력값이 있는 로지스틱 회귀분석이 있는데요 

41
00:03:02,610 --> 00:03:05,625
n 개의 입력 특성을 갖습니다. 더 일반적인 케이스는, 

42
00:03:05,625 --> 00:03:12,400
l층은 n (l-1)층을 가질 것이고 이 입력값이 층마다 있을 것입니다.

43
00:03:12,400 --> 00:03:19,305
만약 입력 activation의 입력 특성이 대략적으로 평균 0 와 편차 1이면

44
00:03:19,305 --> 00:03:22,760
이것은 z가 비슷한 scale을 갖게 할 것이고

45
00:03:22,760 --> 00:03:26,580
문제해결이 되지 않을 것입니다.

46
00:03:26,580 --> 00:03:30,630
하지만 확실히 vanishing과 

47
00:03:30,630 --> 00:03:33,240
exploding gradient 문제를 도와주기는 합니다.

48
00:03:33,240 --> 00:03:36,510
왜냐면 w하는 비중 매트릭스를 세팅해서

49
00:03:36,510 --> 00:03:42,900
1조다 너무 크지 않고, 1보다 너무 작지 않게 해서 너무 빨리 explode하거나 vanish하지 않게 하려는 것입니다.

50
00:03:42,900 --> 00:03:45,075
다른 몇가지 variant들을 언급했었는데요, 

51
00:03:45,075 --> 00:03:47,640
저희가 정의한 버전은 

52
00:03:47,640 --> 00:03:51,270
ReLu activation 함수를 가정한 것인데요, 학술 페이퍼에서 인용한 것입니다.

53
00:03:51,270 --> 00:03:53,750
또 다른 variant가 있는데요, 

54
00:03:53,750 --> 00:03:57,600
여러분이 TanH activation 함수를 이용하는 경우, 

55
00:03:57,600 --> 00:04:02,060
논문에서 알 수 있지만, 상수 2를 쓰는 것이 

56
00:04:02,060 --> 00:04:06,870
상수 1을 쓰는 것보다 낫습니다.

57
00:04:06,870 --> 00:04:12,615
즉, 1나누기 이것, 2 대신에 말이죠. 그리고 이것의 루트와 곱합니다.

58
00:04:12,615 --> 00:04:16,605
그래서 여기 루트로 된 항이 

59
00:04:16,605 --> 00:04:23,165
여기 있는 항을 대체할 것입니다. TanH activation 함수를 이용하는 것입니다.

60
00:04:23,165 --> 00:04:26,790
이것을 Xavier initialization이라고 하는데요

61
00:04:26,790 --> 00:04:30,870
또 다른 버전은 Yoshua Bengio외 몇 명의 학자들이 가르쳤는데요, 

62
00:04:30,870 --> 00:04:32,190
여러분은 논문에서 봤을 수도 있습니다.

63
00:04:32,190 --> 00:04:36,140
이 공식을 이용합니다.

64
00:04:36,140 --> 00:04:40,300
여러 이론적인 정의가 있겠죠

65
00:04:40,300 --> 00:04:43,800
제가 말하고 싶은 것은 만약 ReLu activation 함수를 이용하는 경우, 

66
00:04:43,800 --> 00:04:46,605
가장 흔한 activation 함수인데요, 

67
00:04:46,605 --> 00:04:48,555
저는 이 공식을 사용할 것입니다.

68
00:04:48,555 --> 00:04:53,610
만약 TanH를 사용한다면 이 버전을 대신해서 사용할 수 있습니다. 몇몇 저자들은 이것을 이용할 것입니다.

69
00:04:53,610 --> 00:04:58,665
실제로는 그러나 이 공식들은 모두 시작점을 제시합니다.

70
00:04:58,665 --> 00:05:01,210
여러분의 weight matrixes의 초기화의 편차 값의 

71
00:05:01,210 --> 00:05:04,270
기본값을 줍니다.

72
00:05:04,270 --> 00:05:06,760
여러분이 원하면, 여기있는 편차를, 

73
00:05:06,760 --> 00:05:09,875
편차 매개 변수가 여러분이 튜닝할 수 있는

74
00:05:09,875 --> 00:05:13,530
하이퍼 파라미터중 하나 일 수 있습니다.

75
00:05:13,530 --> 00:05:16,930
또 다른 파라미터가 여기 공식에 곱해질 수도 있는데요 

76
00:05:16,930 --> 00:05:21,075
그리하여 하이퍼 파라미터 surge를 위해 그 multiplier 값을 튜닝할 수 있겠습니다. 

77
00:05:21,075 --> 00:05:26,105
가끔씩은 하이퍼 파라미터를 튜닝하는 것이 적당한 크기의 효과가 있습니다.

78
00:05:26,105 --> 00:05:29,670
이것은 제가 튜닝을 시도할 법한 하이퍼 파라미터 중 첫번째는 아닌데요, 

79
00:05:29,670 --> 00:05:33,325
이것을 통해 튜닝을 한 몇 개의 문제들을 접했었는데요

80
00:05:33,325 --> 00:05:37,680
비교적으로 도움이 되는 편입니다. 하지만 저에게는 선호도에 있어서 이것은 

81
00:05:37,680 --> 00:05:42,870
밑에 있는 편입니다. 튜닝할 수 있는 다른 하이퍼 파라미터와 비교했을 때 말이죠.

82
00:05:42,870 --> 00:05:47,520
이것이 폭발적으로 증가는 기울기나 사라지는 것에 대한 

83
00:05:47,520 --> 00:05:49,935
직관적인 부분을 알려주고

84
00:05:49,935 --> 00:05:52,955
비중을 초기화시키는데 합리적인 scaling을 고를 수 있도록 직관적으로 여러분에게 도움이 됐었으면 좋겠습니다. 

85
00:05:52,955 --> 00:05:55,620
그것이 여러분의 비중을 너무 빨리 폭발하지 않고 

86
00:05:55,620 --> 00:05:58,890
너무 빨리 0으로 줄어들지 않도록 할 것입니다.

87
00:05:58,890 --> 00:06:01,650
여러분이 비중이나 기울기가 폭발적으로 늘어나거나 사라지지 않게 

88
00:06:01,650 --> 00:06:05,730
비교적 깊은 네트워크를 트레이닝 시키기 위해서 말이죠.

89
00:06:05,730 --> 00:06:08,580
깊은 네트워크를 트레이닝시킬 때 이것은 신경 네트워크를 많이 트레이닝 

90
00:06:08,580 --> 00:06:11,640
시키는데 도움을 주는 도움을 주는 또 다른 기술이다.