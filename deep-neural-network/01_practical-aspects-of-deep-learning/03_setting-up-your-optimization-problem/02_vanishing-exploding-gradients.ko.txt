신경망을 교육시키는 것의 가장 큰 문제점 중에 하나는, 특히, 심층신경망이 그런데요, 바로 데이터가 사라지는 경우와 기울기가 폭발적인 증가를 하는 경우입니다. 무슨 뜻이냐며, 심층신경망을 트레이닝을 하는 경우, derivatives나 기울기가 굉장히 큰 값을 갖게 되거나, 굉장히 작은 값을 갖게 될 수 있습니다. 심지어 기하급수적으로 작아질 수도 있죠. 이런 경우, 트레이닝이 매우 까다로울 수 있습니다. 이번 비디오에선, 폭발적인 기울기를 갖거나 사라지는 기울기라는 것의 문제점이 무엇인지 보게 될 것입니다. 또한, 무작위 체중 초기화라는 신중한 방법을 통하여 이러한 문제점들을 상당부분 줄이는 것도 보도록 하겠습니다. 아주 깊은 심층신경망을 트레이닝 하지 않는 이상, 슬라이드의 공간을 줄이기 위한 목적으로 말이죠, 한 레이어에 단순히 두 개의 숨겨진 유닛만 있는 것처럼 그렸습니다. 그렇지만 더 있을 수도 있습니다. 신경망은 변수 W1, W2, W3 등등 WL까지 그 값을 가질 것입니다. 단순하게 하기 위해서, Z는 Z의 활성화 함수 G인 함수를 써본다고 가정해봅시다. 일차 활성화 함수죠. 그리고 B는 일단 무시하고, L의 B는 0이라고 해봅시다. 이런 겨우, Y 결과값이 WL 곱하기 WL, 빼기 1, 곱하기 WL, 빼기 2 점. 점. 점. 계속해서 W3, W2, W1 곱하기 X. 저의 계산을 검산해보고 싶으시면, W1 곱하기 X 는 Z1이 될 것이고, B는 0이기 때문이죠. Z1은, 제 생각에는 W1 곱하기 X 그리고 더하기 B이므로 또 0이 됩니다. 반면에 A1은 Z1의 G인데요, 일차 활성화 함수를 쓰기 때문에, 이것은 그냥 Z1이 됩니다. 첫 번째 항 W1X는 A1이 되고요, 추론을 통하여, W2 곱하기 W1 곱하기 X는 A2라는 것을 알 수 있습니다. 이것이 Z2의 G가 되기 때문에, W2의 G 곱하기 A1이고, 이 값을 여기에 대입시킬 수 있습니다. 그렇기 때문에 이것은 A2가 될 것이고, 이것은, A3이 되고, 이렇게 계속해서 매트릭스의 프로토콜이 Y-hat 값을 줄
때까지, 이제 여러분의 WL 매트릭스 각각 비중이 WL은 1 곱하기 아이덴티티 보다 조금 더 큽니다. 그러므로, 1.5_1.5_0_0. 인 거죠. 엄밀히 이야기하자면 마지막 것은 dimension이 다른데요, 나머지 매트릭스 비중이 이 값일 수도 있겠죠. 그러면 Y-hat은, dimension이 다른 마지막 값을 제외하고는, 이 1.5_0_0_1.5 매트릭스의 L 빼기 1 곱하기 X 승이 되겠습니다. 그 이유는 각각의 매트릭스가 이 값이 된다고 가정하는데요, 실질적으로 1.5 곱하기 아이덴티티 메트릭스를 하면 이렇게 산출이 됩니다. 그렇게 해서 Y-hat은 근본적으로 1.5의 L승, L 빼기 1 승 곱하기 X입니다. 그리고 아주 깊은 심경심층 망의 L값이 매우 크다고 하면 Y-hat값 또한 매우 클 것입니다. 사실상, 기하급수적으로 성장하게 됩니다. n이 레이어의 개수라고 하면, 1.5의 n승으로 늘어납니다. 그렇기 때문에 아주 깊은 심층신경망이 있다고 하면, Y의 값은 폭발적인 값이 나올 것입니다. 반대로, 이 값을 0.5로 바꾸면, 그러니까 1보다 작은 값으로 바꾼다고 하면, 이건 0.5의 L승이 됩니다. 이 매트릭스는 0.5의 L 빼기 1 곱하기 X 승이 되고요. 다시 한번 WL은 무시할 경우에 말이죠. 각각의 매트릭스는 1보다 작은 값이 되는데요, 그러면, X1과 X2가 1보다 작은 값이라고 가정해봅시다. 활성 값은 그렇게 하면 2분의1이 되고요. 2분의 1, 4분의 1. 4분의 1, 8분의 1, 8분의 1, 이렇게 2분의 1의 L승이 될 때까지 갑니다. 그러므로, 활성화 값은 def의 식에 따라, 네트워크 레이어의 수 L을 나타내는 식에 따라 기하급수적으로 감소할 것입니다. 그러므로 매우 깊은 네트워크에서는, 활성화가 기하급수적으로 줄어들게 됩니다. 여기서 여러분이 기억할만한 직관적인 부분은 바로 W 비중에 따라서, 다른 모든 것들이 1보다 조금 큰 값이라고 할 때, 또는 모든 것들이 아이덴티티 매트릭스보다 조금 더 큰 값이라고 할 때, 매우 깊은 심층 네트워크로 이루어져 있을 시, 
활성화가 폭발적일 수 있다는 것입니다. 그리고 만약 W값이 아이덴티티보다 조금 작을 경우, 그러면 이 값이 0.9, 0.9 이렇게 될 수 있겠죠, 이런 경우, 아주 깊은 심층망 네트워크로 이루어져 있고, 활성화가 기하급수적으로 감소할 것입니다. 물론 이 논쟁의 의견은 L의 함수를 기반으로 증가하거나 기하급수적으로 감소하는 활성화에 대한 내용에 중점을 두고 있지만 비슷한 논리를 이용하여 컴퓨터가 전송하는 derivative 또는 기울기 값이 기하급수적으로 증가하는 것을 보여줄 수 있고, 아니면 레이어의 개수에 따라 기하급수적으로 감소하는 것도 같이 보여줄 수 있습니다. L값이 150인 최근 버전의 신경망을 이용하여 마이크로소프트는 152개의 레이어를 가진 신경망으로 좋은 결과를 얻었는데요, 이런 매우 깊은 신경망은 L의 함수대로 활성화나 기울기가 기하급수적으로 증가 또는 감소하는 경우 그 값 또한 매우 커지거나 작아질 수 있습니다. 이러한 점이 트레이닝을 어렵게 만드는데요. 특히, L과 비교하여 기울기가 급격히 작으면 기울기 하강이 천천히 이루어질 것입니다. 기울기 하강을 교육시키는 데는 오랜 시간이 소요될 것입니다. 요약하자면, 여러분은 이제까지 깊은 심층신경망에서 사라지거나 폭발적인 기울기 값을 갖는 경우를 보았는데요, 사실 이 문제는 오랫동안 심층신경망을 트레이닝 시키는 과정에서 큰 장벽으로 작용해왔는데요. 알고 보니, 부분적으로나마 해결책이 있었습니다. 문제를 완전히 해결하진 못하지만, 큰 도움이 되는 해결책입니다. 이 방법은 비중을 신중하게 초기화하는 방법인데요. 더 자세히 다루기 위해서 다음 비디오로 넘어가도록 하겠습니다.