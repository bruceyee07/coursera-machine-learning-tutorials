1
00:00:00,610 --> 00:00:03,220
上一段影片中
您學到了梯度檢查

2
00:00:03,220 --> 00:00:06,317
在這段影片, 我希望分享您
一些實作的技巧或

3
00:00:06,317 --> 00:00:10,950
一些筆記在如何真正
建立梯度檢查在您的神經網路上

4
00:00:10,950 --> 00:00:14,590
首先，不要使用梯度檢查來做訓練
只用在除錯時

5
00:00:14,590 --> 00:00:19,470
我的意思是
當計算 dθ approx i 時

6
00:00:19,470 --> 00:00:22,520
對於所有 i
這是很緩慢的計算

7
00:00:22,520 --> 00:00:26,320
所以當建立梯度下降時
您會用反向傳播來計算 dθ

8
00:00:26,320 --> 00:00:29,110
然後用反向傳播來計算導數

9
00:00:29,110 --> 00:00:32,140
而只有當您除錯時
您會計算這個

10
00:00:32,140 --> 00:00:34,218
來確定它接近於 dθ

11
00:00:34,218 --> 00:00:37,048
但當您做完檢查後
您要關掉梯度檢查

12
00:00:37,048 --> 00:00:39,502
不要在每個梯度下降
的迴圈中執行它

13
00:00:39,502 --> 00:00:41,530
因為它就是很慢

14
00:00:41,530 --> 00:00:45,060
第二, 如果演算法在梯度檢查時失敗了
看一下分量

15
00:00:45,060 --> 00:00:48,010
看每一個分量
試著找出錯誤

16
00:00:48,010 --> 00:00:52,124
我的意思是如果 dθ approx 
 遠離於 dθ

17
00:00:52,124 --> 00:00:57,079
我會看哪一個 i 的值使得

18
00:00:57,079 --> 00:01:02,360
dθ approx 是非常不同於 dθ 的值

19
00:01:02,360 --> 00:01:06,842
舉個例子, 如果您發現
 θ (口誤) 或者 dθ 的值

20
00:01:06,842 --> 00:01:11,495
它們遠離於
相對於 db[l] 在一些層中

21
00:01:11,495 --> 00:01:16,162
但 dw 的分量卻相當接近

22
00:01:16,162 --> 00:01:20,803
記得, 不同 θ 的分量代表
不同分量的

23
00:01:20,803 --> 00:01:21,434
b 跟 w

24
00:01:21,434 --> 00:01:25,918
當您發現如此時
您或許發現這個錯誤是

25
00:01:25,918 --> 00:01:30,411
在您計算 db 時發生
導數相對於參數 b

26
00:01:30,411 --> 00:01:35,495
同樣, 反之亦然, 如果您發現值是非常遠的

27
00:01:35,495 --> 00:01:39,610
dθ approx 的值遠離於 dθ

28
00:01:39,610 --> 00:01:44,452
您發現所有這些分量來自於 dw
或者一些層的 dw

29
00:01:44,452 --> 00:01:48,455
這或許幫助您趨近
錯誤的位置

30
00:01:48,455 --> 00:01:51,562
這不一定總是讓您
馬上找出錯誤

31
00:01:51,562 --> 00:01:55,622
但有時候幫助您給您一些猜測
有關哪裡去追這些錯誤

32
00:01:56,782 --> 00:01:59,502
接下來, 當做梯度檢查時

33
00:01:59,502 --> 00:02:03,372
記得您的正則化 (regularization) 項目
如果您使用正則化的話

34
00:02:03,372 --> 00:02:10,052
所以如果您的成本函數是
 J(θ) 等於 1 除以 m 總和於您的

35
00:02:10,052 --> 00:02:15,570
損失而
加上這個正則化項目

36
00:02:15,570 --> 00:02:22,790
然後總和於 l of wl 平方
那這是 J 的定義

37
00:02:22,790 --> 00:02:27,200
您應該有 dθ 
是 J 的梯度

38
00:02:27,200 --> 00:02:30,840
相對於 θ 
包含這個正則化項目

39
00:02:30,840 --> 00:02:32,880
所以請記得包含這個項目

40
00:02:32,880 --> 00:02:37,185
下一個, 梯度檢查對 dropout 沒有用, 因為在每次迴圈時

41
00:02:37,185 --> 00:02:41,307
dropout 隨機消除了
不同子集的隱藏單元

42
00:02:41,307 --> 00:02:45,923
要計算成本函數 J,
當 dropout 

43
00:02:45,923 --> 00:02:48,098
應用在梯度下降時
是很不容易的

44
00:02:48,098 --> 00:02:52,932
實際上 dropout 可視為
一些成本函數 J 的最佳化

45
00:02:52,932 --> 00:02:57,254
但因為成本函數 J 定義為
總和於所有指數大的

46
00:02:57,254 --> 00:03:00,900
節點的子集他們可能
被任何一個迴圈消除

47
00:03:00,900 --> 00:03:04,780
所以成本函數 J 是
很難去計算

48
00:03:04,780 --> 00:03:07,560
因為您會每次取樣於成本函數

49
00:03:07,560 --> 00:03:11,770
在您使用 dropout 時
您會消除不同隨機子集

50
00:03:11,770 --> 00:03:14,730
所以使用梯度檢查
來重複檢查您的

51
00:03:14,730 --> 00:03:16,810
應用 dropout 的計算
是很難的

52
00:03:16,810 --> 00:03:20,360
我通常建立梯度檢查時
不使用 dropout

53
00:03:20,360 --> 00:03:25,285
如果您要的話, 您可以設定
dropout 裡的 keep_prob 為 1.0

54
00:03:25,285 --> 00:03:29,590
然後開啟 dropout 希望
我的 dropout 建置是正確的

55
00:03:30,770 --> 00:03:35,738
有一些方式您可以做
像是固定某種形式的節點消除

56
00:03:35,738 --> 00:03:39,914
利用梯度檢查來測試
這種形式是正確的

57
00:03:39,914 --> 00:03:43,200
但實作時
我通常不這樣用

58
00:03:43,200 --> 00:03:48,010
所以我的建議是先關掉 dropout
使用梯度檢查來檢查

59
00:03:48,010 --> 00:03:52,560
當不使用 dropout 時, 您的演算法是正確的
然後才打開 dropout

60
00:03:52,560 --> 00:03:55,520
最後, 這很微妙

61
00:03:55,520 --> 00:03:59,853
這不是不可能, 或者說
極少發生但不是不可能

62
00:03:59,853 --> 00:04:04,322
您建立梯度下降是正確的
當 w 跟 b 接近於 0

63
00:04:04,322 --> 00:04:06,500
在隨機初始化時

64
00:04:06,500 --> 00:04:10,223
但當您跑梯度下降時
 w 跟 b 越來越大

65
00:04:10,223 --> 00:04:15,089
或許您的反向傳播建置在
w 跟 b 接近於 0 時是正確的

66
00:04:15,089 --> 00:04:18,660
但當 w 跟 b 越來越大時
它變得不正確

67
00:04:18,660 --> 00:04:21,510
您可以做一件事
我通常很少這樣做

68
00:04:21,510 --> 00:04:25,850
但您可以做一件事是
在隨機初始時跑梯度檢查

69
00:04:25,850 --> 00:04:27,940
然後訓練網路一段時間
當 w 跟 b

70
00:04:27,940 --> 00:04:33,198
慢慢從 0 離開
從您小的初始值離開

71
00:04:33,198 --> 00:04:37,620
然後訓練一些迴圈後
再跑一次梯度檢查

72
00:04:37,620 --> 00:04:39,165
所以這是梯度檢查

73
00:04:39,165 --> 00:04:42,760
恭喜您來到這個禮拜
最後的內容

74
00:04:42,760 --> 00:04:47,100
在這一週, 您學習如何
設定您的訓練, 開發及測試集

75
00:04:47,100 --> 00:04:51,254
如何分析偏差及變異
當如果您有高偏差

76
00:04:51,254 --> 00:04:54,230
或高變異時跟
同時高偏差跟高變異時, 怎麼辦

77
00:04:54,230 --> 00:04:57,930
您也學了如何
應用不同種類的正則化 (regularization)

78
00:04:57,930 --> 00:05:02,070
像是 L2 正則化跟
 dropout 在您的神經網路

79
00:05:02,070 --> 00:05:05,318
然後一些技巧來
加快訓練您的神經網路

80
00:05:05,318 --> 00:05:07,920
最後是, 梯度檢查

81
00:05:07,920 --> 00:05:10,480
我想這個禮拜您看了很多內容

82
00:05:10,480 --> 00:05:14,300
您將練習這些觀念在
這個禮拜的程式練習中

83
00:05:14,300 --> 00:05:15,520
祝您幸運

84
00:05:15,520 --> 00:05:17,830
我期望在第二週再遇見您