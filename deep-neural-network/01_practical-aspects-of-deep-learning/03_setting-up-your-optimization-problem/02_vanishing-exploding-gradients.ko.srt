1
00:00:00,000 --> 00:00:02,352
신경망을 교육시키는 것의 가장 큰 문제점 중에 하나는,

2
00:00:02,352 --> 00:00:04,585
특히, 심층신경망이 그런데요,

3
00:00:04,585 --> 00:00:07,395
바로 데이터가 사라지는 경우와 기울기가 폭발적인 증가를 하는 경우입니다.

4
00:00:07,395 --> 00:00:09,180
무슨 뜻이냐며, 심층신경망을 트레이닝을 하는 경우,

5
00:00:09,180 --> 00:00:13,650
derivatives나 기울기가 굉장히 큰 값을 갖게 되거나,

6
00:00:13,650 --> 00:00:15,825
굉장히 작은 값을 갖게 될 수 있습니다.

7
00:00:15,825 --> 00:00:17,420
심지어 기하급수적으로 작아질 수도 있죠.

8
00:00:17,420 --> 00:00:19,450
이런 경우, 트레이닝이 매우 까다로울 수 있습니다.

9
00:00:19,450 --> 00:00:21,690
이번 비디오에선, 폭발적인 기울기를 갖거나

10
00:00:21,690 --> 00:00:25,185
사라지는 기울기라는 것의 문제점이 무엇인지 보게 될 것입니다.

11
00:00:25,185 --> 00:00:28,630
또한, 무작위 체중 초기화라는 신중한 방법을 통하여  

12
00:00:28,630 --> 00:00:32,780
이러한 문제점들을 상당부분 줄이는 것도 보도록 하겠습니다.

13
00:00:32,780 --> 00:00:36,015
아주 깊은 심층신경망을 트레이닝 하지 않는 이상,

14
00:00:36,015 --> 00:00:37,210
슬라이드의 공간을 줄이기 위한 목적으로 말이죠,

15
00:00:37,210 --> 00:00:40,508
한 레이어에 단순히 두 개의 숨겨진 유닛만 있는 것처럼 그렸습니다.

16
00:00:40,508 --> 00:00:42,575
그렇지만 더 있을 수도 있습니다.

17
00:00:42,575 --> 00:00:45,625
신경망은 변수 W1, 

18
00:00:45,625 --> 00:00:51,585
W2, W3 등등 WL까지 그 값을 가질 것입니다.

19
00:00:51,585 --> 00:00:53,025
단순하게 하기 위해서,

20
00:00:53,025 --> 00:00:56,960
Z는 Z의 활성화 함수 G인 함수를 써본다고 가정해봅시다.

21
00:00:56,960 --> 00:00:58,725
일차 활성화 함수죠.

22
00:00:58,725 --> 00:01:02,985
그리고 B는 일단 무시하고, L의 B는 0이라고 해봅시다.

23
00:01:02,985 --> 00:01:07,755
이런 겨우, Y 결과값이

24
00:01:07,755 --> 00:01:13,700
WL 곱하기 WL, 빼기 1, 곱하기 WL, 빼기 2

25
00:01:13,700 --> 00:01:18,193
점. 점. 점. 계속해서 W3,

26
00:01:18,193 --> 00:01:21,445
W2, W1 곱하기 X.

27
00:01:21,445 --> 00:01:23,830
저의 계산을 검산해보고 싶으시면,

28
00:01:23,830 --> 00:01:27,915
W1 곱하기 X 는 Z1이 될 것이고,

29
00:01:27,915 --> 00:01:30,225
B는 0이기 때문이죠.

30
00:01:30,225 --> 00:01:33,540
Z1은, 제 생각에는

31
00:01:33,540 --> 00:01:37,960
W1 곱하기 X 그리고 더하기 B이므로 또 0이 됩니다.

32
00:01:37,960 --> 00:01:42,440
반면에 A1은 Z1의 G인데요,

33
00:01:42,440 --> 00:01:45,150
일차 활성화 함수를 쓰기 때문에,

34
00:01:45,150 --> 00:01:47,755
이것은 그냥 Z1이 됩니다.

35
00:01:47,755 --> 00:01:50,360
첫 번째 항 W1X는 A1이 되고요,

36
00:01:50,360 --> 00:01:57,950
추론을 통하여, W2 곱하기 W1 곱하기 X는 A2라는 것을 알 수 있습니다.

37
00:01:57,950 --> 00:02:00,118
이것이 Z2의 G가 되기 때문에,

38
00:02:00,118 --> 00:02:03,565
W2의 G 곱하기 A1이고,

39
00:02:03,565 --> 00:02:12,570
이 값을 여기에 대입시킬 수 있습니다.

40
00:02:12,570 --> 00:02:16,690
그렇기 때문에 이것은 A2가 될 것이고, 

41
00:02:16,690 --> 00:02:21,505
이것은,

42
00:02:21,505 --> 00:02:29,065
A3이 되고, 이렇게 계속해서 매트릭스의 프로토콜이 Y-hat 값을 줄
때까지,

43
00:02:29,065 --> 00:02:33,080
이제 여러분의 WL 매트릭스 각각 비중이

44
00:02:33,080 --> 00:02:39,677
WL은 1 곱하기 아이덴티티 보다 조금 더 큽니다.

45
00:02:39,677 --> 00:02:43,825
그러므로, 1.5_1.5_0_0. 인 거죠.

46
00:02:43,825 --> 00:02:46,000
엄밀히 이야기하자면 마지막 것은 dimension이 다른데요,

47
00:02:46,000 --> 00:02:49,220
나머지 매트릭스 비중이 이 값일 수도 있겠죠.

48
00:02:49,220 --> 00:02:51,508
그러면 Y-hat은,

49
00:02:51,508 --> 00:02:54,903
dimension이 다른 마지막 값을 제외하고는,

50
00:02:54,903 --> 00:03:01,770
이 1.5_0_0_1.5 매트릭스의 L 빼기 1 곱하기 X 승이 되겠습니다. 

51
00:03:01,770 --> 00:03:08,050
그 이유는 각각의 매트릭스가 이 값이 된다고 가정하는데요,

52
00:03:08,050 --> 00:03:12,945
실질적으로 1.5 곱하기 아이덴티티 메트릭스를 하면 이렇게 산출이 됩니다.

53
00:03:12,945 --> 00:03:19,150
그렇게 해서 Y-hat은 근본적으로 1.5의 L승,

54
00:03:19,150 --> 00:03:21,715
L 빼기 1 승 곱하기 X입니다.

55
00:03:21,715 --> 00:03:24,505
그리고 아주 깊은 심경심층 망의 L값이 매우 크다고 하면 

56
00:03:24,505 --> 00:03:26,640
Y-hat값 또한 매우 클 것입니다.

57
00:03:26,640 --> 00:03:28,375
사실상, 기하급수적으로 성장하게 됩니다.

58
00:03:28,375 --> 00:03:32,145
n이 레이어의 개수라고 하면, 1.5의 n승으로 늘어납니다. 

59
00:03:32,145 --> 00:03:34,290
그렇기 때문에 아주 깊은 심층신경망이 있다고 하면,

60
00:03:34,290 --> 00:03:36,850
Y의 값은 폭발적인 값이 나올 것입니다.

61
00:03:36,850 --> 00:03:40,832
반대로, 이 값을 0.5로 바꾸면,

62
00:03:40,832 --> 00:03:42,268
그러니까 1보다 작은 값으로 바꾼다고 하면,

63
00:03:42,268 --> 00:03:45,860
이건 0.5의 L승이 됩니다.

64
00:03:45,860 --> 00:03:51,515
이 매트릭스는 0.5의 L 빼기 1 곱하기 X 승이 되고요. 다시 한번 WL은 무시할 경우에 말이죠.

65
00:03:51,515 --> 00:03:57,220
각각의 매트릭스는 1보다 작은 값이 되는데요,

66
00:03:57,220 --> 00:04:00,415
그러면, X1과 X2가 1보다 작은 값이라고 가정해봅시다.

67
00:04:00,415 --> 00:04:02,778
활성 값은 그렇게 하면 2분의1이 되고요.

68
00:04:02,778 --> 00:04:04,450
2분의 1, 4분의 1.

69
00:04:04,450 --> 00:04:07,227
4분의 1, 8분의 1, 8분의 1,

70
00:04:07,227 --> 00:04:11,470
이렇게 2분의 1의 L승이 될 때까지 갑니다. 

71
00:04:11,470 --> 00:04:16,710
그러므로, 활성화 값은 def의 식에 따라,

72
00:04:16,710 --> 00:04:19,945
네트워크 레이어의 수 L을 나타내는 식에 따라 기하급수적으로 감소할 것입니다.

73
00:04:19,945 --> 00:04:26,150
그러므로 매우 깊은 네트워크에서는, 활성화가 기하급수적으로 줄어들게 됩니다. 

74
00:04:26,150 --> 00:04:30,940
여기서 여러분이 기억할만한 직관적인 부분은 바로 W 비중에 따라서,

75
00:04:30,940 --> 00:04:33,760
다른 모든 것들이 1보다 조금 큰 값이라고 할 때,

76
00:04:33,760 --> 00:04:36,805
또는 모든 것들이 아이덴티티 매트릭스보다 조금 더 큰 값이라고 할 때,

77
00:04:36,805 --> 00:04:41,290
매우 깊은 심층 네트워크로 이루어져 있을 시, 
활성화가 폭발적일 수 있다는 것입니다.

78
00:04:41,290 --> 00:04:45,525
그리고 만약 W값이 아이덴티티보다 조금 작을 경우,

79
00:04:45,525 --> 00:04:49,020
그러면 이 값이 0.9, 0.9 이렇게 될 수 있겠죠,

80
00:04:49,020 --> 00:04:50,062
이런 경우, 아주 깊은 심층망 네트워크로 이루어져 있고,

81
00:04:50,062 --> 00:04:53,235
활성화가 기하급수적으로 감소할 것입니다.

82
00:04:53,235 --> 00:04:56,175
물론 이 논쟁의 의견은 

83
00:04:56,175 --> 00:05:00,795
L의 함수를 기반으로 증가하거나 기하급수적으로 감소하는 활성화에 대한 내용에 중점을 두고 있지만

84
00:05:00,795 --> 00:05:03,180
비슷한 논리를 이용하여

85
00:05:03,180 --> 00:05:05,515
컴퓨터가 전송하는 derivative 또는 기울기 값이

86
00:05:05,515 --> 00:05:08,485
기하급수적으로 증가하는 것을 보여줄 수 있고,

87
00:05:08,485 --> 00:05:11,720
아니면 레이어의 개수에 따라 기하급수적으로 감소하는 것도 같이 보여줄 수 있습니다. 

88
00:05:11,720 --> 00:05:16,210
L값이 150인 최근 버전의 신경망을 이용하여 

89
00:05:16,210 --> 00:05:19,018
마이크로소프트는 152개의 레이어를 가진 신경망으로 좋은 결과를 얻었는데요,

90
00:05:19,018 --> 00:05:22,900
이런 매우 깊은 신경망은

91
00:05:22,900 --> 00:05:27,760
L의 함수대로 활성화나 기울기가 기하급수적으로 증가 또는 감소하는 경우

92
00:05:27,760 --> 00:05:31,435
그 값 또한 매우 커지거나 작아질 수 있습니다. 

93
00:05:31,435 --> 00:05:33,777
이러한 점이 트레이닝을 어렵게 만드는데요.

94
00:05:33,777 --> 00:05:36,970
특히, L과 비교하여 기울기가 급격히 작으면 

95
00:05:36,970 --> 00:05:40,540
기울기 하강이 천천히 이루어질 것입니다.

96
00:05:40,540 --> 00:05:44,580
기울기 하강을 교육시키는 데는 오랜 시간이 소요될 것입니다.

97
00:05:44,580 --> 00:05:47,380
요약하자면, 여러분은 이제까지 깊은 심층신경망에서 

98
00:05:47,380 --> 00:05:50,945
사라지거나 폭발적인 기울기 값을 갖는 경우를 보았는데요, 

99
00:05:50,945 --> 00:05:52,750
사실 이 문제는 오랫동안

100
00:05:52,750 --> 00:05:56,040
심층신경망을 트레이닝 시키는 과정에서 큰 장벽으로 작용해왔는데요.

101
00:05:56,040 --> 00:05:59,620
알고 보니, 부분적으로나마 해결책이 있었습니다.

102
00:05:59,620 --> 00:06:01,670
문제를 완전히 해결하진 못하지만, 큰 도움이 되는 해결책입니다.

103
00:06:01,670 --> 00:06:04,455
이 방법은 비중을 신중하게 초기화하는 방법인데요.

104
00:06:04,455 --> 00:06:07,090
더 자세히 다루기 위해서 다음 비디오로 넘어가도록 하겠습니다.