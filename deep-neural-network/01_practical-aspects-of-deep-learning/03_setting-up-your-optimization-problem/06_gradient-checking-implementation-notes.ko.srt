1
00:00:00,610 --> 00:00:03,220
이전 비디오에서는 gradient checking에 대한 내용을 배웠는데요, 

2
00:00:03,220 --> 00:00:06,317
이번 비디오에서는, 이런 내용을 신경망 도입에서 

3
00:00:06,317 --> 00:00:10,950
쓸 수 있는 방법에 대한 노트를 공유하고 
실용적인 방법에 대해 이야기해보겠습니다.

4
00:00:10,950 --> 00:00:14,590
첫번째로, 트레이닝에서는 grad check를 사용하지 마십시요. 
오로지 debug하실 때만 사용하십시오. 

5
00:00:14,590 --> 00:00:19,470
제가 무슨말이냐면, 
모든 i값에 대해서 

6
00:00:19,470 --> 00:00:22,520
d 쎄타 approx를 계산하는 경우, 
이것은 아주 느린 계산인데요, 

7
00:00:22,520 --> 00:00:26,320
기울기 강하를 도입하는 데는 
후 방향전파을 사용해서 d 쎄타를 계산하고, 

8
00:00:26,320 --> 00:00:29,110
후 방향전파을 이용해서 derivative를 계산합니다.

9
00:00:29,110 --> 00:00:32,140
그리고 디버깅을 하는 경우에만 

10
00:00:32,140 --> 00:00:34,218
이것을 계산할 텐데요, 
d 쎄타와 근접한 값이 될 수 있도록 합니다.

11
00:00:34,218 --> 00:00:37,048
이렇게 한 경우, grad check를 끄고 

12
00:00:37,048 --> 00:00:39,502
기울기 강하를 반복 할 때 마다, 
사용하지 않습니다.

13
00:00:39,502 --> 00:00:41,530
너무 느리기 때문이죠.

14
00:00:41,530 --> 00:00:45,060
둘째로, 알고리즘이 grad check 실패 경우,
요소들을 보십시오. 

15
00:00:45,060 --> 00:00:48,010
개인별 요소들을 보고
버그를 찾으십시오. 

16
00:00:48,010 --> 00:00:52,124
무슨 뜻이냐면, d theta approx가 
d theta와 매우 다르다면,

17
00:00:52,124 --> 00:00:57,079
제는 개인적으로 i의 다른 값들을 보고

18
00:00:57,079 --> 00:01:02,360
d theta approx와 d theta가 매우 다를 때의 값들을 
찾을 것입니다.

19
00:01:02,360 --> 00:01:06,842
예를 들어, 쎄타의 값 또는 d 쎄타의 값이

20
00:01:06,842 --> 00:01:11,495
매우 다른 경우가 어느 층의 dbl에 연계된다면, 

21
00:01:11,495 --> 00:01:16,162
dw의 요소들은 꽤 가깝죠, 맞죠?

22
00:01:16,162 --> 00:01:20,803
기억할 것은, 쎄타의 다른 요소들은, 

23
00:01:20,803 --> 00:01:21,434
b와 w의 다른 요소들과 부합합니다.

24
00:01:21,434 --> 00:01:25,918
이런 경우, 버그가 db를 계산하는 과정에서 

25
00:01:25,918 --> 00:01:30,411
발생할 수 있습니다. db는
파라미터 b에 대한 derivative입니다.

26
00:01:30,411 --> 00:01:35,495
거꾸로도, 비슷하게, 만약에 매우 먼 값을 찾는 경우, 

27
00:01:35,495 --> 00:01:39,610
d theta approx고 부터의 값이 d theta의 값에서 매우 먼 경우 말이죠. 

28
00:01:39,610 --> 00:01:44,452
모든 요소들을 dw나 특정 층의 dw에서 온 것을 발견할 경우

29
00:01:44,452 --> 00:01:48,455
버그의 위치를 파악할 수 있을 수 있습니다.

30
00:01:48,455 --> 00:01:51,562
이런 방법이 버그를 바로 찾게 항상 도와주는 것은 아닙니다.

31
00:01:51,562 --> 00:01:55,622
가끔씩은 버그를 어디서 트래킹할 수 있는지 그 단서를 제시해줍니다.

32
00:01:56,782 --> 00:01:59,502
다음으로, grad check를 하는 경우, 

33
00:01:59,502 --> 00:02:03,372
일반화를 이용한다면,
일반화 항을 기억하십시오. 

34
00:02:03,372 --> 00:02:10,052
만약 비용함수J의 쎄타가 1 나누기 m

35
00:02:10,052 --> 00:02:15,570
곱하기 loss의 합 그리고 더하기 일반화 항이라면, 

36
00:02:15,570 --> 00:02:22,790
그리고 합 나누기 L 그리고 wl 제곱, 그러면 이것이 J의 정의입니다.

37
00:02:22,790 --> 00:02:27,200
d theta는 J의 기울기 고요

38
00:02:27,200 --> 00:02:30,840
쎄타에 대해서요, 그리고 일반화 항도 있습니다.

39
00:02:30,840 --> 00:02:32,880
잊지 말고 여기 항을 추가하십시오. 

40
00:02:32,880 --> 00:02:37,185
다름으로, grad check는 dropout에서 작동하지 않습니다.

41
00:02:37,185 --> 00:02:41,307
반복할 때마다. dropout이 무작위로 숨겨진 유닛의 일부를 제거하기 때문입니다.

42
00:02:41,307 --> 00:02:45,923
dropout이 기울기 강하를 진행하는 

43
00:02:45,923 --> 00:02:48,098
쉽게 산출할 수 있는 비용함수 J는 따로 없습니다.

44
00:02:48,098 --> 00:02:52,932
dropout은 비용함수 J를 최적화 시켜준다고 할 수 있는데요, 

45
00:02:52,932 --> 00:02:57,254
비용함수J는 반복업무를 통해 제거될 수 있는 

46
00:02:57,254 --> 00:03:00,900
기하급수적으로 큰 값의 노드 일부를
더해서 정의됩니다.

47
00:03:00,900 --> 00:03:04,780
그러므로 비용함수 J는 산출하기 매우 까다로운데요. 

48
00:03:04,780 --> 00:03:07,560
비용함수를 샘플링하는 것입니다.

49
00:03:07,560 --> 00:03:11,770
dropout에서 사용하는 무작위의 subset을 제거할 때마다 말이죠.

50
00:03:11,770 --> 00:03:14,730
dropout에서의 계산은 그러므로

51
00:03:14,730 --> 00:03:16,810
grad check를 이용해서 체크하기 어렵습니다.

52
00:03:16,810 --> 00:03:20,360
저는 개인적으로 dropout이 없이 grad check를 도입합니다.

53
00:03:20,360 --> 00:03:25,285
원한다면, keep-prob과 dropout을 1.0으로 지정합니다.

54
00:03:25,285 --> 00:03:29,590
그 이후로 dropout를 키고난 뒤, dropout의 도입이 
옳았기를 바랄 수 있게죠. 

55
00:03:30,770 --> 00:03:35,738
다른 방법이 몇가지 있는데요, 
드랍된 노드들의 패턴을 고쳐서, 

56
00:03:35,738 --> 00:03:39,914
그 패턴의 grad check이 옳은지 여부를 확인하는 방법이 있습니다.

57
00:03:39,914 --> 00:03:43,200
저는 실제로 이런 방법을 사용하지는 않습니다.

58
00:03:43,200 --> 00:03:48,010
제가 권장 드리는 것은, dropout을 끄고, 
dropout없이 알고리즘이 옳은지 grad check을

59
00:03:48,010 --> 00:03:52,560
통해 확인한 뒤, dropout을 가시 켜는 겁니다.

60
00:03:52,560 --> 00:03:55,520
마지막으로 조금 미묘한 부분인데요, 

61
00:03:55,520 --> 00:03:59,853
불가능하진 않지만, 자주 발생하지는 않습니다. 

62
00:03:59,853 --> 00:04:04,322
w와 b의 값이 0과 근접한 경우, 기울기 강하 의 도입이 옳은 경우가 불가능하진 않습니다.

63
00:04:04,322 --> 00:04:06,500
random initialization에서 말이죠.

64
00:04:06,500 --> 00:04:10,223
하지만 기울기 강하를 운영하면서,
w와 b의 값이 커지면서, 

65
00:04:10,223 --> 00:04:15,089
w와 b의 값이 0인 경우에만
backprop 도입이 맞을 수도 있습니다.

66
00:04:15,089 --> 00:04:18,660
그러나 w와 b의 값이 커지면 커질수록
정확도가 떨어지게 되는 것이죠.

67
00:04:18,660 --> 00:04:21,510
그러므로 한가지 할 수 있는 것은 
저는 자주 이렇게 하지는 않습니다만,

68
00:04:21,510 --> 00:04:25,850
grad check를 random initialization에서 실행시키고 

69
00:04:25,850 --> 00:04:27,940
그 다음에 네트워크를 어느 정도 트레이닝 시킨 뒤, 

70
00:04:27,940 --> 00:04:33,198
w와 b의 값이 0에서 조금 떨어지게 시간을 가질 수 있게 말이죠,
처음의 작은 무작위의 값에서 떨어질 수 있게 말이죠,

71
00:04:33,198 --> 00:04:37,620
그 다음 몇번 트레이닝을 반복한 이후에, 
다시 grad check를 실행하는 것입니다.

72
00:04:37,620 --> 00:04:39,165
gradient checking에 대한 내용은 이것이 전부인데요. 

73
00:04:39,165 --> 00:04:42,760
이번주 마지막 부분까지 오신 것에 축하드립니다.

74
00:04:42,760 --> 00:04:47,100
이번주에는 train, dev, 그리고 테스트세트를 세팅하는 방법을 배웠는데요,

75
00:04:47,100 --> 00:04:51,254
편향과 편차를 분석하는 방법과
각각 편향이 큰 경우와 편차가 큰 경우

76
00:04:51,254 --> 00:04:54,230
대응 방안과, 2개 모두 큰 경우의 방안을 배웠습니다.

77
00:04:54,230 --> 00:04:57,930
일반화에 대한 다른 유형도 살펴보았는데요, 

78
00:04:57,930 --> 00:05:02,070
신경망의 L2 일반화 과 dropout에 대해 알아봤습니다. 

79
00:05:02,070 --> 00:05:05,318
즉, 신경망 트레이닝의 속도를 높이는 방법을 알아봤습니다.

80
00:05:05,318 --> 00:05:07,920
마지막으로, gradient checking, 

81
00:05:07,920 --> 00:05:10,480
이번주에는 많이 보셨겠지만, 

82
00:05:10,480 --> 00:05:14,300
이번 주 내용을 프로그래밍 학습을 통해
많이 연습하실 수 있습니다.

83
00:05:14,300 --> 00:05:15,520
행운을 빌겠습니다.

84
00:05:15,520 --> 00:05:17,830
다음주 과정에서 뵙겠습니다.