1
00:00:00,436 --> 00:00:03,390
當訓練神經網路時,
加快您的訓練的技巧其中之一

2
00:00:03,390 --> 00:00:06,060
是正規化您的輸入

3
00:00:06,060 --> 00:00:07,730
讓我們來看看這是什麼意思

4
00:00:07,730 --> 00:00:10,240
假設訓練集有兩個輸入特徵

5
00:00:10,240 --> 00:00:13,520
所以輸入特徵 x 是二維

6
00:00:13,520 --> 00:00:16,550
這個是您訓練集的散佈圖

7
00:00:16,550 --> 00:00:20,730
正規化您的輸入相當於兩個步驟

8
00:00:20,730 --> 00:00:26,270
第一步是減去
或者讓平均值為零

9
00:00:26,270 --> 00:00:34,140
所以您設 mu = 1 除以 m 總和於 i of X(i)

10
00:00:34,140 --> 00:00:39,786
所以這是一個向量, 然後 X 
設為 X - mu 對於每一個訓練例子

11
00:00:39,786 --> 00:00:44,571
這個意思是您移動
這個訓練集到 0 平均值

12
00:00:44,571 --> 00:00:49,530
第二步驟是正規化它們的變異

13
00:00:49,530 --> 00:00:54,640
請注意這裡的 x1 特徵
有比較大的變異

14
00:00:54,640 --> 00:00:55,410
比起X2特徵來說

15
00:00:55,410 --> 00:01:00,030
所以我們要做的是設 sigma = 1 除以 m

16
00:01:00,030 --> 00:01:04,580
總和 X(i)**2

17
00:01:04,580 --> 00:01:07,220
這個是逐元素的平方

18
00:01:07,220 --> 00:01:13,040
現在 sigma 平方是一個向量
是每一個特徵的變異數

19
00:01:13,040 --> 00:01:15,435
請注意我們已經減去平均值

20
00:01:15,435 --> 00:01:19,600
所以 X(i) 平方
逐元素平方只是變異數

21
00:01:19,600 --> 00:01:24,580
然後拿每一個例子
除以這個向量 sigma 平方

22
00:01:24,580 --> 00:01:28,490
在圖形上, 您最終會變成這樣

23
00:01:28,490 --> 00:01:34,785
現在 X1 跟 X2 的變異都會等於 1

24
00:01:35,975 --> 00:01:42,627
一個提示, 如果您用這樣來
按比例增減您的訓練資料, 那使用相同的 mu 跟

25
00:01:42,627 --> 00:01:47,735
sigma 平方來正規化您測試集, 是吧?

26
00:01:47,735 --> 00:01:51,015
特別是, 您不希望
用不同的正規化在訓練集跟

27
00:01:51,015 --> 00:01:52,865
在測試集中

28
00:01:52,865 --> 00:01:57,520
不管這個值是什麼跟
不管這個值是什麼, 用這這兩個

29
00:01:57,520 --> 00:02:02,190
公式, 所以您用一樣的方式來
按比例縮放您的測試集, 而不是

30
00:02:02,190 --> 00:02:06,500
計算 mu 跟 sigma 平方
分別在訓練集跟測試集

31
00:02:06,500 --> 00:02:10,167
因為您要您的資料
不管是訓練或是測試例子

32
00:02:10,167 --> 00:02:13,831
經歷同樣的轉換
用相同的 mu 跟

33
00:02:13,831 --> 00:02:16,752
sigma 平方從訓練集計算出來的

34
00:02:16,752 --> 00:02:18,210
為什麼做這些事 ？

35
00:02:18,210 --> 00:02:21,290
為什麼我們要對輸入特徵做正規化

36
00:02:21,290 --> 00:02:25,790
記得成本函數是
像在右上角定義的

37
00:02:25,790 --> 00:02:31,030
實際上如果您用非正規化
輸入特徵, 它會

38
00:02:31,030 --> 00:02:35,860
您個成本函數會像這樣
像是被壓扁的碗, 

39
00:02:35,860 --> 00:02:41,500
拉得很長的成本函數
而您試著要找的做小值會在這裡

40
00:02:41,500 --> 00:02:46,890
但如果您的特徵是很不同的比例
假設 x1 特徵

41
00:02:46,890 --> 00:02:52,280
從1 到 1000, 
而 x2特徵從 0 到 1

42
00:02:52,280 --> 00:02:56,790
那會變成對於

43
00:02:56,790 --> 00:03:02,020
參數 w1 跟 w2 的比例或是值的範圍
會非常不同

44
00:03:02,020 --> 00:03:06,771
也許這兩個軸應該是 w1 跟 w2
但我畫成 w 跟 b

45
00:03:06,771 --> 00:03:11,270
那您的成本函數會是
細長的碗狀像那樣

46
00:03:11,270 --> 00:03:14,440
如果您畫這個函數的等高線圖

47
00:03:14,440 --> 00:03:17,705
您會得到一個
細長的函數像那樣

48
00:03:17,705 --> 00:03:19,500
但如果您正規化這些特徵

49
00:03:19,500 --> 00:03:24,570
那您的成本函數
平均來說或比較對稱

50
00:03:24,570 --> 00:03:28,728
如果您跑梯度下降
在左邊的成本函數

51
00:03:28,728 --> 00:03:33,216
那您會使用很小的學習率
因為如果您

52
00:03:33,216 --> 00:03:37,242
在這裡，梯度下降會
需要很多步來回擺動

53
00:03:37,242 --> 00:03:40,800
在最後終於找到到最小值的路徑

54
00:03:40,800 --> 00:03:45,466
而如果您的是球型的等高線圖
不管您從何開始

55
00:03:45,466 --> 00:03:49,125
梯度下降都可以
直接到最小值

56
00:03:49,125 --> 00:03:53,665
您可以用比較大的步伐在梯度下降
而不需要

57
00:03:53,665 --> 00:03:56,345
來回擺動像在左邊一樣

58
00:03:56,345 --> 00:04:00,250
當然, 實務上 w 是高維度的向量

59
00:04:00,250 --> 00:04:04,530
試著畫成 2D 並不能正確表示所有直觀

60
00:04:04,530 --> 00:04:08,220
但大約的直觀是
您的成本函數會比較圓

61
00:04:08,220 --> 00:04:12,510
比較容易最佳化當您的特徵
都在類似的比例時

62
00:04:12,510 --> 00:04:15,600
不是從 1 到 1000, 從 0 到 1, 但

63
00:04:15,600 --> 00:04:20,880
大部分從 -1 到 1 
或者彼此類似的變異

64
00:04:20,880 --> 00:04:25,630
這樣會讓您的成本函數 J
比較容易跟快速來最佳化

65
00:04:25,630 --> 00:04:30,450
實際上如果一個特徵假設 X1
範圍從0 到 1

66
00:04:30,450 --> 00:04:35,530
X2範圍 從 -1 到 1
X3 範圍從 1 到 2

67
00:04:35,530 --> 00:04:38,730
這些都是類似的範圍
這個會作用得很好

68
00:04:38,730 --> 00:04:41,430
就是對於那些
很大不同的範圍像是

69
00:04:41,430 --> 00:04:42,470
一個從 1 到 1000, 

70
00:04:42,470 --> 00:04:46,720
另一個從 0 到 1, 這個
真的會傷害您的最佳化演算法

71
00:04:46,720 --> 00:04:50,664
但只要設定它們都是
平均值 0 跟變異數 1, 像是我們

72
00:04:50,664 --> 00:04:54,857
上個投影片做的, 保證所有的特徵在類似的比例上

73
00:04:54,857 --> 00:04:58,290
通常會幫助您的
學習演算法跑得更快速

74
00:04:58,290 --> 00:05:01,600
如果您個輸入特徵
有不同的比例

75
00:05:01,600 --> 00:05:03,410
也許一些特徵是從 0 到 1

76
00:05:03,410 --> 00:05:08,130
有些從 1 到 1000, 
那正規化您的特徵是很重要的

77
00:05:08,130 --> 00:05:11,630
如果您的特徵來自於類似的比例
那這個步驟比較不重要

78
00:05:11,630 --> 00:05:15,235
雖然執行這個
正規化的步驟幾乎不會有任何

79
00:05:15,235 --> 00:05:19,170
傷害, 所以我總是會執行它
如果我不確定是否

80
00:05:19,170 --> 00:05:21,970
會幫助我加快
訓練我的演算法

81
00:05:22,970 --> 00:05:26,020
所以這是正規化您的輸入特徵

82
00:05:26,020 --> 00:05:29,840
下一段，讓我們繼續談論一些方式
來加快訓鍊您的神經網路