Sinir ağlarını eğitirken kullanılabilecek, eğitme hızınızı arttırabilecek yöntemlerden birisi ise girdilerinizi normalleştirmektir. Hadi ne anlama geldiğini konuşalım Diyelim ki eğitim setiniz iki girdi özelliğine sahip olsun. Girdi olan x 2 boyutlu olsun(x1, x2) burada eğitim setinizin saçılım çizimini görüyoruz Girdilerinizi normalleştirmek iki adımdan oluşur. Birincisi ortalamasını çıkartmak veya ortalamayı sıfıra çekmek yani burada mü = (1/ m) çarpı x lerin toplamı(ortalamasını almak) Daha sonra x, x- mü ye eşitleniyor yani x leri, ortalamaları sıfır olacak şekilde yer değiştiriyoruz. ve ikinci adım ise değişinti leri normalleştirmek burada dikkat ederseniz x1 , x2 den çok daha fazla alana yayılmış Yani yaptığımız şey burada sigmayı (1/m) * x'lerin karelerinin toplamına eştliyoruz Burada her bir elemanın karesini alıyoruz Dolayısıyla kare-sigma her özelliğin değişintisini içermiş oluyor ve daha sonra x'leri ( x1 veya x2) kare-sigmaya bölüyoruz bu şekilde x leri değişintilerine bölerek her birinin değişintisini 1 e eşitlemiş oluyoruz bu da şekilde görülen çizime yol açıyor Eğer bir öneri verecek olursak, eğer bunu eğitim setinizi normalleştirmek için kullanacaksanız, aynı mü ve sigma yı test seti için de kullanın, değil mi? Özellikle, eğitim setini ve test setini farklı bir şekilde normalleştirmek istemezsiniz. Her iki değer ne olursa olsun bu değerleri bu iki formülde yani eğitim ve test setinde aynı şekilde kullanın bu şekilde iki setinizi aynı şekilde normalleştirmiş olursunuz Çünkü hem eğitim hem de test örneklerinde verinizin eğitim setinde hesaplanan aynı mü ve aynı sigma tarafından tanımlanan aynı değişimden geçmesini istersiniz. Peki neden bunu yapıyoruz? Neden girdi değerlerini normalleştirmek istiyoruz? Sağ üstte yazan Bedel fonksiyonunu hatırlayın. Görünen o ki eğer normalleştirilmemiş girdiler kullanırsanız, bedel fonksiyonunuzun bu şekilde gözükme ihtimali yüksek, ezilmiş ve uzatılmış şekilde, belki de bulmak istediğiniz minimum değer burada bir yerdedir. Fakat eğer özellikleriniz çok farklı ölçeklendirmeye sahipse, diyelim ki, x1 1'den 1000 e kadar olsun ve x2 0'dan 1'e kadar olsun bu durumda w1 ve w2 parametreleri için oran ya da değer aralıkları çok farklı sonuçlar alacaktır. (Belki de bu eksenler w1 ve w2 adını almalı fakat ben w ve b şeklinde çizeceğim.) Dolayısıyla bedel fonksiyonunuz bu şekilde uzatılmış kase gibi olacaktır. Eğer bu fonksiyonu incelerseniz, bu şekilde uzatılmış bir görüntü elde edersiniz. Fakat bunun yerine özellikleri normalleştirirseniz, o zaman bedel fonksiyonunuz daha simetrik görünecektir. ve eğer bedel fonksiyonunda solda olduğu gibi eğim azalması çalıştırıyorsanız, bu durumda küçük bir öğrenme hızı kullanmak isteyebilirsiniz çünkü örneğin burada adımlarınız minimumu bulmadan önce fazla miktarda salınım yapabilir. Bunun yerine eğer daha küresel sınırlarınız varsa, bu durumda eğim azalmasını nerede başlatırsanız başlatın bu direk minimuma doğru gidecektir. Bu durumda soldaki resimde görülen salınımlar daha az olacağı için çok daha büyük adımlar(öğrenme hızı)atabilirsiniz. Tabi ki paratikte w yüksek boyutlu bir vektör, dolayısıyla bunu 2 boyutta çizmeye çalışmak tam olarak anlamını yansıtamıyor. Fakat temel iç görü bedel fonksiyonunuzun, eğer özellikler benzer ölçeklerde olursa daha yuvarlak ve eniyilemesi daha kolay olduğu olacaktır. 1'den 1000'e veya 0'dan bire değil çoğunlukla -1'den 1'e veya çoğunlukla benzer değişintilere sahip olacaktır. Bu bedel fonksiyonu j'nin kolay ve hızlı bir şekilde eniyilenmesi anlamına gelir Paratikte, eğer bir özellik, diyelim ki x1, 0'dan 1'e, x2 -1'den 1'e ve x3 1'den ikiye kadarsa, bunlar genel olarak benzer uzaklıklara sahip olduğu için bunlar iyi bir şeklide çalışacaktır. Fakat uzaklıklar arasında çok fazla fark olduğu zaman, örneğin 1'den 1000'e kadarsa ve diğer özellik 0'dan 1'e kadarsa bu, eniyileme algoritmanızı gerçek manada baltalar. Fakat bunların hepsinin ortalamasını 0'a eşitleyerek ve değişintisini 1'e eşitleyerek, -son slaytta yaptığımız gibi-, özelliklerin benzer ölçeklere sahip olmasını garanti edebiliriz ve bu öğrenme algoritmanızın daha hızlı çalışmasına çoğunlukla fayda sağlar Dolayısıyla eğer girdi fonksiyonlarınız çok farklı ölçeklerdeyse, belki bazı özellikler 0'dan 1'e kadar, bazıları 1'den 1000'e kadarsa, bu durumda özelliklerinizi normalleştirmek önemli olacaktır. Eğer özellikleriniz benzer ölçeklerdeyse, o zaman bu işlem daha az önemli olacaktır. Böyle bir normalleştirme hiçbir zarar getirmeyeceği için, eğer işlemlerinizde eğitme hızını artırmak için fayda sağlayıp sağlamayacağından emin değilsem ben genellikle uyguluyorum. Pekala, girdi özelliklerinizi normalleştirmek buraya kadardı, Bir sonraki derste yeni sinir ağınızın eğitimenizi hızlandırma konusunu konuşmaya devam edeceğiz.