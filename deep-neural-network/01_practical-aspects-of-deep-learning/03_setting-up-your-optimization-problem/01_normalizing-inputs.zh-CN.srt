1
00:00:00,436 --> 00:00:03,390
训练神经网络时 有一种加速训练过程的方法

2
00:00:03,390 --> 00:00:06,060
那就是对输入进行归一化

3
00:00:06,060 --> 00:00:07,730
我们看看这是什么意思

4
00:00:07,730 --> 00:00:10,240
考虑一个有两个输入特征的训练集

5
00:00:10,240 --> 00:00:13,520
即是输入特征x是二维的

6
00:00:13,520 --> 00:00:16,550
然后这里就是输入的散点图

7
00:00:16,550 --> 00:00:20,730
对输入进行归一化包含两个步骤

8
00:00:20,730 --> 00:00:26,270
第一步是将减去均值 或说将均值归零

9
00:00:26,270 --> 00:00:34,140
所以令μ为所有Xi的和 其中i是由1到M

10
00:00:34,140 --> 00:00:39,786
得到μ向量后 再将训练集中每个样本x都赋值为x-μ

11
00:00:39,786 --> 00:00:44,571
意思就是将训练集进行平移 直到它的均值变为零

12
00:00:44,571 --> 00:00:49,530
接下来第二部就是将方差进行归一化

13
00:00:49,530 --> 00:00:54,640
注意到这里 特征x1的方差比x2的方差大很多

14
00:00:54,640 --> 00:00:55,410
注意到这里 特征x1的方差比x2的方差大很多

15
00:00:55,410 --> 00:01:00,030
所以我们要做的就是 令σ^2等于

16
00:01:00,030 --> 00:01:04,580
Xi**2之和除以m 其中i是从1到m

17
00:01:04,580 --> 00:01:07,220
我想这个符号表示逐元素取平方

18
00:01:07,220 --> 00:01:13,040
那么σ平方就是表示每个特征的方差的向量

19
00:01:13,040 --> 00:01:15,435
注意我们已经减去了均值

20
00:01:15,435 --> 00:01:19,600
所以Xi平方 即逐元素平方 就等于方差

21
00:01:19,600 --> 00:01:24,580
然后把每个样本都除以σ的平方

22
00:01:24,580 --> 00:01:28,490
把结果画在图上就是这样

23
00:01:28,490 --> 00:01:34,785
现在X1和X2这两个特征的方差都等于1

24
00:01:35,975 --> 00:01:42,627
一个提示:如果你用这种方法来对数据进行缩放

25
00:01:42,627 --> 00:01:47,735
那么务必对测试集和训练集都使用同样的μ和σ^2

26
00:01:47,735 --> 00:01:51,015
具体来说 就是不应该用不同的方式去归一化

27
00:01:51,015 --> 00:01:52,865
训练集与测试集

28
00:01:52,865 --> 00:01:57,520
无论这里μ与σ^2各为何值
都把它们以一样的方式代入这两个式子

29
00:01:57,520 --> 00:02:02,190
以将训练集与测试集用一样的方式归一化

30
00:02:02,190 --> 00:02:06,500
而不是对训练集和测试集分别使用不同的μ与σ^2

31
00:02:06,500 --> 00:02:10,167
因为你的所有数据 包括训练和测试样本

32
00:02:10,167 --> 00:02:13,831
都得经过同样的变换 即由同样的μ与σ^2定义的

33
00:02:13,831 --> 00:02:16,752
也就是由训练集数据计算出来的μ与σ^2

34
00:02:16,752 --> 00:02:18,210
那为什么要做这两步呢?

35
00:02:18,210 --> 00:02:21,290
为什么要对输入特征进行归一化呢?

36
00:02:21,290 --> 00:02:25,790
回想一下 像右上方这样的代价函数

37
00:02:25,790 --> 00:02:31,030
你会发现如果使用了未归一化的输入特征

38
00:02:31,030 --> 00:02:35,860
代价函数看起来就会像这样 像一个压扁了的碗

39
00:02:35,860 --> 00:02:41,500
在这个细长的代价函数中 你寻找的最小值可能在那里

40
00:02:41,500 --> 00:02:46,890
但若这是因为你的特征的尺度不一样 例如说

41
00:02:46,890 --> 00:02:52,280
x1的范围是1到1000 而x2只有0到1

42
00:02:52,280 --> 00:02:56,790
那就会导致 神经网络参数w1和w2的比率

43
00:02:56,790 --> 00:03:02,020
或说w1和w2取值的范围会有很大的不同

44
00:03:02,020 --> 00:03:06,771
这两根轴应当是w1和w2 但暂且用w和b代替

45
00:03:06,771 --> 00:03:11,270
那代价函数就可能像那样是一个扁平的碗

46
00:03:11,270 --> 00:03:14,440
如果你把这个函数的等值线画出来

47
00:03:14,440 --> 00:03:17,705
你就会有一个像这样的扁长的函数

48
00:03:17,705 --> 00:03:19,500
而如果你将特征进行归一化后

49
00:03:19,500 --> 00:03:24,570
你的代价函数通常就会看起来更对称

50
00:03:24,570 --> 00:03:28,728
如果你对左图的那种代价函数使用梯度下降法

51
00:03:28,728 --> 00:03:33,216
那可能必须使用非常小的学习率 因为假如从这里开始

52
00:03:33,216 --> 00:03:37,242
梯度下降法需要经历许多步 反复辗转

53
00:03:37,242 --> 00:03:40,800
才能好不容易终于挪到这个最小值

54
00:03:40,800 --> 00:03:45,466
而如果等值线更趋近于圆形 那无论从何开始

55
00:03:45,466 --> 00:03:49,125
梯度下降法几乎都能直接朝向最小值而去

56
00:03:49,125 --> 00:03:53,665
你可以在梯度下降中采用更长的步长

57
00:03:53,665 --> 00:03:56,345
而无需像左图那样来回摇摆缓慢挪动

58
00:03:56,345 --> 00:04:00,250
当然在实践中 w是一个高维向量

59
00:04:00,250 --> 00:04:04,530
把它画在二维空间中可能无法正确传递出高维中的感觉

60
00:04:04,530 --> 00:04:08,220
但大体上的感觉是你的代价函数会更圆

61
00:04:08,220 --> 00:04:12,510
优化过程更容易进行 因为各种特征的尺度会比较接近

62
00:04:12,510 --> 00:04:15,600
不是有的从1到1000 有的却从0到1

63
00:04:15,600 --> 00:04:20,880
而是大多数都处于-1和1之间 大家的方差范围都接近

64
00:04:20,880 --> 00:04:25,630
这些会使得代价函数J优化起来更容易也更快

65
00:04:25,630 --> 00:04:30,450
其实在实践中 假使有特征x1的范围是0到1

66
00:04:30,450 --> 00:04:35,530
而特征x2的范围是-1到1 特征x3是1到2

67
00:04:35,530 --> 00:04:38,730
这些范围都比较接近 所以优化起来也没什么问题

68
00:04:38,730 --> 00:04:41,430
只有当它们的范围有非常大的不同时

69
00:04:41,430 --> 00:04:42,470
譬如一个特征是1到1000

70
00:04:42,470 --> 00:04:46,720
而另一个是0到1 那就会着实地削弱优化算法的表现了

71
00:04:46,720 --> 00:04:50,664
但只要将它们变换 使均值皆为0 方差皆为1

72
00:04:50,664 --> 00:04:54,857
就如前一页那样 就能保证所有的特征尺度都相近

73
00:04:54,857 --> 00:04:58,290
这通常也能让帮助学习算法运行得更快

74
00:04:58,290 --> 00:05:01,600
所以如果你的输入特征的尺度非常不同

75
00:05:01,600 --> 00:05:03,410
比如可能有些特征取值范围是0到1

76
00:05:03,410 --> 00:05:08,130
有些是1到1000
那对输入进行归一化就很重要

77
00:05:08,130 --> 00:05:11,630
而如果输入特征本来尺度就相近
那么这一步就不那么重要

78
00:05:11,630 --> 00:05:15,235
不过因为归一化的步骤几乎从来没有任何害处

79
00:05:15,235 --> 00:05:19,170
所以我无论如何总是进行归一化
尽管我并不确定

80
00:05:19,170 --> 00:05:21,970
它是否会让训练时的计算变得更快

81
00:05:22,970 --> 00:05:26,020
所以对输入特征进行归一化 就讨论到这

82
00:05:26,020 --> 00:05:29,840
接下来我们会继续讨论加速神经网络训练的方法
GTC字幕组翻译