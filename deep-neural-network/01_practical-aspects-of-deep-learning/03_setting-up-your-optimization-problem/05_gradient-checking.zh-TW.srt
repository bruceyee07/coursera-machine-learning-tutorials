1
00:00:00,400 --> 00:00:04,000
梯度檢查是一種
幫助我節省了大量時間的技術

2
00:00:04,000 --> 00:00:08,500
多次幫助我發現在我的反向傳播
建置時的錯誤

3
00:00:08,500 --> 00:00:10,890
讓我們來看
您也可以使用它來除錯

4
00:00:10,890 --> 00:00:14,885
或者驗證您的反向傳播建置是正確的

5
00:00:14,885 --> 00:00:20,975
您的神經網路有一些參數
像是 W[1], b[1], 等等到  W[L], b[L]

6
00:00:20,975 --> 00:00:23,935
為了要建立梯度檢查, 
第一件事是您應該要做的是拿所有您的

7
00:00:23,935 --> 00:00:28,835
參數然後
重新建立一個大的向量 θ

8
00:00:28,835 --> 00:00:34,860
您應該做的是拿 W 是一個矩陣
把它重塑成一個向量

9
00:00:34,860 --> 00:00:39,850
您必須將所有這些 W 然後
重塑他們成為一個向量, 然後結合

10
00:00:39,850 --> 00:00:45,170
所有這些東西,
變成一個大的向量 θ

11
00:00:45,170 --> 00:00:47,020
大的向量發音為 theta

12
00:00:47,020 --> 00:00:52,720
我們說成本函數 J 是 W 跟 b 的函數

13
00:00:52,720 --> 00:00:58,380
您現在的成本函數 J 會變成 θ 的函數

14
00:00:58,380 --> 00:01:02,160
接下來, 跟 W 跟 b 用相同方式排列

15
00:01:02,160 --> 00:01:07,740
您也一樣將 dw[1], db[1], 等等
重塑它們變成一個

16
00:01:07,740 --> 00:01:12,200
巨大向量 dθ 跟 θ 同維度

17
00:01:12,200 --> 00:01:17,210
跟之前一樣, 我們重塑 dw[1] 這個矩陣, db[1] 已經是向量

18
00:01:17,210 --> 00:01:21,220
重塑 dw[L], 所有 dw 的矩陣

19
00:01:21,220 --> 00:01:24,632
記得 dw[1] 跟 w[1] 同維度

20
00:01:24,632 --> 00:01:27,080
db[1] 跟 b[1] 同維度

21
00:01:27,080 --> 00:01:31,252
用同樣的重塑及結合的運算

22
00:01:31,252 --> 00:01:36,343
您可以重塑所有這些導數
成為一個大的向量 dθ

23
00:01:36,343 --> 00:01:38,750
跟 θ 同維度

24
00:01:38,750 --> 00:01:43,780
現在問題是
這個 dθ 是

25
00:01:43,780 --> 00:01:47,310
成本函數 J 的梯度或者說是斜率嗎 ?

26
00:01:47,310 --> 00:01:49,620
這個就是您如何來建立
梯度檢查

27
00:01:49,620 --> 00:01:52,740
通常梯度檢查縮寫為 grad check

28
00:01:52,740 --> 00:01:57,690
首先我們記住 J 現在是
一個大的參數 θ 的函數

29
00:01:57,690 --> 00:01:58,277
對吧?

30
00:01:58,277 --> 00:02:04,750
所以展開 J 是 θ1, θ2, θ3 等等的函數

31
00:02:06,880 --> 00:02:11,618
無論這個大的參數向量 θ 的維度多少

32
00:02:11,618 --> 00:02:18,519
要建立梯度檢查，
您要做的是建立一個迴圈

33
00:02:18,519 --> 00:02:23,008
對於每一個 i,
對於每一個 θ

34
00:02:23,008 --> 00:02:26,416
計算 dθ approx[i] 為 

35
00:02:26,416 --> 00:02:28,170
讓我們用雙邊差異

36
00:02:28,170 --> 00:02:30,100
所以我拿 J of θ

37
00:02:30,100 --> 00:02:34,440
θ1, θ2, 直到 θi

38
00:02:34,440 --> 00:02:38,380
而我們將微調 θi 加上 ε

39
00:02:38,380 --> 00:02:42,970
只要將 θi 增加 ε,
其他保持一樣

40
00:02:42,970 --> 00:02:46,164
因為我們取雙邊差異

41
00:02:46,164 --> 00:02:51,226
我們同樣取 θi 的另一邊
減去 ε

42
00:02:51,226 --> 00:02:54,520
所以其他元素都不去動它們

43
00:02:54,520 --> 00:02:59,690
然後我們拿這個來除以 2ε (口誤)

44
00:02:59,690 --> 00:03:04,772
我們從前面的投影片看到

45
00:03:04,772 --> 00:03:10,270
這個應該會趨近等於 dθ[i]

46
00:03:10,270 --> 00:03:15,609
也就是 J 的偏導數
相對於

47
00:03:15,609 --> 00:03:21,320
我想是 θi, 如果 dθi 是成本函數 J 的導數

48
00:03:21,320 --> 00:03:25,130
您要做的是計算每一個 i

49
00:03:25,130 --> 00:03:28,360
最後
您會有兩個向量

50
00:03:28,360 --> 00:03:31,793
您最終有這個 dθ approx 跟

51
00:03:31,793 --> 00:03:35,860
這會是同維度的 dθ

52
00:03:35,860 --> 00:03:39,373
而這兩個的維度都跟 θ 一樣

53
00:03:39,373 --> 00:03:43,183
而您要檢查的是
是否這兩個向量幾乎相等

54
00:03:43,183 --> 00:03:44,130
是相同的

55
00:03:44,130 --> 00:03:47,310
詳細的說
您如何定義

56
00:03:47,310 --> 00:03:50,910
兩個向量是否真的
互相彼此接近

57
00:03:50,910 --> 00:03:52,593
我會這樣做

58
00:03:52,593 --> 00:03:57,297
我會計算這兩個向量的
歐幾里德距離

59
00:03:57,297 --> 00:04:02,100
dθ approx 減去 dθ
所以就只是 L2 範數

60
00:04:02,100 --> 00:04:03,851
注意這裡上面沒有平方

61
00:04:03,851 --> 00:04:06,788
這是總和於逐元素的差異的平方

62
00:04:06,788 --> 00:04:09,857
然後取平方根
就像是歐幾里德距離

63
00:04:09,857 --> 00:04:15,512
然後用這些向量的長度
來做正規化

64
00:04:15,512 --> 00:04:19,150
除以 dθ approx 加 dθ

65
00:04:19,150 --> 00:04:22,620
取這兩個向量的
歐幾里德長度

66
00:04:22,620 --> 00:04:28,044
分母這一行就只是萬一任何一個向量很小

67
00:04:28,044 --> 00:04:32,860
或者很大, 您的分母會
將這個公式變成比率

68
00:04:32,860 --> 00:04:35,202
所以當實作時

69
00:04:35,202 --> 00:04:39,898
我用 ε 等於也許是
10 的負7 次方, 所以 -7

70
00:04:39,898 --> 00:04:44,644
使用這個範圍的 ε 
如果您發現這個公式給您

71
00:04:44,644 --> 00:04:49,460
的值類似於 10 的負 7 次方或更小
那很棒

72
00:04:49,460 --> 00:04:53,302
這表示您的導數
近似值可能是對的

73
00:04:53,302 --> 00:04:55,330
這真的是很小的數字

74
00:04:55,330 --> 00:05:00,790
如果也許在 10 的 -5 次方範圍
我會小心的看看

75
00:05:00,790 --> 00:05:02,148
也許沒事

76
00:05:02,148 --> 00:05:05,239
但我會再次檢查
這個向量的分量

77
00:05:05,239 --> 00:05:07,862
確定沒有一個分量是太大的

78
00:05:07,862 --> 00:05:10,649
而如果一些分量的差異很大

79
00:05:10,649 --> 00:05:12,860
或許某個地方您有一個臭蟲 (錯誤)

80
00:05:12,860 --> 00:05:17,719
而如果左邊這個公式是 -3 次方等級
那我會擔心

81
00:05:17,719 --> 00:05:21,728
會更擔心
也許某個地方有錯誤

82
00:05:21,728 --> 00:05:25,083
但您應該得到的值會
遠小於 10 的 -3 次方

83
00:05:25,083 --> 00:05:29,690
如果比 10 的 -3 次方大
那我會很擔心

84
00:05:29,690 --> 00:05:32,970
我會嚴重的擔心
是否真的有錯誤

85
00:05:32,970 --> 00:05:37,204
我會
您應該看每一個

86
00:05:37,204 --> 00:05:41,799
分量的資料是否有特定的 i 

87
00:05:41,799 --> 00:05:45,960
使得 dθ approx i 非常不同於  dθi

88
00:05:45,960 --> 00:05:47,867
使用它試著來追蹤是否

89
00:05:47,867 --> 00:05:51,040
您的一些導數計算是錯誤的

90
00:05:51,040 --> 00:05:54,970
而經過一定的除錯
最後它終於

91
00:05:54,970 --> 00:05:59,820
到這個相當小的值
那您或許有一個正確的建置

92
00:05:59,820 --> 00:06:01,320
所以當建立一個神經網路時

93
00:06:01,320 --> 00:06:04,840
通常我會建立
正向傳播, 建立反向傳播

94
00:06:04,840 --> 00:06:08,612
然後我或許會發現這個
梯度檢查相對大的價值

95
00:06:08,612 --> 00:06:12,460
而當我懷疑有錯誤時
就一直除錯, 除錯, 除錯

96
00:06:12,460 --> 00:06:16,310
經過一段除錯的時間後
我發現以小的值通過梯度檢查時

97
00:06:16,310 --> 00:06:20,110
那我可以比較有信心,
它是正確的

98
00:06:20,110 --> 00:06:22,310
所以您現在知道梯度檢查如何作用

99
00:06:22,310 --> 00:06:24,850
這曾經幫助我找到很多的錯誤
在我建立神經網路時

100
00:06:24,850 --> 00:06:27,330
我希望它也會幫到您

101
00:06:27,330 --> 00:06:29,970
在下一段影片
我想分享您一些技巧

102
00:06:29,970 --> 00:06:33,490
或一些筆記如何
建立梯度檢查

103
00:06:33,490 --> 00:06:34,640
讓我們進入下一段影片