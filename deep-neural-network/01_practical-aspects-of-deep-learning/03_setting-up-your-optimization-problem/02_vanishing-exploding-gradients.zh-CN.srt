1
00:00:00,000 --> 00:00:02,352
当训练神经网络时我们会遇到一个问题

2
00:00:02,352 --> 00:00:04,585
尤其是当训练层数非常多的神经网络时

3
00:00:04,585 --> 00:00:07,395
这个问题就是梯度的消失和爆炸

4
00:00:07,395 --> 00:00:09,180
它的意思是当你在训练一个深度神经网络的时候

5
00:00:09,180 --> 00:00:13,650
损失函数的导数或者说斜率

6
00:00:13,650 --> 00:00:15,825
有时会变得非常大

7
00:00:15,825 --> 00:00:17,420
或者非常小甚至是呈指数级减小

8
00:00:17,420 --> 00:00:19,450
这使训练变得很困难

9
00:00:19,450 --> 00:00:21,690
在这个视频中你将看到

10
00:00:21,690 --> 00:00:25,185
梯度的爆炸和消失是什么含义

11
00:00:25,185 --> 00:00:28,630
以及如何谨慎的选择随机初始化的权重

12
00:00:28,630 --> 00:00:32,780
来显著的减少这种问题的发生

13
00:00:32,780 --> 00:00:36,015
假设你在训练一个层数很多的神经网络

14
00:00:36,015 --> 00:00:37,210
为了在幻灯片上节省空间

15
00:00:37,210 --> 00:00:40,508
我在每层上只画了两个隐藏神经元

16
00:00:40,508 --> 00:00:42,575
但是实际情况也可能会更多

17
00:00:42,575 --> 00:00:45,625
这个神经网络将会有参数w[1] w[2] w[3]等等

18
00:00:45,625 --> 00:00:51,585
直到w[L]

19
00:00:51,585 --> 00:00:53,025
为了简单起见

20
00:00:53,025 --> 00:00:56,960
假设我们使用的激活函数是g(z)=z

21
00:00:56,960 --> 00:00:58,725
一个线性的激活函数

22
00:00:58,725 --> 00:01:02,985
我们忽略b 假设b[L]=0

23
00:01:02,985 --> 00:01:07,755
那么在这种设定下

24
00:01:07,755 --> 00:01:13,700
输出Y就等于w[L]*w[L-1]*w[L-2]...*w[3]*w[2]*w[1]*x

25
00:01:13,700 --> 00:01:18,193
输出Y就等于w[L]*w[L-1]*w[L-2]...*w[3]*w[2]*w[1]*x

26
00:01:18,193 --> 00:01:21,445
输出Y就等于w[L]*w[L-1]*w[L-2]...*w[3]*w[2]*w[1]*x

27
00:01:21,445 --> 00:01:23,830
但是如果要进一步计算一下

28
00:01:23,830 --> 00:01:27,915
w[1]*x=z[1]

29
00:01:27,915 --> 00:01:30,225
因为b=0

30
00:01:30,225 --> 00:01:33,540
所以z[1]=w[1]*x+b

31
00:01:33,540 --> 00:01:37,960
但是因为b=0 所以z[1]=w[1]*x

32
00:01:37,960 --> 00:01:42,440
但是因为a[1]=g(z[1])

33
00:01:42,440 --> 00:01:45,150
但是因为我们用的是线性激活函数

34
00:01:45,150 --> 00:01:47,755
所以a[1]=z[1]

35
00:01:47,755 --> 00:01:50,360
所以这一项w[1]*x就等于a[1]

36
00:01:50,360 --> 00:01:57,950
同理可得w[2]*w[1]*x=a[2]

37
00:01:57,950 --> 00:02:00,118
因为a[2]=g(z[2])=g(w[2]*a[1])

38
00:02:00,118 --> 00:02:03,565
因为a[2]=g(z[2])=g(w[2]*a[1])

39
00:02:03,565 --> 00:02:12,570
然后你可以吧z[1]=w[1]*x代入a[1]

40
00:02:12,570 --> 00:02:16,690
所以我们得到w[2]*w[1]*x=a[2]

41
00:02:16,690 --> 00:02:21,505
w[3]*w[2]*w[1]*x就等于a[3]

42
00:02:21,505 --> 00:02:29,065
以此类推 直到所有矩阵相乘得到y-hat注意不是y

43
00:02:29,065 --> 00:02:33,080
现在我们假设每一个权重矩阵w[L]

44
00:02:33,080 --> 00:02:39,677
都只是比单位矩阵稍微大一些

45
00:02:39,677 --> 00:02:43,825
假设是[1.5 0 0 1.5]

46
00:02:43,825 --> 00:02:46,000
理论上来讲 最后一项的维数与前面不一样

47
00:02:46,000 --> 00:02:49,220
所以我们只假设是x前面这些矩阵

48
00:02:49,220 --> 00:02:51,508
那么忽略掉最后维数不同的这项x y-hat就会等于

49
00:02:51,508 --> 00:02:54,903
那么忽略掉最后维数不同的这项x×y-hat就会等于

50
00:02:54,903 --> 00:03:01,770
[1.5 0 0 1.5]^(L-1) * x

51
00:03:01,770 --> 00:03:08,050
因为我们假设每个权重矩阵都等于[1.5 0 0 1.5]

52
00:03:08,050 --> 00:03:12,945
其实是1.5倍的单位矩阵 最终就会得到这个

53
00:03:12,945 --> 00:03:19,150
所以y-hat其实就等于1.5^(L-1)*x

54
00:03:19,150 --> 00:03:21,715
所以y-hat其实就等于1.5^(L-1)*x

55
00:03:21,715 --> 00:03:24,505
如果对于很深的神经网络L会变得很大

56
00:03:24,505 --> 00:03:26,640
Y-hat 会变得非常大

57
00:03:26,640 --> 00:03:28,375
事实上 它会呈指数速度增大

58
00:03:28,375 --> 00:03:32,145
它以1.5的层数次方的速度增大

59
00:03:32,145 --> 00:03:34,290
所以如果你的神经网络层数很多

60
00:03:34,290 --> 00:03:36,850
y的值就会爆炸

61
00:03:36,850 --> 00:03:40,832
相反的，如果我们用0.5 来替换

62
00:03:40,832 --> 00:03:42,268
0.5 小于1

63
00:03:42,268 --> 00:03:45,860
这就变成了0.5的L次

64
00:03:45,860 --> 00:03:51,515
这个矩阵变成0.5^L 减去1乘X 我们忽略WL

65
00:03:51,515 --> 00:03:57,220
所以每个矩阵都小于1

66
00:03:57,220 --> 00:04:00,415
我们假设X1 和X2 是1 1 

67
00:04:00,415 --> 00:04:02,778
激励变成1/2

68
00:04:02,778 --> 00:04:04,450
1/2 1/4

69
00:04:04,450 --> 00:04:07,227
1/4 1/8 1/8

70
00:04:07,227 --> 00:04:11,470
会变成1/2的L次

71
00:04:11,470 --> 00:04:16,710
激活函数的值会指数级下降

72
00:04:16,710 --> 00:04:19,945
作为网络层数L的函数

73
00:04:19,945 --> 00:04:26,150
所以在很深的网络中，激活函数就会指数级的减少。

74
00:04:26,150 --> 00:04:30,940
所以我希望你从这儿能理解到的就是对于权重系数W,

75
00:04:30,940 --> 00:04:33,760
如果他们只比1大一点点，

76
00:04:33,760 --> 00:04:36,805
或只比单位矩阵大一点点，

77
00:04:36,805 --> 00:04:41,290
那在一个非常深的网络，激活函数就会爆炸

78
00:04:41,290 --> 00:04:45,525
另外如果W只比单位矩阵小一点点，

79
00:04:45,525 --> 00:04:49,020
有可能是0.9, 0.9

80
00:04:49,020 --> 00:04:50,062
而你有一个很深的网络，

81
00:04:50,062 --> 00:04:53,235
激活函数就会指数级的减少。

82
00:04:53,235 --> 00:04:56,175
虽然我这个论证是关于

83
00:04:56,175 --> 00:05:00,795
激活函数随着L指数级的增加或减少，

84
00:05:00,795 --> 00:05:03,180
同样的论证可以用来表明

85
00:05:03,180 --> 00:05:05,515
计算机算出的倒数或梯度

86
00:05:05,515 --> 00:05:08,485
也会指数级增加

87
00:05:08,485 --> 00:05:11,720
或指数级减少

88
00:05:11,720 --> 00:05:16,210
在一些现代的神经网络里，L等于150. 

89
00:05:16,210 --> 00:05:19,018
微软最近用一个152层的神经网络算出了很了不起的结果

90
00:05:19,018 --> 00:05:22,900
但是在这么深的神经网络里，

91
00:05:22,900 --> 00:05:27,760
如果你的激活函数或梯度作为L的函数指数级的增加或减少

92
00:05:27,760 --> 00:05:31,435
这些值会变得非常大或非常小

93
00:05:31,435 --> 00:05:33,777
这会让训练变得非常困难，

94
00:05:33,777 --> 00:05:36,970
尤其是如果你的梯度比L要小指数级别，

95
00:05:36,970 --> 00:05:40,540
梯度下降会很用很小很小步的走。

96
00:05:40,540 --> 00:05:44,580
梯度下降会用很长的时间才能有任何学习，

97
00:05:44,580 --> 00:05:47,380
作为总结，你到现在理解了深层的网络会受制于

98
00:05:47,380 --> 00:05:50,945
梯度逐渐消失或爆炸的问题

99
00:05:50,945 --> 00:05:52,750
事实上，这个问题很长时间以来

100
00:05:52,750 --> 00:05:56,040
都是一个训练深层神经网络的巨大的壁垒

101
00:05:56,040 --> 00:05:59,620
事实上 有一种针对此问题的部分解决方法
虽然不能完全解决问题

102
00:05:59,620 --> 00:06:01,670
但它对于如何谨慎选择初始化权重方法

103
00:06:01,670 --> 00:06:04,455
提供了不少帮助

104
00:06:04,455 --> 00:06:07,090
我们将在下一堂课中介绍