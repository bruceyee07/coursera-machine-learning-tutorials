이전 비디오를 통해서 매우 깊은 심층신경망에서는 vanishing 과 exploding gradient의 문제점이 있을 수 있다는 것을 배웠는데요. 부분적으로 해결할 수 있는 것은, 완전히 해결해주지는 않지만, 여러분의 신경망에서 random initialization을 선정하는데 있어 조금 더 괜찮거나, 조심스러운 선택을 하는 것입니다. 이것을 이해하기 위해, 하나의 신경세포를 초기화하는 방법에 대한 예시로 시작해 보도록 하죠, 우리는 이것을 깊은 네트워크로 일반화 할 것입니다. 단 하나의 신경세포로 된 예를 살펴보고, 그리고 나서 나중에 깊은 그물에 대해 이야기 하겠습니다. 하나의 신경세포를 입력값으로 넣는데요 x1에서부터 x4까지 말입니다. 그리고 a=g(z) 가 있고 y로 마무리합니다. 그리고 나중에 더 깊은 네트워크에서는 이 입력값들은 a(l)이라는 층이 될 것입니다. 하지만 지금은 x라고 그냥 하겠습니다. 그러면 z는 w1x1 + w2x2 +... + WnXn까지 가는데요, b는 일단 0으로 설정하겠습니다. b는 일단 무시하겠습니다. 그러면 z가 폭발적인 값을 갖지 않고 또 너무 작지 않게 하기 위해선 n이 크면 클수록 Wi는 더 작게 원할 것입니다. 맞죠? z는 wixi값의 합이기 때문에 그렇죠, 그리고 이런 항들을 더하는 경우, 각각의 항이 작은 값을 가져야 좋습니다. 한가지 합리적인 방법은 wi의 편차를 1 나누기 n으로 하는 것입니다. 여기서 n은 뉴런으로 들어가는 입력 특성의 개수입니다. 실제로 할 수 있는 것은, 비중인 W 매트릭스를 설정합니다. 특정 층에 대해서 말이죠. 이것은 np.random.randn로 합니다. 그 후로, 이것이 생긴 매트릭스의 모양이 어떻던 간에, 여기 넣고요 이것을 곱하기 루트 1나누기 l 층의 뉴런에 들어가는 입력 특성의 개수입니다. 이것은 n(l-1)인데요. 그 이유는 그것이 l층에 들어가는 각각의 유닛별 숫자입니다. 만약에 ReLu activation 함수를 쓰는 경우에는 1/n을 쓰는 것 대신에, 편차를 2/n으로 설정하는 것이 조금 더 잘 작동합니다. 특히 relu activation 함수를 쓰는 경우에는, 초기화의 경우 볼 수 있는데요, 공식이 gl(z)는 ReLu(z)인 경우이죠. 여러분이 얼마나 random variable에 익숙한지에 따라서, 알고 보니, Gaussian random variable 과 이것의 루트를 곱하는 것이 편차가 이 값이 되게 해줍니다. 2 나누기 n 말이죠. 그리고 제가 n에서 n 위첨자 l-1로 간 이유는, 여기 예제에서의 n개의 입력값이 있는 로지스틱 회귀분석이 있는데요 n 개의 입력 특성을 갖습니다. 더 일반적인 케이스는, l층은 n (l-1)층을 가질 것이고 이 입력값이 층마다 있을 것입니다. 만약 입력 activation의 입력 특성이 대략적으로 평균 0 와 편차 1이면 이것은 z가 비슷한 scale을 갖게 할 것이고 문제해결이 되지 않을 것입니다. 하지만 확실히 vanishing과 exploding gradient 문제를 도와주기는 합니다. 왜냐면 w하는 비중 매트릭스를 세팅해서 1조다 너무 크지 않고, 1보다 너무 작지 않게 해서 너무 빨리 explode하거나 vanish하지 않게 하려는 것입니다. 다른 몇가지 variant들을 언급했었는데요, 저희가 정의한 버전은 ReLu activation 함수를 가정한 것인데요, 학술 페이퍼에서 인용한 것입니다. 또 다른 variant가 있는데요, 여러분이 TanH activation 함수를 이용하는 경우, 논문에서 알 수 있지만, 상수 2를 쓰는 것이 상수 1을 쓰는 것보다 낫습니다. 즉, 1나누기 이것, 2 대신에 말이죠. 그리고 이것의 루트와 곱합니다. 그래서 여기 루트로 된 항이 여기 있는 항을 대체할 것입니다. TanH activation 함수를 이용하는 것입니다. 이것을 Xavier initialization이라고 하는데요 또 다른 버전은 Yoshua Bengio외 몇 명의 학자들이 가르쳤는데요, 여러분은 논문에서 봤을 수도 있습니다. 이 공식을 이용합니다. 여러 이론적인 정의가 있겠죠 제가 말하고 싶은 것은 만약 ReLu activation 함수를 이용하는 경우, 가장 흔한 activation 함수인데요, 저는 이 공식을 사용할 것입니다. 만약 TanH를 사용한다면 이 버전을 대신해서 사용할 수 있습니다. 몇몇 저자들은 이것을 이용할 것입니다. 실제로는 그러나 이 공식들은 모두 시작점을 제시합니다. 여러분의 weight matrixes의 초기화의 편차 값의 기본값을 줍니다. 여러분이 원하면, 여기있는 편차를, 편차 매개 변수가 여러분이 튜닝할 수 있는 하이퍼 파라미터중 하나 일 수 있습니다. 또 다른 파라미터가 여기 공식에 곱해질 수도 있는데요 그리하여 하이퍼 파라미터 surge를 위해 그 multiplier 값을 튜닝할 수 있겠습니다. 가끔씩은 하이퍼 파라미터를 튜닝하는 것이 적당한 크기의 효과가 있습니다. 이것은 제가 튜닝을 시도할 법한 하이퍼 파라미터 중 첫번째는 아닌데요, 이것을 통해 튜닝을 한 몇 개의 문제들을 접했었는데요 비교적으로 도움이 되는 편입니다. 하지만 저에게는 선호도에 있어서 이것은 밑에 있는 편입니다. 튜닝할 수 있는 다른 하이퍼 파라미터와 비교했을 때 말이죠. 이것이 폭발적으로 증가는 기울기나 사라지는 것에 대한 직관적인 부분을 알려주고 비중을 초기화시키는데 합리적인 scaling을 고를 수 있도록 직관적으로 여러분에게 도움이 됐었으면 좋겠습니다. 그것이 여러분의 비중을 너무 빨리 폭발하지 않고 너무 빨리 0으로 줄어들지 않도록 할 것입니다. 여러분이 비중이나 기울기가 폭발적으로 늘어나거나 사라지지 않게 비교적 깊은 네트워크를 트레이닝 시키기 위해서 말이죠. 깊은 네트워크를 트레이닝시킬 때 이것은 신경 네트워크를 많이 트레이닝 시키는데 도움을 주는 도움을 주는 또 다른 기술이다.