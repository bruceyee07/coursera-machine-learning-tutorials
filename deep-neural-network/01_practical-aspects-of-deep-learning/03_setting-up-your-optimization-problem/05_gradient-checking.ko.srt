1
00:00:00,400 --> 00:00:04,000
Gradient checking은 제가 많은 시간을
절약할 수 있도록 도와준 기법입니다.

2
00:00:04,000 --> 00:00:08,500
또한, 후 방향전파를 도입하면서
여러 번 버그도 찾을 수 있게 도와줬죠.

3
00:00:08,500 --> 00:00:10,890
Gradient checking을 통해 어떻게 디버그를 
할 수 있는지 알아보도록 하죠.

4
00:00:10,890 --> 00:00:14,885
또, 여러분의 도입방법과 back process가 옳은지도 
같이 검증해 보겠습니다.

5
00:00:14,885 --> 00:00:20,975
여러분의 새로운 네트워크는 특정 파라미터가 있는데요,
W1, b1 등등 그리고 WL bL까지 있을 수 있습니다.

6
00:00:20,975 --> 00:00:23,935
Gradient checking을 도입하기 위해서는 첫 번째로

7
00:00:23,935 --> 00:00:28,835
모든 파라미터를 자이언트 벡터 데이터 형식으로 재정비하셔야 합니다.

8
00:00:28,835 --> 00:00:34,860
그러니까 여러분이 해야 할 일은 W 매트릭스를 이용하여 벡터로 다시 만들면 됩니다.

9
00:00:34,860 --> 00:00:39,850
모든 W들을 가지고 벡터로 변경하신 후에,

10
00:00:39,850 --> 00:00:45,170
자이언트 벡터 세타를 형성할 수 있도록 벡터를 연결시켜 줍니다.

11
00:00:45,170 --> 00:00:47,020
자이언트 벡터는 theta (쎄타)라고 발음합니다.

12
00:00:47,020 --> 00:00:52,720
원가 함수 J는 W와 B들의 함수라고 할 수 있죠.

13
00:00:52,720 --> 00:00:58,380
즉, 이제는 이 원가 함수가 쎄타의 함수라고 할 수 있겠습니다.

14
00:00:58,380 --> 00:01:02,160
다음으로는, W와 B를 같은 방식으로 되어있으니

15
00:01:02,160 --> 00:01:07,740
dW[1], db[1] 등을 이용하여 

16
00:01:07,740 --> 00:01:12,200
쎄타와 치수가 같은 자이언트 벡터 d쎄타를 
만들 수 있습니다.

17
00:01:12,200 --> 00:01:17,210
이전과 같이, dW[1]를 매트릭스로 끼워 넣어,
db[1]은 이미 벡터형태죠,

18
00:01:17,210 --> 00:01:21,220
모두 매트릭스이기도 한 dW 묶음인데요, 
이런 dW[L]에 변형을 줍니다. 

19
00:01:21,220 --> 00:01:24,632
dW1은 W1과 같은 dimension이라는 것을 잊지 마십시오.

20
00:01:24,632 --> 00:01:27,080
db1 has the same dimension as b1.
db1은 b1과 dimension이 같습니다.

21
00:01:27,080 --> 00:01:31,252
그러므로 이와 같은 모양의 변형 및
개수 연결과 같은 절차를 이용하여

22
00:01:31,252 --> 00:01:36,343
이러한 derivatives에 변형을 주어 
자이언트 벡터 d쎄타를 만들 수 있습니다.

23
00:01:36,343 --> 00:01:38,750
자이언트 벡터 d쎄타는 쎄타와 dimension이 같습니다.

24
00:01:38,750 --> 00:01:43,780
이제 문제는, 쎄타가 gradient인지

25
00:01:43,780 --> 00:01:47,310
아니면 원가함수 J의 slope인지 알아야 할 텐데요.

26
00:01:47,310 --> 00:01:49,620
gradient checking을 도입하는 방법은 이렇습니다.

27
00:01:49,620 --> 00:01:52,740
gradient checking은 보통 줄여서 grad check라고 많이 하는데요,

28
00:01:52,740 --> 00:01:57,690
첫 번째로, 앞서 설명 드렸듯이
J는 이제 자이언트 파라미터의 함수입니다.

29
00:01:57,690 --> 00:01:58,277
쎄타라고하죠?

30
00:01:58,277 --> 00:02:04,750
J로 확대되는 영역은 쎄타 1, 2, 3 등등 이라고 할 수 있습니다.

31
00:02:06,880 --> 00:02:11,618
자이언트 파라미터 벡터 써타의 dimension이 무엇이 되었던 간에 말이죠.

32
00:02:11,618 --> 00:02:18,519
grad check를 도입하기 위해서는, 고리를 도입하여

33
00:02:18,519 --> 00:02:23,008
I 마다 즉, 써타 각각의 요소에

34
00:02:23,008 --> 00:02:26,416
대략적으로 d써타를 i 에서 b 까지 산출해 보겠습니다.

35
00:02:26,416 --> 00:02:28,170
양면의 값을 만들어보겠습니다.

36
00:02:28,170 --> 00:02:30,100
쎄타의 J를 가져오겠습니다.

37
00:02:30,100 --> 00:02:34,440
쎄타1, 쎄타2, 이렇게 쎄타 i까지 가지고 오겠습니다.

38
00:02:34,440 --> 00:02:38,380
그리고, 쎄타를 i 선택해서 엡실론을 더해보겠습니다.

39
00:02:38,380 --> 00:02:42,970
쎄타 i를 엡실론 만큼 증가시키고요,
나머지는 그대로 놔두겠습니다.

40
00:02:42,970 --> 00:02:46,164
양면의 값을 만드는 것이기 때문에,

41
00:02:46,164 --> 00:02:51,226
반대편에도 쎄타i를 이용하여 똑같이 할 텐데요,
이번에는 엡실론을 빼겠습니다.

42
00:02:51,226 --> 00:02:54,520
쎄타의 다른 모든 요소들은 홀로 남겨지게 됩니다.

43
00:02:54,520 --> 00:02:59,690
이 값을 가지고, 2 쎄타로 나누도록 하겠습니다.

44
00:02:59,690 --> 00:03:04,772
이전 비디오에서 봤듯이,

45
00:03:04,772 --> 00:03:10,270
이렇게 하면, 대략적으로 d 쎄타 i 값이 나올 겁니다.

46
00:03:10,270 --> 00:03:15,609
J의 partial derivative 값이기도 하고,

47
00:03:15,609 --> 00:03:21,320
쎄타 i에 대한 값이기도 합니다. d쎄타 가
원가함수 J의 derivative라면 말이죠.

48
00:03:21,320 --> 00:03:25,130
이제 여러분은 모든 i의 값에 대해서 값을 구할 것입니다.

49
00:03:25,130 --> 00:03:28,360
마지막에는 두 가지 벡터가 남을 텐데요.

50
00:03:28,360 --> 00:03:31,793
d써타 approx가 남고,

51
00:03:31,793 --> 00:03:35,860
이것은 d써타의 dimension과 동일할 것입니다.

52
00:03:35,860 --> 00:03:39,373
결과적으로 2개모두
쎄타와 동일한 dimension입니다.

53
00:03:39,373 --> 00:03:43,183
다음으로 할 것은, 이 벡터들이 서로 일치하는지

54
00:03:43,183 --> 00:03:44,130
확인하는 작업입니다.

55
00:03:44,130 --> 00:03:47,310
구체적으로, 어떻게 하면

56
00:03:47,310 --> 00:03:50,910
2개의 벡터가 비교적 가까이 있는지 여부를 
어떻게 정의 내릴까요?

57
00:03:50,910 --> 00:03:52,593
저는 이렇게 합니다.

58
00:03:52,593 --> 00:03:57,297
이 2개의 벡터간의 거리를 산출합니다.

59
00:03:57,297 --> 00:04:02,100
d쎄타 approx 빼기 d써타 
그러니까, 이 값의 L2 norm이 구요,

60
00:04:02,100 --> 00:04:03,851
보시면 알겠지만 제곱 표시가 위에 없습니다.

61
00:04:03,851 --> 00:04:06,788
이것은 요소들의 차이에 대한 제곱 합인데요.

62
00:04:06,788 --> 00:04:09,857
그 다음으로 루트를 적용해서
유클레디안 거리를 얻었습니다.

63
00:04:09,857 --> 00:04:15,512
이 벡터의 길이를 표준화하기 위하여,

64
00:04:15,512 --> 00:04:19,150
D쎄타 approx 더하기 d쎄타 값으로 나눠줍니다.

65
00:04:19,150 --> 00:04:22,620
이 분모에 해당하는 부분도 모두 유클레디안 거리를 적용하도록 합니다.

66
00:04:22,620 --> 00:04:28,044
분모의 역할은 벡터가 매우 작은 값을 갖는 경우,

67
00:04:28,044 --> 00:04:32,860
또는 아주 큰 값을 갖는 경우 이 분모가 식을 비율로 
변경해줍니다.

68
00:04:32,860 --> 00:04:35,202
이것을 실제로 적용하는 경우에,

69
00:04:35,202 --> 00:04:39,898
저는 엡실론을 10의 -7승을 씁니다.

70
00:04:39,898 --> 00:04:44,644
이 범위의 엡실론일 경우 이 공식이

71
00:04:44,644 --> 00:04:49,460
10의 -7승이나 더 작은 값을 주면 아주 좋습니다.

72
00:04:49,460 --> 00:04:53,302
그 뜻은 여러분의 derivative approximation이 맞는 가능성이 높습니다.

73
00:04:53,302 --> 00:04:55,330
이것은 아주 작은 값입니다.

74
00:04:55,330 --> 00:05:00,790
만약에 레인지가 10의 -5승에 해당한다고 하면
저 같은 경우게, 신중히 살펴볼 것 같습니다.

75
00:05:00,790 --> 00:05:02,148
잘하면 괜찮을 수도 있습니다.

76
00:05:02,148 --> 00:05:05,239
하지만 벡터의 구성요소를 다시 한번 봐서

77
00:05:05,239 --> 00:05:07,862
모든 요소들이 너무 크지 않도록 확인할 것입니다.

78
00:05:07,862 --> 00:05:10,649
그리고 이 요소의 차이가 너무 크면

79
00:05:10,649 --> 00:05:12,860
어느 곳에 버그가 있는 것일 수도 있습니다.

80
00:05:12,860 --> 00:05:17,719
만약 여기 왼쪽의 공식이 10의 -3승이라고 하면
저는 걱정이 될 것입니다.

81
00:05:17,719 --> 00:05:21,728
버그가 어느 곳에 있을 것이라고 생각이 들 것입니다.

82
00:05:21,728 --> 00:05:25,083
10의 -3승 보다 훨씬 더 적은 값을 가져야 합니다.

83
00:05:25,083 --> 00:05:29,690
이 값보다 더 큰 값이 나오면 저는 걱정이 될 것입니다.

84
00:05:29,690 --> 00:05:32,970
심각히 걱정이 될 것 같은데요 어느 곳에 버그가 있을 것이라는 생각이 들 것입니다. 

85
00:05:32,970 --> 00:05:37,204
이런 경우엔, 개인적인 

86
00:05:37,204 --> 00:05:41,799
데이터 요소들을 일일이 보면서 i의 값 중에서

87
00:05:41,799 --> 00:05:45,960
d쎄타 값이 d쎄타 i 와 현저히 다른 값을 찾을 것입니다.

88
00:05:45,960 --> 00:05:47,867
이것을 이용해서 derivative 계산이 

89
00:05:47,867 --> 00:05:51,040
틀린 지 여부도 찾아볼 것입니다.

90
00:05:51,040 --> 00:05:54,970
이렇게 해서 어느 정도 디버깅을 한 이후에, 

91
00:05:54,970 --> 00:05:59,820
작은 값으로 결과가 나올 텐데요,
이렇게 되면 정상적으로 도입을 한 것일 겁니다.

92
00:05:59,820 --> 00:06:01,320
그러므로 신경망을 도입할 때는,

93
00:06:01,320 --> 00:06:04,840
보통은 foreprop을 도입하고, 또 
backprop을 도립하고 나서

94
00:06:04,840 --> 00:06:08,612
grade check을 통해 큰 값을 확인할 수 있습니다.

95
00:06:08,612 --> 00:06:12,460
이런 경우, 버그가 있음을 감지하고, 디버그하고 또 디버그하고
또 디버그 할 것입니다.

96
00:06:12,460 --> 00:06:16,310
이렇게 디버깅을 어느 정도 실시한 이루, 
grad check을 통해 작은 결과 값이 나오면

97
00:06:16,310 --> 00:06:20,110
훨씬 더 자신 있게 맞는다고 이야기할 수 있습니다.

98
00:06:20,110 --> 00:06:22,310
자 이제 여러분은 gradient checking이 
어떻게 이루어지는지 배웠는데요,

99
00:06:22,310 --> 00:06:24,850
신경망을 도입하는 시점에서 이 방법은 
제가 버그를 찾는데 큰 도움을 주었습니다.

100
00:06:24,850 --> 00:06:27,330
여러분도 이 방법이 도움이 되길 바랍니다.

101
00:06:27,330 --> 00:06:29,970
여러분도 이 방법이 도움이 되길 바랍니다.

102
00:06:29,970 --> 00:06:33,490
gradient checking을 도입하는 방법에 대한 팁과 
노트를 공유하도록 하겠습니다.

103
00:06:33,490 --> 00:06:34,640
다음 비디오로 넘어가겠습니다.