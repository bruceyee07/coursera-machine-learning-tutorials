Önceki videoda, derin sinir ağlarınında azalan ve artan eğimlerin bazı sorunlara neden olduğunu gördünüz. Bu sorunlar,sinir ağları için rastgele ilkleme işleminin daha iyi ya da dikkatli seçiminin kısmi bir çözüm olduğunu, tamamen çözmese de çözüme oldukça yardım ettiğini ortaya çıkardı. Bunu anlamak için önce tek bir sinir hücresini ilkleme işlemiyle başlayalım ve daha sonra bunu derin ağlar için genelleştirelim. Tek bir sinir hücresi örneği üzerinden gidelim ve daha sonra derin ağlar hakkında konuşalım. Bir sinir hücresinde x1'den x4'e kadar öznitelikler ve ardından a=g(z) ile herhangi bir y olsun. Sonrasında, daha derin bir sinir ağı için bu girişler herhangi bir a(l) katmanı için doğru olacak fakat şimdilik bunu x olarak isimlendirelim. O zaman z, w1x1 + w2x2 +... WnXn'e kadar ifadesine eşit olacak. Bu arada, b=0 alarak b'yi şimdilik görmezden gelelim. Z'nin çok yüksek ya da çok küçük olmaması için n'in büyük olması, Wi'nin küçük olmasını istediğiniz anlamına gelir, fark ettiniz değil mi? Çünkü z, WiXi'nin toplamıdır ve eğer bu terimlerden çok fazla ekliyorsak, terimlerin her birinin daha küçük olmasını isteriz. Gidilebilecek mantıklı bir yol, Wi'nin değişintisinin 1 bölü n'e eşitlenmesidir. Burada n, bir sinir hücresine giden giriş özniteliklerinin sayısıdır. Yani uygulamada yapmanız gereken şey, ağırlık matrisi W'yi belirli bir katman için np.random.randn olacak şekilde belirlemek -ardından matrisin boyutları ne ise buraya yazılacak- çarpı, 1 bölü her bir sinir ağını beslediğim öznitelik sayısının karekökü burada bir n(l-1) terimi olacak çünkü bu,l katmanında beslediğim birimlerin sayısıdır. Eğer aktivasyon fonksiyonu olarak 1 bölü n yerine ReLU kullanıyorsanız,değişintiyi 1 bölü n değil de 2 bölü n olarak belirlemek biraz daha iyidir. Bunu ilklemede sıkça göreceksiniz. Özellikle de ReLU aktivasyon fonksiyonunu kullanıyorsanız. O zaman gl(z), ReLu(z) olur. Ah tabi bu sizin rastgele değişkenlerle ne kadar alakalı olduğunuza bağlıdır. Formül şunu ortaya koyuyor, bir Gauss rastgele değişkeni ve bunu, bu terimin kareköküyle çarpılır ve bu da değişintiyi bu şeye, yani 2 bölü n'e eşitler. Ve n'den bu n üzeri l-1'e geçmemin nedeni, bu örnekte mantıksal bağlanım n adet giriş özniteliği vardır ama daha genel bir durumda l katmanı, her birim için n üzeri l-1 giriş içerecek olmasıydı. O halde, eğer giriş öznitelikleri kabaca sıfırsa ve standart değişinti 1 ise, bu durumda z de benzer bir skalada yer alacaktır. Bu tamamen çözmez fakat bu kesinlikle artan,azalan eğimler sorununun çözümüne yardım edecektir. Çünkü bu, ağırlık matrisi w'daki her elemanı ne birden çok büyük ne de birden çok küçük olacak şekilde belirlemeye çalışıyor. Bu da çok hızlı artmasını ya da azalmasını önlüyor. Bazı farklı örneklere değindim. Ve anlattığımız bu örnekler ReLU aktivasyon fonksiyonunun kullanıldığını varsayıyor. Diğer birkaç örneği olarak, eğer TanH aktivasyon fonksiyonunu kullanıyorsanız o zaman, 2 sabitini kullanmak yerine 1'i kullanmanın daha iyi olduğu bir makalede gösterilmiştir. O halde 2 yerine 1 bölü ve bunun kareköküyle bu çarpılır. O zaman bu karekök terimi tanh aktivasyon fonksiyonu kullanıyorsanız bu terimin yerine geçecek. Bu Xavier ilklemesi olarak adlandırılır. Ayrıca Yoshua Bengio ve çalışma arkadaşları tarafından bulunan ve bazı kağıtlarda görebileceğiniz diğer bir sürümü farklı teorik kanıtlara dayanarak bu şekilde formülize edilmiştir. Ama belirtmeliyim ki; eğer ReLU aktivasyon fonksiyonunu kullanıyorsanız, -ki en yaygın aktivasyon fonksiyonudur- ben olsaydım bu formülü kullanırdım. Eğer TanH kullanıyorsanız, bu versiyonu deneyebilirsiniz. Bazı yazarlar bunu da kullanacaktır fakat uygulamada, bence tüm bu formüller size bir başlama noktası verir size, ağırlık matrisinizi ilkleme işlemini yaparken değişinti olarak kullanacağınız rastgele bir değer verir. Eğer isterseniz, buradaki değişinti bu değişinti parametresi sizin hiperparametreleri değiştirerek belirlediğiniz başka bir şey de olabilirdi. Yani bu formülde farklı bir çarpım parametresi kullanarak ve ayarlayarak hiperparametre arayışında başka bir çarpan elde edebiliriz. Bazen hiperparametre ayarlamanın Bu benim genellikle ayarlamayı denediğim ilk hipermetrelerden biri değil ama bunun ayarlandığı da bazı problemler de gördüm. Eh, makul bir miktar yardımcı olur ama bu genellikle benim için ayarlayabileceğiniz diğer hiperparametrelere göre daha az önemli. Sonuç olarak, umarım bu video size ağırlıkların nasıl tutarlı bir aralığa ilkleneceği, hızlı artan ve azalan eğim probleminin çözümü gibi konularda bazı fikirler vermiştir. Umarım bu ağırlıklarınızın ne çok hızlı yükselmesini ne de çok hızlı sıfıra düşmesini önler ve böylece ağırlıklar ya da eğim hızla artıp azalmadan tutarlı bir derin ağ eğitebilirsiniz. Derin ağlarınızı eğittiğinizde, bu derin ağları daha fazla eğitmede size yardım edecek başka bir yoldur.