신경망을 훈련시킬 때, 트레이닝의 속도를 높일 수 있는 것 중 한가지는 바로 입력 값을 normalize (표준화)시키는 겁니다. 이게 무슨 말인지 한번 보도록 하겠습니다. 두 가지의 입력기능을 가지고 있는 트레이닝 세트를 볼까요 입력기능 x는 2차원인데요 이 것은 여러분의 트레이닝 세트 산 점도 입니다. 입력 값을 표준화 시키는 절차는 2가지 단계가 따르는데요, 첫 번째는 평균값을 빼거나 0으로 만드는 단계입니다. mu를 시그마 1/m (x^i) 인 공식으로 설정합니다. 그러므로 이건 벡터라고 할 수 있는데요, X는 모든 트레이닝의 예시에 대하여 X-mu로 지정되는 것입니다. 그 뜻은 즉, 트레이닝 세트를 평균값이 0이 될 때까지
움직인다는 것입니다. 2번째 순서는 변동 편차를 표준화 하는 단계입니다. X1 기능이 X2보다 훨씬 더 큰 편차를 보이고 있는 것을 보실 수 있습니다. 그렇기 때문에 여기서는 시그마를 (1/m) 곱하기 Xi**2로 설정합니다. element y의 제곱개념이라고 생각하면 될 것 같습니다. 그렇게 되면 시그마 제곱은 각각 특성을 가진 편차의 벡터가 됩니다. 보시면 평균값을 이미 뺏기 때문에, Xi 제곱 값인 element y제곱은 단순히 편차가 되는 것입니다. 각각의 개별 예시를 벡터 시그마의 제곱으로 나눕니다. 그림으로는 이렇게 나오게 되는 거죠. 그렇게 되면 이제 X1, X2 편차가 모두 1이 되는 것입니다. 한가지 팁을 그리자면, 이것을 이용하여 본인의 트레이닝 데이터를 확대시키실 예정이면, 똑같은 mu 시그마 제곱을 쓰셔서 테스트를 세트를 표준화시킬 수 있도록 하십시오. 구체적으로, 트레이닝 세트와 테스트 세트를 다르게 표준화하지 않는 것이 좋습니다. 그 값이 어떻게 나오더라도 이 2가지의 공식에 적용하여 똑같은 방법으로 확대하는 방안을 모색하십시오. mu과 시그마 제곱의 값을 트레이닝 세트와 테스트 세트 각 개별로 추정하지 않도록 하시고요. 그 이유는, 트레이닝 및 테스트 예시가 여러분의 데이터이고 이러한 데이터가 여러분의 트레이닝 데이터에서 산출된 똑같은 mu값과 시그마 제곱 값을 통해 같은 조건으로 변형 되야 하기 때문입니다. 왜 이렇게 하는 것일까요? 입력 값의 특성을 왜 표준화하려고 하는 것일까요? 이전에 말씀 드렸던 것처럼 비용 함수는 오른쪽 위에 나와 있는 것과 같이 정의할 수 있습니다. 표준화되지 않은 입력 특성을 이용하면, 비용함수가 이렇게 보일 가능성이 큽니다. 푹 퍼진 그릇처럼 생겼는데요, 최소값이 아마도 저기 정도로 보이는 쭉 늘어진 모양의 비용함수 입니다. 그러나 그 특성이 매우 다른 값이라고 하면, 예를 들어, X1의 범위가 1에서 1000까지이고, X2의 범위가 0에서 1까지라고 하면, 그 해당 비율 또는 w1과 w2 값의 범위가 매우 다른 값을 띄게 될 것입니다. 그렇다면 아마 축들이 아마 w1과 w2가 되야 할 텐데요 일단 w와 b를 표시하겠습니다. 그렇게 되면, 여러분의 비용함수가 이렇게 접시처럼 쭉 늘어나는 모양일 텐데요, 이 함수의 윤곽선을 한번 나누어 보면, 이렇게 크게 늘어진 함수를 보게 될 것입니다. 반면에, 이 특성을 표준화시키면, 비용함수는 평균적으로 더 대칭 성향을 띄우는 모양이 될 것입니다. 좌측에 보이는 비용함수와 같이 기울기 강하를 이용하면, 굉장히 작은 교육 비율을 이용해야 될 것입니다. 그 이유는, 기울기 강하가 더 많은 단계를 거쳐 최소값에 도달하기 전까지 계속 왔다 갔다 할 수 있기 때문입니다. 반면에 여러분의 함수가 조금 더 구형의 윤곽선을 띄고 있다면 어디서 시작하더라도 기울기 강하가 바로 최소 점에 도달할 수 있습니다. 기울기 강하에서는 보다 큰 절차를 통해 왼쪽 사진처럼 왔다 갔다 시킬 필요 없이 진행할 수 있습니다. 물론 실제로는 w가 고차원 벡터이기 때문에, 정확한 직관적인 부분을 2D로 배열하게 되면 정확하게 전달하지 못하는 부분도 있다. 그러나 대략적으로는 비용함수가 조금 더 구형의 모양을 띄고 여러분의 기능이 모두 유사한 규모라고 하면, 더 쉽게 최적화 시킬 수 있습니다. 0에서 1000으로나, 0에서 1로는 아니며, 대부분 -1에서 1로, 또는 서로 비슷한 편차에서 끼리 움직입니다. 이렇게 하면 비용 함수 J를 더욱 쉽고 빠르게 최적화할 수 있습니다. 어느 한가지의 특성이, 예를 들어, X1이라고 하는 0에서 1까지 범위를 갖는 특성이 있고, X2라고 하는 -1에서 1까지 범위를 갖는 특성이 있고,
X3라고 하는 특성이 1에서 2까지 범위를 갖고 있다면, 실제로는 이 특성들은 서로 비슷한 범위에 있기 때문에 잘 작동할 것입니다. 범위가 크게 다를 경우가 문제인데요, 1에서 1000의 범위의 특성 한가지, 나머지 하나가 0에서 1의 범위를 가지면 authorizaion 알고리즘이 문제가 생깁니다. 그렇지만 이전 슬라이드에서 보았듯이 모든 것을 평균값 0으로 지정하고 편차가 1이 되도록 하면 비슷한 크기의 특성은 학습 알고리즘이 빠른 속도로 작동될 수 있도록 해줍니다. 그러므로, 입력 값의 특성이 각각 크기가 다른 곳에서 
왔다고 하면 예를 들어, 어떤 특성은 0에서 1사이, 다른 특성은 1에서 1,000사이에서 범주하고 있는 경우,
이러한 특성들을 표준화시키는 절차가 매우 중요해집니다. 이런 특성들이 비슷한 크기에서 왔다고 하면, 
이 절차가 덜 중요할 수 있기는 합니다. 그래도 이런 표준화 절차를 진행하는 것이 나쁠 것은 없겠죠. 저는 확실하지 않은 경우엔, 이런 절차를 실행하도록 하겠습니다. 여러분의 트레이닝 속도를 높이는 것이 확실하지 않은 경우에 말이죠. 입력 값 표준화 관련한 내용은 여기까지 다루도록 하겠습니다. 다음으로는, 새로운 네트워크 트레이닝을 빠르게 진행하는 방법에 대해
이야기 해보도록 하겠습니다.