1
00:00:00,000 --> 00:00:02,352
訓練神經網路的問題之一是

2
00:00:02,352 --> 00:00:04,585
特別是很深的神經網路時

3
00:00:04,585 --> 00:00:07,395
是資料消失以及梯度爆炸

4
00:00:07,395 --> 00:00:09,180
這個的意思是當您訓練

5
00:00:09,180 --> 00:00:13,650
很深的網路您的導數
或者您的斜率有時候會變得

6
00:00:13,650 --> 00:00:15,825
很大很大
或者很小很小

7
00:00:15,825 --> 00:00:17,420
也許是指數般的小

8
00:00:17,420 --> 00:00:19,450
而這會讓訓練變得很難

9
00:00:19,450 --> 00:00:21,690
在這段影片您會看到

10
00:00:21,690 --> 00:00:25,185
梯度爆炸跟消失
的問題真正的含義

11
00:00:25,185 --> 00:00:28,630
同時您如何可以謹慎選擇

12
00:00:28,630 --> 00:00:32,780
隨機權重初始可以
明顯地減低這個問題

13
00:00:32,780 --> 00:00:36,015
假設您正在訓練一個
很深的神經網路像這樣

14
00:00:36,015 --> 00:00:37,210
為了節省投影片的空間

15
00:00:37,210 --> 00:00:40,508
我畫成您只有
兩個隱藏單元在每一層

16
00:00:40,508 --> 00:00:42,575
但它可以更多

17
00:00:42,575 --> 00:00:45,625
這個神經網路會有參數 w[1],

18
00:00:45,625 --> 00:00:51,585
w[2], w[3] 等等到 w[L]

19
00:00:51,585 --> 00:00:53,025
為了簡單起見

20
00:00:53,025 --> 00:00:56,960
假設我們使用啟動函數 g of z 等於 z

21
00:00:56,960 --> 00:00:58,725
也就是線性啟動函數

22
00:00:58,725 --> 00:01:02,985
讓我們先忽略 b, 假設 b[l] 等於 0

23
00:01:02,985 --> 00:01:07,755
這種情況下您可以證明輸出 y 會是

24
00:01:07,755 --> 00:01:13,700
w[L] 乘 w[L-1] 乘 w[L-2]

25
00:01:13,700 --> 00:01:18,193
...直到 w[3],

26
00:01:18,193 --> 00:01:21,445
w[2], w[1] 乘 x

27
00:01:21,445 --> 00:01:23,830
但如果您要檢查我的數學

28
00:01:23,830 --> 00:01:27,915
w[1] 乘 x 會是 z1

29
00:01:27,915 --> 00:01:30,225
因為 b 等於 0

30
00:01:30,225 --> 00:01:33,540
所以 z[1] 等於

31
00:01:33,540 --> 00:01:37,960
w[1] 乘 x 然後加 b 但為 0

32
00:01:37,960 --> 00:01:42,440
但然後 a[1] 等於 g of z[1]

33
00:01:42,440 --> 00:01:45,150
但因為我們使用線性啟動函數

34
00:01:45,150 --> 00:01:47,755
這個就等於 z[1]

35
00:01:47,755 --> 00:01:50,360
所以這第一個項目 w[1]x 等於 a[1]

36
00:01:50,360 --> 00:01:57,950
然後用類似的方式我們可以發現
w[2] 乘 w[1] 乘 x 等於 a[2]

37
00:01:57,950 --> 00:02:00,118
因為那將會是 g of z[2]

38
00:02:00,118 --> 00:02:03,565
將會是 g of

39
00:02:03,565 --> 00:02:12,570
w[2] 乘 a[1], 我們可以代入這裡

40
00:02:12,570 --> 00:02:16,690
所以這個東西會等於 a[2]

41
00:02:16,690 --> 00:02:21,505
然後這個東西將會是

42
00:02:21,505 --> 00:02:29,065
a[3], 直到所有這些
矩陣的乘積會給您 y-hat, 不是 y

43
00:02:29,065 --> 00:02:33,080
現在假設每個您個權重矩陣

44
00:02:33,080 --> 00:02:39,677
w[L] 只有比 1 乘單元矩陣 大一點

45
00:02:39,677 --> 00:02:43,825
所以像是 1.5, 1.5, 0, 0

46
00:02:43,825 --> 00:02:46,000
技術上而言最後一個維度有所不同

47
00:02:46,000 --> 00:02:49,220
也許這只是其餘這些權重矩陣

48
00:02:49,220 --> 00:02:51,508
那 y-hat 會是

49
00:02:51,508 --> 00:02:54,903
先忽略最後一個不同維度

50
00:02:54,903 --> 00:03:01,770
這個 1.5, 0, 0, 1.5 矩陣的 L - 1 次方乘上 x

51
00:03:01,770 --> 00:03:08,050
因為我們假設每一個
這些矩陣等於這個東西

52
00:03:08,050 --> 00:03:12,945
這其實是 1.5 乘上單元矩陣，
那您最終會有這個計算

53
00:03:12,945 --> 00:03:19,150
所以 y-hat 基本上會是 1.5 的 L 次方

54
00:03:19,150 --> 00:03:21,715
L - 1 次方乘上 x

55
00:03:21,715 --> 00:03:24,505
如果 L 很大, 是很深的神經網路

56
00:03:24,505 --> 00:03:26,640
y-hat 會非常大

57
00:03:26,640 --> 00:03:28,375
實際上, 這個是指數成長

58
00:03:28,375 --> 00:03:32,145
它會長成 1.5 的層數次方

59
00:03:32,145 --> 00:03:34,290
如果您有很深的神經網路

60
00:03:34,290 --> 00:03:36,850
y 的值會爆炸

61
00:03:36,850 --> 00:03:40,832
現在，相反地，如果我們取代這個為 0.5

62
00:03:40,832 --> 00:03:42,268
是一種小於 1 的數字

63
00:03:42,268 --> 00:03:45,860
那這個會變成 0.5 的 L 次方

64
00:03:45,860 --> 00:03:51,515
這個矩陣會變成 0.5 的 L - 1 次方乘 x, 再次忽略 w[L]

65
00:03:51,515 --> 00:03:57,220
所以每一個矩陣小於 1

66
00:03:57,220 --> 00:04:00,415
假設 x1, x2 是 1, 1

67
00:04:00,415 --> 00:04:02,778
那啟動值會是 一半

68
00:04:02,778 --> 00:04:04,450
一半，四分之一

69
00:04:04,450 --> 00:04:07,227
四分之ㄧ, 八分之ㄧ, 八分之一

70
00:04:07,227 --> 00:04:11,470
直到這個變成 1/2 的 L 次方

71
00:04:11,470 --> 00:04:16,710
啟動值會指數般減少
像一個網路深度的函數

72
00:04:16,710 --> 00:04:19,945
也就是像函數對於 網路層的數目

73
00:04:19,945 --> 00:04:26,150
所以在很深的網路時, 啟動值會指數般減少

74
00:04:26,150 --> 00:04:30,940
這裡學到的直覺是從這個權重 w

75
00:04:30,940 --> 00:04:33,760
如果他們只是比 1 大一點點

76
00:04:33,760 --> 00:04:36,805
或者說把單元矩陣大一點點

77
00:04:36,805 --> 00:04:41,290
那在很深的網路,
啟動值會爆炸

78
00:04:41,290 --> 00:04:45,525
如果  w 比單元矩陣小一點點

79
00:04:45,525 --> 00:04:49,020
那也許是 0.9, 0.9

80
00:04:49,020 --> 00:04:50,062
在您用很深的網路時

81
00:04:50,062 --> 00:04:53,235
啟動值會指數般下降

82
00:04:53,235 --> 00:04:56,175
即使我歷經這個討論是

83
00:04:56,175 --> 00:05:00,795
啟動值指數般增加或減少
是一個對於 L 的函數

84
00:05:00,795 --> 00:05:03,180
類似的討論也可以用在證明

85
00:05:03,180 --> 00:05:05,515
計算導數或者梯度在梯度下降

86
00:05:05,515 --> 00:05:08,485
時也會指數般增加

87
00:05:08,485 --> 00:05:11,720
或指數般減少
為一個對於層數的函數

88
00:05:11,720 --> 00:05:16,210
一些現代的神經網路, L 等於 150

89
00:05:16,210 --> 00:05:19,018
微軟最近用 152 層的神經網路
獲得很好的成果

90
00:05:19,018 --> 00:05:22,900
但在這樣深的神經網路

91
00:05:22,900 --> 00:05:27,760
如果您的啟動值或者梯度
指數般增加或減少為 L 的函數

92
00:05:27,760 --> 00:05:31,435
那這些值可以很大或很小

93
00:05:31,435 --> 00:05:33,777
而這會讓訓練更困難

94
00:05:33,777 --> 00:05:36,970
特別是如果您的梯度是指數般小於 L

95
00:05:36,970 --> 00:05:40,540
那梯度下降會用非常小的步伐

96
00:05:40,540 --> 00:05:44,580
這樣會讓梯度下降
花很長的時間來學習

97
00:05:44,580 --> 00:05:47,380
總結一下, 您看到深度網路如何承受

98
00:05:47,380 --> 00:05:50,945
梯度消失或爆炸的問題

99
00:05:50,945 --> 00:05:52,750
實際上, 長久以來這個問題是

100
00:05:52,750 --> 00:05:56,040
訓練深度神經網路的很大障礙

101
00:05:56,040 --> 00:05:59,620
實際上有部份解決方案
不能完全解決

102
00:05:59,620 --> 00:06:01,670
這個問題但幫助很大
也就是

103
00:06:01,670 --> 00:06:04,455
仔細選擇您的初始權重

104
00:06:04,455 --> 00:06:07,090
讓我們在下一段影片
看這個方案