1
00:00:00,610 --> 00:00:03,220
在上一个视频中你已经了解了
有关梯度检验的知识

2
00:00:03,220 --> 00:00:06,317
在这个视频中
我想向你展示一些实用的技巧

3
00:00:06,317 --> 00:00:10,950
以及一些注意事项
关于如何在你的神经网络中运用这些技巧

4
00:00:10,950 --> 00:00:14,590
首先 不要在训练中使用梯度检查
而仅仅在调试时使用

5
00:00:14,590 --> 00:00:19,470
我的意思是
计算d(θapprox i)

6
00:00:19,470 --> 00:00:22,520
计算当i取所有值的情况
这是一个非常慢的计算过程

7
00:00:22,520 --> 00:00:26,320
所以当运用梯度下降时
你会运用反向传播来计算dθ

8
00:00:26,320 --> 00:00:29,110
也就是运用反向传播来
计算导数

9
00:00:29,110 --> 00:00:32,140
只有当你在调试时
你才会需要计算这个

10
00:00:32,140 --> 00:00:34,218
来保证它与dθ足够接近

11
00:00:34,218 --> 00:00:37,048
但当你完成后
你需要关掉梯度检验

12
00:00:37,048 --> 00:00:39,502
别在每一次进行梯度下降迭代的时候
都运行梯度检验

13
00:00:39,502 --> 00:00:41,530
因为这是在是太慢了

14
00:00:41,530 --> 00:00:45,060
第二 如果一个算法没有通过梯度检测
你需要检查它的组成

15
00:00:45,060 --> 00:00:48,010
检查每一个组成成分
尝试找出漏洞

16
00:00:48,010 --> 00:00:52,124
我的意思是 如果d(θapprox)与dθ差距很大的话

17
00:00:52,124 --> 00:00:57,079
我会这么做 检查不同的i值
看看哪些

18
00:00:57,079 --> 00:01:02,360
d(θapprox)的值
与dθ的值差距最大

19
00:01:02,360 --> 00:01:06,842
举个例子 如果你发现
某些θ或者dθ的值

20
00:01:06,842 --> 00:01:11,495
差距非常大 这与某一层或某些层的
db(l)有关

21
00:01:11,495 --> 00:01:16,162
但是dw的组成又很接近

22
00:01:16,162 --> 00:01:20,803
记得我们说过
theta的不同组成与

23
00:01:20,803 --> 00:01:21,434
b和w的不同组成有关

24
00:01:21,434 --> 00:01:25,918
当你发现这点时
那么你也许会发现漏洞

25
00:01:25,918 --> 00:01:30,411
就在你计算db的方法中
即参数b的导数

26
00:01:30,411 --> 00:01:35,495
类似地 反之亦同
如果你发现这两个数相距非常远

27
00:01:35,495 --> 00:01:39,610
d(θapprox)的值与dθ差距非常大

28
00:01:39,610 --> 00:01:44,452
你会发现所有的原因都来自于
某一层中的dw

29
00:01:44,452 --> 00:01:48,455
这也许能帮你找到漏洞的位置

30
00:01:48,455 --> 00:01:51,562
这也许不能总是让你
马上找到漏洞的位置

31
00:01:51,562 --> 00:01:55,622
但是有时候它能帮助你猜测
你需要在哪里找到漏洞

32
00:01:56,782 --> 00:01:59,502
下一个 当你在进行梯度检验时

33
00:01:59,502 --> 00:02:03,372
如果你使用了正则化
别忘了你的正则项

34
00:02:03,372 --> 00:02:10,052
所以你的代价函数为
J(θ)等于(1/m)乘上代价的和

35
00:02:10,052 --> 00:02:15,570
加上正则项

36
00:02:15,570 --> 00:02:22,790
再加上||w(l)||^2关于l求和
这就是J的定义

37
00:02:22,790 --> 00:02:27,200
还有dθ
即J关于θ的倒数

38
00:02:27,200 --> 00:02:30,840
包括这个正则项

39
00:02:30,840 --> 00:02:32,880
所以你需要记住加入这个项

40
00:02:32,880 --> 00:02:37,185
接下来 梯度检验不能与随机失活(dropout)
一起使用 因为在每一次的迭代中

41
00:02:37,185 --> 00:02:41,307
随机失活(dropout)将随机消除
隐藏层单元的不同子集

42
00:02:41,307 --> 00:02:45,923
在使用随机失活(dropout)
进行梯度下降的过程中

43
00:02:45,923 --> 00:02:48,098
并不存在一个容易计算的代价函数J

44
00:02:48,098 --> 00:02:52,932
随机失活(dropout)可以被视为
对于代价函数的优化

45
00:02:52,932 --> 00:02:57,254
但是这个代价函数的定义是
在每一次迭代中 对所有

46
00:02:57,254 --> 00:03:00,900
非常大的可消除节点集进行求和

47
00:03:00,900 --> 00:03:04,780
所以这个代价函数是很难计算的

48
00:03:04,780 --> 00:03:07,560
你只需要对代价函数进行抽样

49
00:03:07,560 --> 00:03:11,770
在那些使用随机失活(dropout)的集合中
每次消除不同的随机集合

50
00:03:11,770 --> 00:03:14,730
所以使用梯度检验来检查

51
00:03:14,730 --> 00:03:16,810
包含了随机失活(dropout)的运算是很困难的

52
00:03:16,810 --> 00:03:20,360
所以我常常在使用梯度检验的同时
不使用随机失活(dropout)

53
00:03:20,360 --> 00:03:25,285
你可以把keep-prob和dropout设为1.0

54
00:03:25,285 --> 00:03:29,590
然后打开dropout
希望我对于dropout的使用是正确的

55
00:03:30,770 --> 00:03:35,738
还有一些别的事情可以做
比如修正那些舍弃节点的模式

56
00:03:35,738 --> 00:03:39,914
并且使用梯度检验来检查它们的模式是否正确

57
00:03:39,914 --> 00:03:43,200
但实际上我通常不这样做

58
00:03:43,200 --> 00:03:48,010
所以我的建议是 关掉随机失活(dropout)
使用梯度检验来检查你的算法

59
00:03:48,010 --> 00:03:52,560
在没有dropout的情况下至少是正确的
然后再打开dropout

60
00:03:52,560 --> 00:03:55,520
最后的这个内容有些微妙

61
00:03:55,520 --> 00:03:59,853
虽然很少发生
但并不是没有可能

62
00:03:59,853 --> 00:04:04,322
你对于梯度下降的使用是正确的
同时w和b在随机初始化的时候

63
00:04:04,322 --> 00:04:06,500
是很接近0的数

64
00:04:06,500 --> 00:04:10,223
但随着梯度下降的进行
w和b有所增大

65
00:04:10,223 --> 00:04:15,089
也许你的反向传播算法
在w和b接近0的时候是正确的

66
00:04:15,089 --> 00:04:18,660
但是当w和b变大的时候
算法精确度有所下降

67
00:04:18,660 --> 00:04:21,510
所以虽然我不经常使用它
但是你可以尝试的一个方法是

68
00:04:21,510 --> 00:04:25,850
在随机初始化的时候
运行梯度检验

69
00:04:25,850 --> 00:04:27,940
然后训练网络一段时间
那么w和b

70
00:04:27,940 --> 00:04:33,198
将会在0附近摇摆一段时间
即很小的随机初始值

71
00:04:33,198 --> 00:04:37,620
在进行几次训练的迭代后
再运行梯度检验

72
00:04:37,620 --> 00:04:39,165
那么这就是梯度检验的全部内容

73
00:04:39,165 --> 00:04:42,760
祝贺你来到了这一周内容的最后部分

74
00:04:42,760 --> 00:04:47,100
在这一周中 你学会了如何
设置训练 开发 和测试集

75
00:04:47,100 --> 00:04:51,254
如何分析高偏差/高方差的情况
以及面对高偏差或者高方差

76
00:04:51,254 --> 00:04:54,230
你该如何应对

77
00:04:54,230 --> 00:04:57,930
你也看到了如何运用
不同形式的正则化

78
00:04:57,930 --> 00:05:02,070
比如L2正则化还有
对你的神经网络进行随机失活(dropout)

79
00:05:02,070 --> 00:05:05,318
即一些加速神经网络训练的技巧

80
00:05:05,318 --> 00:05:07,920
最后是梯度检验的内容

81
00:05:07,920 --> 00:05:10,480
那么 我想你在这一周看了很多内容

82
00:05:10,480 --> 00:05:14,300
你会在这一周的编程训练中
对于这些概念获得很多的训练

83
00:05:14,300 --> 00:05:15,520
那么祝你好运

84
00:05:15,520 --> 00:05:17,830
期待在第二周的视频中见到你