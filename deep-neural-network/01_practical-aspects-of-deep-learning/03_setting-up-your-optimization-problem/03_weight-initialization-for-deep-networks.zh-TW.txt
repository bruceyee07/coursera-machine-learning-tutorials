在上一段影片中, 您看到了很深的神經網路 可能有梯度消失或爆炸的問題 實際上有部分解決方案 無法完全解決但幫助很大 也就是小心選擇您
神經網路的隨機初始化 要理解這個讓我們從一個
初始一個單一神經元的例子開始 然後我們在一般化到深度網路去 讓我們用一個例子開始 只用一個單一神經元
然後我們等一下再來談深度網路 所以一個單一神經元您也許
輸入四個特徵 x1 到 x4, 然後您有一些 a = g(z) 最後會是一些 y 等一下在深度網路
這些輸入會是 一些層 a[l],
但現在我們先稱這些為 x 所以 z 會是等於 w1x1 + w2x2 + ... 直到 wnxn 假設 b = 0, 所以讓我們先忽略 b 為了讓 z 不會膨脹
也不會變得 太小, 您注意最大的 n 是 在您要的最小的 wi, 是吧? 因為 z 是 wixi 的和 如果您要將很多這些項目相加,
您要每一個項目都小一點 一種合理的方式是設 wi 變異為 1 / n 而 n 是輸入特徵的數目
進入一個神經元 實作上, 您可以做的是設權重矩陣 w 對於特定層 為 np.random.randn 然後無論這個矩陣的維度多少, 
放進參數 然後乘上 平方根 of 1 除以特徵數目
餵入每一個神經元 也就是 n[l-1] 因為這是單元數目
我餵進每一個單元 實際上如果您使用 一個 ReLU 啟動函數, 那與其用 1/n, 實際上 設變異數為 2/n 
會作用得好一點 所以您經常會看到這種初始化,
特別是如果您使用 ReLU 啟動函數所以如果 g(z) 是 ReLU(z) 取決於您多瞭解隨機變數 實際上 高斯隨機變數然後
乘上這個平方根 也就是說變異數 為 2/n,
我從 n 變成 n 上標  l-1 是 在這個例子用羅吉斯迴歸分析
所以是 輸入特徵, 但在一般情況下 l 層會有 n[l-1] 個輸入對於
這一層的每一個單元 如果輸入特徵或者啟動值大約是平均值 0 跟標準變異數 變異數 1, 那這會讓 z 用類似的比例, 
而這個並不能解決 但應該會減低梯度消失, 梯度爆炸的問題, 因為這試著設定 每一個權重矩陣 w 不會太 大於 1, 也不會太小於 1,
所以不會太快爆炸或者消失 我剛提到一些變形 我剛提到的是假設 用 ReLU 啟動函數在
[聽不清](可能是 He's and Glorot's) 的論文中提到 其他的變形 如果您使用 tanh 啟動函數 有一篇論文顯示與其用常數 2 用 1 比較好，所以是 1 除以這個 而不是 2, 所以您乘上這個然後取平方根 所以這個平方根裡的項目會由 這個項目取代, 您用這個
如果您使用 tanh 啟動函數 這是稱為 Xavier 初始化 另一個版本我們會教到是
Yoshua Bengio 跟他的同事 您也許看過一些論文 但使用這樣的公式 有其他一些理論上的理由 但我會說
如果您使用 ReLU 啟動函數 也是最常用到的啟動函數 我會使用這個公式 如果您用 tanh, 您可以試這一版本, 一些作者也會 使用這個, 但實作上,
所有這些公式只是給您一個起點 它給您一個預設值來使用對於 您的各種不同的變異數的權重矩陣初始化 如果您要的話, 這個變異數 這個變異數參數也可以是
另一種您可以 調整的超參數, 所以您可以有 另一個參數乘上這個公式然後 調整這個乘數是
您超參數的一部份 有時候調整超參數有
中等程度的作用 它不會是我通常要
優先調整的超參數 但我也看過一些
調整這個超參數的問題 這個會有一些合理的幫助
但這通常對我而言比較低的優先權 對於其相對於
其他需要調整的超參數而言 所以我希望這會給您一些直觀有關於 梯度消失及爆炸的問題
以及如何去選擇 合理的比例對於如何來初始化權重 希望這樣做會讓您的權重
不要太快爆炸 也不要太快衰退
所以您可以 訓練合理深度的網路而不會 造成權重或者梯度太快爆炸或者消失 當您訓練深度網路
這是另一個技巧來幫助您 更快訓練您的神經網路