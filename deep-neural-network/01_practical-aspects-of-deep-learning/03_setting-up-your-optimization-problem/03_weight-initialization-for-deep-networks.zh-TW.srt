1
00:00:00,530 --> 00:00:04,590
在上一段影片中, 您看到了很深的神經網路

2
00:00:04,590 --> 00:00:08,330
可能有梯度消失或爆炸的問題

3
00:00:08,330 --> 00:00:11,040
實際上有部分解決方案

4
00:00:11,040 --> 00:00:13,455
無法完全解決但幫助很大

5
00:00:13,455 --> 00:00:18,915
也就是小心選擇您
神經網路的隨機初始化

6
00:00:18,915 --> 00:00:23,220
要理解這個讓我們從一個
初始一個單一神經元的例子開始

7
00:00:23,220 --> 00:00:27,505
然後我們在一般化到深度網路去

8
00:00:27,505 --> 00:00:30,040
讓我們用一個例子開始

9
00:00:30,040 --> 00:00:33,140
只用一個單一神經元
然後我們等一下再來談深度網路

10
00:00:33,140 --> 00:00:39,060
所以一個單一神經元您也許
輸入四個特徵 x1 到 x4, 然後您有一些

11
00:00:39,060 --> 00:00:42,465
a = g(z) 最後會是一些 y

12
00:00:42,465 --> 00:00:46,830
等一下在深度網路
這些輸入會是

13
00:00:46,830 --> 00:00:51,780
一些層 a[l],
但現在我們先稱這些為 x

14
00:00:51,780 --> 00:00:59,070
所以 z 會是等於 w1x1 + w2x2 + ... 直到 wnxn

15
00:00:59,070 --> 00:01:08,570
假設 b = 0, 所以讓我們先忽略 b

16
00:01:08,570 --> 00:01:12,390
為了讓 z 不會膨脹
也不會變得

17
00:01:12,390 --> 00:01:16,960
太小, 您注意最大的 n 是

18
00:01:16,960 --> 00:01:22,070
在您要的最小的 wi, 是吧?

19
00:01:22,070 --> 00:01:25,910
因為 z 是 wixi 的和

20
00:01:25,910 --> 00:01:30,680
如果您要將很多這些項目相加,
您要每一個項目都小一點

21
00:01:30,680 --> 00:01:41,045
一種合理的方式是設 wi 變異為 1 / n

22
00:01:41,045 --> 00:01:45,495
而 n 是輸入特徵的數目
進入一個神經元

23
00:01:45,495 --> 00:01:51,441
實作上, 您可以做的是設權重矩陣 w 對於特定層

24
00:01:51,441 --> 00:01:58,045
為 np.random.randn 

25
00:01:58,045 --> 00:02:02,205
然後無論這個矩陣的維度多少, 
放進參數

26
00:02:02,205 --> 00:02:06,810
然後乘上 平方根 of 1

27
00:02:06,810 --> 00:02:12,890
除以特徵數目
餵入每一個神經元

28
00:02:12,890 --> 00:02:14,605
也就是 n[l-1]

29
00:02:14,605 --> 00:02:20,700
因為這是單元數目
我餵進每一個單元

30
00:02:20,700 --> 00:02:23,340
實際上如果您使用

31
00:02:23,340 --> 00:02:28,830
一個 ReLU 啟動函數, 那與其用 1/n, 實際上

32
00:02:28,830 --> 00:02:32,105
設變異數為 2/n 
會作用得好一點

33
00:02:32,105 --> 00:02:35,580
所以您經常會看到這種初始化,
特別是如果您使用

34
00:02:35,580 --> 00:02:42,425
ReLU 啟動函數所以如果 g(z) 是 ReLU(z)

35
00:02:42,425 --> 00:02:45,030
取決於您多瞭解隨機變數

36
00:02:45,030 --> 00:02:46,590
實際上

37
00:02:46,590 --> 00:02:50,835
高斯隨機變數然後
乘上這個平方根

38
00:02:50,835 --> 00:02:54,330
也就是說變異數

39
00:02:54,330 --> 00:02:59,485
為 2/n,
我從 n 變成 n 上標  l-1 是

40
00:02:59,485 --> 00:03:02,610
在這個例子用羅吉斯迴歸分析
所以是

41
00:03:02,610 --> 00:03:05,625
輸入特徵, 但在一般情況下

42
00:03:05,625 --> 00:03:12,400
l 層會有 n[l-1] 個輸入對於
這一層的每一個單元

43
00:03:12,400 --> 00:03:19,305
如果輸入特徵或者啟動值大約是平均值 0 跟標準變異數

44
00:03:19,305 --> 00:03:22,760
變異數 1, 那這會讓 z

45
00:03:22,760 --> 00:03:26,580
用類似的比例, 
而這個並不能解決

46
00:03:26,580 --> 00:03:30,630
但應該會減低梯度消失,

47
00:03:30,630 --> 00:03:33,240
梯度爆炸的問題, 因為這試著設定

48
00:03:33,240 --> 00:03:36,510
每一個權重矩陣 w 不會太

49
00:03:36,510 --> 00:03:42,900
大於 1, 也不會太小於 1,
所以不會太快爆炸或者消失

50
00:03:42,900 --> 00:03:45,075
我剛提到一些變形

51
00:03:45,075 --> 00:03:47,640
我剛提到的是假設

52
00:03:47,640 --> 00:03:51,270
用 ReLU 啟動函數在
[聽不清](可能是 He's and Glorot's) 的論文中提到

53
00:03:51,270 --> 00:03:53,750
其他的變形

54
00:03:53,750 --> 00:03:57,600
如果您使用 tanh 啟動函數

55
00:03:57,600 --> 00:04:02,060
有一篇論文顯示與其用常數 2

56
00:04:02,060 --> 00:04:06,870
用 1 比較好，所以是 1 除以這個

57
00:04:06,870 --> 00:04:12,615
而不是 2, 所以您乘上這個然後取平方根

58
00:04:12,615 --> 00:04:16,605
所以這個平方根裡的項目會由

59
00:04:16,605 --> 00:04:23,165
這個項目取代, 您用這個
如果您使用 tanh 啟動函數

60
00:04:23,165 --> 00:04:26,790
這是稱為 Xavier 初始化

61
00:04:26,790 --> 00:04:30,870
另一個版本我們會教到是
Yoshua Bengio 跟他的同事

62
00:04:30,870 --> 00:04:32,190
您也許看過一些論文

63
00:04:32,190 --> 00:04:36,140
但使用這樣的公式

64
00:04:36,140 --> 00:04:40,300
有其他一些理論上的理由

65
00:04:40,300 --> 00:04:43,800
但我會說
如果您使用 ReLU 啟動函數

66
00:04:43,800 --> 00:04:46,605
也是最常用到的啟動函數

67
00:04:46,605 --> 00:04:48,555
我會使用這個公式

68
00:04:48,555 --> 00:04:53,610
如果您用 tanh, 您可以試這一版本, 一些作者也會

69
00:04:53,610 --> 00:04:58,665
使用這個, 但實作上,
所有這些公式只是給您一個起點

70
00:04:58,665 --> 00:05:01,210
它給您一個預設值來使用對於

71
00:05:01,210 --> 00:05:04,270
您的各種不同的變異數的權重矩陣初始化

72
00:05:04,270 --> 00:05:06,760
如果您要的話, 這個變異數

73
00:05:06,760 --> 00:05:09,875
這個變異數參數也可以是
另一種您可以

74
00:05:09,875 --> 00:05:13,530
調整的超參數, 所以您可以有

75
00:05:13,530 --> 00:05:16,930
另一個參數乘上這個公式然後

76
00:05:16,930 --> 00:05:21,075
調整這個乘數是
您超參數的一部份

77
00:05:21,075 --> 00:05:26,105
有時候調整超參數有
中等程度的作用

78
00:05:26,105 --> 00:05:29,670
它不會是我通常要
優先調整的超參數

79
00:05:29,670 --> 00:05:33,325
但我也看過一些
調整這個超參數的問題

80
00:05:33,325 --> 00:05:37,680
這個會有一些合理的幫助
但這通常對我而言比較低的優先權

81
00:05:37,680 --> 00:05:42,870
對於其相對於
其他需要調整的超參數而言

82
00:05:42,870 --> 00:05:47,520
所以我希望這會給您一些直觀有關於

83
00:05:47,520 --> 00:05:49,935
梯度消失及爆炸的問題
以及如何去選擇

84
00:05:49,935 --> 00:05:52,955
合理的比例對於如何來初始化權重

85
00:05:52,955 --> 00:05:55,620
希望這樣做會讓您的權重
不要太快爆炸

86
00:05:55,620 --> 00:05:58,890
也不要太快衰退
所以您可以

87
00:05:58,890 --> 00:06:01,650
訓練合理深度的網路而不會

88
00:06:01,650 --> 00:06:05,730
造成權重或者梯度太快爆炸或者消失

89
00:06:05,730 --> 00:06:08,580
當您訓練深度網路
這是另一個技巧來幫助您

90
00:06:08,580 --> 00:06:11,640
更快訓練您的神經網路