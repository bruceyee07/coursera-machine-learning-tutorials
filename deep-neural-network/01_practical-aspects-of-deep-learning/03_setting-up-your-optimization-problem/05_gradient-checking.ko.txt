Gradient checking은 제가 많은 시간을
절약할 수 있도록 도와준 기법입니다. 또한, 후 방향전파를 도입하면서
여러 번 버그도 찾을 수 있게 도와줬죠. Gradient checking을 통해 어떻게 디버그를 
할 수 있는지 알아보도록 하죠. 또, 여러분의 도입방법과 back process가 옳은지도 
같이 검증해 보겠습니다. 여러분의 새로운 네트워크는 특정 파라미터가 있는데요,
W1, b1 등등 그리고 WL bL까지 있을 수 있습니다. Gradient checking을 도입하기 위해서는 첫 번째로 모든 파라미터를 자이언트 벡터 데이터 형식으로 재정비하셔야 합니다. 그러니까 여러분이 해야 할 일은 W 매트릭스를 이용하여 벡터로 다시 만들면 됩니다. 모든 W들을 가지고 벡터로 변경하신 후에, 자이언트 벡터 세타를 형성할 수 있도록 벡터를 연결시켜 줍니다. 자이언트 벡터는 theta (쎄타)라고 발음합니다. 원가 함수 J는 W와 B들의 함수라고 할 수 있죠. 즉, 이제는 이 원가 함수가 쎄타의 함수라고 할 수 있겠습니다. 다음으로는, W와 B를 같은 방식으로 되어있으니 dW[1], db[1] 등을 이용하여 쎄타와 치수가 같은 자이언트 벡터 d쎄타를 
만들 수 있습니다. 이전과 같이, dW[1]를 매트릭스로 끼워 넣어,
db[1]은 이미 벡터형태죠, 모두 매트릭스이기도 한 dW 묶음인데요, 
이런 dW[L]에 변형을 줍니다. dW1은 W1과 같은 dimension이라는 것을 잊지 마십시오. db1 has the same dimension as b1.
db1은 b1과 dimension이 같습니다. 그러므로 이와 같은 모양의 변형 및
개수 연결과 같은 절차를 이용하여 이러한 derivatives에 변형을 주어 
자이언트 벡터 d쎄타를 만들 수 있습니다. 자이언트 벡터 d쎄타는 쎄타와 dimension이 같습니다. 이제 문제는, 쎄타가 gradient인지 아니면 원가함수 J의 slope인지 알아야 할 텐데요. gradient checking을 도입하는 방법은 이렇습니다. gradient checking은 보통 줄여서 grad check라고 많이 하는데요, 첫 번째로, 앞서 설명 드렸듯이
J는 이제 자이언트 파라미터의 함수입니다. 쎄타라고하죠? J로 확대되는 영역은 쎄타 1, 2, 3 등등 이라고 할 수 있습니다. 자이언트 파라미터 벡터 써타의 dimension이 무엇이 되었던 간에 말이죠. grad check를 도입하기 위해서는, 고리를 도입하여 I 마다 즉, 써타 각각의 요소에 대략적으로 d써타를 i 에서 b 까지 산출해 보겠습니다. 양면의 값을 만들어보겠습니다. 쎄타의 J를 가져오겠습니다. 쎄타1, 쎄타2, 이렇게 쎄타 i까지 가지고 오겠습니다. 그리고, 쎄타를 i 선택해서 엡실론을 더해보겠습니다. 쎄타 i를 엡실론 만큼 증가시키고요,
나머지는 그대로 놔두겠습니다. 양면의 값을 만드는 것이기 때문에, 반대편에도 쎄타i를 이용하여 똑같이 할 텐데요,
이번에는 엡실론을 빼겠습니다. 쎄타의 다른 모든 요소들은 홀로 남겨지게 됩니다. 이 값을 가지고, 2 쎄타로 나누도록 하겠습니다. 이전 비디오에서 봤듯이, 이렇게 하면, 대략적으로 d 쎄타 i 값이 나올 겁니다. J의 partial derivative 값이기도 하고, 쎄타 i에 대한 값이기도 합니다. d쎄타 가
원가함수 J의 derivative라면 말이죠. 이제 여러분은 모든 i의 값에 대해서 값을 구할 것입니다. 마지막에는 두 가지 벡터가 남을 텐데요. d써타 approx가 남고, 이것은 d써타의 dimension과 동일할 것입니다. 결과적으로 2개모두
쎄타와 동일한 dimension입니다. 다음으로 할 것은, 이 벡터들이 서로 일치하는지 확인하는 작업입니다. 구체적으로, 어떻게 하면 2개의 벡터가 비교적 가까이 있는지 여부를 
어떻게 정의 내릴까요? 저는 이렇게 합니다. 이 2개의 벡터간의 거리를 산출합니다. d쎄타 approx 빼기 d써타 
그러니까, 이 값의 L2 norm이 구요, 보시면 알겠지만 제곱 표시가 위에 없습니다. 이것은 요소들의 차이에 대한 제곱 합인데요. 그 다음으로 루트를 적용해서
유클레디안 거리를 얻었습니다. 이 벡터의 길이를 표준화하기 위하여, D쎄타 approx 더하기 d쎄타 값으로 나눠줍니다. 이 분모에 해당하는 부분도 모두 유클레디안 거리를 적용하도록 합니다. 분모의 역할은 벡터가 매우 작은 값을 갖는 경우, 또는 아주 큰 값을 갖는 경우 이 분모가 식을 비율로 
변경해줍니다. 이것을 실제로 적용하는 경우에, 저는 엡실론을 10의 -7승을 씁니다. 이 범위의 엡실론일 경우 이 공식이 10의 -7승이나 더 작은 값을 주면 아주 좋습니다. 그 뜻은 여러분의 derivative approximation이 맞는 가능성이 높습니다. 이것은 아주 작은 값입니다. 만약에 레인지가 10의 -5승에 해당한다고 하면
저 같은 경우게, 신중히 살펴볼 것 같습니다. 잘하면 괜찮을 수도 있습니다. 하지만 벡터의 구성요소를 다시 한번 봐서 모든 요소들이 너무 크지 않도록 확인할 것입니다. 그리고 이 요소의 차이가 너무 크면 어느 곳에 버그가 있는 것일 수도 있습니다. 만약 여기 왼쪽의 공식이 10의 -3승이라고 하면
저는 걱정이 될 것입니다. 버그가 어느 곳에 있을 것이라고 생각이 들 것입니다. 10의 -3승 보다 훨씬 더 적은 값을 가져야 합니다. 이 값보다 더 큰 값이 나오면 저는 걱정이 될 것입니다. 심각히 걱정이 될 것 같은데요 어느 곳에 버그가 있을 것이라는 생각이 들 것입니다. 이런 경우엔, 개인적인 데이터 요소들을 일일이 보면서 i의 값 중에서 d쎄타 값이 d쎄타 i 와 현저히 다른 값을 찾을 것입니다. 이것을 이용해서 derivative 계산이 틀린 지 여부도 찾아볼 것입니다. 이렇게 해서 어느 정도 디버깅을 한 이후에, 작은 값으로 결과가 나올 텐데요,
이렇게 되면 정상적으로 도입을 한 것일 겁니다. 그러므로 신경망을 도입할 때는, 보통은 foreprop을 도입하고, 또 
backprop을 도립하고 나서 grade check을 통해 큰 값을 확인할 수 있습니다. 이런 경우, 버그가 있음을 감지하고, 디버그하고 또 디버그하고
또 디버그 할 것입니다. 이렇게 디버깅을 어느 정도 실시한 이루, 
grad check을 통해 작은 결과 값이 나오면 훨씬 더 자신 있게 맞는다고 이야기할 수 있습니다. 자 이제 여러분은 gradient checking이 
어떻게 이루어지는지 배웠는데요, 신경망을 도입하는 시점에서 이 방법은 
제가 버그를 찾는데 큰 도움을 주었습니다. 여러분도 이 방법이 도움이 되길 바랍니다. 여러분도 이 방법이 도움이 되길 바랍니다. gradient checking을 도입하는 방법에 대한 팁과 
노트를 공유하도록 하겠습니다. 다음 비디오로 넘어가겠습니다.