Um dos problemas de treinar Redes Neurais, especialmente Redes Neurais muito profundas, é o desaparecimento e a explosão dos gradientes. Isso significa que quando você estiver treinando uma Rede Neural muito profunda, suas derivadas (gradientes) 
ou suas inclinações podem, às vezes, ser muito muito grandes ou muito, muito pequenas, talvez até exponencialmente pequenas, e isso pode dificultar seu treinamento. Neste vídeo você verá o que este problema de explosão ou desaparecimento de
gradientes realmente significa, e também como você pode usar 
 escolhas cuidadosas de pesos na inicialização aleatória para
 reduzir significativamente este problema. Digamos que você esteja treinando uma 
Rede Neural tão profunda como esta para economizar espaço neste "slide", eu desenhei como se houvesse apenas 
duas unidades ocultas por camada, porém, poderiam haver mais, como de costume. Esta Rede Neural terá parâmetros W'¹' W'²', W'³' e assim por diante até W'ᴸ'. Por questões de simplificação, vamos dizer que estamos usando
 uma função de ativação "g(z) = z", uma função de ativação linear. E vamos ignorar b, vamos dizer que
 "b'ˡ' = 0". Então, nesse caso você pode mostrar 
que a saída "y" será W'ᴸ' W'ᴸ⁻¹' W'ᴸ⁻²' e assim por diante, até o 
W'³'W'²' W'¹' x W'³'W'²' W'¹' x Se você quiser verificar os cálculos, veja que W'¹' x = z'¹'. pois "b" é igual a 0. Então, z'¹' é igual a "W'¹' x" mais "b" que é igual zero. Assim, a'¹' = g(z'¹'). Como nós usamos uma Função de Ativação linear, g(z'¹') é simplesmente igual a z'¹' Então este primeiro termo W'¹' x = a'¹'. Analogamente, perceba que W'²' W'¹' x = a'²' pois g(z'²') é igual a g(W'²' a'¹') e substitui-se: a'¹' = z'¹' = W'¹' x. Então W'²' W'¹' x = a'²', este termo W'³' W'²' W'¹' x = a'³', e assim por diante até que o 
produto de todas essas matrizes seja ŷ. Agora, digamos que cada uma dessas matrizes de peso W'ˡ' seja apenas um pouco maior do que a matriz identidade. Seja 1,5 vezes a matriz identidade. Tecnicamente, a última matriz não é quadrada assim isso só vale para as outras matrizes de pesos. Dessa forma, ŷ será, ignorando a última matriz não quadrada, a matriz identidade vezes 1,5 elevado a (L-1) vezes x, pois nós consideramos que cada W fosse igual a 1,5 vezes a matriz identidade, resultando nesse produto. Assim ŷ é igual a 1,5 elevado a L-1 vezes a matriz "x". Se "L" fosse grande, para uma Rede Profunda, ŷ seria muito grande. Na verdade, ele cresce exponencialmente, isto é, 1,5 elevado ao número de camadas. Assim, com uma Rede Neural Profunda, o valor de ŷ irá explodir. Por outro lado, se substituirmos 1,5 por 0,5, um termo menor que 1, o resultado é 0,5 elevado a L. Esta matriz seria 0,5 elevado a L vezes x, ignorando W'ᴸ'. Cada uma dessas matrizes é menor que 1, então, supondo que x₁ e x₂ fossem um, suas ativações seriam um meio, um meio um quarto, um quarto, um oitavo, um oitavo, e assim por diante até 1/2 elevado a L. Os valores de ativação decairão exponencialmente em função do número de camadas L (profundidade) da rede. Portanto, em uma rede muito profunda, as ativações decaem exponencialmente. Então, a noção que você pode aprender aqui é que se os pesos W forem todos um pouco maiores que um, ou um pouco maiores que a matriz identidade, então, com uma rede muito profunda, as ativações podem explodir. Porém, se W for um pouco menor que a identidade, 0,9 vezes a matriz identidade, então, em uma rede muito profunda, os valores de ativação decairão exponencialmente. Mesmo que esta explicação tenha sido em termos de ativação aumentando ou diminuindo exponencialmente em função de L, há uma explicação análoga para mostrar que as derivadas (ou gradientes) calculadas no Gradiente Descendente também aumentação exponencialmente ou decairão exponencialmente em função da quantidade de camadas. Nas Redes Neurais mais atuais, tem-se L igual a 150. A Microsoft teve bons resultados com 152 camadas. Porém, com uma rede tão profunda, se as ativações ou gradientes aumentarem ou diminuírem exponencialmente em função de L, então esses valores poderiam se tornar muito grandes ou pequenos. Isso torna o treino mais difícil, especialmente se os gradientes forem exponencialmente menores que L, então o Gradiente Descendente tomará passos muito pequenos. Levará um bom tempo para que o Gradiente Descendente aprenda algo. Em resumo, você viu como Redes Profundas sofrem de problemas de desaparecimento ou explosão de gradientes. Por um bom tempo, esse problema foi uma grande obstáculo no treinamento de Redes Profundas. Ainda, há uma solução parcial que não resolve todo este problema mas que ajuda bastante, o que é a inicialização cuidadosa dos pesos. Para isso, vamos para o próximo vídeo.
[Tradução: Gabriel Adriano Melo | Revisão: Carlos Lage]