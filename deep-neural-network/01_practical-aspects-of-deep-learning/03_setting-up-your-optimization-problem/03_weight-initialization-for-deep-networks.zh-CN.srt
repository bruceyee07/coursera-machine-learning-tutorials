1
00:00:00,530 --> 00:00:04,590
在上一个视频中 你们看到了非常深的神经网络

2
00:00:04,590 --> 00:00:08,330
会出现梯度消失和梯度爆炸问题

3
00:00:08,330 --> 00:00:11,040
事实上 有一种针对此问题的部分解决方法

4
00:00:11,040 --> 00:00:13,455
虽然不能完全解决它 但帮助很大

5
00:00:13,455 --> 00:00:18,915
该方法就是更好 更细致地随机初始化你的神经网络

6
00:00:18,915 --> 00:00:23,220
为了理解这个方法 我们从初始化单个神经元开始

7
00:00:23,220 --> 00:00:27,505
然后再把它应用到深度网络中

8
00:00:27,505 --> 00:00:30,040
现在让我们从单个神经元的例子说起

9
00:00:30,040 --> 00:00:33,140
稍后再去考虑深度神经网络

10
00:00:33,140 --> 00:00:39,060
单个的神经元可能会输入4个特征 从x1到x4

11
00:00:39,060 --> 00:00:42,465
然后用a=g(z)激活 最后得到y

12
00:00:42,465 --> 00:00:46,830
稍后在更深的网络中你会发现 这些输入是在一些a[l]层的右边

13
00:00:46,830 --> 00:00:51,780
但现在我们先把它当成x

14
00:00:51,780 --> 00:00:59,070
所以z就等于w1*x1+w2*x2+···+wn*xn

15
00:00:59,070 --> 00:01:08,570
我们让b等于0 因此在这里忽略b项

16
00:01:08,570 --> 00:01:12,390
因此 为了不让z项太大或者太小

17
00:01:12,390 --> 00:01:16,960
你会注意到n项的数值越大

18
00:01:16,960 --> 00:01:22,070
你就会希望Wi的值越小 对吗?

19
00:01:22,070 --> 00:01:25,910
因为z是wi*xi的加和

20
00:01:25,910 --> 00:01:30,680
因此 如果加和了很多这样的项 就会希望每一项就尽可能地小

21
00:01:30,680 --> 00:01:41,045
一个合理的做法是 让变量wi等于1/n

22
00:01:41,045 --> 00:01:45,495
这里的n是指输入一个神经元的特征数

23
00:01:45,495 --> 00:01:51,441
在实践中 你可以使用np.random.randn函数

24
00:01:51,441 --> 00:01:58,045
去设置某一层的权重矩阵W

25
00:01:58,045 --> 00:02:02,205
还要把矩阵的形状参数输入到这里

26
00:02:02,205 --> 00:02:06,810
然后乘以 (1/输入每个神经元的特征数)的平方根

27
00:02:06,810 --> 00:02:12,890
然后乘以 (1/输入每个神经元的特征数)的平方根

28
00:02:12,890 --> 00:02:14,605
在这里就是n[l-1]

29
00:02:14,605 --> 00:02:20,700
因为这是输入每个神经元的输入单元的数量

30
00:02:20,700 --> 00:02:23,340
事实上如果你使用ReLU激活函数

31
00:02:23,340 --> 00:02:28,830
就不要在这里使用 1/n

32
00:02:28,830 --> 00:02:32,105
而是把2/n当做变量 会工作地好一些

33
00:02:32,105 --> 00:02:35,580
因此你会经常在初始化时看到这项

34
00:02:35,580 --> 00:02:42,425
特别是使用ReLU激活函数时 此时g(z)等于ReLU(z)

35
00:02:42,425 --> 00:02:45,030
当然这还取决于你对随机变量的熟悉程度

36
00:02:45,030 --> 00:02:46,590
事实上 使用高斯随机变量并乘以这一项的平方根

37
00:02:46,590 --> 00:02:50,835
事实上 使用高斯随机变量并乘以这一项的平方根

38
00:02:50,835 --> 00:02:54,330
就和 2/n 这一项是相等的

39
00:02:54,330 --> 00:02:59,485
我在这儿写n[l-1]而不是n的原因是

40
00:02:59,485 --> 00:03:02,610
在这个例子中 逻辑回归中有n个输入特征

41
00:03:02,610 --> 00:03:05,625
但是在更一般的例子中

42
00:03:05,625 --> 00:03:12,400
每层中的每个单元都有n[l-1]个输入

43
00:03:12,400 --> 00:03:19,305
所以 如果输入经过激活的特征平均值为0并且标准差为1

44
00:03:19,305 --> 00:03:22,760
这就能使z项也呈现相同的分布性质

45
00:03:22,760 --> 00:03:26,580
虽然这样不能完全解决问题

46
00:03:26,580 --> 00:03:30,630
但它降低了梯度消失和梯度爆炸问题的程度

47
00:03:30,630 --> 00:03:33,240
因为这种做法通过设置权重矩阵W

48
00:03:33,240 --> 00:03:36,510
使得W不会比1大很多

49
00:03:36,510 --> 00:03:42,900
也不会比1小很多 因此梯度不会过快地膨胀或者消失

50
00:03:42,900 --> 00:03:45,075
我现在说一些其他的变体

51
00:03:45,075 --> 00:03:47,640
这里我刚刚描述的是假设使用ReLU作为激活函数

52
00:03:47,640 --> 00:03:51,270
这里我刚刚描述的是假设使用ReLU作为激活函数

53
00:03:51,270 --> 00:03:53,750
另外还有一些变体

54
00:03:53,750 --> 00:03:57,600
如果你使用tanh激活函数

55
00:03:57,600 --> 00:04:02,060
那么在一篇论文中显示 与使用2做常数项不同的是

56
00:04:02,060 --> 00:04:06,870
使用1作为常数项更好

57
00:04:06,870 --> 00:04:12,615
因此用1而不是用2 然后计算它的平方根

58
00:04:12,615 --> 00:04:16,605
这个平方根的项就代表了下面这一项

59
00:04:16,605 --> 00:04:23,165
当使用tanh激活函数的时候 就应这样操作

60
00:04:23,165 --> 00:04:26,790
这就是Xavier初始化方法

61
00:04:26,790 --> 00:04:30,870
还有另外一个版本 是Yoshua Bengio和合作者发明的

62
00:04:30,870 --> 00:04:32,190
你可能在一些论文中会见到

63
00:04:32,190 --> 00:04:36,140
它是用这个公式 是有一些理论证明

64
00:04:36,140 --> 00:04:40,300
它是用这个公式 是有一些理论证明

65
00:04:40,300 --> 00:04:43,800
因此 我想说如果你用ReLU这种更为常见的激活函数

66
00:04:43,800 --> 00:04:46,605
因此 我想说如果你用ReLU这种更为常见的激活函数

67
00:04:46,605 --> 00:04:48,555
我会用这个公式

68
00:04:48,555 --> 00:04:53,610
如果你用tanh激活函数 就尝试这个版本 另外一些也会用到这个

69
00:04:53,610 --> 00:04:58,665
但是在实践中 我认为所有的这些公式只是给你一个出发点

70
00:04:58,665 --> 00:05:01,210
它让权重矩阵的初始化变量有一个默认值

71
00:05:01,210 --> 00:05:04,270
它让权重矩阵的初始化变量有一个默认值

72
00:05:04,270 --> 00:05:06,760
如果你希望有这些变量

73
00:05:06,760 --> 00:05:09,875
这些变量参数可以成为超参数的一部分

74
00:05:09,875 --> 00:05:13,530
你可以通过调优确定使用哪个版本

75
00:05:13,530 --> 00:05:16,930
因此你就有另外一个参数 与这一个公式相乘

76
00:05:16,930 --> 00:05:21,075
调优这个乘数作为超参数中的一部分

77
00:05:21,075 --> 00:05:26,105
有时候调优这个超参数会影响模型的规模

78
00:05:26,105 --> 00:05:29,670
这通常不是我要最先调优的超参数之一

79
00:05:29,670 --> 00:05:33,325
但是我经常在调优它的时候遇到一些问题

80
00:05:33,325 --> 00:05:37,680
它会有一些帮助

81
00:05:37,680 --> 00:05:42,870
但是相比于其他可以调优的参数 它的帮助通常会下降

82
00:05:42,870 --> 00:05:47,520
我希望这节课能给一些关于梯度消失或梯度爆炸问题的直观感受

83
00:05:47,520 --> 00:05:49,935
我希望这节课能给一些关于梯度消失或梯度爆炸问题的直观感受

84
00:05:49,935 --> 00:05:52,955
以及如果使用适合的方法去初始化权重

85
00:05:52,955 --> 00:05:55,620
希望这些能使你的权重不会过快地爆炸或者消失

86
00:05:55,620 --> 00:05:58,890
因此训练一个合适的深度网络时

87
00:05:58,890 --> 00:06:01,650
因此训练一个合适的深度网络时

88
00:06:01,650 --> 00:06:05,730
权重或者梯度不会爆炸或消失太严重

89
00:06:05,730 --> 00:06:08,580
但你训练深度网络时 这么做算是一个小技巧

90
00:06:08,580 --> 00:06:11,640
可以帮助你更快地训练神经网络