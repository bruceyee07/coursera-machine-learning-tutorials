当训练神经网络时我们会遇到一个问题 尤其是当训练层数非常多的神经网络时 这个问题就是梯度的消失和爆炸 它的意思是当你在训练一个深度神经网络的时候 损失函数的导数或者说斜率 有时会变得非常大 或者非常小甚至是呈指数级减小 这使训练变得很困难 在这个视频中你将看到 梯度的爆炸和消失是什么含义 以及如何谨慎的选择随机初始化的权重 来显著的减少这种问题的发生 假设你在训练一个层数很多的神经网络 为了在幻灯片上节省空间 我在每层上只画了两个隐藏神经元 但是实际情况也可能会更多 这个神经网络将会有参数w[1] w[2] w[3]等等 直到w[L] 为了简单起见 假设我们使用的激活函数是g(z)=z 一个线性的激活函数 我们忽略b 假设b[L]=0 那么在这种设定下 输出Y就等于w[L]*w[L-1]*w[L-2]...*w[3]*w[2]*w[1]*x 输出Y就等于w[L]*w[L-1]*w[L-2]...*w[3]*w[2]*w[1]*x 输出Y就等于w[L]*w[L-1]*w[L-2]...*w[3]*w[2]*w[1]*x 但是如果要进一步计算一下 w[1]*x=z[1] 因为b=0 所以z[1]=w[1]*x+b 但是因为b=0 所以z[1]=w[1]*x 但是因为a[1]=g(z[1]) 但是因为我们用的是线性激活函数 所以a[1]=z[1] 所以这一项w[1]*x就等于a[1] 同理可得w[2]*w[1]*x=a[2] 因为a[2]=g(z[2])=g(w[2]*a[1]) 因为a[2]=g(z[2])=g(w[2]*a[1]) 然后你可以吧z[1]=w[1]*x代入a[1] 所以我们得到w[2]*w[1]*x=a[2] w[3]*w[2]*w[1]*x就等于a[3] 以此类推 直到所有矩阵相乘得到y-hat注意不是y 现在我们假设每一个权重矩阵w[L] 都只是比单位矩阵稍微大一些 假设是[1.5 0 0 1.5] 理论上来讲 最后一项的维数与前面不一样 所以我们只假设是x前面这些矩阵 那么忽略掉最后维数不同的这项x y-hat就会等于 那么忽略掉最后维数不同的这项x×y-hat就会等于 [1.5 0 0 1.5]^(L-1) * x 因为我们假设每个权重矩阵都等于[1.5 0 0 1.5] 其实是1.5倍的单位矩阵 最终就会得到这个 所以y-hat其实就等于1.5^(L-1)*x 所以y-hat其实就等于1.5^(L-1)*x 如果对于很深的神经网络L会变得很大 Y-hat 会变得非常大 事实上 它会呈指数速度增大 它以1.5的层数次方的速度增大 所以如果你的神经网络层数很多 y的值就会爆炸 相反的，如果我们用0.5 来替换 0.5 小于1 这就变成了0.5的L次 这个矩阵变成0.5^L 减去1乘X 我们忽略WL 所以每个矩阵都小于1 我们假设X1 和X2 是1 1 激励变成1/2 1/2 1/4 1/4 1/8 1/8 会变成1/2的L次 激活函数的值会指数级下降 作为网络层数L的函数 所以在很深的网络中，激活函数就会指数级的减少。 所以我希望你从这儿能理解到的就是对于权重系数W, 如果他们只比1大一点点， 或只比单位矩阵大一点点， 那在一个非常深的网络，激活函数就会爆炸 另外如果W只比单位矩阵小一点点， 有可能是0.9, 0.9 而你有一个很深的网络， 激活函数就会指数级的减少。 虽然我这个论证是关于 激活函数随着L指数级的增加或减少， 同样的论证可以用来表明 计算机算出的倒数或梯度 也会指数级增加 或指数级减少 在一些现代的神经网络里，L等于150. 微软最近用一个152层的神经网络算出了很了不起的结果 但是在这么深的神经网络里， 如果你的激活函数或梯度作为L的函数指数级的增加或减少 这些值会变得非常大或非常小 这会让训练变得非常困难， 尤其是如果你的梯度比L要小指数级别， 梯度下降会很用很小很小步的走。 梯度下降会用很长的时间才能有任何学习， 作为总结，你到现在理解了深层的网络会受制于 梯度逐渐消失或爆炸的问题 事实上，这个问题很长时间以来 都是一个训练深层神经网络的巨大的壁垒 事实上 有一种针对此问题的部分解决方法
虽然不能完全解决问题 但它对于如何谨慎选择初始化权重方法 提供了不少帮助 我们将在下一堂课中介绍