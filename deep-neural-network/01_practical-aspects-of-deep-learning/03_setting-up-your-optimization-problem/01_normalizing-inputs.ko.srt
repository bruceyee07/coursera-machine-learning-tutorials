1
00:00:00,436 --> 00:00:03,390
신경망을 훈련시킬 때, 트레이닝의 속도를 

2
00:00:03,390 --> 00:00:06,060
높일 수 있는 것 중 한가지는 바로 입력 값을 normalize (표준화)시키는 겁니다.

3
00:00:06,060 --> 00:00:07,730
이게 무슨 말인지 한번 보도록 하겠습니다.

4
00:00:07,730 --> 00:00:10,240
두 가지의 입력기능을 가지고 있는 트레이닝 세트를 볼까요

5
00:00:10,240 --> 00:00:13,520
입력기능 x는 2차원인데요

6
00:00:13,520 --> 00:00:16,550
이 것은 여러분의 트레이닝 세트 산 점도 입니다.

7
00:00:16,550 --> 00:00:20,730
입력 값을 표준화 시키는 절차는 2가지 단계가 따르는데요,

8
00:00:20,730 --> 00:00:26,270
첫 번째는 평균값을 빼거나 0으로 만드는 단계입니다.

9
00:00:26,270 --> 00:00:34,140
mu를 시그마 1/m (x^i) 인 공식으로 설정합니다. 

10
00:00:34,140 --> 00:00:39,786
그러므로 이건 벡터라고 할 수 있는데요, X는 모든 트레이닝의 예시에 대하여 X-mu로 지정되는 것입니다. 

11
00:00:39,786 --> 00:00:44,571
그 뜻은 즉, 트레이닝 세트를 평균값이 0이 될 때까지
움직인다는 것입니다.

12
00:00:44,571 --> 00:00:49,530
2번째 순서는 변동 편차를 표준화 하는 단계입니다.

13
00:00:49,530 --> 00:00:54,640
X1 기능이 X2보다 훨씬 더 큰 편차를

14
00:00:54,640 --> 00:00:55,410
보이고 있는 것을 보실 수 있습니다.

15
00:00:55,410 --> 00:01:00,030
그렇기 때문에 여기서는 시그마를 (1/m) 곱하기

16
00:01:00,030 --> 00:01:04,580
Xi**2로 설정합니다.

17
00:01:04,580 --> 00:01:07,220
element y의 제곱개념이라고 생각하면 될 것 같습니다.

18
00:01:07,220 --> 00:01:13,040
그렇게 되면 시그마 제곱은 각각 특성을 가진 편차의 벡터가 됩니다.

19
00:01:13,040 --> 00:01:15,435
보시면 평균값을 이미 뺏기 때문에,

20
00:01:15,435 --> 00:01:19,600
Xi 제곱 값인 element y제곱은 단순히 편차가 되는 것입니다.

21
00:01:19,600 --> 00:01:24,580
각각의 개별 예시를 벡터 시그마의 제곱으로 나눕니다. 

22
00:01:24,580 --> 00:01:28,490
그림으로는 이렇게 나오게 되는 거죠.

23
00:01:28,490 --> 00:01:34,785
그렇게 되면 이제 X1, X2 편차가 모두 1이 되는 것입니다. 

24
00:01:35,975 --> 00:01:42,627
한가지 팁을 그리자면, 이것을 이용하여 본인의 트레이닝 데이터를 확대시키실 예정이면, 똑같은 mu 

25
00:01:42,627 --> 00:01:47,735
시그마 제곱을 쓰셔서 테스트를 세트를 표준화시킬 수 있도록 하십시오.

26
00:01:47,735 --> 00:01:51,015
구체적으로, 트레이닝 세트와 테스트 세트를

27
00:01:51,015 --> 00:01:52,865
다르게 표준화하지 않는 것이 좋습니다.

28
00:01:52,865 --> 00:01:57,520
그 값이 어떻게 나오더라도 이 2가지의 공식에 적용하여

29
00:01:57,520 --> 00:02:02,190
똑같은 방법으로 확대하는 방안을 모색하십시오.

30
00:02:02,190 --> 00:02:06,500
mu과 시그마 제곱의 값을 트레이닝 세트와 테스트 세트 각 개별로 추정하지 않도록 하시고요.

31
00:02:06,500 --> 00:02:10,167
그 이유는, 트레이닝 및 테스트 예시가 여러분의 데이터이고 

32
00:02:10,167 --> 00:02:13,831
이러한 데이터가 여러분의 트레이닝 데이터에서 산출된 똑같은 mu값과 

33
00:02:13,831 --> 00:02:16,752
시그마 제곱 값을 통해 같은 조건으로 변형 되야 하기 때문입니다.

34
00:02:16,752 --> 00:02:18,210
왜 이렇게 하는 것일까요?

35
00:02:18,210 --> 00:02:21,290
입력 값의 특성을 왜 표준화하려고 하는 것일까요?

36
00:02:21,290 --> 00:02:25,790
이전에 말씀 드렸던 것처럼 비용 함수는 오른쪽 위에 나와 있는 것과 같이 정의할 수 있습니다.

37
00:02:25,790 --> 00:02:31,030
표준화되지 않은 입력 특성을 이용하면, 

38
00:02:31,030 --> 00:02:35,860
비용함수가 이렇게 보일 가능성이 큽니다. 푹 퍼진 그릇처럼 생겼는데요, 

39
00:02:35,860 --> 00:02:41,500
최소값이 아마도 저기 정도로 보이는 쭉 늘어진 모양의 비용함수 입니다.

40
00:02:41,500 --> 00:02:46,890
그러나 그 특성이 매우 다른 값이라고 하면, 예를 들어,

41
00:02:46,890 --> 00:02:52,280
X1의 범위가 1에서 1000까지이고, X2의 범위가 0에서 1까지라고 하면,

42
00:02:52,280 --> 00:02:56,790
그 해당 비율 또는 

43
00:02:56,790 --> 00:03:02,020
w1과 w2 값의 범위가 매우 다른 값을 띄게 될 것입니다.

44
00:03:02,020 --> 00:03:06,771
그렇다면 아마 축들이 아마 w1과 w2가 되야 할 텐데요 일단 w와 b를 표시하겠습니다.

45
00:03:06,771 --> 00:03:11,270
그렇게 되면, 여러분의 비용함수가 이렇게 접시처럼 쭉 늘어나는 모양일 텐데요,

46
00:03:11,270 --> 00:03:14,440
이 함수의 윤곽선을 한번 나누어 보면,

47
00:03:14,440 --> 00:03:17,705
이렇게 크게 늘어진 함수를 보게 될 것입니다.

48
00:03:17,705 --> 00:03:19,500
반면에, 이 특성을 표준화시키면,

49
00:03:19,500 --> 00:03:24,570
비용함수는 평균적으로 더 대칭 성향을 띄우는 모양이 될 것입니다.

50
00:03:24,570 --> 00:03:28,728
좌측에 보이는 비용함수와 같이 기울기 강하를 이용하면,

51
00:03:28,728 --> 00:03:33,216
굉장히 작은 교육 비율을 이용해야 될 것입니다. 

52
00:03:33,216 --> 00:03:37,242
그 이유는, 기울기 강하가 더 많은 단계를 거쳐 최소값에 도달하기 

53
00:03:37,242 --> 00:03:40,800
전까지 계속 왔다 갔다 할 수 있기 때문입니다.

54
00:03:40,800 --> 00:03:45,466
반면에 여러분의 함수가 조금 더 구형의 윤곽선을 띄고 있다면 어디서 시작하더라도

55
00:03:45,466 --> 00:03:49,125
기울기 강하가 바로 최소 점에 도달할 수 있습니다.

56
00:03:49,125 --> 00:03:53,665
기울기 강하에서는 보다 큰 절차를 통해 

57
00:03:53,665 --> 00:03:56,345
왼쪽 사진처럼 왔다 갔다 시킬 필요 없이 진행할 수 있습니다.

58
00:03:56,345 --> 00:04:00,250
물론 실제로는 w가 고차원 벡터이기 때문에,

59
00:04:00,250 --> 00:04:04,530
정확한 직관적인 부분을 2D로 배열하게 되면 정확하게 전달하지 못하는 부분도 있다.

60
00:04:04,530 --> 00:04:08,220
그러나 대략적으로는 비용함수가 조금 더 구형의 모양을 띄고 

61
00:04:08,220 --> 00:04:12,510
여러분의 기능이 모두 유사한 규모라고 하면, 더 쉽게 최적화 시킬 수 있습니다. 

62
00:04:12,510 --> 00:04:15,600
0에서 1000으로나, 0에서 1로는 아니며,

63
00:04:15,600 --> 00:04:20,880
대부분 -1에서 1로, 또는 서로 비슷한 편차에서 끼리 움직입니다. 

64
00:04:20,880 --> 00:04:25,630
이렇게 하면 비용 함수 J를 더욱 쉽고 빠르게 최적화할 수 있습니다.

65
00:04:25,630 --> 00:04:30,450
어느 한가지의 특성이, 예를 들어, X1이라고 하는 0에서 1까지 범위를 갖는 특성이 있고, 

66
00:04:30,450 --> 00:04:35,530
X2라고 하는 -1에서 1까지 범위를 갖는 특성이 있고,
X3라고 하는 특성이 1에서 2까지 범위를 갖고 있다면, 

67
00:04:35,530 --> 00:04:38,730
실제로는 이 특성들은 서로 비슷한 범위에 있기 때문에 잘 작동할 것입니다.

68
00:04:38,730 --> 00:04:41,430
범위가 크게 다를 경우가 문제인데요,

69
00:04:41,430 --> 00:04:42,470
1에서 1000의 범위의 특성 한가지,

70
00:04:42,470 --> 00:04:46,720
나머지 하나가 0에서 1의 범위를 가지면 authorizaion 알고리즘이 문제가 생깁니다.

71
00:04:46,720 --> 00:04:50,664
그렇지만 이전 슬라이드에서 보았듯이 모든 것을 평균값 0으로 지정하고 편차가 1이 되도록 하면

72
00:04:50,664 --> 00:04:54,857
비슷한 크기의 특성은 

73
00:04:54,857 --> 00:04:58,290
학습 알고리즘이 빠른 속도로 작동될 수 있도록 해줍니다.

74
00:04:58,290 --> 00:05:01,600
그러므로, 입력 값의 특성이 각각 크기가 다른 곳에서 
왔다고 하면

75
00:05:01,600 --> 00:05:03,410
예를 들어, 어떤 특성은 0에서 1사이,

76
00:05:03,410 --> 00:05:08,130
다른 특성은 1에서 1,000사이에서 범주하고 있는 경우,
이러한 특성들을 표준화시키는 절차가 매우 중요해집니다.

77
00:05:08,130 --> 00:05:11,630
이런 특성들이 비슷한 크기에서 왔다고 하면, 
이 절차가 덜 중요할 수 있기는 합니다.

78
00:05:11,630 --> 00:05:15,235
그래도 이런 표준화 절차를 진행하는 것이

79
00:05:15,235 --> 00:05:19,170
나쁠 것은 없겠죠. 저는 확실하지 않은 경우엔, 이런 절차를 실행하도록 하겠습니다.

80
00:05:19,170 --> 00:05:21,970
여러분의 트레이닝 속도를 높이는 것이 확실하지 않은 경우에 말이죠. 

81
00:05:22,970 --> 00:05:26,020
입력 값 표준화 관련한 내용은 여기까지 다루도록 하겠습니다.

82
00:05:26,020 --> 00:05:29,840
다음으로는, 새로운 네트워크 트레이닝을 빠르게 진행하는 방법에 대해
이야기 해보도록 하겠습니다.