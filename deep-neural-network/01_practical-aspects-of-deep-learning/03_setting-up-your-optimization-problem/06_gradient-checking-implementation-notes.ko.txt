이전 비디오에서는 gradient checking에 대한 내용을 배웠는데요, 이번 비디오에서는, 이런 내용을 신경망 도입에서 쓸 수 있는 방법에 대한 노트를 공유하고 
실용적인 방법에 대해 이야기해보겠습니다. 첫번째로, 트레이닝에서는 grad check를 사용하지 마십시요. 
오로지 debug하실 때만 사용하십시오. 제가 무슨말이냐면, 
모든 i값에 대해서 d 쎄타 approx를 계산하는 경우, 
이것은 아주 느린 계산인데요, 기울기 강하를 도입하는 데는 
후 방향전파을 사용해서 d 쎄타를 계산하고, 후 방향전파을 이용해서 derivative를 계산합니다. 그리고 디버깅을 하는 경우에만 이것을 계산할 텐데요, 
d 쎄타와 근접한 값이 될 수 있도록 합니다. 이렇게 한 경우, grad check를 끄고 기울기 강하를 반복 할 때 마다, 
사용하지 않습니다. 너무 느리기 때문이죠. 둘째로, 알고리즘이 grad check 실패 경우,
요소들을 보십시오. 개인별 요소들을 보고
버그를 찾으십시오. 무슨 뜻이냐면, d theta approx가 
d theta와 매우 다르다면, 제는 개인적으로 i의 다른 값들을 보고 d theta approx와 d theta가 매우 다를 때의 값들을 
찾을 것입니다. 예를 들어, 쎄타의 값 또는 d 쎄타의 값이 매우 다른 경우가 어느 층의 dbl에 연계된다면, dw의 요소들은 꽤 가깝죠, 맞죠? 기억할 것은, 쎄타의 다른 요소들은, b와 w의 다른 요소들과 부합합니다. 이런 경우, 버그가 db를 계산하는 과정에서 발생할 수 있습니다. db는
파라미터 b에 대한 derivative입니다. 거꾸로도, 비슷하게, 만약에 매우 먼 값을 찾는 경우, d theta approx고 부터의 값이 d theta의 값에서 매우 먼 경우 말이죠. 모든 요소들을 dw나 특정 층의 dw에서 온 것을 발견할 경우 버그의 위치를 파악할 수 있을 수 있습니다. 이런 방법이 버그를 바로 찾게 항상 도와주는 것은 아닙니다. 가끔씩은 버그를 어디서 트래킹할 수 있는지 그 단서를 제시해줍니다. 다음으로, grad check를 하는 경우, 일반화를 이용한다면,
일반화 항을 기억하십시오. 만약 비용함수J의 쎄타가 1 나누기 m 곱하기 loss의 합 그리고 더하기 일반화 항이라면, 그리고 합 나누기 L 그리고 wl 제곱, 그러면 이것이 J의 정의입니다. d theta는 J의 기울기 고요 쎄타에 대해서요, 그리고 일반화 항도 있습니다. 잊지 말고 여기 항을 추가하십시오. 다름으로, grad check는 dropout에서 작동하지 않습니다. 반복할 때마다. dropout이 무작위로 숨겨진 유닛의 일부를 제거하기 때문입니다. dropout이 기울기 강하를 진행하는 쉽게 산출할 수 있는 비용함수 J는 따로 없습니다. dropout은 비용함수 J를 최적화 시켜준다고 할 수 있는데요, 비용함수J는 반복업무를 통해 제거될 수 있는 기하급수적으로 큰 값의 노드 일부를
더해서 정의됩니다. 그러므로 비용함수 J는 산출하기 매우 까다로운데요. 비용함수를 샘플링하는 것입니다. dropout에서 사용하는 무작위의 subset을 제거할 때마다 말이죠. dropout에서의 계산은 그러므로 grad check를 이용해서 체크하기 어렵습니다. 저는 개인적으로 dropout이 없이 grad check를 도입합니다. 원한다면, keep-prob과 dropout을 1.0으로 지정합니다. 그 이후로 dropout를 키고난 뒤, dropout의 도입이 
옳았기를 바랄 수 있게죠. 다른 방법이 몇가지 있는데요, 
드랍된 노드들의 패턴을 고쳐서, 그 패턴의 grad check이 옳은지 여부를 확인하는 방법이 있습니다. 저는 실제로 이런 방법을 사용하지는 않습니다. 제가 권장 드리는 것은, dropout을 끄고, 
dropout없이 알고리즘이 옳은지 grad check을 통해 확인한 뒤, dropout을 가시 켜는 겁니다. 마지막으로 조금 미묘한 부분인데요, 불가능하진 않지만, 자주 발생하지는 않습니다. w와 b의 값이 0과 근접한 경우, 기울기 강하 의 도입이 옳은 경우가 불가능하진 않습니다. random initialization에서 말이죠. 하지만 기울기 강하를 운영하면서,
w와 b의 값이 커지면서, w와 b의 값이 0인 경우에만
backprop 도입이 맞을 수도 있습니다. 그러나 w와 b의 값이 커지면 커질수록
정확도가 떨어지게 되는 것이죠. 그러므로 한가지 할 수 있는 것은 
저는 자주 이렇게 하지는 않습니다만, grad check를 random initialization에서 실행시키고 그 다음에 네트워크를 어느 정도 트레이닝 시킨 뒤, w와 b의 값이 0에서 조금 떨어지게 시간을 가질 수 있게 말이죠,
처음의 작은 무작위의 값에서 떨어질 수 있게 말이죠, 그 다음 몇번 트레이닝을 반복한 이후에, 
다시 grad check를 실행하는 것입니다. gradient checking에 대한 내용은 이것이 전부인데요. 이번주 마지막 부분까지 오신 것에 축하드립니다. 이번주에는 train, dev, 그리고 테스트세트를 세팅하는 방법을 배웠는데요, 편향과 편차를 분석하는 방법과
각각 편향이 큰 경우와 편차가 큰 경우 대응 방안과, 2개 모두 큰 경우의 방안을 배웠습니다. 일반화에 대한 다른 유형도 살펴보았는데요, 신경망의 L2 일반화 과 dropout에 대해 알아봤습니다. 즉, 신경망 트레이닝의 속도를 높이는 방법을 알아봤습니다. 마지막으로, gradient checking, 이번주에는 많이 보셨겠지만, 이번 주 내용을 프로그래밍 학습을 통해
많이 연습하실 수 있습니다. 행운을 빌겠습니다. 다음주 과정에서 뵙겠습니다.