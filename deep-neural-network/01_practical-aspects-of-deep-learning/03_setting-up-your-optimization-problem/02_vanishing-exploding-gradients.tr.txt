Sinir ağının eğitim problemlerinden biri, özellikle çok derin sinir ağlarının, verilerin yok olması ve eğimlerin patlamasıdır. Bunun anlamı, çok derin bir ağı eğitirken, türevleriniz ya da eğimleriniz bazen çok, çok büyük ya da çok, çok küçük, hatta belki üstel küçük olabilir. Bu da eğitimi zorlaştırır. Bu videoda, patlayan ve yok olan eğim probleminin gerçekte ne olduğunu anlayacaksınız, ayrıca, bu problemi önemli ölçüde azaltmak için rastgele ağırlık ilklendirmenin seçimlerini nasıl yapacağınızı göreceksiniz. Derin öğrenme ağlarını bu şekilde eğitmezseniz, slaytta yer kazanmak adına, her katmanda sadece 2 gizli birim varmış gibi çizdim, ama daha fazla olabilirdi. Ama bu sinir ağının W(1), W(2), W(3) ve W(L)'ye kadar devam eden parametreleri var. Sadelik adına, Z'nin g(Z) aktivasyon fonksiyonunu kullandığımızı varsayalım, bu yüzden doğrusal aktivasyon fonksiyonu. Ve B'yi görmezden gelelim, B(L) sıfıra eşittir diyelim. Bu durumda, output olan Y'yi şöyle gösterebilirsiniz: W(L) çarpı W(L eksi bir) çarpı W(L eksi iki), W(3)'e kadar nokta, nokta, nokta, W(2) çarpı W(1) çarpı X. Matematiğimi kontrol etmek isterseniz, W(1) çarpı X, Z(1) olacaktır, Çünkü, B sıfıra eşittir. Bu sebeple, Z(1) de sanırım, W(1) çarpı X ve sonra artı B (sıfır)'ye eşit olacaktır. Fakat öte yandan, A(1), g(Z(1))'e eşit. doğrusal aktivasyonunu kullandığımız için, bu sadece Z(1)'e eşit. Yani ilk terimimiz; W(1) çarpı X, A(1)'e eşittir. Ve sonra mantıksal olarak, W(2) çarpı W(1) çarpı X'in A(2)'ye eşit olduğunu anlayabilirsiniz, çünkü bu g(Z(2)) olacak, g(W(2) çarpı A(1) olacak ki bunu da buraya ekleyebilirsiniz. Yani bu A(2)'ye eşit olacak, ve sonra bu da A(3)'e eşit olacak ve bu şekilde devam edecektir, ta ki bu matrislerin ürünü şapkalı Y'yi verene kadar, Y'yi değil. Şimdi, her bir ağırlık matrisi W(L)'nin, özdeşlikten birden biraz daha büyük olduğunu söyleyelim. Yani, 1.5_1.5_0_0. Teknik olarak, sonuncusu farklı boyutlara sahiptir, bu yüzden bu sadece bu ağırlık matrislerinin geri kalanıdır. Sonra, şapkalı Y, bu sonuncusunu farklı bir boyutla göz ardı ederek, bu 1.5_0_0_1.5 matrisin (L eksi 1) kuvveti çarpı X olacaktır. çünkü bu matrislerin her birinin buna eşit olduğunu varsayıyoruz. Bu gerçekten özdeşlik matrisinin 1.5 katı, sonra bu hesaplama ile sonuçlandırırsınız. Ve şapkalı Y temelde 1.5'in (L) kuvvetine, (L-1) kuvveti çarpı X olacaktır, ve eğer L çok derin sinir ağları için büyükse, şapkalı Y çok büyük olacaktır. Aslında, sadece üssel olarak büyür, 1.5'in katman sayısı kuvvetine göre büyür. Eğer çok derin bir sinir ağınız varsa, Y'nin değeri patlayacaktır. Tersine, eğer bunu 0.5 ile değiştirirsek, yani 1'den az bir şey ile, sonra bu, 0,5'in L kuvvetine eşit olur. Bu matris, yine W(L)'yi göz ardı ederek, 0,5'in (L eksi bir) kuvveti çarpı X'e eşit olur. Ve böylece matrislerin her biri 1'den az, sonra X(1)'ye bir, X(2)'ye bir diyelim, sonra aktivasyonlar bir bölü 2, bir bölü 2, bir bölü dört, bir bölü dört, bir bölü sekiz, bir bölü sekiz, olarak devam edecek, ta ki 1 bölü 2'nin (L) kuvveti olana kadar. Bu yüzden, aktivasyon değerleri def'in bir fonksiyonu olarak değişimine göre, ağdaki L katman sayısındaki fonksiyonu olarak değişimine göre katlanarak azalacaktır. Yani çok derin bir ağda, aktivasyonlar katlanarak azalmaktadır. Yani sezgilerden kurtulabileceğinizi umarak, W (L), eğer hepsi sadece birden biraz daha büyükse, ya da özdeşlik matrisinden biraz daha büyükse, daha sonra çok derin bir ağ ile aktivasyonlar patlayabilir. Ve eğer W(L), özdeşlik matrisinden birazcık daha azsa, yani bu belki burada 0.9, 0.9, o zaman çok derin bir ağınız vardır, aktivasyonlar katlanarak azalacaktır. Ve bu argümanı, L'nin bir fonksiyonu olarak katlanarak artan veya azalan aktivasyonlar açısından incelememe rağmen, benzer bir argüman, şunu göstermek için kullanılabilir: bilgisayarın göndereceği türevlerin veya eğimlerin de katlanarak arttığını veya katman sayısı sayısının bir fonksiyonu olarak üssel olarak azalacağını göstermek için kullanılabilir. Bazı modern sinir ağları ile, L 150'ye eşittir. Microsoft yakın zamanda 152 katmanlı sinir ağı ile mükemmel sonuçlar elde etti. Fakat böyle derin bir sinir ağıyla, eğer aktivasyonlarınız veya eğimleriniz L'nin bir fonksiyonu olarak katlanarak artar veya azalırsa, o zaman bu değerler gerçekten büyük ya da çok küçük olabilir. Ve bu eğitimi zorlaştırır, özellikle eğer eğimleriniz katlanarak L'den küçükse, Dereceli azalma küçük adımlar halinde olacaktır. Dereceli Azalmanın bir şey öğrenmesi uzun zaman alacaktır. Özetlemek gerekirse, eğimlerin patlaması ya da yok olması problemlerinden derin ağların ne kadar muzdarip olduğunu gördünüz. Aslında, uzun bir süre boyunca bu problem derin sinir ağlarını eğitmek için büyük bir engeldi. Bu sorunu tam olarak çözmeyen kısmi bir çözüm ortaya çıkıyor, ancak ağırlıkları nasıl başlattığınıza dair dikkatli bir seçim yapmanıza yardımcı oluyor. Bunu görmek için bir sonraki videoya gidelim.