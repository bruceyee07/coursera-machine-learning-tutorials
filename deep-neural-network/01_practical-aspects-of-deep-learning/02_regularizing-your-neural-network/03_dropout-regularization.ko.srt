1
00:00:00,000 --> 00:00:02,560
L2 일반화에 추가로,  

2
00:00:02,560 --> 00:00:06,875
더욱 강력한 일반화 테크닉인 droupout이라는 것이 있습니다.

3
00:00:06,875 --> 00:00:08,715
어떻게 작동하는 것인지 알아보겠습니다.

4
00:00:08,715 --> 00:00:12,600
왼쪽과 같은 신경망을 트레이닝 하는데 overfitting 하는 경우입니다.

5
00:00:12,600 --> 00:00:14,998
이런 경우 dropout으로 하는 것인 이와 같습니다.

6
00:00:14,998 --> 00:00:16,765
여기에다가 신경망을 복사해보겠습니다.

7
00:00:16,765 --> 00:00:21,100
dropout에서는 무엇을 할 것이냐면, 이 네트워크의 각각 층별로 

8
00:00:21,100 --> 00:00:26,350
살펴보면서 신경망의 노드를 제거하는 확률을 세팅해볼 것입니다. 

9
00:00:26,350 --> 00:00:29,305
여기 각각의 층 별로,

10
00:00:29,305 --> 00:00:30,955
각각의 노드별로, 

11
00:00:30,955 --> 00:00:34,165
동전을 던져서 노드를 유지할 것인지

12
00:00:34,165 --> 00:00:38,005
제거할 것인지 각각 0.5 확률이 있다고 할 것입니다.

13
00:00:38,005 --> 00:00:39,820
동전을 던진 후에, 

14
00:00:39,820 --> 00:00:42,865
여기 노드들을 제거하겠다고 할 수도 있습니다.

15
00:00:42,865 --> 00:00:49,775
그러면, 여기서 하는 것은 들어오고 나가는 링크 또한 제거를 같이 할 것입니다.

16
00:00:49,775 --> 00:00:51,550
그렇게 되면 훨씬 더 작은, 

17
00:00:51,550 --> 00:00:53,150
감소된 네트워크가 남을 것입니다.

18
00:00:53,150 --> 00:00:56,145
그 다음에 후 방향전파 트레이닝을 진행합니다.

19
00:00:56,145 --> 00:00:59,705
여기처럼 감소된 네트워크에 예제가 있고, 

20
00:00:59,705 --> 00:01:01,130
또 다른 예제가 있는데요, 

21
00:01:01,130 --> 00:01:03,700
이 경우에는 동전을 몇개 던진 이후에

22
00:01:03,700 --> 00:01:07,585
노드의 뭉치를 유지하거나 제거하는 방법이 있습니다. 

23
00:01:07,585 --> 00:01:09,235
각각의 트레이닝 예시에 대해서, 

24
00:01:09,235 --> 00:01:14,455
여기 있는 방법 중에서 신경만 트레이닝을 시킬 것입니다.

25
00:01:14,455 --> 00:01:16,675
아마도 조금 미친 테크닉이라고 생각할 수도 있는데요, 

26
00:01:16,675 --> 00:01:20,470
이것을 코딩하는 방법이 무작위로 이루어지지만, 

27
00:01:20,470 --> 00:01:22,505
실제로 작동을 합니다.

28
00:01:22,505 --> 00:01:28,480
각각의 예시에 대해서 훨씬 더 작은 네트워크를 트레이닝 시키기 때문에,

29
00:01:28,480 --> 00:01:34,591
또는 네트워크를 일반화시킬 수 있는지에  대해 감을 잡을 수 있을 수도 있습니다,

30
00:01:34,591 --> 00:01:38,590
이런 훨씬 더 작은 네트워크가 트레이닝되고 있기 때문이죠.

31
00:01:38,590 --> 00:01:41,535
dropout을 어떻게 도입하는지 알아보겠습니다.

32
00:01:41,535 --> 00:01:43,425
dropout을 도입하는 방법은 몇가지가 있습니다.

33
00:01:43,425 --> 00:01:44,915
가장 흔한 방법을 보여드리겠습니다.

34
00:01:44,915 --> 00:01:47,865
바로 inverted dropout이라고 하는 기술입니다.

35
00:01:47,865 --> 00:01:49,645
완성도를 위해서, 

36
00:01:49,645 --> 00:01:58,977
이것을 l=3이라는 층으로 묘사한다고 해봅시다.

37
00:01:58,977 --> 00:02:03,000
코딩할때는 이것을 몇개의 "3"들로 적을텐데요, 

38
00:02:03,000 --> 00:02:08,380
싱글 레이어에서 dropout을 묘사하는 방법을 보여드리고 있습니다.

39
00:02:08,380 --> 00:02:12,155
여기서 할 것은, vector d를 설정해서, 

40
00:02:12,155 --> 00:02:16,503
d^3는 3이라는 층에대한 dropout vector일 것인데요, 

41
00:02:16,503 --> 00:02:21,585
d 3이라는 것은 np.random.rand(a) 입니다.

42
00:02:21,585 --> 00:02:27,708
이것은 a3와 같은 모양일 것입니다.

43
00:02:27,708 --> 00:02:31,261
저는 이것이 어떤 숫자보다 작은지 확인할 것입니다.

44
00:02:31,261 --> 00:02:34,470
그걸 keep.prob이라 부를 텐데요. 

45
00:02:34,470 --> 00:02:37,350
그러면 keep.prob은 숫자가 됩니다.

46
00:02:37,350 --> 00:02:39,105
이전에는 0.5였는데요, 

47
00:02:39,105 --> 00:02:42,045
이번 예제에서는 0.8을 이용하겠습니다.

48
00:02:42,045 --> 00:02:47,040
또한, 특정 숨겨진 유닛이 유지될 확률이 있을 것입니다. 

49
00:02:47,040 --> 00:02:49,129
그러므로 keep.prob 은 0.8입니다.

50
00:02:49,129 --> 00:02:54,665
이러면 0.2는 숨겨진 유닛을 제거할 확률입니다.

51
00:02:54,665 --> 00:02:58,130
이것이 하는 것은 random matrix를 생성합니다.

52
00:02:58,130 --> 00:03:00,755
만약 인수분해를 안 한 경우 헤는 이것도 됩니다. 

53
00:03:00,755 --> 00:03:03,180
d3는 그러면 매트릭스일 것입니다.

54
00:03:03,180 --> 00:03:06,660
각각의 예시이나 숨겨진 유닛에 대해서 d3가 1이 

55
00:03:06,660 --> 00:03:10,245
될 확률이 0.8 해당하는 경우을 가르킵니다.

56
00:03:10,245 --> 00:03:12,815
또, 0이 될 확률이 20퍼센트인 경우를 가르키죠.

57
00:03:12,815 --> 00:03:20,900
0.8보다 작은 랜덤 숫자가 0.8의 확률로 1이거나 참일 확률입니다. 

58
00:03:20,900 --> 00:03:24,675
20퍼센트 또는 0.2의 확률로 거짓일 것입니다. 0이 되는 경우를 말하죠.

59
00:03:24,675 --> 00:03:27,569
그 다음으로 할 일은, 세번쩨 층에서 activation을 가집니다.

60
00:03:27,569 --> 00:03:30,945
밑에 예제에서 a3라고 부르겠습니다.

61
00:03:30,945 --> 00:03:33,265
그러면 a3는 산출하는 activation이 있습니다.

62
00:03:33,265 --> 00:03:37,335
그리고 a3를 이전의 a3 곱하기 

63
00:03:37,335 --> 00:03:41,849
이거는 element wise multiplication인데요,

64
00:03:41,849 --> 00:03:44,857
이것은 또한 a3 곱하기는 d3와 동일하다고 적을 수 있습니다.

65
00:03:44,857 --> 00:03:50,625
이것이 하는 것은 d3의 모든 요소가 0인 경우,

66
00:03:50,625 --> 00:03:53,735
이런 경우 즉, 각각의 element가 0인 경우는 20퍼센트 확률이었는데요,

67
00:03:53,735 --> 00:03:57,840
여기 이 multiply operation은 

68
00:03:57,840 --> 00:04:00,980
d3의 요소들을 0으로 만드는 역할을 합니다.

69
00:04:00,980 --> 00:04:02,250
이것을 파이썬에서 하면,

70
00:04:02,250 --> 00:04:05,880
엄밀히 이야기하면 d3가 boolean array 가 됩니다. 그 값이 참, 거짓은 경우를 얘기하죠,

71
00:04:05,880 --> 00:04:06,985
0과 1의 값 대신에요.

72
00:04:06,985 --> 00:04:10,057
그러나 multiply operation은 잘 작동하기 때문에

73
00:04:10,057 --> 00:04:13,390
참, 거짓의 값을 0과 1로 이해하도록 하겠습니다.

74
00:04:13,390 --> 00:04:16,260
이 과정을 직접 파이썬에서 시도해보시면 여러분이 확인할 수 있습니다.

75
00:04:16,260 --> 00:04:22,570
마지막으로, 우리는 a3를 가지고 확장을 할 것입니다.

76
00:04:22,570 --> 00:04:30,015
마지막으로, 우리는 a3를 가지고 확장을 할 것입니다.

77
00:04:30,015 --> 00:04:32,560
이 마지막 역할이 하는 것을 설명하겠습니다.

78
00:04:32,560 --> 00:04:36,040
토론의 목표를 위해서, 50 단위 또는

79
00:04:36,040 --> 00:04:39,930
50개의 신경세포가 세번째 층에 있다고 가정해보겠습니다.

80
00:04:39,930 --> 00:04:43,075
그러면 a3가 50 x 1차원 또는

81
00:04:43,075 --> 00:04:46,650
인수분해를 하는 경우, 50 x m 차원일 수 있겠습니다.

82
00:04:46,650 --> 00:04:51,625
그럼 80퍼센트의 확률로 유지하거나, 20퍼센트 확률로 제거하는 경우의 수가 있다고 해봅시다.

83
00:04:51,625 --> 00:04:53,050
이 뜻은, 평균적으로

84
00:04:53,050 --> 00:04:59,025
10 유닛은 닫거나, 10유닛이 0으로 된다는 뜻과 같습니다.

85
00:04:59,025 --> 00:05:02,020
그러면 이제 z^4의 값을 보면, 

86
00:05:02,020 --> 00:05:08,775
z^4 의 값은 w^4 * a^3 + b^4와 같을 것입니다.

87
00:05:08,775 --> 00:05:10,570
그러면, 예상되는 것은,

88
00:05:10,570 --> 00:05:14,080
이것이 20퍼센까지 줄어들을 것입니다.

89
00:05:14,080 --> 00:05:18,480
무슨 뜻이냐 면, 20퍼센트의 a3의 element가 0이 될 것이라는 겁니다. 

90
00:05:18,480 --> 00:05:22,240
그러면, z^4의 기대 값을 감소하지 않게 하기 위해서는

91
00:05:22,240 --> 00:05:24,380
이것을 가져야 합니다.

92
00:05:24,380 --> 00:05:28,870
그리고 0.8로 나눕니다. 

93
00:05:28,870 --> 00:05:33,635
왜냐면, 이게 조정되거나 다시 20퍼센트만큼 올라갈 수 있기 때문입니다.

94
00:05:33,635 --> 00:05:37,455
그러면 a3의 기대 값이 변하지 않았습니다.

95
00:05:37,455 --> 00:05:43,435
여기 이 라인은 inverted dropout technique이라고 하는 것인데요, 

96
00:05:43,435 --> 00:05:44,830
이것의 효과는, 

97
00:05:44,830 --> 00:05:47,230
keep.prob을 어떻게 설정하더라도, 

98
00:05:47,230 --> 00:05:50,446
0.8이던 0.9이던, 심지어 1이더라도

99
00:05:50,446 --> 00:05:52,135
만약 1로 설정이 되면 dropout은 없습니다.

100
00:05:52,135 --> 00:05:54,565
0.5 이던 어떤 값이라도 모두 유지하기 때문이죠. 

101
00:05:54,565 --> 00:05:57,980
inverted dropout technique은 keep.prob으로 나누면서

102
00:05:57,980 --> 00:06:02,730
a3의 기대값이 동일하게 유지하도록 해줍니다.

103
00:06:02,730 --> 00:06:05,005
그리고 테스트를 하는 경우,

104
00:06:05,005 --> 00:06:06,820
신경망을 평가할 때에,

105
00:06:06,820 --> 00:06:08,300
다음 슬라이드에서 이야기하겠지만,

106
00:06:08,300 --> 00:06:10,065
여기 이 inverted dropout technique은

107
00:06:10,065 --> 00:06:13,160
제가 여기 이렇게 초록색 박스를 그린 이 부분이 바로 해당하는 부분인데요,

108
00:06:13,160 --> 00:06:17,540
이것이 test time을 쉽게 만들어줍니다. Scaling 문제가 덜하기 때문이죠. 

109
00:06:17,540 --> 00:06:20,110
제가 아는 선에서는, 가장 흔한 dropout의

110
00:06:20,110 --> 00:06:22,870
도입방식은 바로 inverted dopout 방식입니다.

111
00:06:22,870 --> 00:06:24,490
저는 여러분께 이것을 도입하길 권장드립니다.

112
00:06:24,490 --> 00:06:27,280
하지만 이전에 dropout에서 반복 테스트 업무를 하던 때에, 

113
00:06:27,280 --> 00:06:30,165
여기 이 keep.prob 라인 부분을 놓친 적이 있었습니다.

114
00:06:30,165 --> 00:06:33,660
그렇게 해서 test time에서 평균이 더욱 더 점차 복잡해지는 것이죠.

115
00:06:33,660 --> 00:06:37,040
하지만 다른 버전들을 이제는 잘 쓰지 않습니다. 

116
00:06:37,040 --> 00:06:40,125
그러면 여러분은 d vector를 사용해서

117
00:06:40,125 --> 00:06:43,390
여러분도 느끼시겠지만 트레이닝 예시의 유형에 따라

118
00:06:43,390 --> 00:06:46,090
서로 다른 숨겨진 유닛을 0으로 만듭니다.

119
00:06:46,090 --> 00:06:49,975
동일한 트레이닝 세트에서 mulltiple pass를 진행하면, 

120
00:06:49,975 --> 00:06:52,566
트레이닝 세트에 통과하는 다른 pass들에 따라서,

121
00:06:52,566 --> 00:06:55,290
다른 숨겨진 유닛을 무작위로 0으로 만들 것입니다.

122
00:06:55,290 --> 00:06:57,270
꼭 동일한 숨겨진 유닛을 0으로 만들고

123
00:06:57,270 --> 00:07:01,155
한 기울기 강하 수행절차에서 말이죠,

124
00:07:01,155 --> 00:07:03,258
그리고 또 다른 

125
00:07:03,258 --> 00:07:05,510
2번째의 기울기 강하에서 

126
00:07:05,510 --> 00:07:07,375
트레이닝 세트를 2번째로

127
00:07:07,375 --> 00:07:09,595
통과시켜서 또 다른 패턴이

128
00:07:09,595 --> 00:07:13,008
숨겨진 유닛을 0으로 만들어야 하는 것이 아닙니다.

129
00:07:13,008 --> 00:07:16,023
3번째 층의 vector d 또는 d3는

130
00:07:16,023 --> 00:07:18,395
어떤 것을 0으로 만들지 결정하는데 사용됩니다.

131
00:07:18,395 --> 00:07:21,565
전 방향전파과 back prop 2개 모두에서 말이죠.

132
00:07:21,565 --> 00:07:22,980
여기서는 전 방향전파만 보여주고 있습니다.

133
00:07:22,980 --> 00:07:26,950
test time에서 알고리즘을 트레이닝 했으니, 이제 이렇게 합니다.

134
00:07:26,950 --> 00:07:30,535
test time에서는 x라는 것이 주어지고, 또는 예측하고 싶어하는 상대가 주어집니다.

135
00:07:30,535 --> 00:07:32,335
그리하여 표준 노테이션을 사용해서

136
00:07:32,335 --> 00:07:33,764
여기서 a^0를 사용하겠습니다.

137
00:07:33,764 --> 00:07:38,180
0개 층의 activation을 나타내는데요, 테스트 예시 x를 나타냅니다.

138
00:07:38,180 --> 00:07:40,760
저희가 할 것은 테스트 타임에

139
00:07:40,760 --> 00:07:44,340
dropout을 사용하지 않을 것입니다.

140
00:07:44,340 --> 00:07:48,314
특히, Z^1= w^1.a^0 + b^1을 말이죠. 

141
00:07:48,314 --> 00:07:56,627
a1원  g^1(z^1 Z)이구요.

142
00:07:56,627 --> 00:08:03,745
z2는 w^2.a^1 + b^2 입니다.

143
00:08:03,745 --> 00:08:04,895
a2 는 등등 말이죠.

144
00:08:04,895 --> 00:08:10,060
마지막 층 예측수치 ŷ까지 내려갑니다. 

145
00:08:10,060 --> 00:08:12,640
확인 하실 수 있는 것인, dropout을 여기서 

146
00:08:12,640 --> 00:08:15,690
명백히 사용하지 않고, 동전을 무작위로 던지지 않고, 

147
00:08:15,690 --> 00:08:20,285
어떤 숨겨진 유닛을 제거할지 동전을 던지지 않는다는 것입니다.

148
00:08:20,285 --> 00:08:22,510
그 이유는 test time에서 예측을 하는 경우, 

149
00:08:22,510 --> 00:08:25,615
여러분의 결과값이 임의의 숫자가 되면 좋지 않기 때문입니다.

150
00:08:25,615 --> 00:08:27,699
만약 여러분이 dropout을 테스트타임에 도입한다 하면, 

151
00:08:27,699 --> 00:08:29,890
그것은 예측수치에 noise를 더할 뿐입니다.

152
00:08:29,890 --> 00:08:34,105
이론적으로, 여러분이 할 수 있는 것은 예측을 몇 번이고 진행하는 것입니다.

153
00:08:34,105 --> 00:08:38,940
임의로 dropout된 숨겨진 유닛을 가지고 말이죠. 

154
00:08:38,940 --> 00:08:43,625
하지만 산출하는 이런 과정이 효율적이지 않고, 어렴풋이 비슷한 결과값을 줄 것입니다.

155
00:08:43,625 --> 00:08:46,880
진행절차는 다르지만 결과는 아주 아주 비슷할 것입니다.

156
00:08:46,880 --> 00:08:47,980
언급하자면,

157
00:08:47,980 --> 00:08:49,385
inverted dropout 은,

158
00:08:49,385 --> 00:08:53,455
이전 라인에서 진행한 것을 기억하시겠지만, 저희는 keep.prob으로 나눴었습니다.

159
00:08:53,455 --> 00:08:56,450
이 효과는 여러분이 테스트타임에 scaling에서 

160
00:08:56,450 --> 00:08:59,664
dropout을 도입하지 않더라도

161
00:08:59,664 --> 00:09:02,050
여기 이런 activation의 기대값은 변하지 않습니다.

162
00:09:02,050 --> 00:09:06,540
그러므로 여러분은 이렇게 쌩뚱맞은 부가적인 scaling 매개 변수를 넣을 필요가 없습니다.

163
00:09:06,540 --> 00:09:08,965
이것은 여러분이 training time에서 있었던 것과는 다릅니다.

164
00:09:08,965 --> 00:09:10,240
이게 dropout에 관한 내용인데요, 

165
00:09:10,240 --> 00:09:13,000
이번주 학습에 이 내용을 적용하면, 

166
00:09:13,000 --> 00:09:16,660
처음으로 경험을 얻을 수 있을 것입니다.

167
00:09:16,660 --> 00:09:18,440
하지만 이것이 왜 어떻게 해서 작동하는 것일까요?

168
00:09:18,440 --> 00:09:20,410
다음 비디오에서는 

169
00:09:20,410 --> 00:09:23,630
dropout이 정확히 무엇을 하는지 조금 더 직관적인 부분을 알아보겠습니다.

170
00:09:23,630 --> 00:09:25,160
다음 비디오로 넘어가겠습니다.