만약 신경망이 데이터에 과도하게 적합하다는 의심이 드는 경우, 그것은 큰 격차 문제가 있다는 것 입니다. 당신이 가장 먼저 시도해야 할 일 중 하나는 아마 일반화(regularization)일 것 입니다. 큰 격차 문제를 해결하기 위한 다른 방법은, 꽤 신뢰 가는 교육 자료를 더 많이 획득하는 것 입니다. 그러나 항상 많은 교육 자료를 획득할 수는 없습니다. 혹은, 더 많은 데이터를 얻는 데에는 비용이 더 많이 들 수 있습니다. 그러나 일반화를 추가하는 것은 과도한 장착을 예방하는 데에 자주 도움이 될 것 입니다. 혹은, 당신의 네트워크 오류를 감소시키는 데에도 도움이 될 것 입니다. 자, 그럼 일반화가 어떻게 이루어지는지 한번 살펴볼까요? 로지스틱 회귀를 사용하여 이 아이디어를 발전시켜 봅시다. 로지스틱 회귀를 위해 다시 불러옵시다. 비용함수 J를 최소화 하도록 시도합니다. 그것은 이 비용함수로 정의되어 있습니다. 교육용 예 중 일부는 다른 예에서 개별 예상치의 손실로, 당신은 이를 로지스틱 회귀에서 w와 b로 다시 불러오고, 이들은 매개 변수들입니다. 그리하여, w는 x치수 파라미터 벡터이고, b는 실수입니다. 따라서, logistic regression에 일반화를 더하기 위해 당신이 해야할 일은 그것을 Lambda라고 불리는 조절 매개 변수에 추가하는 것 입니다. 그것에 대해서는 곧 더 말씀 드리겠습니다. Lambda 나누기 2m 곱하기 w의 노름 곱하기 2 제곱 그리하여 여기 w의 노름 곱하기 2 제곱 은 ∑j=1 to nx to wj제곱 또한 W transpose W 와같습니다. W transpose W는 프라임-벡터 W의 유클리드 표준이다. 그리고 이것은 L2 일반화 라고 불립니다. 그 이유은 여기, 당신이 유클리드 정상치를 사용하거나, L2표준을 벡터 w와 사용하는 것이죠. 그럼, 당신은 왜 매개 변수 w만 일반화 하져? 왜 우리는 여기 b에 무언가를 더 더하지 않는거져? 실제로 이렇게 할 수 있지만, 저는 이 방법은 그냥 생략 하겠습니다. 그 이유는 변수를 보면 w는 보통 꽤 높은 치수의 측정 벡터입니다. 특히, 변동이 심한 문제가 있을 시에는
더욱 그렇습니다. 어쩌면 w가 그저 변수가 많아서 그럴수도 있지만, 그렇다고 모든 파라미터들을 그냥 넣진 않습니다. 반면에, 
b는 그저 단수일 뿐입니다. 그래서 거의 모든 매개 변수들이 b보다는 w에 있습니다. 그리고 만약 당신이 이 마지막 용어를 추가한다면, 큰 차이는 없겠지만, 왜냐하면 b는 많은 파라미터들 중에 단지 하나의 파라미터일 뿐이기
때문입니다. 실제로 저는 주로 그것을 포함 시키는 데에는 신경을 쓰지 않아요. 그러나 당신이 포함 시키고 싶다면 그렇게 하셔도 됩니다. 그래서 L2 일반화가 가장 일반적인 유형의
일반화 입니다. 당신은 다른 사람들이 L1일반화에 대해 얘기하는 
것을 들어본 적이 있을거에요. 그리고 그것은 이 L2 표준 대신에 더하였을 때 입니다. 대신에 이런 요서를 씁니다. 그리고 이것은 벡터 w의 파라미터의 L1 표준이라고 불립니다. 그러니까 저기 아랫쪽의 subscript 1 인거 보이시져? 그리고 제 생각에 당신이 분모에 m을 넣었던지 아니면, 
2m을 넣었는지는 상관없이 단지 스케일링 상수일 뿐입니다. L1 일반화를 사용한다면, w는 결국 
희박하게 될 것 입니다. 그것은 w 벡터 안에 수많은 0이 있다는 것을 의미한다. 그리고 어떤 사람들은 이것이 모델을 압축하는 데에 도움이 된다고 
말한다. 그 이유는, 파라미터 세트는 0이고, 모델을 저장할 메모리가 덜 필요하기 때문입니다. 하지만 실제로는 L1이 모델을 sparse상태로 만드는
일반화입니다. 그것은 조금밖에 도움이 되지 않습니다. 그래서 제 생각에는 이것이 최소한 당신의 모델을 압축할 만큼 그렇게 많이 쓰이는 것 같지는 않습니다만, 사람들이 당신의 네트워크를 훈련할 경우에, L2 일반화는 훨씬 더 자주 사용됩니다. 죄송합니다. 여기에 있는 표기를 살짝 수정할게요. 마지막으로 한가지 세부사항이요, 여기 이 Lambda는 일반화 파라미터라고 불립니다. 그리고 주로 당신은 이것을 development set이나 [경고] 교차 검증 을 사용하여 설치합니다. 훈련 세트와 파라미터가 작아지게 하기 위해 두 표준을 설정하는 것을 교환하는
의미에서 다양한 값과 무엇이 최선인지 확인 후에 두 표준을 설정하는 것을 교환하는 의미에서 다양한 값과 무엇이 최선의 방법인지 확인해야 합니다. 이는 과도한 장착을 예방하는 데에 도움이 됩니다. Lambda는 당신이 조정해야 할지도 모르는 또 다른 하이퍼 파라미터 입니다. 그나저나, 프로그래밍 연습을 위해서는, Lambda는 Python 프로그래밍 언어에서 예약된 키워드 입니다. 그리하여, 프로그래밍 연습에서 우리는 a 없이 Python에 예약된 키워드에 충돌하지 않도록 Lambd를 사용할 것 입니다. 그럼 우리는 Lambda 일반화 매개 변수를 대표하기 위해
Lambd를 사용합니다. 이것이 Logistic regression을 위한 L2를 규현하는 방법 입니다. 신경 네트워크는 어떤가요? 신경 네트워크에는 w[1], b[1] 부터 w[L], b[L]까지의 모든 매개 변수들의 
모든 비용함수가 있는데 대문자 L은 당신의 신경 네트워크에 있는 레이어의 수를 뜻합니다. 그래서 비용함수는 손실의 합계와 같습니다. m 훈련의 예들에 대한 합계 입니다. 일반화는 모든 파라미터 w에 2m 이상의 
Lambda를 더합니다 당신의 파라미터 매트릭스는 w 입니다. 그것을 제곱근이라고 합니다. 매트릭스의 표준에서 제곱근의 표준은 이 매트릭스 제곱근의 각 요소에 대한 i의 합계와 j의 합계로 정의 된다. 그리고 이 합계의 지표들을 원한다면, i=1 에서 n[l-1]에 대한 합계 입니다. j=1에서 n[l]에 대한 합계 입니다. w는 n[l-1] 곱하기 n[l] dimensional matrix이기 때문입니다. 여기서 이 숫자는 레이어 l에 있는 [l-1]의 단위의 개수 입니다. 그래서 이 매트릭스 표준은 Frobenius라고 불립니다. 행렬의 표준은 subscript에 F로 명시되어 있습니다. 그래서, 불가사의한 선형 대수학의 기술적인 이유로, 표준형 매트릭스 l2라고 불리지 않는다 대신에, 그것은 프로베니우스식 매트릭스라고 불린다. 표준형 매트릭스 l2가 더 자연스럽게 들리는 것을 알지만, 매우 불가사의하기에 당신이 꼭 알아야 할 필요는 없다. 관례적으로, 이것은 프로베느우스 표준형이라고 불린다. 그것은 단지 매트릭스 요소들의 제곱의 합을 뜻한다. 그렇다면, 당신은 어떻게 gradient가 이와 함께 하강하도록 
구현할 것인가? 이전에는 backprop을 사용하여 dw를 완성했을 것이다. backprop은 우리에게 w에 관한 J의 편도함수나, w의 임의의 주어진[ㅣ]을 줄 것 입니다. 그 후에 당신은 w[l]을w[l]- 학습률 곱하기 d로 업데이트합니다. 이는 우리가 목표에 이 추가적인 일반화 용어를 추가하기 전 입니다. 자, 이제 우리가 이 일반화 용어를 목표에 추가했으니, 당신이 해야할 일은 dw에 이를 더하세요. lambda/m 곱하기 w. 그 후에, 당신은 그저 이 업데이트 된 것을 이전같이 계산하면 됩니다. 이 새로운 dw[l]의 개체는 당신의 파라미터에 관해서는 비용함수의 도함수의 개체는 여전히 정확한 
개체인 것으로 드러났습니다. 이제 당신은 추가적인 일반화 용어를 맨 마지막에 추가하였기에 그리고 이러한 이유로 L2일반화는 가끔 weight decay라고 불립니다. 따라서, 이러한 dw[l] 개체를 이곳에 그냥 꽂으면, 업데이트된 것은 w[l] = w[l] 곱하기 학습율 alpha 곱하기 backprop +lambda/m 곱하기 w[l]. 마이너스 부호를 이곳에 두고, 이것은 w[l]- alpha lambda / m times w[l]- alpha t 곱하기 backprop에서 얻은 수와 같습니다. 그래서 이 용어는 매트릭스 w[l]이 무엇이든 당신은 좀 더 작게 만들거에요. 그렇죠? 이것은 마치 당신이 매트릭스 w와 그것을 곱하고 매트릭스 w에서 alpha lambda/m 이것을 빼세요. 마치 당신이 매트리스 w를 이 수와 곱하는 것 처럼요. 그것은 1보다 조금 더 적은 숫자일거에요. 그래서 이것이 바로 L2 평균화 일반화가 weight decay라고도 불리는
이유 입니다. 그것은 일반적인 gradient 하강과 유사하기 떄문에 후 방향전파에서 얻은 원래 그라데이션의 알파 배를 뺍니다. 그러나, 이제 그것을 w와 곱할 것 입니다. 그것은 1보다는 조금 적은 수가 될거에요. 그리하여 L2 일반화의 대체명은 weight decay 입니다. 저는 그 이름을 사용하지는 않을 것 입니다. 하지만, 그것을 weight decay라고 칭하는 직관은 바로 여기의 
첫번째 용어가 이것과 같기 때문이다. 그래서 당신은 그저 weight metrics를 1보다 조금 적은 수에 곱하면 됩니다. 그렇게 함으로써 신경네트워크에서 L2 일반화를 시행하느 것 입니다. 자, 한가지 질문이 있어요. 거기, 앤드류, 일반화가 왜 데이터 과잉을 예방하는거져? 다음 영상을 봅시다. 그리고 일반화가 어떻게 데이터 과잉을 예방하는지에 관한
직관을 키워봅시다.