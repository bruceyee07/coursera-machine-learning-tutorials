1
00:00:00,000 --> 00:00:03,025
為什麼正則化幫忙解決過適問題?

2
00:00:03,025 --> 00:00:05,835
為什麼它有助於減低變異問題 ？

3
00:00:05,835 --> 00:00:10,920
讓我們用一些例子來
直觀上看它如何作用

4
00:00:10,920 --> 00:00:16,635
記得高偏差, 高變異

5
00:00:16,635 --> 00:00:25,235
我只是從我們之前影片中
的圖形重畫一遍像這樣

6
00:00:25,235 --> 00:00:27,780
現在讓我們來看大一點的深度神經網路

7
00:00:27,780 --> 00:00:30,550
我知道我不曾畫過
太大或太深的網路

8
00:00:30,550 --> 00:00:34,630
但讓我們看一些過適的神經網路

9
00:00:34,630 --> 00:00:39,520
您有一些成本函數像是 J of w

10
00:00:39,520 --> 00:00:44,390
b 等於損失的總和

11
00:00:44,390 --> 00:00:51,872
我們在正則化作的是增加

12
00:00:51,872 --> 00:00:56,395
這個額外的項目

13
00:00:56,395 --> 00:01:02,690
去懲罰權重矩陣如果太大了

14
00:01:02,690 --> 00:01:04,540
所以這是弗比尼斯範數

15
00:01:04,540 --> 00:01:08,290
所以為什麼用 L2範數或

16
00:01:08,290 --> 00:01:12,445
弗比尼斯範數縮減參數
會比較不過適呢？

17
00:01:12,445 --> 00:01:14,515
一點直觀是如果您

18
00:01:14,515 --> 00:01:17,354
設想正則化 lambda  相當相當大

19
00:01:17,354 --> 00:01:20,005
它們會真的激勵到

20
00:01:20,005 --> 00:01:24,535
設權重矩陣 w 趨近於 0

21
00:01:24,535 --> 00:01:30,460
所以一點直觀是或許設權重趨近於 0 

22
00:01:30,460 --> 00:01:33,340
對於很多的隱藏單元基本上

23
00:01:33,340 --> 00:01:36,675
零化是去除掉這些隱藏單元的影響

24
00:01:36,675 --> 00:01:37,990
如果是這樣

25
00:01:37,990 --> 00:01:44,765
這會簡化神經網路
變成小一點的神經網路

26
00:01:44,765 --> 00:01:48,185
實際上, 這幾乎是羅吉斯迴歸分析

27
00:01:48,185 --> 00:01:50,005
疊成幾層深

28
00:01:50,005 --> 00:01:51,805
所以這會將您從

29
00:01:51,805 --> 00:01:57,635
這樣過適情況趨近於左邊到高偏差情況

30
00:01:57,635 --> 00:02:00,760
但希望會有一個中間值的 lambda

31
00:02:00,760 --> 00:02:04,820
結果會是到在中間 "剛好正確" 的情況

32
00:02:04,820 --> 00:02:07,420
但直觀上設想 lambda 

33
00:02:07,420 --> 00:02:10,510
很大會使得 w 趨近於 0

34
00:02:10,510 --> 00:02:13,280
實際上這不會真的發生

35
00:02:13,280 --> 00:02:17,110
我們可以想成零化或至少減低

36
00:02:17,110 --> 00:02:19,270
很多隱藏單元的影響
所以您最終會

37
00:02:19,270 --> 00:02:21,935
也許像是簡單一點的網路

38
00:02:21,935 --> 00:02:25,920
它們會越來越趨近於
只是使用羅吉斯迴歸分析

39
00:02:25,920 --> 00:02:31,360
這種完全零化一大堆
隱藏單元的直觀並不正確

40
00:02:31,360 --> 00:02:35,225
實際上發生的是它們
還是會使用所有隱藏單元

41
00:02:35,225 --> 00:02:37,610
只是它們每一個都會
變得影響比較小

42
00:02:37,610 --> 00:02:41,255
但您最終會得到一個
比較簡單的網路

43
00:02:41,255 --> 00:02:45,040
就像您有小一點的網路
而這樣比較不會過適

44
00:02:45,040 --> 00:02:47,715
不確定這樣的直觀是否幫助您理解

45
00:02:47,715 --> 00:02:50,765
當您建置正則化
在程式練習中

46
00:02:50,765 --> 00:02:55,360
您實際上自己會看到
一些變異減低的結果

47
00:02:55,360 --> 00:02:57,955
這是另一個嘗試用直觀來

48
00:02:57,955 --> 00:03:01,535
看為什麼正則化
幫助避免過適

49
00:03:01,535 --> 00:03:04,030
這裡, 我先假設我們使用

50
00:03:04,030 --> 00:03:08,465
tanh 啟動函數
像這樣

51
00:03:08,465 --> 00:03:13,515
這是 g of z 等於 tanh of z

52
00:03:13,515 --> 00:03:15,200
如果是這樣

53
00:03:15,200 --> 00:03:19,427
注意到只要 z 很小

54
00:03:19,427 --> 00:03:23,410
所以當 z 只取一個小範圍的參數

55
00:03:23,410 --> 00:03:28,165
也許在這邊
那您只是使用 tanh 函數的線性區域

56
00:03:28,165 --> 00:03:34,080
只要 z 允許到
大一點或小一點的值像這樣

57
00:03:34,080 --> 00:03:37,490
啟動函數會開始
比較不線性

58
00:03:37,490 --> 00:03:40,605
直觀上您可以學到的是
如果 lambda

59
00:03:40,605 --> 00:03:42,750
正則化參數變大

60
00:03:42,750 --> 00:03:46,530
您會讓您的參數變小

61
00:03:46,530 --> 00:03:51,290
因為它們會被懲罰如果成本函數變大

62
00:03:51,290 --> 00:03:56,740
而如果 w 變小因為 z

63
00:03:56,740 --> 00:04:02,550
等於 w 然後技術上是要加上 b

64
00:04:02,550 --> 00:04:04,440
如果 w 變小

65
00:04:04,440 --> 00:04:07,140
那 z 也會變小

66
00:04:07,140 --> 00:04:10,830
特別是, 如果 z 最終以相對較小的值

67
00:04:10,830 --> 00:04:12,787
只在這一小段範圍

68
00:04:12,787 --> 00:04:16,045
那 g of z 會大約是線性

69
00:04:16,045 --> 00:04:22,880
就好像每一層都大約是線性

70
00:04:22,880 --> 00:04:24,800
就像是線性迴歸分析

71
00:04:24,800 --> 00:04:27,860
而我們在第一課程看過
如果每一層

72
00:04:27,860 --> 00:04:31,275
都是線性那您整個網路就只是線性網路

73
00:04:31,275 --> 00:04:33,200
即使是很深的網路

74
00:04:33,200 --> 00:04:35,930
在深度網路使用線性啟動函數

75
00:04:35,930 --> 00:04:39,245
最後它們只能夠計算線性函數

76
00:04:39,245 --> 00:04:43,700
所以不能夠來配適
很複雜的決策

77
00:04:43,700 --> 00:04:49,085
很非線性的決策邊界
允許它很

78
00:04:49,085 --> 00:04:52,940
過適於資料集
像是我們看到的

79
00:04:52,940 --> 00:04:57,485
過適高變異情況
在前一張投影片

80
00:04:57,485 --> 00:04:59,060
總結一下

81
00:04:59,060 --> 00:05:01,665
如果正則化變得很大

82
00:05:01,665 --> 00:05:03,873
參數 w 很小

83
00:05:03,873 --> 00:05:06,350
所以 z 相對變小

84
00:05:06,350 --> 00:05:08,480
先忽略對 b 的影響

85
00:05:08,480 --> 00:05:12,935
所以 z 相對變小

86
00:05:12,935 --> 00:05:16,250
真的，我應該說它只取一小段範圍的值

87
00:05:16,250 --> 00:05:19,890
所以啟動函數如果是 tanh

88
00:05:19,890 --> 00:05:21,790
會將近線性

89
00:05:21,790 --> 00:05:25,790
所以您整個神經網路計算的
不會

90
00:05:25,790 --> 00:05:28,550
離一個大的線性函數很遠
也就是相當

91
00:05:28,550 --> 00:05:32,250
簡單函數而不會是
很複雜非線性函數

92
00:05:32,250 --> 00:05:34,650
那種比較會過適

93
00:05:34,650 --> 00:05:38,870
再提醒一次，當您自己進入程式練習的正則化

94
00:05:38,870 --> 00:05:41,350
您會自己看到這些影響

95
00:05:41,350 --> 00:05:45,680
在結束我們的正則化討論之前

96
00:05:45,680 --> 00:05:48,310
我只是要給您一個建置的提示

97
00:05:48,310 --> 00:05:52,145
也就是當建置正則化時

98
00:05:52,145 --> 00:05:58,730
我們拿我們的成本函數 J 的定義
我們修改它

99
00:05:58,730 --> 00:06:05,810
增加這個額外的項目
如果這個權重太大懲罰它

100
00:06:05,810 --> 00:06:09,230
所以如果您建置梯度下降

101
00:06:09,230 --> 00:06:18,605
一個梯度下降除錯的步驟是
畫成本函數 J 是一個函數

102
00:06:18,605 --> 00:06:22,520
對於梯度下降的迭代數目
而您要看到

103
00:06:22,520 --> 00:06:27,730
成本函數 J 單調下降
經過每一次的梯度下降

104
00:06:27,730 --> 00:06:30,820
如果您建置正則化

105
00:06:30,820 --> 00:06:35,350
請記得 J 現在有新的定義

106
00:06:35,350 --> 00:06:37,735
如果您畫舊的 J 的定義

107
00:06:37,735 --> 00:06:39,370
只有第一項

108
00:06:39,370 --> 00:06:42,290
那您可能看不到單調下降

109
00:06:42,290 --> 00:06:45,030
所以在梯度下降除錯時
請確認您畫的是

110
00:06:45,030 --> 00:06:49,910
新的 J 的定義
包含了第二項

111
00:06:49,910 --> 00:06:54,015
否則您也許看不到 J 單調下降
在每一個單一評估後

112
00:06:54,015 --> 00:06:57,140
所以這是 L2 正則化
實際上

113
00:06:57,140 --> 00:07:01,435
是一種正則化技巧
我用在大部分的深度學習模型中

114
00:07:01,435 --> 00:07:05,480
在深度學習還有另外一種
有時候用來正則化的技巧

115
00:07:05,480 --> 00:07:07,390
稱為 dropout 正則化

116
00:07:07,390 --> 00:07:09,280
讓我們在下一段影片看看它