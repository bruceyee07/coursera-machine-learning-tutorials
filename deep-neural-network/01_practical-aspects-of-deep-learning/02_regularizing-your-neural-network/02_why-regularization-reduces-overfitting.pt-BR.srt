1
00:00:00,000 --> 00:00:03,025
Por que regularização 
ajuda com o sobre-ajuste?

2
00:00:03,025 --> 00:00:05,835
Por que ela ajuda com a redução 
de problemas de variância?

3
00:00:05,835 --> 00:00:10,920
Vamos analisar alguns exemplos para 
ter uma noção sobre como isso funciona.

4
00:00:10,920 --> 00:00:16,635
Então, lembre-se do viés de alta 
(high bias) e da alta variância.

5
00:00:16,635 --> 00:00:25,235
E eu descrevo as imagens do nosso 
vídeo anterior que parecem algo assim.

6
00:00:25,235 --> 00:00:27,780
Agora, vejamos um ajuste de uma estrutura 
de uma rede neural grande e profunda.

7
00:00:27,780 --> 00:00:30,550
Eu sei que não desenhei esta 
muito grande nem muito profunda,

8
00:00:30,550 --> 00:00:34,630
vejamos algumas redes 
neurais e este sobre-ajuste.

9
00:00:34,630 --> 00:00:39,520
Então, você tem uma função 
de custo tipo J em função de W,

10
00:00:39,520 --> 00:00:44,390
b, igual a soma das perdas.

11
00:00:44,390 --> 00:00:51,872
Então, o que fizemos para 
a regularização foi adicionar

12
00:00:51,872 --> 00:00:56,395
este termo extra, que

13
00:00:56,395 --> 00:01:02,690
penaliza as matrizes de peso, 
de serem muito grandes.

14
00:01:02,690 --> 00:01:04,540
Esta era a norma de Frobenius.

15
00:01:04,540 --> 00:01:08,290
Então, porque é que a 
redução da norma L2 ou

16
00:01:08,290 --> 00:01:12,445
da norma Frobenius ou dos parâmetros 
podem causar em um sobre-ajuste menor?

17
00:01:12,445 --> 00:01:14,515
Um conceito é que se você

18
00:01:14,515 --> 00:01:17,354
ajusta o lambda de regularização 
para ser muito, muito grande,

19
00:01:17,354 --> 00:01:20,005
eles serão incentivados a definir

20
00:01:20,005 --> 00:01:24,535
as matrizes de peso W, 
razoavelmente próximas de zero.

21
00:01:24,535 --> 00:01:30,460
Então, uma intuição é talvez definir o 
peso a um valor bem perto de zero

22
00:01:30,460 --> 00:01:33,340
para muitas unidades ocultas, 
que basicamente anulam

23
00:01:33,340 --> 00:01:36,675
muito o impacto 
destas unidades ocultas.

24
00:01:36,675 --> 00:01:37,990
E, se esse é o caso,

25
00:01:37,990 --> 00:01:44,765
então, esta rede neural muito mais 
simplificada, torna-se muito menor.

26
00:01:44,765 --> 00:01:48,185
Na verdade, é quase tipo uma 
unidade de regressão logística,

27
00:01:48,185 --> 00:01:50,005
mas empilhada provavelmente 
na mesma profundidade.

28
00:01:50,005 --> 00:01:51,805
E isso levará este

29
00:01:51,805 --> 00:01:57,635
este caso de sobre-ajuste muito mais perto 
da esquerda para outro caso de "high bias".

30
00:01:57,635 --> 00:02:00,760
Mas, tomara que haja um valor 
intermediário de lambda que

31
00:02:00,760 --> 00:02:04,820
resulte em ema solução mais perto 
desta solução "ajustada" no centro.

32
00:02:04,820 --> 00:02:07,420
Mas a intuição é que, ao 
ajustar lambda em um valor

33
00:02:07,420 --> 00:02:10,510
muito alto, definiremos 
W próximo a zero,

34
00:02:10,510 --> 00:02:13,280
que na prática, não é 
na verdade o que ocorre.

35
00:02:13,280 --> 00:02:17,110
Podemos pensar nisso como a 
anulação, ou pelo menos a redução

36
00:02:17,110 --> 00:02:19,270
do impacto de muitas unidades ocultas, resultando

37
00:02:19,270 --> 00:02:21,935
em uma sensação de obtenção 
de uma rede mais simples.

38
00:02:21,935 --> 00:02:25,920
Eles vão ficando cada vez mais perto, como 
se você estivesse utilizando regressão logística.

39
00:02:25,920 --> 00:02:31,360
A intuição de zerar completamente várias 
unidades ocultas não está muito correta.

40
00:02:31,360 --> 00:02:35,225
O que realmente acaba acontecendo é que eles 
ainda usarão todas as unidades ocultas.

41
00:02:35,225 --> 00:02:37,610
Entretanto, cada uma delas 
tem um efeito muito menor.

42
00:02:37,610 --> 00:02:41,255
Mas você acaba obtendo 
uma rede mais simples,

43
00:02:41,255 --> 00:02:45,040
como se você tivesse uma rede menor, que 
portanto, é menos propensa ao sobre-ajuste.

44
00:02:45,040 --> 00:02:47,715
Então, muito deste 
conceito ajuda mais

45
00:02:47,715 --> 00:02:50,765
quando você implementa 
regularização no exercício de programação.

46
00:02:50,765 --> 00:02:55,360
Na verdade, você consegue ver por si mesmo, 
alguns resultados destas reduções de variância.

47
00:02:55,360 --> 00:02:57,955
Aqui está outra tentativa 
de uma intuição adicional

48
00:02:57,955 --> 00:03:01,535
do porquê regularização 
ajuda a prevenir sobre-ajuste.

49
00:03:01,535 --> 00:03:04,030
E para isso, eu assumirei 
que estamos utilizando

50
00:03:04,030 --> 00:03:08,465
a função de ativação tanh, 
que parece com esta.

51
00:03:08,465 --> 00:03:13,515
Esta é uma função g(z) = tanh (z).

52
00:03:13,515 --> 00:03:15,200
Então, se for o caso,

53
00:03:15,200 --> 00:03:19,427
observe que, enquanto 
Z é muito pequeno,

54
00:03:19,427 --> 00:03:23,410
portanto, se Z recebe um intervalo 
muito pequeno de parâmetros,

55
00:03:23,410 --> 00:03:28,165
mais ou menos por aqui, então você está 
usando somente o regime linear da função tanh.

56
00:03:28,165 --> 00:03:34,080
E somente quando Z começa a receber valores 
maiores ou valores menores, desta forma,

57
00:03:34,080 --> 00:03:37,490
é que a função de ativação 
começa a tornar-se menos linear.

58
00:03:37,490 --> 00:03:40,605
Então, a noção que você pode 
aprender aqui, é que se lambda,

59
00:03:40,605 --> 00:03:42,750
o parâmetro de 
regularização, é grande,

60
00:03:42,750 --> 00:03:46,530
então, resulta que seus parâmetro 
serão relativamente pequenos,

61
00:03:46,530 --> 00:03:51,290
porque eles são penalizados, sendo 
grandes em uma função de custo.

62
00:03:51,290 --> 00:03:56,740
Assim, se os intervalos de W são 
pequenos, então, pelo fato de Z ser

63
00:03:56,740 --> 00:04:02,550
igual a W, então é tecnicamente + b,

64
00:04:02,550 --> 00:04:04,440
mas se W tende a ser muito pequeno,

65
00:04:04,440 --> 00:04:07,140
então Z também será 
relativamente pequeno.

66
00:04:07,140 --> 00:04:10,830
E particularmente, se Z acaba recebendo 
valores relativamente pequenos,

67
00:04:10,830 --> 00:04:12,787
somente neste intervalo,

68
00:04:12,787 --> 00:04:16,045
então g(Z) será mais ou menos linear.

69
00:04:16,045 --> 00:04:22,880
Então, é como se cada 
camada fosse mais ou menos linear.

70
00:04:22,880 --> 00:04:24,800
Como se fosse uma regressão linear.

71
00:04:24,800 --> 00:04:27,860
E vimos no curso 1 que, se cada camada

72
00:04:27,860 --> 00:04:31,275
é linear, então toda a sua rede 
é apenas uma rede linear.

73
00:04:31,275 --> 00:04:33,200
E assim, mesmo uma rede profunda,

74
00:04:33,200 --> 00:04:35,930
uma rede profunda com 
uma função de ativação linear,

75
00:04:35,930 --> 00:04:39,245
finalmente é aquela que consegue 
somente calcular uma função linear.

76
00:04:39,245 --> 00:04:43,700
Então, ela não pode ser usada 
em decisões muito complexas.

77
00:04:43,700 --> 00:04:49,085
Limites de decisões não-lineares, 
que a permitem realmente

78
00:04:49,085 --> 00:04:52,940
um sobre-ajuste correto sobre os 
conjuntos de dados, como vimos

79
00:04:52,940 --> 00:04:57,485
no sobre-ajuste de alta 
variância no slide anterior.

80
00:04:57,485 --> 00:04:59,060
Apenas para resumir:

81
00:04:59,060 --> 00:05:01,665
se a regularização se 
torna muito grande,

82
00:05:01,665 --> 00:05:03,873
o parâmetro W é muito pequeno,

83
00:05:03,873 --> 00:05:06,350
então Z será relativamente pequeno,

84
00:05:06,350 --> 00:05:08,480
meio que ignorando os 
efeitos de b momentaneamente.

85
00:05:08,480 --> 00:05:12,935
Então, Z será relativamente pequeno, ou

86
00:05:12,935 --> 00:05:16,250
realmente, eu diria que recebe 
um intervalo pequeno de valores.

87
00:05:16,250 --> 00:05:19,890
E assim, se a função de ativação for tanh,

88
00:05:19,890 --> 00:05:21,790
será relativamente linear.

89
00:05:21,790 --> 00:05:25,790
Então, toda a sua rede neural estará 
calculando algo não muito diferente do que

90
00:05:25,790 --> 00:05:28,550
uma grande função linear, 
que é desta forma,

91
00:05:28,550 --> 00:05:32,250
uma função simples, ao invés de uma 
função não-linear altamente complexa.

92
00:05:32,250 --> 00:05:34,650
E assim, muito menos capaz 
de realizar um sobre-ajuste.

93
00:05:34,650 --> 00:05:38,870
E novamente, quando você entrar em 
regularização no exercício de programação,

94
00:05:38,870 --> 00:05:41,350
você será capaz de verificar por 
si mesmo alguns destes efeitos.

95
00:05:41,350 --> 00:05:45,680
Antes de encerrar nossa 
discussão sobre regularização,

96
00:05:45,680 --> 00:05:48,310
quero dar a vocês uma 
dica de implementação.

97
00:05:48,310 --> 00:05:52,145
Quando estiver 
implementando regularização,

98
00:05:52,145 --> 00:05:58,730
pegamos nossa definição de função de 
custo J e, na verdade, a modificamos

99
00:05:58,730 --> 00:06:05,810
adicionando este termo extra que 
penaliza o peso quando é muito grande.

100
00:06:05,810 --> 00:06:09,230
Assim, se você está 
implementando gradiente descendente,

101
00:06:09,230 --> 00:06:18,605
uma das etapas para depurar o gradiente descendente 
é plotar a função de custo J, como uma função

102
00:06:18,605 --> 00:06:22,520
do número de elevações do gradiente 
descendente e você deve verificar se

103
00:06:22,520 --> 00:06:27,730
a função de custo J diminui monotonicamente 
após cada elevação do gradiente descendente.

104
00:06:27,730 --> 00:06:30,820
E se você está implementando regularização,

105
00:06:30,820 --> 00:06:35,350
então lembre-se, por favor, que 
J agora tem esta nova definição.

106
00:06:35,350 --> 00:06:37,735
Se você plotar a definição antiga de J,

107
00:06:37,735 --> 00:06:39,370
apenas este primeiro termo,

108
00:06:39,370 --> 00:06:42,290
então talvez você não 
veja um decréscimo contínuo.

109
00:06:42,290 --> 00:06:45,030
Então, para depurar o gradiente descendente, 
assegure-se que você esteja plotando

110
00:06:45,030 --> 00:06:49,910
esta nova definição de J, incluindo 
este segundo termo também.

111
00:06:49,910 --> 00:06:54,015
Caso contrário, você pode não ver o 
decréscimo contínuo do J em cada elevação.

112
00:06:54,015 --> 00:06:57,140
Então, esta é a regularização L2, que é na verdade,

113
00:06:57,140 --> 00:07:01,435
uma técnica de regularização que eu mais uso nos 
módulos de treinamento de aprendizagem profunda.

114
00:07:01,435 --> 00:07:05,480
Em aprendizagem profunda há 
uma outra técnica de regularização

115
00:07:05,480 --> 00:07:07,390
chamada de regularização dropout.

116
00:07:07,390 --> 00:07:09,280
Vamos dar uma olhada nela no próximo vídeo. 
[Tradução: Renato Barata Gomes | Revisão: Carlos Lage]