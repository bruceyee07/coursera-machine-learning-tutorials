もしニューラルネットワークの
オーバーフィッティングが疑われる場合 つまり高バリアンス問題のとき まず試すべきは正則化です 高バリアンスに対処する別の方法は より多くの訓練データを得ることで
確実な方法なのですが いつでも多くのデータを
得られるわけではないですし 多くのデータを得るのは
お金がかかります しかし正則化することで
オーバーフィッティングを防げ ネットワークの誤差を減らせます では正則化のしくみを見ていきましょう ロジスティック回帰を用いて
理解を進めましょう ロジスティック回帰では
コスト関数Ｊを最小化しましたね これがコスト関数の式です 訓練データでの予測に基づく損失の和です ｗやｂも覚えているでしょうか ロジスティック回帰でのパラメータです ｗはｘ次元のパラメータで
ｂは実数です ロジスティック回帰で正則化するために
加えるべきものは このλで正則化パラメータと呼ばれます もう少し説明しましょう λ/2mにｗのノルムの
２乗をかけたものは ここでｗのノルムの２乗は n_x個のw_j^2の和と等しく wTwとも書けます
ｗのユークリッドノルムです これはL2正則化と呼ばれます なぜならここでユークリッドノルムを
用いているためです パラメータベクトルｗのL2ノルムです なぜｗのみに正則化を施すのでしょうか なぜｂに対応するものを
加えないのでしょうか 実際は入れてもよいのですが
私はやりません なぜならパラメータを見たとき
ｗは非常に高次元のベクトルで 高バリアンスの問題が起きやすいのです おそらくｗは大量のパラメータを持ち 全パラメータを適合できないでしょうが
ｂはあくまでひとつの数字です したがって大半のパラメータは
ｂではなくｗにあります もし最後の項を加えてみても さほど違いはないでしょう なぜならbは大量のパラメータのうちの
わずか１つにすぎないのですから 実際のところ
私はこの項をわざわざ入れません 望むなら入れてもよいでしょう このL2正則化は
もっとも一般的な正則化です L1正則化というのも
聞いたことがあるかもしれません それはL2項のかわりに足す項が λ/mにこの和をかけたものになります パラメータベクトルwの
L1ノルムと呼ばれるものです だから下付きで１がありますね 分母がmか2mかは
単に定数倍でしかありません もしL1正則化を用いれば
wはスパースになるでしょう スパースとはwの成分に
０が大量にあるという意味です これはモデルを圧縮するという人もいます ０成分が多ければ
モデルを格納するメモリを節約できます しかしスパースに寄与する
L1正則化の影響は わずかでした したがってこれはあまり使われません 少なくともモデルを圧縮する目的では ネットワークを訓練するとき L2正則化のほうが
はるかに多く使われています 失礼ちょっとここの記号を直します では最後に一点 ここのλは正則化パラメータと呼ばれます 普通は開発セットを用いて決めます あるいは交差検定セットで 様々な値を試して
良い値を探します トレードオフの観点から
訓練セットでうまくいくかと そのパラメータでノルムを小さくし オーバーフィッティングを防げるかを
考慮します したがってλは調整せねばならない
ハイパーパラメータです ところでプログラミング演習のために "lambda"はPythonにおける
予約語なのです よって演習では
"lambd"をかわりに使います "a"を抜きPythonの予約語による
クラッシュを防ぎます "lambd"で
正則化パラメータλをあらわします 以上がロジスティック回帰における
L2正則化の実装です ではニューラルネットでは
どうでしょうか ニューラルネットでのコスト関数は w[1],b[1]からw[L],b[L]までの
パラメータを持ちます ここでＬは層の数をあらわします コスト関数はこうです
損失の和では 訓練例m個ぶんを
足し合わせます 正則化ではλ/2m掛ける すべてのWにわたる和です
Wはパラメータ行列のことです これは２乗ノルムと呼ばれます この２乗ノルム行列を定義します ｉとｊで総和をとるのですが 行列の各要素の２乗を
足し合わせます この足し合わせに
添え字が必要なら ｉは１からn[l-1]まで ｊは１からn[l]までです ｗはn[l-1],n[l]次元の
行列だからです この次元はl-1番目とl番目の
隠れ層の個数に対応しています これはFrobeniusノルムと
呼ばれるものになります 下付きでＦと書かれた行列です 線形代数の不可解な
技術的理由により L2ノルム行列とは呼ばれず かわりにFrobeniusノルム行列と
呼ばれます 単にL2ノルムと呼ぶほうが
自然なのはわかりますが 知る必要のない
本当に不可思議な理由により 慣例的に
Frobeniusノルムと呼ばれます これの意味するところはたんなる
行列要素の２乗の総和です ではこれを用いてどのように
勾配降下を実装すればよいでしょうか 以前dwを
誤差逆伝播法で求めましたが そこでＪのｗによる偏微分が得られます ｗはすべての[l]についてです そしてw[l]を更新します
w[l]引く学習係数掛けるdw[l]です ここまでは目的関数に
この正則化項を加える前の話ですね 次は正則化項にあたる部分を
考えていきます dwにλ/m掛けるｗを加えます 同様にこちらの更新式を求めます このdw[l]の新しい定義は 依然として正しく コスト関数の
パラメータによる微分です 正則化項を付け足しても
変わりません このためL2正則化はまた 重み減衰とも呼ばれます dw[l]のこの定義を用いて
ここに代入すると w[l]の更新式は w[l]に学習係数αをかけ
逆誤差伝播による部分と λ/mかけるw[l]をたします ここにマイナス符号を入れます これを等式変形すると w[l]-αλ/m×w[l]-α× 逆誤差伝播から得られたもの
になります この項が示しているのは
w[l]がどんな値であろうと 徐々に減少することです
いいですか？ これはまさに
行列wを取ってきて 1-αλ/mを掛けている
ようなものです 実際にwを取って
そのαλ/m倍を引いています 行列wに掛けるこの数字は １より少し小さい数になるでしょう L2正則化が
重み減衰と呼ばれる理由は 通常の勾配降下で
wの更新をする際 逆誤差伝播から得られたもとの勾配の
α倍を引くようなものだからです wに掛け合わされるこの値が １よりわずかに小さいとも言えます したがってL2正則化のまたの名は
重み減衰となるのです あまりその呼び方をしませんが
直観的には 重み減衰と呼ばれるのは
この最初の項がこれに等しいからです 重み行列に１より少し小さい値を
かけ合わせているだけなのです 以上がニューラルネットにおける
L2正則化の実装です ここで良く聞かれる質問なのですが
やあアンドリュー どうやって正則化が
オーバーフィッティングを防ぐのかい？ 次のビデオを見て 正則化がオーバーフィットを防ぐ方法を
つかんでください