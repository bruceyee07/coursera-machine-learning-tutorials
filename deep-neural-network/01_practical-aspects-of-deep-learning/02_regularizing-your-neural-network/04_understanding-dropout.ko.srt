1
00:00:00,000 --> 00:00:06,015
Drop out은 여러분의 네트워크에서 유닛을 해체시키는 이상한 현상을 일으킵니다. 

2
00:00:06,015 --> 00:00:08,240
반면에 일반화에서는 어떻게 잘 작동할 수 있는 것일까요?

3
00:00:08,240 --> 00:00:10,665
이해를 위해 직관적인 내용을 좀 더 다루어보도록 하겠습니다. 이전 비디오에선,

4
00:00:10,665 --> 00:00:11,970
이전 강의에서 여러분에게

5
00:00:11,970 --> 00:00:16,705
dropout에서는 무작위로 여러분의 네트워크 유닛을 해체시킨다고 설명해드렸는데요,

6
00:00:16,705 --> 00:00:20,860
그러니까 반복할 때마다 더 작은 신경망으로 일하는 것과 같기 때문에,

7
00:00:20,860 --> 00:00:26,360
마치 더 작은 신경망을 사용하면 일반화효과가 있어야 할 것만 같습니다.

8
00:00:26,360 --> 00:00:28,255
두번째 직관적인 면인데요, 

9
00:00:28,255 --> 00:00:34,795
단일 유닛의 개념으로 생각해보죠, 이것을 예로 보시겠습니다.

10
00:00:34,795 --> 00:00:37,530
이 유닛이 제 역할을 하기 위해선 입력값을 갖고,

11
00:00:37,530 --> 00:00:41,370
또 의미 있는 결과 값을 생성해야 합니다. 

12
00:00:41,370 --> 00:00:42,595
dropout에선,

13
00:00:42,595 --> 00:00:45,555
입력값이 무작위로 제거될 수 있습니다.

14
00:00:45,555 --> 00:00:47,965
간혹 저기 2개의 유닛이 제거될 것입니다.

15
00:00:47,965 --> 00:00:50,530
또 가끔씩은 다른 유닛이 제거될 수 있습니다.

16
00:00:50,530 --> 00:00:52,635
이런 현상이 의미하는 바는, 이 유닛이

17
00:00:52,635 --> 00:00:54,005
이렇게 보라색으로 동그랗게 표시하고 있는 부분이,

18
00:00:54,005 --> 00:00:58,560
어떤 한가지의 특성에 의존하면 안됩니다. 그 이유는 어떤 한가지의 특성은 

19
00:00:58,560 --> 00:01:03,715
언제든지 임의로 제거될 수 있기 때문이죠. 

20
00:01:03,715 --> 00:01:08,070
그렇기 때문에 특히, 모든 것을 하나의 입력값에 

21
00:01:08,070 --> 00:01:10,475
베팅하는 것인 꺼리게 되겠죠. 맞죠?

22
00:01:10,475 --> 00:01:12,990
이렇듯이, 한 입력값에 너무 많은 비중 (weight)을 부여하기 

23
00:01:12,990 --> 00:01:16,035
꺼리게 될 것입니다.

24
00:01:16,035 --> 00:01:20,820
그러므로 여기 이 유닛은 이 쪽 방향으로 퍼지길 원할 것입니다. 그리하여 

25
00:01:20,820 --> 00:01:26,250
여기 4개의 입력값에 비중을 나눠 주는 것이죠.

26
00:01:26,250 --> 00:01:27,765
이렇게 비중을 분산해주면, 

27
00:01:27,765 --> 00:01:34,815
비중의 (weight) 제곱 노름을 축소시키는 효과가 있을 것입니다.

28
00:01:34,815 --> 00:01:38,730
그러므로 L2 일반화에서 본 것과 비슷하게, 

29
00:01:38,730 --> 00:01:41,650
droupout을 도입하는 것은 

30
00:01:41,650 --> 00:01:46,195
그 비중 (weight)을 줄이고 L2 일반화과 같은 것을 진행해 over fitting을 막아줍니다.

31
00:01:46,195 --> 00:01:48,750
알고 보니 공식적으로 dropout은 L2 일반화가

32
00:01:48,750 --> 00:01:52,035
변형된 버전이라고 할 수 있는데요,

33
00:01:52,035 --> 00:01:55,305
그러나 비중 별로 부과하는 L2 페널티가 다릅니다.

34
00:01:55,305 --> 00:01:58,830
곱셈이 적용되는 activation의 크기에 따라 말이죠,

35
00:01:58,830 --> 00:02:02,580
요약하자면, dropout은 l2 일반화 과

36
00:02:02,580 --> 00:02:06,705
비슷한 효과를 가지고 있다고 보여줄 수 있습니다.

37
00:02:06,705 --> 00:02:09,990
L2 일반화 과 적용되는 부분에서의 차이만 있고

38
00:02:09,990 --> 00:02:13,540
다른 입력값의 스케일에 따라 더 변형한다는 차이가 있습니다.

39
00:02:13,540 --> 00:02:15,930
dropout을 도입하는 것에 대한 내용을 한가지 더 말씀 드리자면,

40
00:02:15,930 --> 00:02:19,510
여기는 3개의 입력 특성이 있는 네트워크입니다.

41
00:02:19,510 --> 00:02:21,795
이것은 7개의 숨겨진 유닛이고요,

42
00:02:21,795 --> 00:02:24,625
7, 3, 2, 1입니다.

43
00:02:24,625 --> 00:02:26,915
저희가 선택해야 했던 매개 변수 중 하나는 바로

44
00:02:26,915 --> 00:02:31,395
keep.prob입니다. 이것은 층마다 유닛을 유지할 확률을 나타내죠. 

45
00:02:31,395 --> 00:02:36,550
그렇기 때문에 keep.prob을 층별로 변하게 하는 것이 가능한데요, 

46
00:02:36,550 --> 00:02:38,490
예를 들어 첫번째 층에서는,

47
00:02:38,490 --> 00:02:42,460
W1의 매트릭스는 3 X 7일 것입니다. 

48
00:02:42,460 --> 00:02:46,120
2번째 weight matrix는 7 X 7 일 것이구요, 

49
00:02:46,120 --> 00:02:49,680
W3는 7 X 3일 것입니다. 

50
00:02:49,680 --> 00:02:53,205
그렇게 하면 W2가 사실 가장 큰 weight 매트릭스인데요,

51
00:02:53,205 --> 00:02:55,500
가장 큰 매개 변수 set가 W2에 있을 것이기 때문이죠,

52
00:02:55,500 --> 00:02:58,195
7 X 7 인 것입니다.

53
00:02:58,195 --> 00:03:01,605
그 매트릭스의 over fitting을 방지하기 위해, 

54
00:03:01,605 --> 00:03:03,600
잘하면 여기 이 층을

55
00:03:03,600 --> 00:03:05,205
이것은 2번 층이겠네요, 

56
00:03:05,205 --> 00:03:08,490
이것은 keep-prob이 꽤 낮을 수 있습니다.

57
00:03:08,490 --> 00:03:10,435
예를 들어, 0.5일 수 있겠죠.

58
00:03:10,435 --> 00:03:13,825
over fitting에 대해 그닥 걱정을 안할 층들에서는

59
00:03:13,825 --> 00:03:15,080
조금 더 큰 값의 key-prob이 있을 수 있겠죠. 

60
00:03:15,080 --> 00:03:18,255
잘하면 0.7일 수 있겠습니다.

61
00:03:18,255 --> 00:03:22,715
만약 over fitting에 대해 전혀 걱정하지 않는 경우라면, 

62
00:03:22,715 --> 00:03:25,240
key-prob이 0이 될 수 있겠습니다.

63
00:03:25,240 --> 00:03:30,725
명료성을 위해서 여기 이 숫자들을 여기 이 보라색 박스로 나타내겠습니다.

64
00:03:30,725 --> 00:03:34,635
이것들은 다른 층들에 대한 다른 key-prob값들입니다.

65
00:03:34,635 --> 00:03:39,100
1.0의 key prob 은 모든 유닛을 유지시킨다는 뜻입니다. 그러므로,  

66
00:03:39,100 --> 00:03:41,855
dropout을 그 층에서는 쓰지 않는 것으로 해석할 수 있습니다.

67
00:03:41,855 --> 00:03:44,730
overfitting에 대해 조금 더 걱정을 하는 층에서는,

68
00:03:44,730 --> 00:03:46,660
매개 변수가 많은 층들에서는, 

69
00:03:46,660 --> 00:03:51,600
조금 더 간략한 유형의 dropout을 적용하기 위해서 key-prob을 낮게 설정할 수 있습니다.

70
00:03:51,600 --> 00:03:53,070
이것은 마치 일반화 매개 변수인 L2 regularization Lambda의 값을

71
00:03:53,070 --> 00:03:54,910
올리는 것과 유사합니다.

72
00:03:54,910 --> 00:03:57,960
다른 층보다 더 일반화 하기 위해서 말이죠.

73
00:03:57,960 --> 00:04:02,715
엄밀히 이야기하면 입력 층에도 dropout를 적용시킬 수 있습니다.

74
00:04:02,715 --> 00:04:07,295
여기서는 1개 또는 복수의 입력 특성을 최대치로 하는 확률을 가질 수 있습니다.

75
00:04:07,295 --> 00:04:11,580
실제로 자주 사용하는 방법은 아닙니다.

76
00:04:11,580 --> 00:04:15,270
그래서 입력값으로는 1.0의 key-prob이 상당히 흔합니다. 

77
00:04:15,270 --> 00:04:17,985
또, 0.9와 같은 큰 값을 사용할 수도 있습니다.

78
00:04:17,985 --> 00:04:22,740
그러나 여러분이 절반 입력 특성을 제거하고 싶을 확률은 사실 작죠.

79
00:04:22,740 --> 00:04:25,665
그러니 key-prob은, 규칙을 적용하는 경우, 

80
00:04:25,665 --> 00:04:32,030
1과 가까운 값이 될 것입니다. 입력 층에 dropout을 적용만 하더라도 말이죠. 

81
00:04:32,030 --> 00:04:33,450
요약하자면, 

82
00:04:33,450 --> 00:04:36,330
어떤 층이 다른 층보다 overfitting하는 것이 걱정이 된다면

83
00:04:36,330 --> 00:04:40,320
다른 층보다 더 작은 key-prob을 설정하는 것이 가능합니다.

84
00:04:40,320 --> 00:04:41,430
이것의 단점은, 

85
00:04:41,430 --> 00:04:44,955
상호 검증를 사용하는 검색에 훨씬 더 많은 하이퍼 파라미터들이 생긴다는 것입니다. 

86
00:04:44,955 --> 00:04:48,525
또 다른 방법으로는 어떤 특정 층에서 dropout을 적용시키고

87
00:04:48,525 --> 00:04:50,460
어떤 층에서는 적용시키지 않으면서, 

88
00:04:50,460 --> 00:04:52,630
동시에 한개의 하이퍼 파라미터를 이용해서, 

89
00:04:52,630 --> 00:04:55,910
dropout이 적용되는 경우에, 그 층들의 key-prob이 되는 것입니다.

90
00:04:55,910 --> 00:04:59,070
끝내기 전에, 몇가지 실행 팁을 알려드리겠습니다.

91
00:04:59,070 --> 00:05:03,850
처음의 많은 dropout의 성공적인 도입은 computer vision에서 발생했습니다.

92
00:05:03,850 --> 00:05:05,075
커퓨터 비전에서는,

93
00:05:05,075 --> 00:05:06,890
입력값의 크기가 너무 커서,

94
00:05:06,890 --> 00:05:11,275
모든 픽셀들을 입력하는 데에는 거의 항상 데이터가 부족합니다.

95
00:05:11,275 --> 00:05:14,710
그렇기 때문에 컵퓨터 비진에서 자주 drop이 활용됩니다.

96
00:05:14,710 --> 00:05:18,035
그리고 컴퓨터 비전 리서치 연구원들 중 거의 항상 dropout을 

97
00:05:18,035 --> 00:05:19,750
기본값으로 사용하는 사람들로 있습니다.

98
00:05:19,750 --> 00:05:24,866
기역 하셔야 할 것은, droupout은 일반화 테크닉이고, 

99
00:05:24,866 --> 00:05:27,010
over-fitting 방지에 도움을 준다는 것입니다.

100
00:05:27,010 --> 00:05:30,880
그렇기 때문에 본인의 알고리즘의 over-fitting하지 않는 이상, 

101
00:05:30,880 --> 00:05:33,250
dropout을 쓰라고 하지는 않을 것입니다.

102
00:05:33,250 --> 00:05:36,557
그렇기 때문에 적용분야가 다른 것들에 비해 적은 편인데요, 

103
00:05:36,557 --> 00:05:38,320
컴퓨터 비전 에서는 

104
00:05:38,320 --> 00:05:40,600
보통 데이터가 충분하지 않기 때문에,

105
00:05:40,600 --> 00:05:42,090
거의 항상 over-fitting합니다.

106
00:05:42,090 --> 00:05:46,425
그렇기 때문에 컴퓨터 비전 리서치 연구원 일부는 거의 dropout을 맹신하는 경우가 있죠.

107
00:05:46,425 --> 00:05:52,498
하지만 이런 직관이 다른 분야에 일반화되지는 않는다고 봅니다.

108
00:05:52,498 --> 00:06:00,490
dropout의 큰 단점은 바로 J 비용함수가 더이상 클리어하게 정의되지 않는다는 점입니다.

109
00:06:00,490 --> 00:06:06,635
반복 테스트를 할때마다, 노드 뭉치를 제거해나가기 때문이죠.

110
00:06:06,635 --> 00:06:10,855
그렇기 때문에, 기울기 강하의 성능을 더블체크하는 경우에, 

111
00:06:10,855 --> 00:06:14,590
iteration마다 계속 감소하는 J비용함수가 잘 정의되었다는 것을

112
00:06:14,590 --> 00:06:20,365
더블체크하기는 더 어렵습니다.

113
00:06:20,365 --> 00:06:24,625
그 이유는 최적화하려 고하는 J비용함수는 덜 정확히

114
00:06:24,625 --> 00:06:27,395
정의되어있고, 또는 확실히 계산을 하기가 어렵기 때문입니다.

115
00:06:27,395 --> 00:06:30,160
이런 그래프를 나타내기 위한 디버깅 

116
00:06:30,160 --> 00:06:32,010
도구를 잃는 것입니다.

117
00:06:32,010 --> 00:06:34,805
그래서 제가 주로 하는 것은 dropout을 끄고, 

118
00:06:34,805 --> 00:06:37,060
key-prob을 1로 지정합니다.

119
00:06:37,060 --> 00:06:40,885
그리고 코드를 실행해서 J가 점진적으로 감소하는 것을 확인하고, 

120
00:06:40,885 --> 00:06:43,960
dropout을 키고 바라기 시작하죠.

121
00:06:43,960 --> 00:06:47,035
제가 dropout중에 버그를 코드에 안 가지고 왔다고 바라는 것이죠.

122
00:06:47,035 --> 00:06:49,195
다른 방법이 필요하겠지만,

123
00:06:49,195 --> 00:06:52,060
이러한 수치들을 plot하지 않으면서 dropout에서도 코드가 정상적으로

124
00:06:52,060 --> 00:06:55,900
작동하고, 기울기 강하 가 잘 작동하도록 하겠습니다.

125
00:06:55,900 --> 00:06:58,130
이것을 끝으로, 아직 

126
00:06:58,130 --> 00:07:01,830
일반화 테크닉과 관련하여 알만한 내용이 더 있는데요,

127
00:07:01,830 --> 00:07:04,480
이런 방법은 다음 비디오를 통해 다루도록 하겠습니다.