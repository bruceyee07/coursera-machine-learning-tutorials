1
00:00:00,000 --> 00:00:06,015
随机失活 (dropout) 这种
从网络中随机敲除神经元的做法看起来有些疯狂

2
00:00:06,015 --> 00:00:08,240
但是为什么用于正则化时它的表现这么好呢？

3
00:00:08,240 --> 00:00:10,665
让我们再深究一下

4
00:00:10,665 --> 00:00:11,970
在之前的视频中  我们给出了一个数学上关于梯度

5
00:00:11,970 --> 00:00:16,705
我之前解释dropout会让神经元随机失活

6
00:00:16,705 --> 00:00:20,860
这就使得好像每一次迭代
都会在一个更小的神经网络中计算

7
00:00:20,860 --> 00:00:26,360
而使用更小的神经网络就好像具有正则化效果

8
00:00:26,360 --> 00:00:28,255
这里再给出第二个解释

9
00:00:28,255 --> 00:00:34,795
我们从单一神经元的角度来看这个问题 比如这个点

10
00:00:34,795 --> 00:00:37,530
它的任务就是

11
00:00:37,530 --> 00:00:41,370
利用这些输入单元生成一个有意义的输出

12
00:00:41,370 --> 00:00:42,595
而如果使用了dropout

13
00:00:42,595 --> 00:00:45,555
这些输入会被随机的丢弃

14
00:00:45,555 --> 00:00:47,965
有的时候这两个神经元会被丢弃

15
00:00:47,965 --> 00:00:50,530
有的时候另一个神经元会被丢弃

16
00:00:50,530 --> 00:00:52,635
因此 这就意味着

17
00:00:52,635 --> 00:00:54,005
我用紫色圈起来的这个

18
00:00:54,005 --> 00:00:58,560
它不能依赖于任何一个特征 因为每个都可能被随机丢弃

19
00:00:58,560 --> 00:01:03,715
或者说它的每一个输入都可能随机失活

20
00:01:03,715 --> 00:01:08,070
所以在特定的时候 就不愿把所有的赌注

21
00:01:08,070 --> 00:01:10,475
只放在这一个输入神经元上 对吗？

22
00:01:10,475 --> 00:01:12,990
因为任何一个输入都可能失活

23
00:01:12,990 --> 00:01:16,035
所以我们也不愿把太多的权重放在某一个上

24
00:01:16,035 --> 00:01:20,820
因此这个神经元将会更积极的使用这种方式

25
00:01:20,820 --> 00:01:26,250
对于每个输入都给一个比较小的权重

26
00:01:26,250 --> 00:01:27,765
而泛化这些权值

27
00:01:27,765 --> 00:01:34,815
将有利于压缩这些权重的平方泛数 (平方和)

28
00:01:34,815 --> 00:01:38,730
和L2正则化类似

29
00:01:38,730 --> 00:01:41,650
使用dropout有助于

30
00:01:41,650 --> 00:01:46,195
收缩权值以及防止过拟合

31
00:01:46,195 --> 00:01:48,750
但是 更准确的来说

32
00:01:48,750 --> 00:01:52,035
dropout应该被看作一种自适应形式而不是正则化

33
00:01:52,035 --> 00:01:55,305
L2正则对不同权值的惩罚方式有所不同

34
00:01:55,305 --> 00:01:58,830
这取决于被激活的乘方大小

35
00:01:58,830 --> 00:02:02,580
总之来说

36
00:02:02,580 --> 00:02:06,705
dropout能起到和L2正则类似的效果

37
00:02:06,705 --> 00:02:09,990
只是针对不同的情况L2正则可以有少许的变化

38
00:02:09,990 --> 00:02:13,540
所以适用面更广

39
00:02:13,540 --> 00:02:15,930
当你使用dropout时还要注意一个细节

40
00:02:15,930 --> 00:02:19,510
这里是一个神经网络 有3个输入

41
00:02:19,510 --> 00:02:21,795
这里有7个隐藏神经元

42
00:02:21,795 --> 00:02:24,625
7个 3个 2个 1个

43
00:02:24,625 --> 00:02:26,915
我们必须要确定的一个参数是留存率 (keep prop)

44
00:02:26,915 --> 00:02:31,395
它表示一层中一个神经元不失活的概率

45
00:02:31,395 --> 00:02:36,550
因此 可以对每一层设定不同的留存率

46
00:02:36,550 --> 00:02:38,490
第一层中

47
00:02:38,490 --> 00:02:42,460
W1权值矩阵是3×7

48
00:02:42,460 --> 00:02:46,120
第二层W2是7×7

49
00:02:46,120 --> 00:02:49,680
第三层W3是7×3 以此类推

50
00:02:49,680 --> 00:02:53,205
很明显W2是最大的权值矩阵

51
00:02:53,205 --> 00:02:55,500
因为它的参数最多

52
00:02:55,500 --> 00:02:58,195
达到了7×7

53
00:02:58,195 --> 00:03:01,605
所以为了让这里不容易发生过拟合

54
00:03:01,605 --> 00:03:03,600
可能对于这一层

55
00:03:03,600 --> 00:03:05,205
我猜是第二层

56
00:03:05,205 --> 00:03:08,490
你可以设定一个相对低的留存率

57
00:03:08,490 --> 00:03:10,435
0.5

58
00:03:10,435 --> 00:03:13,825
而对于其他你不太担心会发生过拟合的层

59
00:03:13,825 --> 00:03:15,080
你可以设定一个更高的留存率

60
00:03:15,080 --> 00:03:18,255
比如0.7

61
00:03:18,255 --> 00:03:22,715
如果某一层我们完全不担心会过拟合

62
00:03:22,715 --> 00:03:25,240
你可以把留存率设定为1.0

63
00:03:25,240 --> 00:03:30,725
为清楚起见 我把这些数字用紫色框起来

64
00:03:30,725 --> 00:03:34,635
可以看到不同层有不同的留存率

65
00:03:34,635 --> 00:03:39,100
需要注意的是留存率1.0表示 你保留了每一个神经元

66
00:03:39,100 --> 00:03:41,855
在这一层你并没有使用dropout

67
00:03:41,855 --> 00:03:44,730
但是对于那些容易发生过拟合的层

68
00:03:44,730 --> 00:03:46,660
也就是那些有许多参数的层

69
00:03:46,660 --> 00:03:51,600
为了达到更好的效果 你可以设定一个较小的留存率

70
00:03:51,600 --> 00:03:53,070
这就好像

71
00:03:53,070 --> 00:03:54,910
你想使用L2正则对某些层进行更严格的正则化时

72
00:03:54,910 --> 00:03:57,960
对参数λ进行起始设定

73
00:03:57,960 --> 00:04:02,715
从技术上来说 你也可以在输入层上使用dropout

74
00:04:02,715 --> 00:04:07,295
随机的选择一个或几个输入特征进行组合

75
00:04:07,295 --> 00:04:11,580
但是在实践中 通常不会这样做

76
00:04:11,580 --> 00:04:15,270
最常见的做法是将这一层的留存率设为1.0

77
00:04:15,270 --> 00:04:17,985
当然你也可以设置一个较高的值 比如0.9

78
00:04:17,985 --> 00:04:22,740
但一般不会让一半的特征输入失活

79
00:04:22,740 --> 00:04:25,665
所以对于输入层 如果你要使用dropout

80
00:04:25,665 --> 00:04:32,030
通常是把它设置为一个接近1的数

81
00:04:32,030 --> 00:04:33,450
总结一下

82
00:04:33,450 --> 00:04:36,330
如果你觉得某一层比其他层更容易发生过拟合

83
00:04:36,330 --> 00:04:40,320
就可以给这一层设置更低的留存率

84
00:04:40,320 --> 00:04:41,430
这样的缺点是在交叉验证 (网格) 搜索时

85
00:04:41,430 --> 00:04:44,955
会有更多的超参数 (运行会更费时)

86
00:04:44,955 --> 00:04:48,525
另一个选择就是对一些层使用dropout (留存率相同)

87
00:04:48,525 --> 00:04:50,460
而另一些不使用

88
00:04:50,460 --> 00:04:52,630
这样的话

89
00:04:52,630 --> 00:04:55,910
就只有一个超参数了

90
00:04:55,910 --> 00:04:59,070
在最后总结之前 再说几个实际使用时值得注意的

91
00:04:59,070 --> 00:05:03,850
最早对dropout技术的成功应用 许多是在计算机视觉领域

92
00:05:03,850 --> 00:05:05,075
在这个领域中

93
00:05:05,075 --> 00:05:06,890
它的输入层向量维度非常大

94
00:05:06,890 --> 00:05:11,275
因为要包含每个像素点的值 几乎不可能有足够的数据

95
00:05:11,275 --> 00:05:14,710
因此dropout在计算机视觉领域使用非常频繁

96
00:05:14,710 --> 00:05:18,035
有些研究人员总是使用它

97
00:05:18,035 --> 00:05:19,750
几乎已经成为一种默认了

98
00:05:19,750 --> 00:05:24,866
但需要记住dropout是一种正则化技术

99
00:05:24,866 --> 00:05:27,010
目的是防止过拟合

100
00:05:27,010 --> 00:05:30,880
所以 除非我的算法已经过拟合了

101
00:05:30,880 --> 00:05:33,250
我是不会考虑使用dropout的

102
00:05:33,250 --> 00:05:36,557
所以相对计算机视觉领域

103
00:05:36,557 --> 00:05:38,320
它在其他应用领域使用会少一些

104
00:05:38,320 --> 00:05:40,600
因为一般没有足够的数据

105
00:05:40,600 --> 00:05:42,090
几乎总是发生过拟合

106
00:05:42,090 --> 00:05:46,425
这才导致一些计算机视觉专家特别依赖dropout

107
00:05:46,425 --> 00:05:52,498
但这并不表示其他领域也如此

108
00:05:52,498 --> 00:06:00,490
dropout的另一个缺点是让代价函数J 变得不那么明确

109
00:06:00,490 --> 00:06:06,635
因为每一次迭代 都有一些神经元随机失活

110
00:06:06,635 --> 00:06:10,855
所以当你去检验梯度下降算法表现的时候

111
00:06:10,855 --> 00:06:14,590
你会发现很难确定代价函数是否已经定义的足够好

112
00:06:14,590 --> 00:06:20,365
(随着迭代 值不断变小)

113
00:06:20,365 --> 00:06:24,625
这是因为你对代价函数J 的定义不明确

114
00:06:24,625 --> 00:06:27,395
或者难以计算

115
00:06:27,395 --> 00:06:30,160
因此就不能用绘图的方法去调试错误了

116
00:06:30,160 --> 00:06:32,010
像这样的图

117
00:06:32,010 --> 00:06:34,805
通常这个时候我会关闭dropout

118
00:06:34,805 --> 00:06:37,060
把留存率设为1

119
00:06:37,060 --> 00:06:40,885
然后再运行代码并确保代价函数J 是单调递减的

120
00:06:40,885 --> 00:06:43,960
最后再打开dropout并期待

121
00:06:43,960 --> 00:06:47,035
使用dropout的时候没有引入别的错误

122
00:06:47,035 --> 00:06:49,195
我想 你需要使用其他方法

123
00:06:49,195 --> 00:06:52,060
而不是类似这种画图的方法去确保你的代码

124
00:06:52,060 --> 00:06:55,900
在使用dropout后梯度下降算法依然有效

125
00:06:55,900 --> 00:06:58,130
到此为止

126
00:06:58,130 --> 00:07:01,830
仍然有一些值得我们去了解的正则化技术

127
00:07:01,830 --> 00:07:04,480
我们将在下一段视频中介绍