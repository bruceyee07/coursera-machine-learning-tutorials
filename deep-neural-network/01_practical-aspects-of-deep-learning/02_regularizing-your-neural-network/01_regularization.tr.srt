1
00:00:00,730 --> 00:00:03,660
Eğer sinir ağınızın verinizi aşırı öğrendiğini düşünüyorsanız,

2
00:00:03,660 --> 00:00:05,840
bu yüksek değişinti probleminiz olduğu anlamına gelir.

3
00:00:05,840 --> 00:00:09,400
Bu durumda denemeniz gereken ilk şey muhtemelen düzenlileştirmedir.

4
00:00:09,400 --> 00:00:11,246
Yüksek değişinti problemini ele almanın bir diğer güvenilir yolu

5
00:00:11,246 --> 00:00:13,917
daha fazla eğitim verisi toplamaktır.

6
00:00:13,917 --> 00:00:15,869
Ama her zaman daha fazla veri toplama şansınız olmayabilir, ya da

7
00:00:15,869 --> 00:00:17,850
daha fazla veri toplamak masraflı olabilir.

8
00:00:17,850 --> 00:00:21,760
Ama düzenlileştirme çoğu zaman ağınızdaki aşırı öğrenmeyi önlemeye

9
00:00:21,760 --> 00:00:23,910
veya ağdaki hataları azaltmanıza yardımcı olacaktır.

10
00:00:23,910 --> 00:00:26,020
Dolayısıyla hadi düzenlileştirmenin nasıl çalıştığını görelim.

11
00:00:26,020 --> 00:00:28,780
Bu fikirleri S biçimli bağlanım modelinde uygulayalım.

12
00:00:28,780 --> 00:00:33,220
Hatırlayalım, S biçimli bağlanım modelinde şu şekilde tanımlanan maliyet fonksiyonunu 

13
00:00:33,220 --> 00:00:37,050
en aza indirmeye çalışıyordunuz.

14
00:00:37,050 --> 00:00:41,290
Eğitim örneklerinin ayrı ayrı hesaplanan maliyetlerinin

15
00:00:41,290 --> 00:00:45,140
toplamı. Hatırlayacaksınız ki w ve b

16
00:00:45,140 --> 00:00:48,175
S biçimli bağlanımdaki değişkenlerdir.

17
00:00:48,175 --> 00:00:54,620
W, x boyutlu bir değişken yöneyi ve b bir reel sayıdır.

18
00:00:54,620 --> 00:00:58,979
Ve S biçimli bağlanımı düzenlileştirmek için yapmanız gereken ona 

19
00:00:58,979 --> 00:01:03,154
düzenlileştirme değişkeni lambdayı eklemektir.

20
00:01:03,154 --> 00:01:04,609
Birazdan bundan biraz daha bahsedeceğim

21
00:01:04,609 --> 00:01:10,072
ama lambda bölü 2m çarpı w karenin büyüklüğü.

22
00:01:10,072 --> 00:01:15,840
Yani burada, w karenin büyüklüğü eşittir

23
00:01:15,840 --> 00:01:22,580
j eşittir 1'den nx'e kadar olan wj'lerin karelerinin toplamına. Aynı zamanda

24
00:01:22,580 --> 00:01:27,750
w değişken yöneyinin öklid büyüğü olarak, devrik w ile w çarpımı şeklinde yazılabilir. 

25
00:01:27,750 --> 00:01:31,910
Ve buna L2 düzenlileştirmesi denir.

26
00:01:33,700 --> 00:01:36,618
Çünkü burada, siz öklid büyüklüğü kullanıyorsunuz ya da

27
00:01:36,618 --> 00:01:38,877
başka bir deyişle w değişken döneyinin L2 büyüklüğü.

28
00:01:38,877 --> 00:01:41,780
Peki neden sadece w değişkenini düzenlileştiriyorsunuz?

29
00:01:41,780 --> 00:01:47,130
Neden buraya da b değişkeni için bir şey eklemiyoruz?

30
00:01:47,130 --> 00:01:51,210
Pratikte bunu yapabilirsiniz ama ben genellikle bunu kullanmıyorum.

31
00:01:51,210 --> 00:01:56,310
Çünkü değişkenlere bakarsanız, w genellikle çok büyük boyutlu

32
00:01:56,310 --> 00:02:00,159
bir değişken yöneyi. Özellikle yüksek değişinti probleminde

33
00:02:00,159 --> 00:02:02,250
belki w çok fazla değişkene sahiptir ve bu yüzden

34
00:02:02,250 --> 00:02:06,600
siz tüm değişkenleri iyi öğrenemiyor olabilirsiniz. Ama b sadece bir sayı.

35
00:02:06,600 --> 00:02:10,200
Bu yüzden neredeyse değişkenlerin tamamı b'nin aksine w yöneyinde.

36
00:02:10,200 --> 00:02:12,890
Ve eğer bu son terimi eklerseniz, pratikte

37
00:02:12,890 --> 00:02:14,040
çok büyük bir fark yaratmayacaktır.

38
00:02:14,040 --> 00:02:17,960
Çünkü b çok fazla sayıdaki değişkenden sadece biri.

39
00:02:17,960 --> 00:02:21,500
Pratikte ben genellikle onu eklemeye zahmet etmiyorum.

40
00:02:21,500 --> 00:02:22,962
Ama siz isterseniz ekleyebilirsiniz.

41
00:02:22,962 --> 00:02:27,510
Yani L2 düzenlileştirmesi en yaygın düzenlileştirme türüdür.

42
00:02:27,510 --> 00:02:32,042
Belki bazı insanların L1 düzenlileştirmesi hakkında konuştuğunu duymuşsunuzdur.

43
00:02:32,042 --> 00:02:38,422
ve bu durumda bu L2 büyüklüğü yerine

44
00:02:38,422 --> 00:02:45,674
bu lambda bölü m çapı bunların toplamı terimini eklersiniz.

45
00:02:45,674 --> 00:02:49,716
Bu aynı zamanda w değişken yöneyinin L1 büyüklüğü olarak da adlandırılır.

46
00:02:49,716 --> 00:02:52,843
yani orada küçük alt indis 1 gibi.

47
00:02:52,843 --> 00:02:58,050
Ve sanırım paydaya m veya 2m koymak sadece ölçeği değiştirir.

48
00:02:58,050 --> 00:03:03,020
Eğer siz L1 düzenlileştirmesi kullanırsanız, w yöneyi seyrek olacaktır.

49
00:03:03,020 --> 00:03:08,040
Yani bu w yöneyinde çok fazla sıfır olduğu anlamına geliyor.

50
00:03:08,040 --> 00:03:11,700
Ve bazıları diyor ki bu modeli sıkıştırmaya yardım eder, çünkü

51
00:03:11,700 --> 00:03:16,140
değişkenlerin bir kısmı sıfır ve bu yüzden modeli depolamak için daha az hafızaya ihtiyacınız var.

52
00:03:16,140 --> 00:03:19,850
Buna karşın ben modeli seyrekleştirmek için L1 düzenlileştirmesi kullanmanın pratikte

53
00:03:19,850 --> 00:03:20,870
küçük bir faydası olduğunu sonucuna vardım.

54
00:03:20,870 --> 00:03:23,870
Bu yüzden ben onun çok fazla kullanılmadığını düşünüyorum, en azından

55
00:03:23,870 --> 00:03:26,520
modeli sıkıştırmak amacıyla.

56
00:03:26,520 --> 00:03:28,472
Ve insanlar sinir ağlarını eğitirken,

57
00:03:28,472 --> 00:03:31,423
L2 düzenlileştirmesini çok daha yaygın olarak kullanıyorlar.

58
00:03:31,423 --> 00:03:34,301
Pardon, formüldeki küçük bir gösterim hatasını düzeltiyorum.

59
00:03:34,301 --> 00:03:35,929
Son bir detay,

60
00:03:35,929 --> 00:03:42,823
Buradaki lambdaya düzenlileştirme değişkeni denir.

61
00:03:45,267 --> 00:03:48,172
Ve onu genellikle geliştirme kümesi ya da

62
00:03:48,172 --> 00:03:50,021
holdout çapraz geçerleme metodu kullanarak belirlersiniz.

63
00:03:50,021 --> 00:03:53,274
Bu metotda çeşitli değerleri deneyip ve hangisinin 

64
00:03:53,274 --> 00:03:57,662
eğitim verinizde daha başarılı olduğuna bakar ve aynı zamanda 

65
00:03:57,662 --> 00:04:01,007
değişkenlerinin L2 büyüklüğünü olabildiğince küçültmeye çalışırsınız.

66
00:04:01,007 --> 00:04:03,088
Bu aşırı öğrenmeyi engellemeye yardım eder.

67
00:04:03,088 --> 00:04:07,165
Yani lamba ayarlamanız gereken başka bir hiper değişkendir.

68
00:04:07,165 --> 00:04:09,550
Ve burada, programla alıştırmaları için

69
00:04:09,550 --> 00:04:14,250
lambda Python programlama dilinde ayrılmış bir kelimedir.

70
00:04:14,250 --> 00:04:18,300
Bu yüzden programla alıştırmasında python diliyle 

71
00:04:19,340 --> 00:04:23,690
çakışmamak amacıyla lambda yerine lambd kullanacağız.

72
00:04:23,690 --> 00:04:27,740
Yani düzenlileştirme değişkeni olarak lambd kullnacağız.

73
00:04:29,190 --> 00:04:33,320
Sonuç olarak S biçimli bağlanımlarda L2 düzenlileştirmesinin nasıl yapıldığını öğrendik.

74
00:04:33,320 --> 00:04:35,280
Peki ya sinir ağları için?

75
00:04:35,280 --> 00:04:39,789
Bir sinir ağında maliyet fonksiyonunuz

76
00:04:39,789 --> 00:04:44,621
w[1] ve b[1]'den w[L] ve b[L]'e kadar tüm değişkenlerinize bağlıdır.

77
00:04:44,621 --> 00:04:48,906
Burada L sinir ağınızdaki katman sayısıdır.

78
00:04:48,906 --> 00:04:54,129
Ve maliyet fonksiyonu, m tane eğitim

79
00:04:54,129 --> 00:04:58,066
örneğiniz için yitimlerin toplamına eşittir.

80
00:04:58,066 --> 00:05:03,087
Ve düzenlileştirmede, siz lambda bölü

81
00:05:03,087 --> 00:05:10,190
2m çarpı w değişkenlerinin toplamını eklersiniz,

82
00:05:10,190 --> 00:05:14,857
ya da buna onun kare büyüklüğü de diyebiliriz.

83
00:05:14,857 --> 00:05:19,749
Dizeyin kare büyüklüğü

84
00:05:19,749 --> 00:05:23,922
her i ve j değişkeni için

85
00:05:23,922 --> 00:05:29,250
dizeydeki her bir elemanın karelerinin toplamıdır.

86
00:05:29,250 --> 00:05:31,248
Eğer toplamın endekslerini isterseniz,

87
00:05:31,248 --> 00:05:35,253
bu i eşittir 1'den n[l-1]'a kadar

88
00:05:35,253 --> 00:05:38,537
ve bu j eşittir 1'den n[l]'e kadardır.

89
00:05:38,537 --> 00:05:44,497
Çünkü w dizeyinin boyutları n[l-1] ve n[l]'dir.

90
00:05:44,497 --> 00:05:51,320
Ve bunlar l-1 ve l'inci katmanlardaki eleman sayılarıdır.

91
00:05:51,320 --> 00:05:57,447
Ve bu dizey büyüklüğüne Frobenius

92
00:05:57,447 --> 00:06:03,710
büyüklüğü denir ve alt indiste F ile gösterilir.

93
00:06:03,710 --> 00:06:07,266
Lineer cebirin gizemli teknik sebeplerinden dolayı

94
00:06:07,266 --> 00:06:10,491
buna L2 büyüklüğü denmiyor.

95
00:06:10,491 --> 00:06:14,620
Onun yerine dizeyin Frobenius büyüklüğü deniyor.

96
00:06:14,620 --> 00:06:16,980
Biliyorum ona dizeyin L2 büyüklüğü demek kulağa daha doğal geliyor,

97
00:06:16,980 --> 00:06:21,760
ama gerçekten bilmenizin gerekmediği sebeplerle

98
00:06:21,760 --> 00:06:24,090
uzlaşım için Frobenius büyüklüğü olarak adlandırılıyor.

99
00:06:24,090 --> 00:06:27,232
O sadece dizeyin elemanlarının karelerinin toplamı anlamına gelir.

100
00:06:27,232 --> 00:06:30,060
Peki bunun için bayır inişini nasıl uygularsınız?

101
00:06:30,060 --> 00:06:35,343
Bundan önce, dw'yi geri yayma kullanarak hesaplıyorduk.

102
00:06:35,343 --> 00:06:40,626
Geri yayma bize J'nin w'ye göre 

103
00:06:40,626 --> 00:06:46,166
ya da herhangi w[l]'e göre kısmı türevini veriyordu.

104
00:06:46,166 --> 00:06:52,995
Ve sonra w[l]'yi w[l] eksi öğrenme oranı çarpı dw olarak güncelliyorduk.

105
00:06:52,995 --> 00:06:57,890
Yani ekstra düzenlileştirme terimi eklenmeden önce böyle yapıyorduk.

106
00:06:57,890 --> 00:07:02,941
Şimdi ise bu düzenlileştirme terimini eklediğimiz için, 

107
00:07:02,941 --> 00:07:07,643
yapmanız gereken dw'yi alıp ona lambda bölü m çarpı w eklemektir.

108
00:07:07,643 --> 00:07:10,760
Ve sonra eskiden olduğu gibi bu güncellemeyi hesaplıyoruz.

109
00:07:10,760 --> 00:07:14,829
Ve öyle görünüyor ki bu yeni dw[l] tanımıyla birlikte

110
00:07:14,829 --> 00:07:19,315
maliyet fonksiyonunuzun değişkenlerinize

111
00:07:19,315 --> 00:07:23,385
göre kısmı türevi olma özelliğini koruyor.

112
00:07:23,385 --> 00:07:27,980
,Şimdi siz dw'nin tanımına ekstra bir düzenlileştirme terimi eklediniz,

113
00:07:29,260 --> 00:07:33,990
ve bu sebeple L2 düzenlileştirmesine bazen

114
00:07:33,990 --> 00:07:36,730
ağırlık sönümü(weight decay) de denir.

115
00:07:36,730 --> 00:07:42,348
Yani eğer ben bu dw[l] tanımını alıp burada yerine koyarsam,

116
00:07:42,348 --> 00:07:47,012
o zaman siz güncellemenin w[l] eşittir w[l] çarpı

117
00:07:47,012 --> 00:07:51,994
öğrenme oranı alfa çarpı geri yayılımdan gelen terim

118
00:07:54,311 --> 00:08:02,816
artı lambda bölü m çarpı w[l] haline geldiğini göreceksiniz.

119
00:08:02,816 --> 00:08:04,431
Tabii burada eksi olacak.

120
00:08:04,431 --> 00:08:09,382
Ve bu da w[l] eksi alfa 

121
00:08:09,382 --> 00:08:14,494
çarpı lambda bölü m çarpı w[l] - alfa çarpı

122
00:08:14,494 --> 00:08:18,822
geri yayılımdan gelen terime eşittir. 

123
00:08:18,822 --> 00:08:22,324
Ve bu terim gösteriyor ki w[l] dizeyiniz ne olursa olsun

124
00:08:22,324 --> 00:08:25,480
onu bir parça küçülteceksiniz, değil mi?

125
00:08:25,480 --> 00:08:28,270
Bu aslında w dizeyini alıp onu 1 eksi

126
00:08:28,270 --> 00:08:33,030
alfa çarpı lambda bölü m ile çarpmakla aynı şey.

127
00:08:33,030 --> 00:08:38,279
Siz aslında w dizeyini alıp ondan alfa çarpı lamba bölü m tane bunu çıkarıyorsunuz.

128
00:08:38,279 --> 00:08:41,130
Bu w dizeyini birden biraz daha küçük olan 

129
00:08:41,130 --> 00:08:43,528
bu terimle çarpmak gibi bir şey.

130
00:08:43,528 --> 00:08:48,688
Yani bu L2 düzenlileştirmesine neden ağırlık sönümü dendiğini açıklıyor.

131
00:08:48,688 --> 00:08:53,716
Çünkü yine aynı normal bayır inişinde olduğu gibi

132
00:08:53,716 --> 00:08:59,260
w'yi ondan alfa çarpı geri yayılımdan gelen eğim çıkararak güncelliyorsunuz.

133
00:08:59,260 --> 00:09:04,616
Ama bu sefer w'yi aynı zamanda birden biraz küçük olan

134
00:09:04,616 --> 00:09:08,324
bu terim ile çarpıyoruz.

135
00:09:08,324 --> 00:09:11,782
Dolayısıyla L2 düzenlileştirmesinin diğer adı ağırlık sönümüdür.

136
00:09:11,782 --> 00:09:15,641
Ben gerçekten bu ismi kullanmayacağım, ama böyle

137
00:09:15,641 --> 00:09:21,030
adlandırılmasındaki sezgi buradaki ilk terimdir ve o buna eşittir.

138
00:09:21,030 --> 00:09:25,620
Yani siz yalnızca ağırlık dizeyini birden küçük bir sayıyla çarpıyorsunuz.

139
00:09:25,620 --> 00:09:28,511
Böylelikle L2 düzenlileştirmesinin sinir ağlarında nasıl uygulandığı öğrendiniz.

140
00:09:29,545 --> 00:09:32,796
Şimdi, [birisi] bana şu soruyu sordu. Hey Andrew,

141
00:09:32,796 --> 00:09:35,675
düzenlileştirme aşırı öğrenmeyi neden önlüyor?

142
00:09:35,675 --> 00:09:37,462
Hadi bir sonraki videoya bakalım,

143
00:09:37,462 --> 00:09:41,805
ve düzenlileştirmenin aşırı öğrenmeyi nasıl engellediğine dair daha fazla bilgi sahibi olalım.