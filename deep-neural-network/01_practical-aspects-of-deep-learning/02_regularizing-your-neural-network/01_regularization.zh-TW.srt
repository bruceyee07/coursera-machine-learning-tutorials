1
00:00:00,730 --> 00:00:03,660
如果您懷疑您的神經網路
過適於您的資料

2
00:00:03,660 --> 00:00:05,840
也就是您有高變異問題

3
00:00:05,840 --> 00:00:09,400
您第一件事情應該
要試著做的也許是正則化

4
00:00:09,400 --> 00:00:11,246
另一種方式來解決高變異

5
00:00:11,246 --> 00:00:13,917
是得到更多的資料
這也是相當可靠的方式

6
00:00:13,917 --> 00:00:15,869
但您不可能永遠獲得
更多的資料, 或者

7
00:00:15,869 --> 00:00:17,850
獲得更多的資料可能很貴

8
00:00:17,850 --> 00:00:21,760
但加入正則化通常會
幫助您避免過適問題

9
00:00:21,760 --> 00:00:23,910
或者減低您網路的錯誤

10
00:00:23,910 --> 00:00:26,020
所以讓我們來看正則化如何作用

11
00:00:26,020 --> 00:00:28,780
讓我們用羅吉斯迴歸分析
來發展這個觀念

12
00:00:28,780 --> 00:00:33,220
記得在羅吉斯迴歸分析中
您試著最小化成本函數 J

13
00:00:33,220 --> 00:00:37,050
定義成這個成本函數

14
00:00:37,050 --> 00:00:41,290
總和您的訓練例子的
個別預估值損失在

15
00:00:41,290 --> 00:00:45,140
不同的例子上
您記得 w 跟

16
00:00:45,140 --> 00:00:48,175
b 在羅吉斯迴歸分析中
是參數

17
00:00:48,175 --> 00:00:54,620
所以 w 是 nx 維度參數向量
 b 是一個實數

18
00:00:54,620 --> 00:00:58,979
要加上正則化到
羅吉斯迴歸分析, 您要做的是

19
00:00:58,979 --> 00:01:03,154
這件事, lambda, 
稱為正則化參數

20
00:01:03,154 --> 00:01:04,609
我們等一下會談到

21
00:01:04,609 --> 00:01:10,072
lambda / 2m 乘上 w 的範數平方

22
00:01:10,072 --> 00:01:15,840
這裡
w 範數平方等於

23
00:01:15,840 --> 00:01:22,580
從 j 等於 1 到 nx 總和 wj 平方
或者這也可以寫成 w

24
00:01:22,580 --> 00:01:27,750
轉置 w, 
這只是參數 w 向量的歐氏範數平方

25
00:01:27,750 --> 00:01:31,910
而這稱為 L2 正則化

26
00:01:33,700 --> 00:01:36,618
因為這裡
您使用的是歐氏範數, 

27
00:01:36,618 --> 00:01:38,877
也是稱為 L2 範數
對於參數向量 w

28
00:01:38,877 --> 00:01:41,780
為什麼您只有正則化參數 w

29
00:01:41,780 --> 00:01:47,130
為什麼我們不把 b 也加進去？

30
00:01:47,130 --> 00:01:51,210
實作時, 您可以這樣做
但我通常省略它

31
00:01:51,210 --> 00:01:56,310
因為如果您看您的參數
w 通常是很高維度

32
00:01:56,310 --> 00:02:00,159
參數向量
特別是高變異問題

33
00:02:00,159 --> 00:02:02,250
也許 w 就是有很多參數

34
00:02:02,250 --> 00:02:06,600
所以您無法配適所有的參數
而 b 只是一個數

35
00:02:06,600 --> 00:02:10,200
幾乎所有的參數
在 w 而不在 b

36
00:02:10,200 --> 00:02:12,890
而如果您加上最後一項
實作上

37
00:02:12,890 --> 00:02:14,040
它不會造成多大不同

38
00:02:14,040 --> 00:02:17,960
因為 b 只是很大數目的參數之一

39
00:02:17,960 --> 00:02:21,500
實作上
我通常不去包含它

40
00:02:21,500 --> 00:02:22,962
但如果您要加的話也可以

41
00:02:22,962 --> 00:02:27,510
所以 L2 正則化是
最常見的正則化

42
00:02:27,510 --> 00:02:32,042
您也許也聽過人們談到 L1 正則化

43
00:02:32,042 --> 00:02:38,422
也就是當您加的時候
與其加這個 L2 範數

44
00:02:38,422 --> 00:02:45,674
您取而代之用這個項
lambda/m 總和這個

45
00:02:45,674 --> 00:02:49,716
這也是稱為 L1 範數的參數向量 w

46
00:02:49,716 --> 00:02:52,843
所以這裡有小小下標 1
對吧?

47
00:02:52,843 --> 00:02:58,050
我猜不管您放 m 或者 2m 在分母
都只是一個比例常數

48
00:02:58,050 --> 00:03:03,020
如果您用 L1 正則化
那 w 會變得比較稀疏

49
00:03:03,020 --> 00:03:08,040
這個意思是 w  
向量會有很多的 0 在裡面

50
00:03:08,040 --> 00:03:11,700
有些人說這樣可以
壓縮這個模型， 因為

51
00:03:11,700 --> 00:03:16,140
很多參數為 0 時
您可以用比較少的記憶體來儲存這個模型

52
00:03:16,140 --> 00:03:19,850
雖然我發現， 實作時
 L1 正規化讓您的模型稀疏

53
00:03:19,850 --> 00:03:20,870
只會幫助一點點

54
00:03:20,870 --> 00:03:23,870
所以我並不常用它
至少不會

55
00:03:23,870 --> 00:03:26,520
為了壓縮您的模型用它

56
00:03:26,520 --> 00:03:28,472
而當人們訓練網路時

57
00:03:28,472 --> 00:03:31,423
L2 正則化就是
用的多很多

58
00:03:31,423 --> 00:03:34,301
抱歉, 更正一些記號

59
00:03:34,301 --> 00:03:35,929
最後一點細節

60
00:03:35,929 --> 00:03:42,823
這裡的 lambda 是稱為正則化參數

61
00:03:45,267 --> 00:03:48,172
通常您使用開發集
來設定這個

62
00:03:48,172 --> 00:03:50,021
使用交叉驗證集

63
00:03:50,021 --> 00:03:53,274
當您試著不同的值
看哪一個作用最好

64
00:03:53,274 --> 00:03:57,662
權衡了在訓練集作用得很好
以及

65
00:03:57,662 --> 00:04:01,007
設定了 您參數的L2 範數較小

66
00:04:01,007 --> 00:04:03,088
這樣會避免過適

67
00:04:03,088 --> 00:04:07,165
所以 lambda 是另一個超參數
您必須調整

68
00:04:07,165 --> 00:04:09,550
順便說一下
對於程式作業

69
00:04:09,550 --> 00:04:14,250
lambda 是一個保留關鍵字
在 Python 程式語言

70
00:04:14,250 --> 00:04:18,300
所以在程式練習中
我們用 lambd

71
00:04:19,340 --> 00:04:23,690
少了一個 a, 
為了不跟 Python 保留關鍵字衝突

72
00:04:23,690 --> 00:04:27,740
所以我們用 lambd 來代表
lambda 正則化參數

73
00:04:29,190 --> 00:04:33,320
這是您如何使用
L2 正則化在羅吉斯迴歸分析上

74
00:04:33,320 --> 00:04:35,280
如何用在神經網路上 ？

75
00:04:35,280 --> 00:04:39,789
在神經網路上, 您有一個
成本函數是一個函數

76
00:04:39,789 --> 00:04:44,621
用所有的參數, w[1]
b[1] 直到 w[L], b[L]

77
00:04:44,621 --> 00:04:48,906
而大寫 L 是您神經網路的層數

78
00:04:48,906 --> 00:04:54,129
所以成本函數是這個
對於損失做總和

79
00:04:54,129 --> 00:04:58,066
您 m 個訓練例子的總和

80
00:04:58,066 --> 00:05:03,087
而要加入正規化
您加入 lambda 除以

81
00:05:03,087 --> 00:05:10,190
2m of 總和對於所有您個參數 W
您的參數矩陣 w

82
00:05:10,190 --> 00:05:14,857
我們也稱為範數平方

83
00:05:14,857 --> 00:05:19,749
而矩陣的範數
也就是

84
00:05:19,749 --> 00:05:23,922
範數平方定義為
i 總和 j 總和

85
00:05:23,922 --> 00:05:29,250
所有矩陣的元素平方

86
00:05:29,250 --> 00:05:31,248
如果您要這個總和的索引

87
00:05:31,248 --> 00:05:35,253
這一個是總和從 i = 1 到 n[l-1]

88
00:05:35,253 --> 00:05:38,537
總和從 j=1 到 n[l]

89
00:05:38,537 --> 00:05:44,497
因為 w 是一個 n[l-1] 乘
n[l] 維度向量

90
00:05:44,497 --> 00:05:51,320
而這些數字是
 l - 1 層跟 l 層的單元數目

91
00:05:51,320 --> 00:05:57,447
這個矩陣範數
實際上也稱為弗比尼斯

92
00:05:57,447 --> 00:06:03,710
範數在矩陣上
記一個 F 在下標

93
00:06:03,710 --> 00:06:07,266
為了神秘的線性代數
技術上的原因

94
00:06:07,266 --> 00:06:10,491
這不稱為矩陣 L2 範數

95
00:06:10,491 --> 00:06:14,620
而是稱為
矩陣弗比尼斯範數

96
00:06:14,620 --> 00:06:16,980
我知道聽起來稱它為
矩陣 L2 範數比較自然

97
00:06:16,980 --> 00:06:21,760
但為了神秘的理由
您不需要知道

98
00:06:21,760 --> 00:06:24,090
按照慣例，這就稱為弗比尼斯範數

99
00:06:24,090 --> 00:06:27,232
它只是對於
矩陣元素平方的總和

100
00:06:27,232 --> 00:06:30,060
所以您如何用它
來建置梯度下降

101
00:06:30,060 --> 00:06:35,343
之前, 我們會用
反向傳播來完成 dw

102
00:06:35,343 --> 00:06:40,626
而反向傳播
會給我們偏導數

103
00:06:40,626 --> 00:06:46,166
of J 相對於 w
或者應該說 w 對於任何 [l]

104
00:06:46,166 --> 00:06:52,995
然後您更新 w[l]
為 w[l] 減學習率乘上 d

105
00:06:52,995 --> 00:06:57,890
所以這是加上額外的正則化項目之前的目標

106
00:06:57,890 --> 00:07:02,941
現在我們加入這個
正則化項目到這個目標

107
00:07:02,941 --> 00:07:07,643
您要做的是拿 dw 
加入這個 lambda /m 乘 w

108
00:07:07,643 --> 00:07:10,760
而然後您只要計算這個更新值
像之前一樣

109
00:07:10,760 --> 00:07:14,829
實際上有了這個
新的 dw[l] 定義

110
00:07:14,829 --> 00:07:19,315
這個新的 dw[l] 仍然是
正確的導數定義

111
00:07:19,315 --> 00:07:23,385
對於您的成本函數
相對於您的參數

112
00:07:23,385 --> 00:07:27,980
即使您現在在後面加入這個
額外的正則化項目

113
00:07:29,260 --> 00:07:33,990
而為了這個原因
L2 正規化有時候也

114
00:07:33,990 --> 00:07:36,730
稱為權重衰減

115
00:07:36,730 --> 00:07:42,348
如果我拿這個 dw[l] 的定義
然後代入這個公式

116
00:07:42,348 --> 00:07:47,012
然後您會看到這個更新是
 w[l] = w[l] 乘上(應該是減)

117
00:07:47,012 --> 00:07:51,994
學習率 alpha 乘
這個從反向傳播來的東西

118
00:07:54,311 --> 00:08:02,816
加 lambda / m 乘上 w[l]

119
00:08:02,816 --> 00:08:04,431
丟一個減號在這裡

120
00:08:04,431 --> 00:08:09,382
所以這等於 w[l] 減 alpha

121
00:08:09,382 --> 00:08:14,494
lambda / m 乘 w[l] 減 alpha 乘

122
00:08:14,494 --> 00:08:18,822
您從反向傳播來的東西

123
00:08:18,822 --> 00:08:22,324
而這一項表示
不管 w[l] 矩陣為何

124
00:08:22,324 --> 00:08:25,480
您將會讓它
小一點點, 對吧?

125
00:08:25,480 --> 00:08:28,270
這個實際上是如果您
拿這個矩陣 w

126
00:08:28,270 --> 00:08:33,030
你將它乘以 1- alpha lambda/m

127
00:08:33,030 --> 00:08:38,279
您其實是拿這個矩陣 w 
減去 alpha lambda /m 乘上這個

128
00:08:38,279 --> 00:08:41,130
像是您用這個數字乘上矩陣 w

129
00:08:41,130 --> 00:08:43,528
這會是比 1 小一點點

130
00:08:43,528 --> 00:08:48,688
所以這是為什麼 L2 範數正則化
也稱為權重衰減

131
00:08:48,688 --> 00:08:53,716
因為它像一般
梯度下降, 而您更新

132
00:08:53,716 --> 00:08:59,260
w 用減去 alpha 乘上
從反向傳播得到的一般梯度

133
00:08:59,260 --> 00:09:04,616
但現在您也將
w 乘上這個東西

134
00:09:04,616 --> 00:09:08,324
這個會比 1 小一點

135
00:09:08,324 --> 00:09:11,782
所以另外一個名稱給
 L2 正則化是權重衰減

136
00:09:11,782 --> 00:09:15,641
我將不會使用這個名稱
但直觀上而言

137
00:09:15,641 --> 00:09:21,030
為什麼稱為權重衰減是
因為第一個項目等於這個

138
00:09:21,030 --> 00:09:25,620
所以您將這個權重矩陣
乘上一個數子比 1 稍小一點

139
00:09:25,620 --> 00:09:28,511
所以這是您如何建置
L2 正則化在神經網路

140
00:09:29,545 --> 00:09:32,796
現在一個問題是
嗨, Andrew

141
00:09:32,796 --> 00:09:35,675
為什麼正則化
能夠避免過適

142
00:09:35,675 --> 00:09:37,462
讓我們來看下一段影片

143
00:09:37,462 --> 00:09:41,805
來得到一些直觀
正則化如何避免過適