除了 L2 正則化以及 dropout 正則化 還有一些技術來減低
您神經網路過適的問題 讓我們看看讓我們看看 假設您配適一個貓的分類器 如果您有過適問題,
獲得更多的訓練資料會有所幫助，但獲取更多 訓練資料可能會很昂貴
有時候您就是沒辦法獲得更多資料 但您可以做的是
擴增您的訓練集像是拿一個影像 舉個例子
水平翻轉 將它也加入訓練集 現在不只是這一個例子
在您的訓練集 您可以加入這個到您的訓練例子 所以將影像水平翻轉 您可以將您的訓練集加倍 因為您的訓練集
現在有點多餘不像您 重新收集完全
新的獨立的例子 但您可以這樣做不需要
付任何費用為了 得到更多貓的照片 除了水平翻轉之外 您也可以隨機剪裁影像 這裡是我們旋轉
跟隨機放大這個影像 這還是像一隻貓 所以用隨機扭曲跟轉化影像
您可以 擴增您的資料集
增加假的新例子 再次, 這種額外假的例子
並不會增加 像是您去獲得全新的
獨立貓的例子的資訊 但因為您幾乎可以免費的做這個，除了 一些計算的成本 這個可以是最廉價的方式來
給予您更多的資料 因此也是一種正則化來減低過適 而透過綜合這些例子像這樣
您其實告訴您的演算法 如果一個東西是貓
水平翻轉後還是貓 請注意我並沒有垂直翻轉它 因為也許我們並不想要上下顛倒的貓, 對吧? 然後也許是隨機放大部份影像 這樣應該也還是一隻貓 對於光學文字辨識您也可以
將您的資料集將字元 實行隨機旋轉跟扭曲 所以如果您加入這些到您的訓練集 這些還是字元 4 為了解釋
我應用了很強的扭曲 所以這看起來很彎曲,
實作時您不需要將 4 扭曲 得這麼厲害，但只是些微的扭曲
就像我這邊做的 用這個例子讓您明白, 對吧? 但實作時通常用比較些微的扭曲 因為這看起來真的是很彎曲的 4 所以資料擴增可以用來
做正則化的技巧 實際上類似於正則化 還有一個常用的其他技術
通常稱為早期停止 您要做的是當您跑梯度下降時您會畫 您的不管是訓練誤差 您會使用 0 與 1 分類誤差在訓練集 或者畫成本函數 J 最佳化 那應該是單調遞減像這樣
是吧? 當您訓練時, 希望 您訓練時您的成本函數 J 應該要遞減 使用提前停止
您要做的是您畫這個圖 您也畫開發集誤差 再次, 這可以是開發集上的分類錯誤, 或者一些 像是成本函數, 像是羅吉斯損失
或者對數損失在開發集上 您會發現到您的開發集誤差
通常會降低一會兒 然後會開始增加 而提前停止是
您會說 似乎您的神經網路
在這個循環做得最好 我們希望在訓練神經網路一半時停止 然後採用這個開發集誤差
不管到達何值 所以為什麼這樣可行 ？ 當您還沒跑很多次循環
對於您的 神經網路時
您的參數會接近於 0 因為隨機初始化也許
初始 w 為小的隨機值 在您訓練很久之前,
w 還是很小 而當您開始循環,
w 會越來越大, 直到 也許您有太大的參數 w 對於您的神經網路 而提早停止做的是
利用半途停止您會只有 中度大小的 w 而類似於 L2 正則化使用
選擇一個神經網路比較小的 範數對於您的參數 w,
希望您的神經網路不會過適 而這個提早停止術語
來自於您會 提前停止訓練您的神經網路 我有時候會使用提早停止
當訓練一個神經網路 但它有一個缺點
讓我來解釋 我想機器學習
包含了很多不同的步驟 其一是您要一個演算法
來最佳化成本函數 J 我們有不同的工具來使用
像是梯度下降 然後我們將會談到的其他的像是
momentum 跟 RMS prop 跟 Adam 等等 但經過最佳化以後的成本函數 J
您希望不要過適 我們有一些工具來使用
像是正則化 取得更多資料等等 在機器學習, 我們已經
有很多超參數像大浪襲來 從可能的演算法中來做選擇
已經很複雜的 所以我發現機器學習
比較容易想成 當您有一組工具來
最佳化成本函數 J 而當您注意在
最佳化成本函數 J 您要注意的是找到 w 跟 b
讓 J(w,b) 越小越好 您不去想其他的事
只是要減低這個 而不去過適它
完全是兩碼子的事情 換句話說
減低變異 而當您要處理它時
您有不同的工具來使用 而這個準則有時候
稱為正交法 而這種想法是您要
一次做一件事 我會談到正交法
在以後的影片中 如果您還沒完全搞明白這個觀念
不用擔心 但對我而言這種提前停止
的最主要缺點是 這會耦合這兩件事 所以您不再
將兩件事單獨作業 因為將梯度下降提前停止 您有點破壞了您正在做的
最佳化成本函數 J 因為您不再好好做
減低成本函數 J 的工作 您有點做不好那個 然後同時您又試著不要過適 所以與其使用不同的工具
解決兩個問題 您用了混合了這兩種在一起 而這會使得 您可以試的種種方式中
更加複雜去想用哪些 與其使用提早停止,
一種替代方式是使用 L2 正則化 然後您可以訓練神經網路越久越好 我發現這會使得搜尋
超參數的空間比較容易分解 比較容易搜尋 但缺點是您也許要試很多的 正規化參數 lambda 這會搜尋很多 lambda 的值
會產生很多 昂貴的計算 而提前停止的好處是
跑梯度下降 一次, 您試出小的 w 值, 中間的 w 值跟 大的 w 值,
不需要試很多 L2 正則化 超參數 lambda 如果這些觀念完全聽不明白
沒有關係 我們會談到正交的細節 在以後的影片
我想到時會更有感覺 儘管有它的缺點，很多人還是使用它 我個人比較喜歡
L2 正則化 試著不同的 lambda 值 這前提是您可以提供
必要的計算成本 但提前停止的確
讓您獲得類似效果 不需要明顯地試
很多不同的 lambda 值 您現在看過如何做資料擴增
以及如果您想提前 停止為了減低變異或者
避免過適在您的神經網路 下一段讓我們談談一些關於 設定您的最佳化問題來
讓您的訓練加快的技巧