1
00:00:00,730 --> 00:00:03,660
만약 신경망이 데이터에 과도하게 적합하다는 의심이 드는 경우, 

2
00:00:03,660 --> 00:00:05,840
그것은 큰 격차 문제가 있다는 것 입니다.

3
00:00:05,840 --> 00:00:09,400
당신이 가장 먼저 시도해야 할 일 중 하나는 아마 일반화(regularization)일 것 입니다.

4
00:00:09,400 --> 00:00:11,246
큰 격차 문제를 해결하기 위한 다른 방법은, 

5
00:00:11,246 --> 00:00:13,917
꽤 신뢰 가는 교육 자료를 더 많이 획득하는 것 입니다. 

6
00:00:13,917 --> 00:00:15,869
그러나 항상 많은 교육 자료를 획득할 수는 없습니다. 혹은, 

7
00:00:15,869 --> 00:00:17,850
더 많은 데이터를 얻는 데에는 비용이 더 많이 들 수 있습니다. 

8
00:00:17,850 --> 00:00:21,760
그러나 일반화를 추가하는 것은 과도한 장착을 예방하는 데에 자주 도움이 될 것 입니다. 혹은, 

9
00:00:21,760 --> 00:00:23,910
당신의 네트워크 오류를 감소시키는 데에도 도움이 될 것 입니다.

10
00:00:23,910 --> 00:00:26,020
자, 그럼 일반화가 어떻게 이루어지는지 한번 살펴볼까요?

11
00:00:26,020 --> 00:00:28,780
로지스틱 회귀를 사용하여 이 아이디어를 발전시켜 봅시다.

12
00:00:28,780 --> 00:00:33,220
로지스틱 회귀를 위해 다시 불러옵시다. 비용함수 J를 최소화 하도록 시도합니다. 

13
00:00:33,220 --> 00:00:37,050
그것은 이 비용함수로 정의되어 있습니다. 

14
00:00:37,050 --> 00:00:41,290
교육용 예 중 일부는 다른 예에서 개별 예상치의 손실로, 

15
00:00:41,290 --> 00:00:45,140
당신은 이를 로지스틱 회귀에서 w와

16
00:00:45,140 --> 00:00:48,175
b로 다시 불러오고, 이들은 매개 변수들입니다.

17
00:00:48,175 --> 00:00:54,620
그리하여, w는 x치수 파라미터 벡터이고, b는 실수입니다. 

18
00:00:54,620 --> 00:00:58,979
따라서, logistic regression에 일반화를 더하기 위해 당신이 해야할 일은 그것을

19
00:00:58,979 --> 00:01:03,154
Lambda라고 불리는 조절 매개 변수에 추가하는 것 입니다.

20
00:01:03,154 --> 00:01:04,609
그것에 대해서는 곧 더 말씀 드리겠습니다. 

21
00:01:04,609 --> 00:01:10,072
Lambda 나누기 2m 곱하기 w의 노름 곱하기 2 제곱

22
00:01:10,072 --> 00:01:15,840
그리하여 여기 w의 노름 곱하기 2 제곱 은

23
00:01:15,840 --> 00:01:22,580
∑j=1 to nx to wj제곱 또한 

24
00:01:22,580 --> 00:01:27,750
W transpose W 와같습니다. W transpose W는 프라임-벡터 W의 유클리드 표준이다.

25
00:01:27,750 --> 00:01:31,910
그리고 이것은 L2 일반화 라고 불립니다. 

26
00:01:33,700 --> 00:01:36,618
그 이유은 여기, 당신이 유클리드 정상치를 사용하거나, 

27
00:01:36,618 --> 00:01:38,877
L2표준을 벡터 w와 사용하는 것이죠.

28
00:01:38,877 --> 00:01:41,780
그럼, 당신은 왜 매개 변수 w만 일반화 하져?

29
00:01:41,780 --> 00:01:47,130
왜 우리는 여기 b에 무언가를 더 더하지 않는거져?

30
00:01:47,130 --> 00:01:51,210
실제로 이렇게 할 수 있지만, 저는 이 방법은 그냥 생략 하겠습니다. 

31
00:01:51,210 --> 00:01:56,310
그 이유는 변수를 보면 w는 보통 꽤 높은 치수의 

32
00:01:56,310 --> 00:02:00,159
측정 벡터입니다. 특히, 변동이 심한 문제가 있을 시에는
더욱 그렇습니다.

33
00:02:00,159 --> 00:02:02,250
어쩌면 w가 그저 변수가 많아서 그럴수도 있지만, 

34
00:02:02,250 --> 00:02:06,600
그렇다고 모든 파라미터들을 그냥 넣진 않습니다. 반면에, 
b는 그저 단수일 뿐입니다.

35
00:02:06,600 --> 00:02:10,200
그래서 거의 모든 매개 변수들이 b보다는 w에 있습니다.

36
00:02:10,200 --> 00:02:12,890
그리고 만약 당신이 이 마지막 용어를 추가한다면, 

37
00:02:12,890 --> 00:02:14,040
큰 차이는 없겠지만, 

38
00:02:14,040 --> 00:02:17,960
왜냐하면 b는 많은 파라미터들 중에 단지 하나의 파라미터일 뿐이기
때문입니다.

39
00:02:17,960 --> 00:02:21,500
실제로 저는 주로 그것을 포함 시키는 데에는 신경을 쓰지 않아요.

40
00:02:21,500 --> 00:02:22,962
그러나 당신이 포함 시키고 싶다면 그렇게 하셔도 됩니다.

41
00:02:22,962 --> 00:02:27,510
그래서 L2 일반화가 가장 일반적인 유형의
일반화 입니다.

42
00:02:27,510 --> 00:02:32,042
당신은 다른 사람들이 L1일반화에 대해 얘기하는 
것을 들어본 적이 있을거에요.

43
00:02:32,042 --> 00:02:38,422
그리고 그것은 이 L2 표준 대신에 더하였을 때 입니다.

44
00:02:38,422 --> 00:02:45,674
대신에 이런 요서를 씁니다.

45
00:02:45,674 --> 00:02:49,716
그리고 이것은 벡터 w의 파라미터의 L1 표준이라고 불립니다. 

46
00:02:49,716 --> 00:02:52,843
그러니까 저기 아랫쪽의 subscript 1 인거 보이시져?

47
00:02:52,843 --> 00:02:58,050
그리고 제 생각에 당신이 분모에 m을 넣었던지 아니면, 
2m을 넣었는지는 상관없이 단지 스케일링 상수일 뿐입니다.

48
00:02:58,050 --> 00:03:03,020
L1 일반화를 사용한다면, w는 결국 
희박하게 될 것 입니다. 

49
00:03:03,020 --> 00:03:08,040
그것은 w 벡터 안에 수많은 0이 있다는 것을 의미한다.

50
00:03:08,040 --> 00:03:11,700
그리고 어떤 사람들은 이것이 모델을 압축하는 데에 도움이 된다고 
말한다. 그 이유는, 

51
00:03:11,700 --> 00:03:16,140
파라미터 세트는 0이고, 모델을 저장할 메모리가 덜 필요하기 때문입니다.

52
00:03:16,140 --> 00:03:19,850
하지만 실제로는 L1이 모델을 sparse상태로 만드는
일반화입니다.

53
00:03:19,850 --> 00:03:20,870
그것은 조금밖에 도움이 되지 않습니다.

54
00:03:20,870 --> 00:03:23,870
그래서 제 생각에는 이것이 

55
00:03:23,870 --> 00:03:26,520
최소한 당신의 모델을 압축할 만큼 그렇게 많이 쓰이는 것 같지는 않습니다만, 

56
00:03:26,520 --> 00:03:28,472
사람들이 당신의 네트워크를 훈련할 경우에, 

57
00:03:28,472 --> 00:03:31,423
L2 일반화는 훨씬 더 자주 사용됩니다.

58
00:03:31,423 --> 00:03:34,301
죄송합니다. 여기에 있는 표기를 살짝 수정할게요.

59
00:03:34,301 --> 00:03:35,929
마지막으로 한가지 세부사항이요, 

60
00:03:35,929 --> 00:03:42,823
여기 이 Lambda는 일반화 파라미터라고 불립니다.

61
00:03:45,267 --> 00:03:48,172
그리고 주로 당신은 이것을 development set이나 

62
00:03:48,172 --> 00:03:50,021
[경고] 교차 검증 을 사용하여 설치합니다.

63
00:03:50,021 --> 00:03:53,274
훈련 세트와 파라미터가 작아지게 하기 위해 두 표준을 설정하는 것을 교환하는
의미에서 다양한 값과 무엇이 최선인지 확인 후에 

64
00:03:53,274 --> 00:03:57,662
두 표준을 설정하는 것을 교환하는

65
00:03:57,662 --> 00:04:01,007
의미에서 다양한 값과 무엇이 최선의 방법인지 확인해야 합니다.

66
00:04:01,007 --> 00:04:03,088
이는 과도한 장착을 예방하는 데에 도움이 됩니다. 

67
00:04:03,088 --> 00:04:07,165
Lambda는 당신이 조정해야 할지도 모르는 또 다른 하이퍼 파라미터 입니다.

68
00:04:07,165 --> 00:04:09,550
그나저나, 프로그래밍 연습을 위해서는, 

69
00:04:09,550 --> 00:04:14,250
Lambda는 Python 프로그래밍 언어에서 예약된 키워드 입니다. 

70
00:04:14,250 --> 00:04:18,300
그리하여, 프로그래밍 연습에서 우리는 a 없이 Python에 예약된 키워드에 충돌하지 않도록

71
00:04:19,340 --> 00:04:23,690
Lambd를 사용할 것 입니다.

72
00:04:23,690 --> 00:04:27,740
그럼 우리는 Lambda 일반화 매개 변수를 대표하기 위해
Lambd를 사용합니다.

73
00:04:29,190 --> 00:04:33,320
이것이 Logistic regression을 위한 L2를 규현하는 방법 입니다. 

74
00:04:33,320 --> 00:04:35,280
신경 네트워크는 어떤가요?

75
00:04:35,280 --> 00:04:39,789
신경 네트워크에는

76
00:04:39,789 --> 00:04:44,621
w[1], b[1] 부터 w[L], b[L]까지의 모든 매개 변수들의 
모든 비용함수가 있는데

77
00:04:44,621 --> 00:04:48,906
대문자 L은 당신의 신경 네트워크에 있는 레이어의 수를 뜻합니다.

78
00:04:48,906 --> 00:04:54,129
그래서 비용함수는 손실의 합계와 같습니다. 

79
00:04:54,129 --> 00:04:58,066
m 훈련의 예들에 대한 합계 입니다.

80
00:04:58,066 --> 00:05:03,087
일반화는 모든 파라미터 w에 2m 이상의 
Lambda를 더합니다

81
00:05:03,087 --> 00:05:10,190
당신의 파라미터 매트릭스는 w 입니다.

82
00:05:10,190 --> 00:05:14,857
그것을 제곱근이라고 합니다.

83
00:05:14,857 --> 00:05:19,749
매트릭스의 표준에서

84
00:05:19,749 --> 00:05:23,922
제곱근의 표준은 이 매트릭스 제곱근의 각 요소에 대한

85
00:05:23,922 --> 00:05:29,250
i의 합계와 j의 합계로 정의 된다.

86
00:05:29,250 --> 00:05:31,248
그리고 이 합계의 지표들을 원한다면, 

87
00:05:31,248 --> 00:05:35,253
i=1 에서 n[l-1]에 대한 합계 입니다.

88
00:05:35,253 --> 00:05:38,537
j=1에서 n[l]에 대한 합계 입니다.

89
00:05:38,537 --> 00:05:44,497
w는 n[l-1] 곱하기 n[l] dimensional matrix이기 때문입니다.

90
00:05:44,497 --> 00:05:51,320
여기서 이 숫자는 레이어 l에 있는 [l-1]의 단위의 개수 입니다. 

91
00:05:51,320 --> 00:05:57,447
그래서 이 매트릭스 표준은 Frobenius라고 불립니다.

92
00:05:57,447 --> 00:06:03,710
행렬의 표준은 subscript에 F로 명시되어 있습니다.

93
00:06:03,710 --> 00:06:07,266
그래서, 불가사의한 선형 대수학의 기술적인 이유로, 

94
00:06:07,266 --> 00:06:10,491
표준형 매트릭스 l2라고 불리지 않는다

95
00:06:10,491 --> 00:06:14,620
대신에, 그것은 프로베니우스식 매트릭스라고 불린다.

96
00:06:14,620 --> 00:06:16,980
표준형 매트릭스 l2가 더 자연스럽게 들리는 것을 알지만, 

97
00:06:16,980 --> 00:06:21,760
매우 불가사의하기에 당신이 꼭 알아야 할 필요는 없다.

98
00:06:21,760 --> 00:06:24,090
관례적으로, 이것은 프로베느우스 표준형이라고 불린다.

99
00:06:24,090 --> 00:06:27,232
그것은 단지 매트릭스 요소들의 제곱의 합을 뜻한다.

100
00:06:27,232 --> 00:06:30,060
그렇다면, 당신은 어떻게 gradient가 이와 함께 하강하도록 
구현할 것인가?

101
00:06:30,060 --> 00:06:35,343
이전에는 backprop을 사용하여 dw를 완성했을 것이다.

102
00:06:35,343 --> 00:06:40,626
backprop은 우리에게 w에 관한 J의 편도함수나, 

103
00:06:40,626 --> 00:06:46,166
w의 임의의 주어진[ㅣ]을 줄 것 입니다. 

104
00:06:46,166 --> 00:06:52,995
그 후에 당신은 w[l]을w[l]- 학습률 곱하기 d로 업데이트합니다.

105
00:06:52,995 --> 00:06:57,890
이는 우리가 목표에 이 추가적인 일반화 용어를 추가하기 전 입니다.

106
00:06:57,890 --> 00:07:02,941
자, 이제 우리가 이 일반화 용어를 목표에 추가했으니, 

107
00:07:02,941 --> 00:07:07,643
당신이 해야할 일은 dw에 이를 더하세요. lambda/m 곱하기 w.

108
00:07:07,643 --> 00:07:10,760
그 후에, 당신은 그저 이 업데이트 된 것을 이전같이 계산하면 됩니다.

109
00:07:10,760 --> 00:07:14,829
이 새로운 dw[l]의 개체는 

110
00:07:14,829 --> 00:07:19,315
당신의 파라미터에 관해서는 

111
00:07:19,315 --> 00:07:23,385
비용함수의 도함수의 개체는 여전히 정확한 
개체인 것으로 드러났습니다.

112
00:07:23,385 --> 00:07:27,980
이제 당신은 추가적인 일반화 용어를 맨 마지막에 추가하였기에

113
00:07:29,260 --> 00:07:33,990
그리고 이러한 이유로 L2일반화는 가끔 

114
00:07:33,990 --> 00:07:36,730
weight decay라고 불립니다.

115
00:07:36,730 --> 00:07:42,348
따라서, 이러한 dw[l] 개체를 이곳에 그냥 꽂으면, 

116
00:07:42,348 --> 00:07:47,012
업데이트된 것은 w[l] = w[l] 곱하기

117
00:07:47,012 --> 00:07:51,994
학습율 alpha 곱하기 backprop

118
00:07:54,311 --> 00:08:02,816
+lambda/m 곱하기 w[l].

119
00:08:02,816 --> 00:08:04,431
마이너스 부호를 이곳에 두고, 

120
00:08:04,431 --> 00:08:09,382
이것은 w[l]- alpha lambda 

121
00:08:09,382 --> 00:08:14,494
/ m times w[l]- alpha t 곱하기

122
00:08:14,494 --> 00:08:18,822
backprop에서 얻은 수와 같습니다.

123
00:08:18,822 --> 00:08:22,324
그래서 이 용어는 매트릭스 w[l]이 무엇이든

124
00:08:22,324 --> 00:08:25,480
당신은 좀 더 작게 만들거에요. 그렇죠?

125
00:08:25,480 --> 00:08:28,270
이것은 마치 당신이 매트릭스 w와

126
00:08:28,270 --> 00:08:33,030
그것을 곱하고 

127
00:08:33,030 --> 00:08:38,279
매트릭스 w에서 alpha lambda/m 이것을 빼세요.

128
00:08:38,279 --> 00:08:41,130
마치 당신이 매트리스 w를 이 수와 곱하는 것 처럼요.

129
00:08:41,130 --> 00:08:43,528
그것은 1보다 조금 더 적은 숫자일거에요.

130
00:08:43,528 --> 00:08:48,688
그래서 이것이 바로 L2 평균화 일반화가 weight decay라고도 불리는
이유 입니다.

131
00:08:48,688 --> 00:08:53,716
그것은 일반적인 gradient 하강과 유사하기 떄문에

132
00:08:53,716 --> 00:08:59,260
후 방향전파에서 얻은 원래 그라데이션의 알파 배를 뺍니다.

133
00:08:59,260 --> 00:09:04,616
그러나, 이제 그것을 w와 곱할 것 입니다.

134
00:09:04,616 --> 00:09:08,324
그것은 1보다는 조금 적은 수가 될거에요.

135
00:09:08,324 --> 00:09:11,782
그리하여 L2 일반화의 대체명은 weight decay 입니다.

136
00:09:11,782 --> 00:09:15,641
저는 그 이름을 사용하지는 않을 것 입니다. 하지만, 

137
00:09:15,641 --> 00:09:21,030
그것을 weight decay라고 칭하는 직관은 바로 여기의 
첫번째 용어가 이것과 같기 때문이다.

138
00:09:21,030 --> 00:09:25,620
그래서 당신은 그저 weight metrics를 1보다 조금 적은 수에 곱하면 됩니다.

139
00:09:25,620 --> 00:09:28,511
그렇게 함으로써 신경네트워크에서 L2 일반화를 시행하느 것 입니다.

140
00:09:29,545 --> 00:09:32,796
자, 한가지 질문이 있어요. 거기, 앤드류, 

141
00:09:32,796 --> 00:09:35,675
일반화가 왜 데이터 과잉을 예방하는거져?

142
00:09:35,675 --> 00:09:37,462
다음 영상을 봅시다.

143
00:09:37,462 --> 00:09:41,805
그리고 일반화가 어떻게 데이터 과잉을 예방하는지에 관한
직관을 키워봅시다.