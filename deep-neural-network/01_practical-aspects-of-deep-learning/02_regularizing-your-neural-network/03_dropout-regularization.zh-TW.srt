1
00:00:00,000 --> 00:00:02,560
除了 L2 正則化

2
00:00:02,560 --> 00:00:06,875
有一種有力的正則化技巧稱為 "dropout"

3
00:00:06,875 --> 00:00:08,715
我們來看它如何作用

4
00:00:08,715 --> 00:00:12,600
假設您訓練一個神經網路
像左邊這個是過適的

5
00:00:12,600 --> 00:00:14,998
這是 dropout 的做法

6
00:00:14,998 --> 00:00:16,765
讓我複製一下神經網路

7
00:00:16,765 --> 00:00:21,100
使用 dropout, 我們要做的是通過網路每一層

8
00:00:21,100 --> 00:00:26,350
設定一些機率來消除神經網路的節點

9
00:00:26,350 --> 00:00:29,305
假設對於每一層

10
00:00:29,305 --> 00:00:30,955
我們將對每一個節點

11
00:00:30,955 --> 00:00:34,165
擲一枚硬幣，有 0.5 的機會

12
00:00:34,165 --> 00:00:38,005
保留這個節點, 0.5 機會刪除每一個節點

13
00:00:38,005 --> 00:00:39,820
經過擲硬幣後

14
00:00:39,820 --> 00:00:42,865
我們會決定消除這些節點

15
00:00:42,865 --> 00:00:49,775
我們做的是實際
從這些節點移除所有輸入及輸出

16
00:00:49,775 --> 00:00:51,550
所以我們最終會得到一個小很多

17
00:00:51,550 --> 00:00:53,150
真的裁減很多的網路

18
00:00:53,150 --> 00:00:56,145
然後您做反向傳播訓練

19
00:00:56,145 --> 00:00:59,705
使用一個例子在這裁減很多的網路上

20
00:00:59,705 --> 00:01:01,130
而在不同的例子上

21
00:01:01,130 --> 00:01:03,700
您會再次擲硬幣來用

22
00:01:03,700 --> 00:01:07,585
不同集合的節點然後
丟棄或者去除不同集合的節點

23
00:01:07,585 --> 00:01:09,235
所以對於每一個訓練例子

24
00:01:09,235 --> 00:01:14,455
您會訓練它使用這些縮減網路之一

25
00:01:14,455 --> 00:01:16,675
也許這似乎是一個有點瘋狂的技術

26
00:01:16,675 --> 00:01:20,470
它只是四處隨機剔除節點

27
00:01:20,470 --> 00:01:22,505
但這實際上可行

28
00:01:22,505 --> 00:01:28,480
但您可以想像因為您訓練每一個例子
在比較小的網路上

29
00:01:28,480 --> 00:01:34,591
或者也許給一些感覺
為什麼您最終能夠正則化這個網路

30
00:01:34,591 --> 00:01:38,590
因為這些比較小的網路已經通過訓練了

31
00:01:38,590 --> 00:01:41,535
讓我們來看如何建置 dropout

32
00:01:41,535 --> 00:01:43,425
有一些方式來建置 dropout

33
00:01:43,425 --> 00:01:44,915
我將告訴您最常用的

34
00:01:44,915 --> 00:01:47,865
這種技巧稱為 翻轉 dropout

35
00:01:47,865 --> 00:01:49,645
為了完整描述起見

36
00:01:49,645 --> 00:01:58,977
假設我們用 l = 3 層來描述

37
00:01:58,977 --> 00:02:03,000
所以在程式裡我將寫一堆 3 

38
00:02:03,000 --> 00:02:08,380
這只是說明如何去表示 dropout 在單一層裡

39
00:02:08,380 --> 00:02:12,155
我們要做的是設定一個向量 d

40
00:02:12,155 --> 00:02:16,503
而 d3 將是 dropout 向量在第三層

41
00:02:16,503 --> 00:02:21,585
這就是 3 的意思, np.random.rand

42
00:02:21,585 --> 00:02:27,708
而這將跟 a3 同樣的 shape

43
00:02:27,708 --> 00:02:31,261
而我來看看這個是否小於某個數字

44
00:02:31,261 --> 00:02:34,470
我將稱它為 keep_prob

45
00:02:34,470 --> 00:02:37,350
所以, keep_prob 是一個數字

46
00:02:37,350 --> 00:02:39,105
之前是用 0.5

47
00:02:39,105 --> 00:02:42,045
也許我現在用 0.8 在這個例子上

48
00:02:42,045 --> 00:02:47,040
而這是對於一個隱藏單元
要不要保留的機率

49
00:02:47,040 --> 00:02:49,129
所以 keep_prob = 0.8

50
00:02:49,129 --> 00:02:54,665
意思是有 0.2 的機會
去除掉任一個隱藏單元

51
00:02:54,665 --> 00:02:58,130
它的作用是產生一個隨機矩陣

52
00:02:58,130 --> 00:03:00,755
如果您向量化的話也管用

53
00:03:00,755 --> 00:03:03,180
所以 d3 是一個矩陣

54
00:03:03,180 --> 00:03:06,660
所以每個例子的每個隱藏單元

55
00:03:06,660 --> 00:03:10,245
0.8 的機會在相對於 d3 的位置是 1

56
00:03:10,245 --> 00:03:12,815
20% 的機會是 0

57
00:03:12,815 --> 00:03:20,900
所以這個隨機數字小於 0.8
它有0.8的機會為 1 或者說是為真

58
00:03:20,900 --> 00:03:24,675
20% 或者 0.2 機會為偽, 為 0

59
00:03:24,675 --> 00:03:27,569
然後您要做的是
拿您第三層的啟動值

60
00:03:27,569 --> 00:03:30,945
讓我稱之為 a3
在下面這個例子

61
00:03:30,945 --> 00:03:33,265
所以 a3 是您計算的啟動值

62
00:03:33,265 --> 00:03:37,335
您可以設 a3 等於舊的 a3

63
00:03:37,335 --> 00:03:41,849
乘上, 這是ㄧ個逐元素的乘積

64
00:03:41,849 --> 00:03:44,857
或者您可以也寫成 a3 *= d3

65
00:03:44,857 --> 00:03:50,625
這樣做是對於 d3 
每個等於 0 的元素

66
00:03:50,625 --> 00:03:53,735
每個元素有20%機會為 0

67
00:03:53,735 --> 00:03:57,840
用這個乘的運算來
化它為 0

68
00:03:57,840 --> 00:04:00,980
在 d3 相對應的元素

69
00:04:00,980 --> 00:04:02,250
如果您在 python 做這個

70
00:04:02,250 --> 00:04:05,880
技術上 d3 會是一個布林值矩陣
其值會是真或偽

71
00:04:05,880 --> 00:04:06,985
而不是 1 或 0

72
00:04:06,985 --> 00:04:10,057
但用乘法運算一樣可行, 它會

73
00:04:10,057 --> 00:04:13,390
將真跟偽的值
換成 1 跟 0

74
00:04:13,390 --> 00:04:16,260
如果您自己在 python 試一試, 您會了解

75
00:04:16,260 --> 00:04:22,570
最後, 我們將 a3 按比例放大, 利用除以

76
00:04:22,570 --> 00:04:30,015
0.8, 實際上是除以我們的 keep_prob 參數

77
00:04:30,015 --> 00:04:32,560
讓我來解釋
最後這個步驟在做什麼

78
00:04:32,560 --> 00:04:36,040
假設為了方便解釋起見
您有 50 個單元

79
00:04:36,040 --> 00:04:39,930
或者 50 個神經元在第三個隱藏層

80
00:04:39,930 --> 00:04:43,075
所以也許 a3 是一個 50 乘 1 維度或者

81
00:04:43,075 --> 00:04:46,650
如果您向量化, 或許是 50 乘 m 維度

82
00:04:46,650 --> 00:04:51,625
如果您有 80% 機會保留它們，20% 機會去掉它們

83
00:04:51,625 --> 00:04:53,050
這意思是平均來講

84
00:04:53,050 --> 00:04:59,025
您最終會關閉 10 個單元
或者 10 個單元歸零

85
00:04:59,025 --> 00:05:02,020
現在如果您看 z[4] 的值

86
00:05:02,020 --> 00:05:08,775
z[4] 將等於 w[4] * a[3] + b[4]

87
00:05:08,775 --> 00:05:10,570
所以, 期望值

88
00:05:10,570 --> 00:05:14,080
這個會減少 20%

89
00:05:14,080 --> 00:05:18,480
這個的意思是 a3 20%的元素會被歸零

90
00:05:18,480 --> 00:05:22,240
為了不減低 z[4] 的期望值

91
00:05:22,240 --> 00:05:24,380
您要做的是拿這個

92
00:05:24,380 --> 00:05:28,870
將它除以 0.8. 因為

93
00:05:28,870 --> 00:05:33,635
這會更正它或者說
將它提升回您需要的額外的 20%

94
00:05:33,635 --> 00:05:37,455
這樣就不會改變 a3 的期望值

95
00:05:37,455 --> 00:05:43,435
這一行就是稱為
翻轉 dropout 技巧

96
00:05:43,435 --> 00:05:44,830
而它的影響是

97
00:05:44,830 --> 00:05:47,230
不管您設 keep_prob 多少

98
00:05:47,230 --> 00:05:50,446
不管是 0.8, 0.9 甚至是 1

99
00:05:50,446 --> 00:05:52,135
如果設成 1 就不會有 dropout

100
00:05:52,135 --> 00:05:54,565
因為這會保留所有的
不管是 0.5 或其他

101
00:05:54,565 --> 00:05:57,980
這種翻轉 dropout技巧
是除以 keep_prob

102
00:05:57,980 --> 00:06:02,730
來確保 a3 的期望值不變

103
00:06:02,730 --> 00:06:05,005
實際上在測試時

104
00:06:05,005 --> 00:06:06,820
當您試著評估神經網路

105
00:06:06,820 --> 00:06:08,300
我們將在下一張投影片談到

106
00:06:08,300 --> 00:06:10,065
這種翻轉 dropout

107
00:06:10,065 --> 00:06:13,160
這一行用綠色框框起來

108
00:06:13,160 --> 00:06:17,540
這會讓測試時間容易些
因為您減低了比例問題

109
00:06:17,540 --> 00:06:20,110
至今最常用的 dropout 建置

110
00:06:20,110 --> 00:06:22,870
就我所知是翻轉 dropout

111
00:06:22,870 --> 00:06:24,490
我建議您用這種方式建置

112
00:06:24,490 --> 00:06:27,280
但有些早期 dropout 的迭代

113
00:06:27,280 --> 00:06:30,165
少了 除以 keep_prob 這一行

114
00:06:30,165 --> 00:06:33,660
在測試時演算法變得比較複雜

115
00:06:33,660 --> 00:06:37,040
但人們傾向於
不再用其他版本

116
00:06:37,040 --> 00:06:40,125
您做的是您使用 d 向量

117
00:06:40,125 --> 00:06:43,390
您注意到對於不同的訓練例子

118
00:06:43,390 --> 00:06:46,090
您將不同的隱藏單元歸零

119
00:06:46,090 --> 00:06:49,975
實際上, 如果您使用
多次(循環)處理在同一個訓練集上

120
00:06:49,975 --> 00:06:52,566
那在不同的處理資料集時循環時

121
00:06:52,566 --> 00:06:55,290
您應該隨機
將不同的隱藏單元歸零

122
00:06:55,290 --> 00:06:57,270
所以並不是對於一個例子

123
00:06:57,270 --> 00:07:01,155
您一直將同樣的隱藏單元歸零

124
00:07:01,155 --> 00:07:03,258
在第一次梯度下降循環時

125
00:07:03,258 --> 00:07:05,510
您也許將一些隱藏單元歸零

126
00:07:05,510 --> 00:07:07,375
而在第二次梯度下降循環時

127
00:07:07,375 --> 00:07:09,595
當您歷經訓練集第二次時

128
00:07:09,595 --> 00:07:13,008
也許您會將不同樣子的
隱藏單元歸零

129
00:07:13,008 --> 00:07:16,023
而向量 d 或者 d3 對於第三層

130
00:07:16,023 --> 00:07:18,395
是用來決定哪些該歸零

131
00:07:18,395 --> 00:07:21,565
同樣在正向傳播跟反向傳播時

132
00:07:21,565 --> 00:07:22,980
我們這裡只展示正向傳播

133
00:07:22,980 --> 00:07:26,950
現在如果您要在測試時訓練演算法, 您要這樣做

134
00:07:26,950 --> 00:07:30,535
測試時，您會給予一些 x 來做預測

135
00:07:30,535 --> 00:07:32,335
使用我們標準的符號

136
00:07:32,335 --> 00:07:33,764
我將使用 a[0]

137
00:07:33,764 --> 00:07:38,180
第零層的啟動值來記測試例子 x

138
00:07:38,180 --> 00:07:40,760
我們要做的是不使用

139
00:07:40,760 --> 00:07:44,340
dropout 在測試時, 特別是這是合理的

140
00:07:44,340 --> 00:07:48,314
z[1] = w[1]a[0] + b[1]

141
00:07:48,314 --> 00:07:56,627
a[1] = g[1](z[1])

142
00:07:56,627 --> 00:08:03,745
z[2] = w[2]a[1] + b[2]

143
00:08:03,745 --> 00:08:04,895
a[2] = ...

144
00:08:04,895 --> 00:08:10,060
等等, 直到您到最後一層您做預估 y-hat

145
00:08:10,060 --> 00:08:12,640
但請注意，測試時不使用

146
00:08:12,640 --> 00:08:15,690
dropout, 您不用隨機擲硬幣

147
00:08:15,690 --> 00:08:20,285
您不用擲硬幣來決定
哪一個隱藏單元來去除

148
00:08:20,285 --> 00:08:22,510
而這是因為當您在測試時做預估

149
00:08:22,510 --> 00:08:25,615
您真的不希望您的輸出是隨機的

150
00:08:25,615 --> 00:08:27,699
如果您在測試時使用 dropout

151
00:08:27,699 --> 00:08:29,890
這只會在您的預估中增加雜訊

152
00:08:29,890 --> 00:08:34,105
理論上, 您可以做的是跑預估流程

153
00:08:34,105 --> 00:08:38,940
很多次使用隨機丟棄不同的隱藏單元
然後跨過所有的隱藏單元

154
00:08:38,940 --> 00:08:43,625
但這真的是效率不高的運算
且給您大約類似的結果

155
00:08:43,625 --> 00:08:46,880
相當類似的結果對於這個不同的流程

156
00:08:46,880 --> 00:08:47,980
還有要提醒一下

157
00:08:47,980 --> 00:08:49,385
這種翻轉 dropout

158
00:08:49,385 --> 00:08:53,455
您記得前面投影片我們除以 keep_prob

159
00:08:53,455 --> 00:08:56,450
那個的影響是確認即使當您不做

160
00:08:56,450 --> 00:08:59,664
dropout 在測試時, 比例上

161
00:08:59,664 --> 00:09:02,050
這些啟動值的期望值不會改變

162
00:09:02,050 --> 00:09:06,540
所以您不需要加入
額外的比例參數在測試時

163
00:09:06,540 --> 00:09:08,965
這是跟訓練時不同的地方

164
00:09:08,965 --> 00:09:10,240
所以這是 dropout

165
00:09:10,240 --> 00:09:13,000
當您做這個禮拜的程式練習時

166
00:09:13,000 --> 00:09:16,660
您會得到第一手的經驗

167
00:09:16,660 --> 00:09:18,440
但為什麼它真的可行?

168
00:09:18,440 --> 00:09:20,410
我想在下一段影片中給您

169
00:09:20,410 --> 00:09:23,630
一些比較好的直觀有關於 dropout 真正在做些什麼

170
00:09:23,630 --> 00:09:25,160
讓我們進入下一段影片