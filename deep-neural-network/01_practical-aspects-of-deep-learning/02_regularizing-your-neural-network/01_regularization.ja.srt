1
00:00:00,730 --> 00:00:03,660
もしニューラルネットワークの
オーバーフィッティングが疑われる場合

2
00:00:03,660 --> 00:00:05,840
つまり高バリアンス問題のとき

3
00:00:05,840 --> 00:00:09,400
まず試すべきは正則化です

4
00:00:09,400 --> 00:00:11,246
高バリアンスに対処する別の方法は

5
00:00:11,246 --> 00:00:13,917
より多くの訓練データを得ることで
確実な方法なのですが

6
00:00:13,917 --> 00:00:15,869
いつでも多くのデータを
得られるわけではないですし

7
00:00:15,869 --> 00:00:17,850
多くのデータを得るのは
お金がかかります

8
00:00:17,850 --> 00:00:21,760
しかし正則化することで
オーバーフィッティングを防げ

9
00:00:21,760 --> 00:00:23,910
ネットワークの誤差を減らせます

10
00:00:23,910 --> 00:00:26,020
では正則化のしくみを見ていきましょう

11
00:00:26,020 --> 00:00:28,780
ロジスティック回帰を用いて
理解を進めましょう

12
00:00:28,780 --> 00:00:33,220
ロジスティック回帰では
コスト関数Ｊを最小化しましたね

13
00:00:33,220 --> 00:00:37,050
これがコスト関数の式です

14
00:00:37,050 --> 00:00:41,290
訓練データでの予測に基づく損失の和です

15
00:00:41,290 --> 00:00:45,140
ｗやｂも覚えているでしょうか

16
00:00:45,140 --> 00:00:48,175
ロジスティック回帰でのパラメータです

17
00:00:48,175 --> 00:00:54,620
ｗはｘ次元のパラメータで
ｂは実数です

18
00:00:54,620 --> 00:00:58,979
ロジスティック回帰で正則化するために
加えるべきものは

19
00:00:58,979 --> 00:01:03,154
このλで正則化パラメータと呼ばれます

20
00:01:03,154 --> 00:01:04,609
もう少し説明しましょう

21
00:01:04,609 --> 00:01:10,072
λ/2mにｗのノルムの
２乗をかけたものは

22
00:01:10,072 --> 00:01:15,840
ここでｗのノルムの２乗は

23
00:01:15,840 --> 00:01:22,580
n_x個のw_j^2の和と等しく

24
00:01:22,580 --> 00:01:27,750
wTwとも書けます
ｗのユークリッドノルムです

25
00:01:27,750 --> 00:01:31,910
これはL2正則化と呼ばれます

26
00:01:33,700 --> 00:01:36,618
なぜならここでユークリッドノルムを
用いているためです

27
00:01:36,618 --> 00:01:38,877
パラメータベクトルｗのL2ノルムです

28
00:01:38,877 --> 00:01:41,780
なぜｗのみに正則化を施すのでしょうか

29
00:01:41,780 --> 00:01:47,130
なぜｂに対応するものを
加えないのでしょうか

30
00:01:47,130 --> 00:01:51,210
実際は入れてもよいのですが
私はやりません

31
00:01:51,210 --> 00:01:56,310
なぜならパラメータを見たとき
ｗは非常に高次元のベクトルで

32
00:01:56,310 --> 00:02:00,159
高バリアンスの問題が起きやすいのです

33
00:02:00,159 --> 00:02:02,250
おそらくｗは大量のパラメータを持ち

34
00:02:02,250 --> 00:02:06,600
全パラメータを適合できないでしょうが
ｂはあくまでひとつの数字です

35
00:02:06,600 --> 00:02:10,200
したがって大半のパラメータは
ｂではなくｗにあります

36
00:02:10,200 --> 00:02:12,890
もし最後の項を加えてみても

37
00:02:12,890 --> 00:02:14,040
さほど違いはないでしょう

38
00:02:14,040 --> 00:02:17,960
なぜならbは大量のパラメータのうちの
わずか１つにすぎないのですから

39
00:02:17,960 --> 00:02:21,500
実際のところ
私はこの項をわざわざ入れません

40
00:02:21,500 --> 00:02:22,962
望むなら入れてもよいでしょう

41
00:02:22,962 --> 00:02:27,510
このL2正則化は
もっとも一般的な正則化です

42
00:02:27,510 --> 00:02:32,042
L1正則化というのも
聞いたことがあるかもしれません

43
00:02:32,042 --> 00:02:38,422
それはL2項のかわりに足す項が

44
00:02:38,422 --> 00:02:45,674
λ/mにこの和をかけたものになります

45
00:02:45,674 --> 00:02:49,716
パラメータベクトルwの
L1ノルムと呼ばれるものです

46
00:02:49,716 --> 00:02:52,843
だから下付きで１がありますね

47
00:02:52,843 --> 00:02:58,050
分母がmか2mかは
単に定数倍でしかありません

48
00:02:58,050 --> 00:03:03,020
もしL1正則化を用いれば
wはスパースになるでしょう

49
00:03:03,020 --> 00:03:08,040
スパースとはwの成分に
０が大量にあるという意味です

50
00:03:08,040 --> 00:03:11,700
これはモデルを圧縮するという人もいます

51
00:03:11,700 --> 00:03:16,140
０成分が多ければ
モデルを格納するメモリを節約できます

52
00:03:16,140 --> 00:03:19,850
しかしスパースに寄与する
L1正則化の影響は

53
00:03:19,850 --> 00:03:20,870
わずかでした

54
00:03:20,870 --> 00:03:23,870
したがってこれはあまり使われません

55
00:03:23,870 --> 00:03:26,520
少なくともモデルを圧縮する目的では

56
00:03:26,520 --> 00:03:28,472
ネットワークを訓練するとき

57
00:03:28,472 --> 00:03:31,423
L2正則化のほうが
はるかに多く使われています

58
00:03:31,423 --> 00:03:34,301
失礼ちょっとここの記号を直します

59
00:03:34,301 --> 00:03:35,929
では最後に一点

60
00:03:35,929 --> 00:03:42,823
ここのλは正則化パラメータと呼ばれます

61
00:03:45,267 --> 00:03:48,172
普通は開発セットを用いて決めます

62
00:03:48,172 --> 00:03:50,021
あるいは交差検定セットで

63
00:03:50,021 --> 00:03:53,274
様々な値を試して
良い値を探します

64
00:03:53,274 --> 00:03:57,662
トレードオフの観点から
訓練セットでうまくいくかと

65
00:03:57,662 --> 00:04:01,007
そのパラメータでノルムを小さくし

66
00:04:01,007 --> 00:04:03,088
オーバーフィッティングを防げるかを
考慮します

67
00:04:03,088 --> 00:04:07,165
したがってλは調整せねばならない
ハイパーパラメータです

68
00:04:07,165 --> 00:04:09,550
ところでプログラミング演習のために

69
00:04:09,550 --> 00:04:14,250
"lambda"はPythonにおける
予約語なのです

70
00:04:14,250 --> 00:04:18,300
よって演習では
"lambd"をかわりに使います

71
00:04:19,340 --> 00:04:23,690
"a"を抜きPythonの予約語による
クラッシュを防ぎます

72
00:04:23,690 --> 00:04:27,740
"lambd"で
正則化パラメータλをあらわします

73
00:04:29,190 --> 00:04:33,320
以上がロジスティック回帰における
L2正則化の実装です

74
00:04:33,320 --> 00:04:35,280
ではニューラルネットでは
どうでしょうか

75
00:04:35,280 --> 00:04:39,789
ニューラルネットでのコスト関数は

76
00:04:39,789 --> 00:04:44,621
w[1],b[1]からw[L],b[L]までの
パラメータを持ちます

77
00:04:44,621 --> 00:04:48,906
ここでＬは層の数をあらわします

78
00:04:48,906 --> 00:04:54,129
コスト関数はこうです
損失の和では

79
00:04:54,129 --> 00:04:58,066
訓練例m個ぶんを
足し合わせます

80
00:04:58,066 --> 00:05:03,087
正則化ではλ/2m掛ける

81
00:05:03,087 --> 00:05:10,190
すべてのWにわたる和です
Wはパラメータ行列のことです

82
00:05:10,190 --> 00:05:14,857
これは２乗ノルムと呼ばれます

83
00:05:14,857 --> 00:05:19,749
この２乗ノルム行列を定義します

84
00:05:19,749 --> 00:05:23,922
ｉとｊで総和をとるのですが

85
00:05:23,922 --> 00:05:29,250
行列の各要素の２乗を
足し合わせます

86
00:05:29,250 --> 00:05:31,248
この足し合わせに
添え字が必要なら

87
00:05:31,248 --> 00:05:35,253
ｉは１からn[l-1]まで

88
00:05:35,253 --> 00:05:38,537
ｊは１からn[l]までです

89
00:05:38,537 --> 00:05:44,497
ｗはn[l-1],n[l]次元の
行列だからです

90
00:05:44,497 --> 00:05:51,320
この次元はl-1番目とl番目の
隠れ層の個数に対応しています

91
00:05:51,320 --> 00:05:57,447
これはFrobeniusノルムと
呼ばれるものになります

92
00:05:57,447 --> 00:06:03,710
下付きでＦと書かれた行列です

93
00:06:03,710 --> 00:06:07,266
線形代数の不可解な
技術的理由により

94
00:06:07,266 --> 00:06:10,491
L2ノルム行列とは呼ばれず

95
00:06:10,491 --> 00:06:14,620
かわりにFrobeniusノルム行列と
呼ばれます

96
00:06:14,620 --> 00:06:16,980
単にL2ノルムと呼ぶほうが
自然なのはわかりますが

97
00:06:16,980 --> 00:06:21,760
知る必要のない
本当に不可思議な理由により

98
00:06:21,760 --> 00:06:24,090
慣例的に
Frobeniusノルムと呼ばれます

99
00:06:24,090 --> 00:06:27,232
これの意味するところはたんなる
行列要素の２乗の総和です

100
00:06:27,232 --> 00:06:30,060
ではこれを用いてどのように
勾配降下を実装すればよいでしょうか

101
00:06:30,060 --> 00:06:35,343
以前dwを
誤差逆伝播法で求めましたが

102
00:06:35,343 --> 00:06:40,626
そこでＪのｗによる偏微分が得られます

103
00:06:40,626 --> 00:06:46,166
ｗはすべての[l]についてです

104
00:06:46,166 --> 00:06:52,995
そしてw[l]を更新します
w[l]引く学習係数掛けるdw[l]です

105
00:06:52,995 --> 00:06:57,890
ここまでは目的関数に
この正則化項を加える前の話ですね

106
00:06:57,890 --> 00:07:02,941
次は正則化項にあたる部分を
考えていきます

107
00:07:02,941 --> 00:07:07,643
dwにλ/m掛けるｗを加えます

108
00:07:07,643 --> 00:07:10,760
同様にこちらの更新式を求めます

109
00:07:10,760 --> 00:07:14,829
このdw[l]の新しい定義は

110
00:07:14,829 --> 00:07:19,315
依然として正しく

111
00:07:19,315 --> 00:07:23,385
コスト関数の
パラメータによる微分です

112
00:07:23,385 --> 00:07:27,980
正則化項を付け足しても
変わりません

113
00:07:29,260 --> 00:07:33,990
このためL2正則化はまた

114
00:07:33,990 --> 00:07:36,730
重み減衰とも呼ばれます

115
00:07:36,730 --> 00:07:42,348
dw[l]のこの定義を用いて
ここに代入すると

116
00:07:42,348 --> 00:07:47,012
w[l]の更新式は

117
00:07:47,012 --> 00:07:51,994
w[l]に学習係数αをかけ
逆誤差伝播による部分と

118
00:07:54,311 --> 00:08:02,816
λ/mかけるw[l]をたします

119
00:08:02,816 --> 00:08:04,431
ここにマイナス符号を入れます

120
00:08:04,431 --> 00:08:09,382
これを等式変形すると

121
00:08:09,382 --> 00:08:14,494
w[l]-αλ/m×w[l]-α×

122
00:08:14,494 --> 00:08:18,822
逆誤差伝播から得られたもの
になります

123
00:08:18,822 --> 00:08:22,324
この項が示しているのは
w[l]がどんな値であろうと

124
00:08:22,324 --> 00:08:25,480
徐々に減少することです
いいですか？

125
00:08:25,480 --> 00:08:28,270
これはまさに
行列wを取ってきて

126
00:08:28,270 --> 00:08:33,030
1-αλ/mを掛けている
ようなものです

127
00:08:33,030 --> 00:08:38,279
実際にwを取って
そのαλ/m倍を引いています

128
00:08:38,279 --> 00:08:41,130
行列wに掛けるこの数字は

129
00:08:41,130 --> 00:08:43,528
１より少し小さい数になるでしょう

130
00:08:43,528 --> 00:08:48,688
L2正則化が
重み減衰と呼ばれる理由は

131
00:08:48,688 --> 00:08:53,716
通常の勾配降下で
wの更新をする際

132
00:08:53,716 --> 00:08:59,260
逆誤差伝播から得られたもとの勾配の
α倍を引くようなものだからです

133
00:08:59,260 --> 00:09:04,616
wに掛け合わされるこの値が

134
00:09:04,616 --> 00:09:08,324
１よりわずかに小さいとも言えます

135
00:09:08,324 --> 00:09:11,782
したがってL2正則化のまたの名は
重み減衰となるのです

136
00:09:11,782 --> 00:09:15,641
あまりその呼び方をしませんが
直観的には

137
00:09:15,641 --> 00:09:21,030
重み減衰と呼ばれるのは
この最初の項がこれに等しいからです

138
00:09:21,030 --> 00:09:25,620
重み行列に１より少し小さい値を
かけ合わせているだけなのです

139
00:09:25,620 --> 00:09:28,511
以上がニューラルネットにおける
L2正則化の実装です

140
00:09:29,545 --> 00:09:32,796
ここで良く聞かれる質問なのですが
やあアンドリュー

141
00:09:32,796 --> 00:09:35,675
どうやって正則化が
オーバーフィッティングを防ぐのかい？

142
00:09:35,675 --> 00:09:37,462
次のビデオを見て

143
00:09:37,462 --> 00:09:41,805
正則化がオーバーフィットを防ぐ方法を
つかんでください