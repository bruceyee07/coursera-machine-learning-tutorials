1
00:00:00,000 --> 00:00:03,025
为什么正则化能够防止过拟合呢?

2
00:00:03,025 --> 00:00:05,835
为什么它有助于减少方差问题?

3
00:00:05,835 --> 00:00:10,920
我们通过几个例子来直观地体会一下

4
00:00:10,920 --> 00:00:16,635
请回忆一下 左边是高偏差 右边是高方差

5
00:00:16,635 --> 00:00:25,235
中间的是恰好的情况 我在之前的视频中画过这几张图

6
00:00:25,235 --> 00:00:27,780
现在 来看一下这个庞大的深度神经网络的拟合情况

7
00:00:27,780 --> 00:00:30,550
我知道画出来的这个网络还不够大 也不够深

8
00:00:30,550 --> 00:00:34,630
但是你可以认为这就是过拟合的神经网络

9
00:00:34,630 --> 00:00:39,520
图中就是我们的代价函数J 包含了参数W和B

10
00:00:39,520 --> 00:00:44,390
并且J是一些损失项的总和

11
00:00:44,390 --> 00:00:51,872
所以我所做的正则化就是添加一些

12
00:00:51,872 --> 00:00:56,395
额外的项目

13
00:00:56,395 --> 00:01:02,690
来避免权重矩阵过大

14
00:01:02,690 --> 00:01:04,540
这就是弗罗贝尼乌斯范数

15
00:01:04,540 --> 00:01:08,290
那么为什么通过压缩L-2范式或者

16
00:01:08,290 --> 00:01:12,445
弗罗贝尼乌斯范数 或者参数项就能减轻过拟合情况呢?

17
00:01:12,445 --> 00:01:14,515
关于这个问题的一个直观理解就是

18
00:01:14,515 --> 00:01:17,354
如果你把正则项λ设置的很大

19
00:01:17,354 --> 00:01:20,005
权重矩阵W就会被设置为

20
00:01:20,005 --> 00:01:24,535
非常接近0的值

21
00:01:24,535 --> 00:01:30,460
因此这个直观理解就是
把很多隐藏单元的权重

22
00:01:30,460 --> 00:01:33,340
设置的太接近0了而导致

23
00:01:33,340 --> 00:01:36,675
这些隐藏单元的影响被消除了

24
00:01:36,675 --> 00:01:37,990
如果是这种情况

25
00:01:37,990 --> 00:01:44,765
那么就会使这个大大简化的神经网络变成一个很小的神经网络

26
00:01:44,765 --> 00:01:48,185
事实上 这种情况与逻辑回归单元很像

27
00:01:48,185 --> 00:01:50,005
但很可能是网络的深度更大了

28
00:01:50,005 --> 00:01:51,805
因此这就会使

29
00:01:51,805 --> 00:01:57,635
这个过拟合网络带到更加接近左边高偏差的状态

30
00:01:57,635 --> 00:02:00,760
但是λ存在一个中间值

31
00:02:00,760 --> 00:02:04,820
能够得到一个更加接近中间这个刚刚好的状态

32
00:02:04,820 --> 00:02:07,420
直观理解就是把λ值设的足够高的话

33
00:02:07,420 --> 00:02:10,510
它就会使W接近0

34
00:02:10,510 --> 00:02:13,280
但实际中并不会发生这种情况

35
00:02:13,280 --> 00:02:17,110
我们可以想象成通过把隐藏单元的值归零

36
00:02:17,110 --> 00:02:19,270
来削减了隐藏单元的影响

37
00:02:19,270 --> 00:02:21,935
最终导致这个网络变成了一个更加简单的网络

38
00:02:21,935 --> 00:02:25,920
这个神经网络越来越接近逻辑回归

39
00:02:25,920 --> 00:02:31,360
我们直觉上认为这些隐藏单元的影响被完全消除了
其实并不完全正确

40
00:02:31,360 --> 00:02:35,225
实际上网络仍在使用所有的隐藏单元

41
00:02:35,225 --> 00:02:37,610
但每个隐藏单元的影响变得非常小了

42
00:02:37,610 --> 00:02:41,255
但最终你得到的这个简单的网络

43
00:02:41,255 --> 00:02:45,040
看起来就像一个不容易过拟合的小型的网络

44
00:02:45,040 --> 00:02:47,715
这种直觉在你通过编程练习

45
00:02:47,715 --> 00:02:50,765
实现正则化时对你理解是很有帮助的

46
00:02:50,765 --> 00:02:55,360
你实际上看到了一些方差变小的结果

47
00:02:55,360 --> 00:02:57,955
现在通过另一个例子直观理解一下

48
00:02:57,955 --> 00:03:01,535
为什么正则化可以帮助防止过拟合

49
00:03:01,535 --> 00:03:04,030
在这个例子中 假设我们使用的

50
00:03:04,030 --> 00:03:08,465
tanh激活函数是图中这样的

51
00:03:08,465 --> 00:03:13,515
使用g(z)表示tanh(z)

52
00:03:13,515 --> 00:03:15,200
因此这种情况下

53
00:03:15,200 --> 00:03:19,427
可以发现只要Z的值很小

54
00:03:19,427 --> 00:03:23,410
比如Z只涉及很小范围的参数

55
00:03:23,410 --> 00:03:28,165
也许就像图中这个范围所示<br />那么你其实是在使用tanh函数的线性的条件部分

56
00:03:28,165 --> 00:03:34,080
只有Z的值被允许取到更大的值或者像这种小一点的值的时候

57
00:03:34,080 --> 00:03:37,490
激活函数才开始展现出它的非线性的能力

58
00:03:37,490 --> 00:03:40,605
因此直觉就是 如果λ值

59
00:03:40,605 --> 00:03:42,750
即正则化参数被设置的很大的话

60
00:03:42,750 --> 00:03:46,530
那么激活函数的参数实际上会变小

61
00:03:46,530 --> 00:03:51,290
因为代价函数的参数会被不能过大

62
00:03:51,290 --> 00:03:56,740
并且如果W很小那么由于

63
00:03:56,740 --> 00:04:02,550
Z等于W这项再加上b

64
00:04:02,550 --> 00:04:04,440
但如果W非常非常小

65
00:04:04,440 --> 00:04:07,140
那么Z也会非常小

66
00:04:07,140 --> 00:04:10,830
特别是如果Z的值相对都很小时

67
00:04:10,830 --> 00:04:12,787
就在图中这样的范围内取值的话

68
00:04:12,787 --> 00:04:16,045
那么g(z)函数就会接近于线性函数

69
00:04:16,045 --> 00:04:22,880
因此 每一层都几乎是线性的

70
00:04:22,880 --> 00:04:24,800
就像线性回归一样

71
00:04:24,800 --> 00:04:27,860
就像我们第一课中讲过的那样

72
00:04:27,860 --> 00:04:31,275
如果每层都是线性的那么你的整个网络就是线性网络

73
00:04:31,275 --> 00:04:33,200
因此即使一个很深的神经网络

74
00:04:33,200 --> 00:04:35,930
如果使用线性激活函数

75
00:04:35,930 --> 00:04:39,245
最终也只能计算线性的函数

76
00:04:39,245 --> 00:04:43,700
因此就不能拟合那些很复杂的决策函数

77
00:04:43,700 --> 00:04:49,085
也不过度拟合那些

78
00:04:49,085 --> 00:04:52,940
数据集的非线性决策平面

79
00:04:52,940 --> 00:04:57,485
就像我们在上张幻灯片中看到的过拟合高方差的情况

80
00:04:57,485 --> 00:04:59,060
总结一下

81
00:04:59,060 --> 00:05:01,665
如果正则化变得非常大

82
00:05:01,665 --> 00:05:03,873
而参数W很小

83
00:05:03,873 --> 00:05:06,350
那么Z就会相对很小

84
00:05:06,350 --> 00:05:08,480
此时先暂时忽略b的影响

85
00:05:08,480 --> 00:05:12,935
Z会相对变小

86
00:05:12,935 --> 00:05:16,250
即Z只在小范围内取值

87
00:05:16,250 --> 00:05:19,890
那么激活函数如果是tanh的话

88
00:05:19,890 --> 00:05:21,790
这个激活函数就会呈现相对线性

89
00:05:21,790 --> 00:05:25,790
那么你的整个神经网络就只能计算一些

90
00:05:25,790 --> 00:05:28,550
离线性函数很近的值 也就是相对比较简单的函数

91
00:05:28,550 --> 00:05:32,250
而不能计算 很复杂的非线性函数

92
00:05:32,250 --> 00:05:34,650
因此就不大容易过拟合了

93
00:05:34,650 --> 00:05:38,870
再重申一遍
当你在编程练习作业中实现正则化的时候

94
00:05:38,870 --> 00:05:41,350
你就能亲眼看到这里所说的情况

95
00:05:41,350 --> 00:05:45,680
在总结关于正则化的讨论之前

96
00:05:45,680 --> 00:05:48,310
我想给大家一个实现程序时的小建议

97
00:05:48,310 --> 00:05:52,145
在程序中增加正则项的时候

98
00:05:52,145 --> 00:05:58,730
我们根据代价函数J的定义

99
00:05:58,730 --> 00:06:05,810
加入这个额外的惩罚项来防止权重过大

100
00:06:05,810 --> 00:06:09,230
如果你使用梯度下降方法

101
00:06:09,230 --> 00:06:18,605
调试程序的一个步骤就是画出代价函数J关于

102
00:06:18,605 --> 00:06:22,520
梯度下降的迭代次数的图像

103
00:06:22,520 --> 00:06:27,730
可以看到的是每次迭代后代价函数J都会单调递减

104
00:06:27,730 --> 00:06:30,820
如果你实现了正则化部分

105
00:06:30,820 --> 00:06:35,350
那么请记住J现在有了新的定义

106
00:06:35,350 --> 00:06:37,735
如果你仍然使用原来定义的J

107
00:06:37,735 --> 00:06:39,370
就像这里的第一项

108
00:06:39,370 --> 00:06:42,290
你可能看不到单调递减的函数图像

109
00:06:42,290 --> 00:06:45,030
所以为了调试梯度下降程序请确保你画的图像

110
00:06:45,030 --> 00:06:49,910
是利用这个新定义的J函数
它包含了这里第二个项

111
00:06:49,910 --> 00:06:54,015
否则J不会在每次迭代后都单调递减

112
00:06:54,015 --> 00:06:57,140
以上就是L2正则化

113
00:06:57,140 --> 00:07:01,435
也是我训练深度模型的时候最常用的正则化技巧

114
00:07:01,435 --> 00:07:05,480
在深度学习中也常会碰到另一个时常会用到的正则化技巧

115
00:07:05,480 --> 00:07:07,390
被称作dropout正则化

116
00:07:07,390 --> 00:07:09,280
我们就在下一个视频中看看它是怎么工作的吧
GTC字幕组翻译