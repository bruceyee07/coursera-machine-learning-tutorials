1
00:00:00,000 --> 00:00:03,025
일반화가 어떻게 overfitting문제를 도와줄 수 있는 것일까요?

2
00:00:03,025 --> 00:00:05,835
왜 그것이 편차 문제를 줄이는데 도움이 되는가?

3
00:00:05,835 --> 00:00:10,920
그것이 어떻게 작동하는지에 대한 약간의 직관을 얻기 위해 몇가지 예를 들어보도록 하겠습니다.

4
00:00:10,920 --> 00:00:16,635
큰 편향과 편차가 있는 경우를 다시 한번 보겠습니다.

5
00:00:16,635 --> 00:00:25,235
이전 비디오에서 보여졌던 그림을 이렇게 다시 그려보도록 하겠습니다.

6
00:00:25,235 --> 00:00:27,780
자, 이제 크기가 크고 깊이 있는 신경 네트워크를 봅시다.

7
00:00:27,780 --> 00:00:30,550
이걸 너무 크거나 깊이 있게 그리진 않았지만,

8
00:00:30,550 --> 00:00:34,630
이렇게 overfitting하는 그림을 신경망이라고 생각해봅시다.

9
00:00:34,630 --> 00:00:39,520
비용함수가 J의 W, 

10
00:00:39,520 --> 00:00:44,390
B가 loss의 합인 경우를 보겠습니다.

11
00:00:44,390 --> 00:00:51,872
저희가 알아낸 부분은, 

12
00:00:51,872 --> 00:00:56,395
여기 추가 항이, 

13
00:00:56,395 --> 00:01:02,690
weight matrix를 너무 큰 값을 갖는다는 이유로 처벌하죠.

14
00:01:02,690 --> 00:01:04,540
이것을 Frobenius norm라고 합니다.

15
00:01:04,540 --> 00:01:08,290
이것이 그러다면 왜 L 2 norm 또는 Frobenius norm 
을 줄이는 것이

16
00:01:08,290 --> 00:01:12,445
또는 파라티머를 줄이는 것이 덜 overfitting하게 만드는 것일까요? 

17
00:01:12,445 --> 00:01:14,515
한가지 직관으로는, 

18
00:01:14,515 --> 00:01:17,354
Lambda 일반화를 크게 정하면,

19
00:01:17,354 --> 00:01:20,005
weight matrix인 W의 값을

20
00:01:20,005 --> 00:01:24,535
0에 가까운 값으로 지정하고 싶어할 것입니다.

21
00:01:24,535 --> 00:01:30,460
그리하여 직관적인 부분은 수 많은 숨겨진 유닛에 대하여 

22
00:01:30,460 --> 00:01:33,340
weight를 거의 0에 가까운 값으로 지정하여 

23
00:01:33,340 --> 00:01:36,675
이러한 숨겨진 유닛의 영향을 거의 0으로 만들어 버리는 것입니다.

24
00:01:36,675 --> 00:01:37,990
이런 경우,

25
00:01:37,990 --> 00:01:44,765
이렇게 심플한 신경망은 훨씬 더 작은 신경망이 되는 것입니다.

26
00:01:44,765 --> 00:01:48,185
거의 로지스틱 회기 모형 유닉과 비슷합니다.

27
00:01:48,185 --> 00:01:50,005
대신 다량의 층으로 이루어진 것이죠. 

28
00:01:50,005 --> 00:01:51,805
그런 경우, 

29
00:01:51,805 --> 00:01:57,635
이러한 overfitting한 경우에서 큰 편향을 갖는 왼쪽의 경우처럼 이동시킵니다.

30
00:01:57,635 --> 00:02:00,760
하지만, 이상적으로 Lambda 적당한 중간 값을 통해,

31
00:02:00,760 --> 00:02:04,820
이렇게 가운데의 "just right" 와 같은 케이스가 나올 수도 있습니다.

32
00:02:04,820 --> 00:02:07,420
직관적인 부분은 바로 Lambda의 값을 매우 크게 올리면, 

33
00:02:07,420 --> 00:02:10,510
W의 값을 거의 0으로 만들 것입니다.

34
00:02:10,510 --> 00:02:13,280
물론 실제로는 이렇게 되지는 않습니다.

35
00:02:13,280 --> 00:02:17,110
순히 이야기해서, 조금 더 심플한 신경망 네트워크가 되기 위한 과정으로

36
00:02:17,110 --> 00:02:19,270
수 많은 숨겨진 유닛들의 효과가 

37
00:02:19,270 --> 00:02:21,935
0이 되거나 크게 줄어든다고 생각하면 됩니다.

38
00:02:21,935 --> 00:02:25,920
마치 로지스틱 회귀분석법을 사용하는 것과 같이 조금씩 점차 가까워집니다.

39
00:02:25,920 --> 00:02:31,360
하지만 직관적으로 수 많은 숨겨진 유닛을 0으로 만든다는 것은 사실 옳은 설명은 아닙니다.

40
00:02:31,360 --> 00:02:35,225
실제로 일어나는 현상은, 지속적으로 숨겨진 유닛을 사용할 것입니다.

41
00:02:35,225 --> 00:02:37,610
하지만 각각의 숨겨진 유닛의 효과가 줄어드는 것입니다.

42
00:02:37,610 --> 00:02:41,255
조금 더 심플한 네트워크로 남는 것입니다.

43
00:02:41,255 --> 00:02:45,040
조금 더 심플한 네트워크로 남는다는 것은 overfitting 영향을 덜 받게 된다는 것이기도 합니다.

44
00:02:45,040 --> 00:02:47,715
이런 여러가지 직관은 

45
00:02:47,715 --> 00:02:50,765
일반화를 프로그램 학습에 도입하는데 도움을 줍니다.

46
00:02:50,765 --> 00:02:55,360
여러분은 직접 편차가 줄어드는 것을 보실 수 있습니다.

47
00:02:55,360 --> 00:02:57,955
일반화가 어떻게 overfitting 문제에 도움을 주는디

48
00:02:57,955 --> 00:03:01,535
한가지 예제를 더 살펴보겠습니다.

49
00:03:01,535 --> 00:03:04,030
이 예제에서는, 이렇게 생긴 tan h activation 함수를 

50
00:03:04,030 --> 00:03:08,465
이용한다는 가정을 할 것입니다.

51
00:03:08,465 --> 00:03:13,515
이것은 g(z) 는 tan (z)라는 식입니다.

52
00:03:13,515 --> 00:03:15,200
이 같은 경우에는, 

53
00:03:15,200 --> 00:03:19,427
Z의 값이 꽤 작은 이상, 

54
00:03:19,427 --> 00:03:23,410
즉, Z가 오로지 작은 범위의 파라미터 값만 갖는다면, 

55
00:03:23,410 --> 00:03:28,165
여기정도 될 수 있겠죠, 그럼 여러분은 단순히 tan h 함수의 선형적인 부분을 사용하는 것입니다.

56
00:03:28,165 --> 00:03:34,080
Z값이 이와 같이 큰 값이나 작은 값까지 이동할 수 있는 경우에만, 

57
00:03:34,080 --> 00:03:37,490
activation 함수가 덩 선형적인 함수로 정의되는 것입니다.

58
00:03:37,490 --> 00:03:40,605
여기서 여러분이 생각할 수 있는 부분은, 램다의 값이 

59
00:03:40,605 --> 00:03:42,750
즉, 일반화 파라미터의 값이 큰 경우, 

60
00:03:42,750 --> 00:03:46,530
그럼 파라미터들이 꽤 작을 것입니다.

61
00:03:46,530 --> 00:03:51,290
cos 함수에서 더 큰 값을 갖는다는 이유로 처벌받는 개념이죠. 

62
00:03:51,290 --> 00:03:56,740
만약 그럼 weight 인 W가 장은 경우, Z는 

63
00:03:56,740 --> 00:04:02,550
W이기 때문에, 엄밀히 이야기하면, 이것은 
플러스 b이구요, 

64
00:04:02,550 --> 00:04:04,440
그런데 만약 W값이 작은 경우

65
00:04:04,440 --> 00:04:07,140
Z의 값 또한 꽤 작을 것입니다.

66
00:04:07,140 --> 00:04:10,830
특히, Z가 상대적으로 작은 값을 갖는 경우,

67
00:04:10,830 --> 00:04:12,787
이 범위에서 말이죠, 

68
00:04:12,787 --> 00:04:16,045
그러면 G(z)는 대략적으로 선형일 것입니다.

69
00:04:16,045 --> 00:04:22,880
이것은 마치 모든 층이 대략적으로 선형인 것과 비슷합니다.

70
00:04:22,880 --> 00:04:24,800
마치 선형회귀와 같이 말이죠. 

71
00:04:24,800 --> 00:04:27,860
저희는 코스1에서 보았듯이,

72
00:04:27,860 --> 00:04:31,275
모든 층이 선형이라고 하면 전체 네트워크가 선형 네트워크입니다.

73
00:04:31,275 --> 00:04:33,200
그러므로 심층신경마으이 경우에도, 

74
00:04:33,200 --> 00:04:35,930
선형 activation 함수로 이루어져 있는 심층 네트워크 경우,

75
00:04:35,930 --> 00:04:39,245
결국에는 선형 함수에 한해서 산출할 수 있습니다.

76
00:04:39,245 --> 00:04:43,700
그러기 때문에 복잡한 결정을 피팅하는 것은 불가능합니다.

77
00:04:43,700 --> 00:04:49,085
비선형 의사결정은 불가능 한 것입니다. 데이터를 

78
00:04:49,085 --> 00:04:52,940
overfit할 수 있는 경우 말이죠. 이전 슬라이드에서 봤던 

79
00:04:52,940 --> 00:04:57,485
큰 편차의 overfitting 케이스처럼 말이죠. 

80
00:04:57,485 --> 00:04:59,060
요약해보자면,

81
00:04:59,060 --> 00:05:01,665
일반화가 매우 커지는 경우, 

82
00:05:01,665 --> 00:05:03,873
W 매개 변수가 굉장히 작아지고

83
00:05:03,873 --> 00:05:06,350
그리하여 Z도 상대적으로 작아질 것입니다.

84
00:05:06,350 --> 00:05:08,480
b의 효과는 일단 무시하도록 하겠습니다.

85
00:05:08,480 --> 00:05:12,935
그러므로 Z는 상대적으로 작아지구요, 

86
00:05:12,935 --> 00:05:16,250
사실은 작은 범위의 값을 갖는다고 하는게 더 맞습니다.

87
00:05:16,250 --> 00:05:19,890
activation 함수가 tan h인 경우, 

88
00:05:19,890 --> 00:05:21,790
상대적으로 선형인 함수가 될켄데요, 

89
00:05:21,790 --> 00:05:25,790
여러분의 전체 신경 네트워크는 선형 함수가 산출하는 것과 

90
00:05:25,790 --> 00:05:28,550
크게 다르지 않은 값을 계산할 것입니다.

91
00:05:28,550 --> 00:05:32,250
그러므로 비선형의 복잡한 함수보다는 심플한 일차함수의 성향이 더 강한 것입니다.

92
00:05:32,250 --> 00:05:34,650
그러므로 훨씬 덜 overfit하는 것이구요. 

93
00:05:34,650 --> 00:05:38,870
다 시 말씀드리지만, 프로그램 연습 학습에서 regularization은 직접 입력할때

94
00:05:38,870 --> 00:05:41,350
이러한 효과를 여러분께서 직접 보실 수 있습니다.

95
00:05:41,350 --> 00:05:45,680
일반화에 대한 def 토론을 마치기에 앞서

96
00:05:45,680 --> 00:05:48,310
이행 tip 한가지를 알려드리겠습니다.

97
00:05:48,310 --> 00:05:52,145
일반화를 도입하는 경우에는

98
00:05:52,145 --> 00:05:58,730
비용함수 J를 정의해서 추가적으로 항을 더했는데요

99
00:05:58,730 --> 00:06:05,810
weight가 너무 큰 값을 갖는 것에대해 페널티를 부과하는 항입니다.

100
00:06:05,810 --> 00:06:09,230
그러므로 기울기 강하를 도입하는 경우, 

101
00:06:09,230 --> 00:06:18,605
기울기 강하를 디버깅하는 방법 중 한가지는, 비용함수 J를 

102
00:06:18,605 --> 00:06:22,520
기울기 강하의 elevation 개수로 나타내는 것입니다.

103
00:06:22,520 --> 00:06:27,730
그리하여 비용함수 J가 기울기 강하가 특정 값마다 증가할 때마다 점진적으로 줄어드는 것을 볼 수 있습니다.

104
00:06:27,730 --> 00:06:30,820
그리고, 일반화를 도입하는 경우, 

105
00:06:30,820 --> 00:06:35,350
J가 새로운 뜻이 있다는 것을 기억하십시요. 

106
00:06:35,350 --> 00:06:37,735
이전에 정의되었던 J는

107
00:06:37,735 --> 00:06:39,370
이런 항인데요.

108
00:06:39,370 --> 00:06:42,290
이 경우, 점진적으로 감소하는 것을 못 볼 것입니다.

109
00:06:42,290 --> 00:06:45,030
그러므로 기울기 강하를 디버그 하는 경우, 

110
00:06:45,030 --> 00:06:49,910
이 새로운 J로 plot할 수 있도록 합니다. 이 두번째 항도 추가될 수 있도록 말이죠. 

111
00:06:49,910 --> 00:06:54,015
아니면, 특정 값별로 증가할 때마다 J가 점진적으로 감소하는 것을 못 볼테니깐요. 

112
00:06:54,015 --> 00:06:57,140
이것이 L2 일반화에 대한 내용의 전부인데요. 

113
00:06:57,140 --> 00:07:01,435
사실 대부분의 내용이 제가 딥러닝 모듈에서 사용하는 일반화 테크닉에 관한 내용이였습니다.

114
00:07:01,435 --> 00:07:05,480
딥러닝에서는, ‘dropout regularization’이라고 하는 또 한가지의

115
00:07:05,480 --> 00:07:07,390
일반화 테크닉이 있는데요, 다음 비디오에서 관련 내용을 보도록 하겠습니다.

116
00:07:07,390 --> 00:07:09,280
그럼, 다음 강의에서 살펴 보겠습니다.