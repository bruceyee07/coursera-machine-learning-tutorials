為什麼正則化幫忙解決過適問題? 為什麼它有助於減低變異問題 ？ 讓我們用一些例子來
直觀上看它如何作用 記得高偏差, 高變異 我只是從我們之前影片中
的圖形重畫一遍像這樣 現在讓我們來看大一點的深度神經網路 我知道我不曾畫過
太大或太深的網路 但讓我們看一些過適的神經網路 您有一些成本函數像是 J of w b 等於損失的總和 我們在正則化作的是增加 這個額外的項目 去懲罰權重矩陣如果太大了 所以這是弗比尼斯範數 所以為什麼用 L2範數或 弗比尼斯範數縮減參數
會比較不過適呢？ 一點直觀是如果您 設想正則化 lambda  相當相當大 它們會真的激勵到 設權重矩陣 w 趨近於 0 所以一點直觀是或許設權重趨近於 0 對於很多的隱藏單元基本上 零化是去除掉這些隱藏單元的影響 如果是這樣 這會簡化神經網路
變成小一點的神經網路 實際上, 這幾乎是羅吉斯迴歸分析 疊成幾層深 所以這會將您從 這樣過適情況趨近於左邊到高偏差情況 但希望會有一個中間值的 lambda 結果會是到在中間 "剛好正確" 的情況 但直觀上設想 lambda 很大會使得 w 趨近於 0 實際上這不會真的發生 我們可以想成零化或至少減低 很多隱藏單元的影響
所以您最終會 也許像是簡單一點的網路 它們會越來越趨近於
只是使用羅吉斯迴歸分析 這種完全零化一大堆
隱藏單元的直觀並不正確 實際上發生的是它們
還是會使用所有隱藏單元 只是它們每一個都會
變得影響比較小 但您最終會得到一個
比較簡單的網路 就像您有小一點的網路
而這樣比較不會過適 不確定這樣的直觀是否幫助您理解 當您建置正則化
在程式練習中 您實際上自己會看到
一些變異減低的結果 這是另一個嘗試用直觀來 看為什麼正則化
幫助避免過適 這裡, 我先假設我們使用 tanh 啟動函數
像這樣 這是 g of z 等於 tanh of z 如果是這樣 注意到只要 z 很小 所以當 z 只取一個小範圍的參數 也許在這邊
那您只是使用 tanh 函數的線性區域 只要 z 允許到
大一點或小一點的值像這樣 啟動函數會開始
比較不線性 直觀上您可以學到的是
如果 lambda 正則化參數變大 您會讓您的參數變小 因為它們會被懲罰如果成本函數變大 而如果 w 變小因為 z 等於 w 然後技術上是要加上 b 如果 w 變小 那 z 也會變小 特別是, 如果 z 最終以相對較小的值 只在這一小段範圍 那 g of z 會大約是線性 就好像每一層都大約是線性 就像是線性迴歸分析 而我們在第一課程看過
如果每一層 都是線性那您整個網路就只是線性網路 即使是很深的網路 在深度網路使用線性啟動函數 最後它們只能夠計算線性函數 所以不能夠來配適
很複雜的決策 很非線性的決策邊界
允許它很 過適於資料集
像是我們看到的 過適高變異情況
在前一張投影片 總結一下 如果正則化變得很大 參數 w 很小 所以 z 相對變小 先忽略對 b 的影響 所以 z 相對變小 真的，我應該說它只取一小段範圍的值 所以啟動函數如果是 tanh 會將近線性 所以您整個神經網路計算的
不會 離一個大的線性函數很遠
也就是相當 簡單函數而不會是
很複雜非線性函數 那種比較會過適 再提醒一次，當您自己進入程式練習的正則化 您會自己看到這些影響 在結束我們的正則化討論之前 我只是要給您一個建置的提示 也就是當建置正則化時 我們拿我們的成本函數 J 的定義
我們修改它 增加這個額外的項目
如果這個權重太大懲罰它 所以如果您建置梯度下降 一個梯度下降除錯的步驟是
畫成本函數 J 是一個函數 對於梯度下降的迭代數目
而您要看到 成本函數 J 單調下降
經過每一次的梯度下降 如果您建置正則化 請記得 J 現在有新的定義 如果您畫舊的 J 的定義 只有第一項 那您可能看不到單調下降 所以在梯度下降除錯時
請確認您畫的是 新的 J 的定義
包含了第二項 否則您也許看不到 J 單調下降
在每一個單一評估後 所以這是 L2 正則化
實際上 是一種正則化技巧
我用在大部分的深度學習模型中 在深度學習還有另外一種
有時候用來正則化的技巧 稱為 dropout 正則化 讓我們在下一段影片看看它