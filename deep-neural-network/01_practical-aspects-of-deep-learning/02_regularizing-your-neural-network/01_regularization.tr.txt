Eğer sinir ağınızın verinizi aşırı öğrendiğini düşünüyorsanız, bu yüksek değişinti probleminiz olduğu anlamına gelir. Bu durumda denemeniz gereken ilk şey muhtemelen düzenlileştirmedir. Yüksek değişinti problemini ele almanın bir diğer güvenilir yolu daha fazla eğitim verisi toplamaktır. Ama her zaman daha fazla veri toplama şansınız olmayabilir, ya da daha fazla veri toplamak masraflı olabilir. Ama düzenlileştirme çoğu zaman ağınızdaki aşırı öğrenmeyi önlemeye veya ağdaki hataları azaltmanıza yardımcı olacaktır. Dolayısıyla hadi düzenlileştirmenin nasıl çalıştığını görelim. Bu fikirleri S biçimli bağlanım modelinde uygulayalım. Hatırlayalım, S biçimli bağlanım modelinde şu şekilde tanımlanan maliyet fonksiyonunu en aza indirmeye çalışıyordunuz. Eğitim örneklerinin ayrı ayrı hesaplanan maliyetlerinin toplamı. Hatırlayacaksınız ki w ve b S biçimli bağlanımdaki değişkenlerdir. W, x boyutlu bir değişken yöneyi ve b bir reel sayıdır. Ve S biçimli bağlanımı düzenlileştirmek için yapmanız gereken ona düzenlileştirme değişkeni lambdayı eklemektir. Birazdan bundan biraz daha bahsedeceğim ama lambda bölü 2m çarpı w karenin büyüklüğü. Yani burada, w karenin büyüklüğü eşittir j eşittir 1'den nx'e kadar olan wj'lerin karelerinin toplamına. Aynı zamanda w değişken yöneyinin öklid büyüğü olarak, devrik w ile w çarpımı şeklinde yazılabilir. Ve buna L2 düzenlileştirmesi denir. Çünkü burada, siz öklid büyüklüğü kullanıyorsunuz ya da başka bir deyişle w değişken döneyinin L2 büyüklüğü. Peki neden sadece w değişkenini düzenlileştiriyorsunuz? Neden buraya da b değişkeni için bir şey eklemiyoruz? Pratikte bunu yapabilirsiniz ama ben genellikle bunu kullanmıyorum. Çünkü değişkenlere bakarsanız, w genellikle çok büyük boyutlu bir değişken yöneyi. Özellikle yüksek değişinti probleminde belki w çok fazla değişkene sahiptir ve bu yüzden siz tüm değişkenleri iyi öğrenemiyor olabilirsiniz. Ama b sadece bir sayı. Bu yüzden neredeyse değişkenlerin tamamı b'nin aksine w yöneyinde. Ve eğer bu son terimi eklerseniz, pratikte çok büyük bir fark yaratmayacaktır. Çünkü b çok fazla sayıdaki değişkenden sadece biri. Pratikte ben genellikle onu eklemeye zahmet etmiyorum. Ama siz isterseniz ekleyebilirsiniz. Yani L2 düzenlileştirmesi en yaygın düzenlileştirme türüdür. Belki bazı insanların L1 düzenlileştirmesi hakkında konuştuğunu duymuşsunuzdur. ve bu durumda bu L2 büyüklüğü yerine bu lambda bölü m çapı bunların toplamı terimini eklersiniz. Bu aynı zamanda w değişken yöneyinin L1 büyüklüğü olarak da adlandırılır. yani orada küçük alt indis 1 gibi. Ve sanırım paydaya m veya 2m koymak sadece ölçeği değiştirir. Eğer siz L1 düzenlileştirmesi kullanırsanız, w yöneyi seyrek olacaktır. Yani bu w yöneyinde çok fazla sıfır olduğu anlamına geliyor. Ve bazıları diyor ki bu modeli sıkıştırmaya yardım eder, çünkü değişkenlerin bir kısmı sıfır ve bu yüzden modeli depolamak için daha az hafızaya ihtiyacınız var. Buna karşın ben modeli seyrekleştirmek için L1 düzenlileştirmesi kullanmanın pratikte küçük bir faydası olduğunu sonucuna vardım. Bu yüzden ben onun çok fazla kullanılmadığını düşünüyorum, en azından modeli sıkıştırmak amacıyla. Ve insanlar sinir ağlarını eğitirken, L2 düzenlileştirmesini çok daha yaygın olarak kullanıyorlar. Pardon, formüldeki küçük bir gösterim hatasını düzeltiyorum. Son bir detay, Buradaki lambdaya düzenlileştirme değişkeni denir. Ve onu genellikle geliştirme kümesi ya da holdout çapraz geçerleme metodu kullanarak belirlersiniz. Bu metotda çeşitli değerleri deneyip ve hangisinin eğitim verinizde daha başarılı olduğuna bakar ve aynı zamanda değişkenlerinin L2 büyüklüğünü olabildiğince küçültmeye çalışırsınız. Bu aşırı öğrenmeyi engellemeye yardım eder. Yani lamba ayarlamanız gereken başka bir hiper değişkendir. Ve burada, programla alıştırmaları için lambda Python programlama dilinde ayrılmış bir kelimedir. Bu yüzden programla alıştırmasında python diliyle çakışmamak amacıyla lambda yerine lambd kullanacağız. Yani düzenlileştirme değişkeni olarak lambd kullnacağız. Sonuç olarak S biçimli bağlanımlarda L2 düzenlileştirmesinin nasıl yapıldığını öğrendik. Peki ya sinir ağları için? Bir sinir ağında maliyet fonksiyonunuz w[1] ve b[1]'den w[L] ve b[L]'e kadar tüm değişkenlerinize bağlıdır. Burada L sinir ağınızdaki katman sayısıdır. Ve maliyet fonksiyonu, m tane eğitim örneğiniz için yitimlerin toplamına eşittir. Ve düzenlileştirmede, siz lambda bölü 2m çarpı w değişkenlerinin toplamını eklersiniz, ya da buna onun kare büyüklüğü de diyebiliriz. Dizeyin kare büyüklüğü her i ve j değişkeni için dizeydeki her bir elemanın karelerinin toplamıdır. Eğer toplamın endekslerini isterseniz, bu i eşittir 1'den n[l-1]'a kadar ve bu j eşittir 1'den n[l]'e kadardır. Çünkü w dizeyinin boyutları n[l-1] ve n[l]'dir. Ve bunlar l-1 ve l'inci katmanlardaki eleman sayılarıdır. Ve bu dizey büyüklüğüne Frobenius büyüklüğü denir ve alt indiste F ile gösterilir. Lineer cebirin gizemli teknik sebeplerinden dolayı buna L2 büyüklüğü denmiyor. Onun yerine dizeyin Frobenius büyüklüğü deniyor. Biliyorum ona dizeyin L2 büyüklüğü demek kulağa daha doğal geliyor, ama gerçekten bilmenizin gerekmediği sebeplerle uzlaşım için Frobenius büyüklüğü olarak adlandırılıyor. O sadece dizeyin elemanlarının karelerinin toplamı anlamına gelir. Peki bunun için bayır inişini nasıl uygularsınız? Bundan önce, dw'yi geri yayma kullanarak hesaplıyorduk. Geri yayma bize J'nin w'ye göre ya da herhangi w[l]'e göre kısmı türevini veriyordu. Ve sonra w[l]'yi w[l] eksi öğrenme oranı çarpı dw olarak güncelliyorduk. Yani ekstra düzenlileştirme terimi eklenmeden önce böyle yapıyorduk. Şimdi ise bu düzenlileştirme terimini eklediğimiz için, yapmanız gereken dw'yi alıp ona lambda bölü m çarpı w eklemektir. Ve sonra eskiden olduğu gibi bu güncellemeyi hesaplıyoruz. Ve öyle görünüyor ki bu yeni dw[l] tanımıyla birlikte maliyet fonksiyonunuzun değişkenlerinize göre kısmı türevi olma özelliğini koruyor. ,Şimdi siz dw'nin tanımına ekstra bir düzenlileştirme terimi eklediniz, ve bu sebeple L2 düzenlileştirmesine bazen ağırlık sönümü(weight decay) de denir. Yani eğer ben bu dw[l] tanımını alıp burada yerine koyarsam, o zaman siz güncellemenin w[l] eşittir w[l] çarpı öğrenme oranı alfa çarpı geri yayılımdan gelen terim artı lambda bölü m çarpı w[l] haline geldiğini göreceksiniz. Tabii burada eksi olacak. Ve bu da w[l] eksi alfa çarpı lambda bölü m çarpı w[l] - alfa çarpı geri yayılımdan gelen terime eşittir. Ve bu terim gösteriyor ki w[l] dizeyiniz ne olursa olsun onu bir parça küçülteceksiniz, değil mi? Bu aslında w dizeyini alıp onu 1 eksi alfa çarpı lambda bölü m ile çarpmakla aynı şey. Siz aslında w dizeyini alıp ondan alfa çarpı lamba bölü m tane bunu çıkarıyorsunuz. Bu w dizeyini birden biraz daha küçük olan bu terimle çarpmak gibi bir şey. Yani bu L2 düzenlileştirmesine neden ağırlık sönümü dendiğini açıklıyor. Çünkü yine aynı normal bayır inişinde olduğu gibi w'yi ondan alfa çarpı geri yayılımdan gelen eğim çıkararak güncelliyorsunuz. Ama bu sefer w'yi aynı zamanda birden biraz küçük olan bu terim ile çarpıyoruz. Dolayısıyla L2 düzenlileştirmesinin diğer adı ağırlık sönümüdür. Ben gerçekten bu ismi kullanmayacağım, ama böyle adlandırılmasındaki sezgi buradaki ilk terimdir ve o buna eşittir. Yani siz yalnızca ağırlık dizeyini birden küçük bir sayıyla çarpıyorsunuz. Böylelikle L2 düzenlileştirmesinin sinir ağlarında nasıl uygulandığı öğrendiniz. Şimdi, [birisi] bana şu soruyu sordu. Hey Andrew, düzenlileştirme aşırı öğrenmeyi neden önlüyor? Hadi bir sonraki videoya bakalım, ve düzenlileştirmenin aşırı öğrenmeyi nasıl engellediğine dair daha fazla bilgi sahibi olalım.