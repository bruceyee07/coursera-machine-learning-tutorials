1
00:00:00,730 --> 00:00:03,660
Se você suspeitar que a sua rede neural está sobre-ajustando seus dados,

2
00:00:03,660 --> 00:00:05,840
ou seja, você está tendo um problema de alta variância,

3
00:00:05,840 --> 00:00:09,400
uma das primeiras coisas que você 
deve tentar talvez seja a regularização.

4
00:00:09,400 --> 00:00:11,246
Outro modo de se lidar com alta variância,

5
00:00:11,246 --> 00:00:13,917
é compilar mais dados de treino, o que é bem confiável.

6
00:00:13,917 --> 00:00:15,869
Mas você nem sempre consegue mais dados de treino,

7
00:00:15,869 --> 00:00:17,850
ou seria caro conseguir mais dados.

8
00:00:17,850 --> 00:00:21,760
Porém, adicionando regularização geralmente
 prevenirá o sobre-ajustamento,

9
00:00:21,760 --> 00:00:23,910
ou reduzirá os erros na sua rede.

10
00:00:23,910 --> 00:00:26,020
Então, vamos ver como a regularização funciona.

11
00:00:26,020 --> 00:00:28,780
Vamos desenvolver estas ideias,
usando regressão logística.

12
00:00:28,780 --> 00:00:33,220
Lembre-se que para regressão logística,
você tenta minimizar a função de custo J,

13
00:00:33,220 --> 00:00:37,050
a qual é definida como esta função de custo,

14
00:00:37,050 --> 00:00:41,290
soma dos seus exemplos de treino, das perdas das previsões individuais

15
00:00:41,290 --> 00:00:45,140
em exemplos diferentes, onde você se lembra que w

16
00:00:45,140 --> 00:00:48,175
e b na regressão logística, são os parâmetros.

17
00:00:48,175 --> 00:00:54,620
Logo, 'w' é um vetor parâmetro, de dimensão nₓ,
 e 'b' ´é um número real.

18
00:00:54,620 --> 00:00:58,979
E para adicionar regularização à regressão logística,
o que você faz é adicionar a ela isto,

19
00:00:58,979 --> 00:01:03,154
que é chamado parâmetro de regularização λ
 (lambda).

20
00:01:03,154 --> 00:01:04,609
Eu vou falar mais sobre isso em um segundo.

21
00:01:04,609 --> 00:01:10,072
Mas lambda/2m vezes a [(norma de w) ao quadrado].

22
00:01:10,072 --> 00:01:15,840
Então aqui, a norma de w ao quadrado
 é apenas igual a

23
00:01:15,840 --> 00:01:22,580
somatória de j (variando de 1 até nx) de wj ao quadrado,
ou isso também pode ser escrito como

24
00:01:22,580 --> 00:01:27,750
w transposto w, ´´é apenas uma norma euclidiana quadrada do vetor de parâmetro w.

25
00:01:27,750 --> 00:01:31,910
E isso é chamado de 
regularização L₂

26
00:01:33,700 --> 00:01:36,618
Porque aqui, 
você está usando a norma euclidiana,

27
00:01:36,618 --> 00:01:38,877
ou a norma L₂ 
com o vetor de parâmetro w.

28
00:01:38,877 --> 00:01:41,780
Agora, por que você regulariza apenas o parâmetro w?

29
00:01:41,780 --> 00:01:47,130
Por que não adicionamos algo aqui sobre b também?

30
00:01:47,130 --> 00:01:51,210
Na prática, você poderia fazer isso, 
mas eu geralmente apenas o omito.

31
00:01:51,210 --> 00:01:56,310
Porque se você olhar para seus parâmetros,
w é normalmente um vetor de parâmetro

32
00:01:56,310 --> 00:02:00,159
de alta dimensão, especialmente 
com um problema de variância alta.

33
00:02:00,159 --> 00:02:02,250
Talvez w apenas tenha um monte de parâmetros,

34
00:02:02,250 --> 00:02:06,600
então não está encaixando todos os parâmetros bem,
 enquanto b é apenas um número.

35
00:02:06,600 --> 00:02:10,200
Então, quase todos os parâmetros estão em w, ao invés de b.

36
00:02:10,200 --> 00:02:12,890
E você adicionar este último termo, na prática,

37
00:02:12,890 --> 00:02:14,040
não vai fazer muita diferença,

38
00:02:14,040 --> 00:02:17,960
porque b é apenas um parâmetro 
em meio a um grande número de parâmetros.

39
00:02:17,960 --> 00:02:21,500
Na prática, eu geralmente 
não me dou o trabalho de inclui-lo.

40
00:02:21,500 --> 00:02:22,962
Mas você pode, se quiser.

41
00:02:22,962 --> 00:02:27,510
Então a regularização L₂
 é o tipo mais comum de regularização.

42
00:02:27,510 --> 00:02:32,042
Você também já deve ter ouvido algumas pessoas 
falarem sobre regularização L₁

43
00:02:32,042 --> 00:02:38,422
quando você adiciona, 
ao invés da norma L₂,

44
00:02:38,422 --> 00:02:45,674
um termo, que é lambda/m da somatória disto.

45
00:02:45,674 --> 00:02:49,716
E isso também é chamado de 
norma L₁ do vetor de parâmetro w,

46
00:02:49,716 --> 00:02:52,843
por isso o subscrito 1 está ali, certo?

47
00:02:52,843 --> 00:02:58,050
E eu acho que colocar 'm' ou '2m' no denominador
é apenas uma constante de escala.

48
00:02:58,050 --> 00:03:03,020
Se você usar a regularização L₁,
 w vai acabar ficando disperso.

49
00:03:03,020 --> 00:03:08,040
E o que isso significa é que o vetor w 
ficará com um monte de zeros.

50
00:03:08,040 --> 00:03:11,700
E alguns dizem que isso pode 
ajudar a comprimir o modelo,

51
00:03:11,700 --> 00:03:16,140
porque o conjunto de parâmetros é zero,
e você precisará de menos memória para armazenar o modelo.

52
00:03:16,140 --> 00:03:19,850
Embora eu ache que, na prática, 
a regularização L₁ não ajude muito

53
00:03:19,850 --> 00:03:20,870
a tornar seu modelo esparso.

54
00:03:20,870 --> 00:03:23,870
Então, eu não acho que ela seja tão usada assim,

55
00:03:23,870 --> 00:03:26,520
ao menos não para o propósito de comprimir um modelo.

56
00:03:26,520 --> 00:03:28,472
E quando as pessoas treinam suas redes,

57
00:03:28,472 --> 00:03:31,423
a regularização L₂ é usada com mais frequência.

58
00:03:31,423 --> 00:03:34,301
Perdão, só consertando um pouco da notação aqui.

59
00:03:34,301 --> 00:03:35,929
Um último detalhe.

60
00:03:35,929 --> 00:03:42,823
Lambda aqui é chamada
de parâmetro de regularização.

61
00:03:45,267 --> 00:03:48,172
E normalmente, você estabelece isso usando conjunto de desenvolvimento,

62
00:03:48,172 --> 00:03:50,021
ou usando validação cruzada.

63
00:03:50,021 --> 00:03:53,274
Quando você tenta uma 
variedade de valores e vê qual funciona melhor,

64
00:03:53,274 --> 00:03:57,662
em termos de pesar entre 
fazer bem no seu conjunto de treino

65
00:03:57,662 --> 00:04:01,007
ou também definir que dois dos seus parâmetros normais diminuam,

66
00:04:01,007 --> 00:04:03,088
o que ajuda a prevenir sobre-ajustamento.

67
00:04:03,088 --> 00:04:07,165
Então lambda é um outro 
hiperparâmetro que talvez precisa ser ajustado.

68
00:04:07,165 --> 00:04:09,550
E a propósito, quanto aos exercícios de programação,

69
00:04:09,550 --> 00:04:14,250
lambda é uma palavra-chave reservada 
na linguagem de programação Python.

70
00:04:14,250 --> 00:04:18,300
Então no exercício de programação, nós teremos lambd,

71
00:04:19,340 --> 00:04:23,690
sem o 'a', para não conflitar com a
 palavra-chave reservada em Python.

72
00:04:23,690 --> 00:04:27,740
Então nós usamos 'lambd' para representar
o parâmetro de regularização de lambda.

73
00:04:29,190 --> 00:04:33,320
Então é assim que você implementa 
a regularização L₂ para regressão logística.

74
00:04:33,320 --> 00:04:35,280
E em relação a rede neural?

75
00:04:35,280 --> 00:04:39,789
Em uma rede neural, você tem uma função de custo,

76
00:04:39,789 --> 00:04:44,621
que é uma função de todos os seus parâmetros,
w'¹', b'¹' até w'ᴸ'. x + b'ᴸ'

77
00:04:44,621 --> 00:04:48,906
onde L maiúsculo é o 
número de camadas em sua rede neural.

78
00:04:48,906 --> 00:04:54,129
Assim, a função de custo é esta,
1/m vezes a somatória das perdas,

79
00:04:54,129 --> 00:04:58,066
(i = 1 até m) 
dos seus m exemplos de treino.

80
00:04:58,066 --> 00:05:03,087
E na regularização, você adiciona lambda

81
00:05:03,087 --> 00:05:10,190
sobre 2m vezes a somatória de todos os seus parâmetros w,
 sua matriz de parâmetro é w,

82
00:05:10,190 --> 00:05:14,857
(para l = 1 até L) da norma quadrada,

83
00:05:14,857 --> 00:05:19,749
onde esta norma de uma matriz, ou seja,

84
00:05:19,749 --> 00:05:23,922
a norma ao quadrado é definida como a somatória de i, somatória de j,

85
00:05:23,922 --> 00:05:29,250
de cada dos elementos dessa matriz, ao quadrado.

86
00:05:29,250 --> 00:05:31,248
E você quiser os indicadores desta soma,

87
00:05:31,248 --> 00:05:35,253
é a somatória de i=1 a n'ˡ ̄ ¹',

88
00:05:35,253 --> 00:05:38,537
soma de j=1 a n'ˡ',

89
00:05:38,537 --> 00:05:44,497
porque w é uma matriz de 
dimensões n'ˡ ̄ ¹' x n'ˡ',

90
00:05:44,497 --> 00:05:51,320
onde estes são o número de unidades
nas camadas ocultas [l-1] ou nas camadas [l].

91
00:05:51,320 --> 00:05:57,447
Então esta norma da matriz é chamada de 
norma de Frobenius

92
00:05:57,447 --> 00:06:03,710
da matriz, e é assinalada com um F no subscrito.

93
00:06:03,710 --> 00:06:07,266
Então por razões técnicas ocultas de Álgebra Linear,

94
00:06:07,266 --> 00:06:10,491
isso não é chamado de 'norma L₂'
de uma matriz.

95
00:06:10,491 --> 00:06:14,620
Ao invés disso, é chamado de 
norma de Frobenius de uma matriz.

96
00:06:14,620 --> 00:06:16,980
Eu sei que parece que seria mais natural 
apenas chamá-la 'norma L₂ da matriz',

97
00:06:16,980 --> 00:06:21,760
mas realmente por razões ocultas, 
que você não precisa saber,

98
00:06:21,760 --> 00:06:24,090
por convenção, 
isto é chamado de norma de Frobenius.

99
00:06:24,090 --> 00:06:27,232
Isso é apenas a 
somatória dos quadrados dos elementos de uma matriz.

100
00:06:27,232 --> 00:06:30,060
Então, como você pode 
implementar gradiente descendente com isso?

101
00:06:30,060 --> 00:06:35,343
Anteriormente, nós completaríamos dw 
usando retro-propagação ("backprop"),

102
00:06:35,343 --> 00:06:40,626
onde a retro-propagação nos daria 
a derivada parcial

103
00:06:40,626 --> 00:06:46,166
de J em relação a w (∂j/∂w), 
ou realmente w para qualquer [l] dado.

104
00:06:46,166 --> 00:06:52,995
E em seguida, você atualiza w'ˡ', para
w'ˡ' - [(a taxa de aprendizagem α) vezes dw'ˡ'].

105
00:06:52,995 --> 00:06:57,890
E isso ocorre antes de adicionarmos
este termo de regularização extra ao objetivo.

106
00:06:57,890 --> 00:07:02,941
Agora que adicionamos 
este termo de regularização ao objetivo,

107
00:07:02,941 --> 00:07:07,643
o que você faz é pegar dw e adiciona a ele lambda/m vezes w'ˡ'.

108
00:07:07,643 --> 00:07:10,760
E então você apenas calcula esta atualização, 
da mesma forma que antes.

109
00:07:10,760 --> 00:07:14,829
Acontece que com esta nova 
definição de dw'ˡ',

110
00:07:14,829 --> 00:07:19,315
este dw'ˡ' novo ainda é uma definição 
correta da derivada

111
00:07:19,315 --> 00:07:23,385
da sua função de custo, 
em relação aos seus parâmetros,

112
00:07:23,385 --> 00:07:27,980
agora que você adicionou 
o termo de regularização extra ao final.

113
00:07:29,260 --> 00:07:33,990
E é por essa razão que a regularização L₂
 às vezes, é

114
00:07:33,990 --> 00:07:36,730
chamada decaimento* (do peso).
                                                                    [* NT: "weight decay" - Hinton 1989 ].

115
00:07:36,730 --> 00:07:42,348
Então, se eu pegar esta definição de dw'ˡ'
 e apenas a ligá-la aqui,

116
00:07:42,348 --> 00:07:47,012
você pode ver que a atualização é w'ˡ' = w'ˡ' vezes

117
00:07:47,012 --> 00:07:51,994
a taxa de aprendizagem alfa vezes o resultado da retro-propagação

118
00:07:54,311 --> 00:08:02,816
+ (lambda/m) vezes w'ˡ'.

119
00:08:02,816 --> 00:08:04,431
Ponha o sinal de menos ali.

120
00:08:04,431 --> 00:08:09,382
E isso é igual a w'ˡ'- alfa vezes

121
00:08:09,382 --> 00:08:14,494
lambda/m vezes w'ˡ' - alfa vezes

122
00:08:14,494 --> 00:08:18,822
o que você obteve na retro-propagação.

123
00:08:18,822 --> 00:08:22,324
E este termo mostra que 
qualquer que seja a matriz w'ˡ',

124
00:08:22,324 --> 00:08:25,480
você a tornará 
um pouco menor, certo?

125
00:08:25,480 --> 00:08:28,270
Isso, na verdade, é como se 
você estivesse pegando a matriz w

126
00:08:28,270 --> 00:08:33,030
e multiplicando por 
1-alfa vezes lambda/m.

127
00:08:33,030 --> 00:08:38,279
Você está realmente pegando a matriz w 
e subtraindo alfa lambda/m vezes isto.

128
00:08:38,279 --> 00:08:41,130
Como você está multiplicando a 
matriz w por este número,

129
00:08:41,130 --> 00:08:43,528
que vai ficar um pouco menor que 1.

130
00:08:43,528 --> 00:08:48,688
E é por isso que a regularização da norma L₂
também é chamada de decaimento (do peso).

131
00:08:48,688 --> 00:08:53,716
Porque é como se fosse 
o gradiente descendente comum, onde você atualiza

132
00:08:53,716 --> 00:08:59,260
w, subtraindo alfa vezes o gradiente original
 que você obteve através da retro-propagação.

133
00:08:59,260 --> 00:09:04,616
Mas agora você também está multiplicando w 
por esta coisa,

134
00:09:04,616 --> 00:09:08,324
a qual é um pouco menor que 1.

135
00:09:08,324 --> 00:09:11,782
Então, o nome alternativo para a regularização L₂ é
decaimento (do peso).

136
00:09:11,782 --> 00:09:15,641
Eu não vou usar esse nome, mas o motivo

137
00:09:15,641 --> 00:09:21,030
para que ela seja chamada de decaimento
é que este primeiro termo aqui é igual a este.

138
00:09:21,030 --> 00:09:25,620
Então você está apenas multiplicando as métricas de peso
por um número ligeiramente menor que 1.

139
00:09:25,620 --> 00:09:28,511
E é assim que você implementa a 
regularização L₂ em uma rede neural.

140
00:09:29,545 --> 00:09:32,796
Agora, uma pergunta que geralmente me fazem é: "Andrew,

141
00:09:32,796 --> 00:09:35,675
por que a regularização previne o sobre-ajuste?"

142
00:09:35,675 --> 00:09:37,462
Veremos isso no próximo vídeo,

143
00:09:37,462 --> 00:09:41,805
e intuiremos como a regularização previne o sobre-ajuste.