1
00:00:00,410 --> 00:00:04,180
L2 일반화 과 drop out 일반화 외에도 

2
00:00:04,180 --> 00:00:08,050
신경망의 overfitting을 줄일 수 있는 몇가지의 
테크닉이 있습니다. 한번 살펴보겠습니다.

3
00:00:08,050 --> 00:00:09,200
한번 자세히 살펴볼까요.

4
00:00:09,200 --> 00:00:10,955
CAT crossfire를 피팅한다고 해봅시다.

5
00:00:10,955 --> 00:00:15,590
만약 overfitting의 경우에는, 
트레이닝 데이터가 도와줄 수 있는데요,

6
00:00:15,590 --> 00:00:20,970
트레이닝데이터는 비쌀 수 있습니다.
그리고 또 데이터를 단순히 더 수집할 수 없는 경우도 있습니다.

7
00:00:20,970 --> 00:00:24,670
하지만 이런 이미지를 가지고 트레이닝 세트를 증가시킬 수 있는데요,

8
00:00:24,670 --> 00:00:27,440
그리고 예를 들어, 
가로로 뒤집어서 트레이닝 세트에 

9
00:00:27,440 --> 00:00:29,570
추가할 수도 있고요,

10
00:00:29,570 --> 00:00:32,950
그러면 여기 트레이닝 세트의
한가지 예뿐만 아니라,

11
00:00:32,950 --> 00:00:35,320
이 트레이닝 예시도 넣을 수 있습니다.

12
00:00:35,320 --> 00:00:38,040
그러면 이렇게 가로로 이미지를 뒤집어서,

13
00:00:38,040 --> 00:00:40,670
트레이닝 세트의 크기를 2배로 만들 수 있는 것입니다.

14
00:00:40,670 --> 00:00:44,530
이제 여려 분의 트레이닝 세트가 중복되기 때문에

15
00:00:44,530 --> 00:00:50,200
완전히 추가로 새로운 독립적인 이미지 샘플을 추가한 것보다는
좋지 못합니다.

16
00:00:50,200 --> 00:00:55,290
하지만 더 추가적인 고양이 사진을 찾는데
시간 소비할 필요 없이 

17
00:00:55,290 --> 00:00:57,100
이렇게 빨리 진행할 수 있는 점이 있습니다.

18
00:00:57,100 --> 00:00:59,710
가로로 뒤집는 방법 말고 또,

19
00:00:59,710 --> 00:01:02,170
이미지를 크롭할 수 있습니다.

20
00:01:02,170 --> 00:01:06,220
여기서는 회전하고 임의로 이미지를 줌인 처리 했는데요

21
00:01:06,220 --> 00:01:07,720
이 이미지는 아직까지 고양이처럼 보입니다.

22
00:01:07,720 --> 00:01:11,830
이렇게 이미지를 찌그러트리거나
변형을 주어 

23
00:01:11,830 --> 00:01:16,490
데이터세트를 확장하는 방법이 있습니다.
바로 이와 같이 가짜의 추가 트레이닝 샘플을 만드는 것이죠.

24
00:01:16,490 --> 00:01:20,780
다시 한번 말씀 드리자면, 이러한 가짜의 샘플은
정보적으로는 완전히 새로운 고양이 이미지를 

25
00:01:20,780 --> 00:01:25,900
추가하는 것보다는 도움이 되지 않습니다.

26
00:01:25,900 --> 00:01:28,982
하지만 이렇게 하는 것 또한 
방법이기 때문에 무료로 

27
00:01:28,982 --> 00:01:30,562
대치 비용없이 진행할 수 있습니다.

28
00:01:30,562 --> 00:01:37,007
비싸지 않은 방법으로써,
알고리즘에 데이터를 더 제공하고 

29
00:01:37,007 --> 00:01:42,762
일반화를 통해 over fitting을 결과적으로
줄일 수 있는 하나의 방법입니다.

30
00:01:42,762 --> 00:01:47,299
이런 샘플들을 합성하여 
여러분의 알고리즘에게 알려주는 것은 바로

31
00:01:47,299 --> 00:01:51,570
고양이의 사진은 가로로 뒤집더라도 
고양이 사진이라는 것입니다.

32
00:01:51,570 --> 00:01:53,100
눈치채셨겠지만, 저는 세로로 뒤집지는 않았습니다.

33
00:01:53,100 --> 00:01:55,820
왜냐면, 거꾸로 서있는 고양이는 원치 않기 때문이죠, 맞죠?

34
00:01:55,820 --> 00:01:58,940
또, 임의로 이미지 일부분을 줌인 하였는데요. 

35
00:01:58,940 --> 00:02:00,270
이 경우에도 역시 고양이 사진은 마찬가지로 동일합니다.

36
00:02:00,270 --> 00:02:04,750
시각적 캐릭터 인식 기능에서는 여러분은 

37
00:02:04,750 --> 00:02:08,510
특정 숫자를 갖고 임의의 회전과 찌그러트림으로
데이터세트를 불러올 수 있습니다.

38
00:02:08,510 --> 00:02:11,620
만약에 이런 것을 트레이닝 세트에 추가하면, 

39
00:02:11,620 --> 00:02:13,490
이 숫자들은 아직 모두 4입니다.

40
00:02:14,740 --> 00:02:18,320
예제에서 저는 아주 강항 찌그러트림을 적용했는데요,

41
00:02:18,320 --> 00:02:23,215
이것은 웨이브가 많이 적용됐고, 
실제로 이렇게 많이 찌그러트릴 필요는 없습니다만

42
00:02:23,215 --> 00:02:27,095
여기서보다 조금 덜 적용시키면 되겠습니다.

43
00:02:27,095 --> 00:02:29,255
이 예제에서 조금 더 확실히 보여드리기 위해서 그랬습니다.
아시겠죠?

44
00:02:29,255 --> 00:02:32,945
하지만 실제로는 조금 덜 찌그러트리는데요

45
00:02:32,945 --> 00:02:35,490
이것은 거의 휘어진 것과 같은 숫자 4인데요

46
00:02:35,490 --> 00:02:40,410
그리하여 data augmentation을 이용해서 
일반화 테크닉에 일부로 적용시키면 됩니다.	

47
00:02:40,410 --> 00:02:43,020
일반화와 비슷하게, 

48
00:02:43,020 --> 00:02:46,970
쓰이는 기술 중 early stopping이라고 하는 기술이 있습니다.

49
00:02:46,970 --> 00:02:52,010
여기서는 무엇을 할 것이냐면, 
기울기 강하를 실행하면서

50
00:02:52,010 --> 00:02:54,010
트레이닝 오류나, 

51
00:02:54,010 --> 00:02:57,860
트레이닝 세트에서의 01 classification 오류 또는,

52
00:02:57,860 --> 00:03:00,860
최적화 J 비용함수를 그리십시오.

53
00:03:00,860 --> 00:03:04,200
그 그래프는 점진적으로 이렇게 감소하겠죠. 맞죠? 

54
00:03:04,200 --> 00:03:05,610
왜냐면 여러분이 점차 트레이닝 시키면서

55
00:03:05,610 --> 00:03:09,120
J 비용함수를 둘러싸 트레이닝 시키므로
이상적으로는 감소하겠죠.

56
00:03:09,120 --> 00:03:11,870
early stopping에서는 이 그래프를 그리고, 

57
00:03:11,870 --> 00:03:15,970
또한, dev set 오류를 그립니다.

58
00:03:17,020 --> 00:03:20,920
다시한번, 이것은 development set에서의 classification 오류
일수 있고 또는,

59
00:03:20,920 --> 00:03:25,979
logistic loss처럼 비용함수일 수도 있습니다.
또는 dev set에서의 log loss과 같은 함수일수도 있고요. 

60
00:03:25,979 --> 00:03:29,770
여러분이 발견하겠지만, 
dev set 오류는 어느 정도

61
00:03:29,770 --> 00:03:32,950
감소하다가 이 지점 이후로 다시 증가하는
것을 볼 수 있습니다.

62
00:03:32,950 --> 00:03:35,876
early stopping이 하는 것은

63
00:03:35,876 --> 00:03:40,167
신경망이 여기까지 테스트를 잘 수행했으니

64
00:03:40,167 --> 00:03:43,640
반쯤 지난 시점에서 신경망에서의 트레이닝을 그만 진행하고 
싶습니다.

65
00:03:43,640 --> 00:03:47,310
값은 여기까지만 dev set오류를 얻은 데 까지만 갖는 것입니다.

66
00:03:47,310 --> 00:03:48,260
이것이 어떻게 작동하는 것일까요?

67
00:03:48,260 --> 00:03:51,490
만약 신경망에서 반복 테스트를 몇 번 수행 안 한 경우, 

68
00:03:51,490 --> 00:03:55,185
w 매개 변수는 거의 0에 가까울 것입니다.

69
00:03:55,185 --> 00:03:59,720
임의의 초기화에서 아마 w를 작은 값으로 초기화하기 때문에

70
00:03:59,720 --> 00:04:04,190
w를 긴 시간 트레이닝 하기 전에는 작은 값일 것입니다. 

71
00:04:04,190 --> 00:04:08,060
그리고 반복 수행을 하면서 w의 값은 커지고 또 커질텐데요,

72
00:04:08,060 --> 00:04:14,120
그러면 여기에서는 신경망에서의 w 파라미터수는 
훨씬 더 큰 값으로 되어 있을 것입니다.

73
00:04:14,120 --> 00:04:18,560
early stopping은 w의 비율이 중간 정도 되는 시점에서 

74
00:04:18,560 --> 00:04:23,286
중지시키는 역할을 합니다.

75
00:04:23,286 --> 00:04:28,920
그러므로 L2 일반화와 비슷하게, 
신경망에서 w 파라미터수가 비슷한 노름을 선정하여

76
00:04:28,920 --> 00:04:34,630
결과적으로 신경망이 덜 overfitting하게 만들어주는
것입니다.

77
00:04:34,630 --> 00:04:37,270
그리고 early stopping 이라는 용어는 
말 그대로 신경망의 

78
00:04:37,270 --> 00:04:40,760
트레이닝을 일찍 중간에 
스탑시키는 것을 말합니다.

79
00:04:40,760 --> 00:04:43,760
저는 신경망을 트레이닝 시킬 때 
가끔 early stopping 을 이용합니다.

80
00:04:43,760 --> 00:04:46,650
한가지 단점이 있기는 한데요.
설명해드리겠습니다.

81
00:04:46,650 --> 00:04:50,870
저는 머신러닝이 몇가지 다른 단계를 
갖고 있다고 생각합니다.

82
00:04:50,870 --> 00:04:55,960
첫번째로, j 비용함수를 최적화 시킬 수 있는
알고리즘을 원할 텐데요

83
00:04:55,960 --> 00:04:59,660
기울기 강하와 같은 여러 가지 도구를 이용해서 
그렇게 할 수 있죠.

84
00:04:59,660 --> 00:05:04,350
그리고 나중에 다른 알고리즘에 대해서 이야기 할 텐데요,

85
00:05:04,350 --> 00:05:08,070
RMS prop 그리고 Atom과 같은 알고리즘 말이죠. 

86
00:05:08,070 --> 00:05:15,100
그러나 j 비용함수를 최적화시킨 다음으로는 
overfit또한 원치 않았습니다.

87
00:05:15,100 --> 00:05:20,018
이렇게 하기 위한 일반화와 같은 도구도 있었죠. 

88
00:05:20,018 --> 00:05:22,300
또는 단순히 데이터를 더 많이 수집하는 방법도 있습니다.

89
00:05:22,300 --> 00:05:26,110
이미 머신러닝에서는 너무 많은 하이퍼 파라미터가 있습니다.

90
00:05:26,110 --> 00:05:31,160
가능한 알고리즘들 사이에서 고르는 것이 굉장히 복잡합니다.

91
00:05:31,160 --> 00:05:34,340
저는 머신런닝에대해 조금 더 쉽게 이해 할 수 있는데요, 

92
00:05:34,340 --> 00:05:37,800
J 비용함수를 최적화 시킬 수 있는 도구가 있거나

93
00:05:37,800 --> 00:05:41,120
J 비용함수를 최적화하는데 집중을 하는 경우에 말이죠. 

94
00:05:41,120 --> 00:05:46,770
유일히 신경 쓰는 것은 w와 b를 찾고
J(w, b)가 최대한 작게 되도록 만드는 것입니다.

95
00:05:46,770 --> 00:05:50,020
이것을 작게 만드는 것 외에는 신경 쓰지 않습니다.

96
00:05:50,020 --> 00:05:55,346
그리고, overfit을 하지 않기 위한 방법은
완전히 별개의 업무입니다.

97
00:05:55,346 --> 00:05:57,560
다시 말해, 편차를 줄이기 위한 방법 말이죠. 

98
00:05:57,560 --> 00:06:01,670
이것을 진행하는 경우에 쓰이는 도구가 따로 있습니다.

99
00:06:01,670 --> 00:06:06,570
이 원리는 가끔 직교화(orthogonalization)라고 불리는데요. 

100
00:06:06,570 --> 00:06:10,220
이것은 아이디어를 하나씩 생각한다는 건데요.

101
00:06:10,220 --> 00:06:14,640
나중에 이어지는 강의 비디오에서 
직교화에 대해 더 이야기 하겠습니다.

102
00:06:14,640 --> 00:06:17,600
컨셉이 전체적으로 완전히 이해가 안 가시더라도
걱정하지 마십시요. 

103
00:06:17,600 --> 00:06:21,015
하지만 저에게는
early stopping의 주된 단점은, 

104
00:06:21,015 --> 00:06:23,945
이것이 이 2개의 업무를 묶는다는 것입니다.

105
00:06:23,945 --> 00:06:28,165
그러면 이 2개의 별개 문제를 
단독으로 풀 수가 없게 됩니다.

106
00:06:28,165 --> 00:06:30,625
기울기 강하를 일찍이 정지시키기 때문에, 

107
00:06:30,625 --> 00:06:34,330
J 비용함수를 최적화시키는 진행 단계에서 중간에 
자르는 것과 마찬가지 인데요, 

108
00:06:34,330 --> 00:06:37,300
그 이유는 J 비용함수를 잘 못 줄이고 
있기 때문에 그런 것이죠 

109
00:06:37,300 --> 00:06:39,250
어떻게 보면 잘 못했기 때문이죠. 

110
00:06:39,250 --> 00:06:43,510
그와 동시에 overfit이 되지 않도록 
하는 것입니다.

111
00:06:43,510 --> 00:06:46,430
그렇기 때문에 이런 2문제를 
다른 도구를 사용해서 푸는 것이 아니라, 

112
00:06:46,430 --> 00:06:48,600
한가지를 이용해서 
2가지가 약간 섞이는 것입니다.

113
00:06:48,600 --> 00:06:51,250
그러면 결과적으로 여러 가지

114
00:06:52,370 --> 00:06:56,690
시도할 수 있는 방법들에 대해서
더 복잡하게 만드는 경향이 있습니다.

115
00:06:56,690 --> 00:07:01,840
early stopping를 사용하는 대신 
그냥 L2 일반화를 사용하는 방법이 있는데요, 

116
00:07:01,840 --> 00:07:05,030
이 경우, 신경망을 최대한 길게 트레이닝 시키면 되겠습니다.

117
00:07:05,030 --> 00:07:09,000
저는 이 경우가 더 하이퍼 파라미터의 서치 넓이를
분해시키는데 쉽다고 생각합니다.

118
00:07:09,000 --> 00:07:10,720
그리고 다시 서치하는데도 말이죠.

119
00:07:10,720 --> 00:07:14,200
하지만 이것의 단점은, 
여러 가지 일반화 

120
00:07:14,200 --> 00:07:16,420
파라미터 Lambda 값들을 시도해봐야 한다는 점입니다.

121
00:07:16,420 --> 00:07:21,040
그렇기 때문에 여러가지 Lambda값을 서칭하는 것은

122
00:07:21,040 --> 00:07:22,060
더 비싼 방법이겠습니다.

123
00:07:22,060 --> 00:07:26,500
그리고 of early stopping 의 장점은 
기울기 강하를 한번만 실행하도

124
00:07:26,500 --> 00:07:30,910
작은 w값, 중간 w값,

125
00:07:30,910 --> 00:07:35,960
큰 w값을 한번에 시도할 수 있습니다.
L2 하이퍼 파라키터 일반화의 여러 가지 Lambda 값들을 

126
00:07:35,960 --> 00:07:40,300
시도해 볼 필요없이 말이죠. 

127
00:07:40,300 --> 00:07:43,910
이 컨셉이 완전히 이해가 되지 않으셔도
걱정하지 마십시요. 

128
00:07:43,910 --> 00:07:46,608
직교화 에 대해 

129
00:07:46,608 --> 00:07:49,784
나중에 강의 비디오에서 이야기하도록 하겠습니다.
그 때 더 이해가 많이 되실 것입니다.

130
00:07:49,784 --> 00:07:52,860
단점이 있음에도 불구하고
사람들은 많이 사용합니다.

131
00:07:52,860 --> 00:07:55,728
저는 개인적으로 L2 일반화를
이용해서

132
00:07:55,728 --> 00:07:57,484
여러가지 Lambda의 값을 시도하는 것을 선호하는데요.

133
00:07:57,484 --> 00:08:00,530
이것은 computation 비용을 감안했을 때 그렇겠죠.

134
00:08:00,530 --> 00:08:03,350
하지만 early stopping 또한
비슷한 효과를 얻게 해주는데요, 

135
00:08:03,350 --> 00:08:06,910
Lambda의 여러가지 값을 시도할 필요없이 말이죠, 

136
00:08:06,910 --> 00:08:12,480
이제 여러분은 데이터 확장을 어떻게 
활용하는지 배웠는데요,

137
00:08:12,480 --> 00:08:17,550
또한 편차 또는 신경망의 overfitting을 막기 위한 
early stopping에 대해서도 배웠습니다.

138
00:08:17,550 --> 00:08:19,830
다음으로, 트레이닝을 빠르게 하도록 
최적화 문제를 

139
00:08:19,830 --> 00:08:23,320
세팅하는 기술들에 대해 배우겠습니다.