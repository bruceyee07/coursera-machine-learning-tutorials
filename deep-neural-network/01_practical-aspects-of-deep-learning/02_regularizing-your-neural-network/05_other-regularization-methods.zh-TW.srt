1
00:00:00,410 --> 00:00:04,180
除了 L2 正則化以及 dropout 正則化

2
00:00:04,180 --> 00:00:08,050
還有一些技術來減低
您神經網路過適的問題

3
00:00:08,050 --> 00:00:09,200
讓我們看看讓我們看看

4
00:00:09,200 --> 00:00:10,955
假設您配適一個貓的分類器

5
00:00:10,955 --> 00:00:15,590
如果您有過適問題,
獲得更多的訓練資料會有所幫助，但獲取更多

6
00:00:15,590 --> 00:00:20,970
訓練資料可能會很昂貴
有時候您就是沒辦法獲得更多資料

7
00:00:20,970 --> 00:00:24,670
但您可以做的是
擴增您的訓練集像是拿一個影像

8
00:00:24,670 --> 00:00:27,440
舉個例子
水平翻轉

9
00:00:27,440 --> 00:00:29,570
將它也加入訓練集

10
00:00:29,570 --> 00:00:32,950
現在不只是這一個例子
在您的訓練集

11
00:00:32,950 --> 00:00:35,320
您可以加入這個到您的訓練例子

12
00:00:35,320 --> 00:00:38,040
所以將影像水平翻轉

13
00:00:38,040 --> 00:00:40,670
您可以將您的訓練集加倍

14
00:00:40,670 --> 00:00:44,530
因為您的訓練集
現在有點多餘不像您

15
00:00:44,530 --> 00:00:50,200
重新收集完全
新的獨立的例子

16
00:00:50,200 --> 00:00:55,290
但您可以這樣做不需要
付任何費用為了

17
00:00:55,290 --> 00:00:57,100
得到更多貓的照片

18
00:00:57,100 --> 00:00:59,710
除了水平翻轉之外

19
00:00:59,710 --> 00:01:02,170
您也可以隨機剪裁影像

20
00:01:02,170 --> 00:01:06,220
這裡是我們旋轉
跟隨機放大這個影像

21
00:01:06,220 --> 00:01:07,720
這還是像一隻貓

22
00:01:07,720 --> 00:01:11,830
所以用隨機扭曲跟轉化影像
您可以

23
00:01:11,830 --> 00:01:16,490
擴增您的資料集
增加假的新例子

24
00:01:16,490 --> 00:01:20,780
再次, 這種額外假的例子
並不會增加

25
00:01:20,780 --> 00:01:25,900
像是您去獲得全新的
獨立貓的例子的資訊

26
00:01:25,900 --> 00:01:28,982
但因為您幾乎可以免費的做這個，除了

27
00:01:28,982 --> 00:01:30,562
一些計算的成本

28
00:01:30,562 --> 00:01:37,007
這個可以是最廉價的方式來
給予您更多的資料

29
00:01:37,007 --> 00:01:42,762
因此也是一種正則化來減低過適

30
00:01:42,762 --> 00:01:47,299
而透過綜合這些例子像這樣
您其實告訴您的演算法

31
00:01:47,299 --> 00:01:51,570
如果一個東西是貓
水平翻轉後還是貓

32
00:01:51,570 --> 00:01:53,100
請注意我並沒有垂直翻轉它

33
00:01:53,100 --> 00:01:55,820
因為也許我們並不想要上下顛倒的貓, 對吧?

34
00:01:55,820 --> 00:01:58,940
然後也許是隨機放大部份影像

35
00:01:58,940 --> 00:02:00,270
這樣應該也還是一隻貓

36
00:02:00,270 --> 00:02:04,750
對於光學文字辨識您也可以
將您的資料集將字元

37
00:02:04,750 --> 00:02:08,510
實行隨機旋轉跟扭曲

38
00:02:08,510 --> 00:02:11,620
所以如果您加入這些到您的訓練集

39
00:02:11,620 --> 00:02:13,490
這些還是字元 4

40
00:02:14,740 --> 00:02:18,320
為了解釋
我應用了很強的扭曲

41
00:02:18,320 --> 00:02:23,215
所以這看起來很彎曲,
實作時您不需要將 4 扭曲

42
00:02:23,215 --> 00:02:27,095
得這麼厲害，但只是些微的扭曲
就像我這邊做的

43
00:02:27,095 --> 00:02:29,255
用這個例子讓您明白, 對吧?

44
00:02:29,255 --> 00:02:32,945
但實作時通常用比較些微的扭曲

45
00:02:32,945 --> 00:02:35,490
因為這看起來真的是很彎曲的 4

46
00:02:35,490 --> 00:02:40,410
所以資料擴增可以用來
做正則化的技巧

47
00:02:40,410 --> 00:02:43,020
實際上類似於正則化

48
00:02:43,020 --> 00:02:46,970
還有一個常用的其他技術
通常稱為早期停止

49
00:02:46,970 --> 00:02:52,010
您要做的是當您跑梯度下降時您會畫

50
00:02:52,010 --> 00:02:54,010
您的不管是訓練誤差

51
00:02:54,010 --> 00:02:57,860
您會使用 0 與 1 分類誤差在訓練集

52
00:02:57,860 --> 00:03:00,860
或者畫成本函數 J 最佳化

53
00:03:00,860 --> 00:03:04,200
那應該是單調遞減像這樣
是吧?

54
00:03:04,200 --> 00:03:05,610
當您訓練時, 希望

55
00:03:05,610 --> 00:03:09,120
您訓練時您的成本函數 J 應該要遞減

56
00:03:09,120 --> 00:03:11,870
使用提前停止
您要做的是您畫這個圖

57
00:03:11,870 --> 00:03:15,970
您也畫開發集誤差

58
00:03:17,020 --> 00:03:20,920
再次, 這可以是開發集上的分類錯誤, 或者一些

59
00:03:20,920 --> 00:03:25,979
像是成本函數, 像是羅吉斯損失
或者對數損失在開發集上

60
00:03:25,979 --> 00:03:29,770
您會發現到您的開發集誤差
通常會降低一會兒

61
00:03:29,770 --> 00:03:32,950
然後會開始增加

62
00:03:32,950 --> 00:03:35,876
而提前停止是
您會說

63
00:03:35,876 --> 00:03:40,167
似乎您的神經網路
在這個循環做得最好

64
00:03:40,167 --> 00:03:43,640
我們希望在訓練神經網路一半時停止

65
00:03:43,640 --> 00:03:47,310
然後採用這個開發集誤差
不管到達何值

66
00:03:47,310 --> 00:03:48,260
所以為什麼這樣可行 ？

67
00:03:48,260 --> 00:03:51,490
當您還沒跑很多次循環
對於您的

68
00:03:51,490 --> 00:03:55,185
神經網路時
您的參數會接近於 0

69
00:03:55,185 --> 00:03:59,720
因為隨機初始化也許
初始 w 為小的隨機值

70
00:03:59,720 --> 00:04:04,190
在您訓練很久之前,
w 還是很小

71
00:04:04,190 --> 00:04:08,060
而當您開始循環,
w 會越來越大, 直到

72
00:04:08,060 --> 00:04:14,120
也許您有太大的參數 w 對於您的神經網路

73
00:04:14,120 --> 00:04:18,560
而提早停止做的是
利用半途停止您會只有

74
00:04:18,560 --> 00:04:23,286
中度大小的 w

75
00:04:23,286 --> 00:04:28,920
而類似於 L2 正則化使用
選擇一個神經網路比較小的

76
00:04:28,920 --> 00:04:34,630
範數對於您的參數 w,
希望您的神經網路不會過適

77
00:04:34,630 --> 00:04:37,270
而這個提早停止術語
來自於您會

78
00:04:37,270 --> 00:04:40,760
提前停止訓練您的神經網路

79
00:04:40,760 --> 00:04:43,760
我有時候會使用提早停止
當訓練一個神經網路

80
00:04:43,760 --> 00:04:46,650
但它有一個缺點
讓我來解釋

81
00:04:46,650 --> 00:04:50,870
我想機器學習
包含了很多不同的步驟

82
00:04:50,870 --> 00:04:55,960
其一是您要一個演算法
來最佳化成本函數 J 

83
00:04:55,960 --> 00:04:59,660
我們有不同的工具來使用
像是梯度下降

84
00:04:59,660 --> 00:05:04,350
然後我們將會談到的其他的像是
momentum 跟

85
00:05:04,350 --> 00:05:08,070
RMS prop 跟 Adam 等等

86
00:05:08,070 --> 00:05:15,100
但經過最佳化以後的成本函數 J
您希望不要過適

87
00:05:15,100 --> 00:05:20,018
我們有一些工具來使用
像是正則化

88
00:05:20,018 --> 00:05:22,300
取得更多資料等等

89
00:05:22,300 --> 00:05:26,110
在機器學習, 我們已經
有很多超參數像大浪襲來

90
00:05:26,110 --> 00:05:31,160
從可能的演算法中來做選擇
已經很複雜的

91
00:05:31,160 --> 00:05:34,340
所以我發現機器學習
比較容易想成

92
00:05:34,340 --> 00:05:37,800
當您有一組工具來
最佳化成本函數 J

93
00:05:37,800 --> 00:05:41,120
而當您注意在
最佳化成本函數 J

94
00:05:41,120 --> 00:05:46,770
您要注意的是找到 w 跟 b
讓 J(w,b) 越小越好

95
00:05:46,770 --> 00:05:50,020
您不去想其他的事
只是要減低這個

96
00:05:50,020 --> 00:05:55,346
而不去過適它
完全是兩碼子的事情

97
00:05:55,346 --> 00:05:57,560
換句話說
減低變異

98
00:05:57,560 --> 00:06:01,670
而當您要處理它時
您有不同的工具來使用

99
00:06:01,670 --> 00:06:06,570
而這個準則有時候
稱為正交法

100
00:06:06,570 --> 00:06:10,220
而這種想法是您要
一次做一件事

101
00:06:10,220 --> 00:06:14,640
我會談到正交法
在以後的影片中

102
00:06:14,640 --> 00:06:17,600
如果您還沒完全搞明白這個觀念
不用擔心

103
00:06:17,600 --> 00:06:21,015
但對我而言這種提前停止
的最主要缺點是

104
00:06:21,015 --> 00:06:23,945
這會耦合這兩件事

105
00:06:23,945 --> 00:06:28,165
所以您不再
將兩件事單獨作業

106
00:06:28,165 --> 00:06:30,625
因為將梯度下降提前停止

107
00:06:30,625 --> 00:06:34,330
您有點破壞了您正在做的
最佳化成本函數 J

108
00:06:34,330 --> 00:06:37,300
因為您不再好好做
減低成本函數 J 的工作

109
00:06:37,300 --> 00:06:39,250
您有點做不好那個

110
00:06:39,250 --> 00:06:43,510
然後同時您又試著不要過適

111
00:06:43,510 --> 00:06:46,430
所以與其使用不同的工具
解決兩個問題

112
00:06:46,430 --> 00:06:48,600
您用了混合了這兩種在一起

113
00:06:48,600 --> 00:06:51,250
而這會使得

114
00:06:52,370 --> 00:06:56,690
您可以試的種種方式中
更加複雜去想用哪些

115
00:06:56,690 --> 00:07:01,840
與其使用提早停止,
一種替代方式是使用 L2 正則化

116
00:07:01,840 --> 00:07:05,030
然後您可以訓練神經網路越久越好

117
00:07:05,030 --> 00:07:09,000
我發現這會使得搜尋
超參數的空間比較容易分解

118
00:07:09,000 --> 00:07:10,720
比較容易搜尋

119
00:07:10,720 --> 00:07:14,200
但缺點是您也許要試很多的

120
00:07:14,200 --> 00:07:16,420
正規化參數 lambda

121
00:07:16,420 --> 00:07:21,040
這會搜尋很多 lambda 的值
會產生很多

122
00:07:21,040 --> 00:07:22,060
昂貴的計算

123
00:07:22,060 --> 00:07:26,500
而提前停止的好處是
跑梯度下降

124
00:07:26,500 --> 00:07:30,910
一次, 您試出小的 w 值, 中間的 w 值跟

125
00:07:30,910 --> 00:07:35,960
大的 w 值,
不需要試很多 L2 正則化

126
00:07:35,960 --> 00:07:40,300
超參數 lambda

127
00:07:40,300 --> 00:07:43,910
如果這些觀念完全聽不明白
沒有關係

128
00:07:43,910 --> 00:07:46,608
我們會談到正交的細節

129
00:07:46,608 --> 00:07:49,784
在以後的影片
我想到時會更有感覺

130
00:07:49,784 --> 00:07:52,860
儘管有它的缺點，很多人還是使用它

131
00:07:52,860 --> 00:07:55,728
我個人比較喜歡
L2 正則化

132
00:07:55,728 --> 00:07:57,484
試著不同的 lambda 值

133
00:07:57,484 --> 00:08:00,530
這前提是您可以提供
必要的計算成本

134
00:08:00,530 --> 00:08:03,350
但提前停止的確
讓您獲得類似效果

135
00:08:03,350 --> 00:08:06,910
不需要明顯地試
很多不同的 lambda 值

136
00:08:06,910 --> 00:08:12,480
您現在看過如何做資料擴增
以及如果您想提前

137
00:08:12,480 --> 00:08:17,550
停止為了減低變異或者
避免過適在您的神經網路

138
00:08:17,550 --> 00:08:19,830
下一段讓我們談談一些關於

139
00:08:19,830 --> 00:08:23,320
設定您的最佳化問題來
讓您的訓練加快的技巧