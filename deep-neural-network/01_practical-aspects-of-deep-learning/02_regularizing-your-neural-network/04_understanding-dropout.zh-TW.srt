1
00:00:00,000 --> 00:00:06,015
Dropout 做著隨機敲掉您網路上的單元
看似瘋狂的事

2
00:00:06,015 --> 00:00:08,240
為什麼它可以做到正則化?

3
00:00:08,240 --> 00:00:10,665
讓我們來得到一些比較好的直覺

4
00:00:10,665 --> 00:00:11,970
在先前的影片中,在先前的影片中,

5
00:00:11,970 --> 00:00:16,705
我用 dropout 隨機敲掉您網路上的單元
的這種直覺

6
00:00:16,705 --> 00:00:20,860
這樣講好像是在每個循環中
您作用在小一點的神經網路

7
00:00:20,860 --> 00:00:26,360
所以使用小一點的神經網路
似乎像是有正則化的影響

8
00:00:26,360 --> 00:00:28,255
第二種直覺是

9
00:00:28,255 --> 00:00:34,795
讓我們從一個單一的單元來看
假設是這一個

10
00:00:34,795 --> 00:00:37,530
對於這個單元它的工作是對於輸入

11
00:00:37,530 --> 00:00:41,370
它需要產生有意義的輸出

12
00:00:41,370 --> 00:00:42,595
現在因為 dropout

13
00:00:42,595 --> 00:00:45,555
這些輸入有可能隨機被去掉

14
00:00:45,555 --> 00:00:47,965
有時候這兩個單元被消除

15
00:00:47,965 --> 00:00:50,530
有時候不同的單元被消除

16
00:00:50,530 --> 00:00:52,635
這個意思是這個單元

17
00:00:52,635 --> 00:00:54,005
我用紫色來畫一個圈

18
00:00:54,005 --> 00:00:58,560
它不能依賴任何一個特徵
因為任一個特徵都有可能

19
00:00:58,560 --> 00:01:03,715
隨機跑走或者
任一個它的輸入都可能隨機跑掉

20
00:01:03,715 --> 00:01:08,070
特別是會造成將所有的賭注
放在這些輸入上的遲疑

21
00:01:08,070 --> 00:01:10,475
假設說這個輸入

22
00:01:10,475 --> 00:01:12,990
權重來說,  我們會遲疑放太多的權重

23
00:01:12,990 --> 00:01:16,035
在任一個輸入因為它會跑掉

24
00:01:16,035 --> 00:01:20,820
所以這個單元會主動地分散權重

25
00:01:20,820 --> 00:01:26,250
會給一點點權重到
這四項每一個的輸入

26
00:01:26,250 --> 00:01:27,765
而分散所有的權重

27
00:01:27,765 --> 00:01:34,815
這會傾向縮減權重的平方範數的影響

28
00:01:34,815 --> 00:01:38,730
所以類似於我們看到在 L2正則化

29
00:01:38,730 --> 00:01:41,650
建置 dropout 的影響是它會縮減

30
00:01:41,650 --> 00:01:46,195
權重來做類似於 L2 正則化的動作
來幫助避免過適

31
00:01:46,195 --> 00:01:48,750
但實際上 dropout 可以正式地

32
00:01:48,750 --> 00:01:52,035
證明是一種自我調適的形式而不需要正則化

33
00:01:52,035 --> 00:01:55,305
但 L2 懲罰在不同的權重是有所不同

34
00:01:55,305 --> 00:01:58,830
它依據啟動值的大小來
用它的方法相乘

35
00:01:58,830 --> 00:02:02,580
但總結來說， 是可能證明 dropout 

36
00:02:02,580 --> 00:02:06,705
跟 L2 正則化有類似的影響

37
00:02:06,705 --> 00:02:09,990
只是 L2 正則化應用不同的方式可能

38
00:02:09,990 --> 00:02:13,540
些微不同, 甚至於能適應於
不同比例的輸入

39
00:02:13,540 --> 00:02:15,930
另一個細節當您建置 dropout

40
00:02:15,930 --> 00:02:19,510
這裡有一個網路有三個輸入特徵

41
00:02:19,510 --> 00:02:21,795
這裡有七個隱藏單元

42
00:02:21,795 --> 00:02:24,625
七個, 三個, 兩個, 一個

43
00:02:24,625 --> 00:02:26,915
我們需要選擇參數
其中之一是

44
00:02:26,915 --> 00:02:31,395
keep_prob, 也就是保留
每一層的單元的機會

45
00:02:31,395 --> 00:02:36,550
所以, 改變 keep_prob 在每一層也是可行的

46
00:02:36,550 --> 00:02:38,490
所以對於第一層

47
00:02:38,490 --> 00:02:42,460
您的矩陣會是 3 乘 7

48
00:02:42,460 --> 00:02:46,120
您的第二個權重矩陣會是 7 乘 7

49
00:02:46,120 --> 00:02:49,680
w[3] 會是 7 乘 3 等等

50
00:02:49,680 --> 00:02:53,205
所以 w2 實際上是最大一個權重矩陣

51
00:02:53,205 --> 00:02:55,500
因為它實際上是最大的參數集

52
00:02:55,500 --> 00:02:58,195
在 w[2] 是 7 乘 7

53
00:02:58,195 --> 00:03:01,605
所以為了避免, 為了減低那個矩陣過適

54
00:03:01,605 --> 00:03:03,600
也許對於這一層

55
00:03:03,600 --> 00:03:05,205
我猜這是第二層

56
00:03:05,205 --> 00:03:08,490
您也許讓 keep_prob 相對低一點

57
00:03:08,490 --> 00:03:10,435
假設說是 0.5

58
00:03:10,435 --> 00:03:13,825
但對於不同層您或許比較不擔心過適

59
00:03:13,825 --> 00:03:15,080
您可以有高一點的 keep_prob

60
00:03:15,080 --> 00:03:18,255
也許就是 0.7

61
00:03:18,255 --> 00:03:22,715
如果一些層我們一點都不擔心過適

62
00:03:22,715 --> 00:03:25,240
您可以將 keep_prob 設為 1.0

63
00:03:25,240 --> 00:03:30,725
為了清楚起見, 我用紫色框框畫起來的數字

64
00:03:30,725 --> 00:03:34,635
這些可以是不同的 keep_prob 在不同層

65
00:03:34,635 --> 00:03:39,100
請注意設 keep_prob 為 1.0 意思是
您將保留所有的單元

66
00:03:39,100 --> 00:03:41,855
您實際上並不使用 dropout 在那一層

67
00:03:41,855 --> 00:03:44,730
但對於那些您比較擔心會過適的層級

68
00:03:44,730 --> 00:03:46,660
像是有很多參數的層級

69
00:03:46,660 --> 00:03:51,600
您可以設 keep_prob 小一點來
應用 dropout 強而又力的形式

70
00:03:51,600 --> 00:03:53,070
有點像是調整音量似的

71
00:03:53,070 --> 00:03:54,910
正則化參數 lambda

72
00:03:54,910 --> 00:03:57,960
在 L2 正則化您試著
正則化一些層比某些層多

73
00:03:57,960 --> 00:04:02,715
技術上而言, 您可以應用 dropout 到輸入層

74
00:04:02,715 --> 00:04:07,295
您可以有一些機會來丟掉一些輸入特徵

75
00:04:07,295 --> 00:04:11,580
雖然實作上, 通常不這麼做

76
00:04:11,580 --> 00:04:15,270
所以，在輸入層通常設 keep_prob 為 1.0

77
00:04:15,270 --> 00:04:17,985
您也可以用很高的數字像是 0.9

78
00:04:17,985 --> 00:04:22,740
但比較少看到您想去除一半的輸入特徵

79
00:04:22,740 --> 00:04:25,665
所以 keep_prob, 如果您用這個法則

80
00:04:25,665 --> 00:04:32,030
即使您應用 dropout 在輸入層
也會是一個接近於 1 的數字

81
00:04:32,030 --> 00:04:33,450
總結一下

82
00:04:33,450 --> 00:04:36,330
如果您比較擔心一些層會過適
比起其他層而言

83
00:04:36,330 --> 00:04:40,320
您可以設低一點的 keep_prob 比起其他層

84
00:04:40,320 --> 00:04:41,430
缺點是, 這會讓您

85
00:04:41,430 --> 00:04:44,955
需要搜尋更多的超參數使用在交叉驗證集

86
00:04:44,955 --> 00:04:48,525
另外一種方式是一些層您應用

87
00:04:48,525 --> 00:04:50,460
dropout 而一些層您不用 dropout

88
00:04:50,460 --> 00:04:52,630
然後只用一個超參數

89
00:04:52,630 --> 00:04:55,910
也就是 keep_prob 對於
您想應用 dropout 的那些層

90
00:04:55,910 --> 00:04:59,070
在結束之前, 一些建置的提示

91
00:04:59,070 --> 00:05:03,850
很多第一個成功建置 dropout 是在電腦視覺

92
00:05:03,850 --> 00:05:05,075
在電腦視覺裡

93
00:05:05,075 --> 00:05:06,890
輸入大小是如此之大

94
00:05:06,890 --> 00:05:11,275
輸入所有這些像素值
資料幾乎從來都不夠
(因為輸入特徵太多, 訓練例子會不夠)

95
00:05:11,275 --> 00:05:14,710
所以 dropout 很常用於電腦視覺

96
00:05:14,710 --> 00:05:18,035
有一些電腦視覺研究人員幾乎總是使用它

97
00:05:18,035 --> 00:05:19,750
幾乎是預設

98
00:05:19,750 --> 00:05:24,866
但請記得, dropout 是一種正則化技巧

99
00:05:24,866 --> 00:05:27,010
來幫助避免過適

100
00:05:27,010 --> 00:05:30,880
所以, 除非我們演算法過適

101
00:05:30,880 --> 00:05:33,250
我不會麻煩用到 dropout

102
00:05:33,250 --> 00:05:36,557
所以在其他應用領域用的比較少

103
00:05:36,557 --> 00:05:38,320
只是在電腦視覺裡

104
00:05:38,320 --> 00:05:40,600
您通常沒有足夠資料

105
00:05:40,600 --> 00:05:42,090
所以您幾乎總是過適

106
00:05:42,090 --> 00:05:46,425
這是為什麼一些電腦視覺研究者要用 dropout

107
00:05:46,425 --> 00:05:52,498
但這樣的直觀並不總是一般化到其他領域

108
00:05:52,498 --> 00:06:00,490
一個重大的缺點是 dropout 
的成本函數 J 不再完整定義

109
00:06:00,490 --> 00:06:06,635
在每一個循環中,
您隨機去掉一些節點

110
00:06:06,635 --> 00:06:10,855
如果您重複檢查
梯度下降的表現

111
00:06:10,855 --> 00:06:14,590
實際上比較難檢驗您有

112
00:06:14,590 --> 00:06:20,365
定義良好的成本函數 J 
在每一次循環時望下坡走

113
00:06:20,365 --> 00:06:24,625
因為您的成本函數 J 在做
最佳化時實際上是比較小的

114
00:06:24,625 --> 00:06:27,395
定義不明, 或者說很難去計算

115
00:06:27,395 --> 00:06:30,160
您會失掉除錯工具來畫圖

116
00:06:30,160 --> 00:06:32,010
像這樣圖形

117
00:06:32,010 --> 00:06:34,805
我通常會將 dropout 關掉

118
00:06:34,805 --> 00:06:37,060
您會設 keep_prob  等於 1

119
00:06:37,060 --> 00:06:40,885
然後我跑程式，確定它是單調遞減的函數 J

120
00:06:40,885 --> 00:06:43,960
然後將 dropout 開啟, 希望

121
00:06:43,960 --> 00:06:47,035
我在使用 dropout 時
我的新程式不會有臭蟲

122
00:06:47,035 --> 00:06:49,195
因為我想您需要其他方式(來除錯)

123
00:06:49,195 --> 00:06:52,060
但不適用畫這圖來確定您的程式可行

124
00:06:52,060 --> 00:06:55,900
來讓程式可行
即使使用 dropout

125
00:06:55,900 --> 00:06:58,130
所以有了這個, 還有

126
00:06:58,130 --> 00:07:01,830
一些更多的正則化技巧值得您知道

127
00:07:01,830 --> 00:07:04,480
讓我們在下一段影片中談更多的技巧