L2 regülarizasyon ve drop out regülarizasyona  ek olarak aşırı uyumu(overfitting ) nöron ağlarında düşürmek için yöntemler bulunmaktadır. Hadi inceleyelim. diyelim ki kedi sınıflandırması yapıyoruz eğer aşırı uyuma(over fitting) varsa daha fazla eğitim seti yardımcı olabilir ama eğer eğitim verisi pahalı olabilir ve bazen daha fazla elde edemezsiniz. ama siz eğitim setini coğaltmak (augment) yaparak ve görüntüyü şu şekilde ve örnek olarak yatay olarak çevirerek ve bu görüntüyü eğitim setine ekleyerek şimdi sadece bu örnek yerine sizin veri setinizde siz bunu kendi eğitim setinize ekleyebilirsiniz görüntüleri yatay olarak çevirerek eğitim setini iki katına çıkarabilirsiniz çünkü sizin eğitim setiniz işinizi yapmayan dır ve bu eğer ek olarak yeni bağımsız örnekler toplanmışsa o kadar iyi olmayacak ama ama bu işi dışarya çıkmaya ihityaç duymayıp ve zaman harcamadan fazla kedi fotosu elede edilebilir. ve yapay çevirmekten başka rastgele bir şekilde görüntüleri kırpabilirsin. burada görüntüye rastgele bir şekilde zumlayarak ve döndürerek ve bu hala bir kediye benzemektedir. görüntü üzerinde rastgele bükülme ve tebdil yaparak veri setini çoğaltma yapılabilir ve ek bir yapay eğitim öğrenği oluşturulabilir tekrar,extra sahte öğrenme seti bilgi eklemesi konusunda yeni ve bağımsız kedi örneklerinden aldığımız kadar bilgi eklemez. ama cünkü bu işi yaklaşık olarak bedavaya yapıldığı için ve diğerinden ki bazı uyuşumsal maliyeti olduğu için bunu yapabilirsiniz bu pahalı olmayan bir yol olarak fazladan data algoritmanıza verebilirsiniz. onun için bir çeşit regülarizasyondur ve aşırı uymu (overfittingi ) azaltır. ve örnekleri buna benzer sentezleyerek siz algoritmanıza şunu demektesiniz ki eğer bir şey kedi ise yatay çevirerek de hala bir kedidir. dikkat ediniz ben dikey olarak çevirmedim. çünkü belki istemiyoruz kedinin yukarı kısmını aşağıya getirelim. doğru? ve sonra belki rastgele görüntünün bazı kısımlarına zum ederek bir oğlasılıkla hala bir kedidir. Optik karakter tanıma için data veri setinizi rakamlar alarak ve rastgele dönmeler ve deformasyonlar yaparak da getirebilirsiniz ve eğer bunların eğitim setine eklerseniz bunlar hala bir rakamlardır. gösterim için ben güçlü bir bükülme(distorsiyon) uyguladım. ve bu çok dalgalı bir dörde bezemeke.Pratikte dördü bu kadar agresif şekilde distorsyion uğratmaya gerek yok. sadece hafif bir distorsiyon ki bu öğreniğe sizlere daha açıklıyor olması için şimdi gösteriyorum..doğru mu? ama pratikte çok daha hafif distorsiyon olmaktadır. çünkü bu gerçekten bürünmüş bir dörde benzemektedir. o zaman data Çoğaltmak (augmentation ) bir regülarizasyon tekniği olarak kullanılabilir. aslında regülarizasyona benzemektedir. bir başka teknik ki çoğu zaman kullanılmaktadadır erken durdurma denilmektedir. o zaman sen ne yapmalısın gradient düşümünü uygularken senin ya eğitim hatanı 0 1 sınıflandırma hatası eğitim seti üzerinde kullanarak yada J maliyet fonksiyonun çizerek ve monotonik azalmalı ve buna benzer,doğru değil mi çünkü eğitime yapıyorsunuz ,umarım maliyet fonksiyonu J azaltma etrafında eğitme yapıyorsunuz. erken durdurma ile ne yapıyorsun bunu çizerek sen aynı zamanda geliştirme seti hatasını çiziyorsun. ve tekrar,bu bir sınıflandırma hatasi geliştirme setinde olabilir.ya bazen maliyet fonkisyonuna benzer,lojistik kaybına benzer ya log kaybı geliştirme setinde olabilir. şimdi ne ile karşılaşırsınız budur ki geliştirme seti hatasi genelde düşer bir süre için ve sonra artar. şunu söylebilirsiniz ki erken durdurma ne yapıyor buna benzemektedir ki nöron ağları en iyi sonucunu iterasyon içerisinde yapmaktadır. biz sadece isteğimiz eğitim yarı yolda durdurmakdır ve geliştirme seti hatasi hangi değere ulaşırsa onu alın. peki niçin bu çalışmaktadır? çok iterasyon çalıştırmadığınız zaman sizin nöron ağlarınız henüz ve parametreleriniz w sıfıra yakın olucaktır. rastgele bir başlangıç olduğu belki w küçük rastgele değerler ile başladınız uzun süre eğitmeden önce,w bir küçük değerdir. ve iterasyon yapıldığında,eğitim yapıldığında, w büyüyecektir ve büyüyecektir ta burada belki siz nöron ağlarının w parametresinin çok büyük bir değeri sizde var. o zaman erken durdurma ne yapıyor budur ki yarı yolda durdurarak w'nin orta büyüklükte elde ediliyor. ve L2 regülarizasyona benzer nöron ağlarında küçük norm değerlerini w parametresine alarak,nöron ağlarında daha az aşırı uymu(over fitting) umuluyor. ve erken durdurma değimi şu gereçeğe geri dönmektedir ki eğitiminizin ereken bir zaman da durdurmaktadır. ben bazen erken durdurmayı nöron ağlarını eğitmde kullanıyorum. ama o bir tane dezavantajı vardır izin verin anlatıyım. düşünüyorum ki makine öğrenme prosesinde farklı adımları karşılaştırarak bir, algoritimden bunu istiyorsunuz maliyet fonkisyonunu j optimize etsin. ve farklı yollar var ki maliyet foknisyonun kontrol etsin mesla gradient descent ve bundan sonra biz başka algoritmalardan bahsedeceğiz,mesela momentum ve RMS prop ve adam ve.. ama maliyet fonksiyonu j optimize ettikten sonra siz aşırı uyum (overfitting )olmasını istemezsiniz. ve başka araçlar da bu regülarizasyona benzer yapmaktadır, çok data olmak ve buna benzer. şimdi makine öğrenmesinde bir çok hyperparameter bulunmaktadır. Olası algoritmaların alanı arasında seçim yapmak zaten çok karmaşıktır. ve ben makine öğrenilmesi üzerinde düşünmesini kolay buldum. Maliyet fonksiyonunu J optimize etmek için bir takım araçlarınız varsa, ve sen maliyet fonksiyonu J üzerinde odaklanarak yetkisini denerken Tek dikkat ettiğim şey, w ve b'yi bulmaktır, böylece J (w, b) mümkün olduğunca küçüktür. Sadece bir şey düşünmüyorsun - bunu azaltmaktan başka. ve sonra tamamen başka bir görevdir ki aşırı uyum olmasın başka bir değişle,varyansı düşürmek. Bunu yaptığınızda, bunu yapmak için ayrı bir araç setine sahipsiniz. Ve bu prensip bazen ortogonalizasyon denir. Ve bu fikir, her seferinde bir görev hakkında düşünmek isteyebileceğiniz bir fikir var. Daha sonra bir videoda ortoganizasyon hakkında daha çok şey söyleyeceğim,o zaman Henüz kavramı tam olarak kavramadıysanız, endişelenmeyin. ama bana göre erken durdurmanın ana dezavantajı budur ki Bu iki görevi birleştirir. Yani artık bu iki problem üzerinde bağımsız olarak çalışamazsınız, çünkü erken gradyan inişini durdurarak, Maliyet fonksiyonunu J optimize etmek için ne yapıyorsanız onu bölüyorsunuz , çünkü şimdi maliyet fonksiyonunu azaltarak harika bir iş yapmıyorsunuz. O kadar iyi yapmadın Ve aynı zamanda eşzamanlı olarak uyum sağlamaya da çalışıyorsunuz. Yani, iki problemi çözmek için farklı araçlar kullanmak yerine Bu türden iki tane kullanıyorsunuz. Ve bu sadece deneyebileceğiniz şeylerin setini düşünmek için daha karmaşık hale getirir. Erken durdurmayı kullanmak yerine, bir alternatif sadece L2 regülasyonu kullanmaktır o zaman nöron ağını olabildiğince uzun süre eğitebilirsin. Bunun, hiper parametrelerin arama alanını ayrıştırmayı kolaylaştırdığını ve daha kolay arama yapabildiğini buldum. Ancak bunun olumsuz yanı, çok fazla değeri denemeniz gerekebileceğidir. lambda regülarizasyons parametresi. Ve bu, lambda'nın birçok lamda değeri üzerinde arama yapaması hesaplamalı pahalı Ve erken durdurmanın avantajı, gradyan iniş sürecini yürütmesidir. sadece bir kez, küçük w, orta boy w değerleri denemek ve büyük w, L2 regülarizsayonun bir çok değerini denemeye gerek kalmadan hiperparametre lambda. Bu konsept henüz sizin için bir anlam ifade etmiyorsa, endişelenmeyin. Ortogonalizasyon hakkında daha fazla konuşacağız bir sonraki video da,Ben bu biraz daha mantıklı olacağını düşünüyorum. Dezavantajlarına rağmen, birçok insan bunu kullanıyor. Ben şahsen L2 regülasyonunu kullanmayı tercih ediyorum ve Farklı lambda değerlerini deneyerek. Bunu yapmak için hesaplamayı karşılayabileceğinizi varsayarız. Fakat erken durdurma, benzer bir etkiye sahip olmanıza izin veriyor ama açıkça lambda çok farklı değerleri denemeğe gerek kalmadan. Bu nedenle, veri büyütmeyi nasıl kullanacağınızı ve ayrıca,erken durdurmayı isterseniz, varyansı azaltmak veya nöron ağınızı aşırı uyumdan kaçınmak için. Sonraki için bazı teknikler hakkında konuşalım Eğitiminizin hızlıca ilerlemesini sağlamak için optimizasyon probleminizi ayarlıyor.