일반화가 어떻게 overfitting문제를 도와줄 수 있는 것일까요? 왜 그것이 편차 문제를 줄이는데 도움이 되는가? 그것이 어떻게 작동하는지에 대한 약간의 직관을 얻기 위해 몇가지 예를 들어보도록 하겠습니다. 큰 편향과 편차가 있는 경우를 다시 한번 보겠습니다. 이전 비디오에서 보여졌던 그림을 이렇게 다시 그려보도록 하겠습니다. 자, 이제 크기가 크고 깊이 있는 신경 네트워크를 봅시다. 이걸 너무 크거나 깊이 있게 그리진 않았지만, 이렇게 overfitting하는 그림을 신경망이라고 생각해봅시다. 비용함수가 J의 W, B가 loss의 합인 경우를 보겠습니다. 저희가 알아낸 부분은, 여기 추가 항이, weight matrix를 너무 큰 값을 갖는다는 이유로 처벌하죠. 이것을 Frobenius norm라고 합니다. 이것이 그러다면 왜 L 2 norm 또는 Frobenius norm 
을 줄이는 것이 또는 파라티머를 줄이는 것이 덜 overfitting하게 만드는 것일까요? 한가지 직관으로는, Lambda 일반화를 크게 정하면, weight matrix인 W의 값을 0에 가까운 값으로 지정하고 싶어할 것입니다. 그리하여 직관적인 부분은 수 많은 숨겨진 유닛에 대하여 weight를 거의 0에 가까운 값으로 지정하여 이러한 숨겨진 유닛의 영향을 거의 0으로 만들어 버리는 것입니다. 이런 경우, 이렇게 심플한 신경망은 훨씬 더 작은 신경망이 되는 것입니다. 거의 로지스틱 회기 모형 유닉과 비슷합니다. 대신 다량의 층으로 이루어진 것이죠. 그런 경우, 이러한 overfitting한 경우에서 큰 편향을 갖는 왼쪽의 경우처럼 이동시킵니다. 하지만, 이상적으로 Lambda 적당한 중간 값을 통해, 이렇게 가운데의 "just right" 와 같은 케이스가 나올 수도 있습니다. 직관적인 부분은 바로 Lambda의 값을 매우 크게 올리면, W의 값을 거의 0으로 만들 것입니다. 물론 실제로는 이렇게 되지는 않습니다. 순히 이야기해서, 조금 더 심플한 신경망 네트워크가 되기 위한 과정으로 수 많은 숨겨진 유닛들의 효과가 0이 되거나 크게 줄어든다고 생각하면 됩니다. 마치 로지스틱 회귀분석법을 사용하는 것과 같이 조금씩 점차 가까워집니다. 하지만 직관적으로 수 많은 숨겨진 유닛을 0으로 만든다는 것은 사실 옳은 설명은 아닙니다. 실제로 일어나는 현상은, 지속적으로 숨겨진 유닛을 사용할 것입니다. 하지만 각각의 숨겨진 유닛의 효과가 줄어드는 것입니다. 조금 더 심플한 네트워크로 남는 것입니다. 조금 더 심플한 네트워크로 남는다는 것은 overfitting 영향을 덜 받게 된다는 것이기도 합니다. 이런 여러가지 직관은 일반화를 프로그램 학습에 도입하는데 도움을 줍니다. 여러분은 직접 편차가 줄어드는 것을 보실 수 있습니다. 일반화가 어떻게 overfitting 문제에 도움을 주는디 한가지 예제를 더 살펴보겠습니다. 이 예제에서는, 이렇게 생긴 tan h activation 함수를 이용한다는 가정을 할 것입니다. 이것은 g(z) 는 tan (z)라는 식입니다. 이 같은 경우에는, Z의 값이 꽤 작은 이상, 즉, Z가 오로지 작은 범위의 파라미터 값만 갖는다면, 여기정도 될 수 있겠죠, 그럼 여러분은 단순히 tan h 함수의 선형적인 부분을 사용하는 것입니다. Z값이 이와 같이 큰 값이나 작은 값까지 이동할 수 있는 경우에만, activation 함수가 덩 선형적인 함수로 정의되는 것입니다. 여기서 여러분이 생각할 수 있는 부분은, 램다의 값이 즉, 일반화 파라미터의 값이 큰 경우, 그럼 파라미터들이 꽤 작을 것입니다. cos 함수에서 더 큰 값을 갖는다는 이유로 처벌받는 개념이죠. 만약 그럼 weight 인 W가 장은 경우, Z는 W이기 때문에, 엄밀히 이야기하면, 이것은 
플러스 b이구요, 그런데 만약 W값이 작은 경우 Z의 값 또한 꽤 작을 것입니다. 특히, Z가 상대적으로 작은 값을 갖는 경우, 이 범위에서 말이죠, 그러면 G(z)는 대략적으로 선형일 것입니다. 이것은 마치 모든 층이 대략적으로 선형인 것과 비슷합니다. 마치 선형회귀와 같이 말이죠. 저희는 코스1에서 보았듯이, 모든 층이 선형이라고 하면 전체 네트워크가 선형 네트워크입니다. 그러므로 심층신경마으이 경우에도, 선형 activation 함수로 이루어져 있는 심층 네트워크 경우, 결국에는 선형 함수에 한해서 산출할 수 있습니다. 그러기 때문에 복잡한 결정을 피팅하는 것은 불가능합니다. 비선형 의사결정은 불가능 한 것입니다. 데이터를 overfit할 수 있는 경우 말이죠. 이전 슬라이드에서 봤던 큰 편차의 overfitting 케이스처럼 말이죠. 요약해보자면, 일반화가 매우 커지는 경우, W 매개 변수가 굉장히 작아지고 그리하여 Z도 상대적으로 작아질 것입니다. b의 효과는 일단 무시하도록 하겠습니다. 그러므로 Z는 상대적으로 작아지구요, 사실은 작은 범위의 값을 갖는다고 하는게 더 맞습니다. activation 함수가 tan h인 경우, 상대적으로 선형인 함수가 될켄데요, 여러분의 전체 신경 네트워크는 선형 함수가 산출하는 것과 크게 다르지 않은 값을 계산할 것입니다. 그러므로 비선형의 복잡한 함수보다는 심플한 일차함수의 성향이 더 강한 것입니다. 그러므로 훨씬 덜 overfit하는 것이구요. 다 시 말씀드리지만, 프로그램 연습 학습에서 regularization은 직접 입력할때 이러한 효과를 여러분께서 직접 보실 수 있습니다. 일반화에 대한 def 토론을 마치기에 앞서 이행 tip 한가지를 알려드리겠습니다. 일반화를 도입하는 경우에는 비용함수 J를 정의해서 추가적으로 항을 더했는데요 weight가 너무 큰 값을 갖는 것에대해 페널티를 부과하는 항입니다. 그러므로 기울기 강하를 도입하는 경우, 기울기 강하를 디버깅하는 방법 중 한가지는, 비용함수 J를 기울기 강하의 elevation 개수로 나타내는 것입니다. 그리하여 비용함수 J가 기울기 강하가 특정 값마다 증가할 때마다 점진적으로 줄어드는 것을 볼 수 있습니다. 그리고, 일반화를 도입하는 경우, J가 새로운 뜻이 있다는 것을 기억하십시요. 이전에 정의되었던 J는 이런 항인데요. 이 경우, 점진적으로 감소하는 것을 못 볼 것입니다. 그러므로 기울기 강하를 디버그 하는 경우, 이 새로운 J로 plot할 수 있도록 합니다. 이 두번째 항도 추가될 수 있도록 말이죠. 아니면, 특정 값별로 증가할 때마다 J가 점진적으로 감소하는 것을 못 볼테니깐요. 이것이 L2 일반화에 대한 내용의 전부인데요. 사실 대부분의 내용이 제가 딥러닝 모듈에서 사용하는 일반화 테크닉에 관한 내용이였습니다. 딥러닝에서는, ‘dropout regularization’이라고 하는 또 한가지의 일반화 테크닉이 있는데요, 다음 비디오에서 관련 내용을 보도록 하겠습니다. 그럼, 다음 강의에서 살펴 보겠습니다.