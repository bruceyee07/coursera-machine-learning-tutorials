1
00:00:00,410 --> 00:00:04,180
除了L2正则化和 随机失活(dropout)正则化之外

2
00:00:04,180 --> 00:00:08,050
还有一些其他方法 可以减少神经网络的过拟合

3
00:00:08,050 --> 00:00:09,200
让我们一起来看看让我们一起来看看

4
00:00:09,200 --> 00:00:10,955
我们假设你正在拟合猫的图片分类器

5
00:00:10,955 --> 00:00:15,590
如果你过拟合了 可以增加训练数据

6
00:00:15,590 --> 00:00:20,970
但扩大训练集代价很高 而且有时候就是无法得到更多数据

7
00:00:20,970 --> 00:00:24,670
但你可以通过像这样处理图片 来扩增训练集

8
00:00:24,670 --> 00:00:27,440
比如 把它水平翻转

9
00:00:27,440 --> 00:00:29,570
然后也加到你的训练集当中

10
00:00:29,570 --> 00:00:32,950
现在你的训练集里 不仅仅有这张图

11
00:00:32,950 --> 00:00:35,320
这一张也可以加进去

12
00:00:35,320 --> 00:00:38,040
所以 通过水平翻转图像

13
00:00:38,040 --> 00:00:40,670
你可以把训练集的数据量翻倍

14
00:00:40,670 --> 00:00:44,530
因为你的训练集现在有些冗杂 所以相比之下

15
00:00:44,530 --> 00:00:50,200
不如另收集全新 独立的数据

16
00:00:50,200 --> 00:00:55,290
不过这种方法可以节省你出门

17
00:00:55,290 --> 00:00:57,100
拍更多猫照片的成本

18
00:00:57,100 --> 00:00:59,710
除了水平翻转之外

19
00:00:59,710 --> 00:01:02,170
你也可以随机裁剪图片

20
00:01:02,170 --> 00:01:06,220
这里我们旋转再放大了一下图片

21
00:01:06,220 --> 00:01:07,720
看上去仍然是一只猫

22
00:01:07,720 --> 00:01:11,830
所以通过随机的扭曲 变换图片

23
00:01:11,830 --> 00:01:16,490
你可以增强数据集 做出额外的伪训练样本

24
00:01:16,490 --> 00:01:20,780
再次强调 这些额外的伪训练样本 它们能增加的信息量

25
00:01:20,780 --> 00:01:25,900
不如全新的 独立的猫照片多

26
00:01:25,900 --> 00:01:28,982
但因为这么做是几乎不需要任何开销的

27
00:01:28,982 --> 00:01:30,562
只有一些计算代价

28
00:01:30,562 --> 00:01:37,007
所以这是一个廉价的方式 来为你的算法获得更多数据

29
00:01:37,007 --> 00:01:42,762
因此可以算作正则化 而且减少了过拟合

30
00:01:42,762 --> 00:01:47,299
并且 通过像这样的合成样本 你实际上是在告诉你的算法

31
00:01:47,299 --> 00:01:51,570
如果这是一只猫 那它水平翻转之后还是一只猫

32
00:01:51,570 --> 00:01:53,100
注意 我并没有垂直翻转它

33
00:01:53,100 --> 00:01:55,820
因为我们大概并不想要倒置的猫

34
00:01:55,820 --> 00:01:58,940
随机放大图片的一部分

35
00:01:58,940 --> 00:02:00,270
这很可能仍然是一只猫

36
00:02:00,270 --> 00:02:04,750
对于字符识别 你也可以扩增数据集

37
00:02:04,750 --> 00:02:08,510
通过给数字加上随机的旋转和变形

38
00:02:08,510 --> 00:02:11,620
所以 如果把这些加到你的训练集里

39
00:02:11,620 --> 00:02:13,490
它们仍然是数字4

40
00:02:14,740 --> 00:02:18,320
为了举例 我用了一个很强的扭曲

41
00:02:18,320 --> 00:02:23,215
所以看上去是一个波纹4 实际操作中

42
00:02:23,215 --> 00:02:27,095
你不需要把4扭曲的这么夸张 比我这种再细微点就可以了

43
00:02:27,095 --> 00:02:29,255
这个例子是为了让你们看得更清楚

44
00:02:29,255 --> 00:02:32,945
但实际中通常用比较细微的变形

45
00:02:32,945 --> 00:02:35,490
因为这个4看起来真的太扭曲了

46
00:02:35,490 --> 00:02:40,410
所以数据集扩增(data augmentation) 可以作为一种正则化技术

47
00:02:40,410 --> 00:02:43,020
准确地说是接近正则化

48
00:02:43,020 --> 00:02:46,970
还有另一个常用的方法 叫做早终止法(early stopping)

49
00:02:46,970 --> 00:02:52,010
你要做的是 在运行梯度下降(gradient descent)时

50
00:02:52,010 --> 00:02:54,010
画一张训练误差的图

51
00:02:54,010 --> 00:02:57,860
可以用训练集的0-1分类误差

52
00:02:57,860 --> 00:03:00,860
或者把 成本函数J(也称代价函数)画出来

53
00:03:00,860 --> 00:03:04,200
它应该要单调递减 像图中这样

54
00:03:04,200 --> 00:03:05,610
因为当你训练的时候

55
00:03:05,610 --> 00:03:09,120
理想状况下 你训练时的成本函数J应该是递减的

56
00:03:09,120 --> 00:03:11,870
用early stopping 你要做的是画出这个曲线

57
00:03:11,870 --> 00:03:15,970
同时也画出开发集误差的曲线

58
00:03:17,020 --> 00:03:20,920
一样地 可以是开发集的分类误差 或者成本函数

59
00:03:20,920 --> 00:03:25,979
比如开发集的对数损失函数(log loss)

60
00:03:25,979 --> 00:03:29,770
现在你可以看出 通常开发集误差会先下降一段

61
00:03:29,770 --> 00:03:32,950
然后接着开始增大

62
00:03:32,950 --> 00:03:35,876
所以early stopping做的就是

63
00:03:35,876 --> 00:03:40,167
你会发现 在那次迭代附近 你的神经网络表现得最好

64
00:03:40,167 --> 00:03:43,640
那我们想做的就是 把神经网络的训练过程停住

65
00:03:43,640 --> 00:03:47,310
并且选取这个(最小)开发集误差所对应的值

66
00:03:47,310 --> 00:03:48,260
那么为什么这个方法是有用的呢?

67
00:03:48,260 --> 00:03:51,490
当你刚开始在神经网络上迭代时

68
00:03:51,490 --> 00:03:55,185
你的参数w会接近于0

69
00:03:55,185 --> 00:03:59,720
因为通过随机初始化(random initialization) 很可能你给w的初始值是一个较小值

70
00:03:59,720 --> 00:04:04,190
因此在训练足够长时间前 w仍然很小

71
00:04:04,190 --> 00:04:08,060
然后随着你继续迭代 训练 w越来越大

72
00:04:08,060 --> 00:04:14,120
直到这里 可能你的神经网络就有一个很大的参数w了

73
00:04:14,120 --> 00:04:18,560
所以early stopping做的是 通过停在半路

74
00:04:18,560 --> 00:04:23,286
你能得到一个不大不小的w值

75
00:04:23,286 --> 00:04:28,920
这点和L2正则化有点像 通过选一个

76
00:04:28,920 --> 00:04:34,630
参数w范数较小的神经网络 理想状况下就能少点过拟合了

77
00:04:34,630 --> 00:04:37,270
而early stopping这个词就是指

78
00:04:37,270 --> 00:04:40,760
你会提前终止神经网络的训练

79
00:04:40,760 --> 00:04:43,760
在训练神经网络时 我有时候会用early stoppping

80
00:04:43,760 --> 00:04:46,650
但它有个缺点 我来解释一下

81
00:04:46,650 --> 00:04:50,870
我把机器学习过程看作几个不同的步骤

82
00:04:50,870 --> 00:04:55,960
其中之一是 你需要一个算法 能够最优化成本函数J

83
00:04:55,960 --> 00:04:59,660
我们有很多工具可以做到这点 比如梯度下降

84
00:04:59,660 --> 00:05:04,350
之后我们会讲到其他算法 比如Momentum算法

85
00:05:04,350 --> 00:05:08,070
RMSProp算法 Adam算法等等

86
00:05:08,070 --> 00:05:15,100
然而即便优化了成本函数J 你还是希望不要过拟合

87
00:05:15,100 --> 00:05:20,018
我们有些工具可以做到这点 比如正则化

88
00:05:20,018 --> 00:05:22,300
获取更多数据等等

89
00:05:22,300 --> 00:05:26,110
现在的机器学习中已经激增了 很多超参数(hyper-parameter)

90
00:05:26,110 --> 00:05:31,160
在诸多可能的算法中选择 已经相当复杂了

91
00:05:31,160 --> 00:05:34,340
因此我认为 机器学习可以变得更简单

92
00:05:34,340 --> 00:05:37,800
如果你有一套工具来优化成本函数J

93
00:05:37,800 --> 00:05:41,120
而当你专注于优化成本函数J时

94
00:05:41,120 --> 00:05:46,770
你在乎的只是找到合适的w和b 使得J(w,b)尽可能的小

95
00:05:46,770 --> 00:05:50,020
其他东西都不用考虑 你只要减小它就好

96
00:05:50,020 --> 00:05:55,346
然后 避免过拟合 也可以说成减小方差

97
00:05:55,346 --> 00:05:57,560
就是另一项完全不同的任务

98
00:05:57,560 --> 00:06:01,670
而你在做这件事的时候 又有另一套完全不同的工具来实现

99
00:06:01,670 --> 00:06:06,570
这个原则我们有时候叫它 正交化(orthogonalization)

100
00:06:06,570 --> 00:06:10,220
这概念就是指同一时间只考虑一个任务

101
00:06:10,220 --> 00:06:14,640
在之后的视频里我会再讲到正交化

102
00:06:14,640 --> 00:06:17,600
如果你还没能完全理解这个概念 也不需要太担心

103
00:06:17,600 --> 00:06:21,015
但对我而言 early stopping的主要缺点就是

104
00:06:21,015 --> 00:06:23,945
它把这两个任务结合了

105
00:06:23,945 --> 00:06:28,165
所以你无法分开解决这两个问题

106
00:06:28,165 --> 00:06:30,625
因为提早停止了梯度下降

107
00:06:30,625 --> 00:06:34,330
意味着打断了优化成本函数J的过程

108
00:06:34,330 --> 00:06:37,300
因为现在在降低成本函数J这件事上

109
00:06:37,300 --> 00:06:39,250
你做得就不够好了

110
00:06:39,250 --> 00:06:43,510
同时你又想做到避免过拟合

111
00:06:43,510 --> 00:06:46,430
所以你没有 用不同方法来解决这两个问题

112
00:06:46,430 --> 00:06:48,600
而是用一个工具解决两个问题

113
00:06:48,600 --> 00:06:51,250
这就意味着你要做的事情

114
00:06:52,370 --> 00:06:56,690
考虑起来更复杂了

115
00:06:56,690 --> 00:07:01,840
如果不用early stopping 可以替代的选择是L2正则化

116
00:07:01,840 --> 00:07:05,030
那么你可以尽可能久的训练神经网络

117
00:07:05,030 --> 00:07:09,000
这样可以让超参数的搜索空间更易分解

118
00:07:09,000 --> 00:07:10,720
也因此更易搜索

119
00:07:10,720 --> 00:07:14,200
但这么做的缺点是 你可能必须尝试

120
00:07:14,200 --> 00:07:16,420
大量的正则化参数λ的值

121
00:07:16,420 --> 00:07:21,040
这使得搜索这么多λ值的

122
00:07:21,040 --> 00:07:22,060
计算代价很高

123
00:07:22,060 --> 00:07:26,500
而early stopping的优势是 只要运行一次梯度下降过程

124
00:07:26,500 --> 00:07:30,910
你需要尝试小w值 中等w值和大w值

125
00:07:30,910 --> 00:07:35,960
而不用尝试L2正则化中

126
00:07:35,960 --> 00:07:40,300
超参数λ的一大堆值

127
00:07:40,300 --> 00:07:43,910
如果你目前还不能完全明白 不用担心

128
00:07:43,910 --> 00:07:46,608
我们还会在之后的视频中

129
00:07:46,608 --> 00:07:49,784
很详细的讲到正交化 我想那时这个概念就更好理解了

130
00:07:49,784 --> 00:07:52,860
尽管有些缺点 很多人还是在使用它

131
00:07:52,860 --> 00:07:55,728
我个人偏爱只用L2正则化

132
00:07:55,728 --> 00:07:57,484
并尝试不同的λ值

133
00:07:57,484 --> 00:08:00,530
这预设你的计算能力是足够的

134
00:08:00,530 --> 00:08:03,350
但早终止法确实能够实现相近的效果

135
00:08:03,350 --> 00:08:06,910
而不用一个个尝试不同的λ值

136
00:08:06,910 --> 00:08:12,480
所以 你们现在已经知道了如何扩增数据集 以及early stopping

137
00:08:12,480 --> 00:08:17,550
来减小方差 或说是避免神经网络的过拟合

138
00:08:17,550 --> 00:08:19,830
接下来我们要讲一些

139
00:08:19,830 --> 00:08:23,320
优化问题的配置方法 来加速训练过程