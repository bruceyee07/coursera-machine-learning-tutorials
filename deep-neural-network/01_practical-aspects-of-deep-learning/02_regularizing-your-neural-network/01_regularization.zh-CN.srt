1
00:00:00,730 --> 00:00:03,660
如果你怀疑你的神经网络在数据上发生了过拟合

2
00:00:03,660 --> 00:00:05,840
也就是存在高方差问题

3
00:00:05,840 --> 00:00:09,400
你也许需要首先尝试使用正则化

4
00:00:09,400 --> 00:00:11,246
获取更多数据也是解决高方差问题的

5
00:00:11,246 --> 00:00:13,917
一个很可靠的方法

6
00:00:13,917 --> 00:00:15,869
但你并不是总能获取到更多的训练数据

7
00:00:15,869 --> 00:00:17,850
或者获取更多数据的代价太大

8
00:00:17,850 --> 00:00:21,760
但使用正则化通常有助于防止过拟合

9
00:00:21,760 --> 00:00:23,910
并降低网络的误差

10
00:00:23,910 --> 00:00:26,020
下面我们来看看正则化是如何生效的

11
00:00:26,020 --> 00:00:28,780
我们以逻辑回归为例进行阐述

12
00:00:28,780 --> 00:00:33,220
在逻辑回归中 你会尝试最小化代价函数J

13
00:00:33,220 --> 00:00:37,050
该代价函数定义为

14
00:00:37,050 --> 00:00:41,290
每一个训练样本的预测的损失之和

15
00:00:41,290 --> 00:00:45,140
其中w和b是

16
00:00:45,140 --> 00:00:48,175
逻辑回归的参数

17
00:00:48,175 --> 00:00:54,620
因此w是一个x维的参数向量 b是一个实数

18
00:00:54,620 --> 00:00:58,979
要为逻辑回归正则化 你需要

19
00:00:58,979 --> 00:01:03,154
加上这个lambda 它称为正则化参数

20
00:01:03,154 --> 00:01:04,609
我稍后会详细介绍

21
00:01:04,609 --> 00:01:10,072
lambda/2m 再乘以w范数的平方

22
00:01:10,072 --> 00:01:15,840
这里w的范数的平方等于

23
00:01:15,840 --> 00:01:22,580
对于j从1到n_x 求w_j的平方和

24
00:01:22,580 --> 00:01:27,750
也可以写成w的转置乘以w
即参数矢量w的欧几里得范数的平方

25
00:01:27,750 --> 00:01:31,910
这称为L2正则化

26
00:01:33,700 --> 00:01:36,618
因为这里使用的是参数矢量w的

27
00:01:36,618 --> 00:01:38,877
欧几里得范数 也称为L2范数

28
00:01:38,877 --> 00:01:41,780
为什么只对参数w进行正则化呢?

29
00:01:41,780 --> 00:01:47,130
为什么我们不把b的相关项也加进去呢？

30
00:01:47,130 --> 00:01:51,210
实际上你可以这样做 但我通常会把它省略掉

31
00:01:51,210 --> 00:01:56,310
因为你可以看一下你的参数
w往往是一个非常高维的参数矢量

32
00:01:56,310 --> 00:02:00,159
尤其是在发生高方差问题的情况下

33
00:02:00,159 --> 00:02:02,250
可能w有非常多的参数

34
00:02:02,250 --> 00:02:06,600
你没能很好地拟合所有的参数
而b只是单个数字

35
00:02:06,600 --> 00:02:10,200
几乎所有的参数都集中在w中 而不是b中

36
00:02:10,200 --> 00:02:12,890
即使你加上了最后这一项

37
00:02:12,890 --> 00:02:14,040
实际上也不会起到太大的作用

38
00:02:14,040 --> 00:02:17,960
因为b只是大量参数中的一个参数

39
00:02:17,960 --> 00:02:21,500
在实践中我通常就不费力气去包含它了

40
00:02:21,500 --> 00:02:22,962
但如果你想的话也可以(包含b)

41
00:02:22,962 --> 00:02:27,510
L2正则化是最常见的一种正则化方式

42
00:02:27,510 --> 00:02:32,042
你可能也听说过L1正则化

43
00:02:32,042 --> 00:02:38,422
即不使用L2范数

44
00:02:38,422 --> 00:02:45,674
而是使用lambda/m乘以这一项的和

45
00:02:45,674 --> 00:02:49,716
这称为参数矢量w的L1范数

46
00:02:49,716 --> 00:02:52,843
这里有一个数字1的小角标

47
00:02:52,843 --> 00:02:58,050
无论你在分母中使用m还是2m 它只是一个缩放常量

48
00:02:58,050 --> 00:03:03,020
如果你使用L1正则化 w最后会变得稀疏

49
00:03:03,020 --> 00:03:08,040
这意味着w矢量中有很多0

50
00:03:08,040 --> 00:03:11,700
有些人认为这有助于压缩模型

51
00:03:11,700 --> 00:03:16,140
因为有一部分参数是0 只需较少的内存来存储模型

52
00:03:16,140 --> 00:03:19,850
然而我在实践中发现 通过L1正则化让模型变得稀疏

53
00:03:19,850 --> 00:03:20,870
带来的收效甚微

54
00:03:20,870 --> 00:03:23,870
所以我觉得至少在压缩模型的目标上

55
00:03:23,870 --> 00:03:26,520
它的作用不大

56
00:03:26,520 --> 00:03:28,472
在训练网络时

57
00:03:28,472 --> 00:03:31,423
L2正则化使用得频繁得多

58
00:03:31,423 --> 00:03:34,301
抱歉 修改一下这里的标记

59
00:03:34,301 --> 00:03:35,929
最后要说明的是

60
00:03:35,929 --> 00:03:42,823
这里的lambda称为正则化参数

61
00:03:45,267 --> 00:03:48,172
通常使用开发集或hold-out交叉验证

62
00:03:48,172 --> 00:03:50,021
来配置这个参数

63
00:03:50,021 --> 00:03:53,274
通过尝试一系列的值 找出最好的那个

64
00:03:53,274 --> 00:03:57,662
即在训练集上得到较好的结果

65
00:03:57,662 --> 00:04:01,007
和保持参数的L2范数较小以避免过拟合

66
00:04:01,007 --> 00:04:03,088
之间进行取舍

67
00:04:03,088 --> 00:04:07,165
lambda是你需要调优的另一个超参数

68
00:04:07,165 --> 00:04:09,550
顺便一提 在编程练习中

69
00:04:09,550 --> 00:04:14,250
lambda是Python编程语言的保留关键字

70
00:04:14,250 --> 00:04:18,300
所以在编程练习中 我们把它写为lambd

71
00:04:19,340 --> 00:04:23,690
不写a 以避免和Python的保留关键字冲突

72
00:04:23,690 --> 00:04:27,740
我们使用lambd表示正则化参数lambda

73
00:04:29,190 --> 00:04:33,320
以上是在逻辑回归中实现L2正则化的方法

74
00:04:33,320 --> 00:04:35,280
那么在神经网络中呢?

75
00:04:35,280 --> 00:04:39,789
在神经网络中 你有一个代价函数

76
00:04:39,789 --> 00:04:44,621
它是你所有参数的函数 包括w[1] b[1]到w[L] b[L]

77
00:04:44,621 --> 00:04:48,906
这里大写的L是神经网络的层数

78
00:04:48,906 --> 00:04:54,129
因此 代价函数是m个训练样本上

79
00:04:54,129 --> 00:04:58,066
的损失之和

80
00:04:58,066 --> 00:05:03,087
至于正则化 再加上lambda/2m

81
00:05:03,087 --> 00:05:10,190
乘以所有参数W的范数的平方之和

82
00:05:10,190 --> 00:05:14,857
这里W是你的参数矩阵

83
00:05:14,857 --> 00:05:19,749
这里矩阵范数的平方定义为

84
00:05:19,749 --> 00:05:23,922
对于i和j

85
00:05:23,922 --> 00:05:29,250
对矩阵中每一个元素的平方求和

86
00:05:29,250 --> 00:05:31,248
如果你想为这个求和加上索引

87
00:05:31,248 --> 00:05:35,253
这个求和是i从1到n[l-1]

88
00:05:35,253 --> 00:05:38,537
j从1到n[l]

89
00:05:38,537 --> 00:05:44,497
因为w是一个n[l-1]列 n[l]行的矩阵

90
00:05:44,497 --> 00:05:51,320
这些是第l-1层和第l层的隐藏单元数量单元数量
或

91
00:05:51,320 --> 00:05:57,447
这个矩阵的范数 称为矩阵的弗罗贝尼乌斯范数

92
00:05:57,447 --> 00:06:03,710
使用角标F标记

93
00:06:03,710 --> 00:06:07,266
由于线性代数中某些神秘的技术原因

94
00:06:07,266 --> 00:06:10,491
这不叫矩阵的L2范数

95
00:06:10,491 --> 00:06:14,620
而是称为矩阵的弗罗贝尼乌斯范数

96
00:06:14,620 --> 00:06:16,980
我知道把它称为矩阵的L2范数会更加自然

97
00:06:16,980 --> 00:06:21,760
但由于某些你不需要了解的神秘原因

98
00:06:21,760 --> 00:06:24,090
习惯上把它称为弗罗贝尼乌斯范数

99
00:06:24,090 --> 00:06:27,232
它表示矩阵中元素的平方和

100
00:06:27,232 --> 00:06:30,060
有了它 现在要如何实现梯度下降呢?

101
00:06:30,060 --> 00:06:35,343
之前我们使用反向传播计算dw

102
00:06:35,343 --> 00:06:40,626
通过反向传播 我们能得到J关于w的偏导数

103
00:06:40,626 --> 00:06:46,166
准确来说是任意给定l的w

104
00:06:46,166 --> 00:06:52,995
之后更新w[l] 为w[l]减去学习速率乘以dw

105
00:06:52,995 --> 00:06:57,890
这是为目标函数添加正则化项之前的步骤

106
00:06:57,890 --> 00:07:02,941
现在我们为目标函数添加了这个正则化项

107
00:07:02,941 --> 00:07:07,643
就需要为这个dw加上lambda/m*w[l]

108
00:07:07,643 --> 00:07:10,760
然后就可以像之前一样计算这个更新

109
00:07:10,760 --> 00:07:14,829
可以证明 这个新的dw[l]

110
00:07:14,829 --> 00:07:19,315
仍然正确地定义了

111
00:07:19,315 --> 00:07:23,385
在添加了额外的正则化项之后

112
00:07:23,385 --> 00:07:27,980
代价函数关于参数的倒数

113
00:07:29,260 --> 00:07:33,990
出于这个原因 L2正则化有时也被称为

114
00:07:33,990 --> 00:07:36,730
权重衰减

115
00:07:36,730 --> 00:07:42,348
如果我把这个dw[l]的定义代入这里

116
00:07:42,348 --> 00:07:47,012
你会发现w[l]的更新是 w[l]乘以学习速率alpha

117
00:07:47,012 --> 00:07:51,994
乘以由反向传播得到的这一项

118
00:07:54,311 --> 00:08:02,816
加上lambda/m*w[l]

119
00:08:02,816 --> 00:08:04,431
在这里加上减号

120
00:08:04,431 --> 00:08:09,382
这等于

121
00:08:09,382 --> 00:08:14,494
[如图所示]

122
00:08:14,494 --> 00:08:18,822
[如图所示]

123
00:08:18,822 --> 00:08:22,324
这一项表示无论矩阵w[l]是多少

124
00:08:22,324 --> 00:08:25,480
你都会让它变得稍小一点

125
00:08:25,480 --> 00:08:28,270
实际上 这相当于在矩阵w上乘以

126
00:08:28,270 --> 00:08:33,030
1-alpha*lambda/m

127
00:08:33,030 --> 00:08:38,279
你实际上是让矩阵w
减去alpha*lambda/m乘以它本身

128
00:08:38,279 --> 00:08:41,130
就像让矩阵w乘以这个

129
00:08:41,130 --> 00:08:43,528
略小于1的数字

130
00:08:43,528 --> 00:08:48,688
这就是L2范数正则化又被称为权重衰减的原因

131
00:08:48,688 --> 00:08:53,716
因为它就像普通的梯度下降

132
00:08:53,716 --> 00:08:59,260
更新w为w减去alpha乘以从反向传播得到的原梯度

133
00:08:59,260 --> 00:09:04,616
但现在你还会让w乘以这个

134
00:09:04,616 --> 00:09:08,324
略小于1的项

135
00:09:08,324 --> 00:09:11,782
因此L2正则化又称为权重衰减

136
00:09:11,782 --> 00:09:15,641
我不会使用那个名称

137
00:09:15,641 --> 00:09:21,030
它被称为权重衰减的原因是第一项等于这个

138
00:09:21,030 --> 00:09:25,620
你只是让权重矩阵乘以了一个略小于1的数字

139
00:09:25,620 --> 00:09:28,511
以上就是在神经网络中实现L2正则化的方式

140
00:09:29,545 --> 00:09:32,796
可能有人会问 嘿Andrew

141
00:09:32,796 --> 00:09:35,675
为什么正则化能够防止过拟合?

142
00:09:35,675 --> 00:09:37,462
让我们继续看下一个视频

143
00:09:37,462 --> 00:09:41,805
其中介绍了正则化能防止过拟合的原因