1
00:00:00,410 --> 00:00:04,180
Além da regularização de L2 e do abandono da regularização,

2
00:00:04,180 --> 00:00:08,050
há algumas outras técnicas para reduzir
sobreajuste na sua rede neural.

3
00:00:08,050 --> 00:00:09,200
Vamos analisar.

4
00:00:09,200 --> 00:00:10,955
Digamos que você esteja ajustando 
um classificador de gato.

5
00:00:10,955 --> 00:00:15,590
Se você estiver sobreajustando,
obter mais dados de treino pode ajudar, mas obter mais

6
00:00:15,590 --> 00:00:20,970
dados de treino pode ser caro e,
 às vezes, você simplesmente não tem como fazê-lo.

7
00:00:20,970 --> 00:00:24,670
Mas, o que você pode fazer é aumentar
o seu conjunto de treino, pegando uma imagem como esta,

8
00:00:24,670 --> 00:00:27,440
e por exemplo, 
invertendo-a horizontalmente

9
00:00:27,440 --> 00:00:29,570
e implementando isso também 
no seu conjunto de treino.

10
00:00:29,570 --> 00:00:32,950
Então, agora, ao invés de apenas
um exemplo no seu conjunto de treino,

11
00:00:32,950 --> 00:00:35,320
você pode adicionar isso
 ao seu exemplo de treinamento.

12
00:00:35,320 --> 00:00:38,040
Então, invertendo imagens horizontalmente,

13
00:00:38,040 --> 00:00:40,670
você pode dobrar o tamanho 
do seu conjunto de treino.

14
00:00:40,670 --> 00:00:44,530
Por seu conjunto de treino estar um pouco redundante
 agora, isso pode não ser tão bom quanto se você tivesse

15
00:00:44,530 --> 00:00:50,200
coletado um conjunto adicional
independente de exemplos novos em folha.

16
00:00:50,200 --> 00:00:55,290
Porém, você poderia fazer isso
sem precisar pagar a despesa de sair por aí

17
00:00:55,290 --> 00:00:57,100
atrás de mais fotos de gatos.

18
00:00:57,100 --> 00:00:59,710
Mas, além de inverter horizontalmente,

19
00:00:59,710 --> 00:01:02,170
você também pode pegar
cortes aleatórios da imagem.

20
00:01:02,170 --> 00:01:06,220
Então, aqui nós giramos e
ampliamos a imagem aleatoriamente

21
00:01:06,220 --> 00:01:07,720
e, ainda assim, 
se parece com um gato.

22
00:01:07,720 --> 00:01:11,830
Assim, fazendo distorções aleatórias 
e girando da imagem,

23
00:01:11,830 --> 00:01:16,490
você pode aumentar o conjunto de dados
e produzir exemplos de treino adicionais falsos.

24
00:01:16,490 --> 00:01:20,780
Novamente, estes falsos exemplos de treino extras
não adicionam muita informação,

25
00:01:20,780 --> 00:01:25,900
uma vez que não são um 
novo exemplo independente de um gato.

26
00:01:25,900 --> 00:01:28,982
Contudo, fazer isso quase de graça, ao invés de

27
00:01:28,982 --> 00:01:30,562
enfrentar custos computacionais,

28
00:01:30,562 --> 00:01:37,007
pode se tornar uma forma barata
de fornecer mais dados ao seu algoritmo,

29
00:01:37,007 --> 00:01:42,762
e dessa forma, regularizá-lo e
reduzir sobreajuste.

30
00:01:42,762 --> 00:01:47,299
E sintetizando exemplos como este,
o que você está realmente dizendo ao seu algoritmo

31
00:01:47,299 --> 00:01:51,570
é que se algo é um gato, mesmo depois
de ter sido invertido horizontalmente, continua sendo um gato.

32
00:01:51,570 --> 00:01:53,100
Note que eu não o inverti verticalmente,

33
00:01:53,100 --> 00:01:55,820
porque talvez nós não queiramos
gatos de cabeça para baixo, certo?

34
00:01:55,820 --> 00:01:58,940
E também, mesmo depois de ter 
ampliado e cortado parte da imagem,

35
00:01:58,940 --> 00:02:00,270
continua a ser um gato.

36
00:02:00,270 --> 00:02:04,750
Para reconhecimento de caractere óptico,
você também pode aumentar o seu conjunto de dados

37
00:02:04,750 --> 00:02:08,510
pegando dígitos e impondo rotações e
distorções aleatórias a eles.

38
00:02:08,510 --> 00:02:11,620
Assim, se você adicionar estas coisas ao seu conjunto de treino,

39
00:02:11,620 --> 00:02:13,490
estes também ainda serão dígitos 4.

40
00:02:14,740 --> 00:02:18,320
Para exemplificar, eu apliquei 
uma distorção bem forte.

41
00:02:18,320 --> 00:02:23,215
Este está bem ondulado, mas na prática,
você não precisa distorcer o quatro tão

42
00:02:23,215 --> 00:02:27,095
agressivamente, faça apenas uma distorção mais sutil
do que a que eu estou mostrando aqui,

43
00:02:27,095 --> 00:02:29,255
para fazer com que este exemplo fique 
mais claro para você.

44
00:02:29,255 --> 00:02:32,945
Então, uma distorção mais sutil 
é mais usada na prática,

45
00:02:32,945 --> 00:02:35,490
porque estes números quatro à direita
parecem muito distorcidos.

46
00:02:35,490 --> 00:02:40,410
Assim, aumentar cada dado
pode ser usado como uma técnica de regularização,

47
00:02:40,410 --> 00:02:43,020
na verdade, similar à regularização.

48
00:02:43,020 --> 00:02:46,970
Há uma outra técnica que é muito usada
chamada interrupção precoce (early stopping).

49
00:02:46,970 --> 00:02:52,010
O que você vai fazer é,
ao executar o gradiente descendente, traçar

50
00:02:52,010 --> 00:02:54,010
das duas uma: ou o erro de treinamento,

51
00:02:54,010 --> 00:02:57,860
você usará erro de classificação 01
 no conjunto de treino,

52
00:02:57,860 --> 00:03:00,860
ou apenas traçar a função de custo J, otimizando,

53
00:03:00,860 --> 00:03:04,200
e isso decresceria monotonicamente, 
desta forma, tudo bem?

54
00:03:04,200 --> 00:03:05,610
Pois, à medida que você treina, com sorte,

55
00:03:05,610 --> 00:03:09,120
a sua função de custo J deve decrescer.

56
00:03:09,120 --> 00:03:11,870
Assim, com a interrupção precoce,
o que você faz é traçar isto,

57
00:03:11,870 --> 00:03:15,970
e você também traça o seu erro 
do conjunto de desenvolvimento.

58
00:03:17,020 --> 00:03:20,920
E novamente, isso poderia ser um erro de classificação
em um conjunto de desenvolvimento, ou algo

59
00:03:20,920 --> 00:03:25,979
como a função de custo, como a perda logística
ou o logaritmo da perda do conjunto de desenvolvimento.

60
00:03:25,979 --> 00:03:29,770
Daí, percebemos que o erro do seu 
conjunto de desenvolvimento geralmente vai cair

61
00:03:29,770 --> 00:03:32,950
um pouco, e então vai aumentar a partir daqui.

62
00:03:32,950 --> 00:03:35,876
Então, o que a interrupção
precoce faz é que, você dirá, bem,

63
00:03:35,876 --> 00:03:40,167
parece que a sua rede neural estava funcionando melhor
perto daquela iteração,

64
00:03:40,167 --> 00:03:43,640
então, você quer apenas deixar de treinar
na sua rede neural na metade do caminho,

65
00:03:43,640 --> 00:03:47,310
e pegar qualquer valor atingido 
nesse erro do conjunto de desenvolvimento.

66
00:03:47,310 --> 00:03:48,260
E por que isso funciona?

67
00:03:48,260 --> 00:03:51,490
Quando você ainda não executou 
muitas iterações

68
00:03:51,490 --> 00:03:55,185
para a sua rede neural,
seus parâmetros w ficarão próximos de zero,

69
00:03:55,185 --> 00:03:59,720
pois, com inicialização aleatória,
você possivelmente inicializa w a valores pequenos

70
00:03:59,720 --> 00:04:04,190
aleatórios, então, antes de você treinar por um bom tempo,
w ainda continua bem pequeno.

71
00:04:04,190 --> 00:04:08,060
E à medida que você itera, enquanto você treina,
w fica maior e maior até

72
00:04:08,060 --> 00:04:14,120
aqui, onde talvez você tenha um valor muito maior
de parâmetros w para a sua rede neural.

73
00:04:14,120 --> 00:04:18,560
Assim, o que a interrupção precoce faz é
parar no meio do caminho, onde você tem apenas

74
00:04:18,560 --> 00:04:23,286
um peso w de porte médio.

75
00:04:23,286 --> 00:04:28,920
E similarmente à regularização L2,
escolhendo uma rede neural com norma

76
00:04:28,920 --> 00:04:34,630
menor para os seus parâmetros w,
com sorte, sua rede neural sobreajustará menos.

77
00:04:34,630 --> 00:04:37,270
E o termo "interrupção precoce"
refere-se ao fato de que você apenas

78
00:04:37,270 --> 00:04:40,760
interrompe o treino da sua rede neural mais cedo.

79
00:04:40,760 --> 00:04:43,760
Eu às vezes, uso interrupção precoce,
quando estou treinando uma rede neural.

80
00:04:43,760 --> 00:04:46,650
Mas, há uma desvantagem nisso, deixe-me explicar.

81
00:04:46,650 --> 00:04:50,870
Eu vejo que o processo de aprendizagem de máquina
compreende vários outros passos diferentes.

82
00:04:50,870 --> 00:04:55,960
Um, é que você quer um algoritmo para
otimizar a função de custo J

83
00:04:55,960 --> 00:04:59,660
e temos várias ferramentas para fazer isso,
como o gradiente descendente,

84
00:04:59,660 --> 00:05:04,350
e falaremos mais tarde sobre outros algoritmos,
como momentum,

85
00:05:04,350 --> 00:05:08,070
RMSprop e Adam, dentre outros.

86
00:05:08,070 --> 00:05:15,100
Mas, depois de ter otimizado a função de custo J,
você também queria não sobreajustar.

87
00:05:15,100 --> 00:05:20,018
E temos algumas ferramentas para fazer isso,
como a sua regularização,

88
00:05:20,018 --> 00:05:22,300
obtendo mais dados, etc.

89
00:05:22,300 --> 00:05:26,110
Agora, em aprendizado de máquina,
nós já temos tantos hiperparâmetros que surgem ao longo do tempo,

90
00:05:26,110 --> 00:05:31,160
que já é muito complicado escolher
entre o espaço de possíveis algoritmos.

91
00:05:31,160 --> 00:05:34,340
E então, eu acho mais fácil pensar 
em aprendizado de máquina,

92
00:05:34,340 --> 00:05:37,800
quando você tem um conjunto de ferramentas
para otimizar a função de custo J,

93
00:05:37,800 --> 00:05:41,120
e quando você está focando em 
otimizar a função de custo J,

94
00:05:41,120 --> 00:05:46,770
o que importa é encontrar w e b,
para que J(w,b) seja o menor possível.

95
00:05:46,770 --> 00:05:50,020
Você apenas não pensa em mais nada, 
além de reduzir isto.

96
00:05:50,020 --> 00:05:55,346
E então, é uma tarefa completamente
separada para não sobreajustar,

97
00:05:55,346 --> 00:05:57,560
ou seja, para reduzir a variância.

98
00:05:57,560 --> 00:06:01,670
E você tem um conjunto separado 
de ferramentas para fazer isso.

99
00:06:01,670 --> 00:06:06,570
E este princípio, às vezes, é chamado 
de ortogonalização.

100
00:06:06,570 --> 00:06:10,220
E há esta ideia de que você quer
ser capaz de pensar em uma tarefa de cada vez.

101
00:06:10,220 --> 00:06:14,640
Falarei mais sobre ortogonalização em um outro vídeo,

102
00:06:14,640 --> 00:06:17,600
então, se você não estiver entendendo agora,
não se preocupe com isso.

103
00:06:17,600 --> 00:06:21,015
Mas, para mim, a principal desvantagem 
da interrupção precoce

104
00:06:21,015 --> 00:06:23,945
são esses dois pares, essas duas tarefas.

105
00:06:23,945 --> 00:06:28,165
Você não pode mais trabalhar
com estes dois problemas independentemente,

106
00:06:28,165 --> 00:06:30,625
pois, interromper o gradiente descendente mais cedo,

107
00:06:30,625 --> 00:06:34,330
pode parar o que quer que você esteja fazendo
para otimizar a função de custo J,

108
00:06:34,330 --> 00:06:37,300
porque agora, você não está fazendo um bom trabalho
reduzindo a função de custo J.

109
00:06:37,300 --> 00:06:39,250
Você não o fez muito bem.

110
00:06:39,250 --> 00:06:43,510
E você também tenta simultaneamente, 
não sobreajustar.

111
00:06:43,510 --> 00:06:46,430
Logo, ao invés de usar ferramentas diferentes
para resolver os dois problemas,

112
00:06:46,430 --> 00:06:48,600
você está usando uma que mistura as duas.

113
00:06:48,600 --> 00:06:51,250
E isso faz com que fique mais complicado

114
00:06:52,370 --> 00:06:56,690
pensar sobre o conjunto de coisas que você poderia tentar.

115
00:06:56,690 --> 00:07:01,840
Ao invés de usar a interrupção precoce,
outra alternativa seria usar a regularização de L2,

116
00:07:01,840 --> 00:07:05,030
assim você consegue treinar a rede neural o máximo possível.

117
00:07:05,030 --> 00:07:09,000
Eu acho que isso faz com que o espaço
de pesquisa de hiperparâmetros fique mais fácil de decompor,

118
00:07:09,000 --> 00:07:10,720
e mais fácil de sondar.

119
00:07:10,720 --> 00:07:14,200
Mas, a desvantagem disso é que
você pode ter de tentar um monte de valores

120
00:07:14,200 --> 00:07:16,420
do parâmetro de regularização lambda.

121
00:07:16,420 --> 00:07:21,040
E isso faz com que a pesquisa sobre muitos valores de lambda
seja computacionalmente

122
00:07:21,040 --> 00:07:22,060
mais cara.

123
00:07:22,060 --> 00:07:26,500
E a vantagem da interrupção precoce é que,
ao se executar o processo de gradiente descendente

124
00:07:26,500 --> 00:07:30,910
apenas uma vez, você consegue
experimentar valores de w pequeno, w médio e

125
00:07:30,910 --> 00:07:35,960
w grande, sem precisar tentar um monte de valores
do hiperparâmetro de regularização

126
00:07:35,960 --> 00:07:40,300
L2 lambda.

127
00:07:40,300 --> 00:07:43,910
Se você ainda não estiver entendendo este conceito,
não se preocupe.

128
00:07:43,910 --> 00:07:46,608
Nós falaremos melhor sobre ortogonalização

129
00:07:46,608 --> 00:07:49,784
em outro vídeo mais tarde,
acho que vai ficar mais claro.

130
00:07:49,784 --> 00:07:52,860
Apesar dessas desvantagens, 
muitas pessoas a utilizam.

131
00:07:52,860 --> 00:07:55,728
Eu pessoalmente prefiro usar apenas
a regularização L2,

132
00:07:55,728 --> 00:07:57,484
e tentar valores diferentes de lambda.

133
00:07:57,484 --> 00:08:00,530
Isso, assumindo que você pode arcar
 com a computação para isso.

134
00:08:00,530 --> 00:08:03,350
Mas, a interrupção precoce faz
com que você consiga um efeito similar,

135
00:08:03,350 --> 00:08:06,910
sem ter de tentar explicitamente
um monte de valores de lambda.

136
00:08:06,910 --> 00:08:12,480
Então, você viu como usar o aumento do dado,
assim como usar a interrupção

137
00:08:12,480 --> 00:08:17,550
precoce para reduzir variância
ou para prevenir sobreajuste na sua rede neural.

138
00:08:17,550 --> 00:08:19,830
A seguir, falaremos sobre algumas técnicas para

139
00:08:19,830 --> 00:08:23,320
configurar seu problema de otimização,
para que seu treinamento seja executado rapidamente.
[Tradução: Diogo dos Santos Farias | Revisão: Carlos Lage]