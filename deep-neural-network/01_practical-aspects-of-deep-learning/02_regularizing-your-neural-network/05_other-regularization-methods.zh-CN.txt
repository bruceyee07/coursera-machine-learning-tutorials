除了L2正则化和 随机失活(dropout)正则化之外 还有一些其他方法 可以减少神经网络的过拟合 让我们一起来看看让我们一起来看看 我们假设你正在拟合猫的图片分类器 如果你过拟合了 可以增加训练数据 但扩大训练集代价很高 而且有时候就是无法得到更多数据 但你可以通过像这样处理图片 来扩增训练集 比如 把它水平翻转 然后也加到你的训练集当中 现在你的训练集里 不仅仅有这张图 这一张也可以加进去 所以 通过水平翻转图像 你可以把训练集的数据量翻倍 因为你的训练集现在有些冗杂 所以相比之下 不如另收集全新 独立的数据 不过这种方法可以节省你出门 拍更多猫照片的成本 除了水平翻转之外 你也可以随机裁剪图片 这里我们旋转再放大了一下图片 看上去仍然是一只猫 所以通过随机的扭曲 变换图片 你可以增强数据集 做出额外的伪训练样本 再次强调 这些额外的伪训练样本 它们能增加的信息量 不如全新的 独立的猫照片多 但因为这么做是几乎不需要任何开销的 只有一些计算代价 所以这是一个廉价的方式 来为你的算法获得更多数据 因此可以算作正则化 而且减少了过拟合 并且 通过像这样的合成样本 你实际上是在告诉你的算法 如果这是一只猫 那它水平翻转之后还是一只猫 注意 我并没有垂直翻转它 因为我们大概并不想要倒置的猫 随机放大图片的一部分 这很可能仍然是一只猫 对于字符识别 你也可以扩增数据集 通过给数字加上随机的旋转和变形 所以 如果把这些加到你的训练集里 它们仍然是数字4 为了举例 我用了一个很强的扭曲 所以看上去是一个波纹4 实际操作中 你不需要把4扭曲的这么夸张 比我这种再细微点就可以了 这个例子是为了让你们看得更清楚 但实际中通常用比较细微的变形 因为这个4看起来真的太扭曲了 所以数据集扩增(data augmentation) 可以作为一种正则化技术 准确地说是接近正则化 还有另一个常用的方法 叫做早终止法(early stopping) 你要做的是 在运行梯度下降(gradient descent)时 画一张训练误差的图 可以用训练集的0-1分类误差 或者把 成本函数J(也称代价函数)画出来 它应该要单调递减 像图中这样 因为当你训练的时候 理想状况下 你训练时的成本函数J应该是递减的 用early stopping 你要做的是画出这个曲线 同时也画出开发集误差的曲线 一样地 可以是开发集的分类误差 或者成本函数 比如开发集的对数损失函数(log loss) 现在你可以看出 通常开发集误差会先下降一段 然后接着开始增大 所以early stopping做的就是 你会发现 在那次迭代附近 你的神经网络表现得最好 那我们想做的就是 把神经网络的训练过程停住 并且选取这个(最小)开发集误差所对应的值 那么为什么这个方法是有用的呢? 当你刚开始在神经网络上迭代时 你的参数w会接近于0 因为通过随机初始化(random initialization) 很可能你给w的初始值是一个较小值 因此在训练足够长时间前 w仍然很小 然后随着你继续迭代 训练 w越来越大 直到这里 可能你的神经网络就有一个很大的参数w了 所以early stopping做的是 通过停在半路 你能得到一个不大不小的w值 这点和L2正则化有点像 通过选一个 参数w范数较小的神经网络 理想状况下就能少点过拟合了 而early stopping这个词就是指 你会提前终止神经网络的训练 在训练神经网络时 我有时候会用early stoppping 但它有个缺点 我来解释一下 我把机器学习过程看作几个不同的步骤 其中之一是 你需要一个算法 能够最优化成本函数J 我们有很多工具可以做到这点 比如梯度下降 之后我们会讲到其他算法 比如Momentum算法 RMSProp算法 Adam算法等等 然而即便优化了成本函数J 你还是希望不要过拟合 我们有些工具可以做到这点 比如正则化 获取更多数据等等 现在的机器学习中已经激增了 很多超参数(hyper-parameter) 在诸多可能的算法中选择 已经相当复杂了 因此我认为 机器学习可以变得更简单 如果你有一套工具来优化成本函数J 而当你专注于优化成本函数J时 你在乎的只是找到合适的w和b 使得J(w,b)尽可能的小 其他东西都不用考虑 你只要减小它就好 然后 避免过拟合 也可以说成减小方差 就是另一项完全不同的任务 而你在做这件事的时候 又有另一套完全不同的工具来实现 这个原则我们有时候叫它 正交化(orthogonalization) 这概念就是指同一时间只考虑一个任务 在之后的视频里我会再讲到正交化 如果你还没能完全理解这个概念 也不需要太担心 但对我而言 early stopping的主要缺点就是 它把这两个任务结合了 所以你无法分开解决这两个问题 因为提早停止了梯度下降 意味着打断了优化成本函数J的过程 因为现在在降低成本函数J这件事上 你做得就不够好了 同时你又想做到避免过拟合 所以你没有 用不同方法来解决这两个问题 而是用一个工具解决两个问题 这就意味着你要做的事情 考虑起来更复杂了 如果不用early stopping 可以替代的选择是L2正则化 那么你可以尽可能久的训练神经网络 这样可以让超参数的搜索空间更易分解 也因此更易搜索 但这么做的缺点是 你可能必须尝试 大量的正则化参数λ的值 这使得搜索这么多λ值的 计算代价很高 而early stopping的优势是 只要运行一次梯度下降过程 你需要尝试小w值 中等w值和大w值 而不用尝试L2正则化中 超参数λ的一大堆值 如果你目前还不能完全明白 不用担心 我们还会在之后的视频中 很详细的讲到正交化 我想那时这个概念就更好理解了 尽管有些缺点 很多人还是在使用它 我个人偏爱只用L2正则化 并尝试不同的λ值 这预设你的计算能力是足够的 但早终止法确实能够实现相近的效果 而不用一个个尝试不同的λ值 所以 你们现在已经知道了如何扩增数据集 以及early stopping 来减小方差 或说是避免神经网络的过拟合 接下来我们要讲一些 优化问题的配置方法 来加速训练过程