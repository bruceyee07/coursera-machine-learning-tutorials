Drop out은 여러분의 네트워크에서 유닛을 해체시키는 이상한 현상을 일으킵니다. 반면에 일반화에서는 어떻게 잘 작동할 수 있는 것일까요? 이해를 위해 직관적인 내용을 좀 더 다루어보도록 하겠습니다. 이전 비디오에선, 이전 강의에서 여러분에게 dropout에서는 무작위로 여러분의 네트워크 유닛을 해체시킨다고 설명해드렸는데요, 그러니까 반복할 때마다 더 작은 신경망으로 일하는 것과 같기 때문에, 마치 더 작은 신경망을 사용하면 일반화효과가 있어야 할 것만 같습니다. 두번째 직관적인 면인데요, 단일 유닛의 개념으로 생각해보죠, 이것을 예로 보시겠습니다. 이 유닛이 제 역할을 하기 위해선 입력값을 갖고, 또 의미 있는 결과 값을 생성해야 합니다. dropout에선, 입력값이 무작위로 제거될 수 있습니다. 간혹 저기 2개의 유닛이 제거될 것입니다. 또 가끔씩은 다른 유닛이 제거될 수 있습니다. 이런 현상이 의미하는 바는, 이 유닛이 이렇게 보라색으로 동그랗게 표시하고 있는 부분이, 어떤 한가지의 특성에 의존하면 안됩니다. 그 이유는 어떤 한가지의 특성은 언제든지 임의로 제거될 수 있기 때문이죠. 그렇기 때문에 특히, 모든 것을 하나의 입력값에 베팅하는 것인 꺼리게 되겠죠. 맞죠? 이렇듯이, 한 입력값에 너무 많은 비중 (weight)을 부여하기 꺼리게 될 것입니다. 그러므로 여기 이 유닛은 이 쪽 방향으로 퍼지길 원할 것입니다. 그리하여 여기 4개의 입력값에 비중을 나눠 주는 것이죠. 이렇게 비중을 분산해주면, 비중의 (weight) 제곱 노름을 축소시키는 효과가 있을 것입니다. 그러므로 L2 일반화에서 본 것과 비슷하게, droupout을 도입하는 것은 그 비중 (weight)을 줄이고 L2 일반화과 같은 것을 진행해 over fitting을 막아줍니다. 알고 보니 공식적으로 dropout은 L2 일반화가 변형된 버전이라고 할 수 있는데요, 그러나 비중 별로 부과하는 L2 페널티가 다릅니다. 곱셈이 적용되는 activation의 크기에 따라 말이죠, 요약하자면, dropout은 l2 일반화 과 비슷한 효과를 가지고 있다고 보여줄 수 있습니다. L2 일반화 과 적용되는 부분에서의 차이만 있고 다른 입력값의 스케일에 따라 더 변형한다는 차이가 있습니다. dropout을 도입하는 것에 대한 내용을 한가지 더 말씀 드리자면, 여기는 3개의 입력 특성이 있는 네트워크입니다. 이것은 7개의 숨겨진 유닛이고요, 7, 3, 2, 1입니다. 저희가 선택해야 했던 매개 변수 중 하나는 바로 keep.prob입니다. 이것은 층마다 유닛을 유지할 확률을 나타내죠. 그렇기 때문에 keep.prob을 층별로 변하게 하는 것이 가능한데요, 예를 들어 첫번째 층에서는, W1의 매트릭스는 3 X 7일 것입니다. 2번째 weight matrix는 7 X 7 일 것이구요, W3는 7 X 3일 것입니다. 그렇게 하면 W2가 사실 가장 큰 weight 매트릭스인데요, 가장 큰 매개 변수 set가 W2에 있을 것이기 때문이죠, 7 X 7 인 것입니다. 그 매트릭스의 over fitting을 방지하기 위해, 잘하면 여기 이 층을 이것은 2번 층이겠네요, 이것은 keep-prob이 꽤 낮을 수 있습니다. 예를 들어, 0.5일 수 있겠죠. over fitting에 대해 그닥 걱정을 안할 층들에서는 조금 더 큰 값의 key-prob이 있을 수 있겠죠. 잘하면 0.7일 수 있겠습니다. 만약 over fitting에 대해 전혀 걱정하지 않는 경우라면, key-prob이 0이 될 수 있겠습니다. 명료성을 위해서 여기 이 숫자들을 여기 이 보라색 박스로 나타내겠습니다. 이것들은 다른 층들에 대한 다른 key-prob값들입니다. 1.0의 key prob 은 모든 유닛을 유지시킨다는 뜻입니다. 그러므로, dropout을 그 층에서는 쓰지 않는 것으로 해석할 수 있습니다. overfitting에 대해 조금 더 걱정을 하는 층에서는, 매개 변수가 많은 층들에서는, 조금 더 간략한 유형의 dropout을 적용하기 위해서 key-prob을 낮게 설정할 수 있습니다. 이것은 마치 일반화 매개 변수인 L2 regularization Lambda의 값을 올리는 것과 유사합니다. 다른 층보다 더 일반화 하기 위해서 말이죠. 엄밀히 이야기하면 입력 층에도 dropout를 적용시킬 수 있습니다. 여기서는 1개 또는 복수의 입력 특성을 최대치로 하는 확률을 가질 수 있습니다. 실제로 자주 사용하는 방법은 아닙니다. 그래서 입력값으로는 1.0의 key-prob이 상당히 흔합니다. 또, 0.9와 같은 큰 값을 사용할 수도 있습니다. 그러나 여러분이 절반 입력 특성을 제거하고 싶을 확률은 사실 작죠. 그러니 key-prob은, 규칙을 적용하는 경우, 1과 가까운 값이 될 것입니다. 입력 층에 dropout을 적용만 하더라도 말이죠. 요약하자면, 어떤 층이 다른 층보다 overfitting하는 것이 걱정이 된다면 다른 층보다 더 작은 key-prob을 설정하는 것이 가능합니다. 이것의 단점은, 상호 검증를 사용하는 검색에 훨씬 더 많은 하이퍼 파라미터들이 생긴다는 것입니다. 또 다른 방법으로는 어떤 특정 층에서 dropout을 적용시키고 어떤 층에서는 적용시키지 않으면서, 동시에 한개의 하이퍼 파라미터를 이용해서, dropout이 적용되는 경우에, 그 층들의 key-prob이 되는 것입니다. 끝내기 전에, 몇가지 실행 팁을 알려드리겠습니다. 처음의 많은 dropout의 성공적인 도입은 computer vision에서 발생했습니다. 커퓨터 비전에서는, 입력값의 크기가 너무 커서, 모든 픽셀들을 입력하는 데에는 거의 항상 데이터가 부족합니다. 그렇기 때문에 컵퓨터 비진에서 자주 drop이 활용됩니다. 그리고 컴퓨터 비전 리서치 연구원들 중 거의 항상 dropout을 기본값으로 사용하는 사람들로 있습니다. 기역 하셔야 할 것은, droupout은 일반화 테크닉이고, over-fitting 방지에 도움을 준다는 것입니다. 그렇기 때문에 본인의 알고리즘의 over-fitting하지 않는 이상, dropout을 쓰라고 하지는 않을 것입니다. 그렇기 때문에 적용분야가 다른 것들에 비해 적은 편인데요, 컴퓨터 비전 에서는 보통 데이터가 충분하지 않기 때문에, 거의 항상 over-fitting합니다. 그렇기 때문에 컴퓨터 비전 리서치 연구원 일부는 거의 dropout을 맹신하는 경우가 있죠. 하지만 이런 직관이 다른 분야에 일반화되지는 않는다고 봅니다. dropout의 큰 단점은 바로 J 비용함수가 더이상 클리어하게 정의되지 않는다는 점입니다. 반복 테스트를 할때마다, 노드 뭉치를 제거해나가기 때문이죠. 그렇기 때문에, 기울기 강하의 성능을 더블체크하는 경우에, iteration마다 계속 감소하는 J비용함수가 잘 정의되었다는 것을 더블체크하기는 더 어렵습니다. 그 이유는 최적화하려 고하는 J비용함수는 덜 정확히 정의되어있고, 또는 확실히 계산을 하기가 어렵기 때문입니다. 이런 그래프를 나타내기 위한 디버깅 도구를 잃는 것입니다. 그래서 제가 주로 하는 것은 dropout을 끄고, key-prob을 1로 지정합니다. 그리고 코드를 실행해서 J가 점진적으로 감소하는 것을 확인하고, dropout을 키고 바라기 시작하죠. 제가 dropout중에 버그를 코드에 안 가지고 왔다고 바라는 것이죠. 다른 방법이 필요하겠지만, 이러한 수치들을 plot하지 않으면서 dropout에서도 코드가 정상적으로 작동하고, 기울기 강하 가 잘 작동하도록 하겠습니다. 이것을 끝으로, 아직 일반화 테크닉과 관련하여 알만한 내용이 더 있는데요, 이런 방법은 다음 비디오를 통해 다루도록 하겠습니다.