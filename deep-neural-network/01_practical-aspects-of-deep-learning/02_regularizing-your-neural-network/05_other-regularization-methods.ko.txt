L2 일반화 과 drop out 일반화 외에도 신경망의 overfitting을 줄일 수 있는 몇가지의 
테크닉이 있습니다. 한번 살펴보겠습니다. 한번 자세히 살펴볼까요. CAT crossfire를 피팅한다고 해봅시다. 만약 overfitting의 경우에는, 
트레이닝 데이터가 도와줄 수 있는데요, 트레이닝데이터는 비쌀 수 있습니다.
그리고 또 데이터를 단순히 더 수집할 수 없는 경우도 있습니다. 하지만 이런 이미지를 가지고 트레이닝 세트를 증가시킬 수 있는데요, 그리고 예를 들어, 
가로로 뒤집어서 트레이닝 세트에 추가할 수도 있고요, 그러면 여기 트레이닝 세트의
한가지 예뿐만 아니라, 이 트레이닝 예시도 넣을 수 있습니다. 그러면 이렇게 가로로 이미지를 뒤집어서, 트레이닝 세트의 크기를 2배로 만들 수 있는 것입니다. 이제 여려 분의 트레이닝 세트가 중복되기 때문에 완전히 추가로 새로운 독립적인 이미지 샘플을 추가한 것보다는
좋지 못합니다. 하지만 더 추가적인 고양이 사진을 찾는데
시간 소비할 필요 없이 이렇게 빨리 진행할 수 있는 점이 있습니다. 가로로 뒤집는 방법 말고 또, 이미지를 크롭할 수 있습니다. 여기서는 회전하고 임의로 이미지를 줌인 처리 했는데요 이 이미지는 아직까지 고양이처럼 보입니다. 이렇게 이미지를 찌그러트리거나
변형을 주어 데이터세트를 확장하는 방법이 있습니다.
바로 이와 같이 가짜의 추가 트레이닝 샘플을 만드는 것이죠. 다시 한번 말씀 드리자면, 이러한 가짜의 샘플은
정보적으로는 완전히 새로운 고양이 이미지를 추가하는 것보다는 도움이 되지 않습니다. 하지만 이렇게 하는 것 또한 
방법이기 때문에 무료로 대치 비용없이 진행할 수 있습니다. 비싸지 않은 방법으로써,
알고리즘에 데이터를 더 제공하고 일반화를 통해 over fitting을 결과적으로
줄일 수 있는 하나의 방법입니다. 이런 샘플들을 합성하여 
여러분의 알고리즘에게 알려주는 것은 바로 고양이의 사진은 가로로 뒤집더라도 
고양이 사진이라는 것입니다. 눈치채셨겠지만, 저는 세로로 뒤집지는 않았습니다. 왜냐면, 거꾸로 서있는 고양이는 원치 않기 때문이죠, 맞죠? 또, 임의로 이미지 일부분을 줌인 하였는데요. 이 경우에도 역시 고양이 사진은 마찬가지로 동일합니다. 시각적 캐릭터 인식 기능에서는 여러분은 특정 숫자를 갖고 임의의 회전과 찌그러트림으로
데이터세트를 불러올 수 있습니다. 만약에 이런 것을 트레이닝 세트에 추가하면, 이 숫자들은 아직 모두 4입니다. 예제에서 저는 아주 강항 찌그러트림을 적용했는데요, 이것은 웨이브가 많이 적용됐고, 
실제로 이렇게 많이 찌그러트릴 필요는 없습니다만 여기서보다 조금 덜 적용시키면 되겠습니다. 이 예제에서 조금 더 확실히 보여드리기 위해서 그랬습니다.
아시겠죠? 하지만 실제로는 조금 덜 찌그러트리는데요 이것은 거의 휘어진 것과 같은 숫자 4인데요 그리하여 data augmentation을 이용해서 
일반화 테크닉에 일부로 적용시키면 됩니다. 일반화와 비슷하게, 쓰이는 기술 중 early stopping이라고 하는 기술이 있습니다. 여기서는 무엇을 할 것이냐면, 
기울기 강하를 실행하면서 트레이닝 오류나, 트레이닝 세트에서의 01 classification 오류 또는, 최적화 J 비용함수를 그리십시오. 그 그래프는 점진적으로 이렇게 감소하겠죠. 맞죠? 왜냐면 여러분이 점차 트레이닝 시키면서 J 비용함수를 둘러싸 트레이닝 시키므로
이상적으로는 감소하겠죠. early stopping에서는 이 그래프를 그리고, 또한, dev set 오류를 그립니다. 다시한번, 이것은 development set에서의 classification 오류
일수 있고 또는, logistic loss처럼 비용함수일 수도 있습니다.
또는 dev set에서의 log loss과 같은 함수일수도 있고요. 여러분이 발견하겠지만, 
dev set 오류는 어느 정도 감소하다가 이 지점 이후로 다시 증가하는
것을 볼 수 있습니다. early stopping이 하는 것은 신경망이 여기까지 테스트를 잘 수행했으니 반쯤 지난 시점에서 신경망에서의 트레이닝을 그만 진행하고 
싶습니다. 값은 여기까지만 dev set오류를 얻은 데 까지만 갖는 것입니다. 이것이 어떻게 작동하는 것일까요? 만약 신경망에서 반복 테스트를 몇 번 수행 안 한 경우, w 매개 변수는 거의 0에 가까울 것입니다. 임의의 초기화에서 아마 w를 작은 값으로 초기화하기 때문에 w를 긴 시간 트레이닝 하기 전에는 작은 값일 것입니다. 그리고 반복 수행을 하면서 w의 값은 커지고 또 커질텐데요, 그러면 여기에서는 신경망에서의 w 파라미터수는 
훨씬 더 큰 값으로 되어 있을 것입니다. early stopping은 w의 비율이 중간 정도 되는 시점에서 중지시키는 역할을 합니다. 그러므로 L2 일반화와 비슷하게, 
신경망에서 w 파라미터수가 비슷한 노름을 선정하여 결과적으로 신경망이 덜 overfitting하게 만들어주는
것입니다. 그리고 early stopping 이라는 용어는 
말 그대로 신경망의 트레이닝을 일찍 중간에 
스탑시키는 것을 말합니다. 저는 신경망을 트레이닝 시킬 때 
가끔 early stopping 을 이용합니다. 한가지 단점이 있기는 한데요.
설명해드리겠습니다. 저는 머신러닝이 몇가지 다른 단계를 
갖고 있다고 생각합니다. 첫번째로, j 비용함수를 최적화 시킬 수 있는
알고리즘을 원할 텐데요 기울기 강하와 같은 여러 가지 도구를 이용해서 
그렇게 할 수 있죠. 그리고 나중에 다른 알고리즘에 대해서 이야기 할 텐데요, RMS prop 그리고 Atom과 같은 알고리즘 말이죠. 그러나 j 비용함수를 최적화시킨 다음으로는 
overfit또한 원치 않았습니다. 이렇게 하기 위한 일반화와 같은 도구도 있었죠. 또는 단순히 데이터를 더 많이 수집하는 방법도 있습니다. 이미 머신러닝에서는 너무 많은 하이퍼 파라미터가 있습니다. 가능한 알고리즘들 사이에서 고르는 것이 굉장히 복잡합니다. 저는 머신런닝에대해 조금 더 쉽게 이해 할 수 있는데요, J 비용함수를 최적화 시킬 수 있는 도구가 있거나 J 비용함수를 최적화하는데 집중을 하는 경우에 말이죠. 유일히 신경 쓰는 것은 w와 b를 찾고
J(w, b)가 최대한 작게 되도록 만드는 것입니다. 이것을 작게 만드는 것 외에는 신경 쓰지 않습니다. 그리고, overfit을 하지 않기 위한 방법은
완전히 별개의 업무입니다. 다시 말해, 편차를 줄이기 위한 방법 말이죠. 이것을 진행하는 경우에 쓰이는 도구가 따로 있습니다. 이 원리는 가끔 직교화(orthogonalization)라고 불리는데요. 이것은 아이디어를 하나씩 생각한다는 건데요. 나중에 이어지는 강의 비디오에서 
직교화에 대해 더 이야기 하겠습니다. 컨셉이 전체적으로 완전히 이해가 안 가시더라도
걱정하지 마십시요. 하지만 저에게는
early stopping의 주된 단점은, 이것이 이 2개의 업무를 묶는다는 것입니다. 그러면 이 2개의 별개 문제를 
단독으로 풀 수가 없게 됩니다. 기울기 강하를 일찍이 정지시키기 때문에, J 비용함수를 최적화시키는 진행 단계에서 중간에 
자르는 것과 마찬가지 인데요, 그 이유는 J 비용함수를 잘 못 줄이고 
있기 때문에 그런 것이죠 어떻게 보면 잘 못했기 때문이죠. 그와 동시에 overfit이 되지 않도록 
하는 것입니다. 그렇기 때문에 이런 2문제를 
다른 도구를 사용해서 푸는 것이 아니라, 한가지를 이용해서 
2가지가 약간 섞이는 것입니다. 그러면 결과적으로 여러 가지 시도할 수 있는 방법들에 대해서
더 복잡하게 만드는 경향이 있습니다. early stopping를 사용하는 대신 
그냥 L2 일반화를 사용하는 방법이 있는데요, 이 경우, 신경망을 최대한 길게 트레이닝 시키면 되겠습니다. 저는 이 경우가 더 하이퍼 파라미터의 서치 넓이를
분해시키는데 쉽다고 생각합니다. 그리고 다시 서치하는데도 말이죠. 하지만 이것의 단점은, 
여러 가지 일반화 파라미터 Lambda 값들을 시도해봐야 한다는 점입니다. 그렇기 때문에 여러가지 Lambda값을 서칭하는 것은 더 비싼 방법이겠습니다. 그리고 of early stopping 의 장점은 
기울기 강하를 한번만 실행하도 작은 w값, 중간 w값, 큰 w값을 한번에 시도할 수 있습니다.
L2 하이퍼 파라키터 일반화의 여러 가지 Lambda 값들을 시도해 볼 필요없이 말이죠. 이 컨셉이 완전히 이해가 되지 않으셔도
걱정하지 마십시요. 직교화 에 대해 나중에 강의 비디오에서 이야기하도록 하겠습니다.
그 때 더 이해가 많이 되실 것입니다. 단점이 있음에도 불구하고
사람들은 많이 사용합니다. 저는 개인적으로 L2 일반화를
이용해서 여러가지 Lambda의 값을 시도하는 것을 선호하는데요. 이것은 computation 비용을 감안했을 때 그렇겠죠. 하지만 early stopping 또한
비슷한 효과를 얻게 해주는데요, Lambda의 여러가지 값을 시도할 필요없이 말이죠, 이제 여러분은 데이터 확장을 어떻게 
활용하는지 배웠는데요, 또한 편차 또는 신경망의 overfitting을 막기 위한 
early stopping에 대해서도 배웠습니다. 다음으로, 트레이닝을 빠르게 하도록 
최적화 문제를 세팅하는 기술들에 대해 배우겠습니다.