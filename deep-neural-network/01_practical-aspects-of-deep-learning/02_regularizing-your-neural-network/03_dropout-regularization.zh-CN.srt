1
00:00:00,000 --> 00:00:02,560
除了L2正则化

2
00:00:02,560 --> 00:00:06,875
另一种非常强大的正则化技术是 随机失活正则化(丢弃法 dropout)

3
00:00:06,875 --> 00:00:08,715
让我们来看看是如何工作的

4
00:00:08,715 --> 00:00:12,600
假设你训练左图所示的神经网络并发现过拟合现象

5
00:00:12,600 --> 00:00:14,998
你可以随机失活技术来处理它

6
00:00:14,998 --> 00:00:16,765
让我先拷贝这个神经网络图

7
00:00:16,765 --> 00:00:21,100
使用随机失活技术 我们要遍历这个网络的每一层

8
00:00:21,100 --> 00:00:26,350
并且为丢弃(drop)网络中的某个节点置一个概率值

9
00:00:26,350 --> 00:00:29,305
即对于网络中的每一层

10
00:00:29,305 --> 00:00:30,955
我们将对每一个结点作一次公平投币

11
00:00:30,955 --> 00:00:34,165
使这个节点有50%的几率被保留

12
00:00:34,165 --> 00:00:38,005
50%的的几率被丢弃

13
00:00:38,005 --> 00:00:39,820
抛完这些硬币

14
00:00:39,820 --> 00:00:42,865
我们会决定消除哪些节点

15
00:00:42,865 --> 00:00:49,775
然后清除那些节点上所有正在进行的运算

16
00:00:49,775 --> 00:00:51,550
所以你最后得到的是一个小得多的

17
00:00:51,550 --> 00:00:53,150
被简化了很多的网络

18
00:00:53,150 --> 00:00:56,145
然后再做反向传播训练

19
00:00:56,145 --> 00:00:59,705
这是一个被简化的神经网络的例子

20
00:00:59,705 --> 00:01:01,130
对于不同的训练样例(examples)

21
00:01:01,130 --> 00:01:03,700
你可以为所有的节点重新进行若干轮投币

22
00:01:03,700 --> 00:01:07,585
保留和消除不同的节点

23
00:01:07,585 --> 00:01:09,235
对每一个训练样例

24
00:01:09,235 --> 00:01:14,455
你可以选择其中任意一个网络进行训练

25
00:01:14,455 --> 00:01:16,675
也许这个技术看起来有点疯狂

26
00:01:16,675 --> 00:01:20,470
他们只是按照随机的编码决定这些节点的去留

27
00:01:20,470 --> 00:01:22,505
但是这个技术确实是有效的

28
00:01:22,505 --> 00:01:28,480
因为对于每一个训练样例你都在训练一个小得多的网络

29
00:01:28,480 --> 00:01:34,591
这样或许能让你理解为什么你能正则化整个网络

30
00:01:34,591 --> 00:01:38,590
因为被训练的是一些小得多的网络

31
00:01:38,590 --> 00:01:41,535
让我们看看如何实现随机失活算法

32
00:01:41,535 --> 00:01:43,425
有几种方法可以实现随机失活算法

33
00:01:43,425 --> 00:01:44,915
我将展示最常用的一种

34
00:01:44,915 --> 00:01:47,865
反向随机失活(inverted dropout)

35
00:01:47,865 --> 00:01:49,645
完整起见

36
00:01:49,645 --> 00:01:58,977
我们在l=3的层上演示这个技术

37
00:01:58,977 --> 00:02:03,000
在我准备写的代码中 这里有一堆的3

38
00:02:03,000 --> 00:02:08,380
我将演示如何实现单层的随机失活技术

39
00:02:08,380 --> 00:02:12,155
设置一个矢量d

40
00:02:12,155 --> 00:02:16,503
d3表示层3的失活向量

41
00:02:16,503 --> 00:02:21,585
3将作为np.random.rand(a)中a的后缀

42
00:02:21,585 --> 00:02:27,708
d3将获得和a3一样的形状

43
00:02:27,708 --> 00:02:31,261
当d3中的某个元素小于某个值

44
00:02:31,261 --> 00:02:34,470
这个值命名为keep.prob

45
00:02:34,470 --> 00:02:37,350
即keep.prob是一个数值

46
00:02:37,350 --> 00:02:39,105
之前我将它赋值为0.5

47
00:02:39,105 --> 00:02:42,045
在这个样例中它赋值为0.8

48
00:02:42,045 --> 00:02:47,040
这是给定隐藏单元将被保留的概率值

49
00:02:47,040 --> 00:02:49,129
keep.prob=0.8

50
00:02:49,129 --> 00:02:54,665
意味着这个隐藏单元有0.2的几率被丢弃

51
00:02:54,665 --> 00:02:58,130
因此它将生成一个随机矩阵

52
00:02:58,130 --> 00:03:00,755
这个方法也适用与矢量化运算

53
00:03:00,755 --> 00:03:03,180
这种情况下d3将是一个矩阵

54
00:03:03,180 --> 00:03:06,660
因此任意一个训练样例及隐藏单元的组合

55
00:03:06,660 --> 00:03:10,245
其对应的d3中的元素都有0.8的几率取值为1

56
00:03:10,245 --> 00:03:12,815
0.2的几率取值为0

57
00:03:12,815 --> 00:03:20,900
这个表达式表示这个随机数有0.8的几率取值为1 或为真(True)

58
00:03:20,900 --> 00:03:24,675
20%或0.2的几率取值为非(False) 或0

59
00:03:24,675 --> 00:03:27,569
然后取层3的激活矩阵

60
00:03:27,569 --> 00:03:30,945
用a3来表示

61
00:03:30,945 --> 00:03:33,265
a3为刚才计算的激活矩阵

62
00:03:33,265 --> 00:03:37,335
它是用原来的a3与d3相乘得到的矩阵

63
00:03:37,335 --> 00:03:41,849
这里的相乘是逐元素相乘

64
00:03:41,849 --> 00:03:44,857
也可以写成a3*=d3

65
00:03:44,857 --> 00:03:50,625
这样做的作用是 对于d3中值为0的元素

66
00:03:50,625 --> 00:03:53,735
每个元素有20%的几率取值为0

67
00:03:53,735 --> 00:03:57,840
通过点乘将a3中0值对应位置的元素

68
00:03:57,840 --> 00:04:00,980
一一清零

69
00:04:00,980 --> 00:04:02,250
如果你用python实现

70
00:04:02,250 --> 00:04:05,880
技术上来说d3是一个值为True或False的布尔值数组

71
00:04:05,880 --> 00:04:06,985
而不是1或0

72
00:04:06,985 --> 00:04:10,057
但是用1和0表示True和False

73
00:04:10,057 --> 00:04:13,390
做点乘运算确实能达到效果

74
00:04:13,390 --> 00:04:16,260
你可以自己用python验证一下

75
00:04:16,260 --> 00:04:22,570
最后我们要放大a3

76
00:04:22,570 --> 00:04:30,015
将a3除以0.8 实际上是除以keep.prob参数

77
00:04:30,015 --> 00:04:32,560
我来解释以下最后一步

78
00:04:32,560 --> 00:04:36,040
方便起见 假设层3有50个单元

79
00:04:36,040 --> 00:04:39,930
或者说50个神经元

80
00:04:39,930 --> 00:04:43,075
所以a3的维数是50x1

81
00:04:43,075 --> 00:04:46,650
如果你做矢量化的运算 它的维数是50xm

82
00:04:46,650 --> 00:04:51,625
所以每个神经元有80%的几率被 20%的几率被丢弃

83
00:04:51,625 --> 00:04:53,050
这意味着平均起来

84
00:04:53,050 --> 00:04:59,025
将有10个单元失活或者被清零

85
00:04:59,025 --> 00:05:02,020
现在再看看z4的值

86
00:05:02,020 --> 00:05:08,775
z4=w4*a3+b4

87
00:05:08,775 --> 00:05:10,570
它的期望值

88
00:05:10,570 --> 00:05:14,080
将减少20%

89
00:05:14,080 --> 00:05:18,480
也就是说a3中20%的元素都被清零了

90
00:05:18,480 --> 00:05:22,240
为了不减少z4的期望值

91
00:05:22,240 --> 00:05:24,380
我们需要除以0.8

92
00:05:24,380 --> 00:05:28,870
因为

93
00:05:28,870 --> 00:05:33,635
它能提供你所需要的大约20%的校正值

94
00:05:33,635 --> 00:05:37,455
这样a3的期望值就不会被改变

95
00:05:37,455 --> 00:05:43,435
这就是所谓的反向随机失活技术(inverted dropout technique)

96
00:05:43,435 --> 00:05:44,830
它的作用在于

97
00:05:44,830 --> 00:05:47,230
你可以将keep.prob设为任意值

98
00:05:47,230 --> 00:05:50,446
0.8或0.9甚至1

99
00:05:50,446 --> 00:05:52,135
如果值为1那就没有丢弃 因为它保留了所有神经元

100
00:05:52,135 --> 00:05:54,565
这个值也可以是0.5或随便什么

101
00:05:54,565 --> 00:05:57,980
反向随机失活技术通过除以keep.prob

102
00:05:57,980 --> 00:06:02,730
确保a3的期望值不变

103
00:06:02,730 --> 00:06:05,005
而且你会发现在测试阶段

104
00:06:05,005 --> 00:06:06,820
也就是你要评估一个神经网络时

105
00:06:06,820 --> 00:06:08,300
--这个我们将在下一页幻灯片讨论

106
00:06:08,300 --> 00:06:10,065
反向随机失活技术

107
00:06:10,065 --> 00:06:13,160
就是这条线指向的随机失活正则化过程中这个绿色的框表示的部分

108
00:06:13,160 --> 00:06:17,540
它简化了神经网络的测试部分 因为它减少了可能引入的缩放问题

109
00:06:17,540 --> 00:06:20,110
到目前为止随机失活正则化最普遍的实现

110
00:06:20,110 --> 00:06:22,870
据我所知就是反向随机失活

111
00:06:22,870 --> 00:06:24,490
我建议你自己操作一下

112
00:06:24,490 --> 00:06:27,280
但是在一些早期的随机失活法的版本中

113
00:06:27,280 --> 00:06:30,165
并没有除以keep.prob这个操作

114
00:06:30,165 --> 00:06:33,660
所以在测试过程中求平均值变得越来越复杂

115
00:06:33,660 --> 00:06:37,040
但是人们已经不再使用那些版本了

116
00:06:37,040 --> 00:06:40,125
我们使用矢量d

117
00:06:40,125 --> 00:06:43,390
而且你会注意到 不同的训练样例的训练

118
00:06:43,390 --> 00:06:46,090
实际上对不同的隐藏单元实施了清零

119
00:06:46,090 --> 00:06:49,975
实际上 如果用同一个训练集进行迭代

120
00:06:49,975 --> 00:06:52,566
在不同的训练轮次中

121
00:06:52,566 --> 00:06:55,290
你应该随机地将不同的隐藏单元清零

122
00:06:55,290 --> 00:06:57,270
因此这并不意味着同一个训练样例的训练

123
00:06:57,270 --> 00:07:01,155
应该保证一直丢弃相同的隐藏单元

124
00:07:01,155 --> 00:07:03,258
--在梯度下降法的一次迭代中

125
00:07:03,258 --> 00:07:05,510
你把一些隐藏单元清零了

126
00:07:05,510 --> 00:07:07,375
在第二次迭代时

127
00:07:07,375 --> 00:07:09,595
也就是第二次遍历测试集的时候

128
00:07:09,595 --> 00:07:13,008
你可以用不同的模式给隐藏单元清零

129
00:07:13,008 --> 00:07:16,023
矢量d或者说层3对应的d3

130
00:07:16,023 --> 00:07:18,395
将决定哪些被清零

131
00:07:18,395 --> 00:07:21,565
--无论是在正向传播还是反向传播过程中

132
00:07:21,565 --> 00:07:22,980
我们在这里只展示了正向传播

133
00:07:22,980 --> 00:07:26,950
用这个算法完成训练后 我们来看看测试阶段的算法

134
00:07:26,950 --> 00:07:30,535
在测试阶段 你想对一些x做预测

135
00:07:30,535 --> 00:07:32,335
使用我们的标准表示法

136
00:07:32,335 --> 00:07:33,764
我用a0

137
00:07:33,764 --> 00:07:38,180
即层0的激活函数输出代表测试样例x

138
00:07:38,180 --> 00:07:40,760
我们要做的是 不在测试阶段使用随机失活算法

139
00:07:40,760 --> 00:07:44,340
具体来说

140
00:07:44,340 --> 00:07:48,314
Z^1= w^1.a^0 + b^1.

141
00:07:48,314 --> 00:07:56,627
a^1 = g^1(z^1 Z).

142
00:07:56,627 --> 00:08:03,745
Z^2 = w^2.a^1 + b^2.

143
00:08:03,745 --> 00:08:04,895
a^2 =...

144
00:08:04,895 --> 00:08:10,060
直到到达了最后一层并得到一个预测y^

145
00:08:10,060 --> 00:08:12,640
但请注意在测试阶段你并没有

146
00:08:12,640 --> 00:08:15,690
在哪里使用随机失活算法 没有抛硬币

147
00:08:15,690 --> 00:08:20,285
你不用抛硬币来决定哪些隐藏单元要被消除

148
00:08:20,285 --> 00:08:22,510
这是因为在测试阶段做预测的时候

149
00:08:22,510 --> 00:08:25,615
你并不想让你的输出也是随机的

150
00:08:25,615 --> 00:08:27,699
在测试阶段也使用随机失活算法

151
00:08:27,699 --> 00:08:29,890
只会为预测增加噪声

152
00:08:29,890 --> 00:08:34,105
理论上来说 可以做的一件事是

153
00:08:34,105 --> 00:08:38,940
用不同的随机失活的神经网络进行多次预测取并平均值

154
00:08:38,940 --> 00:08:43,625
但是这个方法运算效率不高而且会得到几乎相同的预测结果

155
00:08:43,625 --> 00:08:46,880
每次不同的预测过程将给出非常非常相似的结果

156
00:08:46,880 --> 00:08:47,980
刚才提到过

157
00:08:47,980 --> 00:08:49,385
反向随机失活

158
00:08:49,385 --> 00:08:53,455
记得上一页我们有做除以keep.prob的运算

159
00:08:53,455 --> 00:08:56,450
它的作用是保证如果测试过程

160
00:08:56,450 --> 00:08:59,664
没有针对随机失活算法进行缩放(scaling)

161
00:08:59,664 --> 00:09:02,050
那么激活函数的期望输出也不会改变

162
00:09:02,050 --> 00:09:06,540
所以不用在测试过程中加入额外的缩放参数

163
00:09:06,540 --> 00:09:08,965
这与训练过程不同

164
00:09:08,965 --> 00:09:10,240
这就是随机失活正则化算法

165
00:09:10,240 --> 00:09:13,000
你会在这周的编程作业中练习它的操作

166
00:09:13,000 --> 00:09:16,660
并获得更多的一手体验

167
00:09:16,660 --> 00:09:18,440
但是为什么它真的有效呢？

168
00:09:18,440 --> 00:09:20,410
下一个视频中我将更直接地解释

169
00:09:20,410 --> 00:09:23,630
随机失活的原理

170
00:09:23,630 --> 00:09:25,160
我们下一节再见