Neden düzenlileştirme (regularization) işlemi, aşırı öğrenme (overfitting) konusunda yardımcı olur? Varyans problemlerini azaltmada neden yardımcı olur? Nasıl çalıştığına dair bazı sezgiler kazanmak için birkaç örnek verelim. Yüksek yanlılık (high bias), yüksek varyansı hatırlayalım. Ve daha önceki videomdan böyle bir şeye benzeyen resimler çizdim. Şimdi, uygun (fitting) büyük ve derin bir sinir ağını görelim. Bunu çok büyük veya çok derin çizmediğimi biliyorum. bazı sinir ağlarını ve şu anda aşırı öğrenmeyi düşünmedikçe. W'nin J gibi bir maliyet fonksiyonuna sahipseniz B, kayıpların toplamına eşittir. Düzenlileştirme (regularization) için yapılan şey bu ekstra terimi eklemekti ki bu terim ağırlık matrisini çok büyük olmasından dolayı cezalandırır. Yani Frobenius normuydu. Öyleyse neden L iki normu veya Frobenius normunu daralır ya da parametreler daha az aşırı öğrenmeye (overfitting) neden olur? Sezginin bir parçası şudur ki : eğer siz lambda'nın gerçekten çok büyük olması için düzenli hale getirme işlemini (regularization) gerçekleştirirseniz, bu teşvik edici olacaktır Ağırlık matrisleri W'i makul olarak sıfıra yakın olacak şekilde ayarlamak için Yani sezginin bir parçası belki de ağırlık sıfıra yakın olacak şekilde ayarlanmış olabilir birçok gizli birim için Bu gizli birimlerin etkisini büyük ölçüde sıfırlayan Ve eğer durum buysa, o zaman bu kadar basitleştirilmiş sinir ağı çok daha küçük bir sinir ağına dönüşür. Aslında, neredeyse bir lojistik regresyon birimi (logistic regression unit) gibi, fakat büyük olasılıkla derin olacak şekilde yığılmış. Ve böylece bu sola çok yakın aşırı öğrenme durumundan (overfitting case) diğer yüksek önyargı durumuna (high bias case) götürecek. Ama umarım lambda'nın bir ara değeri olur, ortadaki bu doğru duruma daha yakın olacak bir sonuçla sonuçlanan. Ama sezgiye göre, lambda'nın gerçekten çok büyük olması, W'ye sıfıra yakın bir değere ayarlanacağıdır ki, bu pratikte gerçek olmamaktadır. Bunu, sıfırlama veya en azından gizli birimlerin etkisini azaltma olarak düşünebiliriz böylece daha basit bir ağ gibi hissedebileceğiniz şekilde sonuçlanır. Sadece lojistik regresyon kullanıyormuşsunuz gibi yaklaşırlar. Bir grup gizli birimin tamamen sıfırlanması sezgisi tam olarak doğru değil. Gerçekte olan şey şu ki, tüm gizli birimleri kullanılacak, ama her birinin daha küçük bir etkileri olacaktır. Ama daha basit bir ağ ile sonuçlandırılır ve sanki aşırı öğrenmeye (overfitting) daha az eğilimli daha küçük bir ağınız var gibi olur. Yani bu sezginin daha çok yardımcı olduğunu program alıştırmasını düzenlileştirdiğinizde, aslında bu varyans azaltma sonuçlarının bazılarından kendiniz görüyorsunuz. İşte ek sezgiye yönelik yeni bir yaklaşım düzenlileştirmenin aşırı öğrenmeyi neden engellediği konusunda . Ve bunun için, bu gibi görünen tanh aktivasyon fonksiyonunu kullandığımızı varsayacağım. z'nin g'si, z'nin tanh'ına eşittir. Yani bu durumda, Z oldukça küçük olduğu sürece, böylece Z parametreleri sadece ufacık bir aralık üzerinde değer alırsa, belki buralarda, o zaman sadece tanh fonksiyonunun lineer rejimini kullanıyor olursun. Z daha büyük değerlere veya daha küçük değerlere dolaşmaya izin veriyorsa, aktivasyon fonksiyonu daha az doğrusal olmaya başlar. Bundan kurtulabileceğiniz sezgi, eğer Düzenleme parametresi olan lambdanın büyük olmasıdır. o zaman parametreler nispeten küçük olur çünkü onlar bir cos fonksiyonuna büyük ceza olarak verilirler. Ve böylece W'ler küçüktür çünkü Z W'e eşittir ve daha sonra teknik olarak b ile toplanır ama eğer W çok küçük olma eğilimindeyse, o zaman Z nispeten küçük olacaktır. Ve özellikle, eğer Z nispeten küçük değerler alırsa, tüm bu aralıkta, Z'nin G'si doğrusal olacaktır. Yani her katman kabaca doğrusal olacaktır. Doğrusal regresyon gibi Ve derslerde gördük ki, eğer her katman doğrusal ise, tüm ağ sadece bir doğrusal ağdır. Ve böylece çok derin bir ağ, lineer aktivasyon fonksiyonuna sahip derin bir ağa sahip olduklarında, sonunda sadece doğrusal bir fonksiyon hesaplayabilirler. Yani bu çok fazla karmaşık kararlara uymuyor. Çok doğrusal olmayan karar sınırları (Very non-linear decision boundaries) gerçekten veri kümesinin aşırı öğrenilmesine izin verir bir önceki slayttaki aşırı yüksek varyans durumunda gördüğümüz gibi. Yani özetlemek gerekirse, Düzenlileştirme (regularization) çok büyük olursa, W parametreleri çok küçük dolayısıyla Z'de küçük olacaktır. şimdilik b'nin etkilerini görmezden gelirsek, Z'nin nispeten küçük veya gerçekten de, küçük bir değerler aralığında olduğunu söylemeliyim. Ve böylece aktivasyon işlevi tanh ise, dolayısıyla doğrusal olacağı söylenebilir. Ve böylece tüm sinir ağınızın hesaplayacağı şey; büyük bir doğrusal fonksiyondan çok uzak olmayan, ki o çok karmaşık doğrusal olmayan bir işlev yerine basit bir fonksiyondur. Ve ayrıca aşırı öğrenmenin çok daha az mümkün olduğu. Ve yine, program alıştırmasında kendiniz için düzenlileştirme uyguladığınızda, bu efektlerin bazılarını kendiniz görebileceksiniz. Düzenlileştirme konusundaki tartışmamızı tamamlamadan önce, size sadece bir uygulama ipucu vermek istiyorum. Bu ipucu, düzenlileştirmeyi uygularken maliyet fonksiyonu J tanımımızı aldık ve aslında değiştirdik ağırlığı fazla büyük olan bu ekstra terimi ekleyerek. Ve eğer bayır inişi (gradient descent) uygularsanız, bayır inişinde (gradient descent) hata ayıklama (debug) adımlarından biri, maliyet fonksiyonu J'nin bayır inişinin yükselti (elevation) sayısının bir fonksiyonu olarak çizmektir ve siz görmek istediğiniz şey Maliyet fonksiyonu J'nin, bayır inişinin her yükseltisinden sonra monoton olarak azalmasıdır. Ve eğer düzenlileştirmeyi (regularization) uyguluyorsanız, lütfen J'nin bu yeni tanıma sahip olduğunu unutmayın. Eğer J'nin eski tanımına göre çizerseniz, sadece bu ilk terim, o zaman monoton bir düşüş görmeyebilirsiniz. Bu nedenle bayır inişinde hata ayıklamak için emin olmalısınız gereken şey bu ikinci terimi de içeren J'nin bu yeni tanımını çiziyorsunuz Aksi halde, her bir yükseltide J'nin monoton olarak azaldığını göremeyebilirsiniz. Bu yüzden L için düzenlileştirme aslında benim derin öğrenme modüllerinin eğitiminde sık kullandığım bir düzenlileştirme tekniğidir. Derin öğrenmede, bazen başka bir düzenlileştirme tekniği de kullanılır bu seyreltme düzenlileştirmesi (dropout regularization) olarak isimlendirilir. Buna bir sonraki videoda bakalım.