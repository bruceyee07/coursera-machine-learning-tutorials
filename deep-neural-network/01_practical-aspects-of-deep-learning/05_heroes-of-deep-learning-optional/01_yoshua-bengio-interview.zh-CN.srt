1
00:00:03,182 --> 00:00:05,735
Yoshua 你好 很高兴<br />今天你能来这儿给我们讲讲你的故事

2
00:00:05,735 --> 00:00:06,602
我也很高兴

3
00:00:06,602 --> 00:00:11,892
现在 你不仅是深度学习的<br />研究人员或工程师

4
00:00:11,892 --> 00:00:16,458
你已经是科研机构的代表和<br />深度学习的标志性人物 不过

5
00:00:16,458 --> 00:00:19,660
我真的很想听听<br />你的故事是怎么开始的

6
00:00:19,660 --> 00:00:26,242
你怎么会走进深度学习<br />然后一直追寻这段旅程

7
00:00:26,242 --> 00:00:31,289
这么说吧 实际上<br />我小时候就开始了 青春期的时候

8
00:00:31,289 --> 00:00:35,912
我读了很多科幻小说<br />我猜我们当中大多数人都那样

9
00:00:35,912 --> 00:00:42,374
1985年我进入研究生阶段的学习时<br />开始阅读神经网络的论文

10
00:00:42,374 --> 00:00:48,040
我当时非常激动 真的有一种激情

11
00:00:48,040 --> 00:00:52,230
那么 在80年代中期 比如1985年<br />阅读这些论文

12
00:00:52,230 --> 00:00:54,492
是种什么感觉呢 你还记得吗

13
00:00:54,492 --> 00:00:55,545
— 是的

14
00:00:59,565 --> 00:01:05,277
嗯 是在我学过的课程中<br />关于使用专家系统的经典AI的

15
00:01:05,277 --> 00:01:09,981
我突然发现了 有一个研究人类的学习方式

16
00:01:09,981 --> 00:01:14,445
和人类智能的世界

17
00:01:14,445 --> 00:01:19,347
还有我们如何描绘它们与人工智能和

18
00:01:19,347 --> 00:01:21,100
计算机之间的联系

19
00:01:21,100 --> 00:01:25,230
当时我真的很激动<br />当我发现这个方向后

20
00:01:25,230 --> 00:01:27,730
我开始阅读 联结主义的文献

21
00:01:27,730 --> 00:01:31,682
Geoff Hinton的论文等等

22
00:01:31,682 --> 00:01:38,462
后面我研究了
循环网络 研究了语音识别

23
00:01:38,462 --> 00:01:42,666
研究了隐马模型(HMN) 图模型

24
00:01:42,666 --> 00:01:50,295
很快 我加入了AT&T贝尔实验室和MIT<br />我在那里做博士后

25
00:01:50,295 --> 00:01:54,629
在那儿我发现了一些训练神经网络时的

26
00:01:54,629 --> 00:01:57,335
长期依赖(long-term dependency)的问题

27
00:01:57,335 --> 00:02:02,505
不久 我回到了蒙特利尔<br />加入了蒙特利尔大学(UdeM)

28
00:02:02,505 --> 00:02:06,475
在那儿我度过了<br />大部分的青年岁月

29
00:02:08,260 --> 00:02:12,948
所以作为近几十年一直都在<br />见证了所有的人

30
00:02:12,948 --> 00:02:17,049
能不能给我讲讲你对

31
00:02:17,049 --> 00:02:21,610
深度学习的想法<br />讲讲神经网络的演进史

32
00:02:22,740 --> 00:02:25,590
我们从实验开始 从直观的感受开始

33
00:02:25,590 --> 00:02:27,030
然后才有的理论

34
00:02:27,030 --> 00:02:30,540
我们现在理解得深入多了 比如

35
00:02:30,540 --> 00:02:35,132
为什么反向传播(Back Prop)效果那么好<br />为什么深度如此重要

36
00:02:35,132 --> 00:02:41,904
对于这些概念 <br />那时候我们没有可靠的论证

37
00:02:41,904 --> 00:02:46,488
21世纪初 当我们开始研究深度网络时<br />我们直觉上认为

38
00:02:46,488 --> 00:02:50,680
更深的网络应该更强大 这点很有道理

39
00:02:50,680 --> 00:02:54,450
但是我们不知道怎么利用它 

40
00:02:54,450 --> 00:02:57,680
证明它<br />当然 我们的实验一开始没有效果

41
00:02:59,260 --> 00:02:59,910
实际上

42
00:02:59,910 --> 00:03:04,580
你觉得那些后来被证明是对的事情中<br />最重要的是哪些

43
00:03:04,580 --> 00:03:08,120
以及哪些后来证实的错误<br />最让你意外

44
00:03:08,120 --> 00:03:09,680
对比我们三十年前的认知

45
00:03:11,070 --> 00:03:15,880
当然 我犯的最大错误之一是

46
00:03:15,880 --> 00:03:18,686
和90年代的其他人观点一样

47
00:03:18,686 --> 00:03:24,980
你需要平滑的非线性<br />来让反向传播发挥作用

48
00:03:24,980 --> 00:03:31,330
因为我认为如果要做类似非线性校正

49
00:03:31,330 --> 00:03:35,010
会有一个平坦的区域 那么训练就变得很难

50
00:03:35,010 --> 00:03:38,090
因为在许多地方的导数都为0

51
00:03:38,090 --> 00:03:41,981
当我们2010年左右开始使用ReLU

52
00:03:41,981 --> 00:03:48,065
深度网络中试验的时候<br />我当时执迷于一个理念

53
00:03:48,065 --> 00:03:55,270
我们应该注意神经元<br />是否会零导数部分过饱和

54
00:03:55,270 --> 00:03:59,679
但是最后发现<br />事实上ReLU比

55
00:03:59,679 --> 00:04:03,751
逻辑曲线(sigmoid)的效果要好很多<br />这是个很大的惊喜

56
00:04:03,751 --> 00:04:07,969
我们探索尝试这个方法<br />实际是因为生物学上的联结

57
00:04:07,969 --> 00:04:11,105
而不是因为我们觉得它<br />优化起来比较容易

58
00:04:11,105 --> 00:04:16,300
但是最后证明它的效果更好<br />尽管我认为训练起来更难

59
00:04:16,300 --> 00:04:17,490
那么我想问下你

60
00:04:17,490 --> 00:04:20,890
深度学习和大脑之间有什么关系

61
00:04:20,890 --> 00:04:25,491
虽然已经有明确的答案了<br />但是我很想知道你会怎么回答

62
00:04:25,491 --> 00:04:31,093
哦 最初让我对神经网络兴奋的见解

63
00:04:31,093 --> 00:04:37,839
来自联结主义<br />它认为信息

64
00:04:37,839 --> 00:04:43,003
分布在很多神经元的激活中

65
00:04:43,003 --> 00:04:47,431
它不是通过某个祖母细胞来表示

66
00:04:47,431 --> 00:04:51,209
他们是这么叫的<br />这是种符号化的表示

67
00:04:51,209 --> 00:04:54,972
这是经典AI中的传统观点

68
00:04:54,972 --> 00:04:58,860
而且我仍然认为这真的是<br />非常重要的事情

69
00:04:58,860 --> 00:05:03,573
甚至最近我还看到人们重新挖掘了它的重要性

70
00:05:03,573 --> 00:05:06,850
所以这真是基础

71
00:05:06,850 --> 00:05:12,919
深度相关的东西是在后来出现的<br />大约在21世纪初

72
00:05:12,919 --> 00:05:16,563
但它不是我在90年代思考的东西

73
00:05:16,563 --> 00:05:21,318
对 我记得你为词向量(word embedding)<br />构建了很多相对浅的

74
00:05:21,318 --> 00:05:26,859
但极为分布式的表达 对的 非常早

75
00:05:26,859 --> 00:05:28,897
是的 是这样的 对

76
00:05:28,897 --> 00:05:33,661
这是我在90年代末感到非常兴奋的事情之一

77
00:05:33,661 --> 00:05:38,351
事实上我的兄弟Samy和我一直在探索一个理念<br />就是我们可以使用神经网络

78
00:05:38,351 --> 00:05:42,171
来处理维数灾难(curse of dimensionality)

79
00:05:42,171 --> 00:05:45,980
它被认为是统计学习的核心问题之一

80
00:05:45,980 --> 00:05:51,440
有了这些分布式表达 我们可以用它们来

81
00:05:51,440 --> 00:05:57,150
非常高效地表示很多随机变量的联合分布

82
00:05:57,150 --> 00:06:01,250
结果相当好 然后我把它扩展到了

83
00:06:01,250 --> 00:06:04,920
词序列的联合分布<br />这就是词向量的起源

84
00:06:04,920 --> 00:06:10,525
因为我认为它可以容许

85
00:06:10,525 --> 00:06:16,373
意义相近的词的泛化等等

86
00:06:16,373 --> 00:06:20,974
所以在过去二十年<br />你的研究团队创造了很多理念

87
00:06:20,974 --> 00:06:24,030
在短短的几分钟之内没人能总结完

88
00:06:24,030 --> 00:06:26,879
那么我有点好奇 在你的团队中

89
00:06:26,879 --> 00:06:29,810
最值得骄傲的发明或理念时什么

90
00:06:29,810 --> 00:06:35,640
嗯 我提到了长期依赖和相关的研究

91
00:06:35,640 --> 00:06:40,770
我觉得人们还没有充分理解它

92
00:06:40,770 --> 00:06:45,214
然后我提到了维数灾难

93
00:06:45,214 --> 00:06:49,887
用神经网络处理联合分布这些方面的故事

94
00:06:49,887 --> 00:06:52,435
Hugo Larochelle最近在做这方面的研究

95
00:06:52,435 --> 00:06:55,255
并且就像我刚才说的 它引出了各种

96
00:06:55,255 --> 00:06:59,660
词的联合分布相关的<br />学习词向量的研究

97
00:06:59,660 --> 00:07:04,410
接着 可能是我们在深度学习领域<br />最为人所知的成果

98
00:07:04,410 --> 00:07:07,639
栈式的自编码器以及栈式的RBM

99
00:07:09,580 --> 00:07:15,500
还有就是 深化了对一些问题的理解

100
00:07:15,500 --> 00:07:20,530
初始化的思路训练深度网络的难点

101
00:07:20,530 --> 00:07:24,840
以及深度网络中的梯度消失

102
00:07:24,840 --> 00:07:29,440
这项研究事实上<br />激发了一系列试验

103
00:07:29,440 --> 00:07:34,986
说明了分段线性激活函数的重要性

104
00:07:34,986 --> 00:07:38,804
然后在非监督学习的研究方面 我觉得

105
00:07:38,804 --> 00:07:43,706
我们做的最重要的成果<br />包括降噪自动编码器

106
00:07:43,706 --> 00:07:48,340
还有现在很流行的GAN 也就是<br />生成式对抗网络(Generative Adversarial Network)

107
00:07:48,340 --> 00:07:54,563
我们做的基于注意力的<br />神经网络机器翻译研究

108
00:07:54,563 --> 00:08:01,132
在后来的翻译相关领域非常重要

109
00:08:01,132 --> 00:08:05,540
现在它已经在业界使用<br />如谷歌翻译

110
00:08:05,540 --> 00:08:09,800
但是这个注意力机制<br />事实上真的改变了我对神经网络的看法

111
00:08:09,800 --> 00:08:14,860
以前我们认为神经网络是能把<br />一个向量映射到另一个向量的机器

112
00:08:14,860 --> 00:08:19,300
但是使用注意力机制后<br />你现在可以处理任何类型的数据结构

113
00:08:19,300 --> 00:08:24,565
这实际上开辟了<br />很多有趣的大道

114
00:08:24,565 --> 00:08:27,415
有一个方向 实际上是与生物学相关

115
00:08:27,415 --> 00:08:31,403
最近几年我一直在研究

116
00:08:31,403 --> 00:08:36,970
就是我们怎样去找到一些类似反向传播<br />但可以由大脑实现的东西

117
00:08:36,970 --> 00:08:41,500
我们在这方面有一些论文

118
00:08:41,500 --> 00:08:43,930
神经科学学者可能会感兴趣

119
00:08:43,930 --> 00:08:46,240
当然 我们也在继续这个方向的研究

120
00:08:47,890 --> 00:08:50,785
有个课题我知道你一直很关心

121
00:08:50,785 --> 00:08:52,990
就是深度学习和大脑之间的关系

122
00:08:52,990 --> 00:08:56,720
你能跟我们谈一谈这方面的东西吗

123
00:08:56,720 --> 00:09:03,030
生物学的东西实际上是我一直在想的

124
00:09:03,030 --> 00:09:08,110
可以说我经常做这方面的白日梦

125
00:09:08,110 --> 00:09:12,897
因为我把它想成一个拼图

126
00:09:12,897 --> 00:09:16,957
我们有一些佐证<br />来自于对大脑和大脑学习过程的了解

127
00:09:16,957 --> 00:09:21,042
像是脉冲时间相关的突触可塑性<br />(Spike Timing Dependent Plasticity)

128
00:09:21,042 --> 00:09:27,490
另一方面 我们有从机器学习<br />得来的所有概念

129
00:09:27,490 --> 00:09:31,822
依照目标函数从全局训练整个系统的理念

130
00:09:31,822 --> 00:09:35,440
以及反向传播的理念

131
00:09:35,440 --> 00:09:37,370
那么反向传播有什么含义呢

132
00:09:37,370 --> 00:09:42,710
就像 信用分配(credit assignment)指什么

133
00:09:42,710 --> 00:09:47,515
当我开始思考大脑<br />如何做类似反向传播的事情时

134
00:09:47,515 --> 00:09:53,070
我又想起或许可能<br />在反向传播背后还有更一般的概念

135
00:09:53,070 --> 00:09:58,271
来保证它的高效率<br />让我们用反向传播变得高效

136
00:09:58,271 --> 00:10:02,217
在信用分配方面可能<br />有更多同类的方法

137
00:10:02,217 --> 00:10:06,973
这与强化学习的研究人员<br />一直探讨的问题有点联系

138
00:10:06,973 --> 00:10:12,161
有一点很有趣<br />有时候问一个简单的问题

139
00:10:12,161 --> 00:10:18,366
会让你思考很多不同的东西<br />这迫使你思考

140
00:10:18,366 --> 00:10:23,387
很多你想放到一起的东西<br />就像一个巨大的拼图

141
00:10:23,387 --> 00:10:26,510
这种情况持续了几年

142
00:10:26,510 --> 00:10:30,538
我得说整个这项工作

143
00:10:30,538 --> 00:10:34,714
就如同我已经做的很多工作一样<br />很大程度上受到了Jeff Hinton的想法的启发

144
00:10:34,714 --> 00:10:41,171
具体来说 2007年他举办了一个讲座

145
00:10:41,171 --> 00:10:46,013
第一个深度学习的研讨会

146
00:10:46,013 --> 00:10:51,270
在研讨会上<br />他讲了他对大脑工作原理的想法

147
00:10:52,840 --> 00:10:56,515
怎么使用时间编码

148
00:10:56,515 --> 00:11:00,719
有可能完成反向传播

149
00:11:00,719 --> 00:11:06,700
这激发了最近几年<br />我在这方面探索过的很多想法

150
00:11:07,850 --> 00:11:12,700
对 这是个有趣的故事

151
00:11:13,830 --> 00:11:16,090
到现在已经大概有十年了

152
00:11:17,100 --> 00:11:21,030
我听你也提过多次的话题当中 有一个是

153
00:11:21,030 --> 00:11:23,090
无监督学习

154
00:11:23,090 --> 00:11:25,000
你能跟我们分享下你的观点吗?

155
00:11:26,010 --> 00:11:29,880
好的 无监督学习真的很重要

156
00:11:29,880 --> 00:11:34,744
现在 我们的产业系统是基于有监督学习

157
00:11:34,744 --> 00:11:40,259
它本质上需要人类<br />定义与问题有关的

158
00:11:40,259 --> 00:11:43,845
重要概念<br />并在数据中标注出这些概念

159
00:11:43,845 --> 00:11:49,740
我们使用它构建出了<br />这些令人惊叹的玩具和服务

160
00:11:49,740 --> 00:11:52,397
但人类能够做更多的事情

161
00:11:52,397 --> 00:11:57,869
他们能够通过观察和与世界的交互探索

162
00:11:57,869 --> 00:12:00,360
和发现新的概念

163
00:12:00,360 --> 00:12:05,362
两岁的小孩能够<br />理解直觉物理学

164
00:12:05,362 --> 00:12:08,490
换句话说<br />她能理解重力 理解压力

165
00:12:08,490 --> 00:12:11,990
理解惯性

166
00:12:11,990 --> 00:12:14,480
她能理解液体 固体

167
00:12:14,480 --> 00:12:18,140
当然 她的父母从来没有教<br />她这些东西 对吧

168
00:12:18,140 --> 00:12:21,110
那么她是怎么想出来的呢

169
00:12:21,110 --> 00:12:26,160
这是无监督学习试图回答的问题

170
00:12:26,160 --> 00:12:30,460
它不是说我们有标签<br />或是说我们没有标签

171
00:12:30,460 --> 00:12:33,790
实际上它是说构建一个

172
00:12:33,790 --> 00:12:38,630
心灵的建构 通过观察来解释世界的工作方式

173
00:12:38,630 --> 00:12:42,430
最近 我一直努力

174
00:12:42,430 --> 00:12:45,810
把无监督学习的理念与<br />增强学习的理念结合起来

175
00:12:45,810 --> 00:12:50,430
因为我相信<br />很可能存在一些

176
00:12:50,430 --> 00:12:54,899
潜藏的重要概念 我们正试图理清

177
00:12:54,899 --> 00:12:57,020
或者说努力区分

178
00:12:58,150 --> 00:13:03,166
人类或机器能够通过与世界互动

179
00:13:03,166 --> 00:13:08,969
通过探索世界和尝试事物 或控制事物<br />而得到的东西

180
00:13:08,969 --> 00:13:13,598
我觉得这些都与非监督学习

181
00:13:13,598 --> 00:13:14,354
最初的理念紧密关联

182
00:13:14,354 --> 00:13:17,082
所以我研究无监督学习

183
00:13:17,082 --> 00:13:22,027
15年前当我们开始<br />做这些以及RBM等等时

184
00:13:22,027 --> 00:13:26,819
我们聚焦在<br />学习良好的表示方法上

185
00:13:26,819 --> 00:13:29,350
现在我还是认为<br />这是个核心问题

186
00:13:29,350 --> 00:13:34,370
但是我们不知道是<br />良好的表示法是什么 要怎么获取

187
00:13:34,370 --> 00:13:39,569
例如 我们怎么表示目标函数?

188
00:13:39,569 --> 00:13:41,945
多年来我们尝试了很多东西

189
00:13:41,945 --> 00:13:46,262
实际上这是无监督学习研究中<br />很酷的方向之一

190
00:13:46,262 --> 00:13:48,449
也有很多不同的观点

191
00:13:48,449 --> 00:13:51,079
不同的途径 来处理这个问题

192
00:13:51,079 --> 00:13:56,482
也可能明年的发现就完全不同了

193
00:13:56,482 --> 00:14:01,066
也许大脑使用的机制<br />与之前的研究完全不同

194
00:14:01,066 --> 00:14:03,197
所以它不是渐进的研究

195
00:14:03,197 --> 00:14:06,300
它本身就是<br />非常值得探索的东西

196
00:14:07,500 --> 00:14:11,150
在什么是正确的目标函数方面<br />我们没有良好的定义

197
00:14:11,150 --> 00:14:14,446
就更不用说度量某个系统<br />在无监督学习上的效果好不好

198
00:14:14,446 --> 00:14:19,710
当然它非常有挑战 但同时

199
00:14:19,710 --> 00:14:23,140
它也留下了广泛的可能性

200
00:14:23,140 --> 00:14:26,980
这是研究人员非常热爱的东西<br />至少这是我非常喜欢的东西

201
00:14:28,600 --> 00:14:31,536
现在有很多深度学习方面的研究

202
00:14:31,536 --> 00:14:34,175
而且我觉得我们已经过了

203
00:14:34,175 --> 00:14:37,410
有人有能力阅读所有发表的<br />深度学习论文的时代

204
00:14:38,590 --> 00:14:44,397
那么 我有点好奇 <br />在现在的深度学习中什么让你最兴奋

205
00:14:44,397 --> 00:14:49,059
我的目标非常高 我觉得

206
00:14:49,059 --> 00:14:54,780
深度学习科学的当前状态<br />离我的期望相当远

207
00:14:54,780 --> 00:15:01,060
我的印象是我们的系统<br />现在犯的那些错误类型

208
00:15:01,060 --> 00:15:05,480
说明它们对<br />世界的了解相当肤浅

209
00:15:06,510 --> 00:15:11,504
现在最让我兴奋的是<br />这样的研究方向

210
00:15:11,504 --> 00:15:15,527
不再致力于做一些<br />所谓有用的系统

211
00:15:15,527 --> 00:15:21,494
而是回到原理<br />有关计算机如何观察世界

212
00:15:21,494 --> 00:15:26,030
与世界互动<br />并发现世界的运行方式

213
00:15:26,030 --> 00:15:30,348
即使那个世界相当简单<br />简单到我们可以把它

214
00:15:30,348 --> 00:15:32,718
写成一个游戏<br />但我们不知道怎么才能做好

215
00:15:32,718 --> 00:15:36,655
而且那很酷 因为我不需要<br />和谷歌 Facebook

216
00:15:36,655 --> 00:15:38,400
百度等公司竞争 对吧

217
00:15:38,400 --> 00:15:41,300
因为这是种基础研究

218
00:15:41,300 --> 00:15:45,640
任何人都能在他们的车库完成<br />但可以改变世界

219
00:15:45,640 --> 00:15:50,130
当然有很多方向可以解决这个问题

220
00:15:50,130 --> 00:15:54,509
但是我看到在深度学习和强化学习<br />之间有很多富有成效的互动

221
00:15:54,509 --> 00:15:59,311
它们相当重要

222
00:15:59,311 --> 00:16:03,076
我其实很激动<br />因为这方面的进步

223
00:16:03,076 --> 00:16:06,940
实际上会对实际应用<br />产生巨大的影响

224
00:16:06,940 --> 00:16:11,774
因为如果你观察我们现在碰到的<br />一些巨大挑战

225
00:16:11,774 --> 00:16:14,044
比如我们如何处理样本非常少的

226
00:16:14,044 --> 00:16:16,921
新的领域或分类

227
00:16:16,921 --> 00:16:21,100
以及人类非常擅长<br />解决的问题那些地方

228
00:16:21,100 --> 00:16:25,336
所以有这些迁移学习<br />和转化的问题

229
00:16:25,336 --> 00:16:30,201
如果我们的系统能够更好地理解<br />世界是怎么运转的

230
00:16:30,201 --> 00:16:33,821
那么处理这些问题会容易得多

231
00:16:33,821 --> 00:16:35,280
或者说更深地理解 对吧

232
00:16:35,280 --> 00:16:36,215
实际正在发生什么

233
00:16:36,215 --> 00:16:40,218
我看到的东西背后有什么原因

234
00:16:40,218 --> 00:16:44,170
我怎么通过行动来影响<br />我看到的东西

235
00:16:44,170 --> 00:16:50,542
那么这些是我最近<br />非常感兴趣的问题类型

236
00:16:50,542 --> 00:16:56,029
我认为联结 还有深度学习研究演化了几十年

237
00:16:56,029 --> 00:17:01,060
已经触及了人工智能中更古老的问题

238
00:17:01,060 --> 00:17:07,760
因为深度学习的很多成功<br />是与感知有关的

239
00:17:07,760 --> 00:17:08,917
所以还剩什么呢

240
00:17:08,917 --> 00:17:11,305
剩下的是某种高层级的条件

241
00:17:11,305 --> 00:17:14,890
在抽象的层级上<br />理解事物的工作原理

242
00:17:14,890 --> 00:17:19,093
所以我们计划去理解高层级的抽象

243
00:17:19,093 --> 00:17:23,109
我觉得我们还没达到那种高级抽象<br />我们必须要达到那个水平

244
00:17:23,109 --> 00:17:28,796
我们必须思考推理<br />思考信息的顺序处理

245
00:17:28,796 --> 00:17:31,087
必须思考因果关系<br />如何发挥作用

246
00:17:31,087 --> 00:17:34,540
以及机器如何自己做到<br />所有这些东西

247
00:17:34,540 --> 00:17:39,555
可能需要人工来引导<br />但是尽可能自动地做

248
00:17:39,555 --> 00:17:42,395
你刚刚说的话听起来就像

249
00:17:42,395 --> 00:17:46,160
你对你现在试验的研究方法<br />非常着迷

250
00:17:46,160 --> 00:17:49,730
我打算使用玩具问题这个词<br />但没有贬低的意思

251
00:17:49,730 --> 00:17:51,354
(没事)<br />是指这个小问题

252
00:17:51,354 --> 00:17:55,670
你很乐观<br />觉得它后面可以转化到更大的问题上

253
00:17:55,670 --> 00:18:00,634
对 对 我觉得它会这么转化

254
00:18:00,634 --> 00:18:05,223
当然我们必须要开展一些研究<br />让它扩大化

255
00:18:05,223 --> 00:18:08,170
并解决那些问题

256
00:18:08,170 --> 00:18:11,295
但是我研究这些玩具问题的最大动力是

257
00:18:11,295 --> 00:18:17,233
我们可以<br />更好的理解我们的失败

258
00:18:17,233 --> 00:18:22,233
而且我们可以把问题化简为<br />我们可以直观地操作

259
00:18:22,233 --> 00:18:26,528
并且更容易理解

260
00:18:26,528 --> 00:18:31,031
所以这有点像经典的<br />分治法(divide and conquer)

261
00:18:31,031 --> 00:18:35,591
而且 大家没有充分考虑的一点是

262
00:18:35,591 --> 00:18:38,750
研究周期可以更快 对吗

263
00:18:38,750 --> 00:18:44,225
如果我可以在几个小时内完成实验<br />我能够进步更快

264
00:18:44,225 --> 00:18:49,448
如果我不得不测试巨大的模型<br />来掌握所有的常识

265
00:18:49,448 --> 00:18:55,511
和各方面的知识<br />虽然最终我们能做到

266
00:18:55,511 --> 00:18:59,010
只是使用当前的硬件<br />每个实验都要很长的时间

267
00:18:59,010 --> 00:19:02,984
所以当我们的硬件朋友<br />在创造比现在快上千倍

268
00:19:02,984 --> 00:19:06,050
甚至上百万倍的机器时<br />我在做那些玩具实验

269
00:19:06,050 --> 00:19:11,094
你知道 我还听你讲过

270
00:19:11,094 --> 00:19:15,904
深度学习科学<br />而不仅仅是工程原理

271
00:19:15,904 --> 00:19:19,610
但需要做更多的研究来了解<br />实际发生了什么

272
00:19:19,610 --> 00:19:22,185
你能跟我们分享下这方面的想法吗

273
00:19:22,185 --> 00:19:24,287
—不用谢

274
00:19:24,287 --> 00:19:29,105
我担心我们现在做的很多工作<br />有点像盲人

275
00:19:29,105 --> 00:19:30,278
在努力地找路

276
00:19:30,278 --> 00:19:37,247
如果你运气特别好<br />可以用这种方式找到有趣的东西

277
00:19:37,247 --> 00:19:40,487
但是真的如果我们<br />稍微停一停

278
00:19:40,487 --> 00:19:45,619
设法用可以转化的方式<br />理解我们现在做的东西

279
00:19:45,619 --> 00:19:49,220
因为我们已经从<br />原理深入到了理论

280
00:19:49,220 --> 00:19:53,378
但是当我说理论的时候<br />我不一定指数学

281
00:19:53,378 --> 00:19:57,733
当然我喜欢数学等等<br />但我觉得我们不需要把

282
00:19:57,733 --> 00:20:01,221
所有的东西用数学形式化<br />但需要从逻辑上形式化

283
00:20:01,221 --> 00:20:05,567
从这个角度讲<br />我能让人相信 这样应该有用

284
00:20:05,567 --> 00:20:07,348
这样是否有意义

285
00:20:07,348 --> 00:20:09,550
这是最重要的一方面

286
00:20:09,550 --> 00:20:14,650
然后数学可以让我们<br />把这点变得越来越牢固

287
00:20:14,650 --> 00:20:17,330
但实际上它更在于理解

288
00:20:17,330 --> 00:20:21,145
它还涉及做研究的态度

289
00:20:21,145 --> 00:20:25,396
不是为了成为下一个基线或者基准

290
00:20:25,396 --> 00:20:30,850
或是打败其他实验室<br />或其他公司的其他人

291
00:20:30,850 --> 00:20:35,148
更多的是应该问哪种问题<br />这种问题是否能

292
00:20:35,148 --> 00:20:38,200
让我们更好的理解<br />研究领域中的现象

293
00:20:38,200 --> 00:20:40,330
比如 是什么使得

294
00:20:40,330 --> 00:20:45,247
训练更深的或当前的神经网络更加困难

295
00:20:45,247 --> 00:20:48,110
我们有一些思路<br />但还有很多我们不了解

296
00:20:49,310 --> 00:20:54,624
所以我们或许可以设计一些试验<br />目标不是得到更好的算法

297
00:20:54,624 --> 00:20:58,987
只是更好的理解<br />我们已经有的算法 或者

298
00:20:58,987 --> 00:21:03,857
在什么情况下<br />特定的算法效果更好 原因是什么

299
00:21:03,857 --> 00:21:05,346
原因很重要

300
00:21:05,346 --> 00:21:06,595
科学就是探索原因的

301
00:21:06,595 --> 00:21:07,669
就是原因

302
00:21:07,669 --> 00:21:09,826
好的<br />现在有很多人希望

303
00:21:09,826 --> 00:21:10,764
进入这个领域

304
00:21:10,764 --> 00:21:14,496
我相信你在一对一的谈话中<br />多次回答过这个问题 但是

305
00:21:14,496 --> 00:21:18,288
现在有很多人在观看视频<br />你有哪些建议要

306
00:21:18,288 --> 00:21:21,238
送给想进入人工智能<br />进入深度学习的人呢

307
00:21:21,238 --> 00:21:26,160
好的 首先<br />大家的动机各有不同

308
00:21:26,160 --> 00:21:28,537
可以做的事情也各有不同

309
00:21:28,537 --> 00:21:33,064
做深度学习的<br />研究人员需要的东西可能与

310
00:21:33,064 --> 00:21:37,333
使用深度学习构建产品的工程师<br />需要的东西不一样

311
00:21:37,333 --> 00:21:40,844
这两种情况下<br />要求的理解程度不同

312
00:21:40,844 --> 00:21:46,090
但是不管怎么样<br />两种情况都需要练习

313
00:21:46,090 --> 00:21:51,004
为了真正掌握一个<br />像深度学习那样的学科

314
00:21:51,004 --> 00:21:54,166
当然你需要阅读很多东西

315
00:21:54,166 --> 00:21:56,899
你必须自己动手<br />练习编写很多东西的程序

316
00:21:58,450 --> 00:22:02,516
面试的时候<br />我经常碰到一些使用软件的学生

317
00:22:02,516 --> 00:22:06,788
现在有很多很好的软件<br />你只需要接入 使用

318
00:22:06,788 --> 00:22:09,690
但完全不用理解其中的原理

319
00:22:09,690 --> 00:22:12,890
如果停在表面这种层级上<br />那么很难

320
00:22:12,890 --> 00:22:16,027
得出它什么时候会出问题<br />哪儿会出问题

321
00:22:16,027 --> 00:22:19,880
所以实际上你要设法自己实现这些功能<br />尽管可能效率不高

322
00:22:19,880 --> 00:22:24,366
但是仅仅是为了确保你真正理解背后的东西

323
00:22:24,366 --> 00:22:26,972
这点非常有用 自己多尝试

324
00:22:26,972 --> 00:22:29,886
也就是说不要只用那些编程框架<br />让你可以用几行代码

325
00:22:29,886 --> 00:22:33,432
就完成所有功能<br />但你实际不知道底层原理

326
00:22:33,432 --> 00:22:37,480
就是这样<br />我想说我们应该更进一步

327
00:22:37,480 --> 00:22:42,911
如果可以的话<br />设法自己从基本原理中推出这些东西

328
00:22:42,911 --> 00:22:44,597
这真的很有帮助

329
00:22:44,597 --> 00:22:48,275
但是通常情况下 你必须阅读

330
00:22:48,275 --> 00:22:52,110
看其他人的代码 写自己的代码

331
00:22:52,110 --> 00:22:57,066
做很多试验<br />确保你理解你做的所有事情

332
00:22:57,066 --> 00:23:00,621
特别对于科学来说 这是其中的一部分

333
00:23:00,621 --> 00:23:05,810
问问自己为什么我要做这些<br />为什么其他人在做这些

334
00:23:05,810 --> 00:23:10,470
可能答案就在书本的某一页<br />你必须读更多的书

335
00:23:11,490 --> 00:23:14,340
如果你实际上能自己想出来<br />那就更好

336
00:23:15,580 --> 00:23:16,992
对 这样很酷

337
00:23:16,992 --> 00:23:21,547
事实上 在我读过的文章中<br />你 Ian Goodfellow

338
00:23:21,547 --> 00:23:25,207
Aaron Courville<br />写了一本评价非常高的书

339
00:23:25,207 --> 00:23:27,240
泰勒：谢谢，谢谢。

340
00:23:27,240 --> 00:23:28,607
对 它卖得很好

341
00:23:28,607 --> 00:23:30,206
有点疯狂

342
00:23:30,206 --> 00:23:35,027
我觉得现在读这本书的人可能

343
00:23:35,027 --> 00:23:36,816
比能理解这本书的人多

344
00:23:36,816 --> 00:23:40,188
不过ICLRI大会汇刊

345
00:23:40,188 --> 00:23:44,968
收集的好论文可能最多

346
00:23:44,968 --> 00:23:49,145
当然NIPS和ICML 还有其他的会议<br />也有很好的论文

347
00:23:49,145 --> 00:23:54,345
但是如果你真的想接触很多好论文<br />只要阅读最近几期的

348
00:23:54,345 --> 00:23:59,648
ICLR汇刊<br />这能让你真正地看清这个领域

349
00:23:59,648 --> 00:24:01,454
对

350
00:24:01,454 --> 00:24:02,940
还有其他想法吗？

351
00:24:02,940 --> 00:24:09,337
大家想问问你<br />怎么才能成为深度学习的高手

352
00:24:09,337 --> 00:24:14,949
当然<br />这取决于你从哪儿开始

353
00:24:14,949 --> 00:24:17,590
不要害怕数学

354
00:24:17,590 --> 00:24:22,557
只要发展直觉<br />然后只要你从直觉上把握了

355
00:24:22,557 --> 00:24:27,870
事物背后的原理<br />数学就会变得相当容易

356
00:24:27,870 --> 00:24:32,218
还有个好消息是你不需要<br />5年的博士学习

357
00:24:32,218 --> 00:24:34,300
来成为深度学习的高手

358
00:24:34,300 --> 00:24:35,637
实际上如果你有很好的<br />计算机科学和数学基础

359
00:24:35,637 --> 00:24:40,040
只要学习短短几个月

360
00:24:40,040 --> 00:24:44,742
你就可以用好它 构建出东西

361
00:24:44,742 --> 00:24:48,962
并且开始做研究试验

362
00:24:48,962 --> 00:24:53,312
如果接受过良好的训练<br />大概只要六个月

363
00:24:53,312 --> 00:24:56,224
可能他们一点都不了解<br />机器学习 但是

364
00:24:56,224 --> 00:24:59,427
如果他们擅长数学和<br />计算机科学 这个过程会很快

365
00:24:59,427 --> 00:25:02,722
当然 这意味着在<br />数学和计算机科学方面

366
00:25:02,722 --> 00:25:03,640
你需要有良好的训练

367
00:25:03,640 --> 00:25:08,920
有时候 你在计算机科学课程中<br />学到的还不够

368
00:25:08,920 --> 00:25:13,928
特别是 需要补充一些数学

369
00:25:13,928 --> 00:25:20,309
比如概率论 代数和优化

370
00:25:20,309 --> 00:25:22,313
我明白了 还有微积分

371
00:25:22,313 --> 00:25:24,037
对 微积分

372
00:25:24,037 --> 00:25:28,809
Joshua 非常感谢你<br />分享这些观点 见解和指导

373
00:25:28,809 --> 00:25:32,561
虽然我们认识很长时间了<br />但直到今天

374
00:25:32,561 --> 00:25:35,084
我才了解你早年这些故事的细节<br />所以 谢谢你

375
00:25:35,084 --> 00:25:39,880
Andrew 谢谢你做了

376
00:25:39,880 --> 00:25:44,819
这次特别的视频<br />你现在做的事情很有意义

377
00:25:44,819 --> 00:25:47,150
我希望会有很多人能用上它们