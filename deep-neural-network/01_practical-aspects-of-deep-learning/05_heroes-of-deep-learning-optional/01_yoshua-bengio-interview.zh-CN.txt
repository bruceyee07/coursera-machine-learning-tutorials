Yoshua 你好 很高兴<br />今天你能来这儿给我们讲讲你的故事 我也很高兴 现在 你不仅是深度学习的<br />研究人员或工程师 你已经是科研机构的代表和<br />深度学习的标志性人物 不过 我真的很想听听<br />你的故事是怎么开始的 你怎么会走进深度学习<br />然后一直追寻这段旅程 这么说吧 实际上<br />我小时候就开始了 青春期的时候 我读了很多科幻小说<br />我猜我们当中大多数人都那样 1985年我进入研究生阶段的学习时<br />开始阅读神经网络的论文 我当时非常激动 真的有一种激情 那么 在80年代中期 比如1985年<br />阅读这些论文 是种什么感觉呢 你还记得吗 — 是的 嗯 是在我学过的课程中<br />关于使用专家系统的经典AI的 我突然发现了 有一个研究人类的学习方式 和人类智能的世界 还有我们如何描绘它们与人工智能和 计算机之间的联系 当时我真的很激动<br />当我发现这个方向后 我开始阅读 联结主义的文献 Geoff Hinton的论文等等 后面我研究了
循环网络 研究了语音识别 研究了隐马模型(HMN) 图模型 很快 我加入了AT&T贝尔实验室和MIT<br />我在那里做博士后 在那儿我发现了一些训练神经网络时的 长期依赖(long-term dependency)的问题 不久 我回到了蒙特利尔<br />加入了蒙特利尔大学(UdeM) 在那儿我度过了<br />大部分的青年岁月 所以作为近几十年一直都在<br />见证了所有的人 能不能给我讲讲你对 深度学习的想法<br />讲讲神经网络的演进史 我们从实验开始 从直观的感受开始 然后才有的理论 我们现在理解得深入多了 比如 为什么反向传播(Back Prop)效果那么好<br />为什么深度如此重要 对于这些概念 <br />那时候我们没有可靠的论证 21世纪初 当我们开始研究深度网络时<br />我们直觉上认为 更深的网络应该更强大 这点很有道理 但是我们不知道怎么利用它  证明它<br />当然 我们的实验一开始没有效果 实际上 你觉得那些后来被证明是对的事情中<br />最重要的是哪些 以及哪些后来证实的错误<br />最让你意外 对比我们三十年前的认知 当然 我犯的最大错误之一是 和90年代的其他人观点一样 你需要平滑的非线性<br />来让反向传播发挥作用 因为我认为如果要做类似非线性校正 会有一个平坦的区域 那么训练就变得很难 因为在许多地方的导数都为0 当我们2010年左右开始使用ReLU 深度网络中试验的时候<br />我当时执迷于一个理念 我们应该注意神经元<br />是否会零导数部分过饱和 但是最后发现<br />事实上ReLU比 逻辑曲线(sigmoid)的效果要好很多<br />这是个很大的惊喜 我们探索尝试这个方法<br />实际是因为生物学上的联结 而不是因为我们觉得它<br />优化起来比较容易 但是最后证明它的效果更好<br />尽管我认为训练起来更难 那么我想问下你 深度学习和大脑之间有什么关系 虽然已经有明确的答案了<br />但是我很想知道你会怎么回答 哦 最初让我对神经网络兴奋的见解 来自联结主义<br />它认为信息 分布在很多神经元的激活中 它不是通过某个祖母细胞来表示 他们是这么叫的<br />这是种符号化的表示 这是经典AI中的传统观点 而且我仍然认为这真的是<br />非常重要的事情 甚至最近我还看到人们重新挖掘了它的重要性 所以这真是基础 深度相关的东西是在后来出现的<br />大约在21世纪初 但它不是我在90年代思考的东西 对 我记得你为词向量(word embedding)<br />构建了很多相对浅的 但极为分布式的表达 对的 非常早 是的 是这样的 对 这是我在90年代末感到非常兴奋的事情之一 事实上我的兄弟Samy和我一直在探索一个理念<br />就是我们可以使用神经网络 来处理维数灾难(curse of dimensionality) 它被认为是统计学习的核心问题之一 有了这些分布式表达 我们可以用它们来 非常高效地表示很多随机变量的联合分布 结果相当好 然后我把它扩展到了 词序列的联合分布<br />这就是词向量的起源 因为我认为它可以容许 意义相近的词的泛化等等 所以在过去二十年<br />你的研究团队创造了很多理念 在短短的几分钟之内没人能总结完 那么我有点好奇 在你的团队中 最值得骄傲的发明或理念时什么 嗯 我提到了长期依赖和相关的研究 我觉得人们还没有充分理解它 然后我提到了维数灾难 用神经网络处理联合分布这些方面的故事 Hugo Larochelle最近在做这方面的研究 并且就像我刚才说的 它引出了各种 词的联合分布相关的<br />学习词向量的研究 接着 可能是我们在深度学习领域<br />最为人所知的成果 栈式的自编码器以及栈式的RBM 还有就是 深化了对一些问题的理解 初始化的思路训练深度网络的难点 以及深度网络中的梯度消失 这项研究事实上<br />激发了一系列试验 说明了分段线性激活函数的重要性 然后在非监督学习的研究方面 我觉得 我们做的最重要的成果<br />包括降噪自动编码器 还有现在很流行的GAN 也就是<br />生成式对抗网络(Generative Adversarial Network) 我们做的基于注意力的<br />神经网络机器翻译研究 在后来的翻译相关领域非常重要 现在它已经在业界使用<br />如谷歌翻译 但是这个注意力机制<br />事实上真的改变了我对神经网络的看法 以前我们认为神经网络是能把<br />一个向量映射到另一个向量的机器 但是使用注意力机制后<br />你现在可以处理任何类型的数据结构 这实际上开辟了<br />很多有趣的大道 有一个方向 实际上是与生物学相关 最近几年我一直在研究 就是我们怎样去找到一些类似反向传播<br />但可以由大脑实现的东西 我们在这方面有一些论文 神经科学学者可能会感兴趣 当然 我们也在继续这个方向的研究 有个课题我知道你一直很关心 就是深度学习和大脑之间的关系 你能跟我们谈一谈这方面的东西吗 生物学的东西实际上是我一直在想的 可以说我经常做这方面的白日梦 因为我把它想成一个拼图 我们有一些佐证<br />来自于对大脑和大脑学习过程的了解 像是脉冲时间相关的突触可塑性<br />(Spike Timing Dependent Plasticity) 另一方面 我们有从机器学习<br />得来的所有概念 依照目标函数从全局训练整个系统的理念 以及反向传播的理念 那么反向传播有什么含义呢 就像 信用分配(credit assignment)指什么 当我开始思考大脑<br />如何做类似反向传播的事情时 我又想起或许可能<br />在反向传播背后还有更一般的概念 来保证它的高效率<br />让我们用反向传播变得高效 在信用分配方面可能<br />有更多同类的方法 这与强化学习的研究人员<br />一直探讨的问题有点联系 有一点很有趣<br />有时候问一个简单的问题 会让你思考很多不同的东西<br />这迫使你思考 很多你想放到一起的东西<br />就像一个巨大的拼图 这种情况持续了几年 我得说整个这项工作 就如同我已经做的很多工作一样<br />很大程度上受到了Jeff Hinton的想法的启发 具体来说 2007年他举办了一个讲座 第一个深度学习的研讨会 在研讨会上<br />他讲了他对大脑工作原理的想法 怎么使用时间编码 有可能完成反向传播 这激发了最近几年<br />我在这方面探索过的很多想法 对 这是个有趣的故事 到现在已经大概有十年了 我听你也提过多次的话题当中 有一个是 无监督学习 你能跟我们分享下你的观点吗? 好的 无监督学习真的很重要 现在 我们的产业系统是基于有监督学习 它本质上需要人类<br />定义与问题有关的 重要概念<br />并在数据中标注出这些概念 我们使用它构建出了<br />这些令人惊叹的玩具和服务 但人类能够做更多的事情 他们能够通过观察和与世界的交互探索 和发现新的概念 两岁的小孩能够<br />理解直觉物理学 换句话说<br />她能理解重力 理解压力 理解惯性 她能理解液体 固体 当然 她的父母从来没有教<br />她这些东西 对吧 那么她是怎么想出来的呢 这是无监督学习试图回答的问题 它不是说我们有标签<br />或是说我们没有标签 实际上它是说构建一个 心灵的建构 通过观察来解释世界的工作方式 最近 我一直努力 把无监督学习的理念与<br />增强学习的理念结合起来 因为我相信<br />很可能存在一些 潜藏的重要概念 我们正试图理清 或者说努力区分 人类或机器能够通过与世界互动 通过探索世界和尝试事物 或控制事物<br />而得到的东西 我觉得这些都与非监督学习 最初的理念紧密关联 所以我研究无监督学习 15年前当我们开始<br />做这些以及RBM等等时 我们聚焦在<br />学习良好的表示方法上 现在我还是认为<br />这是个核心问题 但是我们不知道是<br />良好的表示法是什么 要怎么获取 例如 我们怎么表示目标函数? 多年来我们尝试了很多东西 实际上这是无监督学习研究中<br />很酷的方向之一 也有很多不同的观点 不同的途径 来处理这个问题 也可能明年的发现就完全不同了 也许大脑使用的机制<br />与之前的研究完全不同 所以它不是渐进的研究 它本身就是<br />非常值得探索的东西 在什么是正确的目标函数方面<br />我们没有良好的定义 就更不用说度量某个系统<br />在无监督学习上的效果好不好 当然它非常有挑战 但同时 它也留下了广泛的可能性 这是研究人员非常热爱的东西<br />至少这是我非常喜欢的东西 现在有很多深度学习方面的研究 而且我觉得我们已经过了 有人有能力阅读所有发表的<br />深度学习论文的时代 那么 我有点好奇 <br />在现在的深度学习中什么让你最兴奋 我的目标非常高 我觉得 深度学习科学的当前状态<br />离我的期望相当远 我的印象是我们的系统<br />现在犯的那些错误类型 说明它们对<br />世界的了解相当肤浅 现在最让我兴奋的是<br />这样的研究方向 不再致力于做一些<br />所谓有用的系统 而是回到原理<br />有关计算机如何观察世界 与世界互动<br />并发现世界的运行方式 即使那个世界相当简单<br />简单到我们可以把它 写成一个游戏<br />但我们不知道怎么才能做好 而且那很酷 因为我不需要<br />和谷歌 Facebook 百度等公司竞争 对吧 因为这是种基础研究 任何人都能在他们的车库完成<br />但可以改变世界 当然有很多方向可以解决这个问题 但是我看到在深度学习和强化学习<br />之间有很多富有成效的互动 它们相当重要 我其实很激动<br />因为这方面的进步 实际上会对实际应用<br />产生巨大的影响 因为如果你观察我们现在碰到的<br />一些巨大挑战 比如我们如何处理样本非常少的 新的领域或分类 以及人类非常擅长<br />解决的问题那些地方 所以有这些迁移学习<br />和转化的问题 如果我们的系统能够更好地理解<br />世界是怎么运转的 那么处理这些问题会容易得多 或者说更深地理解 对吧 实际正在发生什么 我看到的东西背后有什么原因 我怎么通过行动来影响<br />我看到的东西 那么这些是我最近<br />非常感兴趣的问题类型 我认为联结 还有深度学习研究演化了几十年 已经触及了人工智能中更古老的问题 因为深度学习的很多成功<br />是与感知有关的 所以还剩什么呢 剩下的是某种高层级的条件 在抽象的层级上<br />理解事物的工作原理 所以我们计划去理解高层级的抽象 我觉得我们还没达到那种高级抽象<br />我们必须要达到那个水平 我们必须思考推理<br />思考信息的顺序处理 必须思考因果关系<br />如何发挥作用 以及机器如何自己做到<br />所有这些东西 可能需要人工来引导<br />但是尽可能自动地做 你刚刚说的话听起来就像 你对你现在试验的研究方法<br />非常着迷 我打算使用玩具问题这个词<br />但没有贬低的意思 (没事)<br />是指这个小问题 你很乐观<br />觉得它后面可以转化到更大的问题上 对 对 我觉得它会这么转化 当然我们必须要开展一些研究<br />让它扩大化 并解决那些问题 但是我研究这些玩具问题的最大动力是 我们可以<br />更好的理解我们的失败 而且我们可以把问题化简为<br />我们可以直观地操作 并且更容易理解 所以这有点像经典的<br />分治法(divide and conquer) 而且 大家没有充分考虑的一点是 研究周期可以更快 对吗 如果我可以在几个小时内完成实验<br />我能够进步更快 如果我不得不测试巨大的模型<br />来掌握所有的常识 和各方面的知识<br />虽然最终我们能做到 只是使用当前的硬件<br />每个实验都要很长的时间 所以当我们的硬件朋友<br />在创造比现在快上千倍 甚至上百万倍的机器时<br />我在做那些玩具实验 你知道 我还听你讲过 深度学习科学<br />而不仅仅是工程原理 但需要做更多的研究来了解<br />实际发生了什么 你能跟我们分享下这方面的想法吗 —不用谢 我担心我们现在做的很多工作<br />有点像盲人 在努力地找路 如果你运气特别好<br />可以用这种方式找到有趣的东西 但是真的如果我们<br />稍微停一停 设法用可以转化的方式<br />理解我们现在做的东西 因为我们已经从<br />原理深入到了理论 但是当我说理论的时候<br />我不一定指数学 当然我喜欢数学等等<br />但我觉得我们不需要把 所有的东西用数学形式化<br />但需要从逻辑上形式化 从这个角度讲<br />我能让人相信 这样应该有用 这样是否有意义 这是最重要的一方面 然后数学可以让我们<br />把这点变得越来越牢固 但实际上它更在于理解 它还涉及做研究的态度 不是为了成为下一个基线或者基准 或是打败其他实验室<br />或其他公司的其他人 更多的是应该问哪种问题<br />这种问题是否能 让我们更好的理解<br />研究领域中的现象 比如 是什么使得 训练更深的或当前的神经网络更加困难 我们有一些思路<br />但还有很多我们不了解 所以我们或许可以设计一些试验<br />目标不是得到更好的算法 只是更好的理解<br />我们已经有的算法 或者 在什么情况下<br />特定的算法效果更好 原因是什么 原因很重要 科学就是探索原因的 就是原因 好的<br />现在有很多人希望 进入这个领域 我相信你在一对一的谈话中<br />多次回答过这个问题 但是 现在有很多人在观看视频<br />你有哪些建议要 送给想进入人工智能<br />进入深度学习的人呢 好的 首先<br />大家的动机各有不同 可以做的事情也各有不同 做深度学习的<br />研究人员需要的东西可能与 使用深度学习构建产品的工程师<br />需要的东西不一样 这两种情况下<br />要求的理解程度不同 但是不管怎么样<br />两种情况都需要练习 为了真正掌握一个<br />像深度学习那样的学科 当然你需要阅读很多东西 你必须自己动手<br />练习编写很多东西的程序 面试的时候<br />我经常碰到一些使用软件的学生 现在有很多很好的软件<br />你只需要接入 使用 但完全不用理解其中的原理 如果停在表面这种层级上<br />那么很难 得出它什么时候会出问题<br />哪儿会出问题 所以实际上你要设法自己实现这些功能<br />尽管可能效率不高 但是仅仅是为了确保你真正理解背后的东西 这点非常有用 自己多尝试 也就是说不要只用那些编程框架<br />让你可以用几行代码 就完成所有功能<br />但你实际不知道底层原理 就是这样<br />我想说我们应该更进一步 如果可以的话<br />设法自己从基本原理中推出这些东西 这真的很有帮助 但是通常情况下 你必须阅读 看其他人的代码 写自己的代码 做很多试验<br />确保你理解你做的所有事情 特别对于科学来说 这是其中的一部分 问问自己为什么我要做这些<br />为什么其他人在做这些 可能答案就在书本的某一页<br />你必须读更多的书 如果你实际上能自己想出来<br />那就更好 对 这样很酷 事实上 在我读过的文章中<br />你 Ian Goodfellow Aaron Courville<br />写了一本评价非常高的书 泰勒：谢谢，谢谢。 对 它卖得很好 有点疯狂 我觉得现在读这本书的人可能 比能理解这本书的人多 不过ICLRI大会汇刊 收集的好论文可能最多 当然NIPS和ICML 还有其他的会议<br />也有很好的论文 但是如果你真的想接触很多好论文<br />只要阅读最近几期的 ICLR汇刊<br />这能让你真正地看清这个领域 对 还有其他想法吗？ 大家想问问你<br />怎么才能成为深度学习的高手 当然<br />这取决于你从哪儿开始 不要害怕数学 只要发展直觉<br />然后只要你从直觉上把握了 事物背后的原理<br />数学就会变得相当容易 还有个好消息是你不需要<br />5年的博士学习 来成为深度学习的高手 实际上如果你有很好的<br />计算机科学和数学基础 只要学习短短几个月 你就可以用好它 构建出东西 并且开始做研究试验 如果接受过良好的训练<br />大概只要六个月 可能他们一点都不了解<br />机器学习 但是 如果他们擅长数学和<br />计算机科学 这个过程会很快 当然 这意味着在<br />数学和计算机科学方面 你需要有良好的训练 有时候 你在计算机科学课程中<br />学到的还不够 特别是 需要补充一些数学 比如概率论 代数和优化 我明白了 还有微积分 对 微积分 Joshua 非常感谢你<br />分享这些观点 见解和指导 虽然我们认识很长时间了<br />但直到今天 我才了解你早年这些故事的细节<br />所以 谢谢你 Andrew 谢谢你做了 这次特别的视频<br />你现在做的事情很有意义 我希望会有很多人能用上它们