1
00:00:00,210 --> 00:00:04,380
これまでハイパーパラメータの探し方をたくさん聞いてきましたね。

2
00:00:04,380 --> 00:00:08,090
この議論を終える前に、

3
00:00:08,090 --> 00:00:11,670
ハイパーパラメータサーチのプロセスを整理するために使える

4
00:00:11,670 --> 00:00:14,775
コツをいくつか紹介しましょう。

5
00:00:14,775 --> 00:00:19,259
ディープラーニングは今や多くの異なる応用分野で使われています。

6
00:00:19,259 --> 00:00:24,342
ハイパーパラメータ設定について、1つの応用分野からの知識は、異なる応用分野にそのまま使えるかもしれませんし、

7
00:00:24,342 --> 00:00:26,670
使えないかもしれません。

8
00:00:26,670 --> 00:00:31,466
異なる応用分野の間で、たくさんの相互交流があります。

9
00:00:31,466 --> 00:00:36,055
例えば、私はコンピュータビジョンのコミュニティで発展したConvNetやResNetのようなアイデアが、

10
00:00:36,055 --> 00:00:40,301
後のコースで話す

11
00:00:40,301 --> 00:00:42,400
スピーチ（認識）の分野に、うまく適用されたことを見てきました。

12
00:00:42,400 --> 00:00:46,620
私は、最初スピーチの分野で発展したアイデアが、自然言語処理の分野でうまく適用されたことを見てきました。

13
00:00:46,620 --> 00:00:47,870
などなど。

14
00:00:47,870 --> 00:00:52,613
だから、ディープラーニングにおける良い発展は、異なる応用分野の人々が、

15
00:00:52,613 --> 00:00:57,223
他の応用分野の研究論文を読むことで、

16
00:00:57,223 --> 00:01:00,180
お互いの分野のインスピレーションを探すことにあります。

17
00:01:00,180 --> 00:01:01,580
ハイパーパラメータ設定に関する直観は

18
00:01:01,580 --> 00:01:06,420
使えなくなることが多々あります。

19
00:01:06,420 --> 00:01:11,360
例えば物流とか、一つの問題に取り組んでいる過程で

20
00:01:11,360 --> 00:01:15,700
良いハイパーパラメータの設定を見つけ、そのアルゴリズムを開発し続けたとしよう。

21
00:01:15,700 --> 00:01:20,465
しかし数ヶ月の間にデータが少しずつ変化していったとか、

22
00:01:20,465 --> 00:01:25,070
データセンターのサーバーをアップグレードしたかもしれない。

23
00:01:25,070 --> 00:01:26,380
それらの変化によって

24
00:01:26,380 --> 00:01:29,500
あなたが考えたハイパーパラメータの最良な設定は使えなくなる可能性があります。

25
00:01:29,500 --> 00:01:32,005
だから、私は再テストすること、

26
00:01:32,005 --> 00:01:35,860
あるいは、ハイパーパラメータを再評価することを数ヶ月ごとに行うことを推奨します。

27
00:01:35,860 --> 00:01:39,390
持っている値が基準を満たしていることを確認するためです。

28
00:01:39,390 --> 00:01:42,150
最後に、人々がどうハイパーパラメータを探すかについては、

29
00:01:42,150 --> 00:01:46,430
2つのメジャーな思想流派、

30
00:01:46,430 --> 00:01:50,370
もしくは、人々がとる2つの道があります。

31
00:01:50,370 --> 00:01:52,920
1つの方法は、1モデルをベビーシットすることです。

32
00:01:52,920 --> 00:01:57,671
この方法をとる場合は主に、たくさんのデータを持っているが、

33
00:01:57,671 --> 00:02:01,390
CPUやGPUなどの計算資源が少ないため

34
00:02:01,390 --> 00:02:05,100
一度に一つか、少数のモデルしか訓練することができない場合です。

35
00:02:05,100 --> 00:02:11,070
こんなときは、そのモデルの学習中にもベビーシットする場合もあります。

36
00:02:11,070 --> 00:02:15,180
例えばday0に、パラメータをランダムに初期化し、

37
00:02:15,180 --> 00:02:16,370
訓練をスタートする。

38
00:02:16,370 --> 00:02:21,626
それから学習曲線、コスト関数 J、

39
00:02:21,626 --> 00:02:27,333
あるいはエラーや他の何かが段階的に減ることを初日に観察したとします。

40
00:02:27,333 --> 00:02:31,300
だとするとその日の終わりに、「学習がとてもうまくいっているようだ。

41
00:02:31,300 --> 00:02:35,000
学習率を少し大きくして様子を見よう」というかもしれない。

42
00:02:35,000 --> 00:02:37,090
そして、パフォーマンスはさらに良くなるとします。

43
00:02:37,090 --> 00:02:38,870
day2のパフォーマンスです。

44
00:02:38,870 --> 00:02:42,150
day2のあと、「まだうまくいっている。

45
00:02:42,150 --> 00:02:46,339
momentumを少し変えて、学習率も下げよう」と考え

46
00:02:46,339 --> 00:02:47,994
そして3日目に入ったとします。

47
00:02:47,994 --> 00:02:52,750
そのようにして毎日モデルを観察し、パラメータを上下にいじっていく中で

48
00:02:52,750 --> 00:02:55,646
ある時、学習率が大きすぎることが分かったら、

49
00:02:55,646 --> 00:02:58,649
前の日のモデルに戻ることができます。

50
00:02:58,649 --> 00:03:03,445
要は、数日・数週間の間、毎日

51
00:03:03,445 --> 00:03:08,080
1つのモデルをベビーシットするということです。

52
00:03:08,080 --> 00:03:12,010
これが一つ目のアプローチです。ベビーシットしているモデルのパフォーマンスを見ながら、

53
00:03:12,010 --> 00:03:17,390
忍耐強く学習率を手動で上下させる。

54
00:03:17,390 --> 00:03:21,010
先ほど述べたように、このパターンは多くのモデルを同時に訓練するほどの

55
00:03:21,010 --> 00:03:24,210
CPUやGPUなどの資源がない時に取るパターンです。

56
00:03:24,210 --> 00:03:28,480
2つ目のアプローチは多くのモデルを並列して訓練する方法です。

57
00:03:28,480 --> 00:03:32,010
その時はいくつかのハイパーパラメタの設定を考えており

58
00:03:32,010 --> 00:03:36,050
モデルをそのうちの1つの設定で訓練させる、1日でもそれ以上でも。

59
00:03:36,050 --> 00:03:38,180
すると図のような学習曲線を得ます。

60
00:03:38,180 --> 00:03:42,180
これはコスト関数 J や訓練データの誤差、あるいは

61
00:03:42,180 --> 00:03:45,670
データセット誤差のプロットと考えてもらって構いません。

62
00:03:45,670 --> 00:03:48,636
そしてそれと同時にまた別のハイパーパラメータの設定で

63
00:03:48,636 --> 00:03:50,490
もう一つモデルを訓練させます。

64
00:03:50,490 --> 00:03:54,030
こっちのほうのモデルは

65
00:03:54,030 --> 00:03:55,960
こんな感じの学習曲線を描くとしましょう。

66
00:03:55,960 --> 00:03:57,510
「ふむ、こっちのほうがよく見えるな。」

67
00:03:57,510 --> 00:03:59,980
そして3つ目のモデルも訓練してみましょう。

68
00:03:59,980 --> 00:04:03,914
こんな感じの学習曲線かな。

69
00:04:03,914 --> 00:04:06,680
4つ目の曲線はこんな感じとしましょう。

70
00:04:06,680 --> 00:04:10,280
このようにしてたくさんのモデルを訓練します。

71
00:04:10,280 --> 00:04:13,570
このオレンジ色のはまた別のモデルです。

72
00:04:13,570 --> 00:04:16,620
結果として、いろんなハイパーパラメータの設定でモデルを同時に訓練させ

73
00:04:16,620 --> 00:04:21,090
その中から一番よさそうなモデルを選択することができます。

74
00:04:21,090 --> 00:04:25,600
この中では多分この曲線が一番よさそうですね。

75
00:04:25,600 --> 00:04:27,340
たとえ話をすると

76
00:04:27,340 --> 00:04:30,760
左のアプローチをパンダアプローチを呼びましょう。

77
00:04:30,760 --> 00:04:33,822
パンダは基本少数の子供を産みますね、

78
00:04:33,822 --> 00:04:35,507
1度に1匹。

79
00:04:35,507 --> 00:04:40,350
そしてそれらの子供たちがそれぞれちゃんと成長できるように<br />全力を注ぎます。

80
00:04:40,350 --> 00:04:41,640
つまりベイビーシッティングですね、

81
00:04:41,640 --> 00:04:44,280
一つのモデルは1匹の子供パンダ。

82
00:04:44,280 --> 00:04:48,000
一方で右のアプローチは魚の産卵みたいですね。

83
00:04:48,000 --> 00:04:50,380
キャビアアプローチとでも呼びましょうか。

84
00:04:50,380 --> 00:04:55,540
魚の中には同時に1億の卵を産む種類があります。

85
00:04:55,540 --> 00:04:58,960
しかしその魚は1億個の卵

86
00:04:58,960 --> 00:05:01,740
一つ一つに注意を払うのではなく

87
00:05:01,740 --> 00:05:05,970
その中から確率的に成長できそうなものが<br />産まれる感じになります。

88
00:05:05,970 --> 00:05:10,340
要約すると、哺乳類と

89
00:05:10,340 --> 00:05:15,030
魚や爬虫類の繁殖の違いのようなものです。

90
00:05:15,030 --> 00:05:17,980
ここではパンダアプローチとキャビアアプローチと呼びます、

91
00:05:17,980 --> 00:05:20,210
そのほうが覚えやすそうですしね。

92
00:05:20,210 --> 00:05:23,550
この2つのアプローチの選び方は

93
00:05:23,550 --> 00:05:26,500
手元にどれだけのCPUやGPUがあるかによります。

94
00:05:26,500 --> 00:05:30,460
モデルを並列で訓練できるほどの資源があるなら

95
00:05:31,920 --> 00:05:34,670
キャビアアプローチを選択し

96
00:05:34,670 --> 00:05:37,780
いろんなハイパーパラメータを同時に試しましょう。

97
00:05:37,780 --> 00:05:42,520
しかし例えばオンライン広告など特定の分野では

98
00:05:42,520 --> 00:05:45,940
データが多すぎるし

99
00:05:45,940 --> 00:05:48,670
モデルも大きいので

100
00:05:48,670 --> 00:05:53,220
別のモデルを同時進行で訓練するキャパがない場合があります。

101
00:05:53,220 --> 00:05:55,640
このように分野によるとしか言えませんが

102
00:05:55,640 --> 00:06:00,150
様々なコミュニティーでパンダアプローチのように

103
00:06:00,150 --> 00:06:03,260
一つのモデルをベビーシットしながら

104
00:06:03,260 --> 00:06:08,340
パラメータを上げ下げしてモデルを訓練する方法が主流です。

105
00:06:08,340 --> 00:06:12,165
でもパンダアプローチといっても

106
00:06:12,165 --> 00:06:15,580
最初に訓練したモデルがうまくいかなくて

107
00:06:15,580 --> 00:06:19,870
2週目や3週目で「よしもう一度別の設定でやり直そう」と

108
00:06:19,870 --> 00:06:23,910
いうこともあるでしょう。

109
00:06:23,910 --> 00:06:28,800
パンダも一人っ子である必要はありませんから。

110
00:06:28,800 --> 00:06:32,888
さて、このビデオではハイパーパラメータのサーチをどのようにして行うべきかを

111
00:06:32,888 --> 00:06:34,170
一緒に考えました。

112
00:06:34,170 --> 00:06:37,090
しかし実はお教えしていないテクニックがまだあります。

113
00:06:37,090 --> 00:06:41,360
このテクニックはあなたのモデルを<br />ハイパーパラメータの選択に対してロバストにします。

114
00:06:41,360 --> 00:06:44,180
すべてのモデルに組み込むことはできませんが、<br />できる場合は

115
00:06:44,180 --> 00:06:48,670
ハイパーパラメータのサーチをもっと簡単に、<br />そして訓練のスピードもより速くすることができます。

116
00:06:48,670 --> 00:06:50,780
このテクニックは次のビデオで紹介しましょう。