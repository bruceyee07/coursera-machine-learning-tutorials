さて ここまでで ニューラルネットワークを学習するには 様々なハイパーパラメタの設定を
しなければならないことがわかりました ではそのハイパーパラメタの良い設定を見つけるには
どうすればよいでしょうか このビデオではそのガイドラインやコツなど システマティックにハイパーパラメタを
チューニングする方法についてご紹介します これによって良いハイパーパラメタ設定に より効率的にたどり着けるようになればと思います ディープネットワークを学習する時の悩みとして ハイパーパラメタの数の多さが挙げられます 例えば学習率α
モメンタムを使うのであればβ Adam最適化で使われる方の
ハイパーパラメタであれば β1 β2 ε レイヤー数もあるでしょうし 隠れ層ごとにユニット数も選ぶ必要があります また減衰をあつかうことで 変化する学習率を使うこともあります 当然 ミニバッチサイズも選ばなくてはなりません 実はこれらハイパーパラメタの重要度は
それぞれ異なります ほとんどのアプリケーションでは 学習率αがもっとも重要な
ハイパーパラメタだと言っていいでしょう α以外で私が次にチューニングするならば おそらくモメンタム項のβ 0.9なんかは良いデフォルト値ですね 他にはミニバッチサイズを変えて 最適化アルゴリズムが効率的に走るようにしたり また隠れ層のユニット数などもよくいじります これらオレンジ色で囲った3つは 学習率αに次いで 大事なものと言えるでしょう それらをいじった後としては レイヤー数は大きな違いを生むことがありますし 学習率減衰などもそうです またAdamアルゴリズムを使うとき
それらのハイパーパラメタ β1 β2 εのチューニングをすることはまずありません やってみても構いませんが 私はほぼ必ず0.9 0.999 10の-8乗を使います さてここまでで
ハイパーパラメタの重要度の違いについて ざっと理解できたでしょうか おそらくαが最も重要で 次にオレンジ色で囲んだもの 更に紫色で囲んだものと続きます ただし絶対ではありませんし ディープラーニングを実践されている方でも 違った考えの方がいらっしゃるでしょう さて ハイパーパラメタをセットでチューニングする時 どうやって探索すべき値のセットを
選べばいいでしょうか 初期の機械学習アルゴリズムでは 例えば2つのハイパーパラメタ ハイパーパラメタ1 ハイパーパラメタ2
があった場合 典型的な手法として
このように格子状に点を抽出し システマティックにこれらの値を探索します ここでは例として5x5にしました 実際はそれ以外の数もありえますが
この例では全25点の値を試し 1番性能の良かったハイパーパラメタを採用します この手法はハイパーパラメタの種類が
比較的少ない時は大丈夫なのですが ディープラーニングでは
どちらかというと他の手法を 私は使いますし皆さんにもおすすめします ランダムに点を選ぶのです どんどん選んでいきましょう
同じ数にしましょうか そしてこのランダムに選んだ25点を
探索するのです なぜこうした方がいいのでしょうか
それはどのハイパーパラメタが その問題において重要なのか
事前にはわからないからです 前のスライドで見たとおり ハイパーパラメタは種類によって
その重要性が違います 例えば ハイパーパラメタ1が学習率αだとします 極端な例として ハイパーパラメタ2が Adamアルゴリズムの分母中の
εだったとしましょう αの選び方で結果は大きく変わりますが
εでは殆ど変わりません 格子状に値を選ぶということは 実質的には5種類のαを
試したことにすぎません おそらくεの値が変わっても 基本的には同じ結果が
返ってくることになるでしょう つまり25種類のモデルを学習したとしても より重要度の高い学習率αを 5種類試行したに過ぎない
ということなのです 一方 ランダムにサンプリングすると 25種類の異なる学習率αを 試行することになるので 性能の良い値をより見つけやすくなります この例を説明するにあたって ハイパーパラメタを2つだけ使いましたが 実際にはずっと多くのハイパーパラメタを
探索をすることもあります 例えばハイパーパラメタ3つならば 正方形を探索する代わりに ハイパーパラメタ3が3次元目になった
立方体を探索する事になりますね そうすると値のサンプリングが 3次元の立方体からされることになり それぞれのハイパーパラメタにとっても
より多くの値が試行出来ることになります 実際には 4つ以上のハイパーパラメタを
探索することもあり どれがそのアプリケーションにとって
本当に重要なハイパーパラメタであるか 事前にはわからないこともあります 格子状ではなくランダムにサンプリングしたほうが 重要度の高いハイパーパラメタについて より潤沢な値探索ができることが分かります ハイパーパラメタのサンプリングをする時に もう一つよく使われるのが
粗密サンプリングスキームです 2次元の例で考えてみましょう
これらの点をサンプリングしました 例えばこの点が1番性能がいい またその周りにも何点か
性能の良い点が見つかったとします その時 粗密スキームでは何をするかというと その小さな領域にズームインし
その中でより細かくサンプリングをします もしくは全体を再度ランダムサーチしてもいいですが 他のエリアより力を入れて
この青い四角の中を探します いちばん性能のいい設定
つまりハイパーパラメタの組み合わせが 多分この領域にあるだろうと疑っているわけです まとめると
全領域に粗いサンプリングをかけて 次にフォーカスする小さい領域を見つけます そしてその小さい領域で
より密なサンプリングをします この種の粗密探索も頻繁に使われます 異なる値のハイパーパラメタを試すことで 例えばトレーニングセットで
ベスト性能を発揮する値 またはバリデーションセットで
最もうまくいく値 その他値探索プロセスで最適化したい
様々なものを選び出すことが出来るのです さて ハイパーパラメタ探索を
よりシステマティックに 行う方法についてご理解いただけたでしょうか キーポイントとしては 格子状ではなくランダムに探索すること 必要に応じて粗密探索を使うこと
の2点です しかしハイパーパラメタ探索は
これで終わりではありません 次のビデオでは
どうやって適切なスケールで ハイパーパラメタのサンプリングをするか
について見ていきましょう