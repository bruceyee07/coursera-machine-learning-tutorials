1
00:00:00,000 --> 00:00:01,710
嗨,歡迎回來.嗨,歡迎回來.

2
00:00:01,710 --> 00:00:04,140
你已經知道訓練類神經網路需要

3
00:00:04,140 --> 00:00:07,415
設定各式各樣的超參數(hyperparameters)

4
00:00:07,415 --> 00:00:11,155
那，你該如何找出一組好的超參數呢？

5
00:00:11,155 --> 00:00:13,710
在這部影片，我想分享一些指南，

6
00:00:13,710 --> 00:00:18,235
一些技巧，來有系統地調校超參數

7
00:00:18,235 --> 00:00:20,640
希望這能讓你有效率地

8
00:00:20,640 --> 00:00:23,760
找出一組好的超參數

9
00:00:23,760 --> 00:00:25,929
訓練深的網路，最痛苦的在於

10
00:00:25,929 --> 00:00:29,250
你必須要處理很大量的超參數

11
00:00:29,250 --> 00:00:35,935
從學習率alpha，到momentum的參數beta
－如果用的是momentum方法

12
00:00:35,935 --> 00:00:41,370
或是Adam優化法中的beta_1,

13
00:00:41,370 --> 00:00:44,185
beta_2和epsilon

14
00:00:44,185 --> 00:00:47,270
或許你需要決定層數

15
00:00:47,270 --> 00:00:50,820
或許你要決定每一層的隱藏單元個數

16
00:00:50,820 --> 00:00:55,093
也許你想使用學習率衰減
(learning rate decay)

17
00:00:55,093 --> 00:00:59,899
所以不光只是單一個學習率alpha

18
00:00:59,899 --> 00:01:01,065
還有當然

19
00:01:01,065 --> 00:01:06,220
你可能還要選擇迷你批次
(mini-batch)的大小

20
00:01:06,220 --> 00:01:09,990
實際上，有些超參數比其他的超參數還重要

21
00:01:09,990 --> 00:01:12,235
我會說，對於大部分的情況

22
00:01:12,235 --> 00:01:16,015
alpha，也就是學習率，是最重要
需要調校的超參數

23
00:01:16,015 --> 00:01:21,595
除了alpha，接下來我或許會調校

24
00:01:21,595 --> 00:01:25,040
momentum法的超參數

25
00:01:25,040 --> 00:01:27,795
例如 0.9 會是個好的預設值

26
00:01:27,795 --> 00:01:30,700
我也會調校迷你批次的大小，

27
00:01:30,700 --> 00:01:34,465
來確保最佳化的時候更有效率

28
00:01:34,465 --> 00:01:36,985
我也常調調看隱藏單元的個數

29
00:01:36,985 --> 00:01:39,250
我這邊用橘色圈起來的

30
00:01:39,250 --> 00:01:43,660
這三項是我覺得僅次於alpha

31
00:01:43,660 --> 00:01:46,060
第二重要的。然後

32
00:01:46,060 --> 00:01:49,060
弄完那些以後，第三重要的話

33
00:01:49,060 --> 00:01:51,925
層數有時影響很大

34
00:01:51,925 --> 00:01:55,000
學習率衰減也有可能

35
00:01:55,000 --> 00:01:58,870
然後我在用Adam優化法的時候，我幾乎
沒調過beta_1

36
00:01:58,870 --> 00:02:00,434
beta_2和epsilon

37
00:02:00,434 --> 00:02:01,930
我通常都會用 0.9,

38
00:02:01,930 --> 00:02:08,570
0.999 和 10^-8。雖然你想要的話
也可以嘗試調看看

39
00:02:08,570 --> 00:02:12,130
總之，希望這能帶給你一些概念

40
00:02:12,130 --> 00:02:16,463
哪些超參數比較重要。

41
00:02:16,463 --> 00:02:19,005
alpha當然是最重要的

42
00:02:19,005 --> 00:02:22,270
然後可能是這些用橘色圈起來的

43
00:02:22,270 --> 00:02:25,235
接下來大概是這些紫色的

44
00:02:25,235 --> 00:02:27,760
不過，這不是唯一的鐵則。我想

45
00:02:27,760 --> 00:02:30,490
其他在實作深度學習的人也可能

46
00:02:30,490 --> 00:02:33,670
不同意我，或是有不同的直覺

47
00:02:33,670 --> 00:02:37,240
那，如果你要調整某個超參數

48
00:02:37,240 --> 00:02:40,060
你該怎麼選擇哪些數值來嘗試呢

49
00:02:40,060 --> 00:02:42,845
在早期的機器學習演算法中

50
00:02:42,845 --> 00:02:44,660
如果你有兩個超參數

51
00:02:44,660 --> 00:02:47,662
假設叫超參數一和超參數二

52
00:02:47,662 --> 00:02:53,380
實作上會像這樣，用格子狀的方法取樣

53
00:02:53,380 --> 00:02:59,435
以有系統地嘗試這些數值

54
00:02:59,435 --> 00:03:00,935
這邊我取了五乘五的格子點

55
00:03:00,935 --> 00:03:06,070
實際上有可能比五乘五多或少。
在這例子

56
00:03:06,070 --> 00:03:12,430
你試過所有25個點，
然後取效果最好的那組超參數

57
00:03:12,430 --> 00:03:18,010
當超參數的數量不多，這種方法還不錯

58
00:03:18,010 --> 00:03:19,840
然而在深度學習，我們傾向於

59
00:03:19,840 --> 00:03:21,415
而且我建議最好是

60
00:03:21,415 --> 00:03:23,975
隨機取樣那些點

61
00:03:23,975 --> 00:03:27,970
就這樣去選一樣數量的點

62
00:03:27,970 --> 00:03:34,590
隨機的25個點，
然後嘗試這25個點代表的超參數

63
00:03:34,590 --> 00:03:38,350
這樣做的原因是，我們很難事先知道

64
00:03:38,350 --> 00:03:43,040
哪種超參數在你的應用裡是最重要的

65
00:03:43,040 --> 00:03:44,480
就像你在前一張投影片看到的

66
00:03:44,480 --> 00:03:47,910
有些超參數比其他的還重要許多

67
00:03:47,910 --> 00:03:49,190
舉個例子

68
00:03:49,190 --> 00:03:53,505
假設這裡的超參數一，實際上是alpha，學習率

69
00:03:53,505 --> 00:03:55,175
再假設一個極端的情況

70
00:03:55,175 --> 00:03:58,180
假設超參數二是

71
00:03:58,180 --> 00:04:02,730
Adam優化法裡面，分母的那個epsilon

72
00:04:02,730 --> 00:04:07,455
所以alpha選多少很重要，
而epsilon選多少則不重要

73
00:04:07,455 --> 00:04:12,410
所以如果用格子法取樣，實際上

74
00:04:12,410 --> 00:04:16,300
你只嘗試了五種alpha的值

75
00:04:16,300 --> 00:04:18,550
而且你可能發現這其中的

76
00:04:18,550 --> 00:04:21,190
五種不同的epsilon都給你同樣的效果

77
00:04:21,190 --> 00:04:24,400
雖然你訓練了25個模型，

78
00:04:24,400 --> 00:04:27,925
但僅僅試了5種學習率alpha

79
00:04:27,925 --> 00:04:29,740
而alpha是很重要的

80
00:04:29,740 --> 00:04:33,430
相較之下，如果隨機地取樣

81
00:04:33,430 --> 00:04:37,960
你就能嘗試25種不同的

82
00:04:37,960 --> 00:04:40,390
學習率alpha，所以更有可能

83
00:04:40,390 --> 00:04:43,690
找到有好效果的值

84
00:04:43,690 --> 00:04:44,980
在這個例子，

85
00:04:44,980 --> 00:04:47,160
我只用了兩個超參數來解釋

86
00:04:47,160 --> 00:04:50,270
實際上，你可能會找更多個超參數

87
00:04:50,270 --> 00:04:52,000
假設你有

88
00:04:52,000 --> 00:04:55,080
三個超參數，這不再是在平面上尋找

89
00:04:55,080 --> 00:05:00,820
你是在一個立方體中尋找，
這第三個維度代表超參數三

90
00:05:00,820 --> 00:05:03,010
那在這個

91
00:05:03,010 --> 00:05:05,380
三維的立方體裡取樣，

92
00:05:05,380 --> 00:05:08,080
對於每一個超參數，你就能嘗試更多不同的值

93
00:05:08,080 --> 00:05:11,440
在實務中，你要找的超參數

94
00:05:11,440 --> 00:05:14,980
甚至可能比三個還多，而且有時

95
00:05:14,980 --> 00:05:17,160
很難事先知道哪些超參數

96
00:05:17,160 --> 00:05:22,120
在你的應用中會比較重要。

97
00:05:22,120 --> 00:05:25,390
比起格子法，隨機取樣更能保證

98
00:05:25,390 --> 00:05:28,085
你嘗試過更多可能的數值

99
00:05:28,085 --> 00:05:31,045
無論最重要的超參數是哪幾個

100
00:05:31,045 --> 00:05:33,130
當你在取樣超參數時，

101
00:05:33,130 --> 00:05:37,875
另一個常見手法是「由粗略到精細」的取樣

102
00:05:37,875 --> 00:05:42,130
假設在這個二維的例子，你找了這些點

103
00:05:42,130 --> 00:05:45,600
發現這個點效果最好

104
00:05:45,600 --> 00:05:49,210
然後可能周圍的這幾個點效果也不錯

105
00:05:49,210 --> 00:05:53,530
在「由粗略到精細」的方案下，你可能會縮小

106
00:05:53,530 --> 00:06:00,820
到這一小塊區域，然後在裡面更密集地尋找超參數

107
00:06:00,820 --> 00:06:02,795
或者說，還是一樣隨機地找

108
00:06:02,795 --> 00:06:06,690
但是花更多的力氣在

109
00:06:06,690 --> 00:06:11,265
在這個藍色方框裡，如果你猜想

110
00:06:11,265 --> 00:06:13,600
最好的超參數在這個區域裡

111
00:06:13,600 --> 00:06:18,365
所以在這整塊區域大略掃過一遍後

112
00:06:18,365 --> 00:06:22,375
發現要集中火力在這小塊地方

113
00:06:22,375 --> 00:06:26,105
接下來你在這小區塊更密集地取樣

114
00:06:26,105 --> 00:06:29,720
這種由粗略到精細的尋找手法也很常用

115
00:06:29,720 --> 00:06:33,565
在嘗試過各種超參數的值以後，你就能

116
00:06:33,565 --> 00:06:37,740
挑選讓訓練集的目標函數最好的超參數

117
00:06:37,740 --> 00:06:41,230
或是在開發集表現最好的

118
00:06:41,230 --> 00:06:46,660
或是各種你想最佳化的目標

119
00:06:46,660 --> 00:06:48,570
我希望這能引導你

120
00:06:48,570 --> 00:06:51,670
更有系統地找尋超參數

121
00:06:51,670 --> 00:06:53,200
有兩個要點：

122
00:06:53,200 --> 00:06:55,930
用隨機取樣的方式，不要用格子法

123
00:06:55,930 --> 00:07:01,585
還有雖然非必要，考慮用由粗略到精細的搜尋手法

124
00:07:01,585 --> 00:07:04,750
不過，要找到好的超參數還不只這些

125
00:07:04,750 --> 00:07:07,300
在下一段影片，讓我們來探討更多

126
00:07:07,300 --> 00:07:10,020
如何在正確的尺度比例上尋找你的超參數