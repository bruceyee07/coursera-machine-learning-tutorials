1
00:00:00,000 --> 00:00:01,710
Olá, e bem vindo de volta.

2
00:00:01,710 --> 00:00:04,140
Você viu até agora que 
alterar as redes neurais pode

3
00:00:04,140 --> 00:00:07,415
envolver a configuração de 
muitos hiperparâmetros distintos.

4
00:00:07,415 --> 00:00:11,155
Mas, e como saber o que é uma boa 
configuração para esses hiperparâmetros?

5
00:00:11,155 --> 00:00:13,710
Neste vídeo, quero compartilhar 
com você algumas orientações,

6
00:00:13,710 --> 00:00:18,235
algumas dicas de como organizar sistematicamente
o processo de ajuste dos hiperparâmetros,

7
00:00:18,235 --> 00:00:20,640
que, espero, tornarão mais eficiente a

8
00:00:20,640 --> 00:00:23,760
convergência, quando realizamos uma boa 
configuração dos hiperparâmetros.

9
00:00:23,760 --> 00:00:25,929
Uma das coisas difíceis ao treinar 
redes [neurais] profundas

10
00:00:25,929 --> 00:00:29,250
é o número exato de hiperparâmetros
 com o qual você tem de lidar,

11
00:00:29,250 --> 00:00:35,935
desde alfa, a taxa de aprendizagem 
 até beta, o termo de impulso ('momentum'), se tiver usando 'momentum',

12
00:00:35,935 --> 00:00:41,370
ou os hiperparâmetros do Algoritmo de Otimização de Adam,
beta 1,

13
00:00:41,370 --> 00:00:44,185
beta 2 e épsilon.

14
00:00:44,185 --> 00:00:47,270
Talvez tenha que escolher 
o número de camadas,

15
00:00:47,270 --> 00:00:50,820
e também o número de unidades ocultas
 para as diferentes camadas,

16
00:00:50,820 --> 00:00:55,093
e talvez queira controlar a 
redução da taxa de aprendizagem,

17
00:00:55,093 --> 00:00:59,899
não usando assim uma única 
taxa de aprendizagem alfa.

18
00:00:59,899 --> 00:01:01,065
E então, é claro,

19
00:01:01,065 --> 00:01:06,220
talvez precise escolher o 
tamanho do "mini-batch".

20
00:01:06,220 --> 00:01:09,990
Assim, alguns destes hiperparâmetros 
são mais importantes do que outros.

21
00:01:09,990 --> 00:01:12,235
Para a maioria dos aplicativos 
de aprendizagem, eu diria que

22
00:01:12,235 --> 00:01:16,015
alfa, a taxa de aprendizagem é o hiperparâmetro
mais importante a ser ajustado.

23
00:01:16,015 --> 00:01:21,595
Além de alfa, eu tendeira a ajustar alguns 
outros poucos hiperparâmetros em seguida,

24
00:01:21,595 --> 00:01:25,040
talvez beta, o termo 
de impulso ('momentum'),

25
00:01:25,040 --> 00:01:27,795
digamos que 0,9 é um 
bom valor inicial para ele.

26
00:01:27,795 --> 00:01:30,700
Também ajustaria o tamanho 
do mini-batch para certificar

27
00:01:30,700 --> 00:01:34,465
que o algoritmo de aprendizagem está 
funcionando adequadamente.

28
00:01:34,465 --> 00:01:36,985
Também sempre penso 
nas unidades ocultas.

29
00:01:36,985 --> 00:01:39,250
Os quais circulei em laranja,

30
00:01:39,250 --> 00:01:43,660
são realmente os três que considero
mais importantes após a

31
00:01:43,660 --> 00:01:46,060
taxa de aprendizagem alfa, e em seguida,
terceiros

32
00:01:46,060 --> 00:01:49,060
na importância, após ajustar os outros,

33
00:01:49,060 --> 00:01:51,925
o número de camadas,
que pode fazer uma enorme diferença,

34
00:01:51,925 --> 00:01:55,000
e a taxa de redução de alfa.

35
00:01:55,000 --> 00:01:58,870
E depois, se estiver usando o algoritmo de Adam,
que praticamente eu nunca ajustei: beta 1,

36
00:01:58,870 --> 00:02:00,434
beta 2 e épsilon.

37
00:02:00,434 --> 00:02:01,930
Eu uso sempre 0,9,

38
00:02:01,930 --> 00:02:08,570
0,999 e 10 ^ -8 (dez à menos 8),
embora você possa tentar ajustá-los como quiser.

39
00:02:08,570 --> 00:02:12,130
Mas, espero que isto posto,
 tenha lhe dado a de ideia de quais dos hiperparâmetros

40
00:02:12,130 --> 00:02:16,463
podem ser mais importantes que os outros: alfa,

41
00:02:16,463 --> 00:02:19,005
o mais importante, com certeza,

42
00:02:19,005 --> 00:02:22,270
seguido de, talvez, os circulados em laranja,
[beta, unidades ocultas, mini-batch]

43
00:02:22,270 --> 00:02:25,235
seguidos, talvez, pelos circulados em roxo
[número de camadas e taxa de redução de alfa].

44
00:02:25,235 --> 00:02:27,760
Mas, essa não é uma regra 
difícil e rápida, e penso que

45
00:02:27,760 --> 00:02:30,490
outros praticantes da 
aprendizagem profunda podem

46
00:02:30,490 --> 00:02:33,670
discordar de mim ou ter 
outros conceitos sobre ela.

47
00:02:33,670 --> 00:02:37,240
Agora, se estiver tentando ajustar 
um conjunto de hiperparâmetros,

48
00:02:37,240 --> 00:02:40,060
como selecionar um conjunto de valores
a serem explorados?

49
00:02:40,060 --> 00:02:42,845
Nas gerações iniciais dos algoritmos de
aprendizagem de máquina,

50
00:02:42,845 --> 00:02:44,660
se tivéssemos dois hiperparâmetros,

51
00:02:44,660 --> 00:02:47,662
os quais estou chamando de 
hiperparâmetro um e hiperparâmetro dois,

52
00:02:47,662 --> 00:02:53,380
era prática comum provar os 
pontos em uma grade como

53
00:02:53,380 --> 00:02:59,435
esta e sistematicamente 
explorar tais valores.

54
00:02:59,435 --> 00:03:00,935
Aqui estou colocando 
uma grade cinco por cinco.

55
00:03:00,935 --> 00:03:06,070
Na prática, poderíamos ter mais ou menos do que
 uma grade cinco por cinco, mas experimente

56
00:03:06,070 --> 00:03:12,430
este exemplo, todos os 25 pontos e então, 
pegue aqueles hiperparâmetros que funcionarem melhor.

57
00:03:12,430 --> 00:03:18,010
Esta prática funciona bem quando o número de 
hiperparâmetros é relativamente pequeno.

58
00:03:18,010 --> 00:03:19,840
Na aprendizagem profunda, o que tendemos a fazer,

59
00:03:19,840 --> 00:03:21,415
e o que eu recomendo que você faça, ao invés disso,

60
00:03:21,415 --> 00:03:23,975
é escolher os pontos aleatoriamente.

61
00:03:23,975 --> 00:03:27,970
Então, vá em frente e escolha, talvez, 
o mesmo número de pontos,

62
00:03:27,970 --> 00:03:34,590
25 pontos, e então experimente os hiperparâmetros
deste conjunto de pontos escolhidos aleatoriamente.

63
00:03:34,590 --> 00:03:38,350
E a razão para fazer assim, 
é que é muito difícil saber

64
00:03:38,350 --> 00:03:43,040
de antemão quais hiperparâmetros serão os 
mais importantes para o seu problema.

65
00:03:43,040 --> 00:03:44,480
E como visto no slide anterior,

66
00:03:44,480 --> 00:03:47,910
alguns hiperparâmetros são, de fato,
muito mais importantes do que outros.

67
00:03:47,910 --> 00:03:49,190
Então, como exemplo,

68
00:03:49,190 --> 00:03:53,505
digamos que hiperparâmetro 1 seja o alfa, a taxa de aprendizagem.

69
00:03:53,505 --> 00:03:55,175
E num exemplo extremo,

70
00:03:55,175 --> 00:03:58,180
digamos que o hiperparâmetro 2 era o

71
00:03:58,180 --> 00:04:02,730
valor épsilon que temos no denominador
do algoritmo de Adam.

72
00:04:02,730 --> 00:04:07,455
A sua escolha de alfa importa muito
e sua escolha de épsilon quase não faz falta.

73
00:04:07,455 --> 00:04:12,410
Se você testar a partir da grade,
então você realmente terá usado

74
00:04:12,410 --> 00:04:16,300
cinco valores de alfa

75
00:04:16,300 --> 00:04:18,550
e poderá ter encontrado, com 
todos os valores diferentes

76
00:04:18,550 --> 00:04:21,190
de épsilon, basicamente 
a mesma resposta.

77
00:04:21,190 --> 00:04:24,400
Então, após treinar 25 
modelos, usou apenas

78
00:04:24,400 --> 00:04:27,925
cinco valores para a taxa 
de aprendizagem alfa,

79
00:04:27,925 --> 00:04:29,740
a qual é, de fato, realmente importante.

80
00:04:29,740 --> 00:04:33,430
Ao passo que, em contraste,
pegando por amostragem aleatória,

81
00:04:33,430 --> 00:04:37,960
então, você terá tentado 
25 valores distintos de

82
00:04:37,960 --> 00:04:40,390
alfa e, portanto, você estará mais

83
00:04:40,390 --> 00:04:43,690
propenso a encontrar um valor
que, de fato, funcione bem.

84
00:04:43,690 --> 00:04:44,980
Dei este exemplo,

85
00:04:44,980 --> 00:04:47,160
usando apenas dois hiperparâmetros.

86
00:04:47,160 --> 00:04:50,270
Na prática, você pesquisará usando
muito mais do que estes hiperparâmetros,

87
00:04:50,270 --> 00:04:52,000
então, se tiver, digamos,

88
00:04:52,000 --> 00:04:55,080
três hiperparâmetros, acho que
ao invés de pesquisar num quadrado,

89
00:04:55,080 --> 00:05:00,820
você deverá pesquisar num cubo,
onde esta terceira dimensão é o hiperparâmetro 3 e

90
00:05:00,820 --> 00:05:03,010
então, por amostragem, neste

91
00:05:03,010 --> 00:05:05,380
cubo tridimensional, você terá que

92
00:05:05,380 --> 00:05:08,080
testar muitos valores mais de cada um
dos três hiperparâmetros.

93
00:05:08,080 --> 00:05:11,440
E, na prática, você estará pesquisando

94
00:05:11,440 --> 00:05:14,980
sobre um número ainda maior de 
hiperparâmetros do que apenas três e, às vezes,

95
00:05:14,980 --> 00:05:17,160
será difícil saber de antemão
quais deles serão

96
00:05:17,160 --> 00:05:22,120
realmente importantes hiperparâmetros para seu
aplicativo, assim, por amostragem aleatória ao invés de

97
00:05:22,120 --> 00:05:25,390
usar a grade, parece que você terá
uma abordagem mais rica

98
00:05:25,390 --> 00:05:28,085
na exploração do conjunto de valores possíveis

99
00:05:28,085 --> 00:05:31,045
para os hiperparâmetros mais importantes,
quaisquer que eles venham a ser.

100
00:05:31,045 --> 00:05:33,130
Quando pegamos por amostragem
os hiperparâmetros,

101
00:05:33,130 --> 00:05:37,875
outra prática comum é usar
um esquema "do mais abrangente para o mais restrito" (zoom in).

102
00:05:37,875 --> 00:05:42,130
Então, digamos que neste exemplo bidimensional
onde criamos estes pontos,

103
00:05:42,130 --> 00:05:45,600
e talvez, possamos concluir que ele funciona melhor e

104
00:05:45,600 --> 00:05:49,210
talvez, alguns outros pontos ao redor
ele tenda a funcionar realmente melhor,

105
00:05:49,210 --> 00:05:53,530
então, a abordagem do mais abrangente para o mais restrito (zoom in) em

106
00:05:53,530 --> 00:06:00,820
uma região menor de hiperparâmetros
e então, testar mais neste espaço, tornando-o mais denso.

107
00:06:00,820 --> 00:06:02,795
Ou talvez, novamente de forma aleatória,

108
00:06:02,795 --> 00:06:06,690
mas agora, focando mais nos recursos de busca dentro

109
00:06:06,690 --> 00:06:11,265
deste quadrado azul, se estiver suspeitando que 
ele terá os melhores valores

110
00:06:11,265 --> 00:06:13,600
para os hiperparâmetros, talvez nesta região.

111
00:06:13,600 --> 00:06:18,365
Assim, após realizar um teste mais abrangente,
na região do quadrado maior,

112
00:06:18,365 --> 00:06:22,375
e isso ter-lhe apontado para 
focar mais neste quadrado menor.

113
00:06:22,375 --> 00:06:26,105
Você pode, então, testar mais 
densamente neste quadrado menor.

114
00:06:26,105 --> 00:06:29,720
Esse tipo de pesquisa da abrangente para a restrita
é também usado frequentemente.

115
00:06:29,720 --> 00:06:33,565
E ao experimentar estes valores distintos 
nos seus hiperparâmetros, você pode então

116
00:06:33,565 --> 00:06:37,740
pegar qualquer valor que permita que você obtenha o melhor 
do objetivo do seu conjunto de treinamento ("training set")

117
00:06:37,740 --> 00:06:41,230
ou obtenha o melhor com seu conjunto 
de desenvolvimento ("dev set") ou

118
00:06:41,230 --> 00:06:46,660
o quer que você estiver tentando otimizar
no seu processo de pesquisa de hiperparâmetros.

119
00:06:46,660 --> 00:06:48,570
Espero que isso lhe dê uma forma mais

120
00:06:48,570 --> 00:06:51,670
sistemática de organizar o seu processo de
pesquisa de hiperparâmetro.

121
00:06:51,670 --> 00:06:53,200
Então, os dois pontos-chave 
que devem ser lembrados são:

122
00:06:53,200 --> 00:06:55,930
usar amostragem aleatória 
e pesquisar de modo adequado, e

123
00:06:55,930 --> 00:07:01,585
opcionalmente, considere implementar
um processo de mais abrangente para mais restrito (grosso para fino).

124
00:07:01,585 --> 00:07:04,750
Mas, há muita coisa mais sobre pesquisas de 
hiperparâmetros do que o que vimos aqui.

125
00:07:04,750 --> 00:07:07,300
Falaremos mais no próximo vídeo
sobre como escolher

126
00:07:07,300 --> 00:07:10,020
a escala adequada
para testar seus hiperparâmetros.
[Tradução: Carlos Lage | Revisão: Renato Barata Gomes]