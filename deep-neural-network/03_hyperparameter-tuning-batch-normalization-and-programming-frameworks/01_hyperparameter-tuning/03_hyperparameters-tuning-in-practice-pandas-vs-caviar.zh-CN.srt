1
00:00:00,210 --> 00:00:04,380
到现在 我们已经学习了 如何探寻合适的超参数

2
00:00:04,380 --> 00:00:08,090
在对超参数探寻的各种方式进行总结之前

3
00:00:08,090 --> 00:00:11,670
我想和大家分享一些关于如何有规划地

4
00:00:11,670 --> 00:00:14,775
探寻合适超参数的小技巧

5
00:00:14,775 --> 00:00:19,259
现在 深度学习已经被应用在生活中的方方面面

6
00:00:19,259 --> 00:00:24,342
而涉及某一个领域的超参数设置

7
00:00:24,342 --> 00:00:26,670
对于其它领域来说 可能碰巧适合 但也可能不适合

8
00:00:26,670 --> 00:00:31,466
有许多不同领域 可以抽象出很多共同点

9
00:00:31,466 --> 00:00:36,055
例如 为计算机视觉识别领域建立模型的某些思路

10
00:00:36,055 --> 00:00:40,301
比如Confonets和ResNets 我们在之后的课程中会提及

11
00:00:40,301 --> 00:00:42,400
同样适用于演讲领域

12
00:00:42,400 --> 00:00:46,620
我曾见证过在演讲领域发挥作用的构思 同样被成功应用于NLP

13
00:00:46,620 --> 00:00:47,870
等等很多项等等很多项

14
00:00:47,870 --> 00:00:52,613
所以在深度学习领域 有一个良好的发展趋势

15
00:00:52,613 --> 00:00:57,223
不同领域的研究者 通过参考日益增多的其他领域的论文

16
00:00:57,223 --> 00:01:00,180
可以跨领域找到灵感

17
00:01:00,180 --> 00:01:01,580
在超参数设置的问题上

18
00:01:01,580 --> 00:01:06,420
无论是直觉还是思路 都应该与时俱进

19
00:01:06,420 --> 00:01:11,360
哪怕你只研究某一个问题 例如逻辑学 并且

20
00:01:11,360 --> 00:01:15,700
你已经找了到最优超参数设置 用来持续优化你的算法

21
00:01:15,700 --> 00:01:20,465
或许在过去几个月中 你在逐渐改变你的数据集

22
00:01:20,465 --> 00:01:25,070
或者你的数据中心有了更先进的服务器

23
00:01:25,070 --> 00:01:26,380
基于以上这些改变

24
00:01:26,380 --> 00:01:29,500
原先那些被你定义为最优的超参数 很可能已经成为明日黄花

25
00:01:29,500 --> 00:01:32,005
所以我建议至少每隔几个月重新检测或者重新评估一次

26
00:01:32,005 --> 00:01:35,860
你认为最优的超参数

27
00:01:35,860 --> 00:01:39,390
以此来确保这些数值依然是最优解

28
00:01:39,390 --> 00:01:42,150
现在我们进入正题 关于如何探寻超参数的问题

29
00:01:42,150 --> 00:01:46,430
我曾见过两种主流思想

30
00:01:46,430 --> 00:01:50,370
或者说两种不同途径

31
00:01:50,370 --> 00:01:52,920
其中一种场景 是你精心照料某个单一的模型

32
00:01:52,920 --> 00:01:57,671
通常你需要处理一个非常庞大的数据集 但没有充足的计算资源

33
00:01:57,671 --> 00:02:01,390
比如没有很多CPU 没有很多GPU

34
00:02:01,390 --> 00:02:05,100
那你只能一次训练一个或者非常少量的模型

35
00:02:05,100 --> 00:02:11,070
这种情况下 即使枯燥的训练你也要盯着

36
00:02:11,070 --> 00:02:15,180
例如 在某一天 你把参数进行随机初始化

37
00:02:15,180 --> 00:02:16,370
然后开始训练

38
00:02:16,370 --> 00:02:21,626
你会紧盯着你的学习曲线 或许是损失函数J

39
00:02:21,626 --> 00:02:27,333
或许是数据集误差或者其它曲线 在第一天逐渐下降

40
00:02:27,333 --> 00:02:31,300
然后第一天结束 你或许会说 哇 我真是天才

41
00:02:31,300 --> 00:02:35,000
现在我要增加一点学习率 看看是不是能锦上添花

42
00:02:35,000 --> 00:02:37,090
然后这个曲线可能变得更优美了

43
00:02:37,090 --> 00:02:38,870
假设这是第二天的性能表现

44
00:02:38,870 --> 00:02:42,150
两天过后 你可能会说 我果然是个天才

45
00:02:42,150 --> 00:02:46,339
或许我可以尝试一下动量方法或者减少一点学习率

46
00:02:46,339 --> 00:02:47,994
然后我们来到了第三天

47
00:02:47,994 --> 00:02:52,750
就象这样 你每天都在照看你的模型 尝试微调参数

48
00:02:52,750 --> 00:02:55,646
或许某天你突然发现学习率设置得太大了

49
00:02:55,646 --> 00:02:58,649
所以你会换回几天前模型继续跑下去 或者类似这样的做法

50
00:02:58,649 --> 00:03:03,445
你就像一个母亲一样 每天精心照料着你的模型

51
00:03:03,445 --> 00:03:08,080
即使是在好几天甚至几周的训练过程中 都不离不弃

52
00:03:08,080 --> 00:03:12,010
这是通常我们在照料一个模型的时候 都会做的事情

53
00:03:12,010 --> 00:03:17,390
照看着模型 观察性能曲线 耐心地微调学习率

54
00:03:17,390 --> 00:03:21,010
这通常发生在你没有足够的计算资源

55
00:03:21,010 --> 00:03:24,210
同时训练几个模型的情况下

56
00:03:24,210 --> 00:03:28,480
另外一种情形 就是并行训练许多个模型

57
00:03:28,480 --> 00:03:32,010
这种情况下你可能设置一些超参数

58
00:03:32,010 --> 00:03:36,050
然后让模型自己运行一天或几天

59
00:03:36,050 --> 00:03:38,180
然后你可以得到像这样的学习曲线

60
00:03:38,180 --> 00:03:42,180
这可能是损失函数J或者训练的错误率

61
00:03:42,180 --> 00:03:45,670
或者数据集错误率 基本都是你会关心的指标

62
00:03:45,670 --> 00:03:48,636
与此同时 你可能会使用完全不同的超参数

63
00:03:48,636 --> 00:03:50,490
开始运行另一个不同的模型

64
00:03:50,490 --> 00:03:54,030
而第二个模型可能会反馈一条不同的学习曲线

65
00:03:54,030 --> 00:03:55,960
或许看上去是这样的

66
00:03:55,960 --> 00:03:57,510
这条看上去似乎好一些

67
00:03:57,510 --> 00:03:59,980
同时 你可能开始训练第三个模型

68
00:03:59,980 --> 00:04:03,914
然后会生成这样一条学习曲线 看上去这样

69
00:04:03,914 --> 00:04:06,680
或许有些发散的结果 看上去这样 或者其他结果

70
00:04:06,680 --> 00:04:10,280
或许你会同时运行许多不同的模型

71
00:04:10,280 --> 00:04:13,570
在这里我用橙色曲线表示不同的模型

72
00:04:13,570 --> 00:04:16,620
用这样的方式 你就可以尝试不同的超参数设置

73
00:04:16,620 --> 00:04:21,090
这可以让超参数选择变得简单 只要找一个最终结果最好的就行了

74
00:04:21,090 --> 00:04:25,600
在这个例子里 可能这条曲线代表的模型最好

75
00:04:25,600 --> 00:04:27,340
如果要用一个形象的比喻

76
00:04:27,340 --> 00:04:30,760
我认为左边这个形式比较像熊猫

77
00:04:30,760 --> 00:04:33,822
当熊猫产子时 往往数量很稀少

78
00:04:33,822 --> 00:04:35,507
通常一次都只有一个

79
00:04:35,507 --> 00:04:40,350
它们就会投入全部精力 确保自己的孩子能平安成长

80
00:04:40,350 --> 00:04:41,640
非常无微不至

81
00:04:41,640 --> 00:04:44,280
你的模型就像你的熊猫宝宝

82
00:04:44,280 --> 00:04:48,000
右边这个模式 则更像鱼类的行为

83
00:04:48,000 --> 00:04:50,380
为了简单易懂 我称之为鱼子酱策略

84
00:04:50,380 --> 00:04:55,540
有许多鱼 在交配季节能产下一亿枚卵

85
00:04:55,540 --> 00:04:58,960
鱼类的繁殖方式 就是产下数之不尽的卵

86
00:04:58,960 --> 00:05:01,740
然后无需投入太多精力去照看某个鱼卵

87
00:05:01,740 --> 00:05:05,970
只希望其中一个或者一部分能够存活

88
00:05:05,970 --> 00:05:10,340
所以 我觉得这两种方式就是哺乳动物

89
00:05:10,340 --> 00:05:15,030
和鱼类以及爬虫类繁衍方式的区别之处

90
00:05:15,030 --> 00:05:17,980
所以为了增添趣味性从而让这两种方式变得更容易记忆

91
00:05:17,980 --> 00:05:20,210
我将它们分别称之为熊猫模式和鱼子酱模式

92
00:05:20,210 --> 00:05:23,550
那么如何挑选适合你的模式呢？

93
00:05:23,550 --> 00:05:26,500
这取决于你有多少计算资源

94
00:05:26,500 --> 00:05:30,460
如果你有足够的计算机来并行训练很多模型

95
00:05:31,920 --> 00:05:34,670
那不用考虑别的 采用鱼子酱模式就行了

96
00:05:34,670 --> 00:05:37,780
尝试大量不同的超参数 看看结果如何

97
00:05:37,780 --> 00:05:42,520
但是在某些应用领域 例如在线广告设置

98
00:05:42,520 --> 00:05:45,940
以及计算机视觉识别

99
00:05:45,940 --> 00:05:48,670
都有海量的数据和大量的模型需要去训练

100
00:05:48,670 --> 00:05:53,220
而同时训练大量模型是极其困难的事情

101
00:05:53,220 --> 00:05:55,640
这实际上是由行业性质所决定的

102
00:05:55,640 --> 00:06:00,150
然而我看到很多研究小组更喜欢使用熊猫模式

103
00:06:00,150 --> 00:06:03,260
他们对单个模型的处理过程犹如照看一个婴儿

104
00:06:03,260 --> 00:06:08,340
对参数进行微调 尝试能让它有期望的结果

105
00:06:08,340 --> 00:06:12,165
尽管熊猫模式的本质 是训练一个单一模型

106
00:06:12,165 --> 00:06:15,580
然后判断是否可行 结果却可能是二到三周之后

107
00:06:15,580 --> 00:06:19,870
发现要重新初始化一个不同的模型 然后重新照料它

108
00:06:19,870 --> 00:06:23,910
就像熊猫那样 我猜熊猫即使一次只生几个孩子

109
00:06:23,910 --> 00:06:28,800
它一生也会有好多的孩子

110
00:06:28,800 --> 00:06:32,888
希望通过这节课的学习 能让你对于探寻超参数的方式

111
00:06:32,888 --> 00:06:34,170
有一个印象深刻的概念

112
00:06:34,170 --> 00:06:37,090
现在有一种技术

113
00:06:37,090 --> 00:06:41,360
可以让你的神经网络对于超参数的选择上不再那么敏感

114
00:06:41,360 --> 00:06:44,180
这可能没法普及至所有神经网络 但是一旦奏效

115
00:06:44,180 --> 00:06:48,670
能够让超参数探寻过程变得更容易 也可以让训练过程缩短许多

116
00:06:48,670 --> 00:06:50,780
我们下节课见
翻译 | 审阅：Cousera Global Translator Community