1
00:00:00,000 --> 00:00:01,710
안녕하세요, 환영합니다.

2
00:00:01,710 --> 00:00:04,140
여러분은 이제 신경망을 바꾸는데 있어

3
00:00:04,140 --> 00:00:07,415
다양한 하이퍼 파라미터를 세팅하는 과정이 개입되어 있다는 것을 보았습니다.

4
00:00:07,415 --> 00:00:11,155
그러면 최적의 하이퍼 파라미터 세팅을 찾는 방법은 어떻게 알 수 있을까요?

5
00:00:11,155 --> 00:00:13,710
이번 비디오에서는, 가이드라인에 대해서 공유하도록 하겠습니다.

6
00:00:13,710 --> 00:00:18,235
하이퍼 파라미터 튜닝 절차를 시스템적으로 조직화하는 방법과 팁에 대한 가이드라인 말이죠. 

7
00:00:18,235 --> 00:00:20,640
이런 가이드라이은은 이상적으로 여러분이

8
00:00:20,640 --> 00:00:23,760
좋은 하이퍼 파라미터 값에 수렴하도록 효율적인 방안을 제시해 줄 것입니다. 

9
00:00:23,760 --> 00:00:25,929
여러분이 깊은 네트워크를 트레이닝하는 과정에서 힘든 부분중에 한가지는

10
00:00:25,929 --> 00:00:29,250
신경써야하는 많은 양의 하이퍼 파라미터 개수입니다.

11
00:00:29,250 --> 00:00:35,935
이런 하이퍼 파라미터들은 러닝속도를 나타내는 알파값, 모멘텀을 쓰는 경우의

12
00:00:35,935 --> 00:00:41,370
모멘텀 값, 아니면 Adam 최적화 알고리즘의 하이퍼 파라미터인 베타1, 

13
00:00:41,370 --> 00:00:44,185
베타2, 앱실론이 포함되고요, 

14
00:00:44,185 --> 00:00:47,270
층의 개수도 골라야할 수 있죠.

15
00:00:47,270 --> 00:00:50,820
이러한 각각의 다른층마다 hidden unit을 골라야할 수도 있습니다. 

16
00:00:50,820 --> 00:00:55,093
또한 learning rate decay도 이용하고 싶어할 수 있죠. 

17
00:00:55,093 --> 00:00:59,899
이런 경우, 러닝속도 알파 값만 쓰는 것이 아니겠죠. 

18
00:00:59,899 --> 00:01:01,065
그런 후에, 당연한 것일 수도 있지만

19
00:01:01,065 --> 00:01:06,220
미니 배치 사이즈를 골라야할 것입니다.

20
00:01:06,220 --> 00:01:09,990
하이퍼 파라미터를 설정하고 고르는데 있어 특정 하이퍼 파라미터가 더 특별히 중요할 수 있겠죠. 

21
00:01:09,990 --> 00:01:12,235
대부분의 러닝 어플의 경우에서는, 

22
00:01:12,235 --> 00:01:16,015
러닝속도인 알파의 값이 튜닝하는데 있어 가장 중요한 하이퍼 파라미터라고 할 수 있을 것입니다.

23
00:01:16,015 --> 00:01:21,595
알파 값외에도, 몇개의 다른 하이퍼 파라미터 튜닝이 중요할 수 있는데요, 

24
00:01:21,595 --> 00:01:25,040
아마 모멘텀 용어도 중요할 수 있을 것입니다.

25
00:01:25,040 --> 00:01:27,795
제 생각에는 0.9가 적당한 기본값이라고 생각하는데요. 

26
00:01:27,795 --> 00:01:30,700
또한, 최적화 알고리즘이 효율적으로 

27
00:01:30,700 --> 00:01:34,465
운영되도록 저 같은 경우엔, 미니 배치 사이즈도 튜닝할 것입니다.

28
00:01:34,465 --> 00:01:36,985
그리고 저는 hidden unit 또한, 튜닝을 합니다. 

29
00:01:36,985 --> 00:01:39,250
오렌지색으로 동그라미친 것들은, 

30
00:01:39,250 --> 00:01:43,660
여기 이 3개의 하이퍼 파라미터는 러닝속도

31
00:01:43,660 --> 00:01:46,060
알파 값 다음으로 중요한 것입니다. 

32
00:01:46,060 --> 00:01:49,060
그리고 세번째로 중요한 것은, 

33
00:01:49,060 --> 00:01:51,925
층의 개수도 아주 큰 차이를 불러올 수 있고, 

34
00:01:51,925 --> 00:01:55,000
learning rate decay도 마찬가지입니다.

35
00:01:55,000 --> 00:01:58,870
그리고 Adam 알고리즘을 이용하는 경우에는, 저는 거의 항상 베타1, 

36
00:01:58,870 --> 00:02:00,434
베타2, 그리고 앱실론은 튜닝하지 않습니다.

37
00:02:00,434 --> 00:02:01,930
거의 항상 0.9, 

38
00:02:01,930 --> 00:02:08,570
0.999, 10의 마이너스 8승 값으로 사용하는데요, 여러분이 원하시면 이 값들도
튜닝하실 수 있습니다. 

39
00:02:08,570 --> 00:02:12,130
방금 이야기한 내용이 여러분에게 대략적으로 하이퍼 파라미터에 대한 감을 주고, 

40
00:02:12,130 --> 00:02:16,463
어떤 것이 더 중요한지 알려줬으면 좋겠습니다. 예를 들어, 

41
00:02:16,463 --> 00:02:19,005
알파가 가장 중요한 것이 되겠고요, 

42
00:02:19,005 --> 00:02:22,270
그 다음으로 오렌지색으로 동그라미 친 것이 중요하고, 

43
00:02:22,270 --> 00:02:25,235
그 다음은 보라색으로 동그라미 친 것이 중요할 것입니다.

44
00:02:25,235 --> 00:02:27,760
하지만 이것은 엄격한 규칙은 아닌데요, 

45
00:02:27,760 --> 00:02:30,490
제 생각에는 딥러닝 종사자분들중에 

46
00:02:30,490 --> 00:02:33,670
저와 반대하는 분들도 계시거나 다른 직관적인 시각을 가지고 계신 분이 있으실 것입니다.

47
00:02:33,670 --> 00:02:37,240
그렇다면, 여러분이 하이퍼 파라미터 값을 튜닝하려고 하는 경우에, 

48
00:02:37,240 --> 00:02:40,060
탐색하기 위한 하이퍼 파라미터값은 어떻게 설정할까요?

49
00:02:40,060 --> 00:02:42,845
머신 러닝 알고리즘의 초기에는, 

50
00:02:42,845 --> 00:02:44,660
2개의 하이퍼 파라미터가 있는 경우에, 

51
00:02:44,660 --> 00:02:47,662
이 경우에 하이퍼 파라미터 1과 하이퍼 파라미터 2 라고 하겠습니다.

52
00:02:47,662 --> 00:02:53,380
점들을 이런식으로 격자판 형식으로 샘플링하는 경우가 많았습니다. 

53
00:02:53,380 --> 00:02:59,435
그리고 이러한 값들을 탐색해나가는 것입니다.

54
00:02:59,435 --> 00:03:00,935
여기서는 5 x 5 격자판 형식으로 설정했는데요, 

55
00:03:00,935 --> 00:03:06,070
실제로는 더 큰 격자판이나 이 것보다 작은 격자판으로 설정할 수도 있습니다.

56
00:03:06,070 --> 00:03:12,430
이 경우에서는 25개의 포인트 지점이 있는데요, 이중에서 가장 잘 작동하는 하이퍼 파라미터를 고르는 것입니다.

57
00:03:12,430 --> 00:03:18,010
이런 방법은 하이퍼 파라미터 의 개수가 비교적 많지 않은 경우에 잘 작동하는 편입니다.

58
00:03:18,010 --> 00:03:19,840
딥러닝의 경우, 저희는 보통, , 

59
00:03:19,840 --> 00:03:21,415
제가 추천드리는 다른 방식은, 

60
00:03:21,415 --> 00:03:23,975
지점들을 임의로 선정하는 것입니다.

61
00:03:23,975 --> 00:03:27,970
그러면 포인트의 개수는 똑같이 해서 진행합니다. 아시겠죠?

62
00:03:27,970 --> 00:03:34,590
25개를 지정한 다음에, 이렇게 임의로 지정된 포인트에서 임의로 하이퍼 파라미터를 골라
시도해봅니다.

63
00:03:34,590 --> 00:03:38,350
이렇게 하는 이유는, 여러분이 이 단계에서는

64
00:03:38,350 --> 00:03:43,040
어떤 하이퍼 파라미터들이 문제에서 가장 중요할지 미리 알 수 없기 때문입니다.

65
00:03:43,040 --> 00:03:44,480
이전 슬라이드에서 보셨듯이, 

66
00:03:44,480 --> 00:03:47,910
특정 hyperparameter들은 다른 것들보다 더 중요할 수 있습니다.

67
00:03:47,910 --> 00:03:49,190
예를 들어서, 

68
00:03:49,190 --> 00:03:53,505
 하이퍼 파라미터 1의 값이 러닝속도를 나타내는 알파값이였다고 해봅시다.

69
00:03:53,505 --> 00:03:55,175
그리고 극단적인 경우로, 

70
00:03:55,175 --> 00:03:58,180
 하이퍼 파라미터 2는 

71
00:03:58,180 --> 00:04:02,730
Adam 알고리즘의 분모에 속하는 앱실론 값이였다고 해보겠습니다.

72
00:04:02,730 --> 00:04:07,455
이런 경우에는, 알파값이 여러분에게 매주 중요하고, 앱실론의 선택은 거의 중요하지 않겠죠. 

73
00:04:07,455 --> 00:04:12,410
격자판에 샘플링을 진행하면, 이런식으로

74
00:04:12,410 --> 00:04:16,300
5개의 알파값을 나열하겠죠. 

75
00:04:16,300 --> 00:04:18,550
그리고 다양한 앱실론의

76
00:04:18,550 --> 00:04:21,190
값이 동일한 답을 줄수 있습니다.

77
00:04:21,190 --> 00:04:24,400
그러면 여러분은 25개의 모델을 트레이닝 

78
00:04:24,400 --> 00:04:27,925
시켰는데 오로지 5개의 러닝속도 알파값만 

79
00:04:27,925 --> 00:04:29,740
시도할 수 있었습니다. 중요한 값인데도 말이죠. 

80
00:04:29,740 --> 00:04:33,430
반대로, 여러분이 샘플링을 무작위로 진행하였을때는, 

81
00:04:33,430 --> 00:04:37,960
각각의 25개의 러닝속도 알파값을

82
00:04:37,960 --> 00:04:40,390
시도해보시게 됐을 것입니다. 그러므로 

83
00:04:40,390 --> 00:04:43,690
잘 작동하는 값을 찾을 확률이 늘어나는 것이죠. 

84
00:04:43,690 --> 00:04:44,980
저는 예제를 오로지 2개의

85
00:04:44,980 --> 00:04:47,160
 하이퍼 파라미터만 사용하여 설명드렸는데요, 

86
00:04:47,160 --> 00:04:50,270
실제로는 여러분이 어 많은 숫자의 하이퍼 파라미터를 사용하여 그 값을 찾을 수 있습니다.

87
00:04:50,270 --> 00:04:52,000
예를 들어, 

88
00:04:52,000 --> 00:04:55,080
3개의 하이퍼 파라미터값일 수도 있겠죠, 이런 경우레는 사각형이 아니라

89
00:04:55,080 --> 00:05:00,820
여기 이 다이멘션이 하이퍼 파라미터 3인 정육면체에서 값을 찾을 것입니다.

90
00:05:00,820 --> 00:05:03,010
그러면, 샘플링을

91
00:05:03,010 --> 00:05:05,380
이런 3차원의 정육면체에서 진행하여 

92
00:05:05,380 --> 00:05:08,080
이런 하이퍼 파라미터들을 두고 더 많은 값들을 시도할 수 있습니다.

93
00:05:08,080 --> 00:05:11,440
실제로는 여러분이 이것보다

94
00:05:11,440 --> 00:05:14,980
더 많은 수의 하이퍼 파라미터에서 서칭을 진행할 수도 있습니다. 

95
00:05:14,980 --> 00:05:17,160
하지만 여러분이 미리 어떤 하이퍼 파라미터가 

96
00:05:17,160 --> 00:05:22,120
작업하는 어플에서 더 중요한지 알아내기 어렵기 때문에, 샘플링 작업을

97
00:05:22,120 --> 00:05:25,390
격자판식으로 하는 것보다 무작위로 샘플링을 하는 방식이 

98
00:05:25,390 --> 00:05:28,085
대부분의 하이퍼 파라미터에 대해서 

99
00:05:28,085 --> 00:05:31,045
더 많은 값을 탐색할 수 있기 때문에, 그것들의 결과가 어떻든 더 광범위하게 진행할 수 있습니다.

100
00:05:31,045 --> 00:05:33,130
 하이퍼 파라미터를 샘플링하는 경우에, 

101
00:05:33,130 --> 00:05:37,875
흔한 진행 방식은 coarse to fine sampling scheme 입니다.

102
00:05:37,875 --> 00:05:42,130
이렇게 생긴 2차원 예제에서, 여기 포인트들을 샘플링한다고 해보겠습니다.

103
00:05:42,130 --> 00:05:45,600
그리하여, 여기 이 포인트가 가장 잘 작동한다고 알아냈다고 해봅시다. 

104
00:05:45,600 --> 00:05:49,210
그리고 그 주위의 멱채의 포인트들도 말이죠. 

105
00:05:49,210 --> 00:05:53,530
그러면 마지막 진행절차 도중에, 여러분은 여기 이 조금한 하이퍼 파라미터 부분을

106
00:05:53,530 --> 00:06:00,820
줌인하여 여기 이 공간에대해 조금도 밀도있게 샘플링을 진행합니다.

107
00:06:00,820 --> 00:06:02,795
또다시, 임의의 샘플링을 진행할 수도 있겠죠. 

108
00:06:02,795 --> 00:06:06,690
일단 여기 안쪽에서 서칭하는 것에 집중한다고 하면, 

109
00:06:06,690 --> 00:06:11,265
여러분이 생각하기에 여기 이 부분이 가장 최적의 하이퍼 파라미터 세팅이 있다고 하면, 

110
00:06:11,265 --> 00:06:13,600
여기 파란색 사각형 내부에서 말이죠. 

111
00:06:13,600 --> 00:06:18,365
여기 사각형 전체에서 coarse 샘플링을 진행하면, 

112
00:06:18,365 --> 00:06:22,375
그 다음으로 더 조금한 사각형에 집중하라고할 것입니다.

113
00:06:22,375 --> 00:06:26,105
그러면 여기 이 조금한 사각형에서 밀도있게 샘플링을 진행할 수 있죠.

114
00:06:26,105 --> 00:06:29,720
이런 방식의 coarse to fine 서칭은 자주 이용되는 방법인데요, 

115
00:06:29,720 --> 00:06:33,565
이러한 다양한 하이퍼 파라미터 값들을 시도하여, 

116
00:06:33,565 --> 00:06:37,740
여러분의 training set objective나 development set 또는 

117
00:06:37,740 --> 00:06:41,230
여러분이 최적화하려고하는 하이퍼 파라미터 부분에서 

118
00:06:41,230 --> 00:06:46,660
가장 좋은 값을 찾게 해줍니다.

119
00:06:46,660 --> 00:06:48,570
오늘 배운 내용을 통해 여러분이 

120
00:06:48,570 --> 00:06:51,670
 하이퍼 파라미터 서칭 단계에서 더 시스템적으로 조직화시킬 수 있는 계기가 되었길 바랍니다.

121
00:06:51,670 --> 00:06:53,200
2가지 염두하실 점은, 

122
00:06:53,200 --> 00:06:55,930
임의의 샘플링을 진행하고, 충분한 서칭을 진행합니다.

123
00:06:55,930 --> 00:07:01,585
그리고 선택적으로 coarse to fine 서칭 단계를 도입할지 고려합니다. 

124
00:07:01,585 --> 00:07:04,750
하지만 이것말고소 하이퍼 파라미터 서칭에는 다른 중요한 요소들이 있는데요.

125
00:07:04,750 --> 00:07:07,300
다음 비디오를 통해 여러분이 하이퍼 파라미터를 샘플하는데 있어

126
00:07:07,300 --> 00:07:10,020
올바른 scale을 찾는 방법을 다루도록 하겠습니다.