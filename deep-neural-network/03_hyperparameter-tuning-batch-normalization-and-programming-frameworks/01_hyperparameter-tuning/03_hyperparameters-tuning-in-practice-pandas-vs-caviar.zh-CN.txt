到现在 我们已经学习了 如何探寻合适的超参数 在对超参数探寻的各种方式进行总结之前 我想和大家分享一些关于如何有规划地 探寻合适超参数的小技巧 现在 深度学习已经被应用在生活中的方方面面 而涉及某一个领域的超参数设置 对于其它领域来说 可能碰巧适合 但也可能不适合 有许多不同领域 可以抽象出很多共同点 例如 为计算机视觉识别领域建立模型的某些思路 比如Confonets和ResNets 我们在之后的课程中会提及 同样适用于演讲领域 我曾见证过在演讲领域发挥作用的构思 同样被成功应用于NLP 等等很多项等等很多项 所以在深度学习领域 有一个良好的发展趋势 不同领域的研究者 通过参考日益增多的其他领域的论文 可以跨领域找到灵感 在超参数设置的问题上 无论是直觉还是思路 都应该与时俱进 哪怕你只研究某一个问题 例如逻辑学 并且 你已经找了到最优超参数设置 用来持续优化你的算法 或许在过去几个月中 你在逐渐改变你的数据集 或者你的数据中心有了更先进的服务器 基于以上这些改变 原先那些被你定义为最优的超参数 很可能已经成为明日黄花 所以我建议至少每隔几个月重新检测或者重新评估一次 你认为最优的超参数 以此来确保这些数值依然是最优解 现在我们进入正题 关于如何探寻超参数的问题 我曾见过两种主流思想 或者说两种不同途径 其中一种场景 是你精心照料某个单一的模型 通常你需要处理一个非常庞大的数据集 但没有充足的计算资源 比如没有很多CPU 没有很多GPU 那你只能一次训练一个或者非常少量的模型 这种情况下 即使枯燥的训练你也要盯着 例如 在某一天 你把参数进行随机初始化 然后开始训练 你会紧盯着你的学习曲线 或许是损失函数J 或许是数据集误差或者其它曲线 在第一天逐渐下降 然后第一天结束 你或许会说 哇 我真是天才 现在我要增加一点学习率 看看是不是能锦上添花 然后这个曲线可能变得更优美了 假设这是第二天的性能表现 两天过后 你可能会说 我果然是个天才 或许我可以尝试一下动量方法或者减少一点学习率 然后我们来到了第三天 就象这样 你每天都在照看你的模型 尝试微调参数 或许某天你突然发现学习率设置得太大了 所以你会换回几天前模型继续跑下去 或者类似这样的做法 你就像一个母亲一样 每天精心照料着你的模型 即使是在好几天甚至几周的训练过程中 都不离不弃 这是通常我们在照料一个模型的时候 都会做的事情 照看着模型 观察性能曲线 耐心地微调学习率 这通常发生在你没有足够的计算资源 同时训练几个模型的情况下 另外一种情形 就是并行训练许多个模型 这种情况下你可能设置一些超参数 然后让模型自己运行一天或几天 然后你可以得到像这样的学习曲线 这可能是损失函数J或者训练的错误率 或者数据集错误率 基本都是你会关心的指标 与此同时 你可能会使用完全不同的超参数 开始运行另一个不同的模型 而第二个模型可能会反馈一条不同的学习曲线 或许看上去是这样的 这条看上去似乎好一些 同时 你可能开始训练第三个模型 然后会生成这样一条学习曲线 看上去这样 或许有些发散的结果 看上去这样 或者其他结果 或许你会同时运行许多不同的模型 在这里我用橙色曲线表示不同的模型 用这样的方式 你就可以尝试不同的超参数设置 这可以让超参数选择变得简单 只要找一个最终结果最好的就行了 在这个例子里 可能这条曲线代表的模型最好 如果要用一个形象的比喻 我认为左边这个形式比较像熊猫 当熊猫产子时 往往数量很稀少 通常一次都只有一个 它们就会投入全部精力 确保自己的孩子能平安成长 非常无微不至 你的模型就像你的熊猫宝宝 右边这个模式 则更像鱼类的行为 为了简单易懂 我称之为鱼子酱策略 有许多鱼 在交配季节能产下一亿枚卵 鱼类的繁殖方式 就是产下数之不尽的卵 然后无需投入太多精力去照看某个鱼卵 只希望其中一个或者一部分能够存活 所以 我觉得这两种方式就是哺乳动物 和鱼类以及爬虫类繁衍方式的区别之处 所以为了增添趣味性从而让这两种方式变得更容易记忆 我将它们分别称之为熊猫模式和鱼子酱模式 那么如何挑选适合你的模式呢？ 这取决于你有多少计算资源 如果你有足够的计算机来并行训练很多模型 那不用考虑别的 采用鱼子酱模式就行了 尝试大量不同的超参数 看看结果如何 但是在某些应用领域 例如在线广告设置 以及计算机视觉识别 都有海量的数据和大量的模型需要去训练 而同时训练大量模型是极其困难的事情 这实际上是由行业性质所决定的 然而我看到很多研究小组更喜欢使用熊猫模式 他们对单个模型的处理过程犹如照看一个婴儿 对参数进行微调 尝试能让它有期望的结果 尽管熊猫模式的本质 是训练一个单一模型 然后判断是否可行 结果却可能是二到三周之后 发现要重新初始化一个不同的模型 然后重新照料它 就像熊猫那样 我猜熊猫即使一次只生几个孩子 它一生也会有好多的孩子 希望通过这节课的学习 能让你对于探寻超参数的方式 有一个印象深刻的概念 现在有一种技术 可以让你的神经网络对于超参数的选择上不再那么敏感 这可能没法普及至所有神经网络 但是一旦奏效 能够让超参数探寻过程变得更容易 也可以让训练过程缩短许多 我们下节课见
翻译 | 审阅：Cousera Global Translator Community