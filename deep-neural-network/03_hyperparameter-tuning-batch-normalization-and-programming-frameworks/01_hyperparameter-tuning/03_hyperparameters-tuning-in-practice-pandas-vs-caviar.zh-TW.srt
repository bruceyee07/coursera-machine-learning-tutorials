1
00:00:00,210 --> 00:00:04,380
你現在聽過很多尋找
好的超參數的方法了

2
00:00:04,380 --> 00:00:08,090
在結束這部份的討論之前

3
00:00:08,090 --> 00:00:11,670
我想分享幾個提示和小技巧

4
00:00:11,670 --> 00:00:14,775
關於如何組織你尋找超參數的流程

5
00:00:14,775 --> 00:00:19,259
現今的深度學習已經應用在很多領域

6
00:00:19,259 --> 00:00:24,342
而在某個領域中對於超參數設定的經驗

7
00:00:24,342 --> 00:00:26,670
不一定能套用到另一個領域

8
00:00:26,670 --> 00:00:31,466
在不同的應用領域常有很多的經驗交流

9
00:00:31,466 --> 00:00:36,055
例如，我看過一些在電腦視覺社群提出的點子

10
00:00:36,055 --> 00:00:40,301
像是 ConvNets 或 ResNet
－我們會在後面的課程談到

11
00:00:40,301 --> 00:00:42,400
能成功應用在語音上

12
00:00:42,400 --> 00:00:46,620
我看過最初在語音領域的想法，
能成功運用在自然語言處理

13
00:00:46,620 --> 00:00:47,870
等等

14
00:00:47,870 --> 00:00:52,613
所以在深度學習的演進中，有件好事是不同領域的人

15
00:00:52,613 --> 00:00:57,223
會去讀越來越多其他領域的論文

16
00:00:57,223 --> 00:01:00,180
以尋找靈感，相互交流

17
00:01:00,180 --> 00:01:01,580
但是，對於

18
00:01:01,580 --> 00:01:06,420
設定超參數這方面，我發現這樣的經驗會碰壁

19
00:01:06,420 --> 00:01:11,360
就算你只是處理一個問題，例如物流，你可能發現

20
00:01:11,360 --> 00:01:15,700
一組好的超參數，然後持續開發你的演算法

21
00:01:15,700 --> 00:01:20,465
抑或你的資料可能在過去幾個月逐漸改變

22
00:01:20,465 --> 00:01:25,070
或者只是升級你在資料中心的伺服器

23
00:01:25,070 --> 00:01:26,380
因為這些改變，

24
00:01:26,380 --> 00:01:29,500
原本最好的超參數設定
可能已不再美好

25
00:01:29,500 --> 00:01:32,005
所以我建議可能重新測試

26
00:01:32,005 --> 00:01:35,860
或是重新評估你的超參數
至少每幾個月一次

27
00:01:35,860 --> 00:01:39,390
來確保你對其值仍然滿意

28
00:01:39,390 --> 00:01:42,150
最後，對於大家如何尋找

29
00:01:42,150 --> 00:01:46,430
超參數，我發現主要有兩類的想法

30
00:01:46,430 --> 00:01:50,370
或者說，有兩種不同的方法大家會去執行

31
00:01:50,370 --> 00:01:52,920
一種方法是你照顧一個模型

32
00:01:52,920 --> 00:01:57,671
通常當你有很大量的資料，但並沒有很多

33
00:01:57,671 --> 00:02:01,390
運算的資源，沒有很多CPU和GPU，
基本上你只能負荷

34
00:02:01,390 --> 00:02:05,100
一次訓練一個模型，或是很少個模型

35
00:02:05,100 --> 00:02:11,070
在這樣的情況下，你可能在訓練這個模型的途中，
慢慢地照顧他

36
00:02:11,070 --> 00:02:15,180
例如，在最一開始，你可能會隨便給定你的參數

37
00:02:15,180 --> 00:02:16,370
然後就開始訓練

38
00:02:16,370 --> 00:02:21,626
然後你漸漸看著你的學習曲線
可能是成本函數J，或是

39
00:02:21,626 --> 00:02:27,333
你的開發集錯誤率或是其他的，在第一天慢慢地下降

40
00:02:27,333 --> 00:02:31,300
然後第一天後，你可能會說，哇，
這看起來學得超棒的

41
00:02:31,300 --> 00:02:35,000
我要試試增加一點學習率，
看會怎麼樣

42
00:02:35,000 --> 00:02:37,090
然後也許做得更好

43
00:02:37,090 --> 00:02:38,870
這是你第二天的成效

44
00:02:38,870 --> 00:02:42,150
經過兩天後，你說，ok
目前表現仍然不錯

45
00:02:42,150 --> 00:02:46,339
也許我會加點momentum項，或是少些學習率

46
00:02:46,339 --> 00:02:47,994
然後就這樣進入第三天

47
00:02:47,994 --> 00:02:52,750
每天你觀察，試著上下微調你的參數

48
00:02:52,750 --> 00:02:55,646
也或許某天，你發現你的學習率太高了

49
00:02:55,646 --> 00:02:58,649
所以你可能回到前一天的模型，等等等

50
00:02:58,649 --> 00:03:03,445
不過某種角度你就像照顧這個模型
一次一天，在這訓練途中

51
00:03:03,445 --> 00:03:08,080
可能長達若干天或好幾個禮拜

52
00:03:08,080 --> 00:03:12,010
所以這是其中一個方法，照顧一個模型

53
00:03:12,010 --> 00:03:17,390
監視著成效，有耐心地上下調整學習率

54
00:03:17,390 --> 00:03:21,010
不過這通常是由於你沒有足夠的計算能力

55
00:03:21,010 --> 00:03:24,210
去同時訓練多個模型

56
00:03:24,210 --> 00:03:28,480
另一個方式則是同時訓練很多個模型

57
00:03:28,480 --> 00:03:32,010
你可能有某個超參數的組合

58
00:03:32,010 --> 00:03:36,050
就放下去讓他跑，跑個一天或好幾天

59
00:03:36,050 --> 00:03:38,180
然後得到了類似像這樣的學習曲線

60
00:03:38,180 --> 00:03:42,180
這可能是成本函數的圖，或是訓練誤差

61
00:03:42,180 --> 00:03:45,670
或是開發集的誤差，某個你追蹤的度量

62
00:03:45,670 --> 00:03:48,636
然後同一時間，你可能會開練另一個模型

63
00:03:48,636 --> 00:03:50,490
用不同一組超參數的設置

64
00:03:50,490 --> 00:03:54,030
那麼，你的第二個模型可能產生不同的學習曲線

65
00:03:54,030 --> 00:03:55,960
可能長得像這樣

66
00:03:55,960 --> 00:03:57,510
我會說這條看起來比較優

67
00:03:57,510 --> 00:03:59,980
在此同時，你可能會訓練第三個模型

68
00:03:59,980 --> 00:04:03,914
可能會產生像這樣的學習曲線
然後可能有另一條

69
00:04:03,914 --> 00:04:06,680
可能他發散了，所以看起來像這樣，等等

70
00:04:06,680 --> 00:04:10,280
或者你可能同時就訓練非常多的模型

71
00:04:10,280 --> 00:04:13,570
這些橘線代表著不同的模型

72
00:04:13,570 --> 00:04:16,620
這樣就可以嘗試很多不同組的超參數

73
00:04:16,620 --> 00:04:21,090
然後最後就很快挑一組表現最好的

74
00:04:21,090 --> 00:04:25,600
在這個例子，看起來可能是這條線最好

75
00:04:25,600 --> 00:04:27,340
所以打個比喻

76
00:04:27,340 --> 00:04:30,760
我稱左邊這個方法叫熊貓方法

77
00:04:30,760 --> 00:04:33,822
當熊貓養小孩時，他們只有很少的小孩

78
00:04:33,822 --> 00:04:35,507
通常一次只有一隻

79
00:04:35,507 --> 00:04:40,350
然後他們花了非常多的心力
確保熊貓寶寶生存下去

80
00:04:40,350 --> 00:04:41,640
所以這真的像照顧小孩一樣

81
00:04:41,640 --> 00:04:44,280
一個模型，或是一隻熊貓寶寶

82
00:04:44,280 --> 00:04:48,000
而右邊的方法比較像魚

83
00:04:48,000 --> 00:04:50,380
我稱之為魚子醬策略

84
00:04:50,380 --> 00:04:55,540
有些魚會在交配季節產下一億顆魚卵

85
00:04:55,540 --> 00:04:58,960
不過呢魚繁殖的方式是他們產下很多卵

86
00:04:58,960 --> 00:05:01,740
不花太多心思在任何一個上

87
00:05:01,740 --> 00:05:05,970
就希望其中某一個或某一些能過得好

88
00:05:05,970 --> 00:05:10,340
所以我猜，這就是哺乳動物繁殖與

89
00:05:10,340 --> 00:05:15,030
魚或爬蟲動物繁殖的不同之處

90
00:05:15,030 --> 00:05:17,980
不過呢，我就稱他們為「熊貓法」跟「魚子醬法」

91
00:05:17,980 --> 00:05:20,210
因為這好玩又好記

92
00:05:20,210 --> 00:05:23,550
所以要如何從兩者中選擇，是和

93
00:05:23,550 --> 00:05:26,500
你有多少計算資源有關

94
00:05:26,500 --> 00:05:30,460
如果你有夠多的電腦可以同時訓練很多個模型

95
00:05:31,920 --> 00:05:34,670
那就盡量採取魚子醬法，

96
00:05:34,670 --> 00:05:37,780
嘗試多組不同的超參數，看看哪個成功

97
00:05:37,780 --> 00:05:42,520
不過在某些應用領域，
我在某些網路廣告的狀況看過

98
00:05:42,520 --> 00:05:45,940
或是電腦視覺的應用，他們有

99
00:05:45,940 --> 00:05:48,670
太大量的資料，要訓練的模型實在

100
00:05:48,670 --> 00:05:53,220
太大，以至於要同時訓練很多個模型
是很困難的

101
00:05:53,220 --> 00:05:55,640
當然，這終究和應用有關，不過

102
00:05:55,640 --> 00:06:00,150
我發覺那幾個領域用熊貓法
用得比較多一些

103
00:06:00,150 --> 00:06:03,260
也就是你一直照顧一個模型，

104
00:06:03,260 --> 00:06:08,340
參數調上調下，試著讓這個模型成功

105
00:06:08,340 --> 00:06:12,165
即便如此，當然就算是貓熊法
訓練了一個模型

106
00:06:12,165 --> 00:06:15,580
然後觀察他成不成功，可能兩三禮拜後

107
00:06:15,580 --> 00:06:19,870
或許我應該開始另一個不同的模型，
轉而照顧那一個

108
00:06:19,870 --> 00:06:23,910
就像即使是熊貓，我猜，也可能
在他們的一生也會有多個小孩

109
00:06:23,910 --> 00:06:28,800
就算一段期間內
只有一個或很少個

110
00:06:28,800 --> 00:06:32,888
總之，希望這能給你一些概念，
如何進行超參數

111
00:06:32,888 --> 00:06:34,170
尋找的流程

112
00:06:34,170 --> 00:06:37,090
那，其實呢還有另一個技巧可以

113
00:06:37,090 --> 00:06:41,360
讓你的神經網路能比較不受超參數選擇的影響

114
00:06:41,360 --> 00:06:44,180
這不見得能用在所有的神經網路，
但當可以時，這能

115
00:06:44,180 --> 00:06:48,670
讓超參數的搜尋更簡單，
也能讓模型訓練快很多

116
00:06:48,670 --> 00:06:50,780
讓我們在下部影片談談這技巧