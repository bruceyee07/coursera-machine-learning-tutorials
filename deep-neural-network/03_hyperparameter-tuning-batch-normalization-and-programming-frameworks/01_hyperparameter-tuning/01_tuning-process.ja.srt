1
00:00:00,000 --> 00:00:01,710
さて ここまでで

2
00:00:01,710 --> 00:00:04,140
ニューラルネットワークを学習するには

3
00:00:04,140 --> 00:00:07,415
様々なハイパーパラメタの設定を
しなければならないことがわかりました

4
00:00:07,415 --> 00:00:11,155
ではそのハイパーパラメタの良い設定を見つけるには
どうすればよいでしょうか

5
00:00:11,155 --> 00:00:13,710
このビデオではそのガイドラインやコツなど

6
00:00:13,710 --> 00:00:18,235
システマティックにハイパーパラメタを
チューニングする方法についてご紹介します

7
00:00:18,235 --> 00:00:20,640
これによって良いハイパーパラメタ設定に

8
00:00:20,640 --> 00:00:23,760
より効率的にたどり着けるようになればと思います

9
00:00:23,760 --> 00:00:25,929
ディープネットワークを学習する時の悩みとして

10
00:00:25,929 --> 00:00:29,250
ハイパーパラメタの数の多さが挙げられます

11
00:00:29,250 --> 00:00:35,935
例えば学習率α
モメンタムを使うのであればβ

12
00:00:35,935 --> 00:00:41,370
Adam最適化で使われる方の
ハイパーパラメタであれば

13
00:00:41,370 --> 00:00:44,185
β1 β2 ε

14
00:00:44,185 --> 00:00:47,270
レイヤー数もあるでしょうし

15
00:00:47,270 --> 00:00:50,820
隠れ層ごとにユニット数も選ぶ必要があります

16
00:00:50,820 --> 00:00:55,093
また減衰をあつかうことで

17
00:00:55,093 --> 00:00:59,899
変化する学習率を使うこともあります

18
00:00:59,899 --> 00:01:01,065
当然

19
00:01:01,065 --> 00:01:06,220
ミニバッチサイズも選ばなくてはなりません

20
00:01:06,220 --> 00:01:09,990
実はこれらハイパーパラメタの重要度は
それぞれ異なります

21
00:01:09,990 --> 00:01:12,235
ほとんどのアプリケーションでは

22
00:01:12,235 --> 00:01:16,015
学習率αがもっとも重要な
ハイパーパラメタだと言っていいでしょう

23
00:01:16,015 --> 00:01:21,595
α以外で私が次にチューニングするならば

24
00:01:21,595 --> 00:01:25,040
おそらくモメンタム項のβ

25
00:01:25,040 --> 00:01:27,795
0.9なんかは良いデフォルト値ですね

26
00:01:27,795 --> 00:01:30,700
他にはミニバッチサイズを変えて

27
00:01:30,700 --> 00:01:34,465
最適化アルゴリズムが効率的に走るようにしたり

28
00:01:34,465 --> 00:01:36,985
また隠れ層のユニット数などもよくいじります

29
00:01:36,985 --> 00:01:39,250
これらオレンジ色で囲った3つは

30
00:01:39,250 --> 00:01:43,660
学習率αに次いで

31
00:01:43,660 --> 00:01:46,060
大事なものと言えるでしょう

32
00:01:46,060 --> 00:01:49,060
それらをいじった後としては

33
00:01:49,060 --> 00:01:51,925
レイヤー数は大きな違いを生むことがありますし

34
00:01:51,925 --> 00:01:55,000
学習率減衰などもそうです

35
00:01:55,000 --> 00:01:58,870
またAdamアルゴリズムを使うとき
それらのハイパーパラメタ

36
00:01:58,870 --> 00:02:00,434
β1 β2 εのチューニングをすることはまずありません

37
00:02:00,434 --> 00:02:01,930
やってみても構いませんが

38
00:02:01,930 --> 00:02:08,570
私はほぼ必ず0.9 0.999 10の-8乗を使います

39
00:02:08,570 --> 00:02:12,130
さてここまでで
ハイパーパラメタの重要度の違いについて

40
00:02:12,130 --> 00:02:16,463
ざっと理解できたでしょうか

41
00:02:16,463 --> 00:02:19,005
おそらくαが最も重要で

42
00:02:19,005 --> 00:02:22,270
次にオレンジ色で囲んだもの

43
00:02:22,270 --> 00:02:25,235
更に紫色で囲んだものと続きます

44
00:02:25,235 --> 00:02:27,760
ただし絶対ではありませんし

45
00:02:27,760 --> 00:02:30,490
ディープラーニングを実践されている方でも

46
00:02:30,490 --> 00:02:33,670
違った考えの方がいらっしゃるでしょう

47
00:02:33,670 --> 00:02:37,240
さて ハイパーパラメタをセットでチューニングする時

48
00:02:37,240 --> 00:02:40,060
どうやって探索すべき値のセットを
選べばいいでしょうか

49
00:02:40,060 --> 00:02:42,845
初期の機械学習アルゴリズムでは

50
00:02:42,845 --> 00:02:44,660
例えば2つのハイパーパラメタ

51
00:02:44,660 --> 00:02:47,662
ハイパーパラメタ1 ハイパーパラメタ2
があった場合

52
00:02:47,662 --> 00:02:53,380
典型的な手法として
このように格子状に点を抽出し

53
00:02:53,380 --> 00:02:59,435
システマティックにこれらの値を探索します

54
00:02:59,435 --> 00:03:00,935
ここでは例として5x5にしました

55
00:03:00,935 --> 00:03:06,070
実際はそれ以外の数もありえますが
この例では全25点の値を試し

56
00:03:06,070 --> 00:03:12,430
1番性能の良かったハイパーパラメタを採用します

57
00:03:12,430 --> 00:03:18,010
この手法はハイパーパラメタの種類が
比較的少ない時は大丈夫なのですが

58
00:03:18,010 --> 00:03:19,840
ディープラーニングでは
どちらかというと他の手法を

59
00:03:19,840 --> 00:03:21,415
私は使いますし皆さんにもおすすめします

60
00:03:21,415 --> 00:03:23,975
ランダムに点を選ぶのです

61
00:03:23,975 --> 00:03:27,970
どんどん選んでいきましょう
同じ数にしましょうか

62
00:03:27,970 --> 00:03:34,590
そしてこのランダムに選んだ25点を
探索するのです

63
00:03:34,590 --> 00:03:38,350
なぜこうした方がいいのでしょうか
それはどのハイパーパラメタが

64
00:03:38,350 --> 00:03:43,040
その問題において重要なのか
事前にはわからないからです

65
00:03:43,040 --> 00:03:44,480
前のスライドで見たとおり

66
00:03:44,480 --> 00:03:47,910
ハイパーパラメタは種類によって
その重要性が違います

67
00:03:47,910 --> 00:03:49,190
例えば

68
00:03:49,190 --> 00:03:53,505
ハイパーパラメタ1が学習率αだとします

69
00:03:53,505 --> 00:03:55,175
極端な例として

70
00:03:55,175 --> 00:03:58,180
ハイパーパラメタ2が

71
00:03:58,180 --> 00:04:02,730
Adamアルゴリズムの分母中の
εだったとしましょう

72
00:04:02,730 --> 00:04:07,455
αの選び方で結果は大きく変わりますが
εでは殆ど変わりません

73
00:04:07,455 --> 00:04:12,410
格子状に値を選ぶということは

74
00:04:12,410 --> 00:04:16,300
実質的には5種類のαを
試したことにすぎません

75
00:04:16,300 --> 00:04:18,550
おそらくεの値が変わっても

76
00:04:18,550 --> 00:04:21,190
基本的には同じ結果が
返ってくることになるでしょう

77
00:04:21,190 --> 00:04:24,400
つまり25種類のモデルを学習したとしても

78
00:04:24,400 --> 00:04:27,925
より重要度の高い学習率αを

79
00:04:27,925 --> 00:04:29,740
5種類試行したに過ぎない
ということなのです

80
00:04:29,740 --> 00:04:33,430
一方 ランダムにサンプリングすると

81
00:04:33,430 --> 00:04:37,960
25種類の異なる学習率αを

82
00:04:37,960 --> 00:04:40,390
試行することになるので

83
00:04:40,390 --> 00:04:43,690
性能の良い値をより見つけやすくなります

84
00:04:43,690 --> 00:04:44,980
この例を説明するにあたって

85
00:04:44,980 --> 00:04:47,160
ハイパーパラメタを2つだけ使いましたが

86
00:04:47,160 --> 00:04:50,270
実際にはずっと多くのハイパーパラメタを
探索をすることもあります

87
00:04:50,270 --> 00:04:52,000
例えばハイパーパラメタ3つならば

88
00:04:52,000 --> 00:04:55,080
正方形を探索する代わりに

89
00:04:55,080 --> 00:05:00,820
ハイパーパラメタ3が3次元目になった
立方体を探索する事になりますね

90
00:05:00,820 --> 00:05:03,010
そうすると値のサンプリングが

91
00:05:03,010 --> 00:05:05,380
3次元の立方体からされることになり

92
00:05:05,380 --> 00:05:08,080
それぞれのハイパーパラメタにとっても
より多くの値が試行出来ることになります

93
00:05:08,080 --> 00:05:11,440
実際には

94
00:05:11,440 --> 00:05:14,980
4つ以上のハイパーパラメタを
探索することもあり

95
00:05:14,980 --> 00:05:17,160
どれがそのアプリケーションにとって
本当に重要なハイパーパラメタであるか

96
00:05:17,160 --> 00:05:22,120
事前にはわからないこともあります

97
00:05:22,120 --> 00:05:25,390
格子状ではなくランダムにサンプリングしたほうが

98
00:05:25,390 --> 00:05:28,085
重要度の高いハイパーパラメタについて

99
00:05:28,085 --> 00:05:31,045
より潤沢な値探索ができることが分かります

100
00:05:31,045 --> 00:05:33,130
ハイパーパラメタのサンプリングをする時に

101
00:05:33,130 --> 00:05:37,875
もう一つよく使われるのが
粗密サンプリングスキームです

102
00:05:37,875 --> 00:05:42,130
2次元の例で考えてみましょう
これらの点をサンプリングしました

103
00:05:42,130 --> 00:05:45,600
例えばこの点が1番性能がいい

104
00:05:45,600 --> 00:05:49,210
またその周りにも何点か
性能の良い点が見つかったとします

105
00:05:49,210 --> 00:05:53,530
その時 粗密スキームでは何をするかというと

106
00:05:53,530 --> 00:06:00,820
その小さな領域にズームインし
その中でより細かくサンプリングをします

107
00:06:00,820 --> 00:06:02,795
もしくは全体を再度ランダムサーチしてもいいですが

108
00:06:02,795 --> 00:06:06,690
他のエリアより力を入れて
この青い四角の中を探します

109
00:06:06,690 --> 00:06:11,265
いちばん性能のいい設定
つまりハイパーパラメタの組み合わせが

110
00:06:11,265 --> 00:06:13,600
多分この領域にあるだろうと疑っているわけです

111
00:06:13,600 --> 00:06:18,365
まとめると
全領域に粗いサンプリングをかけて

112
00:06:18,365 --> 00:06:22,375
次にフォーカスする小さい領域を見つけます

113
00:06:22,375 --> 00:06:26,105
そしてその小さい領域で
より密なサンプリングをします

114
00:06:26,105 --> 00:06:29,720
この種の粗密探索も頻繁に使われます

115
00:06:29,720 --> 00:06:33,565
異なる値のハイパーパラメタを試すことで

116
00:06:33,565 --> 00:06:37,740
例えばトレーニングセットで
ベスト性能を発揮する値

117
00:06:37,740 --> 00:06:41,230
またはバリデーションセットで
最もうまくいく値

118
00:06:41,230 --> 00:06:46,660
その他値探索プロセスで最適化したい
様々なものを選び出すことが出来るのです

119
00:06:46,660 --> 00:06:48,570
さて ハイパーパラメタ探索を
よりシステマティックに

120
00:06:48,570 --> 00:06:51,670
行う方法についてご理解いただけたでしょうか

121
00:06:51,670 --> 00:06:53,200
キーポイントとしては

122
00:06:53,200 --> 00:06:55,930
格子状ではなくランダムに探索すること

123
00:06:55,930 --> 00:07:01,585
必要に応じて粗密探索を使うこと
の2点です

124
00:07:01,585 --> 00:07:04,750
しかしハイパーパラメタ探索は
これで終わりではありません

125
00:07:04,750 --> 00:07:07,300
次のビデオでは
どうやって適切なスケールで

126
00:07:07,300 --> 00:07:10,020
ハイパーパラメタのサンプリングをするか
について見ていきましょう