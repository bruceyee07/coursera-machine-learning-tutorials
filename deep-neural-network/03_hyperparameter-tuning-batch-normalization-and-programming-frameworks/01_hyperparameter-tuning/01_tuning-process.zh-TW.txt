嗨,歡迎回來.嗨,歡迎回來. 你已經知道訓練類神經網路需要 設定各式各樣的超參數(hyperparameters) 那，你該如何找出一組好的超參數呢？ 在這部影片，我想分享一些指南， 一些技巧，來有系統地調校超參數 希望這能讓你有效率地 找出一組好的超參數 訓練深的網路，最痛苦的在於 你必須要處理很大量的超參數 從學習率alpha，到momentum的參數beta
－如果用的是momentum方法 或是Adam優化法中的beta_1, beta_2和epsilon 或許你需要決定層數 或許你要決定每一層的隱藏單元個數 也許你想使用學習率衰減
(learning rate decay) 所以不光只是單一個學習率alpha 還有當然 你可能還要選擇迷你批次
(mini-batch)的大小 實際上，有些超參數比其他的超參數還重要 我會說，對於大部分的情況 alpha，也就是學習率，是最重要
需要調校的超參數 除了alpha，接下來我或許會調校 momentum法的超參數 例如 0.9 會是個好的預設值 我也會調校迷你批次的大小， 來確保最佳化的時候更有效率 我也常調調看隱藏單元的個數 我這邊用橘色圈起來的 這三項是我覺得僅次於alpha 第二重要的。然後 弄完那些以後，第三重要的話 層數有時影響很大 學習率衰減也有可能 然後我在用Adam優化法的時候，我幾乎
沒調過beta_1 beta_2和epsilon 我通常都會用 0.9, 0.999 和 10^-8。雖然你想要的話
也可以嘗試調看看 總之，希望這能帶給你一些概念 哪些超參數比較重要。 alpha當然是最重要的 然後可能是這些用橘色圈起來的 接下來大概是這些紫色的 不過，這不是唯一的鐵則。我想 其他在實作深度學習的人也可能 不同意我，或是有不同的直覺 那，如果你要調整某個超參數 你該怎麼選擇哪些數值來嘗試呢 在早期的機器學習演算法中 如果你有兩個超參數 假設叫超參數一和超參數二 實作上會像這樣，用格子狀的方法取樣 以有系統地嘗試這些數值 這邊我取了五乘五的格子點 實際上有可能比五乘五多或少。
在這例子 你試過所有25個點，
然後取效果最好的那組超參數 當超參數的數量不多，這種方法還不錯 然而在深度學習，我們傾向於 而且我建議最好是 隨機取樣那些點 就這樣去選一樣數量的點 隨機的25個點，
然後嘗試這25個點代表的超參數 這樣做的原因是，我們很難事先知道 哪種超參數在你的應用裡是最重要的 就像你在前一張投影片看到的 有些超參數比其他的還重要許多 舉個例子 假設這裡的超參數一，實際上是alpha，學習率 再假設一個極端的情況 假設超參數二是 Adam優化法裡面，分母的那個epsilon 所以alpha選多少很重要，
而epsilon選多少則不重要 所以如果用格子法取樣，實際上 你只嘗試了五種alpha的值 而且你可能發現這其中的 五種不同的epsilon都給你同樣的效果 雖然你訓練了25個模型， 但僅僅試了5種學習率alpha 而alpha是很重要的 相較之下，如果隨機地取樣 你就能嘗試25種不同的 學習率alpha，所以更有可能 找到有好效果的值 在這個例子， 我只用了兩個超參數來解釋 實際上，你可能會找更多個超參數 假設你有 三個超參數，這不再是在平面上尋找 你是在一個立方體中尋找，
這第三個維度代表超參數三 那在這個 三維的立方體裡取樣， 對於每一個超參數，你就能嘗試更多不同的值 在實務中，你要找的超參數 甚至可能比三個還多，而且有時 很難事先知道哪些超參數 在你的應用中會比較重要。 比起格子法，隨機取樣更能保證 你嘗試過更多可能的數值 無論最重要的超參數是哪幾個 當你在取樣超參數時， 另一個常見手法是「由粗略到精細」的取樣 假設在這個二維的例子，你找了這些點 發現這個點效果最好 然後可能周圍的這幾個點效果也不錯 在「由粗略到精細」的方案下，你可能會縮小 到這一小塊區域，然後在裡面更密集地尋找超參數 或者說，還是一樣隨機地找 但是花更多的力氣在 在這個藍色方框裡，如果你猜想 最好的超參數在這個區域裡 所以在這整塊區域大略掃過一遍後 發現要集中火力在這小塊地方 接下來你在這小區塊更密集地取樣 這種由粗略到精細的尋找手法也很常用 在嘗試過各種超參數的值以後，你就能 挑選讓訓練集的目標函數最好的超參數 或是在開發集表現最好的 或是各種你想最佳化的目標 我希望這能引導你 更有系統地找尋超參數 有兩個要點： 用隨機取樣的方式，不要用格子法 還有雖然非必要，考慮用由粗略到精細的搜尋手法 不過，要找到好的超參數還不只這些 在下一段影片，讓我們來探討更多 如何在正確的尺度比例上尋找你的超參數