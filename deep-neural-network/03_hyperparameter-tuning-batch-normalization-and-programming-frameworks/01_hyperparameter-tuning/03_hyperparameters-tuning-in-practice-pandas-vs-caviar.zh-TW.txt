你現在聽過很多尋找
好的超參數的方法了 在結束這部份的討論之前 我想分享幾個提示和小技巧 關於如何組織你尋找超參數的流程 現今的深度學習已經應用在很多領域 而在某個領域中對於超參數設定的經驗 不一定能套用到另一個領域 在不同的應用領域常有很多的經驗交流 例如，我看過一些在電腦視覺社群提出的點子 像是 ConvNets 或 ResNet
－我們會在後面的課程談到 能成功應用在語音上 我看過最初在語音領域的想法，
能成功運用在自然語言處理 等等 所以在深度學習的演進中，有件好事是不同領域的人 會去讀越來越多其他領域的論文 以尋找靈感，相互交流 但是，對於 設定超參數這方面，我發現這樣的經驗會碰壁 就算你只是處理一個問題，例如物流，你可能發現 一組好的超參數，然後持續開發你的演算法 抑或你的資料可能在過去幾個月逐漸改變 或者只是升級你在資料中心的伺服器 因為這些改變， 原本最好的超參數設定
可能已不再美好 所以我建議可能重新測試 或是重新評估你的超參數
至少每幾個月一次 來確保你對其值仍然滿意 最後，對於大家如何尋找 超參數，我發現主要有兩類的想法 或者說，有兩種不同的方法大家會去執行 一種方法是你照顧一個模型 通常當你有很大量的資料，但並沒有很多 運算的資源，沒有很多CPU和GPU，
基本上你只能負荷 一次訓練一個模型，或是很少個模型 在這樣的情況下，你可能在訓練這個模型的途中，
慢慢地照顧他 例如，在最一開始，你可能會隨便給定你的參數 然後就開始訓練 然後你漸漸看著你的學習曲線
可能是成本函數J，或是 你的開發集錯誤率或是其他的，在第一天慢慢地下降 然後第一天後，你可能會說，哇，
這看起來學得超棒的 我要試試增加一點學習率，
看會怎麼樣 然後也許做得更好 這是你第二天的成效 經過兩天後，你說，ok
目前表現仍然不錯 也許我會加點momentum項，或是少些學習率 然後就這樣進入第三天 每天你觀察，試著上下微調你的參數 也或許某天，你發現你的學習率太高了 所以你可能回到前一天的模型，等等等 不過某種角度你就像照顧這個模型
一次一天，在這訓練途中 可能長達若干天或好幾個禮拜 所以這是其中一個方法，照顧一個模型 監視著成效，有耐心地上下調整學習率 不過這通常是由於你沒有足夠的計算能力 去同時訓練多個模型 另一個方式則是同時訓練很多個模型 你可能有某個超參數的組合 就放下去讓他跑，跑個一天或好幾天 然後得到了類似像這樣的學習曲線 這可能是成本函數的圖，或是訓練誤差 或是開發集的誤差，某個你追蹤的度量 然後同一時間，你可能會開練另一個模型 用不同一組超參數的設置 那麼，你的第二個模型可能產生不同的學習曲線 可能長得像這樣 我會說這條看起來比較優 在此同時，你可能會訓練第三個模型 可能會產生像這樣的學習曲線
然後可能有另一條 可能他發散了，所以看起來像這樣，等等 或者你可能同時就訓練非常多的模型 這些橘線代表著不同的模型 這樣就可以嘗試很多不同組的超參數 然後最後就很快挑一組表現最好的 在這個例子，看起來可能是這條線最好 所以打個比喻 我稱左邊這個方法叫熊貓方法 當熊貓養小孩時，他們只有很少的小孩 通常一次只有一隻 然後他們花了非常多的心力
確保熊貓寶寶生存下去 所以這真的像照顧小孩一樣 一個模型，或是一隻熊貓寶寶 而右邊的方法比較像魚 我稱之為魚子醬策略 有些魚會在交配季節產下一億顆魚卵 不過呢魚繁殖的方式是他們產下很多卵 不花太多心思在任何一個上 就希望其中某一個或某一些能過得好 所以我猜，這就是哺乳動物繁殖與 魚或爬蟲動物繁殖的不同之處 不過呢，我就稱他們為「熊貓法」跟「魚子醬法」 因為這好玩又好記 所以要如何從兩者中選擇，是和 你有多少計算資源有關 如果你有夠多的電腦可以同時訓練很多個模型 那就盡量採取魚子醬法， 嘗試多組不同的超參數，看看哪個成功 不過在某些應用領域，
我在某些網路廣告的狀況看過 或是電腦視覺的應用，他們有 太大量的資料，要訓練的模型實在 太大，以至於要同時訓練很多個模型
是很困難的 當然，這終究和應用有關，不過 我發覺那幾個領域用熊貓法
用得比較多一些 也就是你一直照顧一個模型， 參數調上調下，試著讓這個模型成功 即便如此，當然就算是貓熊法
訓練了一個模型 然後觀察他成不成功，可能兩三禮拜後 或許我應該開始另一個不同的模型，
轉而照顧那一個 就像即使是熊貓，我猜，也可能
在他們的一生也會有多個小孩 就算一段期間內
只有一個或很少個 總之，希望這能給你一些概念，
如何進行超參數 尋找的流程 那，其實呢還有另一個技巧可以 讓你的神經網路能比較不受超參數選擇的影響 這不見得能用在所有的神經網路，
但當可以時，這能 讓超參數的搜尋更簡單，
也能讓模型訓練快很多 讓我們在下部影片談談這技巧