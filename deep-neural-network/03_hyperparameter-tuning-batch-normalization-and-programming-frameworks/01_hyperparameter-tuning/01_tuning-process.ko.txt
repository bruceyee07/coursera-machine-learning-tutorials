안녕하세요, 환영합니다. 여러분은 이제 신경망을 바꾸는데 있어 다양한 하이퍼 파라미터를 세팅하는 과정이 개입되어 있다는 것을 보았습니다. 그러면 최적의 하이퍼 파라미터 세팅을 찾는 방법은 어떻게 알 수 있을까요? 이번 비디오에서는, 가이드라인에 대해서 공유하도록 하겠습니다. 하이퍼 파라미터 튜닝 절차를 시스템적으로 조직화하는 방법과 팁에 대한 가이드라인 말이죠. 이런 가이드라이은은 이상적으로 여러분이 좋은 하이퍼 파라미터 값에 수렴하도록 효율적인 방안을 제시해 줄 것입니다. 여러분이 깊은 네트워크를 트레이닝하는 과정에서 힘든 부분중에 한가지는 신경써야하는 많은 양의 하이퍼 파라미터 개수입니다. 이런 하이퍼 파라미터들은 러닝속도를 나타내는 알파값, 모멘텀을 쓰는 경우의 모멘텀 값, 아니면 Adam 최적화 알고리즘의 하이퍼 파라미터인 베타1, 베타2, 앱실론이 포함되고요, 층의 개수도 골라야할 수 있죠. 이러한 각각의 다른층마다 hidden unit을 골라야할 수도 있습니다. 또한 learning rate decay도 이용하고 싶어할 수 있죠. 이런 경우, 러닝속도 알파 값만 쓰는 것이 아니겠죠. 그런 후에, 당연한 것일 수도 있지만 미니 배치 사이즈를 골라야할 것입니다. 하이퍼 파라미터를 설정하고 고르는데 있어 특정 하이퍼 파라미터가 더 특별히 중요할 수 있겠죠. 대부분의 러닝 어플의 경우에서는, 러닝속도인 알파의 값이 튜닝하는데 있어 가장 중요한 하이퍼 파라미터라고 할 수 있을 것입니다. 알파 값외에도, 몇개의 다른 하이퍼 파라미터 튜닝이 중요할 수 있는데요, 아마 모멘텀 용어도 중요할 수 있을 것입니다. 제 생각에는 0.9가 적당한 기본값이라고 생각하는데요. 또한, 최적화 알고리즘이 효율적으로 운영되도록 저 같은 경우엔, 미니 배치 사이즈도 튜닝할 것입니다. 그리고 저는 hidden unit 또한, 튜닝을 합니다. 오렌지색으로 동그라미친 것들은, 여기 이 3개의 하이퍼 파라미터는 러닝속도 알파 값 다음으로 중요한 것입니다. 그리고 세번째로 중요한 것은, 층의 개수도 아주 큰 차이를 불러올 수 있고, learning rate decay도 마찬가지입니다. 그리고 Adam 알고리즘을 이용하는 경우에는, 저는 거의 항상 베타1, 베타2, 그리고 앱실론은 튜닝하지 않습니다. 거의 항상 0.9, 0.999, 10의 마이너스 8승 값으로 사용하는데요, 여러분이 원하시면 이 값들도
튜닝하실 수 있습니다. 방금 이야기한 내용이 여러분에게 대략적으로 하이퍼 파라미터에 대한 감을 주고, 어떤 것이 더 중요한지 알려줬으면 좋겠습니다. 예를 들어, 알파가 가장 중요한 것이 되겠고요, 그 다음으로 오렌지색으로 동그라미 친 것이 중요하고, 그 다음은 보라색으로 동그라미 친 것이 중요할 것입니다. 하지만 이것은 엄격한 규칙은 아닌데요, 제 생각에는 딥러닝 종사자분들중에 저와 반대하는 분들도 계시거나 다른 직관적인 시각을 가지고 계신 분이 있으실 것입니다. 그렇다면, 여러분이 하이퍼 파라미터 값을 튜닝하려고 하는 경우에, 탐색하기 위한 하이퍼 파라미터값은 어떻게 설정할까요? 머신 러닝 알고리즘의 초기에는, 2개의 하이퍼 파라미터가 있는 경우에, 이 경우에 하이퍼 파라미터 1과 하이퍼 파라미터 2 라고 하겠습니다. 점들을 이런식으로 격자판 형식으로 샘플링하는 경우가 많았습니다. 그리고 이러한 값들을 탐색해나가는 것입니다. 여기서는 5 x 5 격자판 형식으로 설정했는데요, 실제로는 더 큰 격자판이나 이 것보다 작은 격자판으로 설정할 수도 있습니다. 이 경우에서는 25개의 포인트 지점이 있는데요, 이중에서 가장 잘 작동하는 하이퍼 파라미터를 고르는 것입니다. 이런 방법은 하이퍼 파라미터 의 개수가 비교적 많지 않은 경우에 잘 작동하는 편입니다. 딥러닝의 경우, 저희는 보통, , 제가 추천드리는 다른 방식은, 지점들을 임의로 선정하는 것입니다. 그러면 포인트의 개수는 똑같이 해서 진행합니다. 아시겠죠? 25개를 지정한 다음에, 이렇게 임의로 지정된 포인트에서 임의로 하이퍼 파라미터를 골라
시도해봅니다. 이렇게 하는 이유는, 여러분이 이 단계에서는 어떤 하이퍼 파라미터들이 문제에서 가장 중요할지 미리 알 수 없기 때문입니다. 이전 슬라이드에서 보셨듯이, 특정 hyperparameter들은 다른 것들보다 더 중요할 수 있습니다. 예를 들어서, 하이퍼 파라미터 1의 값이 러닝속도를 나타내는 알파값이였다고 해봅시다. 그리고 극단적인 경우로, 하이퍼 파라미터 2는 Adam 알고리즘의 분모에 속하는 앱실론 값이였다고 해보겠습니다. 이런 경우에는, 알파값이 여러분에게 매주 중요하고, 앱실론의 선택은 거의 중요하지 않겠죠. 격자판에 샘플링을 진행하면, 이런식으로 5개의 알파값을 나열하겠죠. 그리고 다양한 앱실론의 값이 동일한 답을 줄수 있습니다. 그러면 여러분은 25개의 모델을 트레이닝 시켰는데 오로지 5개의 러닝속도 알파값만 시도할 수 있었습니다. 중요한 값인데도 말이죠. 반대로, 여러분이 샘플링을 무작위로 진행하였을때는, 각각의 25개의 러닝속도 알파값을 시도해보시게 됐을 것입니다. 그러므로 잘 작동하는 값을 찾을 확률이 늘어나는 것이죠. 저는 예제를 오로지 2개의 하이퍼 파라미터만 사용하여 설명드렸는데요, 실제로는 여러분이 어 많은 숫자의 하이퍼 파라미터를 사용하여 그 값을 찾을 수 있습니다. 예를 들어, 3개의 하이퍼 파라미터값일 수도 있겠죠, 이런 경우레는 사각형이 아니라 여기 이 다이멘션이 하이퍼 파라미터 3인 정육면체에서 값을 찾을 것입니다. 그러면, 샘플링을 이런 3차원의 정육면체에서 진행하여 이런 하이퍼 파라미터들을 두고 더 많은 값들을 시도할 수 있습니다. 실제로는 여러분이 이것보다 더 많은 수의 하이퍼 파라미터에서 서칭을 진행할 수도 있습니다. 하지만 여러분이 미리 어떤 하이퍼 파라미터가 작업하는 어플에서 더 중요한지 알아내기 어렵기 때문에, 샘플링 작업을 격자판식으로 하는 것보다 무작위로 샘플링을 하는 방식이 대부분의 하이퍼 파라미터에 대해서 더 많은 값을 탐색할 수 있기 때문에, 그것들의 결과가 어떻든 더 광범위하게 진행할 수 있습니다. 하이퍼 파라미터를 샘플링하는 경우에, 흔한 진행 방식은 coarse to fine sampling scheme 입니다. 이렇게 생긴 2차원 예제에서, 여기 포인트들을 샘플링한다고 해보겠습니다. 그리하여, 여기 이 포인트가 가장 잘 작동한다고 알아냈다고 해봅시다. 그리고 그 주위의 멱채의 포인트들도 말이죠. 그러면 마지막 진행절차 도중에, 여러분은 여기 이 조금한 하이퍼 파라미터 부분을 줌인하여 여기 이 공간에대해 조금도 밀도있게 샘플링을 진행합니다. 또다시, 임의의 샘플링을 진행할 수도 있겠죠. 일단 여기 안쪽에서 서칭하는 것에 집중한다고 하면, 여러분이 생각하기에 여기 이 부분이 가장 최적의 하이퍼 파라미터 세팅이 있다고 하면, 여기 파란색 사각형 내부에서 말이죠. 여기 사각형 전체에서 coarse 샘플링을 진행하면, 그 다음으로 더 조금한 사각형에 집중하라고할 것입니다. 그러면 여기 이 조금한 사각형에서 밀도있게 샘플링을 진행할 수 있죠. 이런 방식의 coarse to fine 서칭은 자주 이용되는 방법인데요, 이러한 다양한 하이퍼 파라미터 값들을 시도하여, 여러분의 training set objective나 development set 또는 여러분이 최적화하려고하는 하이퍼 파라미터 부분에서 가장 좋은 값을 찾게 해줍니다. 오늘 배운 내용을 통해 여러분이 하이퍼 파라미터 서칭 단계에서 더 시스템적으로 조직화시킬 수 있는 계기가 되었길 바랍니다. 2가지 염두하실 점은, 임의의 샘플링을 진행하고, 충분한 서칭을 진행합니다. 그리고 선택적으로 coarse to fine 서칭 단계를 도입할지 고려합니다. 하지만 이것말고소 하이퍼 파라미터 서칭에는 다른 중요한 요소들이 있는데요. 다음 비디오를 통해 여러분이 하이퍼 파라미터를 샘플하는데 있어 올바른 scale을 찾는 방법을 다루도록 하겠습니다.