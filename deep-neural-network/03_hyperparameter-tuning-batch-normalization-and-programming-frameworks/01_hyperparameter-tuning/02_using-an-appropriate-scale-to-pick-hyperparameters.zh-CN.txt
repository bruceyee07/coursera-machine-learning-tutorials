上一节你们看到了,超参数值域的随机抽样 能让你更有效地搜索超参数空间 但实际上,随机抽样并不意味着在有效值范围内的 均匀随机抽样(sampleing uniformly at random) 相反,更重要的是选取适当的尺度(scale) 用以研究这些超参数 这一节我想向你们展示这个方法 假设你现在要选择第l层隐藏单元的个数n[l] 假设你现在要选择第l层隐藏单元的个数n[l] 再假设你认为50~100是个不错的范围 那么,参考数轴上从50~100的范围 也许你会在这个线段上随机地取一些数值 这是搜索这个超参数的非常直观的方法 或者如果你要决定神经网络的层数 我们用大写L来表示 也许你认为这个层数应该在2~4之间 那么均匀随机的抽样,即2,3,4,这样也是合理的 或者甚至用网格搜索,显式计算 l=2,3,4的结果,可能也是合理的 所以这里有好几个例子说明在你所关注的域中 均匀随机抽样是合理的方案 但它并不是对所有的超参数都适用 从这里开始 增加θ 似乎这也是我希望得到的 也就是 比如你正在搜索超参数alpha,即学习率 假设你认为它的下限是0.0001 上限是1 现在画出从0.0001~1的数轴 并均匀随机地抽取样本值 那么90%的样本值将落在0.1~1的范围内 即你用90%的资源搜索0.1~1 只有10%的资源用于搜索0.0001~0.1范围内的值 看起来不大对 更合理的方法似乎应该以对数尺度(log scale)来搜索 而不是用线性尺度(linear scale)<br />那么0.0001在这里 然后是0.001,0.01,0.1和1 接下来你可以在这个对数尺度上均匀随机的取样 现在你可以有更多的资源致力于搜索 0.0001~0.001,0.001~0.01这样的范围了 用Python实现就是 r=-4*np.random.rand() 然后再随机取alpha值,即alpha=10^r 第一行运行之后,r为-4~0的随机数 则alpha值在10^(-4)~10^0之间 10^(-4)在最左边,这里 1=10^0 更一般的来说 如果你要在10^a~10^b的范围内取样 这个例子里,这里是10^a 你可以用基于10的对数计算a 这里a=-4 右边的值是10^b 计算出b的值 这里b=log10(1)=0 那么你要做的就是在a~b的范围内均匀随机取样 这个例子中的范围为-4~0 然后为alpha赋值为 10^r,r为随机取样的基于10的对数 整理一下,要基于对数尺度取样,首先取得下限值 取其对数得到a 再取上限值,取其对数得到b 然后在对数尺度上在10^a~10^b范围内取样 即在a~b的范围内均匀随机的取r值 最后得到超参数值为10^r 这就是对数尺度上取样方法的实现 最后,另一个棘手的情况是超参数beta的取样 beta用于计算指数加权平均值 假设你认为beta值应该在0.9~0.999之间 也许这是你要搜索的值空间 要记住的是,用0.9计算指数加权平均值 相当于计算最后10个值的平均值 比如计算10天气温的平均值 而使用0.999就相当于计算1000个值的平均值 类似上一页ppt所展示,如果你要搜索 0.9~0.999的范围,线性尺度的取样 即均匀的,随机的,0.9至0.999范围内的搜索,没什么意义 那么考虑这个问题的最好的方法是 将这个范围展开为1-beta 得到0.1~0.001的范围 那么我们对beta的采样范围 为0.1~0.001 现在运用我们之前学到的方法 这是10^(-1),这是10^(-3) 请注意,之前的数轴是从左至右递增的 这里我们要反过来 大的值在左边,小的值在右边 所以你要做的是在-3~-1的范围内均匀随机的取样 然后置1-beta=10^r,即beta=1-10^r 就得到了这个超参数在适当尺度上 的随机取样值 希望这能解释: 你在探索0.9~0.99和0.99~0.999的范围时 使用了同样数量的资源 如果你想要知道关于这个做法的更规范的数学证明 也就是说,为什么以线性尺度取样是个坏主意？ 这是因为随着beta趋近于1<br />其结果对于beta的改变非常敏感 即使是对beta非常小的改变 如果beta从0.9变成0.9005 这没什么大不了,你的结果几乎没有任何变化 但是如果beta从0.999变成0.9995 它将会对你正在运行的算法产生巨大的影响 在前一个例子中,都是取大约10个值的平均 但是这里,取指数加权平均的情况下 它从取最后1000个样例变成了取最后2000个样例的平均 因为我们的公式是1/(1-beta) 所以当beta趋近于1时,它对beta的改变非常敏感 那么上述整个取样过程所做的 就是使你在beta趋近于1的区域内以更大的密度取样 换一种说法,也就是在1-beta趋近于0的时候 因此你能得到更高效的样本分布 即在搜索可能的超参数空间时更有效率 我希望这能够帮助你 在对超参数取样时选择正确的尺度 如果对于某个超参数你最后选择的尺度是不对的 也不用太担心 即使在存在更优尺度的情况下<br />你依然选择了均匀尺度(uniform scale) 你仍然可能得到不错的结果 尤其是如果你采取从粗到精(coarse to fine)的搜索策略<br />在后期的迭代中 你将更专注于在最有用的超参数值域取样 我希望这能对你的超参数搜索有所帮助 下一节我将分享 如何组织超参数搜索过程的一些想法 希望能使你的工作流程更有效率