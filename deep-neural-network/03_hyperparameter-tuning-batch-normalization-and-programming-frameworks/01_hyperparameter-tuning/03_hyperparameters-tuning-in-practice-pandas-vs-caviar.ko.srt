1
00:00:00,210 --> 00:00:04,380
이제 여러분은 좋은 하이퍼 파라미터를 찾는 방법데 대해 
많이 들으셨을 텐데요 

2
00:00:04,380 --> 00:00:08,090
하이퍼 파라미터 서치에 대한
토론내용을 정리하기 전에 앞서, 

3
00:00:08,090 --> 00:00:11,670
마지막으로 몇가지의 팁을 공유하고 싶습니다.

4
00:00:11,670 --> 00:00:14,775
하이퍼 파라미터 서치 과정을 조직화하는 방법에 대해서 말이죠. 

5
00:00:14,775 --> 00:00:19,259
딥러닝은 오늘날 많은 다양한 
어플분야에서 적용되고 있는데요

6
00:00:19,259 --> 00:00:24,342
한 어플 분야에서 사용되는 하이퍼 파라미터의 직관이 

7
00:00:24,342 --> 00:00:26,670
다른 어플로 이동되지 않을 수 있습니다.

8
00:00:26,670 --> 00:00:31,466
서로 다른 어플 도메인에서는 
cross-fertilization이 많이 있습니다.

9
00:00:31,466 --> 00:00:36,055
예를 들어, 저는 컴퓨터 비전 커뮤니티에서
아이디어가 생기는 것을 많이 보았는데요, 

10
00:00:36,055 --> 00:00:40,301
Confonets or ResNets 와 같은 것들 말이죠. 
이것들은 나중의 코스에서 다루도록 하겠습니다.

11
00:00:40,301 --> 00:00:42,400
이런 아이디어는 성공적으로 스피치 분야에 적용되었습니다.

12
00:00:42,400 --> 00:00:46,620
이러한 아이디어들이 처음으로 개발되고 
스피치 분야에서 NLP로 성공적으로 
적용되고 말이죠.

13
00:00:46,620 --> 00:00:47,870
등등이 계속 이어집니다.

14
00:00:47,870 --> 00:00:52,613
그렇게해서 딥러닝에서 보여준 훌륭한 개발은 

15
00:00:52,613 --> 00:00:57,223
다른 어플 영역, 분야의 사람들이 
다양한 분야의 논문을 더 많이 읽고

16
00:00:57,223 --> 00:01:00,180
cross-fertilization의 영감을 얻으려 한다는 것입니다.

17
00:01:00,180 --> 00:01:01,580
하지만 하이퍼 파라미터의

18
00:01:01,580 --> 00:01:06,420
설정에 있어서는 이런 직관적인 부분이
잘 이루어지지 않는 것을 보게됩니다.

19
00:01:06,420 --> 00:01:11,360
여러분이 한개의 문제를 다루게 되더라도, 
예를 들어, 로지스틱 말이죠. 

20
00:01:11,360 --> 00:01:15,700
하이퍼 파라미터를 위한 좋은 값을 찾았을 수 있습니다. 
그리소 계속 알고리즘을 개발했겠죠. 

21
00:01:15,700 --> 00:01:20,465
또는 데이터가 차차 몇개월간 
변해가는 것을 경험했겠어죠.

22
00:01:20,465 --> 00:01:25,070
또는 여러분의 데이터 센터에 있는 서버를 
업그레이드 했을 수도 있죠. 

23
00:01:25,070 --> 00:01:26,380
이러한 변화 때문에, 

24
00:01:26,380 --> 00:01:29,500
여러분의 하이퍼 파라미터 최적 설정값이 
오래될 수 있습니다.

25
00:01:29,500 --> 00:01:32,005
그렇기 때문에 저는 여러분이 retesting 하거나

26
00:01:32,005 --> 00:01:35,860
하이퍼 파라미터를 재평가하는 것을 추천드립니다.

27
00:01:35,860 --> 00:01:39,390
몇개월에 한번씩은 말이죠. 여러분이 가지고 있는 값들에 대해서
만족하기 위해서 말입니다.

28
00:01:39,390 --> 00:01:42,150
그리고 마지막으로 사람들이 
하이퍼 파라미터를 서칭하는 것에 대해 

29
00:01:42,150 --> 00:01:46,430
이야기해볼 텐데요, 여기서는 2가지 정도가 떠 오릅니다.

30
00:01:46,430 --> 00:01:50,370
또는 사람들이 하는 2가지 방법이죠. 

31
00:01:50,370 --> 00:01:52,920
한가지 방법은 바로 1개의 모델을 babysit하는 것입니다.

32
00:01:52,920 --> 00:01:57,671
이것은 아주 큰 데이터가 있고 산출 자원은 많이 없을 때 적용될텐데요, 

33
00:01:57,671 --> 00:02:01,390
산출자원이라 하면 CPU나 GPU같은것이 많이 없어

34
00:02:01,390 --> 00:02:05,100
1가지 모델만 트레이닝할 여력 밖에 없는 경우 또는
아주 작은 수의 모델만 한번에 트레이닝 시킬 수 밖에 없는 경우에 해당합니다.

35
00:02:05,100 --> 00:02:11,070
이런 경우, 차차 이 모델을 babysit하는 것입니다. 트레이닝을 할때도 말이죠. 

36
00:02:11,070 --> 00:02:15,180
예를 들어, day 0일 때, 
파라미터를 임의로 초기화시키고

37
00:02:15,180 --> 00:02:16,370
바로 트레이닝을 할 수 있겠죠. 

38
00:02:16,370 --> 00:02:21,626
그리고 점차 러닝커브를 보고, 
아마 J 비용함수일 수 있겠죠 또는

39
00:02:21,626 --> 00:02:27,333
데이터세트이거나 다른 것일 수도 있고요
이런 것들이 점차 첫째날동안 줄어들 것입니다.

40
00:02:27,333 --> 00:02:31,300
그리고 첫째날의 마지막 부분에 이르러서 
아주 잘 러닝하고 있다고 생각하실 수 있습니다.

41
00:02:31,300 --> 00:02:35,000
이런 경우, 러닝속도를 약간 높혀서 
어떤 결과가 나오는지 보겠습니다.

42
00:02:35,000 --> 00:02:37,090
그러면 아마 더 잘할 수도 있겠죠. 

43
00:02:37,090 --> 00:02:38,870
그럼 이것이 둘째날의 성능이 되는 것이고요. 

44
00:02:38,870 --> 00:02:42,150
그리고 이틀후에, 
아직 잘하고 있다고 판단할 수 있습니다.

45
00:02:42,150 --> 00:02:46,339
그러면 이제 모멘텀 항을 조금 더해서 
또는 이제는 러닝 변수를 조금 줄일것이라 할 수 있습니다.

46
00:02:46,339 --> 00:02:47,994
이제 그렇게되면 셋째날로 접어드는 것이죠. 

47
00:02:47,994 --> 00:02:52,750
이렇게 여러분은 매일 보면서
조금씩 위로 또는 밑으로 파라미터에 변형을 줄 수 있는 것이지요. 

48
00:02:52,750 --> 00:02:55,646
그러면 아마 어느날에는 러닝속도가 매우 컸다는 
것을 깨달을 수 있습니다.

49
00:02:55,646 --> 00:02:58,649
그러면 이전 모델로 다시 돌아갈 것입니다.

50
00:02:58,649 --> 00:03:03,445
이렇듯, 모델을 babysitting 하는 것과 같은데요, 
하루씩 babysitting을 하는 것입니다. 트레이닝이 

51
00:03:03,445 --> 00:03:08,080
몇일간 또는 몇주간 이루어지는 경우에도 말이죠. 

52
00:03:08,080 --> 00:03:12,010
이렇게 하는 것이 한가지 접근 방식이구요. 

53
00:03:12,010 --> 00:03:17,390
이런 방식에서는, 사람들이 한가지의 모델을 babysit하여 
성능을 보면서 차분하게 러닝속도를 조정해줍니다.

54
00:03:17,390 --> 00:03:21,010
이 경우는 그러나 산출자원이 많이 없는 경우에 

55
00:03:21,010 --> 00:03:24,210
진행하는 경우이고요, 보통 한번에 여러모델을 트레이닝
시킬 여력이 없을 때 사용하는 방법입니다.

56
00:03:24,210 --> 00:03:28,480
다른 방법으로는 parallel 방식으로
여러 모델을 트레이닝 시키는 방법인데요. 

57
00:03:28,480 --> 00:03:32,010
어떤 하이퍼 파라미터의 특정 설정 값으로 

58
00:03:32,010 --> 00:03:36,050
혼자 작동하게 놔둘 수 있는데요, 
하루 또는 며칠 간 이렇게 놔둘 수 있습니다.

59
00:03:36,050 --> 00:03:38,180
그러면 이렇게 생긴 러닝커브를 얻게 될텐데요, 

60
00:03:38,180 --> 00:03:42,180
이것은 j 비용함수의 그래프일 것이거나 
트레이닝 오류의 비용이거나 

61
00:03:42,180 --> 00:03:45,670
데이터세트의 비용일 수 있을 텐데요, 
여러분이 트래킹하는 것 의 어떤 특정 매트릭일 것입니다.

62
00:03:45,670 --> 00:03:48,636
그리고 동시에 다른 설정값의 하이퍼 파라미터를

63
00:03:48,636 --> 00:03:50,490
가진 다른 모델을 시작할 수 있습니다.

64
00:03:50,490 --> 00:03:54,030
그러면 두번째 모델은 다른 러닝 커브를 줄 것입니다.

65
00:03:54,030 --> 00:03:55,960
이렇게 생길 수도 있겠죠. 

66
00:03:55,960 --> 00:03:57,510
이것이 조금 더 나아 보이네요.

67
00:03:57,510 --> 00:03:59,980
동시에, 세번째 모델도 트레이닝 시킬 수 있습니다.

68
00:03:59,980 --> 00:04:03,914
이렇게 생긴 러닝커브를 만들게 말이죠. 
그리고 또 하나를 만들수도 있고요. 

69
00:04:03,914 --> 00:04:06,680
이것은 이렇게 나뉘어질 수 있는데요, 
이렇게 생길것입니다. 

70
00:04:06,680 --> 00:04:10,280
아니면 여러분은 여러개의 모델을 parallel하게 트레이닝 시킬 수 있습니다.

71
00:04:10,280 --> 00:04:13,570
그러면 여기 오렌지색 선이 다른 모델들인데요, 

72
00:04:13,570 --> 00:04:16,620
이런 방식으로 다른 하이퍼 파라미터 설정값을 시도하고

73
00:04:16,620 --> 00:04:21,090
마지막에 가장 잘 작동하는 것을 찾을 수 있습니다.

74
00:04:21,090 --> 00:04:25,600
여기 예제에서는 이 커브가 가장 잘 보일 수 있겠죠. 

75
00:04:25,600 --> 00:04:27,340
그럼 비유를 하자면, 

76
00:04:27,340 --> 00:04:30,760
여기 왼쪽 방식을 panda approach라고 할 것입니다.

77
00:04:30,760 --> 00:04:33,822
panda가 아이를 갖는 경우, 
굉장히 조금 갖는데요, 

78
00:04:33,822 --> 00:04:35,507
한번에 1명씩 낳습니다.

79
00:04:35,507 --> 00:04:40,350
그렇게 1명을 낳아 그 아기가 잘 생존해서
자랄 수 있도록 많은 노력을 기울입니다.

80
00:04:40,350 --> 00:04:41,640
이것은 babysitting 방법과도 같죠. 

81
00:04:41,640 --> 00:04:44,280
한개의 모델 또는 한게의 아기 panda입니다.

82
00:04:44,280 --> 00:04:48,000
오른쪽 방법은 물고기가 하는 방식과 비슷합니다.

83
00:04:48,000 --> 00:04:50,380
이것을 캐비아 전략이라고 할 것입니다.

84
00:04:50,380 --> 00:04:55,540
1시즌에 1억개가 넘는 생선알을 낳는 물고기가 있는데요, 

85
00:04:55,540 --> 00:04:58,960
물고기가 번식하는 방법은 생선알은 한번에 

86
00:04:58,960 --> 00:05:01,740
많이 낳는 방법인데요, 일일히 신경쓰지 않고 

87
00:05:01,740 --> 00:05:05,970
하나가 잘 되기를 바라면 또는 몇개가 잘되기를 
바라는 원리라고 할 수 있겠습니다.

88
00:05:05,970 --> 00:05:10,340
이것이 사실 포유동물과

89
00:05:10,340 --> 00:05:15,030
물고기 또는 파충류가 번식하는 방법의
차이겠죠.

90
00:05:15,030 --> 00:05:17,980
저는 이것을 인용하여
panda approach와 caviar approach라고 할 것입니다.

91
00:05:17,980 --> 00:05:20,210
이것이 더 흥미롭고 쉽게 외울 수 있는 방법이기 때문입니다.

92
00:05:20,210 --> 00:05:23,550
이 2개의 접근 방식 중 고르는 방법은, 

93
00:05:23,550 --> 00:05:26,500
여러분이 얼마나 산출 자원을 가지고 있느냐에 따라 달라집니다.

94
00:05:26,500 --> 00:05:30,460
여러분이 충분히 많은 컴퓨터를 가지고 있어
여러개의 모델을 parallel 방식으로 트레이닝 시킬 수 있으면, 

95
00:05:31,920 --> 00:05:34,670
caviar approach를 택하셔도 무방합니다.

96
00:05:34,670 --> 00:05:37,780
여러개의 하이퍼 파라미터를 시도하여 
어떤 것이 잘 작동하는 것인지 확인하는 것입니다.

97
00:05:37,780 --> 00:05:42,520
하지만 어떤 어플 영역에서는, 
저는 이것을 몇개의 온라인 광고 설정에서

98
00:05:42,520 --> 00:05:45,940
보기도 하고 컴퓨터 비전 어플에서도 보는데요, 

99
00:05:45,940 --> 00:05:48,670
데이터가 무수히 많아 트레이닝 시키려고 하는 모델이 너무 방대해서

100
00:05:48,670 --> 00:05:53,220
모델들을 한번에 트레이닝 시키기가 매우 어렵습니다.

101
00:05:53,220 --> 00:05:55,640
어플에 마다 물론 다를 수 있겠죠. 

102
00:05:55,640 --> 00:06:00,150
저는 이런 커뮤니티와 같은 경우, 
panda approach를 더 많이 쓰는 경우를 봤습니다.

103
00:06:00,150 --> 00:06:03,260
이 경우, 한개의 모델을 babysitting하고

104
00:06:03,260 --> 00:06:08,340
파라미터의 값을 올리거나 내리는 등의 방식으로
변형을 주어 잘 작동하게 만들어주는 방식입니다.

105
00:06:08,340 --> 00:06:12,165
물론 panda approach도 

106
00:06:12,165 --> 00:06:15,580
한개의 모델에서 작동하는지 그렇지 않은지 확인을 하는 
절차이지만, 2번째 주나, 또는 세번째 주 시점에서

107
00:06:15,580 --> 00:06:19,870
다른 모델로 초기화하여 옆에서 

108
00:06:19,870 --> 00:06:23,910
쌍둥이 panda처럼 babysit해야할 수도 있습니다.
이런 경우 복수의 아기를 갖는 셈이죠.

109
00:06:23,910 --> 00:06:28,800
오로지 한개만 또는 아주 작은 수의 모델만
한번에 처리한다고 해도 말이죠. 

110
00:06:28,800 --> 00:06:32,888
바라건대 여러분이 이 강의를 통해 
하이퍼 파라미터 서치에 대해 조금 더

111
00:06:32,888 --> 00:06:34,170
이해할 수 있었던 강의가 되었길 바랍니다.

112
00:06:34,170 --> 00:06:37,090
이제 여러분의 신경망이 하이퍼 파라미터를 선정하는데 있어 
더 잘 작동하게

113
00:06:37,090 --> 00:06:41,360
하기 위해서 사용할 수 있는 기술이 한개 더 있는데요. 

114
00:06:41,360 --> 00:06:44,180
모든 신경망에서 작동하는 것은 아닙니다만, 

115
00:06:44,180 --> 00:06:48,670
잘 작동하는 경우, 하이퍼 파라미터 서치 절차를 아주 간단하게 해주고
트레이닝의 속도를 높혀줄 수 있습니다.

116
00:06:48,670 --> 00:06:50,780
이 기술에 대해서는 다음 비디오를 통해 이야기하겠습니다.