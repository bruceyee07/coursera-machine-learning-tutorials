1
00:00:00,000 --> 00:00:02,415
那么 为什么BN算法呢?

2
00:00:02,415 --> 00:00:06,680
其中一个理由是<br />我们看到经过归一化的输入特征(用X表示)

3
00:00:06,680 --> 00:00:09,380
它们的均值为0 方差为1

4
00:00:09,380 --> 00:00:10,740
这将大幅加速学习过程

5
00:00:10,740 --> 00:00:13,920
所以与其含有某些在0到1范围内变动的特征

6
00:00:13,920 --> 00:00:15,735
或在1到1000范围内变动的特征

7
00:00:15,735 --> 00:00:18,835
通过归一化所有输入特征X

8
00:00:18,835 --> 00:00:22,975
让它们都拥有相同的变化范围将加速学习

9
00:00:22,975 --> 00:00:25,770
所以 BN算法有效的一个原因是

10
00:00:25,770 --> 00:00:27,750
它同样如此

11
00:00:27,750 --> 00:00:32,770
只不过它应用于隐藏层的值 而不是这里输入特征

12
00:00:32,770 --> 00:00:37,380
现在 我所描述的仅仅是BN算法所做的一部分

13
00:00:37,380 --> 00:00:39,180
还有很多(关于BN算法的)进一步解释

14
00:00:39,180 --> 00:00:43,210
这将有助于我们更深层一步地理解BN算法

15
00:00:43,210 --> 00:00:46,320
我会在本视频中阐述它

16
00:00:46,320 --> 00:00:48,490
BN算法有效的第二个原因是

17
00:00:48,490 --> 00:00:50,295
它产生权重 (w参数)

18
00:00:50,295 --> 00:00:52,575
在深层次网络中

19
00:00:52,575 --> 00:00:56,095
假设在10层的参数w

20
00:00:56,095 --> 00:01:00,300
比神经网络初始的层级 假设为0层更具鲁棒性

21
00:01:00,300 --> 00:01:01,810
为了解释我的意思

22
00:01:01,810 --> 00:01:04,775
让我们参考这个生动的例子

23
00:01:04,775 --> 00:01:06,420
假设我们在某个网络上训练

24
00:01:06,420 --> 00:01:07,770
可能是浅层网络

25
00:01:07,770 --> 00:01:11,715
例如逻辑回归或者是神经网络

26
00:01:11,715 --> 00:01:17,260
可能是浅层网络<br />例如逻辑回归或者是著名的猫咪检测深度网络

27
00:01:17,260 --> 00:01:21,120
著名的猫咪检测深度网络

28
00:01:21,120 --> 00:01:26,745
假设我们的数据集 是在所有黑猫图片上训练得到的

29
00:01:26,745 --> 00:01:31,240
现在我们将其应用于

30
00:01:31,240 --> 00:01:33,325
所有的正确结果并不像左侧黑猫那样的<br />彩色猫咪图片上

31
00:01:33,325 --> 00:01:36,895
所有的正确结果并不像左侧黑猫那样的<br />彩色猫咪图片上

32
00:01:36,895 --> 00:01:40,442
然后右侧的彩色猫咪的确是猫

33
00:01:40,442 --> 00:01:43,160
那么我们的分类将会出错

34
00:01:43,160 --> 00:01:47,907
所以对于图像 如果我们的训练集像这样

35
00:01:47,907 --> 00:01:52,383
正确样本在这里 错误样本又在这里

36
00:01:52,383 --> 00:01:54,970
但我们却试图获得一个数据集

37
00:01:54,970 --> 00:02:02,335
它的正确样本在这里 而错误样本又在这里

38
00:02:02,335 --> 00:02:05,800
那么我们不要指望用左侧数据训练得到的模型<br />适用于右侧

39
00:02:05,800 --> 00:02:09,430
那么我们不要指望用左侧数据训练得到的模型<br />适用于右侧

40
00:02:09,430 --> 00:02:13,901
尽管这里可能存在着相同的有效函数

41
00:02:13,901 --> 00:02:19,230
但你就不要指望你的学习算法<br />能找到那条绿色决策边界

42
00:02:19,230 --> 00:02:21,500
只需看看左侧的数据就知道了

43
00:02:21,500 --> 00:02:26,410
所以我们的数据分布随着某种东西在变化

44
00:02:26,410 --> 00:02:31,657
也就是 协变量

45
00:02:31,657 --> 00:02:33,695
想法是

46
00:02:33,695 --> 00:02:35,910
如果我们学习了某种x-y映射

47
00:02:35,910 --> 00:02:38,435
如果x的分布变化了

48
00:02:38,435 --> 00:02:41,545
那么我们就得重新训练学习算法

49
00:02:41,545 --> 00:02:43,925
这是一定的

50
00:02:43,925 --> 00:02:45,230
尽管x-y映射的真函数没变

51
00:02:45,230 --> 00:02:46,775
尽管x-y映射的真函数没变

52
00:02:46,775 --> 00:02:49,430
尽管x-y映射的真函数没变 如此例

53
00:02:49,430 --> 00:02:51,630
因为真函数就是判断该图是否是猫

54
00:02:51,630 --> 00:02:53,990
因为真函数就是判断该图是否是猫

55
00:02:53,990 --> 00:02:57,590
并且迫切需求保持我们的函数

56
00:02:57,590 --> 00:03:03,510
或者它变得更糟糕如果真函数也在变

57
00:03:03,510 --> 00:03:08,720
那么 协变量问题如何影响神经网络?

58
00:03:08,720 --> 00:03:10,880
假设一个如此的神经网络

59
00:03:10,880 --> 00:03:15,225
让我们从这个层的角度来看待学习过程

60
00:03:15,225 --> 00:03:16,995
也就是第三层

61
00:03:16,995 --> 00:03:22,145
假设网络已经学习得出了参数w3和b3

62
00:03:22,145 --> 00:03:24,860
从第三层的角度来看

63
00:03:24,860 --> 00:03:27,665
它从前面层获得了某些值

64
00:03:27,665 --> 00:03:30,020
然后它会做些处理来使

65
00:03:30,020 --> 00:03:34,305
输出(带帽子的)Y接近真实值Y

66
00:03:34,305 --> 00:03:38,340
让我挡住左侧的节点一会儿

67
00:03:38,340 --> 00:03:41,785
所以从第三层角度来看

68
00:03:41,785 --> 00:03:44,265
它获得了某些值

69
00:03:44,265 --> 00:03:48,520
假设为a21 a22 a23 a24

70
00:03:48,520 --> 00:03:57,102
但是这些值可能和特征值x1 x2 x3 x4一样

71
00:03:57,102 --> 00:04:02,240
第三级隐藏层的作用就是

72
00:04:02,240 --> 00:04:08,225
以这些值做输入 然后找到某种将它们映射到(戴帽子的)Y的方法

73
00:04:08,225 --> 00:04:10,760
你可以假设做梯度衰减

74
00:04:10,760 --> 00:04:17,525
那么这些参数w3 b3以及w4 b4

75
00:04:17,525 --> 00:04:21,800
甚至是w5 b5 可能试图学习这些参数

76
00:04:21,800 --> 00:04:23,360
所以网络做得很好

77
00:04:23,360 --> 00:04:29,460
它把我在左侧画的值映射到输出值(戴帽子的)Y

78
00:04:29,460 --> 00:04:33,470
现在让我们再一次加入左侧网络

79
00:04:33,470 --> 00:04:42,226
网络也会学习参数w2 b2以及w1 b1

80
00:04:42,226 --> 00:04:45,305
所以当这些参数变化时

81
00:04:45,305 --> 00:04:49,795
这些值 如a2 也会变化

82
00:04:49,795 --> 00:04:53,080
所以从第三级隐藏层角度来看

83
00:04:53,080 --> 00:04:56,735
这些隐藏单元值一直在变

84
00:04:56,735 --> 00:04:59,090
所以它受协变量问题影响

85
00:04:59,090 --> 00:05:02,435
所以它受协变量问题影响 这在之前我们谈过

86
00:05:02,435 --> 00:05:04,115
所以BN算法所做的就是

87
00:05:04,115 --> 00:05:10,730
它减少了这些隐藏单元值的 分布的不稳定性

88
00:05:10,730 --> 00:05:14,825
假设我们想画出这些隐藏单元值的分布

89
00:05:14,825 --> 00:05:17,948
技术上讲就是归一化后的z

90
00:05:17,948 --> 00:05:24,970
所以这实际上是z21和z22

91
00:05:24,970 --> 00:05:27,862
我不画四个值 就画两个值

92
00:05:27,862 --> 00:05:29,670
好使我们从2D来看

93
00:05:29,670 --> 00:05:32,015
BN算法的意思是说

94
00:05:32,015 --> 00:05:35,745
z21和z22的值可以变化

95
00:05:35,745 --> 00:05:37,514
当神经网络刷新前几层参数时 它们的确会变化

96
00:05:37,514 --> 00:05:41,215
当神经网络刷新前几层参数时 它们的确会变化

97
00:05:41,215 --> 00:05:44,930
但BN算法确保的是无论它怎么变

98
00:05:44,930 --> 00:05:55,050
z21和z22的均值和方差将保持不变

99
00:05:55,050 --> 00:05:59,900
所以尽管z21和z22的值变化

100
00:05:59,900 --> 00:06:07,115
它们的均值一直为0 方差一直为1

101
00:06:07,115 --> 00:06:09,940
或者 并不一定是均值0 方差1

102
00:06:09,940 --> 00:06:17,295
但是它们的值由beta2 gamma2控制

103
00:06:17,295 --> 00:06:19,228
这两个参数由神经网络选定

104
00:06:19,228 --> 00:06:22,270
所以可以使均值为0 方差为1

105
00:06:22,270 --> 00:06:24,655
或者 可以是其它任何均值和方差

106
00:06:24,655 --> 00:06:26,305
但是这样做

107
00:06:26,305 --> 00:06:32,290
限制了先前层中参数的更新

108
00:06:32,290 --> 00:06:35,110
对第三层 现在所看到和要学习的值的分布的 影响

109
00:06:35,110 --> 00:06:38,790
对第三层 现在所看到和要学习的值的分布的 影响

110
00:06:38,790 --> 00:06:44,370
所以 BN算法减少输入值变化所产生的问题

111
00:06:44,370 --> 00:06:48,895
它的确使这些值变得稳定

112
00:06:48,895 --> 00:06:55,155
所以神经网络的后层 可以有更加稳固的基础

113
00:06:55,155 --> 00:06:57,655
尽管输入分布变化了一点

114
00:06:57,655 --> 00:07:00,580
它变化的更少 实际是

115
00:07:00,580 --> 00:07:03,325
尽管前几层继续学习

116
00:07:03,325 --> 00:07:06,640
后面层适应前面层变化的力量被减弱

117
00:07:06,640 --> 00:07:10,180
后面层适应前面层变化的力量被减弱

118
00:07:10,180 --> 00:07:12,925
如果我们愿意 BN算法削弱了

119
00:07:12,925 --> 00:07:15,445
前面层参数和后层参数之间的耦合

120
00:07:15,445 --> 00:07:18,020
前面层参数和后层参数之间的耦合

121
00:07:18,020 --> 00:07:22,147
所以它允许网络的每一层独立学习

122
00:07:22,147 --> 00:07:25,210
有一点独立于其它层的意思

123
00:07:25,210 --> 00:07:29,145
所以这将有效提升整个网络学习速度

124
00:07:29,145 --> 00:07:32,161
所以我希望这做出了进一步解释

125
00:07:32,161 --> 00:07:35,620
但是结论是 BN算法意味着

126
00:07:35,620 --> 00:07:39,010
尤其是从神经网络某一后层角度来看

127
00:07:39,010 --> 00:07:43,090
前面的层的影响并不会很大

128
00:07:43,090 --> 00:07:46,320
因为它们被同一均值和方差所限制

129
00:07:46,320 --> 00:07:50,260
所以这使后层的学习工作变得更加简单

130
00:07:50,260 --> 00:07:52,669
BN算法还有第二个效果

131
00:07:52,669 --> 00:07:55,940
它具有轻微的正则化效果

132
00:07:55,940 --> 00:07:59,885
mini-batch是BN算法中一个模糊的概念

133
00:07:59,885 --> 00:08:02,090
例如 mini-batch xt

134
00:08:02,090 --> 00:08:04,660
拥有值zt

135
00:08:04,660 --> 00:08:07,225
还有值zl

136
00:08:07,225 --> 00:08:12,730
均值和方差已经在该min-batch上归一化过了

137
00:08:12,730 --> 00:08:15,895
现在 因为我们在该min-batch上计算了均值和方差

138
00:08:15,895 --> 00:08:20,245
而不是在整个数据集上计算

139
00:08:20,245 --> 00:08:22,960
所以该均值和方差包含有噪声

140
00:08:22,960 --> 00:08:25,540
因为这仅仅是在尺寸为64或128或256<br />或更多的训练例子的mini-batch上计算而来

141
00:08:25,540 --> 00:08:28,145
因为这仅仅是在尺寸为64或128或256<br />或更多的训练例子的mini-batch上计算而来

142
00:08:28,145 --> 00:08:32,335
因为这仅仅是在尺寸为64或128或256<br />或更多的训练例子的mini-batch上计算而来

143
00:08:32,335 --> 00:08:35,935
所以均值和方差有点噪声

144
00:08:35,935 --> 00:08:40,195
因为它是由相对较少的数据集评估得来<br />该归一化过程

145
00:08:40,195 --> 00:08:43,363
从zl 到 (带波浪号的)zl

146
00:08:43,363 --> 00:08:46,135
这个过程也会产生噪声

147
00:08:46,135 --> 00:08:51,420
因为它是用带有一定噪声的均值和方差来计算的

148
00:08:51,420 --> 00:08:54,817
所以和dropout算法类似

149
00:08:54,817 --> 00:08:57,980
它会为每个隐藏层的激活函数增加一些噪声

150
00:08:57,980 --> 00:08:59,905
dropout有噪声的原因是

151
00:08:59,905 --> 00:09:04,180
它将隐藏单元以一定概率乘以0

152
00:09:04,180 --> 00:09:06,870
以一定概率乘以1

153
00:09:06,870 --> 00:09:12,350
所以dropout有着许多噪声<br />因为它要么乘以0要么乘以1

154
00:09:12,350 --> 00:09:18,360
然而BN算法有着许多噪声<br />因为被标准方差归一化了

155
00:09:18,360 --> 00:09:21,655
此外 因为减去均值<br />BN算法还有附加噪声

156
00:09:21,655 --> 00:09:25,825
这里 均值和标准方差的评估值都含有噪声

157
00:09:25,825 --> 00:09:29,785
所以 和dropout类似

158
00:09:29,785 --> 00:09:33,220
BN算法因此具有轻微的正则化效果

159
00:09:33,220 --> 00:09:35,435
通过在隐藏单元上增加噪声

160
00:09:35,435 --> 00:09:41,280
它使后续的隐藏单元不要过度依赖其它隐藏单元

161
00:09:41,280 --> 00:09:43,025
所以 和dropout类似

162
00:09:43,025 --> 00:09:47,620
这增加了隐藏层噪声 因具有轻微的正则化效果

163
00:09:47,620 --> 00:09:50,064
因为加入的噪声太小

164
00:09:50,064 --> 00:09:52,572
这没有强大的正则化效果

165
00:09:52,572 --> 00:09:56,760
那么我们会选择一道使用BN和dropout算法

166
00:09:56,760 --> 00:09:59,880
如果你想获得更强大的正则化效果的话

167
00:09:59,880 --> 00:10:03,060
如果你想获得更强大的正则化效果的话

168
00:10:03,060 --> 00:10:06,195
另一个模糊的效果是

169
00:10:06,195 --> 00:10:08,454
如果我们采用更大尺寸的mini-batch

170
00:10:08,454 --> 00:10:11,200
假设尺寸为

171
00:10:11,200 --> 00:10:13,725
512而不是64

172
00:10:13,725 --> 00:10:15,934
通过使用更大尺寸的mini-batch

173
00:10:15,934 --> 00:10:20,940
我们可以减少噪声同时也减少了正则化效果

174
00:10:20,940 --> 00:10:24,030
所以这是dropout算法的一个奇怪特性

175
00:10:24,030 --> 00:10:27,435
那就是通过使用更大尺寸的min-batch

176
00:10:27,435 --> 00:10:29,870
将会削弱正则化效果

177
00:10:29,870 --> 00:10:33,833
说到这里 我肯定不会用BN算法作为正则化器

178
00:10:33,833 --> 00:10:36,625
毕竟这不是BN算法的目的

179
00:10:36,625 --> 00:10:44,250
但是有时候它对我们的学习算法<br />有着些额外的有意或无意的影响

180
00:10:44,250 --> 00:10:48,390
但是 一定不要将BN算法看作是正则化方法

181
00:10:48,390 --> 00:10:50,070
而是把它用来作为归一化

182
00:10:50,070 --> 00:10:53,770
隐藏单元激活函数用以加速学习的方法

183
00:10:53,770 --> 00:10:57,900
我认为正则化效果只是一种无意的副作用

184
00:10:57,900 --> 00:11:02,430
所以我希望这能帮助你们更好的理解BN算法的效果

185
00:11:02,430 --> 00:11:04,580
在做BN算法总结前

186
00:11:04,580 --> 00:11:06,855
还有一个我希望你们了解的细节

187
00:11:06,855 --> 00:11:11,254
那就是BN算法中 一个mini-batch处理一次数据

188
00:11:11,254 --> 00:11:14,520
在该mini-batch上计算均值和方差

189
00:11:14,520 --> 00:11:15,720
所以在测试时

190
00:11:15,720 --> 00:11:18,150
当你尝试做出预测<br />和评估神经网络时

191
00:11:18,150 --> 00:11:20,400
你可能没有包含多个例子的min-batch

192
00:11:20,400 --> 00:11:24,035
你可能每次只处理一个例子

193
00:11:24,035 --> 00:11:26,400
所以 在测试时你需要做些变化

194
00:11:26,400 --> 00:11:29,430
来确保你做出的预测准确

195
00:11:29,430 --> 00:11:32,197
在下个也就是最终关于BN算法的视频中

196
00:11:32,197 --> 00:11:35,490
让我们来讨论 使用BN算法训练神经网络以正确做出预测<br />所要做的细节

197
00:11:35,490 --> 00:11:39,290
让我们来讨论 使用BN算法训练神经网络以正确做出预测<br />所要做的细节