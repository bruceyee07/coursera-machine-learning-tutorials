1
00:00:00,320 --> 00:00:01,820
在深度学习不断兴起的过程中

2
00:00:01,820 --> 00:00:06,460
最重要的创新之一是一种
叫批量归一化 (Batch Normalization) 的算法

3
00:00:06,460 --> 00:00:10,860
它由Sergey Ioffe 和 Christian Szegedy提出

4
00:00:10,860 --> 00:00:14,096
可以让你的超参搜索变得很简单

5
00:00:14,096 --> 00:00:15,970
让你的神经网络变得更加具有鲁棒性

6
00:00:15,970 --> 00:00:20,492
可以让你的神经网络对于超参数的选择上不再那么敏感

7
00:00:20,492 --> 00:00:25,029
而且可以让你更容易地训练非常深的网络

8
00:00:25,029 --> 00:00:27,850
让我们来看看批量归一化是如何工作的

9
00:00:27,850 --> 00:00:32,860
你也许记得当训练一个模型比如说逻辑回归模型时

10
00:00:32,860 --> 00:00:37,990
对输入特征进行归一化可以加速学习过程 这意味着

11
00:00:37,990 --> 00:00:40,680
计算平均值 用训练集减去平均值

12
00:00:40,680 --> 00:00:42,150
计算方差

13
00:00:44,320 --> 00:00:46,090
求x[i]的平方和

14
00:00:46,090 --> 00:00:48,140
这是按元素相乘的平方

15
00:00:49,990 --> 00:00:53,160
然后用方差来归一化你的数据集

16
00:00:53,160 --> 00:00:57,699
在前面我们讲过这是如何让你的机器学习问题

17
00:00:57,699 --> 00:01:02,887
的图形从扁的变得更圆些

18
00:01:02,887 --> 00:01:07,240
这会便于类似梯度下降的算法去做优化

19
00:01:07,240 --> 00:01:12,130
所以对于神经网络或者逻辑回归

20
00:01:12,130 --> 00:01:15,530
针对输入值的归一化处理是有用的

21
00:01:15,530 --> 00:01:17,810
那么对于层数更多的模型呢？

22
00:01:17,810 --> 00:01:22,998
你不仅有输入特征值x 在这一层你还有激活函数的输出结果 a[1]

23
00:01:22,998 --> 00:01:27,210
在这一层还有激活函数的输出结果 a[2] 等等

24
00:01:27,210 --> 00:01:31,220
所以如果你想训练这些参数 w[3] b[3]

25
00:01:32,600 --> 00:01:36,900
除非你能对a[2]进行标准差归一化

26
00:01:36,900 --> 00:01:41,330
否则对于 w[3] b[3]的训练都不会太有效率

27
00:01:43,140 --> 00:01:46,960
在逻辑回归的例子里 我们看到了对x1

28
00:01:46,960 --> 00:01:51,460
x2 x3做归一化可以对对w和b的训练更有效

29
00:01:51,460 --> 00:01:56,060
所以这里的问题就是 对于任何一个隐藏层我们是否也可以

30
00:01:57,980 --> 00:02:02,143
对a的值做归一化呢 比如拿a2举例

31
00:02:02,143 --> 00:02:07,796
其实可以是任何一个隐藏层

32
00:02:07,796 --> 00:02:12,580
是否可以让w3 b3的训练更快呢

33
00:02:12,580 --> 00:02:15,283
因为a2是下一层的输入

34
00:02:15,283 --> 00:02:18,870
它影响着你对w[3] b[3]的训练

35
00:02:20,390 --> 00:02:24,418
这就是batch norm

36
00:02:24,418 --> 00:02:25,287
简单的解释

37
00:02:25,287 --> 00:02:31,730
虽然在实际中我们归一化时针对的
并不是a[2]而是z[2]

38
00:02:31,730 --> 00:02:36,030
对于在激活函数之前做归一化，也就是指z[2]

39
00:02:36,030 --> 00:02:40,760
还是在激活函数之后做归一化，也就是指a[2]

40
00:02:40,760 --> 00:02:45,000
这一点上学术界还是有一些争议的

41
00:02:45,000 --> 00:02:48,655
实际中 对z[2]做归一化要普遍的多

42
00:02:48,655 --> 00:02:51,253
也就是这里所呈现的方法

43
00:02:51,253 --> 00:02:54,550
我也推荐你把它作为默认的方法

44
00:02:54,550 --> 00:02:58,060
下面介绍如何实现batch norm

45
00:02:58,060 --> 00:03:06,070
假使有一些神经网络中的中间值

46
00:03:09,470 --> 00:03:15,270
隐藏单元z(1)到z(m)

47
00:03:15,270 --> 00:03:19,365
这些来源于某一个隐藏层

48
00:03:19,365 --> 00:03:23,686
所以更准确的写法

49
00:03:23,686 --> 00:03:28,750
是加上层数的上标l

50
00:03:28,750 --> 00:03:33,110
但为了简化, 我要省略这个 [l],

51
00:03:33,110 --> 00:03:35,350
像这一行这么写

52
00:03:35,350 --> 00:03:41,260
当给你这些值时 你要做的是像这样计算平均值

53
00:03:41,260 --> 00:03:46,277
所有这些都是针对某一层 但是我省略了这里的[l]

54
00:03:46,277 --> 00:03:51,153
然后再用你喜欢公式计算方差

55
00:03:51,153 --> 00:03:56,043
然后针对每个z(i)去归一化

56
00:03:56,043 --> 00:04:00,908
就是把值减去均值

57
00:04:00,908 --> 00:04:04,310
然后除以标准差

58
00:04:04,310 --> 00:04:09,312
为了避免在某些情况下分母为0

59
00:04:09,312 --> 00:04:14,460
通常你需要在这里加上ε

60
00:04:14,460 --> 00:04:17,405
这样我们就把z归一化为

61
00:04:17,405 --> 00:04:23,010
一组均值0方差1的值了

62
00:04:23,010 --> 00:04:25,903
每一组z都是均值0方差1

63
00:04:25,903 --> 00:04:31,352
但是我们并不希望所有的隐藏单元都是这样的

64
00:04:31,352 --> 00:04:38,953
也许本身它们的分布就有不同

65
00:04:38,953 --> 00:04:42,939
所以我们可以这么做

66
00:04:42,939 --> 00:04:48,434
z tilde = γ * zi * norm + β

67
00:04:48,434 --> 00:04:55,210
这里的γ和β值可以从你的模型中学习

68
00:04:58,957 --> 00:05:03,675
这样我们就可以使用梯度下降算法 或者其他类似算法比如

69
00:05:03,675 --> 00:05:08,136
momentum的梯度下降算法 或者atom算法 来更新γ和β的值

70
00:05:08,136 --> 00:05:11,410
就像更新神经网络的权重一样

71
00:05:11,410 --> 00:05:16,582
事实上 如果γ等于δ^2+ε的平方根

72
00:05:16,582 --> 00:05:22,140
让z波浪线的均值可以是任何数

73
00:05:22,140 --> 00:05:27,720
实际上 如果gamma等于它

74
00:05:28,800 --> 00:05:33,570
这时候这里的z波浪线就等于z_norm

75
00:05:33,570 --> 00:05:39,318
如果beta等于mu 像这个值

76
00:05:39,318 --> 00:05:43,998
这使得gamma z norm + beta 等于

77
00:05:43,998 --> 00:05:49,540
通过选择不同的γ和β
可以让隐藏单元的呈现不同的分布

78
00:05:49,540 --> 00:05:52,284
像之前对z[1] z[2]……的处理一样

79
00:05:52,284 --> 00:05:57,780
那么实际上 z tilde i = zi

80
00:05:57,780 --> 00:06:02,633
所以通过恰当的设置gamma和beta

81
00:06:02,633 --> 00:06:05,321
归一化就是

82
00:06:05,321 --> 00:06:11,175
这四步确定的方程

83
00:06:11,175 --> 00:06:16,112
但是通过选择不同gamma和beta的值 它可以让你的隐藏层

84
00:06:16,112 --> 00:06:19,320
或者 可以是其它任何均值和方差

85
00:06:19,320 --> 00:06:23,538
把这方法运用到你的神经网络就是

86
00:06:23,538 --> 00:06:28,583
在你之前使用这些值z1 z2 等等时

87
00:06:28,583 --> 00:06:35,195
你换成z tilde i 代替 zi

88
00:06:35,195 --> 00:06:39,910
再进行下面的计算

89
00:06:39,910 --> 00:06:45,129
如果你想更精确

90
00:06:45,129 --> 00:06:46,910
可以把[l]再添进去

91
00:06:46,910 --> 00:06:51,319
我希望你从这里学到的是

92
00:06:51,319 --> 00:06:56,140
如何对输入特征X做归一化来帮助神经网络的训练

93
00:06:56,140 --> 00:07:00,029
以及batch norm所做的就是

94
00:07:00,029 --> 00:07:01,283
不仅仅在输入层

95
00:07:01,283 --> 00:07:04,810
而且在一些隐藏层上也做归一化

96
00:07:04,810 --> 00:07:09,085
你使用这种归一化方法

97
00:07:09,085 --> 00:07:12,390
对某些隐藏单元的值z做归一化

98
00:07:12,390 --> 00:07:16,833
但是输入层和隐藏层的归一化还有一点不同

99
00:07:16,833 --> 00:07:21,220
就是隐藏层归一化后并不一定是均值0方差1

100
00:07:21,220 --> 00:07:24,247
比如 如果你的激活函数是sigmoid

101
00:07:24,247 --> 00:07:27,230
你就不希望归一化后的值都聚集在这里

102
00:07:27,230 --> 00:07:31,582
可能希望它们有更大的方差

103
00:07:31,582 --> 00:07:35,322
以便于更好的利用s函数非线性的特性

104
00:07:35,322 --> 00:07:41,060
而不是所有的值都在中间这段近似直线的区域上

105
00:07:41,060 --> 00:07:45,067
这就是为什么通过设置γ和β

106
00:07:45,067 --> 00:07:51,230
你可以控制z(i)在你希望的范围内

107
00:07:51,230 --> 00:07:55,671
或者说它真正实现的是

108
00:07:55,671 --> 00:07:59,226
通过两个参数γ和β

109
00:07:59,226 --> 00:08:03,429
来让你的隐藏单元有可控的放差和均值

110
00:08:03,429 --> 00:08:07,826
而这两个参数是可以在算法中自由设置的

111
00:08:07,826 --> 00:08:13,004
目的就是

112
00:08:13,004 --> 00:08:18,660
可以得到一些修正的均值和方差

113
00:08:18,660 --> 00:08:22,320
这意味可以是均值0方差1

114
00:08:22,320 --> 00:08:26,680
也可以是被参数γ β控制的其他值

115
00:08:26,680 --> 00:08:30,424
希望这一节你对如何实现batch norm有了初步的了解

116
00:08:30,424 --> 00:08:32,830
至少是在单层神经网络上

117
00:08:32,830 --> 00:08:36,104
下一节我将向你展示如何在神经网络中使用batch norm

118
00:08:36,104 --> 00:08:39,052
包括深度神经网络
以及如何用数据训练它

119
00:08:39,052 --> 00:08:41,700
以及如何使它在不同的层都能起到作用

120
00:08:41,700 --> 00:08:45,450
在那之后我们会告诉你一些直观上的理解

121
00:08:45,450 --> 00:08:47,120
为什么batch norm可以帮助你训练神经网络

122
00:08:47,120 --> 00:08:51,424
所以如果到这里还觉得有些困惑 请坚持下去

123
00:08:51,424 --> 00:08:54,949
我想下面两节会让你理解它