1
00:00:00,000 --> 00:00:02,415
자, 배치 정규화는 어떻게 잘 작동하는 것일까요?

2
00:00:02,415 --> 00:00:06,680
한가지 이유는, 여러분이 보셨드시 X 라는 입력 특성을 정규화

3
00:00:06,680 --> 00:00:09,380
하여 평균값을 0, variance를 1로 만들었는데요,

4
00:00:09,380 --> 00:00:10,740
이것이 러닝의 속도를 높혀줍니다.

5
00:00:10,740 --> 00:00:13,920
어떤 특성이 0에서 1의 값을 갖는다거나,

6
00:00:13,920 --> 00:00:15,735
다른거는 1에서 1000의 범위를 갖는 것이 아니라

7
00:00:15,735 --> 00:00:18,835
모든 특성을 정규화함으로써

8
00:00:18,835 --> 00:00:22,975
입력 특성 x가 비슷한 범위의 값을 주고, 이로 인해 러닝의 속도가 증가합니다.

9
00:00:22,975 --> 00:00:25,770
배치 정규화가 잘 작동하는 한가지 직관적인 부분은

10
00:00:25,770 --> 00:00:27,750
이렇게 비슷하게 하는 것입니다.

11
00:00:27,750 --> 00:00:32,770
여기 입력 층에 대한 것뿐만 아니라 숨겨진 유닛을 위해서도 그렇게 하는 것입니다.

12
00:00:32,770 --> 00:00:37,380
이것은 배치 정규화가 하는 것의 일부 그림일 뿐인데요, 

13
00:00:37,380 --> 00:00:39,180
다른 직관적인 부분이 또 있습니다.

14
00:00:39,180 --> 00:00:43,210
이런 내용이 여러분이 더 깊이 배치 정규화의 역할을 이해하는데 
도움을 줄 것입니다.

15
00:00:43,210 --> 00:00:46,320
이번 비디오를 통해 보도록 하겠습니다.

16
00:00:46,320 --> 00:00:48,490
배치 정규화가 잘 작동하는 두번째 이유로 

17
00:00:48,490 --> 00:00:50,295
이것은 weight를 만듭니다. 

18
00:00:50,295 --> 00:00:52,575
여러분의 네트워크보다 더 나중 또는 더 깊은 곳 같은 경우,

19
00:00:52,575 --> 00:00:56,095
예를 들어, 10번째 층에서는 weight의 변화에 더

20
00:00:56,095 --> 00:01:00,300
더 튼튼합니다. 신경망에서 더 이른 쪽의 층, 예를 들어 첫번째 층보다 말이죠. 

21
00:01:00,300 --> 00:01:01,810
무슨 말인지 설명 드리자면, 

22
00:01:01,810 --> 00:01:04,775
더 선명한 예제를 보겠습니다.

23
00:01:04,775 --> 00:01:06,420
여기 네트워크의 트레이닝을 보겠습니다.

24
00:01:06,420 --> 00:01:07,770
옅은 트레이닝일 수도 있는데요, 

25
00:01:07,770 --> 00:01:11,715
로지스틱 회귀분석 또는 신경망일 수도 있는데요, 

26
00:01:11,715 --> 00:01:17,260
이런 regression과 같은 얕은 네트워크나 유명한 고양이

27
00:01:17,260 --> 00:01:21,120
인식 업무와 같은 것 말이죠. 

28
00:01:21,120 --> 00:01:26,745
그런데 여러분이 모든 데이터를 검은 고양이의 사진으로 트레이닝 했다고 해봅시다.

29
00:01:26,745 --> 00:01:31,240
이것을 색깔이 있는 고양이 데이터에

30
00:01:31,240 --> 00:01:33,325
적용하려고 하면

31
00:01:33,325 --> 00:01:36,895
positive example은 왼쪽에 있는 검정색 고양이뿐만 아니라 

32
00:01:36,895 --> 00:01:40,442
오른쪽에 있는 색깔 있는 고양이도 포함될 것입니다.

33
00:01:40,442 --> 00:01:43,160
그러면 classfier가 잘 작동되지 않을 수 있습니다.

34
00:01:43,160 --> 00:01:47,907
그러므로 그림에서, 트레이닝 세트가 만약 이렇게 생겼으면, 

35
00:01:47,907 --> 00:01:52,383
positive example이 여기 있고, negative example이 여기 있는 경우죠, 

36
00:01:52,383 --> 00:01:54,970
그런데 일반화해 야하면, 

37
00:01:54,970 --> 00:02:02,335
여기 이런 데이터세트에 말이죠. 여기서는 positive example이 여기 있고, 
negative example이 여기 있겠죠.

38
00:02:02,335 --> 00:02:05,800
그러면 여기 왼쪽에서 트레이닝 된 모델 같은 경우에

39
00:02:05,800 --> 00:02:09,430
오른쪽에 있는 데이터에서 잘 못 할 것입니다.

40
00:02:09,430 --> 00:02:13,901
똑같이 잘 작동하는 함수여도 말이죠. 

41
00:02:13,901 --> 00:02:19,230
하지만 여러분의 러닝 알고리즘이 이런 초록색 decision 경계선을

42
00:02:19,230 --> 00:02:21,500
단순히 왼쪽의 데이터를 보고 발견하는 것은 예상하지 않은 것입니다.

43
00:02:21,500 --> 00:02:26,410
그리하여 이런 데이터의 분포가 변하는 아이디어를

44
00:02:26,410 --> 00:02:31,657
covariate shift라고 하는 거창한 이름으로 불리는데요, 

45
00:02:31,657 --> 00:02:33,695
기본적인 아이디어는 

46
00:02:33,695 --> 00:02:35,910
만약 X에서 Y로 가는 매핑을 배운 경우, 

47
00:02:35,910 --> 00:02:38,435
x의 분포도가 만일 변경하면

48
00:02:38,435 --> 00:02:41,545
러닝 알고리즘을 다시 트레이닝 시켜야 할 수도 있다는 것입니다.

49
00:02:41,545 --> 00:02:43,925
이것은 x에서 y로 가는 매핑,

50
00:02:43,925 --> 00:02:45,230
ground truth 함수가 변하지

51
00:02:45,230 --> 00:02:46,775
않더라도 참으로 

52
00:02:46,775 --> 00:02:49,430
적용됩니다. 이번 예제같이 말이죠. 

53
00:02:49,430 --> 00:02:51,630
그 이유는 ground truth 함수는 

54
00:02:51,630 --> 00:02:53,990
이 그림이 고양인지 아닌지에 대한 것입니다.

55
00:02:53,990 --> 00:02:57,590
그리고 함수를 다시 트레이닝 시켜야하는 필요성이 

56
00:02:57,590 --> 00:03:03,510
격상되는데요, 또는 더 악화됩니다. ground truth 함수가 이동하는 경우에 말이죠. 

57
00:03:03,510 --> 00:03:08,720
그러면 이러한 covariate shift 문제가 신경망 네트워크에서는 어떻게 적용될까요?

58
00:03:08,720 --> 00:03:10,880
이와 같은 심층 신경망을 고려해보겠습니다.

59
00:03:10,880 --> 00:03:15,225
이제 러닝 절차를 어떤 특정 층의 기준으로 볼텐데요, 

60
00:03:15,225 --> 00:03:16,995
세번째 층이라고 해보겠습니다.

61
00:03:16,995 --> 00:03:22,145
그럼 이 네트워크는 w3와 b3 파라미터를 배웠습니다.

62
00:03:22,145 --> 00:03:24,860
그러면 3번째 층의 시각으로는

63
00:03:24,860 --> 00:03:27,665
특정 값들을 이전 층에서 가지고 오는데요, 

64
00:03:27,665 --> 00:03:30,020
어떤 것들을 진행해서 y hat을 

65
00:03:30,020 --> 00:03:34,305
ground truth 값인 Y와 비슷하게 예측해야 하는데요, 

66
00:03:34,305 --> 00:03:38,340
그러면 잠시 왼쪽에 있는 노드들은 가려보겠습니다.

67
00:03:38,340 --> 00:03:41,785
세번째 숨겨진 층의 기준으로는 

68
00:03:41,785 --> 00:03:44,265
이러한 값을 갖고, 

69
00:03:44,265 --> 00:03:48,520
이 값들을 A_2_1, A_2_2, A_2_3, 그리고 A_2_4 라고 하겠습니다.

70
00:03:48,520 --> 00:03:57,102
그러나 이런 값들은 차라리 X1, X2, X3, X4인

71
00:03:57,102 --> 00:04:02,240
특성이라고 칭하는 것이 낫겠습니다. 그리고 세번째 숨겨진 층의 

72
00:04:02,240 --> 00:04:08,225
역할은 이 값들을 갖고 ŷ 을 매핑하는 것입니다.

73
00:04:08,225 --> 00:04:10,760
그러면 기울기 강하를 진행하는데요

74
00:04:10,760 --> 00:04:17,525
여기 파라미터들 W_3_B_3,W_4_B_4,

75
00:04:17,525 --> 00:04:21,800
그리고 W_5_B_5도 말이죠, 잘하면 이런 파리미터를 배워서 

76
00:04:21,800 --> 00:04:23,360
네트워크가 여기 왼쪽에 그린 값들에 대해

77
00:04:23,360 --> 00:04:29,460
매핑을 잘할 수 있게 될 수도 있겠죠. ŷ 값을 말이죠. 

78
00:04:29,460 --> 00:04:33,470
그럼 이제 왼쪽의 네트워크를 다시 보여드리겠습니다.

79
00:04:33,470 --> 00:04:42,226
네트워크 또한 W_2_B_2 와 W_1B_1의 파라미터를 사용합니다.

80
00:04:42,226 --> 00:04:45,305
그렇기 때문에 여기 파라미터의 값이 변하면

81
00:04:45,305 --> 00:04:49,795
여기 값, 즉 a2의 값도 변할 것입니다.

82
00:04:49,795 --> 00:04:53,080
그러면 세번째 층의 시각에서 보면, 

83
00:04:53,080 --> 00:04:56,735
여기 숨겨진 유닛의 값은 항상 변하는 것입니다.

84
00:04:56,735 --> 00:04:59,090
그러므로 이전 슬라이드에서 언급했던 

85
00:04:59,090 --> 00:05:02,435
covariate shift 문제로 인해 영향을 받게 되는 것입니다.

86
00:05:02,435 --> 00:05:04,115
그러므로 배치 정규화가 하는 것은, 

87
00:05:04,115 --> 00:05:10,730
이런 숨겨진 유닛의 분포가 왔다 갔다 이동하는 양을 줄여줍니다.

88
00:05:10,730 --> 00:05:14,825
여기 숨겨진 유닛 값들에 대해 분포도를 그리자면, 

89
00:05:14,825 --> 00:05:17,948
이것은 엄밀히 이야기하면 정규화 된 Z 값입니다.

90
00:05:17,948 --> 00:05:24,970
그러면 실제로 이것은 Z_2_1 그리고 Z_2_2입니다.

91
00:05:24,970 --> 00:05:27,862
그리고 저는 4개의 값 대신 2개의 값을 나타내어 

92
00:05:27,862 --> 00:05:29,670
2차원의 그래프에서 보도록 하겠습니다.

93
00:05:29,670 --> 00:05:32,015
배치 정규화가 주장하는 것은 

94
00:05:32,015 --> 00:05:35,745
for Z_2_1 Z 와 Z_2_2의 값이 변할 수 있다는 것입니다.

95
00:05:35,745 --> 00:05:37,514
그러면 정말로 신경망이 이전에 있는 층들의 파라미터들을 업데이트하면서 

96
00:05:37,514 --> 00:05:41,215
그 값이 변할 것입니다.

97
00:05:41,215 --> 00:05:44,930
하지만 배치 정규화가 보장하는 것은, 어떻게 변하더라도,

98
00:05:44,930 --> 00:05:55,050
Z_2_1 와 Z_2_2 의 평균값과 variance는 똑같을 것이라는 점입니다.

99
00:05:55,050 --> 00:05:59,900
그러므로 구체적으로 Z_2_1 와 Z_2_2 값이 변하더라도

100
00:05:59,900 --> 00:06:07,115
그들의 평균값과 variance는 변동없이 그대로 각각 0과 1의 값을 유지할 것입니다.

101
00:06:07,115 --> 00:06:09,940
꼭 평균값 0과 variance 1이 아닐수는 있는데요, 

102
00:06:09,940 --> 00:06:17,295
어쨋든 베타2와 감마2에 지배되는 값으로 변동없이 유지될 것입니다.

103
00:06:17,295 --> 00:06:19,228
신경망이 결정하는 경우에는, 

104
00:06:19,228 --> 00:06:22,270
평균값 0과 variance 1을 강요할 수 있습니다.

105
00:06:22,270 --> 00:06:24,655
또는 다른 값으로 지정할 수도 있겠죠. 

106
00:06:24,655 --> 00:06:26,305
이것이 본질적으로 하는 것은 

107
00:06:26,305 --> 00:06:32,290
이전의 층에서 파라미터가 업데이트 되면서 세번째 층에 

108
00:06:32,290 --> 00:06:35,110
영향을 줄 수 있는 양을 

109
00:06:35,110 --> 00:06:38,790
제한시켜 줍니다. 이 변화량은 세번째 층이 새로 배워야 하는 양이기도 하구요. 

110
00:06:38,790 --> 00:06:44,370
그러므로 배치 정규화는 입력값이 변해서 생기는 문제를 줄여줍니다.

111
00:06:44,370 --> 00:06:48,895
여기 이런 값들을 더 안정감 있게 해주고, 

112
00:06:48,895 --> 00:06:55,155
그럼으로 인해, 신경망의 나중의 층들이 그 자리를 지킬 수 있게 틀을 마련해줍니다.

113
00:06:55,155 --> 00:06:57,655
입력값의 분포도가 약간 변하기는 하지만, 

114
00:06:57,655 --> 00:07:00,580
더 작게 변하고, 이것이 하는 것은 

115
00:07:00,580 --> 00:07:03,325
이전 층이 계속 러닝하면서도, 

116
00:07:03,325 --> 00:07:06,640
여기 이 부분이 다른 층에게 압박하는

117
00:07:06,640 --> 00:07:10,180
다음 층들이 적응할 수 있도록 배워야 하는 양이
줄어드는데요,

118
00:07:10,180 --> 00:07:12,925
다르게 말하자면, 앞에층의 파라미터가

119
00:07:12,925 --> 00:07:15,445
배워야하는 것들과 뒷층의 파라미터들이

120
00:07:15,445 --> 00:07:18,020
해야하는 것들의 coupling을 약화시켜줍니다.

121
00:07:18,020 --> 00:07:22,147
그렇게해서 각각의 층이 스스로 러닝을 진행할 수 있도록 해주는 것이죠.

122
00:07:22,147 --> 00:07:25,210
다른 층과는 조금 더 독립적으로 말이죠. 

123
00:07:25,210 --> 00:07:29,145
이것은 결과적으로 전체 네트워크의 속도를 올려주는 효과를 줍니다.

124
00:07:29,145 --> 00:07:32,161
이것이 여러분의 직관에 도움을 줬으면 합니다.

125
00:07:32,161 --> 00:07:35,620
중요한 것은, 배치 정규화의 뜻인데요, 

126
00:07:35,620 --> 00:07:39,010
특히 신경망에서 나중의 층의 시각에서 봤을 때 말입니다.

127
00:07:39,010 --> 00:07:43,090
이전 층은 별로 이동폭이 크지 않습니다. 

128
00:07:43,090 --> 00:07:46,320
똑같은 평균값과 variance를 요하기 때문이죠. 

129
00:07:46,320 --> 00:07:50,260
이것이 다른 층들이 배우는 업무 과정을 쉽게 해줍니다.

130
00:07:50,260 --> 00:07:52,669
배치 정규화는 두번째 효과도 있는데요, 

131
00:07:52,669 --> 00:07:55,940
일반화 효과가 있습니다.

132
00:07:55,940 --> 00:07:59,885
직관적이지 않은 부분은 바로 각각의 미니 배치에 대해서

133
00:07:59,885 --> 00:08:02,090
X_t라고 하겠는데요, 

134
00:08:02,090 --> 00:08:04,660
Z_t의 값을 갖습니다.

135
00:08:04,660 --> 00:08:07,225
그리고 Z_l의 값을 갖구요, 

136
00:08:07,225 --> 00:08:12,730
오로지 여기 미니 배치에서 산출된 scale된 평균값과 variance를 갖습니다.

137
00:08:12,730 --> 00:08:15,895
그러면, 여기 미니 배치 에서 산출된 평균값과variance는

138
00:08:15,895 --> 00:08:20,245
전체 데이터세트에서 계산된 값보다 

139
00:08:20,245 --> 00:08:22,960
더 많은 noise를 함유하고 있기 때문에, 

140
00:08:22,960 --> 00:08:25,540
미니 배치에서만 산출됐기 때문이죠, 

141
00:08:25,540 --> 00:08:28,145
예를 들어, 64 또는 128 또는

142
00:08:28,145 --> 00:08:32,335
256 또는 더 큰 트레이닝 예시일 수 있습니다.

143
00:08:32,335 --> 00:08:35,935
그렇게 mean 과 variance가 더 작은 데이터 샘플에서 계산됐기 때문에

144
00:08:35,935 --> 00:08:40,195
더 noisy 합니다. 그렇기 때문에 scaling 절차도, 

145
00:08:40,195 --> 00:08:43,363
Z_l 에서 Z_2_l 로 가는 절차가 더 

146
00:08:43,363 --> 00:08:46,135
noisy 합니다.

147
00:08:46,135 --> 00:08:51,420
그 이유는 더 noisy한 mean과 variance로 산출됐기 때문이기도 하죠. 

148
00:08:51,420 --> 00:08:54,817
그렇기 때문에 dropout과 비슷하게 

149
00:08:54,817 --> 00:08:57,980
각각의 hidden layer activation에 noise를 더합니다.

150
00:08:57,980 --> 00:08:59,905
dropout 이 noise가 있는 방식은 

151
00:08:59,905 --> 00:09:04,180
숨겨진 유닛을 가지고 0에 곱하고 어떨 확률에 곱합니다.

152
00:09:04,180 --> 00:09:06,870
그리고 1로 어떤 확률을 곱합니다.

153
00:09:06,870 --> 00:09:12,350
그러면 dropout은 복수의 noise가 있는데요, 0 또는 1로 곱해지기 때문이죠, 

154
00:09:12,350 --> 00:09:18,360
반면에 배치 정규화는 복수의 noise가 있습니다. scaling을 표준편차 때문에도 그렇고,

155
00:09:18,360 --> 00:09:21,655
추가적인 noise도 있습니다. 평균값을 빼기 때문이죠. 

156
00:09:21,655 --> 00:09:25,825
여기 평균값과 표준편차의 추정치가 noisy합니다.

157
00:09:25,825 --> 00:09:29,785
그러므로 dropout과 비슷하게

158
00:09:29,785 --> 00:09:33,220
배치 정규화 도 약간의 일반화 효과가 있습니다.

159
00:09:33,220 --> 00:09:35,435
여기 숨겨진 유닛에 noise를 더하여서 

160
00:09:35,435 --> 00:09:41,280
밑에 있는 숨겨진 유닛들이 어느 특정 숨겨진 유닛에 의존하지 않게 하기 때문입니다.

161
00:09:41,280 --> 00:09:43,025
그리고 dropout과 비슷하게, 

162
00:09:43,025 --> 00:09:47,620
숨겨진 층에 noise를 더하여 아주 작은 일반화효과가 생깁니다.

163
00:09:47,620 --> 00:09:50,064
더해지는 noise가 꽤 작기 대문에, 

164
00:09:50,064 --> 00:09:52,572
큰 일반화효과는 아닙니다.

165
00:09:52,572 --> 00:09:56,760
그리고 배치 정규화를 dropout과 같이 사용할수도 있고

166
00:09:56,760 --> 00:09:59,880
dropout에서 조금 더 큰 일반화 효과가 생기길 바라면

167
00:09:59,880 --> 00:10:03,060
배치 정규화를 dropout과 같이 사용할수도 있습니다.

168
00:10:03,060 --> 00:10:06,195
그리고 또 직관적이지 않을 수 있는 부분은, 

169
00:10:06,195 --> 00:10:08,454
더 큰 미니 배치 사이즈를 이용하면

170
00:10:08,454 --> 00:10:11,200
예를 들어, 64 대신에

171
00:10:11,200 --> 00:10:13,725
512를 쓴다고 해보죠

172
00:10:13,725 --> 00:10:15,934
이렇게 더 큰 미니 배치 사이즈를 이용하면

173
00:10:15,934 --> 00:10:20,940
여기 noise를 줄이고 결과적으로 일반화효과를 줄이는 것입니다.

174
00:10:20,940 --> 00:10:24,030
이것이 이상한 dropout의 특성입니다.

175
00:10:24,030 --> 00:10:27,435
바로 더 큰 미니 배치 사이즈를 사용하여

176
00:10:27,435 --> 00:10:29,870
일반화효과를 줄인다는 것입니다.

177
00:10:29,870 --> 00:10:33,833
이것을 말하기 했지만 저 같은 경우엔, 배치 정규화를 일바화로 
사용하지 않을 것입니다.

178
00:10:33,833 --> 00:10:36,625
그것이 배치 정규화의 의도가 아닙니다.

179
00:10:36,625 --> 00:10:44,250
이것을 추가적으로 의도할 수도 있고 안할수도 있는데요

180
00:10:44,250 --> 00:10:48,390
실제로 읿나화로 배치 정규화를 만들지 마십시요. 

181
00:10:48,390 --> 00:10:50,070
정규화하는 방편으로 사용하세요. 

182
00:10:50,070 --> 00:10:53,770
이렇게해서 hidden unit activation이 러닝에 빨라질 수 있게 말이죠. 

183
00:10:53,770 --> 00:10:57,900
그리고 저는 일반화가 거의 의도치 않은 부작용이라고 생각합니다.

184
00:10:57,900 --> 00:11:02,430
이제 배치 정규화가 하는 직관적인 부분을 다웠는데요 ,

185
00:11:02,430 --> 00:11:04,580
배치 정규화 에 대한 내용을 정리하기 전에, 

186
00:11:04,580 --> 00:11:06,855
여러분이 알아야하는 내용이 한가지 있습니다.

187
00:11:06,855 --> 00:11:11,254
그것은 배치 정규화가 한대의 미니 배치씩 데이터 처리한다는 것입니다.

188
00:11:11,254 --> 00:11:14,520
이것은 미니 배치 에서 평균값을 계산하고, variance를 계산합니다.

189
00:11:14,520 --> 00:11:15,720
테스트타임에서는

190
00:11:15,720 --> 00:11:18,150
예사하는 것들을 만드려고 하고, 신경망을 평가하려 합니다.

191
00:11:18,150 --> 00:11:20,400
여러분은 미비 배치 예시가 없을 수도 있는데요 

192
00:11:20,400 --> 00:11:24,035
여러분은 1개의 예시씩 처리하고 있을수도 있습니다.

193
00:11:24,035 --> 00:11:26,400
이런 경우, 테스트타임에 무엇을 해야합니다.

194
00:11:26,400 --> 00:11:29,430
조금다르게 말이죠. 예상하는 것들이 말이 되게요.

195
00:11:29,430 --> 00:11:32,197
다음에 있을 마지막 배치 정규화에 대한 비디오에서는

196
00:11:32,197 --> 00:11:35,490
여러분이 아셔야 하는 냉요에 대해, 

197
00:11:35,490 --> 00:11:39,290
트레이닝 된 신경망으로 배치 정규화를 이용하여 예상하는 방법을 배우겠습니다.