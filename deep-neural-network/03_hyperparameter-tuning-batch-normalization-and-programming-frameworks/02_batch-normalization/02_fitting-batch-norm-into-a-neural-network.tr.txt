Tek bir saklı katmana yığın normunu uygulamak için kullanılabilecek denklemleri gördünüz. Şimdiyse yığın normunun bir derin ağın eğitimine nasıl oturtulduğunu inceleyeceğiz. Diyelim ki buna benzer bir sinirsel ağınız var. Daha önce de belirttiğim gibi buradaki her birim iki şeyi hesaplar Önce Z'yi, ardından da etkilenim işlevi üzerinden A'yı hesaplar. Bu yüzden figürdeki her bir daireyi iki aşamalı hesaplayıcı olarak ele alabiliriz. Aynı şeyi bir sonraki katman için, (yani Z^[2]_1 ve A^[2]_1), ve diğerleri için de gözlemleyebiliriz Yığın normunu uygulamıyor olsaydınız önce bir X girdisini ilk saklı katmana oturtur, ardından Z^[1]'i hesaplardınız, ve bu hesaplama da W^[1] ve B^[1] paremetreleri üzerinden denetlenirdi. Bu koşullar altında Z^[1]'i etkileşim işlevine oturtarak A^[1]'i hesaplardınız. Fakat yığın normu yönteminde bu Z^[1] değerini alıp, ona yığın normunu uygularsınız. Bu uygulama bazen kısaca BN şeklinde belirtilir. Bu işlev iki dağılım, yani Beta^[1] ve Gamma^[1] tarafından yönlendirilir ve bununla birlikte elinize yeni, normalleştirilmiş Z~[1] geçecektir. Ardından, bu değeri etkileşim işlevine oturtarak A^[1]'i, yani Z~[1]'e G^[1]'in uygulanmasının sonucunu elde edersiniz. Böylelikle ilk katmanın hesaplamasını tamamlarsınız. Bu katmanda yığın normu Z'den A'ya doğru yapılan hesaplamanın ortasında ortaya çıkıyor. Sonra, bu A^[1] değerini alıp Z^[2]'yi hesaplamada kullanırsınız, ki bu hesaplama da W^[2] ve B^[2] parametreleri tarafından yönlendirilir. İlk katman için yaptığınıza benzer şekilde Z^[2]'ye yığın normu işlevini uygularsınız (bu işlevi yine BN ile kısaltıyoruz) Bu işlev, bir sonraki katmana has parametreler tarafından yönlendirilmektedir, yani Beta^[2] ve Gamma^[2] ile. Bu işlevin sonucunda elimize Z~[2] geçer. Bu Z~[2]'ye etkileşim işlevini uygulayarak A^[2]'yi elde edersiniz. Bir sonraki katmanlarda da böyle devam eder. Yani yığın normu uygulaması, Z'nin ve A'nın hesaplanması arasında gerçekleşti. Dikkat ederseniz, artık, birinci katmanda normalleştirilmemiş Z değeri (Z^[1]) yerine, normalleştirilmiş Z değerini (Z~[1]) kullanabilirsiniz. Aynısı ikinci katman için de geçerli. normalleştirilmemiş Z_[2] değeri yerine, ortalama ve değişinti değerleriyle normalleştirilmiş Z~[2]'yi kullanabilirsiniz. Bu durumda ağınızın parametreleri W^[1] ve B^[1] oluyor. Bir sonraki sürgüde nedenini göreceğiz ki aslında B parametresinden kurtulabiliriz; ama şimdilik ağımızda yığın normunu uyguladığımız her katman için parametrelerimizin W^[1], B^[1] ... W^[L],B^[L]'den, ve de diğer eklediklerimizden (Beta^[1], Gamma^[1], Beta^[2],Gamma^[2], vb.) ibaret olduğunu varsayalım. Dikkatinizi çekmek isterim ki, buradaki Beta ifadelerinin hiper-parametre betalarla alakası yok. Bahsettiğim, üstelce ağırlaştırılmış ortalamaların hesaplandığı momentumdaki hiper-parametre. "Adam" makalesinin yazarları o hiper-parametreyi beta olarak ifade ediyorlar. "Yığın Dağılım" makalesinin yazarları da kendi parametrelerini beta olarak adlandırmış. ancak bu betaların birbirleriyle hiçbir alakası yok. Ben her iki durumda da "beta" kullanmayı tercih ettim, ki orijinal makaleleri okuduğunuzda daha kolay ilişki kurabilesiniz. Sonuçta bu derste Beta^[1], Beta^[2], vb. diye ifade ettiğimiz, yığın normunun öğrenmeye çalıştığı parametrelerin Momentum, Adam ve RMSprop algoritmalarındaki hiper-parametrelerle alakası yok. Şimdi mademki bunlar algoritmanızın yeni parametreleri, istediğiniz iyileme yöntemini kullanabilir (mesela dereceli alçalma) ve böylelikle algoritmanızı inşa edebilirsiniz. Örneğin, bir katman için dBeta^[l]'yi hesaplayabilir, ardından Beta parametresini "Beta eksi alpha(öğrenme oranı) çarpı dBeta^[l]" şeklinde güncelleyebilirsiniz. Ayrıca, Adam ya da RMSprop ya da momentumu kullanarak da beta ve gamayı güncelleyebilirsiniz. Yani sadece dereceli alçalmayla sınırlı değilsiniz. Bir önceki videoda "Yığın Normu" işleminin ne yaptığını açıklamıştım (ortalamaları ve değişintileri hesaplar, esas değerden ortalamayı çıkartır, kalanı değişintiye böler) Fakat derin öğrenme programlaması için hali hazırda bir yapı iskelesi kullanıyorsanız genellikle "yığın normu" adımını ya da katmanını kendiniz inşa etmek zorunda kalmazsınız. Yapı iskelelerinde bu tek satırlık kodla halledilebilir. Örneğin, Tensorflow yapı iskelesinde yığın normalleştirmesini yazdığım işlevle devreye sokabilirsiniz. Programlama yapı iskeleleri hakkında daha sonra ayrıca konuşacağız; ama pratikte tüm bu detayları büyük ihtimalle kendi başınıza inşa etmek zorunda kalmayacaksınız. Yine de bunların nasıl işlediğini bilmek gerçekten önemli. Böylece yazdığınız kodun ne yaptığını daha iyi anlayabilirsiniz. Tekrar etmek gerekirse, yığın normu genellikle bu tür yapı iskeleleri sayesinde tek bir satırda kodlanabilir. Şimdiye kadar, yığın normu hakkında konuşurken tüm eğitim verisini tek seferde işlediğinizi, yığınsal dereceli alçalma kullandığınızı varsaydık. Aslında bu yöntem çoğu zaman eğitim verisinin daha küçük yığınlara bölünmesiyle uygulanır. Yani gerçekte yığın normunu uygularken ilk mini-yığın (mini-batch)'inizi alıyorsunuz ve Z1'i hesaplıyorsunuz. Tıpkı bir önceki sürgüde gördüğümüz gibi, W^[1] ve B1 ve sonra sadece bu mini-yığını (mini-batch) ve Z1'in bilgisayar ortalaması ve varyansını alırsınız. Z^[1]'in ortalamasını ve değişintisini hesaplayıp, Z^[1]'den ortalamayı çıkartıp değişintiye bölüp, Beta^[1] ve Gamma 1, size Z1'i vermek için Bunların hepsi hala X{1} üzerinden yapılır. Ardından etkileşim işlevini uygulayarak A^[1]'i elde edersiniz, sonra Z^[2]'yi W^[2] ve B^[2]'yi kullanarak hesaplarsınız, vesaire, vesaire. Tüm bu işlemleri X{1}'in üzerinde dereceli alçalmanın bir adımını atmak için kullanırsınız ve ardından ikinci küçük yığınınız, X{2} devreye girer. benzer işlemleri tekrarlarsınız: Z^[1]'i hesaplarsınız, ardından yığın normunu uygulayarak Z~[1]'i elde edersiniz. İşte bu yığın normu adımında Z~'yi sadece X{2}'deki veriyi kullanarak elde edersiniz yani şuradaki BN ile. X{2}'deki örnekleri irdeleyelim: Z^[1]'in ortalamasını ve değişintisini sadece o küçük yığın üzerinden hesaplayıp, Beta ve Gamma ile yeniden ölçeklendirip Z~'yi elde edersiniz, vesaire, vesaire. Bunu üçüncü küçük yığınınızla yaparsınız ve eğitime devam edersiniz. Şimdi, dağılımsallaştırmamıza dair bir ayrıntıyı netleştirmek istiyorum. Size demiştim ki parametrelerimiz her katman için W^[l], B^[l], Beta^[l], ve Gamma^[l]. Dikkat ederseniz Z'yi şöyle hesaplamaktaydık: Z^[l] = W^[l] x A[l-1] + B[l]. Fakat yığın normunun yaptığı, eldeki küçük yığına bakıp Z^[l]'yi 0 ortalama ve standart değişinti ile normalleştirmedir. Ve ardından Beta ve Gamma ile yeniden ölçeklendirmedir. Ama bu ne demektir? B'nin değeri ne olursa olsun çıkarılıp atılacaktır; çünkü o yığın normalleştirilmesi adımında Z^[l]'nin ortalamasını hesaplayıp onu çıkaracaksınız. Yani bu küçük yığındaki tüm örneklere herhangi bir sabit eklemeniz hiç bir şeyi değiştirmeyecektir. Çünkü ekleyeceğiniz her sabit, ortalamaların çıkartılması sırasında sıfırlanacaktır. Yani yığın normunu kullanıyorsanız, o parametreyi aradan çıkartabilirsiniz. Ya da, isterseniz, onu hep 0'a eşitlemek diye düşünebilirsiniz. Böylece dağılımsallaştırmamız sadece Z^[l] = W^[l] x A^[l-1] olur. Sonra Z^[l]'nin normalleşmesini hesaplarsınız, yani Z~[l] = Gamma(Z^[l]) + Beta^[l] Bu Beta^[l] parametresini kullanarak Z~[l]'nin ortalamasının ne olduğuna karar verirsiniz, bu da bir sonraki katmana aktarılır. Genel olarak toparlamak gerekirse, yığın norm bu katmandaki Z^[l] değerlerinin ortalamasını sıfırladığı için bu parametreyi burada tutmanın anlamı yok. Bu yüzden elden çıkarmalısınız. yerine Beta^[l]'yi koymalısınız. Ki bu da yanlı terimlerin yer değişimini kontrol eden bir parametre olur. Son olarak hatırlayınız ki Z^[l]'nin boyutu, bunu tek bir örnek üzerinden yapacaksanız (n^[l],1) olacaktır. B^[l]'in boyutları da (n^[l],1) olacaktır; tabii ki n^[l], l katmanındaki toplam saklı birim sayısını ifade edince. Beta^[l] ve Gamma^[l]'in boyutları da (n^[l],1); yine çünkü n^[l] saklı birim sayısını ifade etmektedir. Beta^[l] ve Gamma^[l], her saklı birimin ortalamasını ve değişintisini ağın istediği değerlere yeniden ölçeklendirmek için kullanılır. Şimdi bunların hepsini bir yerde toplayalım, ve dereceli alçalmayı yığın normuyla nasıl inşa edebileceğinizi tasvir edelim. Dereceli alçalmayı küçük yığınlara uyguladığınızı varsayarsak, T = 1'den küçük yığın sayınıza kadar kendisini yineleyecektir. İleri yayılımı küçük yığılım X{T} üzerinde uygularsınız ve yığın normunu kullanarak Z^[l]'leri Z~[l] ile değiştirirseniz. Bu sayede, bu küçük yığılımda Z normalleştirilmiş ortalama ve değişintiyle eşleşmesini garantileyeceksiniz Bu Z'nin normalleştirilmiş hali Z~[l] olacak. Ve ardından geri yayılımı kullanarak dW'yi, dB'yi, tüm l değerleri için, dBeta'yı, dGamma'yı hesaplarsınız. Esasında, zaten B'den kurtulacağınız için bu terim de ortadan kalkar. Son olarak bu parametreleri güncellersiniz. Bu durumda W her zamanki gibi W eksi öğrenme oranı ile güncellenir Beta, Beta eksi öğrenme oranı çarpı dB ile güncellenir. Gamma da benzer şekilde güncellenir. Ve eğer dereceyi şu şekilde hesapladıysanız, dereceli alçalma kullanabilirsiniz. Ben de buraya aynen onu yazdım ama bu momentumlu dereceli alçalmayla da çalışır. Ya da RMSprop ya da Adam ile. O durumda şuradaki dereceli alçalmanın güncellemesini almak yerine, geçtiğimiz haftalardaki videolarda da bahsettiğimiz diğer algoritmaların güncellemelerini kullanabilirsiniz. Bu diğer iyileme algoritmaları aynı zamanda eklediğimiz Beta ve Gamma parametrelerini iyilemek için de kullanılabilir. Umarım size yığın normunu nasıl sil baştan tanımlayabileceğinize dair bir hissiyat kazandırabilmişimdir. Eğer ki siz derin öğrenme yapı iskelelerinden birini kullanıyorsanız (ki onlar hakkında daha sonra konuşacağız), sadece bir başkasının inşasını göz önünde bulundurabilirsiniz. Bu da yığın normunu kullanmanızı epey kolaylaştıracaktır. Eğer yığın normu sizin gözünüzde gizemini korumaya devam ediyorsa, eğitimi neden çarpıcı bir şekilde hızlandırdığından emin değilseniz gelin bir sonraki videoya geçelim ve yığın normunun neden çalıştığını ve gerçekte neye yaradığını inceleyelim.