1
00:00:00,320 --> 00:00:01,820
딥러닝의 상승하는 시점에서, 

2
00:00:01,820 --> 00:00:06,460
가장 중요했던 아이디어 중 하나는 
정규화(Normailization) 이라는 알고리즘입니다.

3
00:00:06,460 --> 00:00:10,860
이것은 2명의 개발자가 만들었는데요,
Sergey Ioffe 와 Christian Szegedy입니다.

4
00:00:10,860 --> 00:00:14,096
배치 정규화는 여러분의
하이퍼 파라미터 서치 문제를 훨씬 더 쉽게 만들어 줍니다.

5
00:00:14,096 --> 00:00:15,970
그리고 신경망을 더욱 튼튼하게 만들어주죠. 

6
00:00:15,970 --> 00:00:20,492
하이퍼 파라미터 의 선택은 

7
00:00:20,492 --> 00:00:25,029
잘 작동하는 훨씬 더 큰 범위의 하이퍼 파라미터인데요, 
여러분이 심층신경망에서도 더 쉽게 트레이닝 할 수 있도록 해줄 것입니다.

8
00:00:25,029 --> 00:00:27,850
그러면 배치 정규화가 어떻게 작동하는지 한번 보겠습니다.

9
00:00:27,850 --> 00:00:32,860
모델을 트레이닝 하는 경우, 
로지스텍 회귀분석이나 같은 경우에 말이죠, 

10
00:00:32,860 --> 00:00:37,990
이 경우 입력값의 특성을 일반화시키는 것이 속도를 높혀줄 수 있다는 기억하실 것입니다. 그러면 평균값을 구하고,

11
00:00:37,990 --> 00:00:40,680
트레이닝세트에서 평균값을 빼고, 

12
00:00:40,680 --> 00:00:42,150
편차를 계산합니다.

13
00:00:44,320 --> 00:00:46,090
xi 제곱의 합이죠. 

14
00:00:46,090 --> 00:00:48,140
이것은 element-wise squaring입니다.

15
00:00:49,990 --> 00:00:53,160
그리고 편차대로 데이터를 정규화 시켜줍니다.

16
00:00:53,160 --> 00:00:57,699
그리고 절의는 러닝문제의 곡선을 

17
00:00:57,699 --> 00:01:02,887
아주 긴 모양에서 조금 더 동그란 모양으로 변형을 줄 수 있다는 것은
이전 비디오에서 봤는데요, 

18
00:01:02,887 --> 00:01:07,240
기울기 강화와 같은 알고리즘이 최적화하기 쉽게 말이죠.

19
00:01:07,240 --> 00:01:12,130
이것은 입력값을 신경망으로 정규화

20
00:01:12,130 --> 00:01:15,530
하는 것이나 로지스틱 회귀분석으로 하는 점에서 잘 작동합니다.

21
00:01:15,530 --> 00:01:17,810
하지만 더 깊은 모델은 어떨까요?

22
00:01:17,810 --> 00:01:22,998
입력값 x 만 있는것이 아니라
이 층에서는 a1 도 있습니다.

23
00:01:22,998 --> 00:01:27,210
해당 층에서는 
a2 activation도 있고 말이죠.

24
00:01:27,210 --> 00:01:31,220
그러면 여러분이 파라미터를 트레이닝 하고 싶은 경우엔, 
예를 들어, w3, b3과 같이 말이죠, 

25
00:01:32,600 --> 00:01:36,900
그러면 평균값과 a2의 variance를 정규화해서

26
00:01:36,900 --> 00:01:41,330
w3 와 b3의 트레이닝을 더 효율적으로 만드는 것이
좋지 않을까요?

27
00:01:43,140 --> 00:01:46,960
로지스틱 회귀분석 같은 경우엔, 
x1,x2,x3를 정규화 시켜줌으로써 

28
00:01:46,960 --> 00:01:51,460
w와 b를 더 효율적으로 트레이닝 시킬 수 있다는 것을 알았습니다.

29
00:01:51,460 --> 00:01:56,060
여기서 문제는, 어떤 숨겨진 층에 대해,

30
00:01:57,980 --> 00:02:02,143
a의 값을, a2라고 하겠습니다. 
이 값을 정규화할 수 있는지가 문제입니다.

31
00:02:02,143 --> 00:02:07,796
여기 예제에서는 
어떤 숨겨진 층에서도 이런데요, 

32
00:02:07,796 --> 00:02:12,580
w3와 b3를 더 빨리 트레이닝 시키는 방법이겠죠. 맞죠?

33
00:02:12,580 --> 00:02:15,283
a2는 다음 층의 입력값이기 때문에, 

34
00:02:15,283 --> 00:02:18,870
w3와 b3의 트레이닝에 영향을 줍니다.

35
00:02:20,390 --> 00:02:24,418
그러면 이것이 배치 정규화가 하는 역할입니다.

36
00:02:24,418 --> 00:02:25,287
보통 줄여서 batch norm이라고도 합니다.

37
00:02:25,287 --> 00:02:31,730
엄밀히 이야기하면 a2가 아닌 z2를 정규화
시키는 것일텐데요, 

38
00:02:31,730 --> 00:02:36,030
딥러닝 학술에서 여러분이 activation 함수 이전에

39
00:02:36,030 --> 00:02:40,760
z2죠, 정규화 해야 할지, 

40
00:02:40,760 --> 00:02:45,000
a2 activation 함수를 적용한 다음에 
정규화를 해야 하는지에 대한 토론이 있습니다.

41
00:02:45,000 --> 00:02:48,655
실제로는 z2를 정규화 하는 것이 훨씬 더 자주 있습니다.

42
00:02:48,655 --> 00:02:51,253
그러므로 그 버전을 보여드리겠습니다.

43
00:02:51,253 --> 00:02:54,550
그리고 이 값을 기본값으로 하도록
저는 추천드리겠습니다.

44
00:02:54,550 --> 00:02:58,060
그러면 배치 정규화를 어떻게 도입하는지 보겠습니다.

45
00:02:58,060 --> 00:03:06,070
여러분 신경망에서 어떤 중간값이 있는데요, 

46
00:03:09,470 --> 00:03:15,270
여러분이 어떤 hidden unit 값 z1에서, 
zm까지 있다고 해보겠습니다. 

47
00:03:15,270 --> 00:03:19,365
이것은 어떤 숨겨진 층에서 그런 것입니다.

48
00:03:19,365 --> 00:03:23,686
그러므로 이것을 z로 쓰는 것이 조금 더 정확하겠죠, 

49
00:03:23,686 --> 00:03:28,750
어떤 숨겨진 층 i에 대해서 말이죠, 
i는 1에서 m까지가 되겠습니다.

50
00:03:28,750 --> 00:03:33,110
하지만 적는 것을 줄이기 위해서, 
여기 [l]는 생략하겠습니다.

51
00:03:33,110 --> 00:03:35,350
여기 라인에 있는 표기를 심플하게 하기 위해서 말이죠. 

52
00:03:35,350 --> 00:03:41,260
이런 값들이 주어지면, 이제 평균값을 아래와 같이 구합니다.

53
00:03:41,260 --> 00:03:46,277
이것은 모두 l층에 맞춰진 것인데요, 
[l]은 생략하겠습니다.

54
00:03:46,277 --> 00:03:51,153
그리고 variance의 값은 여러분이 예상하는

55
00:03:51,153 --> 00:03:56,043
공식을 이용하여 계산하구요, 
zi들을 각각 이용해서 정규화합니다.

56
00:03:56,043 --> 00:04:00,908
그러면 zi 정규화된값을 
평균값에서 빼고 

57
00:04:00,908 --> 00:04:04,310
표준편차로 나누어주면서 갖게되는데요, 

58
00:04:04,310 --> 00:04:09,312
숫자적으로 안정감을 주기위해서 
보통 앱실론을 분모에 더합니다.

59
00:04:09,312 --> 00:04:14,460
여기 시그마 제곱과 같이 말이죠.
이것은 어떤 추정치의 경우 0이 되는 경우도 있습니다.

60
00:04:14,460 --> 00:04:17,405
그럼 이제 저릐는 z의 값을 갖고, 

61
00:04:17,405 --> 00:04:23,010
평균값이 0, 그리고 standard unit variance를 갖가되게 
정규화 시켰습니다.

62
00:04:23,010 --> 00:04:25,903
그럼 z의 모든 요소는 평균값 0 과 variance 1의 값을 
갖습니다.

63
00:04:25,903 --> 00:04:31,352
그렇지만 저희는 숨겨진 유닛이 항상
평균값 0 과 variance 1의 값을 갖길 바라진 않죠. 

64
00:04:31,352 --> 00:04:38,953
숨겨진 유닛은 아마도 다른 분포도를 갖는 것이
더 말이 되겠죠.

65
00:04:38,953 --> 00:04:42,939
그러므로 저희는 대신해서 

66
00:04:42,939 --> 00:04:48,434
이것을 zi 라고 부를텐데요, 
이것은 감마 zi 정규화 더하기 베타입니다.

67
00:04:48,434 --> 00:04:55,210
그리고 여기 감마와 베타는 배우기가 가능한
파라미터입니다.

68
00:04:58,957 --> 00:05:03,675
그러므로 저희는 gradient descent나 다른 알고리즘을
이용할 것입니다. 기울기 강하 와 모멘텀이나

69
00:05:03,675 --> 00:05:08,136
RmsProp 또는 Adam을 이용해서
감마와 베타 파라미터r를 업데이트 할 것입니다.

70
00:05:08,136 --> 00:05:11,410
신경망의 weight를 업데이트 하는 것과 마찬가지로 말이죠. 

71
00:05:11,410 --> 00:05:16,582
자 그럼 감마와 베타의 효과는 

72
00:05:16,582 --> 00:05:22,140
z tilde의 평균값이 여러분이 원하는 값으로 되게 해주는 것입니다.

73
00:05:22,140 --> 00:05:27,720
만약 감마가 시그마 제곱 더하기 앱실론의  

74
00:05:28,800 --> 00:05:33,570
루트값인 경우, 즉 감마가 
여기 분모 항이라고 하면, 

75
00:05:33,570 --> 00:05:39,318
그리고 베타가 뮤, 
즉 여기 위의 값이라고 하면

76
00:05:39,318 --> 00:05:43,998
감마 z 정규화 더하기 베타의 효과는

77
00:05:43,998 --> 00:05:49,540
이 식을 거꾸로 하는 것과 일치합니다.

78
00:05:49,540 --> 00:05:52,284
이것이 만약 맞다면, 

79
00:05:52,284 --> 00:05:57,780
z tilde i 는 zi와 같을 것입니다.

80
00:05:57,780 --> 00:06:02,633
그러면 감마와 베타 파라미터의 적합한 설정을 통해

81
00:06:02,633 --> 00:06:05,321
여기 정규화 단계는, 

82
00:06:05,321 --> 00:06:11,175
여기 4개의 식은
identity 함수를 계산합니다.

83
00:06:11,175 --> 00:06:16,112
하지만 감마와 베타값을 다르게 고르면서, 

84
00:06:16,112 --> 00:06:19,320
이것은 숨겨진 유닛이 다른 평균값과 variance를 갖게 해줍니다.

85
00:06:19,320 --> 00:06:23,538
그러면 이것을 신경망에 fitting하는 방법은, 

86
00:06:23,538 --> 00:06:28,583
이전에는 z1, z2 등과 같은

87
00:06:28,583 --> 00:06:35,195
값을 사용했는데요, 이제는
zi 대신에 z tilde i를 

88
00:06:35,195 --> 00:06:39,910
신경망에서 나중의 산출방식에 
사용하겠습니다.

89
00:06:39,910 --> 00:06:45,129
그리고 [l]는 다시 넣습니다. 
어떤 층인지 정확히 표기하기 위해서 말이죠.

90
00:06:45,129 --> 00:06:46,910
여기다가 넣으시면 됩니다.

91
00:06:46,910 --> 00:06:51,319
여기서 얻어가실 직관적인 부분은

92
00:06:51,319 --> 00:06:56,140
x의 입력값을 정규화함으로써
신경망의 러닝을 도와줄 수 있다는 것입니다.

93
00:06:56,140 --> 00:07:00,029
그리고 batch norm이 하는 것은 
정규화 절차를 적용하여 

94
00:07:00,029 --> 00:07:01,283
입력 층에만이 아니라, 

95
00:07:01,283 --> 00:07:04,810
깊이 있는 숨겨진 층에도 적용합니다.

96
00:07:04,810 --> 00:07:09,085
그러기 때문에 숨겨진 유닛 z에 대해서도

97
00:07:09,085 --> 00:07:12,390
이러한 정규화를 통해 숨겨진 유닛의
평균값과 variance가 종규화되게 적용할 것입니다.

98
00:07:12,390 --> 00:07:16,833
하지만 트레이닝 입력값과 숨겨진 유닛의 차이는

99
00:07:16,833 --> 00:07:21,220
여러분은 숨겨진 유닛이 강제적으로 평균값 0과 variance 1이 되는 것을
원치 않을 수 있다는 것입니다.

100
00:07:21,220 --> 00:07:24,247
예를 들어, 시그모이드 activation 함수의 경우, 

101
00:07:24,247 --> 00:07:27,230
여기 모든 값이 밀집되어 있는 것은 원치않습니다.

102
00:07:27,230 --> 00:07:31,582
여러분은 이 것이 더 큰 variance나 평균값을 가져

103
00:07:31,582 --> 00:07:35,322
0보다 다른 값을 가지면서 비선형의 시그모이드

104
00:07:35,322 --> 00:07:41,060
함수가 나오길 바랄 것입니다. 
선형함수의 모양을 갖는 것보다 말이죠.

105
00:07:41,060 --> 00:07:45,067
그러므로 파라미터 감마와 베타를 이용해서

106
00:07:45,067 --> 00:07:51,230
여러분의 zi 값이 원하는 범위의 값을 갖도록 할 수 있는 것입니다.

107
00:07:51,230 --> 00:07:55,671
하지만 실질적으로 하는 것은 숨겨진 유닛이 

108
00:07:55,671 --> 00:07:59,226
평균화 한 평균값과 variance값 을 갖고

109
00:07:59,226 --> 00:08:03,429
감마와 베타와 같이 러닝 알고리즘이

110
00:08:03,429 --> 00:08:07,826
설정할 수 있는 값들에 의해 조정됩니다.

111
00:08:07,826 --> 00:08:13,004
그러므로 여기 숨겨진 유닛값의 평균값와 variance대로

112
00:08:13,004 --> 00:08:18,660
정규화되는 것인데요, zi들이 고정된 평균값과 variance를 갖게끔 말이죠.

113
00:08:18,660 --> 00:08:22,320
평균값과 variance는 각각 0과 1일 수 있구요, 또는 또 다른 값이 될 수도 있겠죠. 

114
00:08:22,320 --> 00:08:26,680
그런 이후, 여기 감마와 베타와 같은 값에 의해
조정됩니다.

115
00:08:26,680 --> 00:08:30,424
이 내용이 여러분이 배치 정규화를 도입하는 
원리를 이해하는데 도움을 줬길 바랍니다.

116
00:08:30,424 --> 00:08:32,830
적어고 신경망의 single layer에서의 경우 말이죠. 

117
00:08:32,830 --> 00:08:36,104
다음 비디오에서는 신경망에 배치 정규화를

118
00:08:36,104 --> 00:08:39,052
피팅하는 방법을 배울 것입니다. 심층신경망에서도 말이죠. 

119
00:08:39,052 --> 00:08:41,700
그리하여 몇개의 다른 층의 신경망에서도 잘 작동시키는 방법을 익혀보겠습니다.

120
00:08:41,700 --> 00:08:45,450
그 이후, 배치 정규화가 왜

121
00:08:45,450 --> 00:08:47,120
신경망을 트레이닝 하는데 도움을 줄 수 있는지도 알아보겠습니다.

122
00:08:47,120 --> 00:08:51,424
아직도 왜 작동하는지에 대한 부분에서 미지수인 부분이 남아있는데요, 

123
00:08:51,424 --> 00:08:54,949
2개 정도의 비디오 이후, 조금 더 많이 이해가 되실 것입니다.