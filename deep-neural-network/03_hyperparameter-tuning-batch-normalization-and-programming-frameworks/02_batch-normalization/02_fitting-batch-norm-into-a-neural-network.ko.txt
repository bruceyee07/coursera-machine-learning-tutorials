여러분은 한가지 숨겨진 층에 대해 배치 정규화를 개발하는 공식을 보았는데요, 심층신경망의 트레이닝에는 어떻게 피팅되는지 한번 보겠습니다. 그럼 이렇게 생긴 신경망이 있다고 가정해보겠습니다. 여러분은 제가 이런 개별 유닛을 2가지를 계산한다는 것으로 보면 되다고 이야기한 것을
들으셨을텐데요, 첫번째로, z의 값을 계산하고 다음으로 activation 함수를 적용하여 A를 구합니다. 그러면 여기 각각의 동그라미를 2가지 계산을 대표한다고 생각하면 됩니다. 다음층도 이와 비슷하게, z21, a21 등등 말이죠, 그러므로 여러분이 배치 정규화를 적용하지 않는 것이면, x가 첫번째 숨겨진 유닛으로 삽입되고, 먼저 z1의 값을 산출하고, 이 값은 파라미터 w1, b1에 따라 달라집니다. 그리고 보통은 z1을 activation 함수에 삽입하여 A1의 값을 구할 것입니다. 하지만 배치 정규화에서 할 것은 여기 z1의 값을 갖고, 배치 정규화를 적용시킬 것입니다. 가끔씩 BN으로 줄여서 부릅니다. 그리고 이 값은 파라미터 베타1과 감마 1에 의해 지배될 것입니다. 그러면 여기 새롭게 정규화된 z1 값을 줄 것입니다. 그리고 이 값을 activation 함수에 삽입합니다. A1의 값을 갖기 위해서 말이죠. 이 값은 g1 (z tilde 1)입니다. 그러면 이제 첫번째 층에 대한 계산을 마쳤는데요, Z와 A사이에서 실제로 산출 과정에서 배치 정규화이 발생합니다. 다음으로 여기 A1값을 갖고 z2를 계산합니다. 이제 그러면 이 값은 w2, b2에 의해 결정되겠죠. 그리고 첫번째 층에서 했던 것과 같이 비슷하게, z2의 값을 갖고, 배치 정규화에 적용시킵니다. 이것은 이제 BN으로 줄여서 쓰죠. 이것은 다음 층에 특정한 배치 정규화 파라미터에 의해 지배됩니다. 즉, 베타2와 감마2 겠죠. 그럼 이제 z tilde 2의 값을 얻을텐데요, 이제 이 값을 이용해서 a2를 구합니다. activation 함수를 이용해서 말이죠, 다시 한번, 배치 정규화는 Z와 A를 계산하는 사이에서 발생합니다. 그럼 직관적인 부분은, 정규화가 안된 Z의 값을 사용하는 대신에, 정규화 Z tilde값을 사용할 수 있으면, 그것은 첫번째 층이겠죠. 두번째 층에서도, 마찬가지로 정규화 안된 Z2값을 사용하는 대신에, 평균값고 variance가 정규화된 Z tilde 2값을 이용할 수 있습니다 그러면 파라미터의 네트워크는 w1, b1이 될 것입니다. 저희는 파라미터 들을 없앨 것인데요, 그 이유는 다음 슬라이드에서 보겠습니다. 일단은 파라미터가 기존 w1, b1,wl,의 값이라고 생각하겠습니다. 그리고 여기 새로운 네트워크에 추가적인 파라티머 베타1, 감마1, 베타2 , 감마2를 더했습니다. 여러분이 배치 정규화를 더하는 각각의 층에 말이죠. 명료성을 위해 여기 베타를 보십시오, 여기 값은 여러 기하급수적 가중 평균을 구했을 때 산출했던 모멘텀에서 있었던 하이퍼 파라미터 베타와는 상관이 없습니다. Adam paper의 저자들은 베타를 사용해서 하이퍼 파라미터를 표기합니다. 그리고 배치 정규화 paper의 저자들은 여기 이 파라미터를 나타내기위해 
베타를 썼고요, 하지만 여기 2개는 완전히 다른 베타들입니다. 저는 2개의 경우 모두에서 베타를 쓰기로 했는데요, 논문을 읽으시면, 베타1, 베타2 등등과 같이, 배치 정규화가 배우려고 하는 것은 다른 베타입니다. 모멘텀 에서 사용된 하이퍼 파라미터 Adam과 RmsProp 알고리즘에서의 베타와 비교하였을 때 말이죠, 그러면 이제 이러한 것들이 알고리즘의 새로운 파라미터기 때문에, 이제 어떤 최적화 방법을 사용할 것입니다. 도입하기 위해 기울기 강하 와 같은 방법을 말이죠. 예를 들어, 어떤 층에 대해 d 베타 l을 계산할 수 있겠죠, 그 후로, 파라미터를 업데이트 하는데요, 베타는 베타 빼기 러닝속도 곱하기 d 곱하기 베타 L로 업데이트되는데요, 여러분은 Adam 이나 RmsProp 또는 모멘텀을 사용해서 베타나 감마와 같안
파라미터를 업데이트할 수 있습니다. 기울기 강하뿐만 아니고요. 그리고 이번 비디오에서는, 배치 정규화가 하는 일을 설명 드렸는데요, 평균값, variance를 계산하고 이 값을 빼고 나눈다고 했는데, 딥러닝 프로그래밍 프레임웍을 쓰는 경우엔, 보통은 배치 정규화 단계 또는 배치 정규화 레이어를 본인이 직접 도입할 필요가 없을 것입니다. 그래서 프로빙 프레임웍은, 이것은 한줄의 코드일 수 있는데요, 예를 들어서, tensor flow 프레임웍 같은 경우, 여기 이 함수를 이용해서 배치 정규화를 도입할 수 있습니다. 프로그래밍 프레임웍에 대해서는 나중에 더 이야기 하겠습니다. 하지만 실제로는 여기 이런 상세한 부분들은 본인이 직접 도입할 필요가 없을 것입니다. 여러분이 이것이 어떻게 작동하는지 이해하고 코드가 무엇을 하는지 배우는 것입니다. 그런데 배치 정규화를 도입하는 것은 딥러닝 프레임웍에서 보통 한줄의 코드입니다. 현재까지 배치 정규화가 마치 전체 트레이닝 세트에서 한번에 트레이닝 시키는 것처럼 이야기 했는데요, 마치 배치 기울기 강하를 쓰는 것처럼 말이죠. 실제로는, 배치 정규화는 보통 트레이닝 세트의 미니 배치와 같이 적용됩니다. 그러면 배치 정규화를 적용하는 방식은 첫번째 미니 배치를 가지고 z1을 계산합니다. w1과 b1 파라미터를 사용해서 이전 슬라이드에서 한 것 처럼 말이죠, 그 다음 이 미니 배치를 가지고 Z1의 평균값과 variance 구합니다. z1에서 말이죠. 그러면 배치 정규화는 평균값에서 빼지고 표준편차로 나뉘면서 베타 1,감마 1의 값으로 re-scale되어 z1의 값을 줄 것입니다. 이 모든 것은 그러면 첫번째 미니 배치에 있겠죠. 그 이후에, activation 함수를 적용하여 A1을 구합니다. 그 다음, w2, b2 등을 이용하여 z2를 구합니다. 그러면 여러분은 이 모든 것을 첫번째 미니 배치에서의 기울기 강하 적용하기 위해 진행한 것인데요, 그 다음에 두번째 미니 배치 인 x2로 넘어갑니다. 여기서도 비슷하게 하는데요, 이제는 두번째 미니 배치에서 z1을 계산하고, 배치 정규화를 이용해서 z1 tilde를 구합니다. 그러면 여기 배치 정규화 단계에서 단순히 두번째 미니 배치에 있는 데이터만을 이용해서 z tilde를 종교화합니다. 그럼 여기 배치 정규화 단계에서, 두번째 mini-batch의 example을 볼텐데요 z1의 평균값과 variance을 계산하는데요, 그 다음에 베타와 감마로 re-scaling하여 z tilde를 얻습니다. 그리고 이것을 세번째 미니 배치에서도 하여 계속 트레이닝합니다. parameterization 에서 한가지 정리하고 싶은 상세 부분이 있는데요 이전에 저는파라미터가 각각의 층에 대해서 wl, bl, 그리고 베타 l, 그리고 감마 L이라고 했는데요, z가 계산된 방식을 한번 보겠습니다. ZL = WL x A of L - 1 + B of L 인데요, 그런데 배치 정규화 이 하는 것은 미니 배치를 보고나서 ZL을 첫번째 평균값 0 그리고 표준편차로 정규화하고나서, 베타와 감마로 re-scale할 것입니다. 이것이 뜻하는 바는, bl이 어떤 값을 갖더라도, 이 것은 빠질 것입니다. 그 이유는 배치 정규화 단계에서, ZL의 평균값을 구한 뒤에, 평균값을 뺄 것이기 때문입니다. 그러므로 여기 모든 미니 배치 예제에서 상수를 더하는 것은 아무 변화도 일으키지 않습니다. 그 이유는 상수가 더해지더라도 평균값이 빼지므로 그 효과가 캔슬되기 때문입니다. 그러므로 배치 정규화를 쓰는 경우, 이 파라미터를 제거할 수 있습니다. 아니면 원할 경우, 이것을 영구적으로 0으로 설정하는 것을 생각해보십시요. 그러면 zl의 parameterization wl 곱하기 al 빼기 1이 됩니다. 그 다음에는 ZL 정규화값을 구하는데요, 그 다음 z tilde값은 가마 ZL 더하기 베타인데요 여기 베타 L 파라미터를 쓰게 됩니다. z tilde l의 평균값 을 정하기 위해서 말이죠.  그 다음 이 값이 다음 층으로 넘어갑니다. 복습하자면, 배치 정규화는 평균값을 0으로 만들기 때문에, 여기 층에 있는 ZL 값들에 대해서 말이죠. 그렇기 때문에 여기 BL 파라미터를 갖는 의미가 없습니다. 그러므로 그냥 없애는 것입니다. 그리고 대신해서 베타 L로 대체되었습니다. 베타 L은 이동이나 bias 항들에 영향을 주는 파라미터입니다. 마지막으로 ZL의 다이멘션은 하나의 예시에서 진행할 경우, 그 값은 NL x 1 인데요 그러면 BL 은 다이멘션 NL x 1인데요, 만약 NL이 l층에서의 숨겨진 유닛개수라고 하면 말이죠. 그러면 베타 L과 감마 L의 디아멘션도 역시 마찬가지로 NL x 1이 될것입니다. 그 이유는 이것이 숨겨진 유닛의 개수이기 때문입니다. NL개의 숨겨진 유닛이 있고, 각각의 숨겨진 유닛에 대해서 베타 L, 감마 L을 통해 평균값과 variance이 scale 되는 것입니다. 네트워크가 원하는 설정값대로 말이죠. 이제 그러면 모든 것들을 취합해서 배치 정규화를 이용해서 기울기 강하를 도입하는 방법을 보겠습니다. 여러분이 미니 배치 기울기 강하를 사용한다는 가정하에 T=1에서 미니 배치 숫자 까지  반복하는데요, 그리고 미니 배치 XT에 대해서 전 방향전파을 도입할텐데요, 그리고 각각의 숨겨진 유닛에 대해서 전 방향전파을 하는 것은 ZL을 Z tilde L로 바꾸기 위해 배치 정규화를 사용하십시오. 그러면 여기 미니 배치 안에서 z의 값은 정규화된 평균값과 variance를 갖게 되는데요, 여기 값과 버전은 z tilde l이 됩니다. 그 다음에 후 방향전파를 이용해서 dw,db, 그리고 모든 l의 값에 대해서 d 베타, d 감마, 사실 엄밀히 이야기하면, B를 없앴으므로, 이거는 사실 사라집니다. 마지막으로 파라미터를 업데이트 시킵니다. 그러면 w 는 w 빼기 러닝속도 곱하기, 이걸로 이전과 같이 말이죠, 그리고 베타는 베타 빼기 러닝속도 곱하기 db 감마도 비슷하게 계산합니다. 그리고 여러분이 기울기를 이와 같이 계산한 경우, 기울기 가하를 이용할 수 있습니다. 여기 그렇게 적은 것이고요, 하지만 이것은 기울기 강하 와 모멘텀에서도 잘 작동합니다. 또는 RmsProp 또는 Adam에서도 말이죠. 여기 기울기 강하 업데이트를 갖는 대신에, 여기 다른 알고리즘에서 주어진. 업데이트를 이용할 수 있습니다. 지난 비디오에서 다뤘던 것과 같이 말이죠. 여기 다른 최적화 알고리즘은 베타와 감마와 같은 배치 정규화가 알고리즘에 더한
것들을 이용해서 파리미터를 업데이트 하는데 사용될 수도 있습니다. 바라건대 이 내용이 여러분이 배치 정규화를 처음부터 도입하는 방법을 익히는데 도움이 됐기 바랍니다. 여러분이 만약에 딥러닝 프로그래밍 프레입웍을 쓰는 경우, 나중에 이야기 하겠지만 이상적으로 여러분이 다른 사람의 프로그래밍 프레임웍 도입을 불러서 배치 정규화를 더 쉽게 만들 수 있습니다. 아직 배치 정규화에 대한 내용이 조금 이해하기 어려우시면 왜 트레이닝 속도를 드라마틱하게 올려주는지 잘 모르겠으면 다음 비디오로 넘어가서 왜 배치 정규화가 잘 작동하는지 무엇을 하는지 이야기 해보겠습니다.