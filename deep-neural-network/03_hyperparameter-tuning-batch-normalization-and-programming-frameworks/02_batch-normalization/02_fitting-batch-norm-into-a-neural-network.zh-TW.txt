你已經看過批次標準化的 方程式推導了，在單一個隱藏層上 讓我們來瞧瞧這要怎麼運用在深度網路的訓練上 假設你有個這樣的神經網路 你知道我曾說，你可以把每個神經元
看成在算兩件事 首先算出Z，然後通過啟動函數進一步算出a 所以我們可看作每個圓圈都表示
兩步的計算 下一層也一樣 z[2]_1 和 a[2]_1，以此類推 假設先不套用批次標準法的話 你會有一個輸入 X 餵進去第一個隱藏層 然後先算出 Z[1] 這是由參數W[1], b[1]所控制的 然後正常來說，你會把Z[1]餵進去啟動函數，算出a[1] 但如果是套用批次標準化的時候，我們會拿Z[1]這個值 套用批次標準化 有時簡寫成 BN 那會由這些參數控制 beta[1] 和 gamma[1] 這樣會給你一個新的，標準化z[1]後的值 然後你再拿那個值給啟動函數，得到a[1] 也就是 g[1] 套在 Ztilde[1] 上 現在你算完了第一層 其中批次標準化發生在
從z到a的計算途中 接下來，你拿a[1]這個值去計算出Z[2] 其中由參數W[2], b[2]控制 類似第一層所作的 你拿Z[2]，套用批次標準化，我們在這用縮寫BN 這則是由下一層的批次標準化參數所控制 也就是 beta[2], gamma[2] 這會給你 Z tilde[2] 然後用它，套用到啟動函數算出 a[2]，依此類推 再一次，批次標準化發生在
計算z和計算a的中間 概念上是 本來用沒標準化過的值Z 但現在改成拿標準化的值 Z tilde，例如這邊第一層 第二層也是 本來用沒標準過的Z[2] 你改成用由平均和變異數標準過的值 Ztilde[2] 所以你網路的參數會是W[1], b[1] 實際上我們會拿掉參數b，不過下個投影片再說 現在呢，參數有原本的 W[1] b[1] 到 W[L], b[L]，還有我們剛剛加進去網路的 新的參數 beta[1], gamma[1], beta[2], gamma[2] 等等等，對於你有套用批次標準化的每一層 澄清一下，這邊的 beta 們 這些和超參數beta沒有關係，就是那些我們曾看過 在動量方法中，利用指數加權平均的那個beta Adam法的作者在論文中用了beta來表示那個超參數 批次標準化的作者在論文中也用了beta表示這參數 但此beta非彼beta 我還是在兩邊都寫了beta 以防你去讀了原始論文 不過 beta[1], beta[2], 等等 這些批次標準化所學的，是不同於 在動量法、Adam法、RMSprop演算法
裡面的超參數beta 總之，這些是你演算法中新的參數 你就可以運用任一種你喜歡的最佳化算法 例如梯度下降法來實作之 例如，你可能算出某一層的 d beta[l] 然後更新參數 beta 變成 beta 減掉學習率乘上 d beta[l]。你也可以用 Adam法、RMSprop或動量方法
來更新參數 beta 和 gamma 不一定要用梯度下降法 還有，雖然在之前的影片 我解釋了批次標準化的運算怎麼做 算平均和變異數，減掉它除以它 如果你使用深度學習的程式框架 通常你不用自己實作
批次標準化的步驟或是批次標準化的層 在某些程式框架之下 你可以用一行程式就解決了 例如在TensorFlow 你可以用這個函數來實作批次標準化 我們後面會介紹更多關於程式框架的事 不過實務上，你大概不用自己實作所有的細節 但了解其運作還是很值得的，這樣你可以 更了解你的程式到底在做什麼 不過在深度學習框架，
通常只要一行程式就可以做出批次標準化 那到目前為止，我們介紹批次標準法
都還在假設 你訓練一整個訓練資料，好比用批次梯度下降法 實際上，批次標準法通常用在
訓練集的小批資料(mini-batch)上 所以套用批次標準法的時候，你是拿 第一批小量資料(mini-batch)，算出Z[1] 和前一張投影片一樣，我們用參數 W[1], b[1]，然後你只拿這一小批資料，算出Z[1]的
平均和變異數，只在 這一小批上，然後批次標準化會 減掉平均，除以標準差，然後用beta[1], gamma[1] 重新矯正之，來給你 Z[1] 這全部的步驟都是在第一批小量資料上 然後你套用啟動函數，得到a[1] 然後你用 W[2], b[2] 算出 Z[2]，依此類推。 剛剛所講的，都是你在對第一小批資料 執行了一步的梯度下降。接下來
到了第二批小量資料 X2 然後做類似的事，就是在第二批資料 上計算 Z[1]，然後用批次標準化算出 Z[1]tilde 那麼，在批次標準化的這一步 你只用第二批的小量資料來標準化 z tilde 這邊的 BN 步驟也是 我們看了第二批的小資料 只在那一小批上，算出Z[1]的平均和變異數 用beta和gamma重新縮放得到 Z tilde，依此類推 然後你做在第三批小量資料上，繼續訓練 現在，有個細節，我想要清理一下參數 在剛剛，我說過我們有參數 W[l], b[l] 在每一層，還有 beta[l], gamma[l]。那注意一下 Z[l] 是用下面的公式計算的 Z[l] = W[l] * A[l-1] + b[l]。
但是批次標準化所作的 是他會看這一小批資料，首先標準化 Z[l]，讓它平均為0，變異數為1 然後用 beta 和 gamma 重新縮放 但這也意味著 無論 b[l] 的值為何，實際上他會抵銷掉 因為在批次標準化的步驟 你會算 Z[l] 的平均，然後減掉那個平均 所以當把這批所有的資料，都加一個常數的時候 這並不會有任何改變 因為你加的任何數字，都會在「減掉平均」這一步抵銷掉 所以如果你使用批次標準化 實際上你可以拿掉這個參數 或者也能想成，這個參數永遠被設成0 所以公式變成 z[l] 只是 W[l] 乘以 a[l-1] 然後算出標準後的 z[l] 然後算出 z tilde = gamma * z[l]norm + beta 你最後是用 beta[l] 這個參數來決定 Z tilde[l] 的平均，這個會傳到下一層 總結一下 因為批次標準化會把這層這些 Z[l] 的平均變成 0 所以沒有必要留 b[l] 這個參數 所以你拿掉他 某種角度改用 beta[l] 這參數影響最後的平移，或可說是偏移項 最後，還記得 Z[l] 的維度 如果只做在一筆資料 會是 n[l] 乘 1 所以 b[l] 的維度是 n[l]乘1 假設第 l 層有 n[l] 個隱藏單元 因此，beta[l] 和 gamma[l] 的維度 也會是 n[l] 乘 1，因為那是隱藏單元的個數 你有 n[l] 個隱藏單元，而beta[l]和gamma[l] 是用來縮放每一個隱藏單元的 平均和變異數，看網路想要設成多少 那麼，我們來整理一下，講講要怎麼 在批次標準化之下，實作梯度下降法 假設你用的是小批次梯度下降法 t 從 1 到批的數量迭代下去 你會實作正向傳播在 小批資料 Xt 上，然後對每個隱藏層做正向傳播 用批次標準法， 把 Z[l] 用 Z tilde[l] 代替掉，
所以這確保在這一小批資料內， Z 這個值最終會有標準化過的平均和變異數 標準化後的版本就是這個 Z tilde[l]。然後 你用反向傳播計算 dW db, 對於每個L d beta, d gamma 其實技術上來說，因為你不用 b 了 這個就拿掉了 最後，你更新參數 w 更新為 w 減掉學習率乘這個，和之前一樣 beta 更新為 beta 減掉學習率乘上 d beta gamma 也一樣 如果你算出這些梯度 你可以用梯度下降法 就是我這邊所寫的 但這也可以適用於動量梯次下降法 或 RMSprop，或 Adam 不一定用這個梯度下降方法 來更新，而是用之前影片講過的 其他那些最佳化方法來更新 其他一些最佳化演算法也可以用來更新 批次標準化所新增的參數 beta 和 gamma 那麼，我希望這能給你一些概念，關於 如何從無到有，實作出批次標準化
－如果你想要的話 如果你用 某個深度學習的程式框架，我們會在後面提到 那麼希望你只須呼叫別人寫好的 在框架內的實作，這樣會讓套用批次標準化更簡單 那麼，萬一批次標準化看起來還是很神奇的話 如果你仍不確定
這怎麼能加速訓練到超級快 讓我們進到下部影片，探討更多關於 為什麼批次標準化真的有用，還有他到底做了什麼