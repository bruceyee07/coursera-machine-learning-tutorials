1
00:00:00,320 --> 00:00:01,820
Derin Öğrenme'nin yükselişinde

2
00:00:01,820 --> 00:00:06,460
En önemli fikirlerden biri de "toptan normalleştirme" denen 

3
00:00:06,460 --> 00:00:10,860
Sergey Ioffe ve Christian Szegedy adında iki araştırmacı tarafından yaratılan bir algoritma oldu.

4
00:00:10,860 --> 00:00:14,096
Toptan normalleştirme hipermetre(*)'nizin problemi çok daha kolay aramasını, 

5
00:00:14,096 --> 00:00:15,970
sinir ağınızın çok daha güçlü olmasını sağlar.

6
00:00:15,970 --> 00:00:20,492
Hiperparametrelerin seçimi, gerçekten işe yarayan 
 hiperparametrelerin seçiminden çok daha geniştir.

7
00:00:20,492 --> 00:00:25,029
ve çok derin ağları bile daha kolay bir şekilde
 eğitmenizi sağlayacaktır

8
00:00:25,029 --> 00:00:27,850
Haydi toptan normalleştirmenin nasıl çalıştığını görelim.

9
00:00:27,850 --> 00:00:32,860
Bir modeli eğitirken -mesela mantıksal bağlanım-
 hatırlayabilirsiniz ki 

10
00:00:32,860 --> 00:00:37,990
giriş özniteliklerini normalleştirmek öğrenmeleri hızlandırabilir. 
 Hesaplama anlamında, 

11
00:00:37,990 --> 00:00:40,680
ortalamaları eğitim kümenizden çıkarın

12
00:00:40,680 --> 00:00:42,150
değişiklikleri hesaplayın

13
00:00:44,320 --> 00:00:46,090
xi'nin karesinin toplamı.

14
00:00:46,090 --> 00:00:48,140
bu eleman yönelimli bir kare alma

15
00:00:49,990 --> 00:00:53,160
sonra da veri kümenizi değişkenlere
 göre değiştirin

16
00:00:53,160 --> 00:00:57,699
Ve önceki bir videoda bunun nasıl 
 eğitim problemlerinizin çevritlerini 

17
00:00:57,699 --> 00:01:02,887
ince uzun bir şey olabilecekken
 daha yuvarlak bir olmasını

18
00:01:02,887 --> 00:01:07,240
ve eğim inişi gibi bir algoritmanın iyileştirebilmesi
 için daha kolay bir hale getirebileceğini gördük

19
00:01:07,240 --> 00:01:12,130
Bu, girdi değerlerini bir sinirsel ağa normalleştirmek
 açısından,

20
00:01:12,130 --> 00:01:15,530
bağlanımı değiştirmek için çalışıyor.

21
00:01:15,530 --> 00:01:17,810
Peki ya daha derin bir model?

22
00:01:17,810 --> 00:01:22,998
Sadece x girdisi özniteliği yok, 
bu katmanda a1 etkilenimi, 

23
00:01:22,998 --> 00:01:27,210
bu katmanda, 
 a2 etkilenimi vesaire.

24
00:01:27,210 --> 00:01:31,220
Diyelim ki w3, b3 parametrelerini
 eğitmek istiyorsunuz, o zaman

25
00:01:32,600 --> 00:01:36,900
ortalamayı ve a2'nin değişikliğini 

26
00:01:36,900 --> 00:01:41,330
w3,b3'ün eğitiminin daha etkili olması için
 normalleştirseniz güzel olmaz mıydı?

27
00:01:43,140 --> 00:01:46,960
Mantıksal bağlanım durumunda,
 x1,x2,x3'ün normalleştirilmesinin 

28
00:01:46,960 --> 00:01:51,460
belki de w ve b'yi daha etkili eğitmenizde
 yardımcı olduğunu gördük

29
00:01:51,460 --> 00:01:56,060
Burada soru şu, herhangi bir gizli katman için

30
00:01:57,980 --> 00:02:02,143
a'nın değerlerini,bu örnek için a2 diyelim, 

31
00:02:02,143 --> 00:02:07,796
ama gerçekte herhangi bir gizli katmanda

32
00:02:07,796 --> 00:02:12,580
w3,b3'ü daha hızlı eğitmek için normalleştirebilir miyiz?

33
00:02:12,580 --> 00:02:15,283
a2 diğer katmanın girdisi olduğu için

34
00:02:15,283 --> 00:02:18,870
bu nedenle w3 ve b3'ün eğitimini etkiliyor.

35
00:02:20,390 --> 00:02:24,418
İşte bu toptan normalleştirme,

36
00:02:24,418 --> 00:02:25,287
ya da kısaca toptan norm'u

37
00:02:25,287 --> 00:02:31,730
Aslında teknik olarak a2'nin değil,
 z2'nin değerlerini normalleştireceğiz.

38
00:02:31,730 --> 00:02:36,030
Derin öğrenme literatüründe değeri
 etkilenim fonksiyonundan, yani z2'den,

39
00:02:36,030 --> 00:02:40,760
önce mi normalleştirmelisin?
 Yoksa etkilenim fonksiyonunu,

40
00:02:40,760 --> 00:02:45,000
yani a2'yi, uyguladıktan sonra mı normalleştirmelisin?
Konusunda tartışmalar vardır. 

41
00:02:45,000 --> 00:02:48,655
Pratikte, z2'yi normalleştirme çok daha sık yapılır.

42
00:02:48,655 --> 00:02:51,253
İşte bu benim sunacağım ve size de 

43
00:02:51,253 --> 00:02:54,550
varsayılan seçiminiz olarak önereceğim versiyonu.

44
00:02:54,550 --> 00:02:58,060
Toptan norm'unu şöyle uygularsınız,

45
00:02:58,060 --> 00:03:06,070
Bazı ortalama değerler verildiğinde, sinir ağınızda, 

46
00:03:09,470 --> 00:03:15,270
Diyelim ki z1'den zm'ye kadar gizli ek girdi değerleriniz var,

47
00:03:15,270 --> 00:03:19,365
ve bu seferki gerçekten bir gizli katmandan, 

48
00:03:19,365 --> 00:03:23,686
bunu herhangi bir gizli katmandaki z için

49
00:03:23,686 --> 00:03:28,750
i 1'den m'ye kadar eşittir olarak yazmak daha doğru olur.

50
00:03:28,750 --> 00:03:33,110
Ama yazımı azaltmak için [l]'yi atlayacağım

51
00:03:33,110 --> 00:03:35,350
Sadece bu satırdaki gösterimi basitleştirmek için.

52
00:03:35,350 --> 00:03:41,260
Yani bu değerler verildiğinde, yapmanız gereken şekildeki gibi ortalamayı hesaplamak.

53
00:03:41,260 --> 00:03:46,277
Bunların hepsi bir l katmanına özel, ama [l]'yi atlıyorum.

54
00:03:46,277 --> 00:03:51,153
Ve sonra değişikliği tahmin ettiğiniz
 formülle hesaplıyoruz.

55
00:03:51,153 --> 00:03:56,043
Sonra her bir zi'yi alıp normalleştiriyoruz.

56
00:03:56,043 --> 00:04:00,908
Yani normalleştirilmiş zi'yi
ortalamayı çıkararak ve

57
00:04:00,908 --> 00:04:04,310
standart sapmaya bölerek buluyoruz.

58
00:04:04,310 --> 00:04:09,312
Sayısal istikrar için, genelde paydaya epsilon ekleriz.

59
00:04:09,312 --> 00:04:14,460
Sadece sigmanın karesi herhangi bir tahminde sıfır çıkarsa diye.

60
00:04:14,460 --> 00:04:17,405
Şimdi bu z değerlerini aldık,

61
00:04:17,405 --> 00:04:23,010
ortalamaları sıfır olacak ve
 standart ek girdi değişikliği olacak
 şekilde normalleştirdik

62
00:04:23,010 --> 00:04:25,903
Yani z'nin her bileşeni 0 ortalamasına ve 1 değişikliğine sahip.

63
00:04:25,903 --> 00:04:31,352
Ama gizli ek girdilerin her zaman
 ortalamalarının 0 ve değişikliklerinin 1
 olmasını istemiyoruz.

64
00:04:31,352 --> 00:04:38,953
Belki gizli ek girdilerin farklı bir
 dağıtımlarının olması mantıklı geliyordur.

65
00:04:38,953 --> 00:04:42,939
Yani bunun yerine yapacağımız şunu hesaplamak:

66
00:04:42,939 --> 00:04:48,434
Buna "z tilde = gama zi norm. + beta" diyeceğim.

67
00:04:48,434 --> 00:04:55,210
Ve burada, gama ve beta
modelinizin öğrenilebilir parametreleri.

68
00:04:58,957 --> 00:05:03,675
Yani eğim inişini, ya da başka bir algoritmayı,momentumun

69
00:05:03,675 --> 00:05:08,136
eğim inişi, ya da rms düzgün atomu gibi,
 gama ve beta parametrelerini güncellersiniz.

70
00:05:08,136 --> 00:05:11,410
tıpkı sinir ağınızın ağırlıklarını
güncelleyeceğiniz gibi

71
00:05:11,410 --> 00:05:16,582
Şimdi, gama ve betanın etkisinin

72
00:05:16,582 --> 00:05:22,140
z tildenin ortalamasının istediğiniz
bir şey olmasını sağladığına dikkat edin.

73
00:05:22,140 --> 00:05:27,720
Hatta, eğer gama eşittir karekök sigma kare

74
00:05:28,800 --> 00:05:33,570
artı epsilon, eğer gama bu payda
terimine eşit olsaydı.

75
00:05:33,570 --> 00:05:39,318
Ve beta mu'ya eşit olsaydı,
buradaki değer,

76
00:05:39,318 --> 00:05:43,998
gama z norm. artı beta

77
00:05:43,998 --> 00:05:49,540
kesinlikle bu eşitliğ tersine çevirirdi.

78
00:05:49,540 --> 00:05:52,284
Eğer, bu doğruysa

79
00:05:52,284 --> 00:05:57,780
O zaman aslında z tilde i eşittir zi.

80
00:05:57,780 --> 00:06:02,633
Ve gama ve beta parametrelerinin
uygun bir ayarıyla,

81
00:06:02,633 --> 00:06:05,321
bu normalleştirme adımı, şu ki,

82
00:06:05,321 --> 00:06:11,175
Bu 4 denklem sadece kimlik
fonksiyonunu hesaplıyor.

83
00:06:11,175 --> 00:06:16,112
Ama gama ve betanın başka değerlerini seçmek
gizli girdi değerlerinin

84
00:06:16,112 --> 00:06:19,320
başka ortalamalara ve değişiklik değerlerine
sahip olmasına izin veriyor.

85
00:06:19,320 --> 00:06:23,538
Bunu sinir ağınıza uydurmanın yolu

86
00:06:23,538 --> 00:06:28,583
halbuki daha önce bu z1,z2 ve benzer değerleri kullanıyordunuz,

87
00:06:28,583 --> 00:06:35,195
sinir ağınızdaki daha sonraki hesaplamalarda zi yerine

88
00:06:35,195 --> 00:06:39,910
z tilda i kullanırsınız.

89
00:06:39,910 --> 00:06:45,129
Ve bu [I]i açıkça hangi katmanda olduğunu belirtmek için geri koymak istersiniz,

90
00:06:45,129 --> 00:06:46,910
buraya geri koyabilirsiniz.

91
00:06:46,910 --> 00:06:51,319
Yani, bundan almış olacağınızı umduğum şey, gördük ki

92
00:06:51,319 --> 00:06:56,140
girdi özelliği x'i normalleştirmek bir sinir ağında öğrenmeye yardımcı olabilir.

93
00:06:56,140 --> 00:07:00,029
Ve toptan normun yaptığı bu normalleştirme sürecini sadece

94
00:07:00,029 --> 00:07:01,283
girdi katmanına değil, sinir ağındaki bir saklı katmanın

95
00:07:01,283 --> 00:07:04,810
en dibindeki değerlere de uygulamaktır.

96
00:07:04,810 --> 00:07:09,085
Yani bu tip bir normalleşmeyi saklı ünitenizin bazı değerlerinin, z

97
00:07:09,085 --> 00:07:12,390
ortalama ve varyasını normalleştirmek için uygulayacaktır.

98
00:07:12,390 --> 00:07:16,833
Ancak girdiyi ve bu saklı ünite değerlerini eğitmek arasındaki bir fark,

99
00:07:16,833 --> 00:07:21,220
saklı ünite değerlerinizi ortalama sıfır ve varyans 1 olacak şekilde zorlamak istemeyebilirsiniz.

100
00:07:21,220 --> 00:07:24,247
Örneğin, eğer bir s işlevli etkilenim fonksiyonunuz varsa,

101
00:07:24,247 --> 00:07:27,230
değerlerinizin hep burada kümelenmesini istemezsiniz.

102
00:07:27,230 --> 00:07:31,582
Onların daha geniş bir değişikliğe ya da
 0'dan farklı bir ortalamaya sahip olmasını isteyebilirsiniz.

103
00:07:31,582 --> 00:07:35,322
Sigmoid fonksiyonun düzsel-olmayan yapısından

104
00:07:35,322 --> 00:07:41,060
faydalanmak yerine değerlerinizin
 bu düz düzende olmasını istemeyebileceğinizden dolayı

105
00:07:41,060 --> 00:07:45,067
İşte bu yüzden gama ve beta parametreleriyle

106
00:07:45,067 --> 00:07:51,230
zi değerlerinizin istediğiniz bir değer aralığına
 sahip olduğundan emin olabilirsiniz.

107
00:07:51,230 --> 00:07:55,671
Aslında yaptığı şey gizli ek girdilerinizin standartlaştırılmış

108
00:07:55,671 --> 00:07:59,226
ortalama ve değişikliğe sahip olduğunu gösteriyor.
O da ortalama ve değişikliğin

109
00:07:59,226 --> 00:08:03,429
iki özel gama ve beta parametreleri tarafından kontrol edildiği
,ki öğrenme algoritması onları istedikleri şeye ayarlayabilir,

110
00:08:03,429 --> 00:08:07,826
standartlaştırılmış ortalama ve değişikliğe sahip olduğunu gösteriyor.

111
00:08:07,826 --> 00:08:13,004
Yani gerçekten yaptığı bu gizli ek girdilerin

112
00:08:13,004 --> 00:08:18,660
ortalama ve değişiklik değerlerini, aslında zi ler, sabit bir ortalama ve değişikliğe ayarlıyor.

113
00:08:18,660 --> 00:08:22,320
Ve o ortalama ve değişiklik 0 ve 1 olabilir, ya da tamamen farklı değerler olabilir.

114
00:08:22,320 --> 00:08:26,680
ve bu gama ve beta parametreleriyle kontrol ediliyor.

115
00:08:26,680 --> 00:08:30,424
Yani umuyorum ki bu size toptan normunun mekanikleri hakkında bir algı sağlar.

116
00:08:30,424 --> 00:08:32,830
En azından tek katmanlı bir sinir ağı için.

117
00:08:32,830 --> 00:08:36,104
Gelecek videoda, sizlere toptan normunu nasıl bir sinirsel ağa,

118
00:08:36,104 --> 00:08:39,052
hatta daha da derin bir sinirsel ağa, ve bir sinirsel ağın

119
00:08:39,052 --> 00:08:41,700
pek çok farklı katmanı için nasıl uygulanacağını ve çalışacağını göstereceğim.

120
00:08:41,700 --> 00:08:45,450
Ve ondan sonra, toptan normunun nasıl bize 

121
00:08:45,450 --> 00:08:47,120
sinirsel ağlarınızı eğitmeniz hakkında
 yardım edebileceğinin önsezisini alacağız.

122
00:08:47,120 --> 00:08:51,424
Eğer hala nasıl çalıştığı biraz gizemli geliyorsa, dayanın ve

123
00:08:51,424 --> 00:08:54,949
iki video içinde bunu daha açık hale getireceğiz.