1
00:00:00,000 --> 00:00:02,115
你已經看過批次標準化的

2
00:00:02,115 --> 00:00:05,020
方程式推導了，在單一個隱藏層上

3
00:00:05,020 --> 00:00:08,610
讓我們來瞧瞧這要怎麼運用在深度網路的訓練上

4
00:00:08,610 --> 00:00:10,969
假設你有個這樣的神經網路

5
00:00:10,969 --> 00:00:16,395
你知道我曾說，你可以把每個神經元
看成在算兩件事

6
00:00:16,395 --> 00:00:22,960
首先算出Z，然後通過啟動函數進一步算出a

7
00:00:22,960 --> 00:00:31,005
所以我們可看作每個圓圈都表示
兩步的計算

8
00:00:31,005 --> 00:00:33,130
下一層也一樣

9
00:00:33,130 --> 00:00:41,125
z[2]_1 和 a[2]_1，以此類推

10
00:00:41,125 --> 00:00:45,250
假設先不套用批次標準法的話

11
00:00:45,250 --> 00:00:50,935
你會有一個輸入 X 餵進去第一個隱藏層

12
00:00:50,935 --> 00:00:53,395
然後先算出 Z[1]

13
00:00:53,395 --> 00:00:57,940
這是由參數W[1], b[1]所控制的

14
00:00:57,940 --> 00:01:04,630
然後正常來說，你會把Z[1]餵進去啟動函數，算出a[1]

15
00:01:04,630 --> 00:01:09,165
但如果是套用批次標準化的時候，我們會拿Z[1]這個值

16
00:01:09,165 --> 00:01:12,975
套用批次標準化

17
00:01:12,975 --> 00:01:16,935
有時簡寫成 BN

18
00:01:16,935 --> 00:01:19,685
那會由這些參數控制

19
00:01:19,685 --> 00:01:23,465
beta[1] 和 gamma[1]

20
00:01:23,465 --> 00:01:28,340
這樣會給你一個新的，標準化z[1]後的值

21
00:01:28,340 --> 00:01:32,930
然後你再拿那個值給啟動函數，得到a[1]

22
00:01:32,930 --> 00:01:38,355
也就是 g[1] 套在 Ztilde[1] 上

23
00:01:38,355 --> 00:01:41,770
現在你算完了第一層

24
00:01:41,770 --> 00:01:47,520
其中批次標準化發生在
從z到a的計算途中

25
00:01:47,520 --> 00:01:53,785
接下來，你拿a[1]這個值去計算出Z[2]

26
00:01:53,785 --> 00:01:58,115
其中由參數W[2], b[2]控制

27
00:01:58,115 --> 00:02:01,125
類似第一層所作的

28
00:02:01,125 --> 00:02:06,470
你拿Z[2]，套用批次標準化，我們在這用縮寫BN

29
00:02:06,470 --> 00:02:11,575
這則是由下一層的批次標準化參數所控制

30
00:02:11,575 --> 00:02:14,580
也就是 beta[2], gamma[2]

31
00:02:14,580 --> 00:02:17,845
這會給你 Z tilde[2]

32
00:02:17,845 --> 00:02:25,220
然後用它，套用到啟動函數算出 a[2]，依此類推

33
00:02:25,220 --> 00:02:31,960
再一次，批次標準化發生在
計算z和計算a的中間

34
00:02:31,960 --> 00:02:33,260
概念上是

35
00:02:33,260 --> 00:02:36,115
本來用沒標準化過的值Z

36
00:02:36,115 --> 00:02:40,360
但現在改成拿標準化的值 Z tilde，例如這邊第一層

37
00:02:40,360 --> 00:02:41,480
第二層也是

38
00:02:41,480 --> 00:02:44,310
本來用沒標準過的Z[2]

39
00:02:44,310 --> 00:02:48,990
你改成用由平均和變異數標準過的值 Ztilde[2]

40
00:02:48,990 --> 00:02:56,320
所以你網路的參數會是W[1], b[1]

41
00:02:56,320 --> 00:03:00,355
實際上我們會拿掉參數b，不過下個投影片再說

42
00:03:00,355 --> 00:03:06,740
現在呢，參數有原本的 W[1]

43
00:03:06,740 --> 00:03:12,260
b[1] 到 W[L], b[L]，還有我們剛剛加進去網路的

44
00:03:12,260 --> 00:03:14,420
新的參數 beta[1],

45
00:03:14,420 --> 00:03:18,290
gamma[1], beta[2], gamma[2]

46
00:03:18,290 --> 00:03:24,283
等等等，對於你有套用批次標準化的每一層

47
00:03:24,283 --> 00:03:26,630
澄清一下，這邊的 beta 們

48
00:03:26,630 --> 00:03:30,800
這些和超參數beta沒有關係，就是那些我們曾看過

49
00:03:30,800 --> 00:03:36,165
在動量方法中，利用指數加權平均的那個beta

50
00:03:36,165 --> 00:03:42,620
Adam法的作者在論文中用了beta來表示那個超參數

51
00:03:42,620 --> 00:03:47,405
批次標準化的作者在論文中也用了beta表示這參數

52
00:03:47,405 --> 00:03:49,630
但此beta非彼beta

53
00:03:49,630 --> 00:03:53,300
我還是在兩邊都寫了beta

54
00:03:53,300 --> 00:03:55,114
以防你去讀了原始論文

55
00:03:55,114 --> 00:03:57,230
不過 beta[1],

56
00:03:57,230 --> 00:03:58,535
beta[2], 等等

57
00:03:58,535 --> 00:04:02,650
這些批次標準化所學的，是不同於

58
00:04:02,650 --> 00:04:10,055
在動量法、Adam法、RMSprop演算法
裡面的超參數beta

59
00:04:10,055 --> 00:04:14,795
總之，這些是你演算法中新的參數

60
00:04:14,795 --> 00:04:18,065
你就可以運用任一種你喜歡的最佳化算法

61
00:04:18,065 --> 00:04:21,710
例如梯度下降法來實作之

62
00:04:21,710 --> 00:04:26,885
例如，你可能算出某一層的 d beta[l]

63
00:04:26,885 --> 00:04:28,720
然後更新參數 beta

64
00:04:28,720 --> 00:04:32,270
變成 beta 減掉學習率乘上

65
00:04:32,270 --> 00:04:37,415
d beta[l]。你也可以用

66
00:04:37,415 --> 00:04:43,405
Adam法、RMSprop或動量方法
來更新參數 beta 和 gamma

67
00:04:43,405 --> 00:04:45,575
不一定要用梯度下降法

68
00:04:45,575 --> 00:04:48,065
還有，雖然在之前的影片

69
00:04:48,065 --> 00:04:51,570
我解釋了批次標準化的運算怎麼做

70
00:04:51,570 --> 00:04:55,590
算平均和變異數，減掉它除以它

71
00:04:55,590 --> 00:05:00,625
如果你使用深度學習的程式框架

72
00:05:00,625 --> 00:05:06,485
通常你不用自己實作
批次標準化的步驟或是批次標準化的層

73
00:05:06,485 --> 00:05:07,840
在某些程式框架之下

74
00:05:07,840 --> 00:05:09,990
你可以用一行程式就解決了

75
00:05:09,990 --> 00:05:13,140
例如在TensorFlow

76
00:05:13,140 --> 00:05:17,490
你可以用這個函數來實作批次標準化

77
00:05:17,490 --> 00:05:19,530
我們後面會介紹更多關於程式框架的事

78
00:05:19,530 --> 00:05:24,435
不過實務上，你大概不用自己實作所有的細節

79
00:05:24,435 --> 00:05:27,480
但了解其運作還是很值得的，這樣你可以

80
00:05:27,480 --> 00:05:30,930
更了解你的程式到底在做什麼

81
00:05:30,930 --> 00:05:36,805
不過在深度學習框架，
通常只要一行程式就可以做出批次標準化

82
00:05:36,805 --> 00:05:40,560
那到目前為止，我們介紹批次標準法
都還在假設

83
00:05:40,560 --> 00:05:45,390
你訓練一整個訓練資料，好比用批次梯度下降法

84
00:05:45,390 --> 00:05:51,720
實際上，批次標準法通常用在
訓練集的小批資料(mini-batch)上

85
00:05:51,720 --> 00:05:54,360
所以套用批次標準法的時候，你是拿

86
00:05:54,360 --> 00:05:59,830
第一批小量資料(mini-batch)，算出Z[1]

87
00:05:59,830 --> 00:06:03,460
和前一張投影片一樣，我們用參數 W[1],

88
00:06:03,460 --> 00:06:11,365
b[1]，然後你只拿這一小批資料，算出Z[1]的
平均和變異數，只在

89
00:06:11,365 --> 00:06:14,695
這一小批上，然後批次標準化會

90
00:06:14,695 --> 00:06:21,580
減掉平均，除以標準差，然後用beta[1], gamma[1]

91
00:06:21,580 --> 00:06:24,490
重新矯正之，來給你 Z[1]

92
00:06:24,490 --> 00:06:27,375
這全部的步驟都是在第一批小量資料上

93
00:06:27,375 --> 00:06:33,325
然後你套用啟動函數，得到a[1]

94
00:06:33,325 --> 00:06:38,110
然後你用 W[2], b[2] 算出

95
00:06:38,110 --> 00:06:41,190
Z[2]，依此類推。

96
00:06:41,190 --> 00:06:45,360
剛剛所講的，都是你在對第一小批資料

97
00:06:45,360 --> 00:06:50,660
執行了一步的梯度下降。接下來
到了第二批小量資料 X2

98
00:06:50,660 --> 00:06:54,190
然後做類似的事，就是在第二批資料

99
00:06:54,190 --> 00:06:59,085
上計算 Z[1]，然後用批次標準化算出 Z[1]tilde

100
00:06:59,085 --> 00:07:02,390
那麼，在批次標準化的這一步

101
00:07:02,390 --> 00:07:08,890
你只用第二批的小量資料來標準化 z tilde

102
00:07:08,890 --> 00:07:10,640
這邊的 BN 步驟也是

103
00:07:10,640 --> 00:07:13,580
我們看了第二批的小資料

104
00:07:13,580 --> 00:07:18,320
只在那一小批上，算出Z[1]的平均和變異數

105
00:07:18,320 --> 00:07:24,175
用beta和gamma重新縮放得到 Z tilde，依此類推

106
00:07:24,175 --> 00:07:28,840
然後你做在第三批小量資料上，繼續訓練

107
00:07:28,840 --> 00:07:34,415
現在，有個細節，我想要清理一下參數

108
00:07:34,415 --> 00:07:38,990
在剛剛，我說過我們有參數 W[l], b[l]

109
00:07:38,990 --> 00:07:43,640
在每一層，還有 beta[l],

110
00:07:43,640 --> 00:07:50,900
gamma[l]。那注意一下 Z[l] 是用下面的公式計算的

111
00:07:50,900 --> 00:08:00,590
Z[l] = W[l] * A[l-1] + b[l]。
但是批次標準化所作的

112
00:08:00,590 --> 00:08:02,985
是他會看這一小批資料，首先標準化

113
00:08:02,985 --> 00:08:06,515
Z[l]，讓它平均為0，變異數為1

114
00:08:06,515 --> 00:08:09,275
然後用 beta 和 gamma 重新縮放

115
00:08:09,275 --> 00:08:10,745
但這也意味著

116
00:08:10,745 --> 00:08:15,125
無論 b[l] 的值為何，實際上他會抵銷掉

117
00:08:15,125 --> 00:08:17,735
因為在批次標準化的步驟

118
00:08:17,735 --> 00:08:22,090
你會算 Z[l] 的平均，然後減掉那個平均

119
00:08:22,090 --> 00:08:27,675
所以當把這批所有的資料，都加一個常數的時候

120
00:08:27,675 --> 00:08:28,865
這並不會有任何改變

121
00:08:28,865 --> 00:08:34,170
因為你加的任何數字，都會在「減掉平均」這一步抵銷掉

122
00:08:34,170 --> 00:08:35,960
所以如果你使用批次標準化

123
00:08:35,960 --> 00:08:38,225
實際上你可以拿掉這個參數

124
00:08:38,225 --> 00:08:42,020
或者也能想成，這個參數永遠被設成0

125
00:08:42,020 --> 00:08:49,235
所以公式變成 z[l] 只是 W[l] 乘以 a[l-1]

126
00:08:49,235 --> 00:08:54,375
然後算出標準後的 z[l]

127
00:08:54,375 --> 00:09:04,610
然後算出 z tilde = gamma * z[l]norm + beta

128
00:09:04,610 --> 00:09:09,080
你最後是用 beta[l] 這個參數來決定

129
00:09:09,080 --> 00:09:15,095
Z tilde[l] 的平均，這個會傳到下一層

130
00:09:15,095 --> 00:09:16,430
總結一下

131
00:09:16,430 --> 00:09:24,020
因為批次標準化會把這層這些 Z[l] 的平均變成 0

132
00:09:24,020 --> 00:09:27,445
所以沒有必要留 b[l] 這個參數

133
00:09:27,445 --> 00:09:29,400
所以你拿掉他

134
00:09:29,400 --> 00:09:32,330
某種角度改用 beta[l]

135
00:09:32,330 --> 00:09:39,050
這參數影響最後的平移，或可說是偏移項

136
00:09:39,050 --> 00:09:43,250
最後，還記得 Z[l] 的維度

137
00:09:43,250 --> 00:09:45,255
如果只做在一筆資料

138
00:09:45,255 --> 00:09:48,255
會是 n[l] 乘 1

139
00:09:48,255 --> 00:09:53,270
所以 b[l] 的維度是 n[l]乘1

140
00:09:53,270 --> 00:09:56,365
假設第 l 層有 n[l] 個隱藏單元

141
00:09:56,365 --> 00:10:00,230
因此，beta[l] 和 gamma[l] 的維度

142
00:10:00,230 --> 00:10:07,575
也會是 n[l] 乘 1，因為那是隱藏單元的個數

143
00:10:07,575 --> 00:10:12,555
你有 n[l] 個隱藏單元，而beta[l]和gamma[l]

144
00:10:12,555 --> 00:10:14,670
是用來縮放每一個隱藏單元的

145
00:10:14,670 --> 00:10:19,195
平均和變異數，看網路想要設成多少

146
00:10:19,195 --> 00:10:21,990
那麼，我們來整理一下，講講要怎麼

147
00:10:21,990 --> 00:10:25,195
在批次標準化之下，實作梯度下降法

148
00:10:25,195 --> 00:10:28,925
假設你用的是小批次梯度下降法

149
00:10:28,925 --> 00:10:34,245
t 從 1 到批的數量迭代下去

150
00:10:34,245 --> 00:10:39,265
你會實作正向傳播在

151
00:10:39,265 --> 00:10:44,635
小批資料 Xt 上，然後對每個隱藏層做正向傳播

152
00:10:44,635 --> 00:10:50,330
用批次標準法，

153
00:10:50,330 --> 00:10:57,265
把 Z[l] 用 Z tilde[l] 代替掉，
所以這確保在這一小批資料內，

154
00:10:57,265 --> 00:11:02,810
Z 這個值最終會有標準化過的平均和變異數

155
00:11:02,810 --> 00:11:09,200
標準化後的版本就是這個 Z tilde[l]。然後

156
00:11:09,200 --> 00:11:17,025
你用反向傳播計算 dW

157
00:11:17,025 --> 00:11:20,065
db, 對於每個L

158
00:11:20,065 --> 00:11:23,530
d beta, d gamma

159
00:11:23,530 --> 00:11:26,805
其實技術上來說，因為你不用 b 了

160
00:11:26,805 --> 00:11:28,494
這個就拿掉了

161
00:11:28,494 --> 00:11:33,595
最後，你更新參數

162
00:11:33,595 --> 00:11:40,085
w 更新為 w 減掉學習率乘這個，和之前一樣

163
00:11:40,085 --> 00:11:47,775
beta 更新為 beta 減掉學習率乘上 d beta

164
00:11:47,775 --> 00:11:49,595
gamma 也一樣

165
00:11:49,595 --> 00:11:52,770
如果你算出這些梯度

166
00:11:52,770 --> 00:11:54,805
你可以用梯度下降法

167
00:11:54,805 --> 00:11:56,910
就是我這邊所寫的

168
00:11:56,910 --> 00:12:01,845
但這也可以適用於動量梯次下降法

169
00:12:01,845 --> 00:12:07,200
或 RMSprop，或 Adam

170
00:12:07,200 --> 00:12:08,890
不一定用這個梯度下降方法

171
00:12:08,890 --> 00:12:11,220
來更新，而是用之前影片講過的

172
00:12:11,220 --> 00:12:16,615
其他那些最佳化方法來更新

173
00:12:16,615 --> 00:12:19,790
其他一些最佳化演算法也可以用來更新

174
00:12:19,790 --> 00:12:25,730
批次標準化所新增的參數 beta 和 gamma

175
00:12:25,730 --> 00:12:27,780
那麼，我希望這能給你一些概念，關於

176
00:12:27,780 --> 00:12:30,375
如何從無到有，實作出批次標準化
－如果你想要的話

177
00:12:30,375 --> 00:12:31,530
如果你用

178
00:12:31,530 --> 00:12:34,455
某個深度學習的程式框架，我們會在後面提到

179
00:12:34,455 --> 00:12:37,700
那麼希望你只須呼叫別人寫好的

180
00:12:37,700 --> 00:12:41,720
在框架內的實作，這樣會讓套用批次標準化更簡單

181
00:12:41,720 --> 00:12:45,515
那麼，萬一批次標準化看起來還是很神奇的話

182
00:12:45,515 --> 00:12:49,375
如果你仍不確定
這怎麼能加速訓練到超級快

183
00:12:49,375 --> 00:12:52,140
讓我們進到下部影片，探討更多關於

184
00:12:52,140 --> 00:12:55,210
為什麼批次標準化真的有用，還有他到底做了什麼