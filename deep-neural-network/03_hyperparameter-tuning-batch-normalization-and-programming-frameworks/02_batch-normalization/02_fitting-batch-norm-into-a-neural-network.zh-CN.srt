1
00:00:00,000 --> 00:00:02,115
所以我们已经知道了单隐藏层的Batch Norm的公式

2
00:00:02,115 --> 00:00:05,020
所以我们已经知道了单隐藏层的Batch Norm的公式

3
00:00:05,020 --> 00:00:08,610
让我们看看如何在深度网络中使用它

4
00:00:08,610 --> 00:00:10,969
那么 假设我们有一个像这样的神经网络

5
00:00:10,969 --> 00:00:16,395
我们都知道 每一个节点都可以看作是在计算两个事情

6
00:00:16,395 --> 00:00:22,960
首先 计算z
然后把z传递给激活函数来计算a

7
00:00:22,960 --> 00:00:31,005
所以我们认为每一个圆圈都代表着两步计算

8
00:00:31,005 --> 00:00:33,130
下一层同样如此

9
00:00:33,130 --> 00:00:41,125
这是z21以及a21 此次类推

10
00:00:41,125 --> 00:00:45,250
所以 如果我们不采用Batch Norm

11
00:00:45,250 --> 00:00:50,935
我们让输入x进入到第一级隐藏层

12
00:00:50,935 --> 00:00:53,395
然后首先计算z1

13
00:00:53,395 --> 00:00:57,940
这由参数w1和b1决定

14
00:00:57,940 --> 00:01:04,630
如此这般 我们把z1输入给激活函数来计算a1

15
00:01:04,630 --> 00:01:09,165
但是在Batch Norm中 把z1作为输入会怎么样呢 ?

16
00:01:09,165 --> 00:01:12,975
再次做Batch Norm变换

17
00:01:12,975 --> 00:01:16,935
它的缩写一般为BN

18
00:01:16,935 --> 00:01:19,685
它将由参数Beta1以及Gamma1决定

19
00:01:19,685 --> 00:01:23,465
它将由参数Beta1以及Gamma1决定

20
00:01:23,465 --> 00:01:28,340
这将产生新的归一化值z1

21
00:01:28,340 --> 00:01:32,930
然后我们把它输入给激活函数来获取a1

22
00:01:32,930 --> 00:01:38,355
也就是g1函数作用于(带波浪号的)z1

23
00:01:38,355 --> 00:01:41,770
现在 我们已经完成了第一层的计算

24
00:01:41,770 --> 00:01:47,520
在此 BN算法确实被应用于由z到a的计算过程中

25
00:01:47,520 --> 00:01:53,785
下一步 我们用a1值来计算z2

26
00:01:53,785 --> 00:01:58,115
现在这由参数w2和b2决定

27
00:01:58,115 --> 00:02:01,125
与第一层类似

28
00:02:01,125 --> 00:02:06,470
我们在z2上应用Batch Norm 缩写仍然是BN

29
00:02:06,470 --> 00:02:11,575
这由下一层的特定BN参数决定

30
00:02:11,575 --> 00:02:14,580
也就是Beta2和Gamma2

31
00:02:14,580 --> 00:02:17,845
我们获得了(带波浪号的)z2

32
00:02:17,845 --> 00:02:25,220
我们又可以用它通过激活函数来计算a2 以此类推

33
00:02:25,220 --> 00:02:31,960
再一次 在计算z和a的过程中<br />我们运用了BN算法

34
00:02:31,960 --> 00:02:33,260
直觉是

35
00:02:33,260 --> 00:02:36,115
不要使用未归一化的z值

36
00:02:36,115 --> 00:02:40,360
而是要使用归一化后的(带波浪号的)z值<br />这适用于第一层

37
00:02:40,360 --> 00:02:41,480
也适用于第二层

38
00:02:41,480 --> 00:02:44,310
不要使用未归一化的z2值

39
00:02:44,310 --> 00:02:48,990
而要使用均值和方差归一化后的(带波浪号的)z2值

40
00:02:48,990 --> 00:02:56,320
所以网络参数将是w1 b1

41
00:02:56,320 --> 00:03:00,355
我们可以忽略b参数 我会在下一张幻灯片说明

42
00:03:00,355 --> 00:03:06,740
现在 假设参数就是一般的w1和b1

43
00:03:06,740 --> 00:03:12,260
直到wL bL<br />并且我们还将加入其余的参数到新网络中

44
00:03:12,260 --> 00:03:14,420
并且我们还将加入其余的参数到新网络中 ‑ Beta1

45
00:03:14,420 --> 00:03:18,290
Gamma1 Beta2 Gamma2以此类推

46
00:03:18,290 --> 00:03:24,283
对每个采用BN算法的网络层都要如此

47
00:03:24,283 --> 00:03:26,630
澄清一下 注意到这里的参数Betas

48
00:03:26,630 --> 00:03:30,800
它们与我们在Momentum算法中<br />用于计算各种指数加权平均值的超参数Beta无关

49
00:03:30,800 --> 00:03:36,165
它们与我们在Momentum算法中<br />用于计算各种指数加权平均值的超参数Beta无关

50
00:03:36,165 --> 00:03:42,620
Adam算法的作者使用Beta来表示超参数

51
00:03:42,620 --> 00:03:47,405
而BN算法的作者也使用Beta来表示该参数

52
00:03:47,405 --> 00:03:49,630
但这是两种完全不同的Betas

53
00:03:49,630 --> 00:03:53,300
我决定在两种情况下都使用Beta

54
00:03:53,300 --> 00:03:55,114
以防你们读到原文时候困惑

55
00:03:55,114 --> 00:03:57,230
但是 Beta1

56
00:03:57,230 --> 00:03:58,535
Beta2 以此类推

57
00:03:58,535 --> 00:04:02,650
BN算法中学习的参数Beta

58
00:04:02,650 --> 00:04:10,055
不同于Adam RMSprop和Momentum算法中使用的超参数Beta

59
00:04:10,055 --> 00:04:14,795
所以现在这些是 我们算法 中的新参数

60
00:04:14,795 --> 00:04:18,065
我们然后可以根据需求进行优化

61
00:04:18,065 --> 00:04:21,710
例如采用梯度递减的方法来实现它

62
00:04:21,710 --> 00:04:26,885
例如 我们可能会计算dBeta L(L层Beta的导数)

63
00:04:26,885 --> 00:04:28,720
然后更新Beta参数

64
00:04:28,720 --> 00:04:32,270
用Beta减去学习率乘以dBeta L(L层Beta的导数)

65
00:04:32,270 --> 00:04:37,415
用Beta减去学习率乘以dBeta L(L层Beta的导数)

66
00:04:37,415 --> 00:04:43,405
你也可以用Adam或RMSprop或Momentum算法<br />来更新参数Beta和Gamma

67
00:04:43,405 --> 00:04:45,575
并不仅仅采用梯度递减的方法

68
00:04:45,575 --> 00:04:48,065
尽管在上一个视频中

69
00:04:48,065 --> 00:04:51,570
我已经解释过了BN算法的效果

70
00:04:51,570 --> 00:04:55,590
它计算平均值和方差 并把它们减去和除去

71
00:04:55,590 --> 00:05:00,625
如果你正在使用深度学习编程框架

72
00:05:00,625 --> 00:05:06,485
通常你不必自己去实现BN算法

73
00:05:06,485 --> 00:05:07,840
在编程框架中 它可能仅仅是一行代码

74
00:05:07,840 --> 00:05:09,990
在编程框架中 它可能仅仅是一行代码

75
00:05:09,990 --> 00:05:13,140
例如 在TensorFlow框架中

76
00:05:13,140 --> 00:05:17,490
我们可以用这个函数实现BN

77
00:05:17,490 --> 00:05:19,530
我们待会儿会讲到编程框架

78
00:05:19,530 --> 00:05:24,435
但是实际上 你最后可能不用自己去实现这些细节

79
00:05:24,435 --> 00:05:27,480
了解它怎么工作就可以了

80
00:05:27,480 --> 00:05:30,930
这有助于我们理解自己的代码

81
00:05:30,930 --> 00:05:36,805
一般在深度学习框架下实现BN通常只需要一行代码

82
00:05:36,805 --> 00:05:40,560
现在 到目前为止 我们已经讲过了在整个训练集上<br />利用批量梯度下降训练的BN算法

83
00:05:40,560 --> 00:05:45,390
我们已经讲过了在整个训练集上<br />利用批量梯度下降训练的BN算法

84
00:05:45,390 --> 00:05:51,720
实际上 通常应用到训练集上的<br />是少批量 (mini-batch) BN算法

85
00:05:51,720 --> 00:05:54,360
所以实际使用BN算法时

86
00:05:54,360 --> 00:05:59,830
我们取第一个mini-batch并计算z1

87
00:05:59,830 --> 00:06:03,460
和上张幻灯片一样 我们用参数w1 b1

88
00:06:03,460 --> 00:06:11,365
然后我们取一个适合的mini-batch<br />并利用它计算z1的均值和方差

89
00:06:11,365 --> 00:06:14,695
接着BN算法就会

90
00:06:14,695 --> 00:06:21,580
减去均值 除以标准差<br />然后用参数Beta1 Gamma1来重新调整

91
00:06:21,580 --> 00:06:24,490
从而得到(带波浪号的)z1

92
00:06:24,490 --> 00:06:27,375
这些都是在第一个mini-batch上计算的

93
00:06:27,375 --> 00:06:33,325
然后我们应用激活函数来获得a1

94
00:06:33,325 --> 00:06:38,110
然后我们用w2 b2来计算z2

95
00:06:38,110 --> 00:06:41,190
以此类推

96
00:06:41,190 --> 00:06:45,360
我们做这些是为了

97
00:06:45,360 --> 00:06:50,660
在第一个mini-batch上计算梯度下降<br />接着转到第二个mini-batch上 也就是x2

98
00:06:50,660 --> 00:06:54,190
然后的步骤差不多 我们现在

99
00:06:54,190 --> 00:06:59,085
在第二个mini-batch上计算z1<br />然后利用BN算(带波浪号的)z1

100
00:06:59,085 --> 00:07:02,390
所以在BN这一步中

101
00:07:02,390 --> 00:07:08,890
我们会用第二个mini-batch中的数据来归一化(带波浪号的)z

102
00:07:08,890 --> 00:07:10,640
这就是BN算法在此的作用

103
00:07:10,640 --> 00:07:13,580
我们来看个在第二个mini-batch过程中例子

104
00:07:13,580 --> 00:07:18,320
它会计算z1在此mini-batch上的平均值和方差

105
00:07:18,320 --> 00:07:24,175
并且通过参数Beta和Gamma来重新调整<br />以获得(带波浪号的)z 以此类推

106
00:07:24,175 --> 00:07:28,840
然后我们在第三个mini-batch上重复上述步骤<br />如此一直训练下去

107
00:07:28,840 --> 00:07:34,415
现在 我想澄清关于参数的一个细节

108
00:07:34,415 --> 00:07:38,990
那就是 我曾经说过的每一层的参数 wL bL

109
00:07:38,990 --> 00:07:43,640
以及Beta L和Gamma L

110
00:07:43,640 --> 00:07:50,900
现在注意z是由下面的步骤计算得出的

111
00:07:50,900 --> 00:08:00,590
也就是zL=wL * a(L-1)+bL<br />但是BN算法所做的

112
00:08:00,590 --> 00:08:02,985
就是使用mini-batch并且归一化zL

113
00:08:02,985 --> 00:08:06,515
来满足均值为0以及标准方差

114
00:08:06,515 --> 00:08:09,275
然后通过参数Beta和Gamma来重新调整

115
00:08:09,275 --> 00:08:10,745
这意味着

116
00:08:10,745 --> 00:08:15,125
不管bL的值为多少 实际上都要被减去

117
00:08:15,125 --> 00:08:17,735
因为经过BN这一步

118
00:08:17,735 --> 00:08:22,090
我们将计算zL的均值并将其减去

119
00:08:22,090 --> 00:08:27,675
所以在mini-batch中对所有例子加上一个常量

120
00:08:27,675 --> 00:08:28,865
并不会改变什么

121
00:08:28,865 --> 00:08:34,170
因为无论我们加上什么常量<br />它都会被 减均值 这一步给去除

122
00:08:34,170 --> 00:08:35,960
所以如果你使用BN算法

123
00:08:35,960 --> 00:08:38,225
你可以忽略该参数b

124
00:08:38,225 --> 00:08:42,020
或者可以认为它永远等于0

125
00:08:42,020 --> 00:08:49,235
所以参数就变为了 zL=wL * a(L-1)

126
00:08:49,235 --> 00:08:54,375
然后我们计算归一化zL

127
00:08:54,375 --> 00:09:04,610
并且计算(带波浪号的)z等于Gamma乘以zL加上Beta

128
00:09:04,610 --> 00:09:09,080
我们最终使用该参数Beta L来决定

129
00:09:09,080 --> 00:09:15,095
(带波浪号的)zL的均值<br />它将在下一层中得到转发

130
00:09:15,095 --> 00:09:16,430
复习一下

131
00:09:16,430 --> 00:09:24,020
因为BN算法使层级中各个zL的均值为0

132
00:09:24,020 --> 00:09:27,445
我们就没有理由保留参数bL

133
00:09:27,445 --> 00:09:29,400
所以就把它忽略了

134
00:09:29,400 --> 00:09:32,330
相应地 被Beta L所代替

135
00:09:32,330 --> 00:09:39,050
也就是一个用来控制最终偏移量影响的参数

136
00:09:39,050 --> 00:09:43,250
最后 记住zL的维度

137
00:09:43,250 --> 00:09:45,255
如果我们要在某个例子中应用它

138
00:09:45,255 --> 00:09:48,255
假设zL的维度是这里的n^Lx1

139
00:09:48,255 --> 00:09:53,270
那么bL的维度也是这里的n^Lx1

140
00:09:53,270 --> 00:09:56,365
假设nL为L层中隐藏单元数

141
00:09:56,365 --> 00:10:00,230
那么Beta L和Gamma L的维度

142
00:10:00,230 --> 00:10:07,575
也会是n^Lx1 就等于隐藏单元数

143
00:10:07,575 --> 00:10:12,555
我们有nL隐藏单元数 那么Beta L和Gamma L被用于

144
00:10:12,555 --> 00:10:14,670
调整每一个隐藏单元的均值和方差

145
00:10:14,670 --> 00:10:19,195
这是由网络决定的

146
00:10:19,195 --> 00:10:21,990
好吧 让我们来汇总一下

147
00:10:21,990 --> 00:10:25,195
并且总结如何实现降梯度BN算法

148
00:10:25,195 --> 00:10:28,925
假设我们使用mini-batch梯度下降算法

149
00:10:28,925 --> 00:10:34,245
枚举从t等于1一直到mini-batch数

150
00:10:34,245 --> 00:10:39,265
在mini-batch (Xt) 上计算前向传播

151
00:10:39,265 --> 00:10:44,635
然后在每一层上计算前向传播

152
00:10:44,635 --> 00:10:50,330
使用BN算法

153
00:10:50,330 --> 00:10:57,265
用(带波浪号的)zL来代替zL<br />这表明 利用mini-batch算法

154
00:10:57,265 --> 00:11:02,810
z值将会进行归一化均值和方差处理

155
00:11:02,810 --> 00:11:09,200
这个经历归一化均值和方差的值就是(带波浪的)zL

156
00:11:09,200 --> 00:11:17,025
然后利用反向传播来计算dw

157
00:11:17,025 --> 00:11:20,065
dw db以及所有L层的值

158
00:11:20,065 --> 00:11:23,530
dBeta和dGamma

159
00:11:23,530 --> 00:11:26,805
尽管 技术上来讲 我们已经忽略了参数b

160
00:11:26,805 --> 00:11:28,494
但它并没有完全消除

161
00:11:28,494 --> 00:11:33,595
最终 我们更新参数

162
00:11:33,595 --> 00:11:40,085
所以 w更新为w减去学习率乘以dw

163
00:11:40,085 --> 00:11:47,775
Beta更新为Beta减去学习率乘以db

164
00:11:47,775 --> 00:11:49,595
对Gamma同样如此

165
00:11:49,595 --> 00:11:52,770
如果我们如下计算梯度

166
00:11:52,770 --> 00:11:54,805
这就是梯度下降算法

167
00:11:54,805 --> 00:11:56,910
就如我这里所写的

168
00:11:56,910 --> 00:12:01,845
但它同样适用于<br />Momentum或RMSprop或Adam的梯度下降算法

169
00:12:01,845 --> 00:12:07,200
但它同样适用于<br />Momentum或RMSprop或Adam的梯度下降算法

170
00:12:07,200 --> 00:12:08,890
它们不采用这种梯度下降更新算法

171
00:12:08,890 --> 00:12:11,220
而采用其它的更新算法

172
00:12:11,220 --> 00:12:16,615
这在我们上周视频中有所谈及

173
00:12:16,615 --> 00:12:19,790
这些其它的优化算法也可以被用来

174
00:12:19,790 --> 00:12:25,730
更新BN算法中的新增参数Beta和Gamma

175
00:12:25,730 --> 00:12:27,780
所以 我希望这能帮助你们

176
00:12:27,780 --> 00:12:30,375
从头实现BN算法

177
00:12:30,375 --> 00:12:31,530
如果你正在使用某种深度学习编程框架

178
00:12:31,530 --> 00:12:34,455
如果你正在使用某种深度学习编程框架<br />比如后续的课程中将要讨论的框架

179
00:12:34,455 --> 00:12:37,700
希望你能够调用编程框架中的方法

180
00:12:37,700 --> 00:12:41,720
来轻松实现BN算法

181
00:12:41,720 --> 00:12:45,515
现在 如果你还是对BN算法有些疑惑

182
00:12:45,515 --> 00:12:49,375
还是不理解 它为什么会有效提高训练速度

183
00:12:49,375 --> 00:12:52,140
让我们进入下一视频

184
00:12:52,140 --> 00:12:55,210
来探讨 为什么BN算法如此有效<br />以及它到底做了什么