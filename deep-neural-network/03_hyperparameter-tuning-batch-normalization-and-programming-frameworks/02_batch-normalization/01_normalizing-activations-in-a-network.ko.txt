딥러닝의 상승하는 시점에서, 가장 중요했던 아이디어 중 하나는 
정규화(Normailization) 이라는 알고리즘입니다. 이것은 2명의 개발자가 만들었는데요,
Sergey Ioffe 와 Christian Szegedy입니다. 배치 정규화는 여러분의
하이퍼 파라미터 서치 문제를 훨씬 더 쉽게 만들어 줍니다. 그리고 신경망을 더욱 튼튼하게 만들어주죠. 하이퍼 파라미터 의 선택은 잘 작동하는 훨씬 더 큰 범위의 하이퍼 파라미터인데요, 
여러분이 심층신경망에서도 더 쉽게 트레이닝 할 수 있도록 해줄 것입니다. 그러면 배치 정규화가 어떻게 작동하는지 한번 보겠습니다. 모델을 트레이닝 하는 경우, 
로지스텍 회귀분석이나 같은 경우에 말이죠, 이 경우 입력값의 특성을 일반화시키는 것이 속도를 높혀줄 수 있다는 기억하실 것입니다. 그러면 평균값을 구하고, 트레이닝세트에서 평균값을 빼고, 편차를 계산합니다. xi 제곱의 합이죠. 이것은 element-wise squaring입니다. 그리고 편차대로 데이터를 정규화 시켜줍니다. 그리고 절의는 러닝문제의 곡선을 아주 긴 모양에서 조금 더 동그란 모양으로 변형을 줄 수 있다는 것은
이전 비디오에서 봤는데요, 기울기 강화와 같은 알고리즘이 최적화하기 쉽게 말이죠. 이것은 입력값을 신경망으로 정규화 하는 것이나 로지스틱 회귀분석으로 하는 점에서 잘 작동합니다. 하지만 더 깊은 모델은 어떨까요? 입력값 x 만 있는것이 아니라
이 층에서는 a1 도 있습니다. 해당 층에서는 
a2 activation도 있고 말이죠. 그러면 여러분이 파라미터를 트레이닝 하고 싶은 경우엔, 
예를 들어, w3, b3과 같이 말이죠, 그러면 평균값과 a2의 variance를 정규화해서 w3 와 b3의 트레이닝을 더 효율적으로 만드는 것이
좋지 않을까요? 로지스틱 회귀분석 같은 경우엔, 
x1,x2,x3를 정규화 시켜줌으로써 w와 b를 더 효율적으로 트레이닝 시킬 수 있다는 것을 알았습니다. 여기서 문제는, 어떤 숨겨진 층에 대해, a의 값을, a2라고 하겠습니다. 
이 값을 정규화할 수 있는지가 문제입니다. 여기 예제에서는 
어떤 숨겨진 층에서도 이런데요, w3와 b3를 더 빨리 트레이닝 시키는 방법이겠죠. 맞죠? a2는 다음 층의 입력값이기 때문에, w3와 b3의 트레이닝에 영향을 줍니다. 그러면 이것이 배치 정규화가 하는 역할입니다. 보통 줄여서 batch norm이라고도 합니다. 엄밀히 이야기하면 a2가 아닌 z2를 정규화
시키는 것일텐데요, 딥러닝 학술에서 여러분이 activation 함수 이전에 z2죠, 정규화 해야 할지, a2 activation 함수를 적용한 다음에 
정규화를 해야 하는지에 대한 토론이 있습니다. 실제로는 z2를 정규화 하는 것이 훨씬 더 자주 있습니다. 그러므로 그 버전을 보여드리겠습니다. 그리고 이 값을 기본값으로 하도록
저는 추천드리겠습니다. 그러면 배치 정규화를 어떻게 도입하는지 보겠습니다. 여러분 신경망에서 어떤 중간값이 있는데요, 여러분이 어떤 hidden unit 값 z1에서, 
zm까지 있다고 해보겠습니다. 이것은 어떤 숨겨진 층에서 그런 것입니다. 그러므로 이것을 z로 쓰는 것이 조금 더 정확하겠죠, 어떤 숨겨진 층 i에 대해서 말이죠, 
i는 1에서 m까지가 되겠습니다. 하지만 적는 것을 줄이기 위해서, 
여기 [l]는 생략하겠습니다. 여기 라인에 있는 표기를 심플하게 하기 위해서 말이죠. 이런 값들이 주어지면, 이제 평균값을 아래와 같이 구합니다. 이것은 모두 l층에 맞춰진 것인데요, 
[l]은 생략하겠습니다. 그리고 variance의 값은 여러분이 예상하는 공식을 이용하여 계산하구요, 
zi들을 각각 이용해서 정규화합니다. 그러면 zi 정규화된값을 
평균값에서 빼고 표준편차로 나누어주면서 갖게되는데요, 숫자적으로 안정감을 주기위해서 
보통 앱실론을 분모에 더합니다. 여기 시그마 제곱과 같이 말이죠.
이것은 어떤 추정치의 경우 0이 되는 경우도 있습니다. 그럼 이제 저릐는 z의 값을 갖고, 평균값이 0, 그리고 standard unit variance를 갖가되게 
정규화 시켰습니다. 그럼 z의 모든 요소는 평균값 0 과 variance 1의 값을 
갖습니다. 그렇지만 저희는 숨겨진 유닛이 항상
평균값 0 과 variance 1의 값을 갖길 바라진 않죠. 숨겨진 유닛은 아마도 다른 분포도를 갖는 것이
더 말이 되겠죠. 그러므로 저희는 대신해서 이것을 zi 라고 부를텐데요, 
이것은 감마 zi 정규화 더하기 베타입니다. 그리고 여기 감마와 베타는 배우기가 가능한
파라미터입니다. 그러므로 저희는 gradient descent나 다른 알고리즘을
이용할 것입니다. 기울기 강하 와 모멘텀이나 RmsProp 또는 Adam을 이용해서
감마와 베타 파라미터r를 업데이트 할 것입니다. 신경망의 weight를 업데이트 하는 것과 마찬가지로 말이죠. 자 그럼 감마와 베타의 효과는 z tilde의 평균값이 여러분이 원하는 값으로 되게 해주는 것입니다. 만약 감마가 시그마 제곱 더하기 앱실론의 루트값인 경우, 즉 감마가 
여기 분모 항이라고 하면, 그리고 베타가 뮤, 
즉 여기 위의 값이라고 하면 감마 z 정규화 더하기 베타의 효과는 이 식을 거꾸로 하는 것과 일치합니다. 이것이 만약 맞다면, z tilde i 는 zi와 같을 것입니다. 그러면 감마와 베타 파라미터의 적합한 설정을 통해 여기 정규화 단계는, 여기 4개의 식은
identity 함수를 계산합니다. 하지만 감마와 베타값을 다르게 고르면서, 이것은 숨겨진 유닛이 다른 평균값과 variance를 갖게 해줍니다. 그러면 이것을 신경망에 fitting하는 방법은, 이전에는 z1, z2 등과 같은 값을 사용했는데요, 이제는
zi 대신에 z tilde i를 신경망에서 나중의 산출방식에 
사용하겠습니다. 그리고 [l]는 다시 넣습니다. 
어떤 층인지 정확히 표기하기 위해서 말이죠. 여기다가 넣으시면 됩니다. 여기서 얻어가실 직관적인 부분은 x의 입력값을 정규화함으로써
신경망의 러닝을 도와줄 수 있다는 것입니다. 그리고 batch norm이 하는 것은 
정규화 절차를 적용하여 입력 층에만이 아니라, 깊이 있는 숨겨진 층에도 적용합니다. 그러기 때문에 숨겨진 유닛 z에 대해서도 이러한 정규화를 통해 숨겨진 유닛의
평균값과 variance가 종규화되게 적용할 것입니다. 하지만 트레이닝 입력값과 숨겨진 유닛의 차이는 여러분은 숨겨진 유닛이 강제적으로 평균값 0과 variance 1이 되는 것을
원치 않을 수 있다는 것입니다. 예를 들어, 시그모이드 activation 함수의 경우, 여기 모든 값이 밀집되어 있는 것은 원치않습니다. 여러분은 이 것이 더 큰 variance나 평균값을 가져 0보다 다른 값을 가지면서 비선형의 시그모이드 함수가 나오길 바랄 것입니다. 
선형함수의 모양을 갖는 것보다 말이죠. 그러므로 파라미터 감마와 베타를 이용해서 여러분의 zi 값이 원하는 범위의 값을 갖도록 할 수 있는 것입니다. 하지만 실질적으로 하는 것은 숨겨진 유닛이 평균화 한 평균값과 variance값 을 갖고 감마와 베타와 같이 러닝 알고리즘이 설정할 수 있는 값들에 의해 조정됩니다. 그러므로 여기 숨겨진 유닛값의 평균값와 variance대로 정규화되는 것인데요, zi들이 고정된 평균값과 variance를 갖게끔 말이죠. 평균값과 variance는 각각 0과 1일 수 있구요, 또는 또 다른 값이 될 수도 있겠죠. 그런 이후, 여기 감마와 베타와 같은 값에 의해
조정됩니다. 이 내용이 여러분이 배치 정규화를 도입하는 
원리를 이해하는데 도움을 줬길 바랍니다. 적어고 신경망의 single layer에서의 경우 말이죠. 다음 비디오에서는 신경망에 배치 정규화를 피팅하는 방법을 배울 것입니다. 심층신경망에서도 말이죠. 그리하여 몇개의 다른 층의 신경망에서도 잘 작동시키는 방법을 익혀보겠습니다. 그 이후, 배치 정규화가 왜 신경망을 트레이닝 하는데 도움을 줄 수 있는지도 알아보겠습니다. 아직도 왜 작동하는지에 대한 부분에서 미지수인 부분이 남아있는데요, 2개 정도의 비디오 이후, 조금 더 많이 이해가 되실 것입니다.