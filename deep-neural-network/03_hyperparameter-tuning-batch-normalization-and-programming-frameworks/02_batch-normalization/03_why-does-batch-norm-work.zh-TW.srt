1
00:00:00,000 --> 00:00:02,415
那麼，為什麼批次標準化會有效呢？

2
00:00:02,415 --> 00:00:06,680
這邊是一個理由，你見過把輸入的特徵標準化

3
00:00:06,680 --> 00:00:09,380
也就是 X，標準化成平均為0變異數為1，

4
00:00:09,380 --> 00:00:10,740
這樣能加快訓練

5
00:00:10,740 --> 00:00:13,920
所以與其於有些特徵介在0到1之間，

6
00:00:13,920 --> 00:00:15,735
而有些介在1到1000，

7
00:00:15,735 --> 00:00:18,835
如果把所有的輸入特徵X標準化

8
00:00:18,835 --> 00:00:22,975
到某個類似的範圍，就能加快訓練

9
00:00:22,975 --> 00:00:25,770
所以，一個批次標準化能有效的直覺是

10
00:00:25,770 --> 00:00:27,750
這一樣在做類似的事

11
00:00:27,750 --> 00:00:32,770
只是是對隱藏單元，而不是你的輸入

12
00:00:32,770 --> 00:00:37,380
其實，從這角度看批次標準化還不夠全面

13
00:00:37,380 --> 00:00:39,180
還有其他更多的看法

14
00:00:39,180 --> 00:00:43,210
能夠幫你更了解批次標準化在做什麼

15
00:00:43,210 --> 00:00:46,320
就讓我們在這部影片一窺究竟

16
00:00:46,320 --> 00:00:48,490
批次標準化有用的第二個理由

17
00:00:48,490 --> 00:00:50,295
是讓你的神經網路

18
00:00:50,295 --> 00:00:52,575
比較後面的權重

19
00:00:52,575 --> 00:00:56,095
例如說第十層，能比較不受

20
00:00:56,095 --> 00:01:00,300
網路比較前面的權重，例如第一層，的改變所影響

21
00:01:00,300 --> 00:01:01,810
為了解釋我說的

22
00:01:01,810 --> 00:01:04,775
讓我們看這實際的例子

23
00:01:04,775 --> 00:01:06,420
看看神經網路的訓練

24
00:01:06,420 --> 00:01:07,770
可能是個淺的網路

25
00:01:07,770 --> 00:01:11,715
例如羅吉斯迴歸分析，或者是神經網路

26
00:01:11,715 --> 00:01:17,260
可能是像羅吉斯迴歸那樣淺的網路，也可能是深的網路

27
00:01:17,260 --> 00:01:21,120
例如我們有名的貓貓辨認器

28
00:01:21,120 --> 00:01:26,745
但是，假設你的訓練資料都是黑貓

29
00:01:26,745 --> 00:01:31,240
如果你現在想把這個網路套用到

30
00:01:31,240 --> 00:01:33,325
其他顏色的貓的資料

31
00:01:33,325 --> 00:01:36,895
也就是正面資料不光是像左邊這些黑貓

32
00:01:36,895 --> 00:01:40,442
也有像右邊這樣有色的貓

33
00:01:40,442 --> 00:01:43,160
那麼，你的分類器可能不會表現得好

34
00:01:43,160 --> 00:01:47,907
用圖來表示，如果你的訓練資料像這樣

35
00:01:47,907 --> 00:01:52,383
這邊有些正面資料，還有些反面資料在這

36
00:01:52,383 --> 00:01:54,970
而你想要一般化至

37
00:01:54,970 --> 00:02:02,335
某些資料，其中正面的資料在這，而反面的資料在這

38
00:02:02,335 --> 00:02:05,800
那你大概不能預期用左邊資料

39
00:02:05,800 --> 00:02:09,430
訓練的模型能在右邊的資料上表現好

40
00:02:09,430 --> 00:02:13,901
儘管可能真的存在一個函數，能在兩邊都表現好

41
00:02:13,901 --> 00:02:19,230
但你不能期待你的學習演算法
能發掘這條綠色的分界線

42
00:02:19,230 --> 00:02:21,500
如果只能靠左邊的資料

43
00:02:21,500 --> 00:02:26,410
所以，這種資料分布改變的觀念

44
00:02:26,410 --> 00:02:31,657
有個有點酷的名字：共變量偏移(covariate shift)

45
00:02:31,657 --> 00:02:33,695
這觀念是這樣的：

46
00:02:33,695 --> 00:02:35,910
假設你學到某個 X 到 Y 的對應

47
00:02:35,910 --> 00:02:38,435
如果 X 的分佈改變

48
00:02:38,435 --> 00:02:41,545
你有可能就需要重新訓練你的演算法。

49
00:02:41,545 --> 00:02:43,925
即便是下面的情況，這也是真的：

50
00:02:43,925 --> 00:02:45,230
就算關於事實的函數，也就是

51
00:02:45,230 --> 00:02:46,775
X到Y的對應不改變，

52
00:02:46,775 --> 00:02:49,430
這觀念仍然是對的。在這例子就是如此

53
00:02:49,430 --> 00:02:51,630
因為這邊關於事實的函數

54
00:02:51,630 --> 00:02:53,990
是「這張圖片是否為貓」。

55
00:02:53,990 --> 00:02:57,590
而重新訓練的需要會更加

56
00:02:57,590 --> 00:03:03,510
迫切，或可說是更加嚴重－如果背後事實的函數也變動

57
00:03:03,510 --> 00:03:08,720
那麼，共變量偏移這個問題，和神經網路有什麼關係？

58
00:03:08,720 --> 00:03:10,880
看看上面這個很深的網路

59
00:03:10,880 --> 00:03:15,225
我們從這一個隱藏層的角度，來看學習的過程

60
00:03:15,225 --> 00:03:16,995
這個第三層

61
00:03:16,995 --> 00:03:22,145
所以這個網路目前學到了參數W[3]和b[3]

62
00:03:22,145 --> 00:03:24,860
從第三層的角度來看

63
00:03:24,860 --> 00:03:27,665
他從前面那些層拿到了一些值

64
00:03:27,665 --> 00:03:30,020
然後必須做些事，希望

65
00:03:30,020 --> 00:03:34,305
讓輸出的 y hat 和事實的 y 很相近

66
00:03:34,305 --> 00:03:38,340
那讓我先把左半部遮住一下

67
00:03:38,340 --> 00:03:41,785
所以從第三個隱藏層的角度來說

68
00:03:41,785 --> 00:03:44,265
他得到一些值

69
00:03:44,265 --> 00:03:48,520
假設叫 a[2]_1, a[2]_2, 

70
00:03:48,520 --> 00:03:57,102
a[2]_3, a[2]_4。但是這些值也可看作是特徵 x_1, x_2,

71
00:03:57,102 --> 00:04:02,240
x_3, x_4，而這第三個隱藏層的作用是

72
00:04:02,240 --> 00:04:08,225
拿這些值，想辦法把他們對應到 y hat

73
00:04:08,225 --> 00:04:10,760
所以你可以想像在梯度下降的時候

74
00:04:10,760 --> 00:04:17,525
這些參數 W[3], b[3]，還有 W[4], b[4]

75
00:04:17,525 --> 00:04:21,800
甚至 W[5], b[5]，這些參數會去學習

76
00:04:21,800 --> 00:04:23,360
讓這個網路能表現好

77
00:04:23,360 --> 00:04:29,460
把左邊我用黑色圈起來的這些值，對應到輸出 y hat

78
00:04:29,460 --> 00:04:33,470
不過呢，我們現在把左半部遮掉的拿回來

79
00:04:33,470 --> 00:04:42,226
這個網路也含有參數 W[2], b[2] 和 W[1], b[1]

80
00:04:42,226 --> 00:04:45,305
所以當這些參數改變的時候

81
00:04:45,305 --> 00:04:49,795
這些值，a[2]，也會跟著變

82
00:04:49,795 --> 00:04:53,080
所以從第三層的角度來看

83
00:04:53,080 --> 00:04:56,735
這些隱藏單元的值一直不斷地在更改

84
00:04:56,735 --> 00:04:59,090
所以這會遭受到前張投影片

85
00:04:59,090 --> 00:05:02,435
共變量偏移的問題

86
00:05:02,435 --> 00:05:04,115
因此，批次標準化所做的

87
00:05:04,115 --> 00:05:10,730
是減少這些隱藏單元的分佈變動的幅度

88
00:05:10,730 --> 00:05:14,825
如果我們把這些隱藏單元的分佈畫出來

89
00:05:14,825 --> 00:05:17,948
技術上我們是標準化 Z

90
00:05:17,948 --> 00:05:24,970
所以這邊是 z[2]_1 和 z[2]_2

91
00:05:24,970 --> 00:05:27,862
還有我只畫了兩個值，而不是四個

92
00:05:27,862 --> 00:05:29,670
這樣在 2D 上可以看到

93
00:05:29,670 --> 00:05:32,015
批次標準化告訴我們的是

94
00:05:32,015 --> 00:05:35,745
z[2]_1 和 z[2]_2 的值可以改變

95
00:05:35,745 --> 00:05:37,514
而且他們真的會變，因為神經網路

96
00:05:37,514 --> 00:05:41,215
會更新前面幾層的參數

97
00:05:41,215 --> 00:05:44,930
但批次標準化會保證無論怎麼變

98
00:05:44,930 --> 00:05:55,050
z[2]_1 和 z[2]_2 的平均和變異數還是會一樣

99
00:05:55,050 --> 00:05:59,900
所以就算 z[2]_1 和 z[2]_2 的值變了

100
00:05:59,900 --> 00:06:07,115
他們的平均和變異數至少仍然是0和1

101
00:06:07,115 --> 00:06:09,940
或者說，其實不見得是平均0變異數1

102
00:06:09,940 --> 00:06:17,295
而是由 beta[2] 和 gamma[2] 控制的值

103
00:06:17,295 --> 00:06:19,228
如果是這神經網路所選擇的

104
00:06:19,228 --> 00:06:22,270
可以讓平均變 0，變異數變 1

105
00:06:22,270 --> 00:06:24,655
也可以是其他任何的值。

106
00:06:24,655 --> 00:06:26,305
不過，這邊真正所做的

107
00:06:26,305 --> 00:06:32,290
是他限制了前面幾層更新參數時

108
00:06:32,290 --> 00:06:35,110
對後面值的分佈的影響幅度

109
00:06:35,110 --> 00:06:38,790
也就是第三層看到的、所要學的值。

110
00:06:38,790 --> 00:06:44,370
因此，批次標準化減輕了輸入變化時帶來的問題

111
00:06:44,370 --> 00:06:48,895
他真的讓這些值更穩定

112
00:06:48,895 --> 00:06:55,155
使這網路後面的層能有更堅固的地基。

113
00:06:55,155 --> 00:06:57,655
就算輸入的分佈有變

114
00:06:57,655 --> 00:07:00,580
變的也比較少；這樣的意思是，

115
00:07:00,580 --> 00:07:03,325
就算前面的層不斷地學習

116
00:07:03,325 --> 00:07:06,640
這種「迫使後面的層盡早適應」

117
00:07:06,640 --> 00:07:10,180
這樣子的症狀其實會減輕。

118
00:07:10,180 --> 00:07:12,925
也可以這麼說，就是他減弱了

119
00:07:12,925 --> 00:07:15,445
前面那些層的行為

120
00:07:15,445 --> 00:07:18,020
和後面那些層行為之間的羈絆。

121
00:07:18,020 --> 00:07:22,147
所以，這讓每一層能自我學習，

122
00:07:22,147 --> 00:07:25,210
稍微比較不依靠其他層

123
00:07:25,210 --> 00:07:29,145
如此一來，這會加快整個網路的訓練。

124
00:07:29,145 --> 00:07:32,161
我希望這給了你更好的概念

125
00:07:32,161 --> 00:07:35,620
不過重點在於，批次標準化意味著

126
00:07:35,620 --> 00:07:39,010
特別是從神經網路比較後面的層的角度來說

127
00:07:39,010 --> 00:07:43,090
前面的層變動不會太大

128
00:07:43,090 --> 00:07:46,320
因為他們被限制在相同的平均和變異數

129
00:07:46,320 --> 00:07:50,260
這讓後面隱藏層的學習比較容易

130
00:07:50,260 --> 00:07:52,669
實際上，批次標準化有第二種功效

131
00:07:52,669 --> 00:07:55,940
他可達到稍微的正規化

132
00:07:55,940 --> 00:07:59,885
一個不大直覺的事情是，每個小量資料

133
00:07:59,885 --> 00:08:02,090
假設某批小量資料 Xt

134
00:08:02,090 --> 00:08:04,660
含有值 z[t] (口誤)

135
00:08:04,660 --> 00:08:07,225
含有值 z[l]

136
00:08:07,225 --> 00:08:12,730
標準化時，只看那一小批資料的平均和變異數

137
00:08:12,730 --> 00:08:15,895
那，因為平均和變異數是從

138
00:08:15,895 --> 00:08:20,245
小量資料算出，而不是整個資料集的，

139
00:08:20,245 --> 00:08:22,960
那樣的平均和變異數會有些噪音

140
00:08:22,960 --> 00:08:25,540
因為這只是從你的小批資料算出

141
00:08:25,540 --> 00:08:28,145
可能只有64筆，或128筆

142
00:08:28,145 --> 00:08:32,335
或是256筆或多一點的訓練資料。

143
00:08:32,335 --> 00:08:35,935
所以因為平均和變異數有點噪音
－因為是從

144
00:08:35,935 --> 00:08:40,195
比較少數的資料估計而出 － 那麼

145
00:08:40,195 --> 00:08:43,363
從 z[l] 到 z tilde[l] 這樣的縮放

146
00:08:43,363 --> 00:08:46,135
這個過程也會有一點噪音

147
00:08:46,135 --> 00:08:51,420
因為這是從有些噪音的平均和變異數所算出的

148
00:08:51,420 --> 00:08:54,817
所以和 Dropout 類似，

149
00:08:54,817 --> 00:08:57,980
他會加一些噪音到每個隱藏層的啟動值。

150
00:08:57,980 --> 00:08:59,905
Dropout 能提供噪音

151
00:08:59,905 --> 00:09:04,180
是因為他會有一定機率把一個隱藏單元乘以 0

152
00:09:04,180 --> 00:09:06,870
和有一定機率乘以 1

153
00:09:06,870 --> 00:09:12,350
所以 Dropout 會有多個噪音，因為他會乘以 0 或是 1，

154
00:09:12,350 --> 00:09:18,360
而批次標準化會有多個噪音，因為他會依標準差縮放，

155
00:09:18,360 --> 00:09:21,655
還有加法的噪音，因為他會去減平均，

156
00:09:21,655 --> 00:09:25,825
而其中，對於平均和標準差的估計是有噪音的。

157
00:09:25,825 --> 00:09:29,785
因此，和Dropout類似，

158
00:09:29,785 --> 00:09:33,220
批次標準化也有一點正規化的功效。

159
00:09:33,220 --> 00:09:35,435
這是因為，如果把隱藏單元加上噪音，

160
00:09:35,435 --> 00:09:41,280
這會強迫之後的隱藏單元不能太依賴
任何一個隱藏單元。

161
00:09:41,280 --> 00:09:43,025
因此，和Dropout類似，

162
00:09:43,025 --> 00:09:47,620
他加了一些噪音在隱藏層上，所以會有些許的正規化效果。

163
00:09:47,620 --> 00:09:50,064
而因為加的噪音很小，

164
00:09:50,064 --> 00:09:52,572
所以正規化的效果不是很大，

165
00:09:52,572 --> 00:09:56,760
你可以選擇同時利用批次標準化和Dropout，

166
00:09:56,760 --> 00:09:59,880
你可以選擇同時利用批次標準化和Dropout—

167
00:09:59,880 --> 00:10:03,060
如果你想要比Dropout更強的正規化效果。

168
00:10:03,060 --> 00:10:06,195
此外，可能還有一個不直覺的效果是，

169
00:10:06,195 --> 00:10:08,454
如果你的一小批次的資料比較多

170
00:10:08,454 --> 00:10:11,200
假設你的小量批次(mini-batch)的大小是

171
00:10:11,200 --> 00:10:13,725
512 而不是 64，

172
00:10:13,725 --> 00:10:15,934
當這小批次比較大的時候

173
00:10:15,934 --> 00:10:20,940
你會減少噪音，所以也會減小正規化的效果。

174
00:10:20,940 --> 00:10:24,030
所以這是Dropout一個神奇的特點

175
00:10:24,030 --> 00:10:27,435
也就是小量批次比較大的時候，

176
00:10:27,435 --> 00:10:29,870
你會減小正規化的效果。

177
00:10:29,870 --> 00:10:33,833
話雖如此，我不會真的把批次標準化
當作一個正規化的工具

178
00:10:33,833 --> 00:10:36,625
這不是批次標準化想做的

179
00:10:36,625 --> 00:10:44,250
不過有時他還是有意或無意地，
在學習時產生這樣的效果

180
00:10:44,250 --> 00:10:48,390
不過真的，不要把批次標準化看作是正規化

181
00:10:48,390 --> 00:10:50,070
把他當作一種手段來做標準化

182
00:10:50,070 --> 00:10:53,770
你的隱藏單元的啟動值，來加速學習

183
00:10:53,770 --> 00:10:57,900
我想隨之而來的正規化只是無意的副作用罷了

184
00:10:57,900 --> 00:11:02,430
那麼，我希望這能讓你對批次標準化做的事更有概念

185
00:11:02,430 --> 00:11:04,580
在結束批次標準化的討論之前，

186
00:11:04,580 --> 00:11:06,855
我還想確定你知道另一個細節

187
00:11:06,855 --> 00:11:11,254
也就是，批次標準化一次處理一小批的資料

188
00:11:11,254 --> 00:11:14,520
他計算的是一批資料的平均和變異數。

189
00:11:14,520 --> 00:11:15,720
那麼在測試的時候，

190
00:11:15,720 --> 00:11:18,150
你做了一個預估，想評估這個神經網路

191
00:11:18,150 --> 00:11:20,400
你不一定會有一批一批的資料

192
00:11:20,400 --> 00:11:24,035
你可能一次只處理一筆資料

193
00:11:24,035 --> 00:11:26,400
所以在測試階段，你必須要做些

194
00:11:26,400 --> 00:11:29,430
稍微不一樣的事，確保你的預測有道理

195
00:11:29,430 --> 00:11:32,197
在下一部，也是最後一部關於批次標準化的影片，

196
00:11:32,197 --> 00:11:35,490
讓我們談談你必須做的那些細節，使得

197
00:11:35,490 --> 00:11:39,290
你用批次標準化訓練出的神經網路，也能夠拿來預估