1
00:00:00,320 --> 00:00:01,820
在深度學習的崛起中

2
00:00:01,820 --> 00:00:06,460
最重要的構想之一是一個演算法，叫做
批次標準化(Batch Normalization)

3
00:00:06,460 --> 00:00:10,860
由Sergey Ioffe和Christian Szegedy這兩位研究者提出

4
00:00:10,860 --> 00:00:14,096
批次標準化讓你尋找超參數更加簡單

5
00:00:14,096 --> 00:00:15,970
讓你的神經網路能

6
00:00:15,970 --> 00:00:20,492
比較不受超參數選擇的影響，
會有更大範圍的超參數能表現得好

7
00:00:20,492 --> 00:00:25,029
就算是很深的網路，也能讓你簡單地訓練之

8
00:00:25,029 --> 00:00:27,850
就讓我們看看批次標準化是怎麼運作的

9
00:00:27,850 --> 00:00:32,860
當訓練一個模型，例如羅吉斯迴歸分析，
你可能還記得

10
00:00:32,860 --> 00:00:37,990
把輸入的特徵標準化，能夠加速學習。
你算出平均，

11
00:00:37,990 --> 00:00:40,680
把你的訓練資料減掉平均

12
00:00:40,680 --> 00:00:42,150
算出變異數

13
00:00:44,320 --> 00:00:46,090
這個 X(i) 平方和

14
00:00:46,090 --> 00:00:48,140
這邊的平方是逐個元素的

15
00:00:49,990 --> 00:00:53,160
然後根據這些變異數來標準化你的資料。

16
00:00:53,160 --> 00:00:57,699
而在先前的影片我們看到，這可以把

17
00:00:57,699 --> 00:01:02,887
你的問題的等高線
從很狹長的形狀變得比較圓

18
00:01:02,887 --> 00:01:07,240
讓像是梯度下降法這樣的演算法
更容易去最佳化。

19
00:01:07,240 --> 00:01:12,130
所以，對於神經網路或羅吉斯迴歸的「輸入值」

20
00:01:12,130 --> 00:01:15,530
這樣的標準化有效

21
00:01:15,530 --> 00:01:17,810
那麼，深一點的模型呢？

22
00:01:17,810 --> 00:01:22,998
你不只有輸入特徵 x，
在這一層你有啟動值 a[1]

23
00:01:22,998 --> 00:01:27,210
在這一層你有啟動值 a[2]，依此類推

24
00:01:27,210 --> 00:01:31,220
所以如果你要訓練參數，例如 W[3], b[3]

25
00:01:32,600 --> 00:01:36,900
如果你能把 a[2] 的平均和變異數給標準化

26
00:01:36,900 --> 00:01:41,330
讓 W[3], b[3] 的訓練更有效率，這不是很好嗎

27
00:01:43,140 --> 00:01:46,960
在羅吉斯迴歸分析的例子，我們看到標準化 x1,

28
00:01:46,960 --> 00:01:51,460
x2, x3 可能讓訓練 W 和 b 更有效率

29
00:01:51,460 --> 00:01:56,060
那麼問題來了：對於任何一個隱藏層，
我們能否標準化

30
00:01:57,980 --> 00:02:02,143
a 的值 — 假設在這例子是 a[2]

31
00:02:02,143 --> 00:02:07,796
雖然可以是任何一個隱藏層 —

32
00:02:07,796 --> 00:02:12,580
來讓訓練 W[3], b[3] 變得更快呢？

33
00:02:12,580 --> 00:02:15,283
因為 a[2] 是下一層的輸入，

34
00:02:15,283 --> 00:02:18,870
因此影響到 W[3] 和 b[3] 的訓練

35
00:02:20,390 --> 00:02:24,418
所以這個就是批次標準化，
簡稱 Batch Norm，

36
00:02:24,418 --> 00:02:25,287
所做的。

37
00:02:25,287 --> 00:02:31,730
雖然技術上來說，我們是標準化 Z[2] 的值
而不是 a[2]

38
00:02:31,730 --> 00:02:36,030
在深度學習的文獻中，這邊有些爭論：

39
00:02:36,030 --> 00:02:40,760
你要在啟動函數之前，
也就是 Z[2]，先做標準化呢？

40
00:02:40,760 --> 00:02:45,000
還是過了啟動函數的 a[2] 做標準化呢？

41
00:02:45,000 --> 00:02:48,655
實務上，對Z[2]標準化比較常見

42
00:02:48,655 --> 00:02:51,253
所以我講的是這個版本

43
00:02:51,253 --> 00:02:54,550
我建議這是你的第一選擇。

44
00:02:54,550 --> 00:02:58,060
那麼，接下來是實作批次標準化的方法

45
00:02:58,060 --> 00:03:06,070
給你神經網路一些中間的值

46
00:03:09,470 --> 00:03:15,270
假設你有一些隱藏單元的值

47
00:03:15,270 --> 00:03:19,365
 z(1) 到 z(m) — 其實這是

48
00:03:19,365 --> 00:03:23,686
從某個隱藏層來的，所以更精確的說，

49
00:03:23,686 --> 00:03:28,750
這要寫做z在某個隱藏層，然後 i=1~m。

50
00:03:28,750 --> 00:03:33,110
不過方便起見，我把這個方括號 [l] 省略掉，

51
00:03:33,110 --> 00:03:35,350
讓符號變簡單。

52
00:03:35,350 --> 00:03:41,260
所以有了這些值，你要做的是這樣子計算平均

53
00:03:41,260 --> 00:03:46,277
— 再說一次，這裡都是對於某一層 l 而言，
只不過我省略了 [l] —

54
00:03:46,277 --> 00:03:51,153
然後你用你知道的公式來計算出變異數

55
00:03:51,153 --> 00:03:56,043
然後你對於每一個 Z(i) 去標準化

56
00:03:56,043 --> 00:04:00,908
所以你會得到 Z(i) norm 等於減掉平均，

57
00:04:00,908 --> 00:04:04,310
然後除以標準差。

58
00:04:04,310 --> 00:04:09,312
為了數值運算的穩定性，
通常你都會加 epsilon 到分母

59
00:04:09,312 --> 00:04:14,460
以免萬一sigma平方在某些情況是零。

60
00:04:14,460 --> 00:04:17,405
所以我們拿了這些Z的值，

61
00:04:17,405 --> 00:04:23,010
做了標準化，讓他們平均為0，變異數為1

62
00:04:23,010 --> 00:04:25,903
也就是Z的每個維度都是平均0，變異數1

63
00:04:25,903 --> 00:04:31,352
不過我們不希望這些隱藏單元
平均總是0，變異數總是1

64
00:04:31,352 --> 00:04:38,953
或許不同的隱藏單元會有不同的分布，這也合理。
所以我們真正要做的，是計算

65
00:04:38,953 --> 00:04:42,939
z tilde = 

66
00:04:42,939 --> 00:04:48,434
gamma * z(i) norm + beta

67
00:04:48,434 --> 00:04:55,210
這邊 gamma 和 beta 是模型中待學的參數

68
00:04:58,957 --> 00:05:03,675
所以我們可用梯度下降法，或是其他演算法像是

69
00:05:03,675 --> 00:05:08,136
動量法、RMSprop、或 Adam 優化法，
你就能更新參數 gamma 和 beta

70
00:05:08,136 --> 00:05:11,410
就像你更新神經網路的權重一樣

71
00:05:11,410 --> 00:05:16,582
那麼，注意到這邊 gamma 和 beta 的效果是

72
00:05:16,582 --> 00:05:22,140
他能讓 Z tilde 的平均變成任何你想要的。

73
00:05:22,140 --> 00:05:27,720
實際上，如果 gamma 等於 sigma^2 + epsilon 開根號，

74
00:05:28,800 --> 00:05:33,570
萬一 gamma 等於這個分母，

75
00:05:33,570 --> 00:05:39,318
而且萬一 beta 等於上面這個 mu，

76
00:05:39,318 --> 00:05:43,998
那麼

77
00:05:43,998 --> 00:05:49,540
gamma * Z_norm + beta 的效果
會抵銷掉上面這個方程式

78
00:05:49,540 --> 00:05:52,284
所以，萬一這是真的

79
00:05:52,284 --> 00:05:57,780
那麼 Z(i) tilde 會等於 Z(i)

80
00:05:57,780 --> 00:06:02,633
所以當參數 gamma 和 beta 在某種恰巧的設定下，

81
00:06:02,633 --> 00:06:05,321
這整個標準化的步驟，也就是

82
00:06:05,321 --> 00:06:11,175
這邊四個方程式，本質上就是一個恆等函數
(identity function)

83
00:06:11,175 --> 00:06:16,112
不過如果 gamma 和 beta 取不同的值，這就會讓

84
00:06:16,112 --> 00:06:19,320
隱藏單元有不同的平均和變異數。

85
00:06:19,320 --> 00:06:23,538
所以要把這套用到你的神經網路的方法是

86
00:06:23,538 --> 00:06:28,583
原本之前用 Z(1), Z(2), ... 的情況，

87
00:06:28,583 --> 00:06:35,195
你現在轉而用 Z(i) tilde，而不是 Z(i)

88
00:06:35,195 --> 00:06:39,910
來給你的神經網路做後面的運算

89
00:06:39,910 --> 00:06:45,129
然後你會把這個 [l] 擺回去，指明是哪一層

90
00:06:45,129 --> 00:06:46,910
你可以擺回去那。

91
00:06:46,910 --> 00:06:51,319
我希望你能學到的概念是，我們之前看過

92
00:06:51,319 --> 00:06:56,140
把輸入特徵X標準化，
對神經網路的學習有所幫助

93
00:06:56,140 --> 00:07:00,029
而批次標準化所作的，
是把標準化的程序

94
00:07:00,029 --> 00:07:01,283
運用在不只是輸入層上，

95
00:07:01,283 --> 00:07:04,810
而是還更深入神經網路的隱藏層。

96
00:07:04,810 --> 00:07:09,085
所以你套用這樣的標準化步驟，來標準化

97
00:07:09,085 --> 00:07:12,390
某個隱藏層的值 Z 的平均和變異數。

98
00:07:12,390 --> 00:07:16,833
但是，對於輸入和隱藏層，這兩者不同之處在

99
00:07:16,833 --> 00:07:21,220
你可能不希望隱藏層的值被迫平均為0，變異數為1

100
00:07:21,220 --> 00:07:24,247
舉個例子，如果你的啟動函數是S型函數

101
00:07:24,247 --> 00:07:27,230
你並不希望你的值總是擠在這裡

102
00:07:27,230 --> 00:07:31,582
你可能希望變異數大一些，或平均非零，

103
00:07:31,582 --> 00:07:35,322
以善用S型函數的非線性區，

104
00:07:35,322 --> 00:07:41,060
而非讓你的值都落在這線性區間。

105
00:07:41,060 --> 00:07:45,067
這就是為何有了gamma和beta這兩個參數後

106
00:07:45,067 --> 00:07:51,230
你能確保你的 Z(i) 會有你想要的範圍

107
00:07:51,230 --> 00:07:55,671
或者說，實際上這是確保隱藏層

108
00:07:55,671 --> 00:07:59,226
能有規範好的平均和變異數

109
00:07:59,226 --> 00:08:03,429
其中平均及變異數是由兩個參數所控制：

110
00:08:03,429 --> 00:08:07,826
gamma和beta，而這是由學習演算法自己設定的

111
00:08:07,826 --> 00:08:13,004
所以，這真正在做的是
標準化這些隱藏層的平均和變異數

112
00:08:13,004 --> 00:08:18,660
也就是Z(i)的，讓他們有個固定的平均和變異數

113
00:08:18,660 --> 00:08:22,320
而這個平均和變異數，可能是0與1，
也有可能是其他的值

114
00:08:22,320 --> 00:08:26,680
由參數gamma和beta所控制之。

115
00:08:26,680 --> 00:08:30,424
我希望這能給你實作批次標準化的概念

116
00:08:30,424 --> 00:08:32,830
至少，對於神經網路內單一的一層

117
00:08:32,830 --> 00:08:36,104
在下部影片，我想示範在神經網路上套用批次標準化

118
00:08:36,104 --> 00:08:39,052
甚至是很深的網路，

119
00:08:39,052 --> 00:08:41,700
還有怎麼用在網路的很多層。

120
00:08:41,700 --> 00:08:45,450
接下來，我們會講更多的概念：

121
00:08:45,450 --> 00:08:47,120
為什麼批次標準化對訓練神經網路有幫助

122
00:08:47,120 --> 00:08:51,424
萬一你覺得這背後原理還很撲朔迷離，
跟著我

123
00:08:51,424 --> 00:08:54,949
在接下來的兩部影片，我們會一一釐清