在深度学习不断兴起的过程中 最重要的创新之一是一种
叫批量归一化 (Batch Normalization) 的算法 它由Sergey Ioffe 和 Christian Szegedy提出 可以让你的超参搜索变得很简单 让你的神经网络变得更加具有鲁棒性 可以让你的神经网络对于超参数的选择上不再那么敏感 而且可以让你更容易地训练非常深的网络 让我们来看看批量归一化是如何工作的 你也许记得当训练一个模型比如说逻辑回归模型时 对输入特征进行归一化可以加速学习过程 这意味着 计算平均值 用训练集减去平均值 计算方差 求x[i]的平方和 这是按元素相乘的平方 然后用方差来归一化你的数据集 在前面我们讲过这是如何让你的机器学习问题 的图形从扁的变得更圆些 这会便于类似梯度下降的算法去做优化 所以对于神经网络或者逻辑回归 针对输入值的归一化处理是有用的 那么对于层数更多的模型呢？ 你不仅有输入特征值x 在这一层你还有激活函数的输出结果 a[1] 在这一层还有激活函数的输出结果 a[2] 等等 所以如果你想训练这些参数 w[3] b[3] 除非你能对a[2]进行标准差归一化 否则对于 w[3] b[3]的训练都不会太有效率 在逻辑回归的例子里 我们看到了对x1 x2 x3做归一化可以对对w和b的训练更有效 所以这里的问题就是 对于任何一个隐藏层我们是否也可以 对a的值做归一化呢 比如拿a2举例 其实可以是任何一个隐藏层 是否可以让w3 b3的训练更快呢 因为a2是下一层的输入 它影响着你对w[3] b[3]的训练 这就是batch norm 简单的解释 虽然在实际中我们归一化时针对的
并不是a[2]而是z[2] 对于在激活函数之前做归一化，也就是指z[2] 还是在激活函数之后做归一化，也就是指a[2] 这一点上学术界还是有一些争议的 实际中 对z[2]做归一化要普遍的多 也就是这里所呈现的方法 我也推荐你把它作为默认的方法 下面介绍如何实现batch norm 假使有一些神经网络中的中间值 隐藏单元z(1)到z(m) 这些来源于某一个隐藏层 所以更准确的写法 是加上层数的上标l 但为了简化, 我要省略这个 [l], 像这一行这么写 当给你这些值时 你要做的是像这样计算平均值 所有这些都是针对某一层 但是我省略了这里的[l] 然后再用你喜欢公式计算方差 然后针对每个z(i)去归一化 就是把值减去均值 然后除以标准差 为了避免在某些情况下分母为0 通常你需要在这里加上ε 这样我们就把z归一化为 一组均值0方差1的值了 每一组z都是均值0方差1 但是我们并不希望所有的隐藏单元都是这样的 也许本身它们的分布就有不同 所以我们可以这么做 z tilde = γ * zi * norm + β 这里的γ和β值可以从你的模型中学习 这样我们就可以使用梯度下降算法 或者其他类似算法比如 momentum的梯度下降算法 或者atom算法 来更新γ和β的值 就像更新神经网络的权重一样 事实上 如果γ等于δ^2+ε的平方根 让z波浪线的均值可以是任何数 实际上 如果gamma等于它 这时候这里的z波浪线就等于z_norm 如果beta等于mu 像这个值 这使得gamma z norm + beta 等于 通过选择不同的γ和β
可以让隐藏单元的呈现不同的分布 像之前对z[1] z[2]……的处理一样 那么实际上 z tilde i = zi 所以通过恰当的设置gamma和beta 归一化就是 这四步确定的方程 但是通过选择不同gamma和beta的值 它可以让你的隐藏层 或者 可以是其它任何均值和方差 把这方法运用到你的神经网络就是 在你之前使用这些值z1 z2 等等时 你换成z tilde i 代替 zi 再进行下面的计算 如果你想更精确 可以把[l]再添进去 我希望你从这里学到的是 如何对输入特征X做归一化来帮助神经网络的训练 以及batch norm所做的就是 不仅仅在输入层 而且在一些隐藏层上也做归一化 你使用这种归一化方法 对某些隐藏单元的值z做归一化 但是输入层和隐藏层的归一化还有一点不同 就是隐藏层归一化后并不一定是均值0方差1 比如 如果你的激活函数是sigmoid 你就不希望归一化后的值都聚集在这里 可能希望它们有更大的方差 以便于更好的利用s函数非线性的特性 而不是所有的值都在中间这段近似直线的区域上 这就是为什么通过设置γ和β 你可以控制z(i)在你希望的范围内 或者说它真正实现的是 通过两个参数γ和β 来让你的隐藏单元有可控的放差和均值 而这两个参数是可以在算法中自由设置的 目的就是 可以得到一些修正的均值和方差 这意味可以是均值0方差1 也可以是被参数γ β控制的其他值 希望这一节你对如何实现batch norm有了初步的了解 至少是在单层神经网络上 下一节我将向你展示如何在神经网络中使用batch norm 包括深度神经网络
以及如何用数据训练它 以及如何使它在不同的层都能起到作用 在那之后我们会告诉你一些直观上的理解 为什么batch norm可以帮助你训练神经网络 所以如果到这里还觉得有些困惑 请坚持下去 我想下面两节会让你理解它