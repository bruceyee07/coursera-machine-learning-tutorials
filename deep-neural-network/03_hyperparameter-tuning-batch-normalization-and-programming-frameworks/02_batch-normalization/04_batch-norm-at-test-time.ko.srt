1
00:00:00,000 --> 00:00:03,685
배치 정규화는 여러분의 데이터를 한번에 조금한 집단별로 처리합니다.

2
00:00:03,685 --> 00:00:07,760
하지만 테스트타임은 여러분이 샘플별로 하나씩 처리해야할 수 있습니다.

3
00:00:07,760 --> 00:00:10,585
자 그럼 이렇게 하기 위해서 여러분의 네트워크를 어떻게 변경시킬 수 있는지 한번 보겠습니다.

4
00:00:10,585 --> 00:00:12,233
트레이닝에서 기억하시겠지만,

5
00:00:12,233 --> 00:00:15,260
배치 정규화를 도입하기 위해 사용하는 공식들입니다.

6
00:00:15,260 --> 00:00:17,025
싱글 미니 배치안에서,

7
00:00:17,025 --> 00:00:22,610
평균값을 계산하기 위하여 zi 의 미니 배치값의 합을 구할텐데요,

8
00:00:22,610 --> 00:00:25,910
즉, 여기서는 한개의 미니 배치에 있는 예시들을 합하는 것인데요.

9
00:00:25,910 --> 00:00:28,440
저는 여기에서 M을 사용해 예시의 개수를 나타낼 것입니다.

10
00:00:28,440 --> 00:00:31,870
전체 트레이닝세트에서가 아니라 미니 배치에서의 개수죠.

11
00:00:31,870 --> 00:00:35,955
그 이후에, 편차를 구하고, 다름으로 z norm 을 계산합니다.

12
00:00:35,955 --> 00:00:41,325
z norm 은 평균값 및 앱실론으로 numerical stability를 더한 표준편차의 scaling을 통해 구합니다.

13
00:00:41,325 --> 00:00:47,100
그리고나서, Z total은 Z norm을 가지고 감마와 베타로 rescaling을 하여 얻습니다.

14
00:00:47,100 --> 00:00:51,995
한가지 아실 것은, 여기서 scaling을 위한 산출에서 필요한 것은

15
00:00:51,995 --> 00:00:57,620
뮤와 시그마의 제곱값인데요, 이것들을 모두 전체 미니 배치에서 계산됩니다.

16
00:00:57,620 --> 00:01:00,495
하지만 테스트타임에서는, 64, 128, 또는 256개의

17
00:01:00,495 --> 00:01:05,240
예시들을 한번에 처리할 수 있는 미니 배치가 없을 수 있습니다.

18
00:01:05,240 --> 00:01:07,815
그렇기 때문에 뮤와 시그마제곱을 구하기 위한 어떠한 또 다른 방법이 필요합니다.

19
00:01:07,815 --> 00:01:10,290
그리고 만약 여러분이 1개의 예시밖에 없는 경우,

20
00:01:10,290 --> 00:01:15,060
이런 1개의 값을 가지고 평균값과 편차를 구하는 것은 말이 안됩니다.

21
00:01:15,060 --> 00:01:16,860
그러면 어떻게 되는건가요?

22
00:01:16,860 --> 00:01:21,240
테스트타임에서 신경망을 적용한다는 것은

23
00:01:21,240 --> 00:01:25,735
별도의 뮤와 표준편차의 예측값을 구하는 것입니다.

24
00:01:25,735 --> 00:01:27,975
일반적인 배치 정규화의 도입같은 경우엔,

25
00:01:27,975 --> 00:01:32,145
이 것을 추정하는데요,

26
00:01:32,145 --> 00:01:37,020
기하급수적 가중 평균 방식을 이용하여 그 평균은

27
00:01:37,020 --> 00:01:42,678
미니 배치에서의 값으로하여 구합니다.

28
00:01:42,678 --> 00:01:45,900
이 내용에 대해 구체적으로 말씀드리자면,

29
00:01:45,900 --> 00:01:51,750
여러분이 L이라는 층을 골라서, 미니 배치인 X1,

30
00:01:51,750 --> 00:01:57,500
X2를 같이하여 이에 상응하는 Y값과 함께 보고 있다고 해보겠습니다.

31
00:01:57,500 --> 00:02:02,280
이런 L층의 X1에서 트레이닝을 하는 경우,

32
00:02:02,280 --> 00:02:06,383
뮤 L 값이 나옵니다.

33
00:02:06,383 --> 00:02:12,485
이것은 제가 mu 그다음에 위첨자 첫번째 미니 배치 그리고 그 층으로 표시하겠습니다.

34
00:02:12,485 --> 00:02:15,540
그리고나서 두번째 미니 배치에서 그 층과 미니 배치를 트레이닝하는 경우,

35
00:02:15,540 --> 00:02:19,055
어떠한 2번째의 뮤값을 갖게됩니다.

36
00:02:19,055 --> 00:02:23,194
그리고 세번째 미니 배치에 대해서는, 여기 숨겨진 층에서,

37
00:02:23,194 --> 00:02:29,090
이런 세번째의 뮤값을 갖게됩니다.

38
00:02:29,090 --> 00:02:31,830
방금전에

39
00:02:31,830 --> 00:02:37,600
기하급수적 가중평균법을 사용해 쎄타1, 쎄타2, 쎄타3의

40
00:02:37,600 --> 00:02:40,020
평균값을 구하는 방법을 본 것처럼,

41
00:02:40,020 --> 00:02:44,173
현개 기온의 기하급수적 가중평균을 구했었던 이전 예제와 같이 말이죠,

42
00:02:44,173 --> 00:02:47,250
이렇게 한 것은

43
00:02:47,250 --> 00:02:50,790
최근의 여기 mean vector 값의 평균을 기록하기 위해서 그런 것입니다.

44
00:02:50,790 --> 00:02:54,195
그러면 여기 기하급수적 평균은

45
00:02:54,195 --> 00:03:00,120
숨겨진 층에 대한 z들의 평균값의 추정치가 됩니다.

46
00:03:00,120 --> 00:03:03,930
여기 층의 첫번째 미니 배치에 있는 시그마 제곱값을 트래킹하기위해

47
00:03:03,930 --> 00:03:09,015
그리고 두번째 첫번째 미니 배치에 있는 시그마 제곱값을 트래킹하기위해

48
00:03:09,015 --> 00:03:13,510
등등의 이유로 기하급수적 가중 평균을 사용합니다.

49
00:03:13,510 --> 00:03:18,780
이렇게 뮤과 시그마 제곱값의 평균을 돌립니다.

50
00:03:18,780 --> 00:03:24,535
각각의 층에서 나오는 값들에 대해서 말이죠. 그렇게해서 다른 미니 배치들에 대하여
신경망을 트레이닝 시킵니다.

51
00:03:24,535 --> 00:03:26,895
그리고 최종적으로 테스트타임에서는,

52
00:03:26,895 --> 00:03:30,275
이 공식 대신해서,

53
00:03:30,275 --> 00:03:35,855
어떠한 z값이던 이 값을 이용해서 그냥 z norm의 값을 구하고,

54
00:03:35,855 --> 00:03:39,735
그리고 뮤와 시그마 제곱값에서 기하급수적 가중

55
00:03:39,735 --> 00:03:45,340
평균치를 구합니다. 여기서 scaling을 하기위해 최근 값의 뮤와 시그마를 사용합니다.

56
00:03:45,340 --> 00:03:48,780
그 다음으로, z total 값을 구합니다.

57
00:03:48,780 --> 00:03:53,670
하나의 테스트 예시에 대해서 말이죠, 왼쪽에서 방금 산출한

58
00:03:53,670 --> 00:03:57,240
z norm과 베타,

59
00:03:57,240 --> 00:04:02,695
감마 파라미터와 같이 신경망 네트워크 트레이닝 단계에서 배운 것들을 사용해서 z total값을 구합니다.

60
00:04:02,695 --> 00:04:07,020
여기서 중요한 부분은, 64, 128개 등의 예시에서

61
00:04:07,020 --> 00:04:11,620
전체의 미니 배치에서 트레이닝타임

62
00:04:11,620 --> 00:04:14,580
뮤와 시그마 제곱을 통해 산출됩니다.

63
00:04:14,580 --> 00:04:18,345
하지만 테스트타임 같은 경우에는, 예시별로 한번에 한번씩 처리해야할 수 있습니다.

64
00:04:18,345 --> 00:04:21,605
이렇게 하는 방법은, 트레이닝세트에서 뮤와 시그마

65
00:04:21,605 --> 00:04:25,325
제곱의 값을 예측하는 방법이 있는데요. 그 방법은 여러가지가 있습니다.

66
00:04:25,325 --> 00:04:27,450
이론적으로는 최종 네트워크를 통해서

67
00:04:27,450 --> 00:04:30,960
전체 트레이닝세트를 실행시켜서 뮤와 시그마 제곱의 값을 가질 수 있습니다.

68
00:04:30,960 --> 00:04:33,550
하지만 실제로는, 사람들은 보통

69
00:04:33,550 --> 00:04:36,330
기하급수적 가중평균법을 도입하여

70
00:04:36,330 --> 00:04:38,970
뮤와 시그마 제곱값을 트래킹합니다.

71
00:04:38,970 --> 00:04:42,130
트레이닝에서 보는 값들에 대해서 기하급수적 가중평균을 사용합니다.

72
00:04:42,130 --> 00:04:44,095
running average라고 불리기도 합니다.

73
00:04:44,095 --> 00:04:46,330
대략적인 뮤와 시그마 제곱에 대한

74
00:04:46,330 --> 00:04:49,800
예측치를 알아내기 위해서 말이죠. 그런 뒤에, 이런 뮤과 시그마 제곱의

75
00:04:49,800 --> 00:04:55,860
값을 테스트타임에서 이용하여 숨겨진 유닛값인 z를 scale을 하는데 사용합니다.

76
00:04:55,860 --> 00:04:58,980
실제로 이 절차는 매우 견고한 편인데요,

77
00:04:58,980 --> 00:05:03,125
정확한 뮤와 시그마 제곱의 값을 구하는 방법과 비교했을때도 말이죠,

78
00:05:03,125 --> 00:05:06,440
그렇기 때문에 이렇게 하는 방법에 대해서는

79
00:05:06,440 --> 00:05:09,729
너무 걱정하진 않을 것 같습니다. 그리고 여러분이 딥러닝 프레임웍을 사용하는 경우엔,

80
00:05:09,729 --> 00:05:13,080
기본적으로 보통 뮤와 시그마 제곱 값의

81
00:05:13,080 --> 00:05:17,665
예측수치를 구하는 기본 방법이 있을 것입니다. 이런 기본 방법도 비교적 잘 작동할 것이구요.

82
00:05:17,665 --> 00:05:21,965
실제로는, 만약 숨겨진 유닛 값은 z의 평균값과 편차의 값을 구하는

83
00:05:21,965 --> 00:05:28,600
어떠한 합리적인 방법이라고 하면 왠만해선 다 테스트에서 잘 작동할 것입니다.

84
00:05:28,600 --> 00:05:31,270
이제 배치 정규화와 그 사용법에 대한 내용은 이게 전부인데요.

85
00:05:31,270 --> 00:05:33,520
이제 여러분은 더 깊은 네트워크를 트레이닝 하실 줄 알고,

86
00:05:33,520 --> 00:05:37,205
러닝 알고리즘을 더 빨리 실행시키실 줄 아시게 되었습니다.

87
00:05:37,205 --> 00:05:38,870
이번주 내용을 마치기 이전에,

88
00:05:38,870 --> 00:05:43,080
저는 여러분과 딥러닝 프레임웍에 대한 생각도 공유하고 싶습니다.

89
00:05:43,080 --> 00:05:46,000
이것에 대해서는 다음 비디오에서 이야기하겠습니다.