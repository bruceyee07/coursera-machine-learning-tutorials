所以我们已经知道了单隐藏层的Batch Norm的公式 所以我们已经知道了单隐藏层的Batch Norm的公式 让我们看看如何在深度网络中使用它 那么 假设我们有一个像这样的神经网络 我们都知道 每一个节点都可以看作是在计算两个事情 首先 计算z
然后把z传递给激活函数来计算a 所以我们认为每一个圆圈都代表着两步计算 下一层同样如此 这是z21以及a21 此次类推 所以 如果我们不采用Batch Norm 我们让输入x进入到第一级隐藏层 然后首先计算z1 这由参数w1和b1决定 如此这般 我们把z1输入给激活函数来计算a1 但是在Batch Norm中 把z1作为输入会怎么样呢 ? 再次做Batch Norm变换 它的缩写一般为BN 它将由参数Beta1以及Gamma1决定 它将由参数Beta1以及Gamma1决定 这将产生新的归一化值z1 然后我们把它输入给激活函数来获取a1 也就是g1函数作用于(带波浪号的)z1 现在 我们已经完成了第一层的计算 在此 BN算法确实被应用于由z到a的计算过程中 下一步 我们用a1值来计算z2 现在这由参数w2和b2决定 与第一层类似 我们在z2上应用Batch Norm 缩写仍然是BN 这由下一层的特定BN参数决定 也就是Beta2和Gamma2 我们获得了(带波浪号的)z2 我们又可以用它通过激活函数来计算a2 以此类推 再一次 在计算z和a的过程中<br />我们运用了BN算法 直觉是 不要使用未归一化的z值 而是要使用归一化后的(带波浪号的)z值<br />这适用于第一层 也适用于第二层 不要使用未归一化的z2值 而要使用均值和方差归一化后的(带波浪号的)z2值 所以网络参数将是w1 b1 我们可以忽略b参数 我会在下一张幻灯片说明 现在 假设参数就是一般的w1和b1 直到wL bL<br />并且我们还将加入其余的参数到新网络中 并且我们还将加入其余的参数到新网络中 ‑ Beta1 Gamma1 Beta2 Gamma2以此类推 对每个采用BN算法的网络层都要如此 澄清一下 注意到这里的参数Betas 它们与我们在Momentum算法中<br />用于计算各种指数加权平均值的超参数Beta无关 它们与我们在Momentum算法中<br />用于计算各种指数加权平均值的超参数Beta无关 Adam算法的作者使用Beta来表示超参数 而BN算法的作者也使用Beta来表示该参数 但这是两种完全不同的Betas 我决定在两种情况下都使用Beta 以防你们读到原文时候困惑 但是 Beta1 Beta2 以此类推 BN算法中学习的参数Beta 不同于Adam RMSprop和Momentum算法中使用的超参数Beta 所以现在这些是 我们算法 中的新参数 我们然后可以根据需求进行优化 例如采用梯度递减的方法来实现它 例如 我们可能会计算dBeta L(L层Beta的导数) 然后更新Beta参数 用Beta减去学习率乘以dBeta L(L层Beta的导数) 用Beta减去学习率乘以dBeta L(L层Beta的导数) 你也可以用Adam或RMSprop或Momentum算法<br />来更新参数Beta和Gamma 并不仅仅采用梯度递减的方法 尽管在上一个视频中 我已经解释过了BN算法的效果 它计算平均值和方差 并把它们减去和除去 如果你正在使用深度学习编程框架 通常你不必自己去实现BN算法 在编程框架中 它可能仅仅是一行代码 在编程框架中 它可能仅仅是一行代码 例如 在TensorFlow框架中 我们可以用这个函数实现BN 我们待会儿会讲到编程框架 但是实际上 你最后可能不用自己去实现这些细节 了解它怎么工作就可以了 这有助于我们理解自己的代码 一般在深度学习框架下实现BN通常只需要一行代码 现在 到目前为止 我们已经讲过了在整个训练集上<br />利用批量梯度下降训练的BN算法 我们已经讲过了在整个训练集上<br />利用批量梯度下降训练的BN算法 实际上 通常应用到训练集上的<br />是少批量 (mini-batch) BN算法 所以实际使用BN算法时 我们取第一个mini-batch并计算z1 和上张幻灯片一样 我们用参数w1 b1 然后我们取一个适合的mini-batch<br />并利用它计算z1的均值和方差 接着BN算法就会 减去均值 除以标准差<br />然后用参数Beta1 Gamma1来重新调整 从而得到(带波浪号的)z1 这些都是在第一个mini-batch上计算的 然后我们应用激活函数来获得a1 然后我们用w2 b2来计算z2 以此类推 我们做这些是为了 在第一个mini-batch上计算梯度下降<br />接着转到第二个mini-batch上 也就是x2 然后的步骤差不多 我们现在 在第二个mini-batch上计算z1<br />然后利用BN算(带波浪号的)z1 所以在BN这一步中 我们会用第二个mini-batch中的数据来归一化(带波浪号的)z 这就是BN算法在此的作用 我们来看个在第二个mini-batch过程中例子 它会计算z1在此mini-batch上的平均值和方差 并且通过参数Beta和Gamma来重新调整<br />以获得(带波浪号的)z 以此类推 然后我们在第三个mini-batch上重复上述步骤<br />如此一直训练下去 现在 我想澄清关于参数的一个细节 那就是 我曾经说过的每一层的参数 wL bL 以及Beta L和Gamma L 现在注意z是由下面的步骤计算得出的 也就是zL=wL * a(L-1)+bL<br />但是BN算法所做的 就是使用mini-batch并且归一化zL 来满足均值为0以及标准方差 然后通过参数Beta和Gamma来重新调整 这意味着 不管bL的值为多少 实际上都要被减去 因为经过BN这一步 我们将计算zL的均值并将其减去 所以在mini-batch中对所有例子加上一个常量 并不会改变什么 因为无论我们加上什么常量<br />它都会被 减均值 这一步给去除 所以如果你使用BN算法 你可以忽略该参数b 或者可以认为它永远等于0 所以参数就变为了 zL=wL * a(L-1) 然后我们计算归一化zL 并且计算(带波浪号的)z等于Gamma乘以zL加上Beta 我们最终使用该参数Beta L来决定 (带波浪号的)zL的均值<br />它将在下一层中得到转发 复习一下 因为BN算法使层级中各个zL的均值为0 我们就没有理由保留参数bL 所以就把它忽略了 相应地 被Beta L所代替 也就是一个用来控制最终偏移量影响的参数 最后 记住zL的维度 如果我们要在某个例子中应用它 假设zL的维度是这里的n^Lx1 那么bL的维度也是这里的n^Lx1 假设nL为L层中隐藏单元数 那么Beta L和Gamma L的维度 也会是n^Lx1 就等于隐藏单元数 我们有nL隐藏单元数 那么Beta L和Gamma L被用于 调整每一个隐藏单元的均值和方差 这是由网络决定的 好吧 让我们来汇总一下 并且总结如何实现降梯度BN算法 假设我们使用mini-batch梯度下降算法 枚举从t等于1一直到mini-batch数 在mini-batch (Xt) 上计算前向传播 然后在每一层上计算前向传播 使用BN算法 用(带波浪号的)zL来代替zL<br />这表明 利用mini-batch算法 z值将会进行归一化均值和方差处理 这个经历归一化均值和方差的值就是(带波浪的)zL 然后利用反向传播来计算dw dw db以及所有L层的值 dBeta和dGamma 尽管 技术上来讲 我们已经忽略了参数b 但它并没有完全消除 最终 我们更新参数 所以 w更新为w减去学习率乘以dw Beta更新为Beta减去学习率乘以db 对Gamma同样如此 如果我们如下计算梯度 这就是梯度下降算法 就如我这里所写的 但它同样适用于<br />Momentum或RMSprop或Adam的梯度下降算法 但它同样适用于<br />Momentum或RMSprop或Adam的梯度下降算法 它们不采用这种梯度下降更新算法 而采用其它的更新算法 这在我们上周视频中有所谈及 这些其它的优化算法也可以被用来 更新BN算法中的新增参数Beta和Gamma 所以 我希望这能帮助你们 从头实现BN算法 如果你正在使用某种深度学习编程框架 如果你正在使用某种深度学习编程框架<br />比如后续的课程中将要讨论的框架 希望你能够调用编程框架中的方法 来轻松实现BN算法 现在 如果你还是对BN算法有些疑惑 还是不理解 它为什么会有效提高训练速度 让我们进入下一视频 来探讨 为什么BN算法如此有效<br />以及它到底做了什么