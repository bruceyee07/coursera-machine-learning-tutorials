批次標準化一次處理一小批(mini batch)資料 不過在測試階段，你可能一次只處理一筆資料 讓我們瞧瞧如何調整你的網路應付之 還記得訓練的時候 這些是用來實作批次標準化(batch norm)
的方程式 在單一個小量批次中 你把這一批的資料 z(i) 的值全加起來，算出平均 在這邊，你只是對於一批資料加起來 我用 m 來代表這一批資料的個數 不是整個訓練資料的 然後你計算變異數，然後算出 z norm 藉由平均和標準差縮放得出，還有
分母多加 epsilon 保持數值運算穩定 然後把 z norm 以 gamma 和 beta 再次縮放，
得到 z tilde 那麼，注意這邊的 mu 和 sigma^2 也就是拿來縮放的，他們是對一小批資料整個去計算 但是在測試階段，你可能一次不會處理 一整批的64, 128 或 256 筆的資料 所以你需要另一種方法導出 mu 和 sigma^2 如果你手上只有一筆資料 算那一筆的平均和變異數，這根本沒道理 那到底怎麼做呢？ 為了在測試階段套用你的神經網路 我們要有不同方法來估計出 mu 和 sigma 在批次標準化典型的實作中 你通常會這樣估計： 利用指數加權平均(exponentially weighted average) 來平均過眾多的小批資料 要講的更精確的話， 假設我們選了某層 l，我們依序用了 x1, x2 (與其對應的y), ... 這些小批資料 那，在訓練第 l 層的 x1 時 你算出某個 mu[l]，實際上 我要把他表示成那一層的第一批的mu 然後你訓練那一層的第二批小量資料 你會得到第二個 mu 然後對於這一層的第三批， 你得出第三個 mu 所以，如同我們之前利用 指數加權平均來平均那些 theta_1, theta_2 theta_3, 就是你計算 溫度的指數加權平均的那時候 你會追蹤你看到的 最新的加權平均向量 所以，那時候學的指數加權平均 就可用來估計這一層的這些z的平均 同樣的，你也利用指數加權平均來追蹤 你在這層看到的第一批小資料的 sigma^2, 第二批小資料的 sigma^2, 等等等 所以當你在訓練網路，訓練過眾多的小批資料時， 在每一層，你記錄當時看過的 mu 和 sigma^2
的移動平均 最後，在測試階段 你要做的是在這一個方程式內 在計算 z norm 時，拿你看到的 z， 利用前面記錄到最後的 mu和sigma^2的指數加權平均來做縮放 然後這一筆測試資料的 z tilde 是這樣計算的：拿剛剛左邊的 z norm， 再利用在訓練網路時 學到的參數 beta 和 gamma 來算出 因此，這邊的要點是，在訓練途中 mu和sigma^2是算在整個小批資料上， 可能是 64, 128, 或其他數量的資料。 但是在測試階段，你可能需要一次處理一筆資料 所以，我們用估計的方式，估計出 你訓練集的mu和sigma；這有很多種方式 理論上，你可以讓最終的網路 跑過所有的全部的訓練資料，以得到mu和sigma^2， 但實務上，通常大家實作是用 指數加權平均，也就是你追蹤 訓練的過程中得到的mu和sigma^2， 利用指數加權平均的方式— 有時這也被稱作「移動平均」 —來得到mu和sigma^2大概的估計， 再把那mu和sigma^2的估計值 在測試階段用於隱藏層的z來縮放。 實務上，這整個流程 對於你用哪種方式估計mu和sigma^2，都滿穩定的 所以我不會太擔心你怎麼去進行 而且如果你使用的是深度學習的框架 通常他們會有個預設的方式去估計 mu和sigma^2，大概表現得都不錯。 實際上，只要估計的方式很合理 任何合理估計隱藏層z的平均和變異數的方法，
在測試時應該都能表現得好 那麼，這就是批次標準化及其使用法 我想你已經能訓練更深的網路 還有讓你的學習演算法跑得飛快 在結束這周之前， 我還想分享一些關於深度學習框架的想法 我們在下一段影片
來開始談這個