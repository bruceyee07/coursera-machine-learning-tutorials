배치 정규화는 여러분의 데이터를 한번에 조금한 집단별로 처리합니다. 하지만 테스트타임은 여러분이 샘플별로 하나씩 처리해야할 수 있습니다. 자 그럼 이렇게 하기 위해서 여러분의 네트워크를 어떻게 변경시킬 수 있는지 한번 보겠습니다. 트레이닝에서 기억하시겠지만, 배치 정규화를 도입하기 위해 사용하는 공식들입니다. 싱글 미니 배치안에서, 평균값을 계산하기 위하여 zi 의 미니 배치값의 합을 구할텐데요, 즉, 여기서는 한개의 미니 배치에 있는 예시들을 합하는 것인데요. 저는 여기에서 M을 사용해 예시의 개수를 나타낼 것입니다. 전체 트레이닝세트에서가 아니라 미니 배치에서의 개수죠. 그 이후에, 편차를 구하고, 다름으로 z norm 을 계산합니다. z norm 은 평균값 및 앱실론으로 numerical stability를 더한 표준편차의 scaling을 통해 구합니다. 그리고나서, Z total은 Z norm을 가지고 감마와 베타로 rescaling을 하여 얻습니다. 한가지 아실 것은, 여기서 scaling을 위한 산출에서 필요한 것은 뮤와 시그마의 제곱값인데요, 이것들을 모두 전체 미니 배치에서 계산됩니다. 하지만 테스트타임에서는, 64, 128, 또는 256개의 예시들을 한번에 처리할 수 있는 미니 배치가 없을 수 있습니다. 그렇기 때문에 뮤와 시그마제곱을 구하기 위한 어떠한 또 다른 방법이 필요합니다. 그리고 만약 여러분이 1개의 예시밖에 없는 경우, 이런 1개의 값을 가지고 평균값과 편차를 구하는 것은 말이 안됩니다. 그러면 어떻게 되는건가요? 테스트타임에서 신경망을 적용한다는 것은 별도의 뮤와 표준편차의 예측값을 구하는 것입니다. 일반적인 배치 정규화의 도입같은 경우엔, 이 것을 추정하는데요, 기하급수적 가중 평균 방식을 이용하여 그 평균은 미니 배치에서의 값으로하여 구합니다. 이 내용에 대해 구체적으로 말씀드리자면, 여러분이 L이라는 층을 골라서, 미니 배치인 X1, X2를 같이하여 이에 상응하는 Y값과 함께 보고 있다고 해보겠습니다. 이런 L층의 X1에서 트레이닝을 하는 경우, 뮤 L 값이 나옵니다. 이것은 제가 mu 그다음에 위첨자 첫번째 미니 배치 그리고 그 층으로 표시하겠습니다. 그리고나서 두번째 미니 배치에서 그 층과 미니 배치를 트레이닝하는 경우, 어떠한 2번째의 뮤값을 갖게됩니다. 그리고 세번째 미니 배치에 대해서는, 여기 숨겨진 층에서, 이런 세번째의 뮤값을 갖게됩니다. 방금전에 기하급수적 가중평균법을 사용해 쎄타1, 쎄타2, 쎄타3의 평균값을 구하는 방법을 본 것처럼, 현개 기온의 기하급수적 가중평균을 구했었던 이전 예제와 같이 말이죠, 이렇게 한 것은 최근의 여기 mean vector 값의 평균을 기록하기 위해서 그런 것입니다. 그러면 여기 기하급수적 평균은 숨겨진 층에 대한 z들의 평균값의 추정치가 됩니다. 여기 층의 첫번째 미니 배치에 있는 시그마 제곱값을 트래킹하기위해 그리고 두번째 첫번째 미니 배치에 있는 시그마 제곱값을 트래킹하기위해 등등의 이유로 기하급수적 가중 평균을 사용합니다. 이렇게 뮤과 시그마 제곱값의 평균을 돌립니다. 각각의 층에서 나오는 값들에 대해서 말이죠. 그렇게해서 다른 미니 배치들에 대하여
신경망을 트레이닝 시킵니다. 그리고 최종적으로 테스트타임에서는, 이 공식 대신해서, 어떠한 z값이던 이 값을 이용해서 그냥 z norm의 값을 구하고, 그리고 뮤와 시그마 제곱값에서 기하급수적 가중 평균치를 구합니다. 여기서 scaling을 하기위해 최근 값의 뮤와 시그마를 사용합니다. 그 다음으로, z total 값을 구합니다. 하나의 테스트 예시에 대해서 말이죠, 왼쪽에서 방금 산출한 z norm과 베타, 감마 파라미터와 같이 신경망 네트워크 트레이닝 단계에서 배운 것들을 사용해서 z total값을 구합니다. 여기서 중요한 부분은, 64, 128개 등의 예시에서 전체의 미니 배치에서 트레이닝타임 뮤와 시그마 제곱을 통해 산출됩니다. 하지만 테스트타임 같은 경우에는, 예시별로 한번에 한번씩 처리해야할 수 있습니다. 이렇게 하는 방법은, 트레이닝세트에서 뮤와 시그마 제곱의 값을 예측하는 방법이 있는데요. 그 방법은 여러가지가 있습니다. 이론적으로는 최종 네트워크를 통해서 전체 트레이닝세트를 실행시켜서 뮤와 시그마 제곱의 값을 가질 수 있습니다. 하지만 실제로는, 사람들은 보통 기하급수적 가중평균법을 도입하여 뮤와 시그마 제곱값을 트래킹합니다. 트레이닝에서 보는 값들에 대해서 기하급수적 가중평균을 사용합니다. running average라고 불리기도 합니다. 대략적인 뮤와 시그마 제곱에 대한 예측치를 알아내기 위해서 말이죠. 그런 뒤에, 이런 뮤과 시그마 제곱의 값을 테스트타임에서 이용하여 숨겨진 유닛값인 z를 scale을 하는데 사용합니다. 실제로 이 절차는 매우 견고한 편인데요, 정확한 뮤와 시그마 제곱의 값을 구하는 방법과 비교했을때도 말이죠, 그렇기 때문에 이렇게 하는 방법에 대해서는 너무 걱정하진 않을 것 같습니다. 그리고 여러분이 딥러닝 프레임웍을 사용하는 경우엔, 기본적으로 보통 뮤와 시그마 제곱 값의 예측수치를 구하는 기본 방법이 있을 것입니다. 이런 기본 방법도 비교적 잘 작동할 것이구요. 실제로는, 만약 숨겨진 유닛 값은 z의 평균값과 편차의 값을 구하는 어떠한 합리적인 방법이라고 하면 왠만해선 다 테스트에서 잘 작동할 것입니다. 이제 배치 정규화와 그 사용법에 대한 내용은 이게 전부인데요. 이제 여러분은 더 깊은 네트워크를 트레이닝 하실 줄 알고, 러닝 알고리즘을 더 빨리 실행시키실 줄 아시게 되었습니다. 이번주 내용을 마치기 이전에, 저는 여러분과 딥러닝 프레임웍에 대한 생각도 공유하고 싶습니다. 이것에 대해서는 다음 비디오에서 이야기하겠습니다.