那么 为什么BN算法呢? 其中一个理由是<br />我们看到经过归一化的输入特征(用X表示) 它们的均值为0 方差为1 这将大幅加速学习过程 所以与其含有某些在0到1范围内变动的特征 或在1到1000范围内变动的特征 通过归一化所有输入特征X 让它们都拥有相同的变化范围将加速学习 所以 BN算法有效的一个原因是 它同样如此 只不过它应用于隐藏层的值 而不是这里输入特征 现在 我所描述的仅仅是BN算法所做的一部分 还有很多(关于BN算法的)进一步解释 这将有助于我们更深层一步地理解BN算法 我会在本视频中阐述它 BN算法有效的第二个原因是 它产生权重 (w参数) 在深层次网络中 假设在10层的参数w 比神经网络初始的层级 假设为0层更具鲁棒性 为了解释我的意思 让我们参考这个生动的例子 假设我们在某个网络上训练 可能是浅层网络 例如逻辑回归或者是神经网络 可能是浅层网络<br />例如逻辑回归或者是著名的猫咪检测深度网络 著名的猫咪检测深度网络 假设我们的数据集 是在所有黑猫图片上训练得到的 现在我们将其应用于 所有的正确结果并不像左侧黑猫那样的<br />彩色猫咪图片上 所有的正确结果并不像左侧黑猫那样的<br />彩色猫咪图片上 然后右侧的彩色猫咪的确是猫 那么我们的分类将会出错 所以对于图像 如果我们的训练集像这样 正确样本在这里 错误样本又在这里 但我们却试图获得一个数据集 它的正确样本在这里 而错误样本又在这里 那么我们不要指望用左侧数据训练得到的模型<br />适用于右侧 那么我们不要指望用左侧数据训练得到的模型<br />适用于右侧 尽管这里可能存在着相同的有效函数 但你就不要指望你的学习算法<br />能找到那条绿色决策边界 只需看看左侧的数据就知道了 所以我们的数据分布随着某种东西在变化 也就是 协变量 想法是 如果我们学习了某种x-y映射 如果x的分布变化了 那么我们就得重新训练学习算法 这是一定的 尽管x-y映射的真函数没变 尽管x-y映射的真函数没变 尽管x-y映射的真函数没变 如此例 因为真函数就是判断该图是否是猫 因为真函数就是判断该图是否是猫 并且迫切需求保持我们的函数 或者它变得更糟糕如果真函数也在变 那么 协变量问题如何影响神经网络? 假设一个如此的神经网络 让我们从这个层的角度来看待学习过程 也就是第三层 假设网络已经学习得出了参数w3和b3 从第三层的角度来看 它从前面层获得了某些值 然后它会做些处理来使 输出(带帽子的)Y接近真实值Y 让我挡住左侧的节点一会儿 所以从第三层角度来看 它获得了某些值 假设为a21 a22 a23 a24 但是这些值可能和特征值x1 x2 x3 x4一样 第三级隐藏层的作用就是 以这些值做输入 然后找到某种将它们映射到(戴帽子的)Y的方法 你可以假设做梯度衰减 那么这些参数w3 b3以及w4 b4 甚至是w5 b5 可能试图学习这些参数 所以网络做得很好 它把我在左侧画的值映射到输出值(戴帽子的)Y 现在让我们再一次加入左侧网络 网络也会学习参数w2 b2以及w1 b1 所以当这些参数变化时 这些值 如a2 也会变化 所以从第三级隐藏层角度来看 这些隐藏单元值一直在变 所以它受协变量问题影响 所以它受协变量问题影响 这在之前我们谈过 所以BN算法所做的就是 它减少了这些隐藏单元值的 分布的不稳定性 假设我们想画出这些隐藏单元值的分布 技术上讲就是归一化后的z 所以这实际上是z21和z22 我不画四个值 就画两个值 好使我们从2D来看 BN算法的意思是说 z21和z22的值可以变化 当神经网络刷新前几层参数时 它们的确会变化 当神经网络刷新前几层参数时 它们的确会变化 但BN算法确保的是无论它怎么变 z21和z22的均值和方差将保持不变 所以尽管z21和z22的值变化 它们的均值一直为0 方差一直为1 或者 并不一定是均值0 方差1 但是它们的值由beta2 gamma2控制 这两个参数由神经网络选定 所以可以使均值为0 方差为1 或者 可以是其它任何均值和方差 但是这样做 限制了先前层中参数的更新 对第三层 现在所看到和要学习的值的分布的 影响 对第三层 现在所看到和要学习的值的分布的 影响 所以 BN算法减少输入值变化所产生的问题 它的确使这些值变得稳定 所以神经网络的后层 可以有更加稳固的基础 尽管输入分布变化了一点 它变化的更少 实际是 尽管前几层继续学习 后面层适应前面层变化的力量被减弱 后面层适应前面层变化的力量被减弱 如果我们愿意 BN算法削弱了 前面层参数和后层参数之间的耦合 前面层参数和后层参数之间的耦合 所以它允许网络的每一层独立学习 有一点独立于其它层的意思 所以这将有效提升整个网络学习速度 所以我希望这做出了进一步解释 但是结论是 BN算法意味着 尤其是从神经网络某一后层角度来看 前面的层的影响并不会很大 因为它们被同一均值和方差所限制 所以这使后层的学习工作变得更加简单 BN算法还有第二个效果 它具有轻微的正则化效果 mini-batch是BN算法中一个模糊的概念 例如 mini-batch xt 拥有值zt 还有值zl 均值和方差已经在该min-batch上归一化过了 现在 因为我们在该min-batch上计算了均值和方差 而不是在整个数据集上计算 所以该均值和方差包含有噪声 因为这仅仅是在尺寸为64或128或256<br />或更多的训练例子的mini-batch上计算而来 因为这仅仅是在尺寸为64或128或256<br />或更多的训练例子的mini-batch上计算而来 因为这仅仅是在尺寸为64或128或256<br />或更多的训练例子的mini-batch上计算而来 所以均值和方差有点噪声 因为它是由相对较少的数据集评估得来<br />该归一化过程 从zl 到 (带波浪号的)zl 这个过程也会产生噪声 因为它是用带有一定噪声的均值和方差来计算的 所以和dropout算法类似 它会为每个隐藏层的激活函数增加一些噪声 dropout有噪声的原因是 它将隐藏单元以一定概率乘以0 以一定概率乘以1 所以dropout有着许多噪声<br />因为它要么乘以0要么乘以1 然而BN算法有着许多噪声<br />因为被标准方差归一化了 此外 因为减去均值<br />BN算法还有附加噪声 这里 均值和标准方差的评估值都含有噪声 所以 和dropout类似 BN算法因此具有轻微的正则化效果 通过在隐藏单元上增加噪声 它使后续的隐藏单元不要过度依赖其它隐藏单元 所以 和dropout类似 这增加了隐藏层噪声 因具有轻微的正则化效果 因为加入的噪声太小 这没有强大的正则化效果 那么我们会选择一道使用BN和dropout算法 如果你想获得更强大的正则化效果的话 如果你想获得更强大的正则化效果的话 另一个模糊的效果是 如果我们采用更大尺寸的mini-batch 假设尺寸为 512而不是64 通过使用更大尺寸的mini-batch 我们可以减少噪声同时也减少了正则化效果 所以这是dropout算法的一个奇怪特性 那就是通过使用更大尺寸的min-batch 将会削弱正则化效果 说到这里 我肯定不会用BN算法作为正则化器 毕竟这不是BN算法的目的 但是有时候它对我们的学习算法<br />有着些额外的有意或无意的影响 但是 一定不要将BN算法看作是正则化方法 而是把它用来作为归一化 隐藏单元激活函数用以加速学习的方法 我认为正则化效果只是一种无意的副作用 所以我希望这能帮助你们更好的理解BN算法的效果 在做BN算法总结前 还有一个我希望你们了解的细节 那就是BN算法中 一个mini-batch处理一次数据 在该mini-batch上计算均值和方差 所以在测试时 当你尝试做出预测<br />和评估神经网络时 你可能没有包含多个例子的min-batch 你可能每次只处理一个例子 所以 在测试时你需要做些变化 来确保你做出的预测准确 在下个也就是最终关于BN算法的视频中 让我们来讨论 使用BN算法训练神经网络以正确做出预测<br />所要做的细节 让我们来讨论 使用BN算法训练神经网络以正确做出预测<br />所要做的细节