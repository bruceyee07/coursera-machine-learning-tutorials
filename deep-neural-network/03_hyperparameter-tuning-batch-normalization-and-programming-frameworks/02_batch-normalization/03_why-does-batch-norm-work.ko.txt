자, 배치 정규화는 어떻게 잘 작동하는 것일까요? 한가지 이유는, 여러분이 보셨드시 X 라는 입력 특성을 정규화 하여 평균값을 0, variance를 1로 만들었는데요, 이것이 러닝의 속도를 높혀줍니다. 어떤 특성이 0에서 1의 값을 갖는다거나, 다른거는 1에서 1000의 범위를 갖는 것이 아니라 모든 특성을 정규화함으로써 입력 특성 x가 비슷한 범위의 값을 주고, 이로 인해 러닝의 속도가 증가합니다. 배치 정규화가 잘 작동하는 한가지 직관적인 부분은 이렇게 비슷하게 하는 것입니다. 여기 입력 층에 대한 것뿐만 아니라 숨겨진 유닛을 위해서도 그렇게 하는 것입니다. 이것은 배치 정규화가 하는 것의 일부 그림일 뿐인데요, 다른 직관적인 부분이 또 있습니다. 이런 내용이 여러분이 더 깊이 배치 정규화의 역할을 이해하는데 
도움을 줄 것입니다. 이번 비디오를 통해 보도록 하겠습니다. 배치 정규화가 잘 작동하는 두번째 이유로 이것은 weight를 만듭니다. 여러분의 네트워크보다 더 나중 또는 더 깊은 곳 같은 경우, 예를 들어, 10번째 층에서는 weight의 변화에 더 더 튼튼합니다. 신경망에서 더 이른 쪽의 층, 예를 들어 첫번째 층보다 말이죠. 무슨 말인지 설명 드리자면, 더 선명한 예제를 보겠습니다. 여기 네트워크의 트레이닝을 보겠습니다. 옅은 트레이닝일 수도 있는데요, 로지스틱 회귀분석 또는 신경망일 수도 있는데요, 이런 regression과 같은 얕은 네트워크나 유명한 고양이 인식 업무와 같은 것 말이죠. 그런데 여러분이 모든 데이터를 검은 고양이의 사진으로 트레이닝 했다고 해봅시다. 이것을 색깔이 있는 고양이 데이터에 적용하려고 하면 positive example은 왼쪽에 있는 검정색 고양이뿐만 아니라 오른쪽에 있는 색깔 있는 고양이도 포함될 것입니다. 그러면 classfier가 잘 작동되지 않을 수 있습니다. 그러므로 그림에서, 트레이닝 세트가 만약 이렇게 생겼으면, positive example이 여기 있고, negative example이 여기 있는 경우죠, 그런데 일반화해 야하면, 여기 이런 데이터세트에 말이죠. 여기서는 positive example이 여기 있고, 
negative example이 여기 있겠죠. 그러면 여기 왼쪽에서 트레이닝 된 모델 같은 경우에 오른쪽에 있는 데이터에서 잘 못 할 것입니다. 똑같이 잘 작동하는 함수여도 말이죠. 하지만 여러분의 러닝 알고리즘이 이런 초록색 decision 경계선을 단순히 왼쪽의 데이터를 보고 발견하는 것은 예상하지 않은 것입니다. 그리하여 이런 데이터의 분포가 변하는 아이디어를 covariate shift라고 하는 거창한 이름으로 불리는데요, 기본적인 아이디어는 만약 X에서 Y로 가는 매핑을 배운 경우, x의 분포도가 만일 변경하면 러닝 알고리즘을 다시 트레이닝 시켜야 할 수도 있다는 것입니다. 이것은 x에서 y로 가는 매핑, ground truth 함수가 변하지 않더라도 참으로 적용됩니다. 이번 예제같이 말이죠. 그 이유는 ground truth 함수는 이 그림이 고양인지 아닌지에 대한 것입니다. 그리고 함수를 다시 트레이닝 시켜야하는 필요성이 격상되는데요, 또는 더 악화됩니다. ground truth 함수가 이동하는 경우에 말이죠. 그러면 이러한 covariate shift 문제가 신경망 네트워크에서는 어떻게 적용될까요? 이와 같은 심층 신경망을 고려해보겠습니다. 이제 러닝 절차를 어떤 특정 층의 기준으로 볼텐데요, 세번째 층이라고 해보겠습니다. 그럼 이 네트워크는 w3와 b3 파라미터를 배웠습니다. 그러면 3번째 층의 시각으로는 특정 값들을 이전 층에서 가지고 오는데요, 어떤 것들을 진행해서 y hat을 ground truth 값인 Y와 비슷하게 예측해야 하는데요, 그러면 잠시 왼쪽에 있는 노드들은 가려보겠습니다. 세번째 숨겨진 층의 기준으로는 이러한 값을 갖고, 이 값들을 A_2_1, A_2_2, A_2_3, 그리고 A_2_4 라고 하겠습니다. 그러나 이런 값들은 차라리 X1, X2, X3, X4인 특성이라고 칭하는 것이 낫겠습니다. 그리고 세번째 숨겨진 층의 역할은 이 값들을 갖고 ŷ 을 매핑하는 것입니다. 그러면 기울기 강하를 진행하는데요 여기 파라미터들 W_3_B_3,W_4_B_4, 그리고 W_5_B_5도 말이죠, 잘하면 이런 파리미터를 배워서 네트워크가 여기 왼쪽에 그린 값들에 대해 매핑을 잘할 수 있게 될 수도 있겠죠. ŷ 값을 말이죠. 그럼 이제 왼쪽의 네트워크를 다시 보여드리겠습니다. 네트워크 또한 W_2_B_2 와 W_1B_1의 파라미터를 사용합니다. 그렇기 때문에 여기 파라미터의 값이 변하면 여기 값, 즉 a2의 값도 변할 것입니다. 그러면 세번째 층의 시각에서 보면, 여기 숨겨진 유닛의 값은 항상 변하는 것입니다. 그러므로 이전 슬라이드에서 언급했던 covariate shift 문제로 인해 영향을 받게 되는 것입니다. 그러므로 배치 정규화가 하는 것은, 이런 숨겨진 유닛의 분포가 왔다 갔다 이동하는 양을 줄여줍니다. 여기 숨겨진 유닛 값들에 대해 분포도를 그리자면, 이것은 엄밀히 이야기하면 정규화 된 Z 값입니다. 그러면 실제로 이것은 Z_2_1 그리고 Z_2_2입니다. 그리고 저는 4개의 값 대신 2개의 값을 나타내어 2차원의 그래프에서 보도록 하겠습니다. 배치 정규화가 주장하는 것은 for Z_2_1 Z 와 Z_2_2의 값이 변할 수 있다는 것입니다. 그러면 정말로 신경망이 이전에 있는 층들의 파라미터들을 업데이트하면서 그 값이 변할 것입니다. 하지만 배치 정규화가 보장하는 것은, 어떻게 변하더라도, Z_2_1 와 Z_2_2 의 평균값과 variance는 똑같을 것이라는 점입니다. 그러므로 구체적으로 Z_2_1 와 Z_2_2 값이 변하더라도 그들의 평균값과 variance는 변동없이 그대로 각각 0과 1의 값을 유지할 것입니다. 꼭 평균값 0과 variance 1이 아닐수는 있는데요, 어쨋든 베타2와 감마2에 지배되는 값으로 변동없이 유지될 것입니다. 신경망이 결정하는 경우에는, 평균값 0과 variance 1을 강요할 수 있습니다. 또는 다른 값으로 지정할 수도 있겠죠. 이것이 본질적으로 하는 것은 이전의 층에서 파라미터가 업데이트 되면서 세번째 층에 영향을 줄 수 있는 양을 제한시켜 줍니다. 이 변화량은 세번째 층이 새로 배워야 하는 양이기도 하구요. 그러므로 배치 정규화는 입력값이 변해서 생기는 문제를 줄여줍니다. 여기 이런 값들을 더 안정감 있게 해주고, 그럼으로 인해, 신경망의 나중의 층들이 그 자리를 지킬 수 있게 틀을 마련해줍니다. 입력값의 분포도가 약간 변하기는 하지만, 더 작게 변하고, 이것이 하는 것은 이전 층이 계속 러닝하면서도, 여기 이 부분이 다른 층에게 압박하는 다음 층들이 적응할 수 있도록 배워야 하는 양이
줄어드는데요, 다르게 말하자면, 앞에층의 파라미터가 배워야하는 것들과 뒷층의 파라미터들이 해야하는 것들의 coupling을 약화시켜줍니다. 그렇게해서 각각의 층이 스스로 러닝을 진행할 수 있도록 해주는 것이죠. 다른 층과는 조금 더 독립적으로 말이죠. 이것은 결과적으로 전체 네트워크의 속도를 올려주는 효과를 줍니다. 이것이 여러분의 직관에 도움을 줬으면 합니다. 중요한 것은, 배치 정규화의 뜻인데요, 특히 신경망에서 나중의 층의 시각에서 봤을 때 말입니다. 이전 층은 별로 이동폭이 크지 않습니다. 똑같은 평균값과 variance를 요하기 때문이죠. 이것이 다른 층들이 배우는 업무 과정을 쉽게 해줍니다. 배치 정규화는 두번째 효과도 있는데요, 일반화 효과가 있습니다. 직관적이지 않은 부분은 바로 각각의 미니 배치에 대해서 X_t라고 하겠는데요, Z_t의 값을 갖습니다. 그리고 Z_l의 값을 갖구요, 오로지 여기 미니 배치에서 산출된 scale된 평균값과 variance를 갖습니다. 그러면, 여기 미니 배치 에서 산출된 평균값과variance는 전체 데이터세트에서 계산된 값보다 더 많은 noise를 함유하고 있기 때문에, 미니 배치에서만 산출됐기 때문이죠, 예를 들어, 64 또는 128 또는 256 또는 더 큰 트레이닝 예시일 수 있습니다. 그렇게 mean 과 variance가 더 작은 데이터 샘플에서 계산됐기 때문에 더 noisy 합니다. 그렇기 때문에 scaling 절차도, Z_l 에서 Z_2_l 로 가는 절차가 더 noisy 합니다. 그 이유는 더 noisy한 mean과 variance로 산출됐기 때문이기도 하죠. 그렇기 때문에 dropout과 비슷하게 각각의 hidden layer activation에 noise를 더합니다. dropout 이 noise가 있는 방식은 숨겨진 유닛을 가지고 0에 곱하고 어떨 확률에 곱합니다. 그리고 1로 어떤 확률을 곱합니다. 그러면 dropout은 복수의 noise가 있는데요, 0 또는 1로 곱해지기 때문이죠, 반면에 배치 정규화는 복수의 noise가 있습니다. scaling을 표준편차 때문에도 그렇고, 추가적인 noise도 있습니다. 평균값을 빼기 때문이죠. 여기 평균값과 표준편차의 추정치가 noisy합니다. 그러므로 dropout과 비슷하게 배치 정규화 도 약간의 일반화 효과가 있습니다. 여기 숨겨진 유닛에 noise를 더하여서 밑에 있는 숨겨진 유닛들이 어느 특정 숨겨진 유닛에 의존하지 않게 하기 때문입니다. 그리고 dropout과 비슷하게, 숨겨진 층에 noise를 더하여 아주 작은 일반화효과가 생깁니다. 더해지는 noise가 꽤 작기 대문에, 큰 일반화효과는 아닙니다. 그리고 배치 정규화를 dropout과 같이 사용할수도 있고 dropout에서 조금 더 큰 일반화 효과가 생기길 바라면 배치 정규화를 dropout과 같이 사용할수도 있습니다. 그리고 또 직관적이지 않을 수 있는 부분은, 더 큰 미니 배치 사이즈를 이용하면 예를 들어, 64 대신에 512를 쓴다고 해보죠 이렇게 더 큰 미니 배치 사이즈를 이용하면 여기 noise를 줄이고 결과적으로 일반화효과를 줄이는 것입니다. 이것이 이상한 dropout의 특성입니다. 바로 더 큰 미니 배치 사이즈를 사용하여 일반화효과를 줄인다는 것입니다. 이것을 말하기 했지만 저 같은 경우엔, 배치 정규화를 일바화로 
사용하지 않을 것입니다. 그것이 배치 정규화의 의도가 아닙니다. 이것을 추가적으로 의도할 수도 있고 안할수도 있는데요 실제로 읿나화로 배치 정규화를 만들지 마십시요. 정규화하는 방편으로 사용하세요. 이렇게해서 hidden unit activation이 러닝에 빨라질 수 있게 말이죠. 그리고 저는 일반화가 거의 의도치 않은 부작용이라고 생각합니다. 이제 배치 정규화가 하는 직관적인 부분을 다웠는데요 , 배치 정규화 에 대한 내용을 정리하기 전에, 여러분이 알아야하는 내용이 한가지 있습니다. 그것은 배치 정규화가 한대의 미니 배치씩 데이터 처리한다는 것입니다. 이것은 미니 배치 에서 평균값을 계산하고, variance를 계산합니다. 테스트타임에서는 예사하는 것들을 만드려고 하고, 신경망을 평가하려 합니다. 여러분은 미비 배치 예시가 없을 수도 있는데요 여러분은 1개의 예시씩 처리하고 있을수도 있습니다. 이런 경우, 테스트타임에 무엇을 해야합니다. 조금다르게 말이죠. 예상하는 것들이 말이 되게요. 다음에 있을 마지막 배치 정규화에 대한 비디오에서는 여러분이 아셔야 하는 냉요에 대해, 트레이닝 된 신경망으로 배치 정규화를 이용하여 예상하는 방법을 배우겠습니다.