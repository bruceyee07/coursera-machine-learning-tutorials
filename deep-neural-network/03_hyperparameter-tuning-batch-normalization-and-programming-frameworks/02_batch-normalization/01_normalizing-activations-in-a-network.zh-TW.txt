在深度學習的崛起中 最重要的構想之一是一個演算法，叫做
批次標準化(Batch Normalization) 由Sergey Ioffe和Christian Szegedy這兩位研究者提出 批次標準化讓你尋找超參數更加簡單 讓你的神經網路能 比較不受超參數選擇的影響，
會有更大範圍的超參數能表現得好 就算是很深的網路，也能讓你簡單地訓練之 就讓我們看看批次標準化是怎麼運作的 當訓練一個模型，例如羅吉斯迴歸分析，
你可能還記得 把輸入的特徵標準化，能夠加速學習。
你算出平均， 把你的訓練資料減掉平均 算出變異數 這個 X(i) 平方和 這邊的平方是逐個元素的 然後根據這些變異數來標準化你的資料。 而在先前的影片我們看到，這可以把 你的問題的等高線
從很狹長的形狀變得比較圓 讓像是梯度下降法這樣的演算法
更容易去最佳化。 所以，對於神經網路或羅吉斯迴歸的「輸入值」 這樣的標準化有效 那麼，深一點的模型呢？ 你不只有輸入特徵 x，
在這一層你有啟動值 a[1] 在這一層你有啟動值 a[2]，依此類推 所以如果你要訓練參數，例如 W[3], b[3] 如果你能把 a[2] 的平均和變異數給標準化 讓 W[3], b[3] 的訓練更有效率，這不是很好嗎 在羅吉斯迴歸分析的例子，我們看到標準化 x1, x2, x3 可能讓訓練 W 和 b 更有效率 那麼問題來了：對於任何一個隱藏層，
我們能否標準化 a 的值 — 假設在這例子是 a[2] 雖然可以是任何一個隱藏層 — 來讓訓練 W[3], b[3] 變得更快呢？ 因為 a[2] 是下一層的輸入， 因此影響到 W[3] 和 b[3] 的訓練 所以這個就是批次標準化，
簡稱 Batch Norm， 所做的。 雖然技術上來說，我們是標準化 Z[2] 的值
而不是 a[2] 在深度學習的文獻中，這邊有些爭論： 你要在啟動函數之前，
也就是 Z[2]，先做標準化呢？ 還是過了啟動函數的 a[2] 做標準化呢？ 實務上，對Z[2]標準化比較常見 所以我講的是這個版本 我建議這是你的第一選擇。 那麼，接下來是實作批次標準化的方法 給你神經網路一些中間的值 假設你有一些隱藏單元的值 z(1) 到 z(m) — 其實這是 從某個隱藏層來的，所以更精確的說， 這要寫做z在某個隱藏層，然後 i=1~m。 不過方便起見，我把這個方括號 [l] 省略掉， 讓符號變簡單。 所以有了這些值，你要做的是這樣子計算平均 — 再說一次，這裡都是對於某一層 l 而言，
只不過我省略了 [l] — 然後你用你知道的公式來計算出變異數 然後你對於每一個 Z(i) 去標準化 所以你會得到 Z(i) norm 等於減掉平均， 然後除以標準差。 為了數值運算的穩定性，
通常你都會加 epsilon 到分母 以免萬一sigma平方在某些情況是零。 所以我們拿了這些Z的值， 做了標準化，讓他們平均為0，變異數為1 也就是Z的每個維度都是平均0，變異數1 不過我們不希望這些隱藏單元
平均總是0，變異數總是1 或許不同的隱藏單元會有不同的分布，這也合理。
所以我們真正要做的，是計算 z tilde = gamma * z(i) norm + beta 這邊 gamma 和 beta 是模型中待學的參數 所以我們可用梯度下降法，或是其他演算法像是 動量法、RMSprop、或 Adam 優化法，
你就能更新參數 gamma 和 beta 就像你更新神經網路的權重一樣 那麼，注意到這邊 gamma 和 beta 的效果是 他能讓 Z tilde 的平均變成任何你想要的。 實際上，如果 gamma 等於 sigma^2 + epsilon 開根號， 萬一 gamma 等於這個分母， 而且萬一 beta 等於上面這個 mu， 那麼 gamma * Z_norm + beta 的效果
會抵銷掉上面這個方程式 所以，萬一這是真的 那麼 Z(i) tilde 會等於 Z(i) 所以當參數 gamma 和 beta 在某種恰巧的設定下， 這整個標準化的步驟，也就是 這邊四個方程式，本質上就是一個恆等函數
(identity function) 不過如果 gamma 和 beta 取不同的值，這就會讓 隱藏單元有不同的平均和變異數。 所以要把這套用到你的神經網路的方法是 原本之前用 Z(1), Z(2), ... 的情況， 你現在轉而用 Z(i) tilde，而不是 Z(i) 來給你的神經網路做後面的運算 然後你會把這個 [l] 擺回去，指明是哪一層 你可以擺回去那。 我希望你能學到的概念是，我們之前看過 把輸入特徵X標準化，
對神經網路的學習有所幫助 而批次標準化所作的，
是把標準化的程序 運用在不只是輸入層上， 而是還更深入神經網路的隱藏層。 所以你套用這樣的標準化步驟，來標準化 某個隱藏層的值 Z 的平均和變異數。 但是，對於輸入和隱藏層，這兩者不同之處在 你可能不希望隱藏層的值被迫平均為0，變異數為1 舉個例子，如果你的啟動函數是S型函數 你並不希望你的值總是擠在這裡 你可能希望變異數大一些，或平均非零， 以善用S型函數的非線性區， 而非讓你的值都落在這線性區間。 這就是為何有了gamma和beta這兩個參數後 你能確保你的 Z(i) 會有你想要的範圍 或者說，實際上這是確保隱藏層 能有規範好的平均和變異數 其中平均及變異數是由兩個參數所控制： gamma和beta，而這是由學習演算法自己設定的 所以，這真正在做的是
標準化這些隱藏層的平均和變異數 也就是Z(i)的，讓他們有個固定的平均和變異數 而這個平均和變異數，可能是0與1，
也有可能是其他的值 由參數gamma和beta所控制之。 我希望這能給你實作批次標準化的概念 至少，對於神經網路內單一的一層 在下部影片，我想示範在神經網路上套用批次標準化 甚至是很深的網路， 還有怎麼用在網路的很多層。 接下來，我們會講更多的概念： 為什麼批次標準化對訓練神經網路有幫助 萬一你覺得這背後原理還很撲朔迷離，
跟著我 在接下來的兩部影片，我們會一一釐清