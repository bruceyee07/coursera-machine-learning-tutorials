那麼，為什麼批次標準化會有效呢？ 這邊是一個理由，你見過把輸入的特徵標準化 也就是 X，標準化成平均為0變異數為1， 這樣能加快訓練 所以與其於有些特徵介在0到1之間， 而有些介在1到1000， 如果把所有的輸入特徵X標準化 到某個類似的範圍，就能加快訓練 所以，一個批次標準化能有效的直覺是 這一樣在做類似的事 只是是對隱藏單元，而不是你的輸入 其實，從這角度看批次標準化還不夠全面 還有其他更多的看法 能夠幫你更了解批次標準化在做什麼 就讓我們在這部影片一窺究竟 批次標準化有用的第二個理由 是讓你的神經網路 比較後面的權重 例如說第十層，能比較不受 網路比較前面的權重，例如第一層，的改變所影響 為了解釋我說的 讓我們看這實際的例子 看看神經網路的訓練 可能是個淺的網路 例如羅吉斯迴歸分析，或者是神經網路 可能是像羅吉斯迴歸那樣淺的網路，也可能是深的網路 例如我們有名的貓貓辨認器 但是，假設你的訓練資料都是黑貓 如果你現在想把這個網路套用到 其他顏色的貓的資料 也就是正面資料不光是像左邊這些黑貓 也有像右邊這樣有色的貓 那麼，你的分類器可能不會表現得好 用圖來表示，如果你的訓練資料像這樣 這邊有些正面資料，還有些反面資料在這 而你想要一般化至 某些資料，其中正面的資料在這，而反面的資料在這 那你大概不能預期用左邊資料 訓練的模型能在右邊的資料上表現好 儘管可能真的存在一個函數，能在兩邊都表現好 但你不能期待你的學習演算法
能發掘這條綠色的分界線 如果只能靠左邊的資料 所以，這種資料分布改變的觀念 有個有點酷的名字：共變量偏移(covariate shift) 這觀念是這樣的： 假設你學到某個 X 到 Y 的對應 如果 X 的分佈改變 你有可能就需要重新訓練你的演算法。 即便是下面的情況，這也是真的： 就算關於事實的函數，也就是 X到Y的對應不改變， 這觀念仍然是對的。在這例子就是如此 因為這邊關於事實的函數 是「這張圖片是否為貓」。 而重新訓練的需要會更加 迫切，或可說是更加嚴重－如果背後事實的函數也變動 那麼，共變量偏移這個問題，和神經網路有什麼關係？ 看看上面這個很深的網路 我們從這一個隱藏層的角度，來看學習的過程 這個第三層 所以這個網路目前學到了參數W[3]和b[3] 從第三層的角度來看 他從前面那些層拿到了一些值 然後必須做些事，希望 讓輸出的 y hat 和事實的 y 很相近 那讓我先把左半部遮住一下 所以從第三個隱藏層的角度來說 他得到一些值 假設叫 a[2]_1, a[2]_2, a[2]_3, a[2]_4。但是這些值也可看作是特徵 x_1, x_2, x_3, x_4，而這第三個隱藏層的作用是 拿這些值，想辦法把他們對應到 y hat 所以你可以想像在梯度下降的時候 這些參數 W[3], b[3]，還有 W[4], b[4] 甚至 W[5], b[5]，這些參數會去學習 讓這個網路能表現好 把左邊我用黑色圈起來的這些值，對應到輸出 y hat 不過呢，我們現在把左半部遮掉的拿回來 這個網路也含有參數 W[2], b[2] 和 W[1], b[1] 所以當這些參數改變的時候 這些值，a[2]，也會跟著變 所以從第三層的角度來看 這些隱藏單元的值一直不斷地在更改 所以這會遭受到前張投影片 共變量偏移的問題 因此，批次標準化所做的 是減少這些隱藏單元的分佈變動的幅度 如果我們把這些隱藏單元的分佈畫出來 技術上我們是標準化 Z 所以這邊是 z[2]_1 和 z[2]_2 還有我只畫了兩個值，而不是四個 這樣在 2D 上可以看到 批次標準化告訴我們的是 z[2]_1 和 z[2]_2 的值可以改變 而且他們真的會變，因為神經網路 會更新前面幾層的參數 但批次標準化會保證無論怎麼變 z[2]_1 和 z[2]_2 的平均和變異數還是會一樣 所以就算 z[2]_1 和 z[2]_2 的值變了 他們的平均和變異數至少仍然是0和1 或者說，其實不見得是平均0變異數1 而是由 beta[2] 和 gamma[2] 控制的值 如果是這神經網路所選擇的 可以讓平均變 0，變異數變 1 也可以是其他任何的值。 不過，這邊真正所做的 是他限制了前面幾層更新參數時 對後面值的分佈的影響幅度 也就是第三層看到的、所要學的值。 因此，批次標準化減輕了輸入變化時帶來的問題 他真的讓這些值更穩定 使這網路後面的層能有更堅固的地基。 就算輸入的分佈有變 變的也比較少；這樣的意思是， 就算前面的層不斷地學習 這種「迫使後面的層盡早適應」 這樣子的症狀其實會減輕。 也可以這麼說，就是他減弱了 前面那些層的行為 和後面那些層行為之間的羈絆。 所以，這讓每一層能自我學習， 稍微比較不依靠其他層 如此一來，這會加快整個網路的訓練。 我希望這給了你更好的概念 不過重點在於，批次標準化意味著 特別是從神經網路比較後面的層的角度來說 前面的層變動不會太大 因為他們被限制在相同的平均和變異數 這讓後面隱藏層的學習比較容易 實際上，批次標準化有第二種功效 他可達到稍微的正規化 一個不大直覺的事情是，每個小量資料 假設某批小量資料 Xt 含有值 z[t] (口誤) 含有值 z[l] 標準化時，只看那一小批資料的平均和變異數 那，因為平均和變異數是從 小量資料算出，而不是整個資料集的， 那樣的平均和變異數會有些噪音 因為這只是從你的小批資料算出 可能只有64筆，或128筆 或是256筆或多一點的訓練資料。 所以因為平均和變異數有點噪音
－因為是從 比較少數的資料估計而出 － 那麼 從 z[l] 到 z tilde[l] 這樣的縮放 這個過程也會有一點噪音 因為這是從有些噪音的平均和變異數所算出的 所以和 Dropout 類似， 他會加一些噪音到每個隱藏層的啟動值。 Dropout 能提供噪音 是因為他會有一定機率把一個隱藏單元乘以 0 和有一定機率乘以 1 所以 Dropout 會有多個噪音，因為他會乘以 0 或是 1， 而批次標準化會有多個噪音，因為他會依標準差縮放， 還有加法的噪音，因為他會去減平均， 而其中，對於平均和標準差的估計是有噪音的。 因此，和Dropout類似， 批次標準化也有一點正規化的功效。 這是因為，如果把隱藏單元加上噪音， 這會強迫之後的隱藏單元不能太依賴
任何一個隱藏單元。 因此，和Dropout類似， 他加了一些噪音在隱藏層上，所以會有些許的正規化效果。 而因為加的噪音很小， 所以正規化的效果不是很大， 你可以選擇同時利用批次標準化和Dropout， 你可以選擇同時利用批次標準化和Dropout— 如果你想要比Dropout更強的正規化效果。 此外，可能還有一個不直覺的效果是， 如果你的一小批次的資料比較多 假設你的小量批次(mini-batch)的大小是 512 而不是 64， 當這小批次比較大的時候 你會減少噪音，所以也會減小正規化的效果。 所以這是Dropout一個神奇的特點 也就是小量批次比較大的時候， 你會減小正規化的效果。 話雖如此，我不會真的把批次標準化
當作一個正規化的工具 這不是批次標準化想做的 不過有時他還是有意或無意地，
在學習時產生這樣的效果 不過真的，不要把批次標準化看作是正規化 把他當作一種手段來做標準化 你的隱藏單元的啟動值，來加速學習 我想隨之而來的正規化只是無意的副作用罷了 那麼，我希望這能讓你對批次標準化做的事更有概念 在結束批次標準化的討論之前， 我還想確定你知道另一個細節 也就是，批次標準化一次處理一小批的資料 他計算的是一批資料的平均和變異數。 那麼在測試的時候， 你做了一個預估，想評估這個神經網路 你不一定會有一批一批的資料 你可能一次只處理一筆資料 所以在測試階段，你必須要做些 稍微不一樣的事，確保你的預測有道理 在下一部，也是最後一部關於批次標準化的影片， 讓我們談談你必須做的那些細節，使得 你用批次標準化訓練出的神經網路，也能夠拿來預估