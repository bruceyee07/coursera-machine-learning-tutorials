1
00:00:00,000 --> 00:00:03,685
批次標準化一次處理一小批(mini batch)資料

2
00:00:03,685 --> 00:00:07,760
不過在測試階段，你可能一次只處理一筆資料

3
00:00:07,760 --> 00:00:10,585
讓我們瞧瞧如何調整你的網路應付之

4
00:00:10,585 --> 00:00:12,233
還記得訓練的時候

5
00:00:12,233 --> 00:00:15,260
這些是用來實作批次標準化(batch norm)
的方程式

6
00:00:15,260 --> 00:00:17,025
在單一個小量批次中

7
00:00:17,025 --> 00:00:22,610
你把這一批的資料 z(i) 的值全加起來，算出平均

8
00:00:22,610 --> 00:00:25,910
在這邊，你只是對於一批資料加起來

9
00:00:25,910 --> 00:00:28,440
我用 m 來代表這一批資料的個數

10
00:00:28,440 --> 00:00:31,870
不是整個訓練資料的

11
00:00:31,870 --> 00:00:35,955
然後你計算變異數，然後算出 z norm

12
00:00:35,955 --> 00:00:41,325
藉由平均和標準差縮放得出，還有
分母多加 epsilon 保持數值運算穩定

13
00:00:41,325 --> 00:00:47,100
然後把 z norm 以 gamma 和 beta 再次縮放，
得到 z tilde

14
00:00:47,100 --> 00:00:51,995
那麼，注意這邊的 mu 和 sigma^2

15
00:00:51,995 --> 00:00:57,620
也就是拿來縮放的，他們是對一小批資料整個去計算

16
00:00:57,620 --> 00:01:00,495
但是在測試階段，你可能一次不會處理

17
00:01:00,495 --> 00:01:05,240
一整批的64, 128 或 256 筆的資料

18
00:01:05,240 --> 00:01:07,815
所以你需要另一種方法導出 mu 和 sigma^2

19
00:01:07,815 --> 00:01:10,290
如果你手上只有一筆資料

20
00:01:10,290 --> 00:01:15,060
算那一筆的平均和變異數，這根本沒道理

21
00:01:15,060 --> 00:01:16,860
那到底怎麼做呢？

22
00:01:16,860 --> 00:01:21,240
為了在測試階段套用你的神經網路

23
00:01:21,240 --> 00:01:25,735
我們要有不同方法來估計出 mu 和 sigma

24
00:01:25,735 --> 00:01:27,975
在批次標準化典型的實作中

25
00:01:27,975 --> 00:01:32,145
你通常會這樣估計：

26
00:01:32,145 --> 00:01:37,020
利用指數加權平均(exponentially weighted average)

27
00:01:37,020 --> 00:01:42,678
來平均過眾多的小批資料

28
00:01:42,678 --> 00:01:45,900
要講的更精確的話，

29
00:01:45,900 --> 00:01:51,750
假設我們選了某層 l，我們依序用了 x1,

30
00:01:51,750 --> 00:01:57,500
x2 (與其對應的y), ... 這些小批資料

31
00:01:57,500 --> 00:02:02,280
那，在訓練第 l 層的 x1 時

32
00:02:02,280 --> 00:02:06,383
你算出某個 mu[l]，實際上

33
00:02:06,383 --> 00:02:12,485
我要把他表示成那一層的第一批的mu

34
00:02:12,485 --> 00:02:15,540
然後你訓練那一層的第二批小量資料

35
00:02:15,540 --> 00:02:19,055
你會得到第二個 mu

36
00:02:19,055 --> 00:02:23,194
然後對於這一層的第三批，

37
00:02:23,194 --> 00:02:29,090
你得出第三個 mu

38
00:02:29,090 --> 00:02:31,830
所以，如同我們之前利用

39
00:02:31,830 --> 00:02:37,600
指數加權平均來平均那些 theta_1, theta_2

40
00:02:37,600 --> 00:02:40,020
theta_3, 就是你計算

41
00:02:40,020 --> 00:02:44,173
溫度的指數加權平均的那時候

42
00:02:44,173 --> 00:02:47,250
你會追蹤你看到的

43
00:02:47,250 --> 00:02:50,790
最新的加權平均向量

44
00:02:50,790 --> 00:02:54,195
所以，那時候學的指數加權平均

45
00:02:54,195 --> 00:03:00,120
就可用來估計這一層的這些z的平均

46
00:03:00,120 --> 00:03:03,930
同樣的，你也利用指數加權平均來追蹤

47
00:03:03,930 --> 00:03:09,015
你在這層看到的第一批小資料的 sigma^2,

48
00:03:09,015 --> 00:03:13,510
第二批小資料的 sigma^2, 等等等

49
00:03:13,510 --> 00:03:18,780
所以當你在訓練網路，訓練過眾多的小批資料時，

50
00:03:18,780 --> 00:03:24,535
在每一層，你記錄當時看過的 mu 和 sigma^2
的移動平均

51
00:03:24,535 --> 00:03:26,895
最後，在測試階段

52
00:03:26,895 --> 00:03:30,275
你要做的是在這一個方程式內

53
00:03:30,275 --> 00:03:35,855
在計算 z norm 時，拿你看到的 z，

54
00:03:35,855 --> 00:03:39,735
利用前面記錄到最後的

55
00:03:39,735 --> 00:03:45,340
mu和sigma^2的指數加權平均來做縮放

56
00:03:45,340 --> 00:03:48,780
然後這一筆測試資料的 z tilde

57
00:03:48,780 --> 00:03:53,670
是這樣計算的：拿剛剛左邊的 z norm，

58
00:03:53,670 --> 00:03:57,240
再利用在訓練網路時

59
00:03:57,240 --> 00:04:02,695
學到的參數 beta 和 gamma 來算出

60
00:04:02,695 --> 00:04:07,020
因此，這邊的要點是，在訓練途中

61
00:04:07,020 --> 00:04:11,620
mu和sigma^2是算在整個小批資料上，

62
00:04:11,620 --> 00:04:14,580
可能是 64, 128, 或其他數量的資料。

63
00:04:14,580 --> 00:04:18,345
但是在測試階段，你可能需要一次處理一筆資料

64
00:04:18,345 --> 00:04:21,605
所以，我們用估計的方式，估計出

65
00:04:21,605 --> 00:04:25,325
你訓練集的mu和sigma；這有很多種方式

66
00:04:25,325 --> 00:04:27,450
理論上，你可以讓最終的網路

67
00:04:27,450 --> 00:04:30,960
跑過所有的全部的訓練資料，以得到mu和sigma^2，

68
00:04:30,960 --> 00:04:33,550
但實務上，通常大家實作是用

69
00:04:33,550 --> 00:04:36,330
指數加權平均，也就是你追蹤

70
00:04:36,330 --> 00:04:38,970
訓練的過程中得到的mu和sigma^2，

71
00:04:38,970 --> 00:04:42,130
利用指數加權平均的方式—

72
00:04:42,130 --> 00:04:44,095
有時這也被稱作「移動平均」

73
00:04:44,095 --> 00:04:46,330
—來得到mu和sigma^2大概的估計，

74
00:04:46,330 --> 00:04:49,800
再把那mu和sigma^2的估計值

75
00:04:49,800 --> 00:04:55,860
在測試階段用於隱藏層的z來縮放。

76
00:04:55,860 --> 00:04:58,980
實務上，這整個流程

77
00:04:58,980 --> 00:05:03,125
對於你用哪種方式估計mu和sigma^2，都滿穩定的

78
00:05:03,125 --> 00:05:06,440
所以我不會太擔心你怎麼去進行

79
00:05:06,440 --> 00:05:09,729
而且如果你使用的是深度學習的框架

80
00:05:09,729 --> 00:05:13,080
通常他們會有個預設的方式去估計

81
00:05:13,080 --> 00:05:17,665
mu和sigma^2，大概表現得都不錯。

82
00:05:17,665 --> 00:05:21,965
實際上，只要估計的方式很合理

83
00:05:21,965 --> 00:05:28,600
任何合理估計隱藏層z的平均和變異數的方法，
在測試時應該都能表現得好

84
00:05:28,600 --> 00:05:31,270
那麼，這就是批次標準化及其使用法

85
00:05:31,270 --> 00:05:33,520
我想你已經能訓練更深的網路

86
00:05:33,520 --> 00:05:37,205
還有讓你的學習演算法跑得飛快

87
00:05:37,205 --> 00:05:38,870
在結束這周之前，

88
00:05:38,870 --> 00:05:43,080
我還想分享一些關於深度學習框架的想法

89
00:05:43,080 --> 00:05:46,000
我們在下一段影片
來開始談這個