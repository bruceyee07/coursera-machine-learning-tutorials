Pekala, neden toptan normalleştirme çalışır? Bir sebebi şu, girdi özelliklerini nasıl normalleştireceğimizi görmüştünüz, x'lerin, ortalamasını 0'a ve değişintisini 1'e eşitlediğimizi, ve bunun öğrenmeyi nasıl hızlandırdığını görmüştük. Dolayısıyla 0'dan bire uzanan veya 1'den 1000' uzanan özelliklerine sahip olmaktansa, bütün girdi özelliklerini normalleştirerek, yani benzer aralıklar kazanmalarını sağlayarak öğrenme hızını artırmıştık. Dolayısıyla, toptan normalleştirmenin neden çalıştığını anlamada bir iç görü, toptan normalleştirmenin de benzer bir plan çizdiğini anlamaktır. Farkı artı olarak ise, sadece girdi özellikleri değil, gizli üniteler de bundan etkilenir. Şimdi, bu toptan normalleştirmenin ne yaptığının küçük bir resmi. Toptan normalleştirmenin yaptığını daha derin anlamanıza yardım edecek birkaç farklı sebep daha var. Hadi bu videoda bunları inceleyelim. Toptan normalleştirmenin neden çalıştığı ile ilgili verilebilecek ikinci sebep ise, ağırlıkların değişiminin, katmanların daha sonra veya daha önce gelmesine göre değişmesidir. diyelim ki katman 10'daki ağırlıklar, değişmeye karşı daha erken katmandaki ağırlıklardan daha dirençli olacaktır örneğin birinci katman. Söylemek istediğimi açıklamak için, buradaki canlı örneğe bakalım, Hadi ağ üzerinde bir eğitim görelim, belki dar bir ağ, s biçimli bağlanım veya belki sinir ağı, belki de bu bağlanım gibi dar bir ağ belki de derin ağ, örneğimiz ünlü kedi tanımlama örneği üzerinde olacak. Diyelim ki, veri setinizi bütün siyah kedilerin görüntüleri üzerinde eğittiniz. Eğer şu anda bu ağı pozitif örneklerin sadece soldaki gibi siyah kediler olmadığı, sağdaki gibi renkli kedilerin de olduğu veri setine uygularsanız, bu durumda sınıflandırıcınız iyi sonuç alamayabilir. Dolayısıyla eğer eğitim setiniz, pozitif örneklerin burada ve negatif örneklerin burada olduğu gibi gözüküyorsa ve bu eğitim setinizi genelleştirmeye çalışırken, pozitif örneklerin burada ve negatif örneklerin burada olduğu bir veri setinde genelleştirmeye çalışıyorsanız, bu durumda soldaki eğitim setinde eğittiğiniz modülün sağdaki veri setinde iyi sonuç almasını bekleyemezsiniz. Fonksiyon iyi çalışıyor olmasına rağmen, öğrenme algoritmanızın sadece soldaki verilere bakarak yeşil karar sınırını keşfetmesini bekleyemezsiniz. Dolayısıyla, bu veri dağılımının değişmesi fikrine, şık bir terim gibi gözüken, eş-değişinti kayması adı verilir. Ve fikir ise tam olarak şu; Eğer bir x'ten y'te eşlemeyi öğrendiyseniz, ve eğer x'in dağılımı değişirse, o zaman öğrenme algoritmanızı tekrar eğitmek zorundasınız. Ve bu x'ten y'ye eşlemeyi yapan fonksiyon, temel doğru fonksiyonu, değişmeyecek olsa bile doğrudır. Örneğin bu örnekte değişmemiştir, çünkü temel doğru fonksiyonu bu örnekte bu görüntü kedi mi değil mi'dir. ve fonksiyonunuzu değiştirme ihtiyacı, eğer temel doğru fonksiyonunuz kayarsa daha acil veya daha önemli bir hal alır. Pekala, eş-değişinti kayması problemi sinir ağlarına nasıl uygulanır? Bunun gibi derin bir ağ düşünün, ve gelin bu belirttiğim katmanın açısından öğrenme işlemine bakalım. yani üçüncü katmanın, bu ağ w3 b3 parametrelerini öğrendi, ve üçüncü gizli katmanın açısından bakıldığında, bu katman daha önceki katmanlardan bir takım değerler alıyor, ve bu değerlerle işlemler yapmak zorunda ve bu işlemlerin sonucunda temel doğru değeri y'ye en yakın değerleri vermeye çalışmakta. Dolayısıyla bana izin verin size sol tafafında ne yaptığını açıklıyım. bu üçüncü gizli katmanın açısından, bazı değerler alıyor, diyelim ki a21,a22,a23,a24, fakat bu değerler yine aynı zamanda özellikler de olabilir yani x1,x2,x3,x4 ve üçüncü gizli katmanın görevi ise bu değerleri alıp y şapka'ya eşlemenin yolunu bulmak. Yani, eğim azaltma yöntemi sırasında, bu değerler w3,b3 ve aynı zamanda w4, b4, ve hatta w5, b5, ağın iyi bir iş çıkarması için, soldaki siyahla yazdığım değerlerden çıkış verisi y'ye eşleme yaparken bu parametreleri öğrenmeye çalışıyor. Fakat şimdi ağın sol tarafını tekrar açığa çıkaralım. Ağ aynı zamanda w2,b2 ve w1,b1 parametrelerinde değişiklik yapıyor, dolayısıyla bu parametreler değiştikçe, bu değerler a2 de değişiyor. Dolayısıyla üçüncü gizli katmanın açısından bakıldığında, bu gizli ünite değerleri her zaman değişiyor ve dolayısıyla daha önceki slaytta konuştuğumuz eş-değişinti kayması problemi ortaya çıkıyor. Toptan normalleştirmenin yağtığı şey, gizli ünite değerlerinin dağılımının değişim miktarını kısıtlamaktır ve bu gizli ünite değerlerinin dağılımını çizecek olursak, buna teknik olarak Z normalleştiricisi deniyor. dolayısıyla burada z21 bu , z22 ise bu oluyor, aslında burada dört değer yerine iki değer çizdim, dolayısıyla iki boyutta görselleştirebiliriz. Toptan normalleştirmeninin dediği şey ise şu, z21 ve z22 değerleri değişebilir, ve sinir ağı önceki katmanlardaki parametreleri güncellediğinde bakarsak gerçekten değişecektir, fakat toptan normalleştirmenin temin ettiği şey ise şudur nasıl değişirse değişsin, z21 ve z22'nin ortalaması ve değişintisi sabit kalacaktır. Dolayısıyla z21 ve z22'nin tam değerleri değişse bile, onların ortalamaları ve değişintileri ortalama 0 ve değişinti 1 olacak şekilde aynı kalacaktır. ya da beta2 ve gamma2 şeklinde olacaktır, fakat 0 ve 1 olması zorunlu değildir. Eğer sinir ağı seçerse, bu değerleri ortalama 0 ve değişinti 1 olacak şekilde yapabilir ya da herhangi bir ortalama ve değişinti değeri olacak şekilde yapabilir. Burada yapılan şey genel olarak, önceki katmanlarda yapılan güncellemelerin üçüncü gizli katman değerlerinin dağılımını etkileme derecesini azaltmak ve dolayısıyla öğrenme miktarını azaltmak olarak söylenebilir. Dolayısıyla, toptan normalleştirme girdi değerlerinin değişme problemini azaltır, bu değerlerin daha kararlı hale gelmesine yol açar, ve bu şekilde sinir ağındaki ileri katmanlardaki değerler için daha sağlam bir zemin hazırlar. ve girdi değerlerinin dağılımı bir miktar değişse bile, sonraki değerler daha az değişir ve bu önceki katmanların öğrenmeye devam etmesi durumunda bile, bunun sonraki katmanlarda değişim oluşturma miktarını kısıtlaması veya azaltması veya değiştiği andaki tepkisinin azalması anlamına gelir, diğer bir değişle, bu erken katmanların yaptıklarıyla daha sonraki katmanların yaptıklarının eşleşmesinin zayıflaması anlamına gelir. ve bu ağdaki bütün katmanların diğer katmanlardan biraz daha bağımsız olarak, kendi başlarına öğrenebilmelerine olanak tanır, ve bu bütün ağın öğrenme hızının artması anlamına gelir. umuyorum ki bu biraz sezgi kazandırmıştır, eve götürülecek nokta şu: toptan normalleştirme, özellikle sinir ağının daha sonraki katmanları açısından bakıldığında, önceki katmanların eskiden olduğu kadar kaymasını engeller, çünkü aynı ortalama ve değişinti'ye sahip olma şartları vardır, ve bu daha sonraki katmanların öğrenmelerini daha kolaylaştırır. Görünen o ki toptan normalleştirme ikinci bir etkiye sahip, bir miktar düzenlileştirme etkisi. Toptan normalleştirmenin sezgisel olmayan bir özelliği, her küçük yığının, örnek verecek olursak küçük yığın X_t, Z_l değerlerine sahip olsun. Bu küçük yığın sadece o küçük yığında hesaplanan ortalamaya ve değişintiye sahip olacaktır. Bakıldığı zaman, ortalama ve değişinti, bütün veri setinin aksine sadece o küçük yığında hesaplandığından dolayı, ortalama ve değişinti içerisinde bir miktar gürültü(parazit) barındırır çünkü sadece o küçük yığında hesaplanmıştır, örneğin 64 ya da 128, ya da belki 256 ya da daha fazla eğitim örneğinde. Dolayısıyla, ortalama ve değişinti sadece sınırlı veri örnekleriyle hesaplandığı için bir miktar gürültülü(parazitli) olacaktır. Z_l den Z_2_l'e giden ölçekleme işlemi de bir miktar gürültülü olacaktır çünkü bu değerler de bir miktar gürültülü ortalama ve değişinti ile hesaplanmıştır. Dolayısıyla seyreltmeye benzer şekilde, her gizli ünitenin aktivasyonuna bir miktar gürültü ekleyecektir. Seyreltmenin gürültüye sahip olma sebebi şu, seyreltme ilk önce gizli üniteyi alıyor ve bunu 0 ve bir olasılıkla çarpıyor. veya 1 ve başka bir olasılıkla çarpıyor. dolayısıyla, seyreltme 0 ve 1 ile çarpıldığından dolayı gürültüye sahiptir, bunun gibi toptan normalleştirme ise ölçeklendirme ve standart değişinti'den dolayı gürültüye sahiptir, aynı zamanda ortalamayı çıkardığımız için çoğalan gürültüye sahiptir. burada bakıldığında ortalama ve standart değişinti gürültüye sahip dolayısıyla seyreltmeye benzer şekilde, toptan normalleştirme bir miktar düzenlileştirme etkisine sahip, çünkü gizli ünitelere gürültü ekleyerek, akış yönendeki gizli ünitelerin tek bir gizli üniteye çok fazla bağlı kalmamasına yol açıyor ve dolayısıyla seyreltmeye benzer şekilde, gizli ünitelere gürültü ekliyor ve dolayısıyla küçük bir düzenlileştirme etkisi mevcut. Eklenen gürültü çok küçük olduğu için, bu çok büyük bir düzenlileştirme etkisi oluşturmuyor, dolayısıyla toptan normalleştirmeyi seyreltme ile beraber kullanmayı seçebilirsiniz, ve yine seyreltmenin daha güçlü düzenlileştirme etkisinde faydalanmak istiyorsanız bu ikisini beraber kullanmayı seçebilirsiniz. ve belki sezgisel olmayan bir diğer etkisi ise, eğer geniş küçük yığınlar kullanırsanız, diyelim ki 64 yerine 512, bu gürültüyü düşürmüş olursunuz dolayısıyla bu düzenlileştirme etkisini azaltmış olursunuz. bu da toptan normalleştirmenin ilginç özelliklerinden bir diğeri idi, kısaca söylemek gerekirse, daha büyük geniş küçük yığınlar kullanırsanız, düzenlileştirme etkisini azaltmış olursunuz. Bunu söyledikten sonra, ben olsam toptan normalleştirmeyi düznlileştirici olarak kullanmazdım, bu toptan normalleştirmenin amaçlarından biri değil, fakat bu bazen öğrenme algoritmanızda fazladan ikinci bir amaç veya hesapta olmayan bir etki olabilir. fakat, gerçekten, toptan normalleştirmeyi düzenlileştirme için kullanmayın. Gizli ünitelerinizin aktivasyonlarının normalleştirilmesi ve bu şekilde öğrenmenizin hızının artması için kullanın. ve bence burada düzenlileştirme neredeyse tasarlanmamış bir yan etki olacaktır. Peki, umuyorum ki bu size daha toptan normalleştirmenin ne yaptığı konusunda iyi bir sezgi sağlamıştır. Toptan normalleştirme üzerindeki tartışmamızı sonlandırmadan önce, bildiğinizden emin olmak istediğim son bir detay daha var, bu da toptan normalleştirmenin sırasıyla bir küçük yığın şeklinde veriyi işlemesidir, burada toptan normalleştirme küçük yığınlarda ortalamayı ve değişinti'yi hesaplar, Dolayısıyla test zamanında, tahmin edici yapmaya ve sinir ağını değerlendirmeye çalışacaksınız. küçük yığın örneklerine sahip olmayabilirsiniz, sırasıyla tek bir örneği işliyor olabilirsiniz, dolayısıyla test zamanında, tahminlerinizin anlamlı olması için bir miktar farklı şekilde yapmalısınız. bir sonraki ve toptan normalleştirme ile ilgili son videoda , toptan normalleştirme kullanarak eğitilmiş sinir ağınız ile tahmin yapmak için ne yapmanız gerektiğini ile ilgili konuşacağız.