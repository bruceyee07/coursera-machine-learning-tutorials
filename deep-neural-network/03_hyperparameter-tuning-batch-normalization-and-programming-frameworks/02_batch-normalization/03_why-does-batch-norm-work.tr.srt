1
00:00:00,000 --> 00:00:02,415
Pekala, neden toptan normalleştirme çalışır?

2
00:00:02,415 --> 00:00:06,680
Bir sebebi şu, girdi özelliklerini nasıl normalleştireceğimizi görmüştünüz,

3
00:00:06,680 --> 00:00:09,380
x'lerin, ortalamasını 0'a ve değişintisini 1'e eşitlediğimizi,

4
00:00:09,380 --> 00:00:10,740
ve bunun öğrenmeyi nasıl hızlandırdığını görmüştük.

5
00:00:10,740 --> 00:00:13,920
Dolayısıyla 0'dan bire uzanan veya 

6
00:00:13,920 --> 00:00:15,735
1'den 1000' uzanan özelliklerine sahip olmaktansa,

7
00:00:15,735 --> 00:00:18,835
bütün girdi özelliklerini normalleştirerek,

8
00:00:18,835 --> 00:00:22,975
yani benzer aralıklar kazanmalarını sağlayarak öğrenme hızını artırmıştık.

9
00:00:22,975 --> 00:00:25,770
Dolayısıyla, toptan normalleştirmenin neden çalıştığını anlamada bir iç görü,

10
00:00:25,770 --> 00:00:27,750
toptan normalleştirmenin de benzer bir plan çizdiğini anlamaktır.

11
00:00:27,750 --> 00:00:32,770
Farkı artı olarak ise, sadece girdi özellikleri değil, gizli üniteler de bundan etkilenir.

12
00:00:32,770 --> 00:00:37,380
Şimdi, bu toptan normalleştirmenin ne yaptığının küçük bir resmi.

13
00:00:37,380 --> 00:00:39,180
Toptan normalleştirmenin yaptığını daha derin anlamanıza

14
00:00:39,180 --> 00:00:43,210
yardım edecek birkaç farklı sebep daha var.

15
00:00:43,210 --> 00:00:46,320
Hadi bu videoda bunları inceleyelim.

16
00:00:46,320 --> 00:00:48,490
Toptan normalleştirmenin neden çalıştığı ile ilgili verilebilecek ikinci sebep ise,

17
00:00:48,490 --> 00:00:50,295
ağırlıkların değişiminin, katmanların 

18
00:00:50,295 --> 00:00:52,575
daha sonra veya daha önce gelmesine göre değişmesidir.

19
00:00:52,575 --> 00:00:56,095
diyelim ki katman 10'daki ağırlıklar,

20
00:00:56,095 --> 00:01:00,300
değişmeye karşı daha erken katmandaki ağırlıklardan daha dirençli olacaktır örneğin birinci katman.

21
00:01:00,300 --> 00:01:01,810
Söylemek istediğimi açıklamak için,

22
00:01:01,810 --> 00:01:04,775
buradaki canlı örneğe bakalım,

23
00:01:04,775 --> 00:01:06,420
Hadi ağ üzerinde bir eğitim görelim,

24
00:01:06,420 --> 00:01:07,770
belki dar bir ağ,

25
00:01:07,770 --> 00:01:11,715
s biçimli bağlanım veya belki sinir ağı,

26
00:01:11,715 --> 00:01:17,260
belki de bu bağlanım gibi dar bir ağ belki de derin ağ,

27
00:01:17,260 --> 00:01:21,120
örneğimiz ünlü kedi tanımlama örneği üzerinde olacak.

28
00:01:21,120 --> 00:01:26,745
Diyelim ki, veri setinizi bütün siyah kedilerin görüntüleri üzerinde eğittiniz.

29
00:01:26,745 --> 00:01:31,240
Eğer şu anda bu ağı pozitif örneklerin

30
00:01:31,240 --> 00:01:33,325
sadece soldaki gibi siyah kediler olmadığı,

31
00:01:33,325 --> 00:01:36,895
sağdaki gibi renkli kedilerin de olduğu

32
00:01:36,895 --> 00:01:40,442
veri setine uygularsanız,

33
00:01:40,442 --> 00:01:43,160
bu durumda sınıflandırıcınız iyi sonuç alamayabilir.

34
00:01:43,160 --> 00:01:47,907
Dolayısıyla eğer eğitim setiniz, pozitif örneklerin burada 

35
00:01:47,907 --> 00:01:52,383
ve negatif örneklerin burada olduğu gibi gözüküyorsa 

36
00:01:52,383 --> 00:01:54,970
ve bu eğitim setinizi genelleştirmeye çalışırken,

37
00:01:54,970 --> 00:02:02,335
pozitif örneklerin burada ve negatif örneklerin burada olduğu bir veri setinde genelleştirmeye çalışıyorsanız, 

38
00:02:02,335 --> 00:02:05,800
bu durumda soldaki eğitim setinde eğittiğiniz modülün

39
00:02:05,800 --> 00:02:09,430
sağdaki veri setinde iyi sonuç almasını bekleyemezsiniz.

40
00:02:09,430 --> 00:02:13,901
Fonksiyon iyi çalışıyor olmasına rağmen,

41
00:02:13,901 --> 00:02:19,230
öğrenme algoritmanızın sadece soldaki verilere bakarak

42
00:02:19,230 --> 00:02:21,500
yeşil karar sınırını keşfetmesini bekleyemezsiniz.

43
00:02:21,500 --> 00:02:26,410
Dolayısıyla, bu veri dağılımının değişmesi fikrine,

44
00:02:26,410 --> 00:02:31,657
şık bir terim gibi gözüken, eş-değişinti kayması adı verilir.

45
00:02:31,657 --> 00:02:33,695
Ve fikir ise tam olarak şu;

46
00:02:33,695 --> 00:02:35,910
Eğer bir x'ten y'te eşlemeyi öğrendiyseniz,

47
00:02:35,910 --> 00:02:38,435
ve eğer x'in dağılımı değişirse,

48
00:02:38,435 --> 00:02:41,545
o zaman öğrenme algoritmanızı tekrar eğitmek zorundasınız.

49
00:02:41,545 --> 00:02:43,925
Ve bu x'ten y'ye eşlemeyi yapan fonksiyon,

50
00:02:43,925 --> 00:02:45,230
temel doğru fonksiyonu,

51
00:02:45,230 --> 00:02:46,775
değişmeyecek olsa bile doğrudır.

52
00:02:46,775 --> 00:02:49,430
Örneğin bu örnekte değişmemiştir,

53
00:02:49,430 --> 00:02:51,630
çünkü temel doğru fonksiyonu bu örnekte 

54
00:02:51,630 --> 00:02:53,990
bu görüntü kedi mi değil mi'dir.

55
00:02:53,990 --> 00:02:57,590
ve fonksiyonunuzu değiştirme ihtiyacı, eğer temel doğru fonksiyonunuz kayarsa

56
00:02:57,590 --> 00:03:03,510
daha acil veya daha önemli bir hal alır.

57
00:03:03,510 --> 00:03:08,720
Pekala, eş-değişinti kayması problemi sinir ağlarına nasıl uygulanır?

58
00:03:08,720 --> 00:03:10,880
Bunun gibi derin bir ağ düşünün,

59
00:03:10,880 --> 00:03:15,225
ve gelin bu belirttiğim katmanın açısından öğrenme işlemine bakalım.

60
00:03:15,225 --> 00:03:16,995
yani üçüncü katmanın,

61
00:03:16,995 --> 00:03:22,145
bu ağ w3 b3 parametrelerini öğrendi,

62
00:03:22,145 --> 00:03:24,860
ve üçüncü gizli katmanın açısından bakıldığında,

63
00:03:24,860 --> 00:03:27,665
bu katman daha önceki katmanlardan bir takım değerler alıyor,

64
00:03:27,665 --> 00:03:30,020
ve bu değerlerle işlemler yapmak zorunda ve bu işlemlerin sonucunda

65
00:03:30,020 --> 00:03:34,305
temel doğru değeri y'ye en yakın değerleri vermeye çalışmakta.

66
00:03:34,305 --> 00:03:38,340
Dolayısıyla bana izin verin size sol tafafında ne yaptığını açıklıyım.

67
00:03:38,340 --> 00:03:41,785
bu üçüncü gizli katmanın açısından,

68
00:03:41,785 --> 00:03:44,265
bazı değerler alıyor,

69
00:03:44,265 --> 00:03:48,520
diyelim ki a21,a22,a23,a24,

70
00:03:48,520 --> 00:03:57,102
fakat bu değerler yine aynı zamanda özellikler de olabilir yani x1,x2,x3,x4

71
00:03:57,102 --> 00:04:02,240
ve üçüncü gizli katmanın görevi ise bu değerleri alıp 

72
00:04:02,240 --> 00:04:08,225
y şapka'ya eşlemenin yolunu bulmak.

73
00:04:08,225 --> 00:04:10,760
Yani, eğim azaltma yöntemi sırasında,

74
00:04:10,760 --> 00:04:17,525
bu değerler w3,b3 ve aynı zamanda w4, b4,

75
00:04:17,525 --> 00:04:21,800
ve hatta w5, b5, ağın iyi bir iş çıkarması için,

76
00:04:21,800 --> 00:04:23,360
soldaki siyahla yazdığım değerlerden çıkış verisi y'ye

77
00:04:23,360 --> 00:04:29,460
eşleme yaparken bu parametreleri öğrenmeye çalışıyor.

78
00:04:29,460 --> 00:04:33,470
Fakat şimdi ağın sol tarafını tekrar açığa çıkaralım.

79
00:04:33,470 --> 00:04:42,226
Ağ aynı zamanda w2,b2 ve w1,b1 parametrelerinde değişiklik yapıyor,

80
00:04:42,226 --> 00:04:45,305
dolayısıyla bu parametreler değiştikçe,

81
00:04:45,305 --> 00:04:49,795
bu değerler a2 de değişiyor.

82
00:04:49,795 --> 00:04:53,080
Dolayısıyla üçüncü gizli katmanın açısından bakıldığında,

83
00:04:53,080 --> 00:04:56,735
bu gizli ünite değerleri her zaman değişiyor ve

84
00:04:56,735 --> 00:04:59,090
dolayısıyla daha önceki slaytta konuştuğumuz 

85
00:04:59,090 --> 00:05:02,435
eş-değişinti kayması problemi ortaya çıkıyor.

86
00:05:02,435 --> 00:05:04,115
Toptan normalleştirmenin yağtığı şey,

87
00:05:04,115 --> 00:05:10,730
gizli ünite değerlerinin dağılımının değişim miktarını kısıtlamaktır

88
00:05:10,730 --> 00:05:14,825
ve bu gizli ünite değerlerinin dağılımını çizecek olursak,

89
00:05:14,825 --> 00:05:17,948
buna teknik olarak Z normalleştiricisi deniyor.

90
00:05:17,948 --> 00:05:24,970
dolayısıyla burada z21 bu , z22 ise bu oluyor,

91
00:05:24,970 --> 00:05:27,862
aslında burada dört değer yerine iki değer çizdim,

92
00:05:27,862 --> 00:05:29,670
dolayısıyla iki boyutta görselleştirebiliriz.

93
00:05:29,670 --> 00:05:32,015
Toptan normalleştirmeninin dediği şey ise şu,

94
00:05:32,015 --> 00:05:35,745
z21 ve z22 değerleri değişebilir,

95
00:05:35,745 --> 00:05:37,514
ve sinir ağı önceki katmanlardaki parametreleri güncellediğinde

96
00:05:37,514 --> 00:05:41,215
bakarsak gerçekten değişecektir,

97
00:05:41,215 --> 00:05:44,930
fakat toptan normalleştirmenin temin ettiği şey ise şudur nasıl değişirse değişsin,

98
00:05:44,930 --> 00:05:55,050
z21 ve z22'nin ortalaması ve değişintisi sabit kalacaktır.

99
00:05:55,050 --> 00:05:59,900
Dolayısıyla z21 ve z22'nin tam değerleri değişse bile,

100
00:05:59,900 --> 00:06:07,115
onların ortalamaları ve değişintileri ortalama 0 ve değişinti 1 olacak şekilde aynı kalacaktır.

101
00:06:07,115 --> 00:06:09,940
ya da beta2 ve gamma2 şeklinde olacaktır,

102
00:06:09,940 --> 00:06:17,295
fakat 0 ve 1 olması zorunlu değildir.

103
00:06:17,295 --> 00:06:19,228
Eğer sinir ağı seçerse,

104
00:06:19,228 --> 00:06:22,270
bu değerleri ortalama 0 ve değişinti 1 olacak şekilde yapabilir

105
00:06:22,270 --> 00:06:24,655
ya da herhangi bir ortalama ve değişinti değeri olacak şekilde yapabilir.

106
00:06:24,655 --> 00:06:26,305
Burada yapılan şey genel olarak,

107
00:06:26,305 --> 00:06:32,290
önceki katmanlarda yapılan güncellemelerin üçüncü gizli katman

108
00:06:32,290 --> 00:06:35,110
değerlerinin dağılımını etkileme derecesini azaltmak ve dolayısıyla

109
00:06:35,110 --> 00:06:38,790
öğrenme miktarını azaltmak olarak söylenebilir.

110
00:06:38,790 --> 00:06:44,370
Dolayısıyla, toptan normalleştirme girdi değerlerinin değişme problemini azaltır,

111
00:06:44,370 --> 00:06:48,895
bu değerlerin daha kararlı hale gelmesine yol açar,

112
00:06:48,895 --> 00:06:55,155
ve bu şekilde sinir ağındaki ileri katmanlardaki değerler için daha sağlam bir zemin hazırlar.

113
00:06:55,155 --> 00:06:57,655
ve girdi değerlerinin dağılımı bir miktar değişse bile,

114
00:06:57,655 --> 00:07:00,580
sonraki değerler daha az değişir ve bu

115
00:07:00,580 --> 00:07:03,325
önceki katmanların öğrenmeye devam etmesi durumunda bile,

116
00:07:03,325 --> 00:07:06,640
bunun sonraki katmanlarda değişim oluşturma miktarını kısıtlaması

117
00:07:06,640 --> 00:07:10,180
veya azaltması veya değiştiği andaki tepkisinin azalması anlamına gelir,

118
00:07:10,180 --> 00:07:12,925
diğer bir değişle, bu erken katmanların yaptıklarıyla

119
00:07:12,925 --> 00:07:15,445
daha sonraki katmanların yaptıklarının

120
00:07:15,445 --> 00:07:18,020
eşleşmesinin zayıflaması anlamına gelir.

121
00:07:18,020 --> 00:07:22,147
ve bu ağdaki bütün katmanların diğer katmanlardan biraz daha bağımsız olarak,

122
00:07:22,147 --> 00:07:25,210
kendi başlarına öğrenebilmelerine olanak tanır,

123
00:07:25,210 --> 00:07:29,145
ve bu bütün ağın öğrenme hızının artması anlamına gelir.

124
00:07:29,145 --> 00:07:32,161
umuyorum ki bu biraz sezgi kazandırmıştır,

125
00:07:32,161 --> 00:07:35,620
eve götürülecek nokta şu: toptan normalleştirme,

126
00:07:35,620 --> 00:07:39,010
özellikle sinir ağının daha sonraki katmanları açısından bakıldığında,

127
00:07:39,010 --> 00:07:43,090
önceki katmanların eskiden olduğu kadar kaymasını engeller,

128
00:07:43,090 --> 00:07:46,320
çünkü aynı ortalama ve değişinti'ye sahip olma şartları vardır,

129
00:07:46,320 --> 00:07:50,260
ve bu daha sonraki katmanların öğrenmelerini daha kolaylaştırır.

130
00:07:50,260 --> 00:07:52,669
Görünen o ki toptan normalleştirme ikinci bir etkiye sahip,

131
00:07:52,669 --> 00:07:55,940
bir miktar düzenlileştirme etkisi.

132
00:07:55,940 --> 00:07:59,885
Toptan normalleştirmenin sezgisel olmayan bir özelliği, her küçük yığının,

133
00:07:59,885 --> 00:08:02,090
örnek verecek olursak küçük yığın X_t,

134
00:08:02,090 --> 00:08:04,660
Z_l değerlerine sahip olsun.

135
00:08:04,660 --> 00:08:07,225
Bu küçük yığın sadece

136
00:08:07,225 --> 00:08:12,730
o küçük yığında hesaplanan ortalamaya ve değişintiye sahip olacaktır.

137
00:08:12,730 --> 00:08:15,895
Bakıldığı zaman, ortalama ve değişinti,

138
00:08:15,895 --> 00:08:20,245
bütün veri setinin aksine sadece o küçük yığında hesaplandığından dolayı,

139
00:08:20,245 --> 00:08:22,960
ortalama ve değişinti içerisinde bir miktar gürültü(parazit) barındırır

140
00:08:22,960 --> 00:08:25,540
çünkü sadece o küçük yığında hesaplanmıştır,

141
00:08:25,540 --> 00:08:28,145
örneğin 64 ya da 128,

142
00:08:28,145 --> 00:08:32,335
ya da belki 256 ya da daha fazla eğitim örneğinde.

143
00:08:32,335 --> 00:08:35,935
Dolayısıyla, ortalama ve değişinti sadece sınırlı veri örnekleriyle hesaplandığı için

144
00:08:35,935 --> 00:08:40,195
bir miktar gürültülü(parazitli) olacaktır.

145
00:08:40,195 --> 00:08:43,363
Z_l den Z_2_l'e giden ölçekleme işlemi de

146
00:08:43,363 --> 00:08:46,135
bir miktar gürültülü olacaktır

147
00:08:46,135 --> 00:08:51,420
çünkü bu değerler de bir miktar gürültülü ortalama ve değişinti ile hesaplanmıştır.

148
00:08:51,420 --> 00:08:54,817
Dolayısıyla seyreltmeye benzer şekilde,

149
00:08:54,817 --> 00:08:57,980
her gizli ünitenin aktivasyonuna bir miktar gürültü ekleyecektir.

150
00:08:57,980 --> 00:08:59,905
Seyreltmenin gürültüye sahip olma sebebi şu,

151
00:08:59,905 --> 00:09:04,180
seyreltme ilk önce gizli üniteyi alıyor ve bunu 0 ve bir olasılıkla çarpıyor.

152
00:09:04,180 --> 00:09:06,870
veya 1 ve başka bir olasılıkla çarpıyor.

153
00:09:06,870 --> 00:09:12,350
dolayısıyla, seyreltme 0 ve 1 ile çarpıldığından dolayı gürültüye sahiptir,

154
00:09:12,350 --> 00:09:18,360
bunun gibi toptan normalleştirme ise ölçeklendirme ve standart değişinti'den dolayı gürültüye sahiptir,

155
00:09:18,360 --> 00:09:21,655
aynı zamanda ortalamayı çıkardığımız için çoğalan gürültüye sahiptir.

156
00:09:21,655 --> 00:09:25,825
burada bakıldığında ortalama ve standart değişinti gürültüye sahip

157
00:09:25,825 --> 00:09:29,785
dolayısıyla seyreltmeye benzer şekilde,

158
00:09:29,785 --> 00:09:33,220
toptan normalleştirme bir miktar düzenlileştirme etkisine sahip,

159
00:09:33,220 --> 00:09:35,435
çünkü gizli ünitelere gürültü ekleyerek,

160
00:09:35,435 --> 00:09:41,280
akış yönendeki gizli ünitelerin tek bir gizli üniteye çok fazla bağlı kalmamasına yol açıyor

161
00:09:41,280 --> 00:09:43,025
ve dolayısıyla seyreltmeye benzer şekilde,

162
00:09:43,025 --> 00:09:47,620
gizli ünitelere gürültü ekliyor ve dolayısıyla küçük bir düzenlileştirme etkisi mevcut.

163
00:09:47,620 --> 00:09:50,064
Eklenen gürültü çok küçük olduğu için,

164
00:09:50,064 --> 00:09:52,572
bu çok büyük bir düzenlileştirme etkisi oluşturmuyor,

165
00:09:52,572 --> 00:09:56,760
dolayısıyla toptan normalleştirmeyi seyreltme ile beraber kullanmayı seçebilirsiniz,

166
00:09:56,760 --> 00:09:59,880
ve yine seyreltmenin daha güçlü düzenlileştirme etkisinde

167
00:09:59,880 --> 00:10:03,060
faydalanmak istiyorsanız bu ikisini beraber kullanmayı seçebilirsiniz.

168
00:10:03,060 --> 00:10:06,195
ve belki sezgisel olmayan bir diğer etkisi ise,

169
00:10:06,195 --> 00:10:08,454
eğer geniş küçük yığınlar kullanırsanız,

170
00:10:08,454 --> 00:10:11,200
diyelim ki 64 yerine 512,

171
00:10:11,200 --> 00:10:13,725
bu gürültüyü düşürmüş olursunuz

172
00:10:13,725 --> 00:10:15,934
dolayısıyla bu düzenlileştirme etkisini azaltmış olursunuz.

173
00:10:15,934 --> 00:10:20,940
bu da toptan normalleştirmenin ilginç özelliklerinden bir diğeri idi,

174
00:10:20,940 --> 00:10:24,030
kısaca söylemek gerekirse,

175
00:10:24,030 --> 00:10:27,435
daha büyük geniş küçük yığınlar kullanırsanız,

176
00:10:27,435 --> 00:10:29,870
düzenlileştirme etkisini azaltmış olursunuz.

177
00:10:29,870 --> 00:10:33,833
Bunu söyledikten sonra, ben olsam toptan normalleştirmeyi düznlileştirici olarak kullanmazdım,

178
00:10:33,833 --> 00:10:36,625
bu toptan normalleştirmenin amaçlarından biri değil,

179
00:10:36,625 --> 00:10:44,250
fakat bu bazen öğrenme algoritmanızda fazladan ikinci bir amaç veya hesapta olmayan bir etki olabilir.

180
00:10:44,250 --> 00:10:48,390
fakat, gerçekten, toptan normalleştirmeyi düzenlileştirme için kullanmayın.

181
00:10:48,390 --> 00:10:50,070
Gizli ünitelerinizin aktivasyonlarının normalleştirilmesi 

182
00:10:50,070 --> 00:10:53,770
ve bu şekilde öğrenmenizin hızının artması için kullanın.

183
00:10:53,770 --> 00:10:57,900
ve bence burada düzenlileştirme neredeyse tasarlanmamış bir yan etki olacaktır.

184
00:10:57,900 --> 00:11:02,430
Peki, umuyorum ki bu size daha toptan normalleştirmenin ne yaptığı konusunda iyi bir sezgi sağlamıştır.

185
00:11:02,430 --> 00:11:04,580
Toptan normalleştirme üzerindeki tartışmamızı sonlandırmadan önce,

186
00:11:04,580 --> 00:11:06,855
bildiğinizden emin olmak istediğim son bir detay daha var,

187
00:11:06,855 --> 00:11:11,254
bu da toptan normalleştirmenin sırasıyla bir küçük yığın şeklinde veriyi işlemesidir,

188
00:11:11,254 --> 00:11:14,520
burada toptan normalleştirme küçük yığınlarda ortalamayı ve değişinti'yi hesaplar,

189
00:11:14,520 --> 00:11:15,720
Dolayısıyla test zamanında,

190
00:11:15,720 --> 00:11:18,150
tahmin edici yapmaya ve sinir ağını değerlendirmeye çalışacaksınız.

191
00:11:18,150 --> 00:11:20,400
küçük yığın örneklerine sahip olmayabilirsiniz,

192
00:11:20,400 --> 00:11:24,035
sırasıyla tek bir örneği işliyor olabilirsiniz,

193
00:11:24,035 --> 00:11:26,400
dolayısıyla test zamanında,

194
00:11:26,400 --> 00:11:29,430
tahminlerinizin anlamlı olması için bir miktar farklı şekilde yapmalısınız.

195
00:11:29,430 --> 00:11:32,197
bir sonraki ve toptan normalleştirme ile ilgili son videoda ,

196
00:11:32,197 --> 00:11:35,490
toptan normalleştirme kullanarak eğitilmiş sinir ağınız ile

197
00:11:35,490 --> 00:11:39,290
tahmin yapmak için ne yapmanız gerektiğini ile ilgili konuşacağız.