1
00:00:00,540 --> 00:00:02,320
歡迎來到本週最後的影片

2
00:00:02,320 --> 00:00:05,987
目前有很多很好的深度學習程式框架，

3
00:00:05,987 --> 00:00:07,900
其中一個叫 TensorFlow

4
00:00:07,900 --> 00:00:11,430
我很開心能幫助你、帶你入門

5
00:00:11,430 --> 00:00:14,760
在這部影片我想採取的方式是，

6
00:00:14,760 --> 00:00:18,970
示範 TensorFlow 程式基本的架構，
讓你自行練習，了解更多細節

7
00:00:18,970 --> 00:00:22,150
最後在本周作業練習。

8
00:00:22,150 --> 00:00:24,660
這禮拜的作業會需要比較長的時間，

9
00:00:24,660 --> 00:00:27,622
所以請務必保留更多的時間。

10
00:00:27,622 --> 00:00:29,270
以這個問題來出發，

11
00:00:29,270 --> 00:00:34,210
假設你有某個成本函數 J，想要讓他變最小

12
00:00:34,210 --> 00:00:39,857
在這個例子，我想用一個非常簡單的

13
00:00:39,857 --> 00:00:45,033
成本函數做為例子 J(w) = w^2 - 10w + 25.

14
00:00:45,033 --> 00:00:46,870
所以這個是成本函數

15
00:00:46,870 --> 00:00:53,751
你可能會注意到這個函數實際上是 (w-5) 的平方

16
00:00:53,751 --> 00:00:58,569
如果你展開這個二次式，你會得到上面這個方程式

17
00:00:58,569 --> 00:01:01,552
所以讓其最小的 w 值會等於 5

18
00:01:01,552 --> 00:01:05,770
但假設我們不知道這個答案；你就只有這個函數

19
00:01:05,770 --> 00:01:10,240
讓我們瞧瞧如何用 TensorFlow 解決這個最小化

20
00:01:10,240 --> 00:01:13,960
因為其類似架構的程式可以拿來訓練神經網路：

21
00:01:13,960 --> 00:01:18,337
你會有一個複雜的成本函數 J(W, b)

22
00:01:18,337 --> 00:01:22,934
由你神經網路所有的參數所決定

23
00:01:22,934 --> 00:01:26,910
然後類似地，你可以拿 TensorFlow

24
00:01:26,910 --> 00:01:33,010
來自動找到讓成本最小的 w 和 b 的值

25
00:01:33,010 --> 00:01:36,140
不過，先讓我們從左邊比較簡單的例子來開始

26
00:01:36,140 --> 00:01:41,690
那，我在我的 Jupyter notebook 上執行 Python，
要開始 TensorFlow 的話

27
00:01:41,690 --> 00:01:48,390
你 "import numpy as np"，
然後這是慣例寫法 "import tensorflow as tf"

28
00:01:48,390 --> 00:01:52,979
接下來，讓我定義參數 w

29
00:01:52,979 --> 00:01:58,556
那在 TensorFlow，你會用 tf.Variable 來定義一個參數

30
00:02:01,932 --> 00:02:06,396
dtype=tf.float32

31
00:02:08,480 --> 00:02:10,520
接下來讓我們定義成本函數

32
00:02:10,520 --> 00:02:15,839
還記得我們的成本函數是
 w^2 - 10w + 25

33
00:02:15,839 --> 00:02:19,239
所以讓我用 tf.add

34
00:02:19,239 --> 00:02:26,989
所以我們有 w 平方，加，tf.multiply

35
00:02:26,989 --> 00:02:31,642
第二項是 -10 乘上 w

36
00:02:31,642 --> 00:02:35,491
然後我把那個加上 25

37
00:02:35,491 --> 00:02:40,500
所以讓我在這邊放另一個 tf.add

38
00:02:40,500 --> 00:02:43,310
這樣就定義好我們的成本函數 J 了

39
00:02:43,310 --> 00:02:48,067
然後我要寫

40
00:02:48,067 --> 00:02:53,945
"train = tf.train.GradientDescentOptimizer"

41
00:02:53,945 --> 00:03:01,409
讓我們用 0.01 做為學習率，
然後目標是最小化 (minimize) 成本

42
00:03:01,409 --> 00:03:05,850
最後呢，以下這幾行算是慣例寫法

43
00:03:05,850 --> 00:03:13,560
"init = tf.global_variables_initializer()"

44
00:03:13,560 --> 00:03:18,404
然後 "session = tf.Session()"

45
00:03:18,404 --> 00:03:21,044
所以這會啟動一個 TensorFlow 工作階段 (session)

46
00:03:21,044 --> 00:03:25,890
"session.run(init)" 來初始化全域變數

47
00:03:25,890 --> 00:03:33,135
然後，為了讓 TensorFlow 執行這變數，
我們會用 "sess.run(w)"

48
00:03:33,135 --> 00:03:34,320
我們目前還沒做什麼

49
00:03:34,320 --> 00:03:39,540
所以上面這行，初始 w 為 0，然後定義成本函數

50
00:03:39,540 --> 00:03:42,668
我們定義 train 為我們的學習演算法，

51
00:03:42,668 --> 00:03:46,206
利用梯度下降優化器 (gradient descent optimizer)
把成本函數最小化 (minimize)

52
00:03:46,206 --> 00:03:50,549
但是，我們尚未真正執行這個學習演算法

53
00:03:50,549 --> 00:03:55,778
session.run，我們取值出 w；
讓我印出 session.run 的結果

54
00:03:55,778 --> 00:03:56,852
所以如果我們執行這個

55
00:03:56,852 --> 00:04:01,029
他算出 w 等於 0，因為我們還沒真正執行任何東西

56
00:04:01,029 --> 00:04:06,420
現在呢，讓我們做 "session.run(train)"

57
00:04:06,420 --> 00:04:11,190
所以這一行做的，是執行一步的梯度下降

58
00:04:11,190 --> 00:04:15,814
然後在這一步梯度下降後，

59
00:04:15,814 --> 00:04:19,870
讓我們再取值 w，然後印出來。

60
00:04:19,870 --> 00:04:25,690
所以經過這一步的梯度下降後，w 現在是 0.1

61
00:04:25,690 --> 00:04:33,057
我們現在跑個 1000 步的梯度下降，所以... .run(train)

62
00:04:35,543 --> 00:04:41,880
然後來印個 print(session.run(w))

63
00:04:41,880 --> 00:04:45,501
所以這會跑一千次的梯度下降

64
00:04:45,501 --> 00:04:47,989
而最後 w 變 4.99999

65
00:04:47,989 --> 00:04:51,914
還記得我們要把 (w-5) 的平方最小化

66
00:04:51,914 --> 00:04:56,330
所以 w 最佳值是 5，非常接近這個值。

67
00:04:56,330 --> 00:05:02,680
希望這能給你一些對 TensorFlow 程式大致的架構一些感覺

68
00:05:02,680 --> 00:05:08,038
而當你在做程式作業，自己玩過更多 TensorFlow 的程式後

69
00:05:08,038 --> 00:05:12,653
這邊我用的一些函式會更眼熟的

70
00:05:12,653 --> 00:05:16,836
這邊要注意的是，w 是我要去最佳化的參數

71
00:05:16,836 --> 00:05:19,760
所以我們宣告他為一個變數 (Variable)

72
00:05:19,760 --> 00:05:24,400
再注意到我們要做的，只是定義成本函數
— 利用這些加 (add)、

73
00:05:24,400 --> 00:05:26,654
乘 (multiply) 等等的函式

74
00:05:26,654 --> 00:05:31,450
TensorFlow 自己知道怎麼微分取導數
套用到這些「加」、

75
00:05:31,450 --> 00:05:34,370
「乘」、還有其他函式

76
00:05:34,370 --> 00:05:38,640
這就是為何我們只需實作正向傳播的部份

77
00:05:38,640 --> 00:05:43,560
他自己能知道怎麼做反向傳播或是計算梯度

78
00:05:43,560 --> 00:05:48,070
因為那些計算已經內建在加、乘

79
00:05:48,070 --> 00:05:50,920
還有平方的函式

80
00:05:50,920 --> 00:05:55,078
題外話，如果你覺得這堆英文很醜

81
00:05:55,078 --> 00:05:59,830
實際上 TensorFlow 有把
這些運算子多載 (overload)

82
00:05:59,830 --> 00:06:03,320
對於常見的加啦、減啦、等等

83
00:06:03,320 --> 00:06:07,480
所以你也可以把這成本
寫成這種比較好看的形式，

84
00:06:07,480 --> 00:06:11,600
把上一行註解掉，重跑，也會得到同樣結果

85
00:06:11,600 --> 00:06:15,913
所以只要 w 被定義成一個 TensorFlow 變數，
那些平方、相乘、

86
00:06:15,913 --> 00:06:18,785
加和減的運算子都被多載

87
00:06:18,785 --> 00:06:22,575
所以你就不用我上面那樣醜醜的語法來寫。

88
00:06:22,575 --> 00:06:27,848
還有，我想要示範另一個 TensorFlow 的功能：

89
00:06:27,848 --> 00:06:31,460
在這個例子，最小化的是一個既定的 w 的函數

90
00:06:31,460 --> 00:06:34,880
然而，有時你想最小化的
會是訓練集的函數

91
00:06:34,880 --> 00:06:38,540
所以，萬一你有某些訓練資料 x，

92
00:06:38,540 --> 00:06:43,090
而當訓練神經網路的時候，訓練資料 x 會變動

93
00:06:43,090 --> 00:06:49,242
那麼，你要怎麼把訓練資料弄進 TensorFlow 程式中？

94
00:06:49,242 --> 00:06:50,913
所以我要定義 x

95
00:06:50,913 --> 00:06:54,811
把這看作扮演著訓練集的角色

96
00:06:54,811 --> 00:07:00,008
或者是有x和y的訓練集，只不過在這例子我們只有 x

97
00:07:00,008 --> 00:07:03,504
所以這一行把 x 定義成一個 "placeholder"

98
00:07:03,504 --> 00:07:10,260
而且會是 float32 的型態，而且讓這個是 3乘1 的矩陣

99
00:07:10,260 --> 00:07:15,489
我現在要做的是：雖然這成本函數有固定的係數

100
00:07:15,489 --> 00:07:21,308
就是二次方程式這三項前面的數字：
1 乘上w平方、減掉 10 乘w、再加 25

101
00:07:21,308 --> 00:07:26,821
我們可以把這邊的數字，1, -10 和 25 換成資料

102
00:07:26,821 --> 00:07:33,436
所以我這邊要做的是把成本換掉

103
00:07:33,436 --> 00:07:40,052
用 cost 等於 x[0][0] 乘以w平方

104
00:07:40,052 --> 00:07:47,600
加 x[1][0] 乘以 w，加 x[2][0]

105
00:07:47,600 --> 00:07:49,380
嗯... 乘以 1

106
00:07:49,380 --> 00:07:54,197
所以 x 現在變成像是

107
00:07:54,197 --> 00:07:59,148
控制這二次方程式係數的資料

108
00:07:59,148 --> 00:08:03,805
而這邊的 placeholder 函式告訴 TensorFlow

109
00:08:03,805 --> 00:08:08,470
你之後會再把某個值給 x。

110
00:08:09,850 --> 00:08:16,669
那我們再定義另一個陣列，coefficient = np.array(

111
00:08:19,030 --> 00:08:23,514
[1.], [-10.]

112
00:08:23,514 --> 00:08:27,554
還有對，最後一個值是 [25.]

113
00:08:27,554 --> 00:08:31,120
所以這就是我們等等會放入 x 的資料

114
00:08:32,300 --> 00:08:38,830
那麼最後，我們需要有個方法把這個陣列 coefficients
餵進去給變數 x

115
00:08:38,830 --> 00:08:43,170
做這個的語法呢，是在訓練的步驟

116
00:08:43,170 --> 00:08:49,224
需要把值給 x，所以

117
00:08:49,224 --> 00:08:56,302
我要在這邊設定，feed_dict = x 對應到 coefficients

118
00:08:58,005 --> 00:09:04,236
我要把這改一下，我要把這個拷貝也貼到這邊

119
00:09:04,236 --> 00:09:06,320
好的，希望我語法沒有錯誤

120
00:09:06,320 --> 00:09:13,330
讓我們來重跑這個，希望得到和剛剛一樣的結果。

121
00:09:14,710 --> 00:09:19,540
現在呢，如果你想改變這個二次方程式的係數

122
00:09:19,540 --> 00:09:24,239
假設你把這個 [-10.] 改成 [-20.]

123
00:09:24,239 --> 00:09:26,823
還有把這個改成 100

124
00:09:26,823 --> 00:09:30,947
所以這是代表 (x-10)的平方的函數

125
00:09:30,947 --> 00:09:34,068
那如果我現在重跑一次，希望

126
00:09:34,068 --> 00:09:39,810
我找到讓 (x-10)平方最小的值會是 w=10

127
00:09:39,810 --> 00:09:41,521
讓我們看看，讚

128
00:09:41,521 --> 00:09:46,610
在1000步的梯度下降以後，我們得到的 w 很靠近 10。

129
00:09:46,610 --> 00:09:51,462
所以你在程式作業會看到更多的例子：

130
00:09:51,462 --> 00:09:55,490
TensorFlow的placeholder是一個你
等一下會給值的變數

131
00:09:55,490 --> 00:10:02,297
而這是讓你把訓練集放入成本函數
的一個方便的方法

132
00:10:02,297 --> 00:10:07,011
而要把資料放入成本函數，就是用這個語法

133
00:10:07,011 --> 00:10:10,099
當你在跑訓練迭代的步驟

134
00:10:10,099 --> 00:10:15,070
利用 feed_dict，把 x 設成這邊的 coefficients

135
00:10:15,070 --> 00:10:18,930
而如果你用的是小批次梯度下降，
其中每次的迭代

136
00:10:18,930 --> 00:10:22,914
你必須塞入不同的小批資料，那麼在不同的迭代步驟

137
00:10:22,914 --> 00:10:26,676
你就用 feed_dict 把訓練資料中不同的部份餵進去；

138
00:10:26,676 --> 00:10:31,940
把不同的小批資料塞入
成本函數裡拿資料的地方。

139
00:10:31,940 --> 00:10:36,120
希望這能給你一些概念，知道 TensorFlow 能做什麼

140
00:10:36,120 --> 00:10:37,350
他強大之處

141
00:10:37,350 --> 00:10:42,470
在於你只需要寫好怎麼算出成本函數

142
00:10:42,470 --> 00:10:44,380
然後它就會做微分

143
00:10:44,380 --> 00:10:48,760
還會套用梯度下降最佳化，或是Adam最佳化

144
00:10:48,760 --> 00:10:53,200
或是其他最佳化 — 只需要一兩行的程式就夠了

145
00:10:53,200 --> 00:10:55,980
所以再看一次程式碼

146
00:10:55,980 --> 00:10:57,840
我稍微整理了一下

147
00:10:57,840 --> 00:10:59,830
萬一這些函式

148
00:10:59,830 --> 00:11:04,390
或變數看起來有點神秘，你會更熟悉的 —

149
00:11:04,390 --> 00:11:09,830
當你開始做程式作業，用了他們幾回以後。

150
00:11:09,830 --> 00:11:11,664
最後我想講一件事

151
00:11:11,664 --> 00:11:16,153
這三行程式在 TensorFlow 裡面是很慣例的寫法，

152
00:11:16,153 --> 00:11:20,960
然而有些人會用另一種寫法來代替

153
00:11:20,960 --> 00:11:22,880
其實做的都一樣

154
00:11:22,880 --> 00:11:25,460
把 session 設成 tf.Session() 以啟動這個 session

155
00:11:25,460 --> 00:11:28,050
然後用這個 session 執行 init

156
00:11:28,050 --> 00:11:32,650
然後用 session 來取值/執行 w，然後印出結果。

157
00:11:32,650 --> 00:11:38,000
而這邊的 "with" 架構在 TensorFlow 程式內也很常用到

158
00:11:38,000 --> 00:11:41,511
他和左邊這些意思上差不多

159
00:11:41,511 --> 00:11:47,213
不過在 Python 用 with 這個指令，當執行

160
00:11:47,213 --> 00:11:52,437
裡面這層的指令碰到錯誤的時候，他清理得比較好

161
00:11:52,437 --> 00:11:55,792
所以你也會在程式作業裡看到這種寫法。

162
00:11:55,792 --> 00:11:58,890
那麼，這一個程式到底在做什麼？

163
00:11:58,890 --> 00:12:02,230
讓我們專心看這個式子

164
00:12:02,230 --> 00:12:07,885
TensorFlow 最重要的核心是成本函數的計算，

165
00:12:07,885 --> 00:12:13,410
然後 TensorFlow 自動算出其導數，
來了解如何把成本變最小

166
00:12:13,410 --> 00:12:18,710
所以這個方程式，或說是這行程式碼所作的

167
00:12:18,710 --> 00:12:23,670
是讓 TensorFlow 能夠建造一個計算圖
(computation graph)

168
00:12:23,670 --> 00:12:26,840
而計算圖做這樣的事：他拿著 x[0][0]，

169
00:12:26,840 --> 00:12:32,310
他拿著 w，然後把 w 平方

170
00:12:33,840 --> 00:12:39,382
然後 x[0][0] 和 w平方相乘

171
00:12:39,382 --> 00:12:45,160
所以你得到 x[0][0] * w^2，依此類推

172
00:12:45,160 --> 00:12:49,893
最終呢，你知道的，這一直建構下去，計算出

173
00:12:49,893 --> 00:12:54,650
x[0][0]*w^2 + x[1][0]*w + ...

174
00:12:54,650 --> 00:12:58,539
所以最後你就得到了成本函數

175
00:12:58,539 --> 00:13:03,404
— 我想最後加的一項是 x[2][0]

176
00:13:03,404 --> 00:13:05,751
這邊，加進去這個成本

177
00:13:05,751 --> 00:13:08,398
我就省略不寫這成本其他的公式了。

178
00:13:08,398 --> 00:13:13,206
還有，TensorFlow 一個好處是，你只要實作出

179
00:13:13,206 --> 00:13:17,937
在計算圖上，正向傳播往前算出成本，

180
00:13:17,937 --> 00:13:21,230
TensorFlow 已經內建了

181
00:13:21,230 --> 00:13:24,420
所有會用到的反向函式

182
00:13:24,420 --> 00:13:29,287
還記得訓練神經網路時，會用一些正向的函數

183
00:13:29,287 --> 00:13:31,424
還有一些反向的函數

184
00:13:31,424 --> 00:13:35,661
像 TensorFlow 這種程式框架，早已內建了必要的

185
00:13:35,661 --> 00:13:37,240
反向函式

186
00:13:37,240 --> 00:13:41,892
這就是為什麼，當你用內建的函式計算正向的函數

187
00:13:41,892 --> 00:13:46,129
他能夠自動執行反向的函數，來實踐反向傳播

188
00:13:46,129 --> 00:13:51,351
即使函數再怎麼複雜，他都能幫你算出導數

189
00:13:51,351 --> 00:13:55,180
這就是為何你不需要真的實作反向傳播。

190
00:13:55,180 --> 00:13:58,240
這就是其中的一個例子，程式框架

191
00:13:58,240 --> 00:14:00,140
能讓你更有效率。

192
00:14:00,140 --> 00:14:03,470
如果你去讀 TensorFlow 的文件

193
00:14:03,470 --> 00:14:06,270
我只是想提醒一下，TensowFlow 文件

194
00:14:06,270 --> 00:14:11,023
在畫計算圖的時候，用了和我稍微不同的表示法。

195
00:14:11,023 --> 00:14:14,205
他用 x[0][0]、w

196
00:14:14,205 --> 00:14:17,408
然後他不是把值寫出來，例如 w**2，

197
00:14:17,408 --> 00:14:21,100
TensorFlow 文件裡通常只寫出運算子

198
00:14:21,100 --> 00:14:23,390
所以這會是個「平方運算」

199
00:14:23,390 --> 00:14:28,520
然後這兩個用「乘法運算」合併起來，依此類推

200
00:14:28,520 --> 00:14:30,990
然後最後一個節點，我想會是

201
00:14:30,990 --> 00:14:36,420
一個「加法運算」：你加上 x[2][0]，得到最後結果。

202
00:14:36,420 --> 00:14:40,080
為了這門課，我覺得這一種表示法

203
00:14:40,080 --> 00:14:43,670
會讓你比較容易理解計算圖

204
00:14:43,670 --> 00:14:47,010
不過如果你去讀 TensorFlow 文件

205
00:14:47,010 --> 00:14:51,840
如果你看文件裡面的計算圖

206
00:14:51,840 --> 00:14:55,630
你會看到這另一種慣例，也就是節點標示的

207
00:14:55,630 --> 00:14:58,900
是「運算」而不是「值」

208
00:14:58,900 --> 00:15:01,380
不過呢，這兩種表示法

209
00:15:01,380 --> 00:15:04,270
基本上都是代表同一個計算圖。

210
00:15:04,270 --> 00:15:07,730
而在程式框架，你只要一行程式，就能做非常多

211
00:15:07,730 --> 00:15:08,390
的事

212
00:15:08,390 --> 00:15:11,585
例如，如果你不想用梯度下降法，

213
00:15:11,585 --> 00:15:16,075
而是想用Adam優化法，你只消改這行程式

214
00:15:16,075 --> 00:15:21,685
你就能很快換掉，
用另一個比較好的最佳化演算法來代替之。

215
00:15:21,685 --> 00:15:25,905
所有現代的程式框架都支援

216
00:15:25,905 --> 00:15:27,095
這樣的事情

217
00:15:27,095 --> 00:15:32,360
讓你很容易寫出非常複雜的神經網路。

218
00:15:32,360 --> 00:15:33,730
那麼，我希望這能有幫助

219
00:15:33,730 --> 00:15:38,220
給你一些概念，了解 TensorFlow 程式典型的架構。

220
00:15:38,220 --> 00:15:40,360
回顧一下這周的課程：

221
00:15:40,360 --> 00:15:44,900
你見過怎麼有系統地尋找超參數 (hyperparameter)，

222
00:15:44,900 --> 00:15:46,940
我們也談了批次標準化 (batch normalization)

223
00:15:46,940 --> 00:15:49,890
並且要如何利用他來加速訓練，

224
00:15:49,890 --> 00:15:53,580
最後呢，我們談到深度學習的程式框架

225
00:15:53,580 --> 00:15:55,640
現今有很多很棒的程式框架

226
00:15:55,640 --> 00:15:59,710
而最後這部影片，我們聚焦在 TensorFlow 上。

227
00:15:59,710 --> 00:16:03,420
以上，我希望你能喜歡這禮拜的程式練習

228
00:16:03,420 --> 00:16:06,870
幫助你更熟悉這些觀念