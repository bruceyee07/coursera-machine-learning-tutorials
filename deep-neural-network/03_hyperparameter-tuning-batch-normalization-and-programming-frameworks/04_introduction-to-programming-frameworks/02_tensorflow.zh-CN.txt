欢迎来到这一周的最后一节课 目前有很多非常好的深度学习编程框架 其中一个就是TensorFlow 我感到很激动 能帮助你开始学习使用TensorFlow 在这节视频中 我将向你展示TensorFlow的基础结构 然后在这一周的练习问题中 你们将自己练习并学习更多的细节 这一周的编程练习会需要一些时间 所以请确保留出足够的时间来完成它 让我们从一个动机问题开始吧 比方说 这有个代价函数J 我们希望将其最小化 在这个例子中 我们将使用一个很简单的代价函数 J(w)=w^2-10w+25 这就是代价函数 你可能会注意到这个函数实际上是(w-5)的平方 如果你展开这个二次项 你就会得到以上的表达式 使得这个表达式最小的w的值为w=5 但先假设我们并不知道 而且你只有这个表达式 让我们看看如何在TensorFlow中加点东西
来最小化这个表达式 有一个结构很相似的程序 可以用来训练神经网络 而根据你的神经网络的所有参数 你的神经网络可能会有非常复杂的代价函数J(w,b) 同样的 你也可以使用TensorFlow 自动地寻找w和b的值 来最小化代价函数的值 让我们从左边这个更简单的例子开始 我将在Jupyter Notebook上写Python程序
在启动TensorFlow时 把numpy导入为np
把tensorflow导入成rf 是比较常用的写法 接下来让我定义一下参数w 在TensorFlow里
定义参数的方法是 rf.Variable 类型是 dtype=tf.float32 再让我们定义一下代价函数 还记得代价函数是w的平方减10w加25 所以我们用tf.add 然后括号中是w的平方和rf.multiply 第二项是-10乘以w 然后我们再加上25 我们再在这加上tf.add 这就定义了我们的代价函数J 然后我在这里写 train=tf.train.GradientDescentOptimizer 让我们用0.01作为学习速率
我们的目的是最小化代价函数 最后 接下来的几行代码也是非常常用的 init=tf.global_variables_initializer() 接下来是
session=tf.Session() 这就启动了一个TensorFlow的session 用Session.run(init)来初始化全局变量 要让TensorFlow对变量求值
就写session.run(w) 我们现在还没有做任何事情 上面这一行我们初始化w为0
然后定义了一个代价函数 我们定义train为我们的学习算法 它使用GradientDescentOptimizer
来最小化代价函数 但我们目前还没有运行这个学习算法 所以我们用session.run来对w求值
然后打印出session.run(w) 所以 如果我们运行这个格子中的代码 会得到对w求值的结果为0
因为我们还没有运行任何东西 现在 让我们在train上进行session.run操作 这将会运行一步梯度下降 让我们在运行一步梯度下降后对w求值 并将它打印出来 我们在运行一步梯度下降后这么做
w的值现在变成了0.1 让我们现在迭代运行1000次梯度下降
session.run(train) 然后我们再打印出session.run(w) 这将会迭代运行1000次梯度下降 最后w的值将会是4.9999 记住 我们的目的是最小化(w-5)的平方 所以w的最优解5 这个结果已经非常接近了 希望这能让你对TensorFlow程序的大致结构
有一个初步的印象 随着你完成之后的练习
以及接触更多的TensorFlow代码 你会对我这里使用的这些函数更加熟悉 有一点需要注意的是
w是我在这里尝试优化的值 所以我们将把它声明成一个tf.Variable 同时注意我们要做的
是用这些add和multiply等方法 来定义一个代价函数 TensorFlow会自动知道
如何根据add和multiply等等方法 分别求导 这就是为什么你只需要定义前向传播函数 它就会知道如何计算反向传播函数 或说是梯度 因为它已经内置了加法 乘法以及平方等方法的梯度的计算方法 顺便说一句 如果你觉得写成这样很丑 TensorFlow已经重载了加法减法等 常见的计算 这样你就可以把代价函数写成这个更优雅的形式 把这个注释掉再重新跑一遍 就会得到一样的结果 所以一旦w被声明为TensorFlow中的Variable
它的 平方 乘法 加法 以及减法操作都会被重载 这样你就不需要用我上面写的那种难看的语法 现在我还有一个TensorFlow的功能需要告诉你们 就是这个例子 最小化w的固定函数 其中一个需要最小化的函数就是训练集的函数 当给你一些训练数据x 当你训练一个神经网络时 训练数据x会变化 那你怎样才能把训练数据导入一个TensorFlow的程序呢? 我将在这里定义x 你可以把这个过程想象成正在训练数据 或者更准确的说 是训练x和y 但我们这里只有x 在这里把x定义成placeholder 它的类型是float32 让我们把它设置成一个3行1列的矩阵 这个代价函数三个变量前面的系数都是固定的 这个二项式是1乘以w的平方减10乘以w加25 我们可以把这些数字 1减10和25 都变成数据 我将把这个代价函数替换成 cost=x[0][0]*x的平方 +x[1][0]*w+x[2][0] 乘以1 现在x就变成了一种 能控制这个二项式系数的数据 这个placeholder函数会告诉TensorFlow x就是你之后会赋值的东西 让我们定义另一个矩阵
coefficient=np.array([[1.],[-10.],[25.]]) 让我们定义另一个矩阵
coefficient=np.array([[1.],[-10.],[25.]]) loss的值是25 这就是我们将赋给x的数据 最后我们需要一种方法 来把这个矩阵中的系数赋给变量x 这个的语法就是做一步训练的操作 我们需要赋值给x 我在这儿设置feed_dict=x:coefficients 我把这里也改变一下 复制并粘贴到那里 好 我希望我没有任何语法错误。 让我们重新跑一遍 我们应该会得到和之前一样的结果 现在 如果你想改变这个二项式的系数 比如说 你想把这个[-10.]改成[-20.] 我们再把这个改成100 现在这个方程变成了x-10的平方 如果我重新跑一遍 一切正常的话 使得x-10的平方最小的值是w=10 让我们看看 好的 没问题 1000次梯度下降迭代之后 我们得到的w很接近10 当你做了练习之后 你就会发现 TensorFlow中的placeholder是你之后会赋值的变量 这是把训练数据导入代价函数的一个很方便的办法 这是你把数据导入代价函数的语法 当你迭代训练的时候 在feed_dict中 把x的值设置成coefficients 当你在做最小批梯度下降的时候 由于你要在每一次迭代的时候代入一个最小批 所以你需要用feed_dict把训练集的不同的子集 不同的最小批 代入需要数据的代价方程 希望这让你们对TensorFlow能做的事情 有一个初步了解 它之所以如此强大 是因为你只需要指定如何计算代价函数 然后 只需要一两行代码 它就能求导 使用梯度优化器或者Adam优化器 或者其它优化器 让我们再看一眼那段代码 我把它稍微处理了一下 如果其中有一些函数或者变量看起来不好理解 你也不用太担心 随着你多做几次练习 你会对它们更加熟悉 最后一件需要注意的是 这三行代码在TensorFlow中是比较惯用的 有一些程序员也会用这么一种版本 这两个版本的功能是一样的 把session设置成tf.Session()来启动新的进程 然后用session.run(init) 然后使用这个进程来对w求值 并打印结果。 但是这个with结构也用在TensorFlow的其它一些程序中 它和左边的代码是差不多的意思 不过 当在执行这个内循环有异常出现的时候 Python中的这个with命令 在清理的时候会好用一些 所以 之后的练习中 你也会见到它 那这段代码到底在做什么呢? 让我们仔细看看这个等式 TensorFlow程序的核心是计算代价函数 之后TensorFlow会自动求导 并计算出如何最小化代价函数 所以这个等式 或者说这段代码实际上做的是 让TensorFlow构建一个计算草案 计算图会执行以下操作 它会先取得x[0][0] 再取得w并把w平方 然后让x[0][0]和w的平方相乘 这样你就得到了x[0][0]*w的平方等等 对吧? 最终 这样就建立起了方程并计算xw x[0][0]*w^2+x[1][0]*w等等 最终 得到代价函数 最后一项是x[2][0] 并把它加到代价函数里 我这里就不把代价函数的另一种形式写出来了 TensorFlow的好处是 正如像上面的图一样用前向传播 来计算代价函数 TensorFlow已经内置了 所有必须的后向传播方程 所以请记住训练神经网络有一套前向传播方程 和后向传播方程 像TensorFlow这样的编程框架已经内置 必须的后向传播方程 这就是为什么当你使用内置的函数来计算前向传播方程的时候 即使是非常复杂的函数 它也能自动使用后向传播函数来执行后向传播并求导 这就是为什么你不需要显示地执行后向传播 这是编程框架有助于提高你的效率 的事情之一 如果你看看TensorFlow的文档 我只想指出一点 TensorFlow的文档的计算图中 使用的标示 和我画的计算图中略有不同 开始的结点中两者都是x[0][0]与w 之后的结点中TensorFlow文档通常不写值 比如w^2 TensorFlow文档中一般就写出运算符 所以这里是平方的运算 然后这两个用乘法运算结合在一起 等等 最后这里 应该是加法运算 在这里 把x和0相加等到最终值 关于这节课的目的 我想这种计算图的标示 会让你更容易理解 但如果你看看 TensorFlow的文档 如果你看看在文档中的计算图 你会发现这么一种形式 是用操作而不是值来标记的 但这两种表示方法 实际上代表同一种计算图 还有很多事情
你只需要在编程框架中写一行代码就能完成 还有很多事情 
你只需要在编程框架中写一行代码就能完成 比如说 你不想使用梯度下降 而想使用Adam优化器
只需要更改这一行代码 你就能很快地换一种更好的优化算法 所有现代深度学习的编程框架都支持 像这样的东西 这样哪怕是很复杂的神经网络
编程也会很轻松 我希望这能帮助你 对TensorFlow程序的典型结构有一些感觉 回顾一下这周的内容 你看到了如何系统地安排超参数搜索过程 我们也谈到了批标准化 以及你可以如何用它来加速训练神经网络 最后 我们谈到了深度学习的编程框架 有很多非常好的编程框架 我们这最后一节课主要关注了TensorFlow 我希望这些能让你喜欢这周的编程练习 并让你对这些概念更加熟悉
GTC字幕组翻译