1
00:00:00,540 --> 00:00:02,320
이번주 마지막 강의에 오신 것을 환영합니다.

2
00:00:02,320 --> 00:00:05,987
딥러닝에서는 훌륭한 러닝 프로그래밍 프레임웍이 많습니다.

3
00:00:05,987 --> 00:00:07,900
그 중 한가지는 TensorFlow입니다.

4
00:00:07,900 --> 00:00:11,430
TensorFlow에 대해 배우는 법을 가르칠 수 있게되어 매우 기쁩니다.

5
00:00:11,430 --> 00:00:14,760
이번 비디오에선 기본적인 TensorFlow 프로그램의 구조를

6
00:00:14,760 --> 00:00:18,970
보여드리고 여러분이 배울 수 있고, 더 많이 상세적인 내용을 배우고, 

7
00:00:18,970 --> 00:00:22,150
직접 혼자 연습을 해볼 수 있도록
연습문제를 제공할 것입니다.

8
00:00:22,150 --> 00:00:24,660
이번주 연습문제는 마치는데 시간이 다소 소요될 것입니다.

9
00:00:24,660 --> 00:00:27,622
학습할 수 있도록 여유시간을 꼭 남겨두도록 하세요. 

10
00:00:27,622 --> 00:00:29,270
여러분의 흥미를 돋구기 위한 문제로 시작해보겠습니다,

11
00:00:29,270 --> 00:00:34,210
여러분이 비용함수 J가 있다고 해보십시요. 최소화하고 싶은 값이지요.

12
00:00:34,210 --> 00:00:39,857
이 예제에서는 아주 간단한 비용함수인 

13
00:00:39,857 --> 00:00:45,033
J(w) = w 제곱 - 10w + 25을 사용하겠습니다. 그것은 이 비용함수인데요, 

14
00:00:45,033 --> 00:00:46,870
그래서 이것이 바로 비용 함수입니다.

15
00:00:46,870 --> 00:00:53,751
잘 보시면 이 함수는 (w- 5) 제곱이라는 것을 알텐데요.

16
00:00:53,751 --> 00:00:58,569
이 2차함수를 확장해서 풀면 위와 같이 나타납니다. 그러므로, 

17
00:00:58,569 --> 00:01:01,552
이 값을 최소화시키는 값은 w의 값은 5입니다.

18
00:01:01,552 --> 00:01:05,770
이제 이것을 몰랐다고하고, 이 함수만 있었다고 해보겠습니다.

19
00:01:05,770 --> 00:01:10,240
이것을 최소화하기 위해 TensorFlow에 어떻게 도입시킬 수 있는지 알아보겠습니다. 

20
00:01:10,240 --> 00:01:13,960
아주 유사한 유형의 프로그램을 이용하여

21
00:01:13,960 --> 00:01:18,337
신경망을 트레이닝 시키고 J(w, b)와 같은 복잡한 비용함수를 갖을 수 있기 때문입니다.

22
00:01:18,337 --> 00:01:22,934
신경망의 파라미터에 따라서 말이죠. 

23
00:01:22,934 --> 00:01:26,910
비슷하게, TensorFlow를 이용하여

24
00:01:26,910 --> 00:01:33,010
이 비용함수를 최소화하는 w, b의 값을 자동적으로 찾을 수 있습니다. 

25
00:01:33,010 --> 00:01:36,140
여기 왼쪽에 있는 것과 같이 조금 더 간단한 예제로 시작해보겠습니다.

26
00:01:36,140 --> 00:01:41,690
저는 파이썬의 Jupiter notebook을 이용해서
TensorFlow 시작하는데요, 

27
00:01:41,690 --> 00:01:48,390
넘파이를 np로 import 시키고 tf를 이용해서
Tensorflow를 import 합니다.

28
00:01:48,390 --> 00:01:52,979
다음으로, w 매개변수를 정의하겠습니다.

29
00:01:52,979 --> 00:01:58,556
TensorFlow에서는 tf 변수를 이용해서
파라미터를 정의할 것입니다.

30
00:02:01,932 --> 00:02:06,396
Dtype=tf.float32를 입력합니다.

31
00:02:08,480 --> 00:02:10,520
그 다음으로 비용함수를 정의해보겠습니다.

32
00:02:10,520 --> 00:02:15,839
기억하시겠지만 비용함수는 w 제곱- 10w + 25 였습니다.

33
00:02:15,839 --> 00:02:19,239
저는 그럼 tf.add를 이용하겠습니다.

34
00:02:19,239 --> 00:02:26,989
w 제곱 + tf.multiply 를 치겠습니다.

35
00:02:26,989 --> 00:02:31,642
두번째 항은 -10.w 였습니다.

36
00:02:31,642 --> 00:02:35,491
그 후로, 25를 더합니다.

37
00:02:35,491 --> 00:02:40,500
여기에 tf.add 다시한번 추가하겠습니다.

38
00:02:40,500 --> 00:02:43,310
그럼 저희가 가지고 있는 비용함수J가 정의됩니다.

39
00:02:43,310 --> 00:02:48,067
그 다음, train=tf.train.GradientDescentOptimizer를

40
00:02:48,067 --> 00:02:53,945
입력할 것입니다.

41
00:02:53,945 --> 00:03:01,409
러닝속도 0.01로 지정하고, 
목적은 비용을 최소화하는 것입니다.

42
00:03:01,409 --> 00:03:05,850
마지막 라인은 꽤 자연스러운데요, 

43
00:03:05,850 --> 00:03:13,560
Init = tf.global_variables_initializer , 그리고 그 다음으로

44
00:03:13,560 --> 00:03:18,404
session = tf.Sessions입니다.

45
00:03:18,404 --> 00:03:21,044
그러면 TensorFlow 세션이 시작됩니다.

46
00:03:21,044 --> 00:03:25,890
Session.run init 을 입력하여 global variables를 초기화시킵니다.

47
00:03:25,890 --> 00:03:33,135
그 다음, TensorFlow의 evaluative variable은
sess.run w을 쓰도록 하겠습니다.

48
00:03:33,135 --> 00:03:34,320
아직까지 한 것은 딱히 없습니다.

49
00:03:34,320 --> 00:03:39,540
여기 위의 라인으로 w를 0으로 초기화하고, 비용함수를 정의하십시요. 

50
00:03:39,540 --> 00:03:42,668
train이 러닝 알고리즘이 되도록 정의해줍니다.

51
00:03:42,668 --> 00:03:46,206
이것은 GradientDescentOptimizer를 사용하고, 
비용함수를 최소화시켜 줍니다.

52
00:03:46,206 --> 00:03:50,549
하지만 아직 러닝 알고리즘을 
실행시키지는 않았죠

53
00:03:50,549 --> 00:03:55,778
session.run을 통해 w를 평가합니다.
제가 프린트해보겠습니다.

54
00:03:55,778 --> 00:03:56,852
이것을 실행하면, 

55
00:03:56,852 --> 00:04:01,029
w가 0이 나옵니다. 
아직 실행한 것이 없기 때문이죠. 

56
00:04:01,029 --> 00:04:06,420
이제 session.run train을 해보겠습니다.

57
00:04:06,420 --> 00:04:11,190
이것이 하는 것은 
1개의 기울기 강하(gradient descent) 단계를 실행하고

58
00:04:11,190 --> 00:04:15,814
이제는 w의 값을 1단게의 

59
00:04:15,814 --> 00:04:19,870
GradientDescent이루 평가해보겠습니다.
이제 프린트 해보겠습니다.

60
00:04:19,870 --> 00:04:25,690
1단계의 gradient descent이후로는
w의 값이 0.1이 됩니다.

61
00:04:25,690 --> 00:04:33,057
이제 1000개의 iteration을 수행시키고
그러면 .run(train)인데요 

62
00:04:35,543 --> 00:04:41,880
그 다음에 print(session.run w).을 쳐보겠습니다.

63
00:04:41,880 --> 00:04:45,501
이것은 1000개의 gradient descent 업무를 실행할텐데요 

64
00:04:45,501 --> 00:04:47,989
그 이후로는 w는 4.9999가 됩니다.

65
00:04:47,989 --> 00:04:51,914
기억하시겠지만 저희는 
w-5의 제곱을 최소화 시키는 것입니다.

66
00:04:51,914 --> 00:04:56,330
w의 절대값은 그러면 5입니다. 
그리고 여기 이 값과 매우 근접한 값이 나왔죠. 

67
00:04:56,330 --> 00:05:02,680
이것이 TensorFlow의 전반적인 구조에 대한 
내용을 제공했길 바랍니다.

68
00:05:02,680 --> 00:05:08,038
여기 이어지는 연습문제를 통해 TensorFlow
코드를 본인이 직접 더 실습을 하면,

69
00:05:08,038 --> 00:05:12,653
여기 제가 사용하고 있는 함수들이 익숙해질
것입니다.

70
00:05:12,653 --> 00:05:16,836
여기에서 알만한 것은 
w는 최적화시키려고 하는 parameter입니다.

71
00:05:16,836 --> 00:05:19,760
그렇기 때문에 여기 variable로 나타낼 것입니다.

72
00:05:19,760 --> 00:05:24,400
여기 add와 multiply 함수 등등으로 
저희가 해야 했던 것은 

73
00:05:24,400 --> 00:05:26,654
바로 비용함수를 정의하는 것입니다.

74
00:05:26,654 --> 00:05:31,450
TensorFlow는 그리고 
어떻게 derivative를 갖는지 자동으로 압니다.
add와 multiply에 대해서 말이죠. 

75
00:05:31,450 --> 00:05:34,370
또 다른 함수에대해서도 말입니다.

76
00:05:34,370 --> 00:05:38,640
그러므로 여러분은 기본적으로 forward prop만
도입하면 됐습니다.

77
00:05:38,640 --> 00:05:43,560
그러면 그것이 후 방향전파을 할 줄 알고
gradient computation도 할 줄 알게 되기 때문입니다.

78
00:05:43,560 --> 00:05:48,070
그 이유는 그것들이 이미
add 와 multiply, 

79
00:05:48,070 --> 00:05:50,920
그리고 제곱의 함수에 만들어져 있기 때문입니다.

80
00:05:50,920 --> 00:05:55,078
참고로 표기법이 이상하게 생긴 경우, 

81
00:05:55,078 --> 00:05:59,830
TensorFlow 는 사실 지금 
일반적인 플러스 마이너스

82
00:05:59,830 --> 00:06:03,320
등등의 부호로 과부화되어 있습니다.

83
00:06:03,320 --> 00:06:07,480
이것을 다시 이쁜 형식으로 적을 수 있습니다.

84
00:06:07,480 --> 00:06:11,600
cost 에서는 comment를 달고 이것을 재실행해서 
똑같은 결과값을 갖습니다.

85
00:06:11,600 --> 00:06:15,913
그렇게하여 w가 TensorFlow 로 변수로 결정되는 경우,
제곱, 곱하기 

86
00:06:15,913 --> 00:06:18,785
더하기, 빼기 등의 운영이 과부하되어 있습니다.

87
00:06:18,785 --> 00:06:22,575
그러므로 여기 못생긴 것을 쓸 필요가 없습니다.

88
00:06:22,575 --> 00:06:27,848
또한가지 보여주고 싶은 TensorFlow 특성이 있는데요, 

89
00:06:27,848 --> 00:06:31,460
여기 minimize a fix function of w.라는 예제인데요, 

90
00:06:31,460 --> 00:06:34,880
여러분이 최소화하고 싶은 함수중에 하나는 
바로 트레이닝 세트의 함수입니다.

91
00:06:34,880 --> 00:06:38,540
트레이닝 데이터 x가 있는 경우, 

92
00:06:38,540 --> 00:06:43,090
신경망에서 트레이닝을 시키는 경우,
이 트레이닝데이터 x는 변할 수 있습니다.

93
00:06:43,090 --> 00:06:49,242
그러면 이런 트레이닝 데이터를 
어떻게TensorFlow program으로 
이동시킬까요?

94
00:06:49,242 --> 00:06:50,913
이것을 t로 정의할 것인데요 

95
00:06:50,913 --> 00:06:54,811
이것을 트르이닝데이터의 
역할을 맞는다고 생각하십시요 

96
00:06:54,811 --> 00:07:00,008
x와 y값을 가진 트레이닝데이터 말이죠, 
하지만 이번 예제에서는 x 예시밖에 없습니다.

97
00:07:00,008 --> 00:07:03,504
x를 placeholder를 이용해서 정의할 것입니다.

98
00:07:03,504 --> 00:07:10,260
이것은 type float 32 그리고 이것은 [3,1] array로 만들겠습니다.

99
00:07:10,260 --> 00:07:15,489
그리고 여기서는 
비용이 원래 고정 계수가

100
00:07:15,489 --> 00:07:21,308
2차함수에서 3개의 항에 있었는데 
1곱하기 w 제곱 - 10 곱하기 w 더하기 25인데요

101
00:07:21,308 --> 00:07:26,821
여기 숫자 1-10 과 25를 데이터로 바꿀텐데요 

102
00:07:26,821 --> 00:07:33,436
저는 cost를 대체할 것인데요 

103
00:07:33,436 --> 00:07:40,052
cost = x[0][0]*w 제곱

104
00:07:40,052 --> 00:07:47,600
+ x[1][0]*w + x[2][0]로 바꿀 것인데요 

105
00:07:47,600 --> 00:07:49,380
곱하기 1이죠 

106
00:07:49,380 --> 00:07:54,197
그러면 이제 x는 2차함수의 계수를 조종하는

107
00:07:54,197 --> 00:07:59,148
데이터가 될 것입니다.

108
00:07:59,148 --> 00:08:03,805
그리고 여기 placeholder 함수는 
TensorFlow에게

109
00:08:03,805 --> 00:08:08,470
x는 나중에 제공하는 값이라는 것을 알려줍니다.

110
00:08:09,850 --> 00:08:16,669
그러면 이제 또 다른 array를 정의해보겠습니다.
coefficient = np.array입니다.

111
00:08:19,030 --> 00:08:23,514
[1.], [-10.] 이구요, 맞죠 

112
00:08:23,514 --> 00:08:27,554
loss 값은 [25.]였습니다.

113
00:08:27,554 --> 00:08:31,120
그러면 이것이 우리가 x에 대입하는 
데이터인데요

114
00:08:32,300 --> 00:08:38,830
그러면 마지막으로 우리는 여기 
array coefficients를 변수 x에 넣는 방법을 찾아야하는데요, 

115
00:08:38,830 --> 00:08:43,170
프로그래밍 언어로는 이것은 트레이닝 단계를
하는 것과 똑같습니다.

116
00:08:43,170 --> 00:08:49,224
x의 값이 제공되어야 할텐데요, 

117
00:08:49,224 --> 00:08:56,302
여기서 feed_dict = x: coefficients로 지정할 것입니다.

118
00:08:58,005 --> 00:09:04,236
그리고 이것을 변경할 것인데요, 
복사해서 붙힐 것인데요 

119
00:09:04,236 --> 00:09:06,320
syntax오류가 없었길 바라겠습니다.

120
00:09:06,320 --> 00:09:13,330
자 그럼 다시 실행해보겠습니다. 
이전과 똑같은 값이 나와야 할텐데요.

121
00:09:14,710 --> 00:09:19,540
그러면 이제 여러분이 2차함수의 계수를 변경하고 싶으면,

122
00:09:19,540 --> 00:09:24,239
예를 들어 여기 [-10.]을 [-20]으로 바꾼다 해보겠습니다.

123
00:09:24,239 --> 00:09:26,823
그리고 이것을 100으로 바꾸겠습니다.

124
00:09:26,823 --> 00:09:30,947
그러면 이제 이 함수는 x -10 의 제곱인데요

125
00:09:30,947 --> 00:09:34,068
이것을 이제 재실행하면, 

126
00:09:34,068 --> 00:09:39,810
x-10 제곱의 값을 최소화하는 값은 w=10이길 
바랄 것입니다.

127
00:09:39,810 --> 00:09:41,521
한번 볼까여. 아 좋습니다. 

128
00:09:41,521 --> 00:09:46,610
w의 값이 10과 아주 가까운 값으로 나왔는데요. 
1000개의 gradient descent iteration이후에 말이죠. 

129
00:09:46,610 --> 00:09:51,462
문제 연습을 많이 하면 볼 수 있는 것이, 

130
00:09:51,462 --> 00:09:55,490
placeholder는 TensorFlow 의 변수인데요, 
그 값은 나중에 배정됩니다.

131
00:09:55,490 --> 00:10:02,297
그리고 이것은 트레이닝 데이터를 비용함수로 이동시키는 
편안한 방법입니다.

132
00:10:02,297 --> 00:10:07,011
그렇게 하는 방법은 이 syntax를 이용해서 가능한데요, 

133
00:10:07,011 --> 00:10:10,099
트레이닝 iteration을 실행하는 경우

134
00:10:10,099 --> 00:10:15,070
feed_dict 를 이용하여 x를 여기 계수와 
동일하도록 지정해줍니다.

135
00:10:15,070 --> 00:10:18,930
그리고 여러분이 한번의 ieteration마다,
다른 미니 배치를 입력해야하는 경우와 같은

136
00:10:18,930 --> 00:10:22,914
미니 배치 기울기 겅하를 이용하는 경우

137
00:10:22,914 --> 00:10:26,676
다른 iteration에서는 feed_dict 를 이용해서 
다른 일부의 트레이닝세트를 삽입합니다.

138
00:10:26,676 --> 00:10:31,940
비용함수가 데이터를 볼것이라고 생각하는 곳에 
다른 미니배치를 삽입해주는 것입니다.

139
00:10:31,940 --> 00:10:36,120
이것이 TensorFlow 가 하는 것에 대한 내용을 
제공해줬길 바랍니다.

140
00:10:36,120 --> 00:10:37,350
이것이 굉장히 강력한 이유는

141
00:10:37,350 --> 00:10:42,470
어떻게 비용함수를 산출할건지만 
명시하면 되기 때문입니다.

142
00:10:42,470 --> 00:10:44,380
그 다음에, derivative의 값을 갖고,

143
00:10:44,380 --> 00:10:48,760
gradient optimizer 나 add-on optimizer 또는

144
00:10:48,760 --> 00:10:53,200
한줄 또는 두줄의 코드로 이루어진 
또 다른 optimizer 를 적용합니다.

145
00:10:53,200 --> 00:10:55,980
여기 또 다시 코드인데요, 

146
00:10:55,980 --> 00:10:57,840
이것을 조금 더 깨끗히 만들었습니다. 

147
00:10:57,840 --> 00:10:59,830
그리고 이런 함수들이나 

148
00:10:59,830 --> 00:11:04,390
변수들이 이해하기 힘든 경우에
여러분이 연습문제를 

149
00:11:04,390 --> 00:11:09,830
몇번 진행했을 경우 더 익숙해질 것입니다.

150
00:11:09,830 --> 00:11:11,664
마지막으로 말하고 싶은 것은, 

151
00:11:11,664 --> 00:11:16,153
여기 코드라인은
꽤 자연스러운 관용으로 쓰이는데요,

152
00:11:16,153 --> 00:11:20,960
어떤 프로그래머들은 이것과 같은
대안의 유형을 쓸 것입니다.

153
00:11:20,960 --> 00:11:22,880
결론적으론 똑같습니다.

154
00:11:22,880 --> 00:11:25,460
세션을 시작하기 위해서 tf.Session()로 합니다.

155
00:11:25,460 --> 00:11:28,050
그리고 이 세션을 이용해 init, 을 실행합니다.

156
00:11:28,050 --> 00:11:32,650
그 다음에 세션을 이용해 평가합니다. 
say, w 그 다음에 결과를 print합니다.

157
00:11:32,650 --> 00:11:38,000
여기 with construction 은 몇개의 TensorFlow programs에서
쓰이기도 하는데요,

158
00:11:38,000 --> 00:11:41,511
여기 왼쪽에 있는 것과 똑같은 뜻인데요, 

159
00:11:41,511 --> 00:11:47,213
파이썬의 with 명령이 조금더 

160
00:11:47,213 --> 00:11:52,437
error in exception의 경우 정리를 더 잘해줍니다.
inner loop를 실행할때 말이죠. 

161
00:11:52,437 --> 00:11:55,792
이것은 연습문제에서 볼 수 있을 것입니다.

162
00:11:55,792 --> 00:11:58,890
그러면 여기 이코드는 무엇을 하는 것일까요?

163
00:11:58,890 --> 00:12:02,230
여기 공식에 중점을 두겠습니다.

164
00:12:02,230 --> 00:12:07,885
TensorFlow program 의 핵심은 
cost로 어떤 것을 산출한 다음에, 

165
00:12:07,885 --> 00:12:13,410
TensorFlow 가 자동적으로 이런 비용을 최소화 시킬 수 있는
derivative를 알려줍니다.

166
00:12:13,410 --> 00:12:18,710
그러면 여기 이 공식이 또는
여기 코드라인이 하는 것은

167
00:12:18,710 --> 00:12:23,670
TensorFlow 가 computation draft를 
만들 수 있도록 해줍니다.

168
00:12:23,670 --> 00:12:26,840
computation draft는 이런 것을 합니다.
x[0][0] 값을 갖고

169
00:12:26,840 --> 00:12:32,310
w를 갖고 그 값이 제곱이 됩니다.

170
00:12:33,840 --> 00:12:39,382
그런 다음 x[0][0] 은 w 제곱과 곱해집니다.

171
00:12:39,382 --> 00:12:45,160
그러면 x[0][0]*w squared, 값을 갖게 되겠죠 맞죠?

172
00:12:45,160 --> 00:12:49,893
마지막으로 이것이 만들어져 여기 xw을 산출하게 해줍니다.

173
00:12:49,893 --> 00:12:54,650
x[0][0]*w squared + x[1][0]*w + 등등 과 같이 말이죠.

174
00:12:54,650 --> 00:12:58,539
그럼 결과적으로 비용함수를 갖게 되는데요,

175
00:12:58,539 --> 00:13:03,404
마지막으로 더해질 항은 '
x [2][0] 인데요, 이 값은

176
00:13:03,404 --> 00:13:05,751
비용으로 더해집니다.

177
00:13:05,751 --> 00:13:08,398
비용의 포맷은 따로 적지 않겠습니다.

178
00:13:08,398 --> 00:13:13,206
그리고 TensorFlow 의 장점은 
이런 computation draft를 통해 4개의 

179
00:13:13,206 --> 00:13:17,937
prop 어플을 도입하여 

180
00:13:17,937 --> 00:13:21,230
TensorFlow는 이미 있습니다.

181
00:13:21,230 --> 00:13:24,420
모든 필요한 backward 함수말이죠. 

182
00:13:24,420 --> 00:13:29,287
기억하시겠지만 심층신경망을 트레이닝 시키는 것은 
forward 함수의 세트에서입니다.

183
00:13:29,287 --> 00:13:31,424
backward 함수가 아니라요.

184
00:13:31,424 --> 00:13:35,661
Tensor Flow와 같은 프로그래밍 프레임웍은 

185
00:13:35,661 --> 00:13:37,240
이미 빌트인 backward 함수가 있습니다.

186
00:13:37,240 --> 00:13:41,892
그러므로 빌트인 함수를 통해 forward 함수를 
산출하면

187
00:13:41,892 --> 00:13:46,129
자동적으로 backward 함수도 이용해서 

188
00:13:46,129 --> 00:13:51,351
후 방향전파을 도입해서 
derivative를 산출할 수 있습니다.

189
00:13:51,351 --> 00:13:55,180
그렇기 때문에 후 방향전파을 도입할 필요가 없습니다.

190
00:13:55,180 --> 00:13:58,240
이것이 프로그래밍 프레임웍이 

191
00:13:58,240 --> 00:14:00,140
효율적으로 만들어 줄 수 있는 것 중에 한가지
이유입니다.

192
00:14:00,140 --> 00:14:03,470
TensorFlow documentation을 보면, 

193
00:14:03,470 --> 00:14:06,270
TensorFlow documentation은 

194
00:14:06,270 --> 00:14:11,023
조금 computation draft를 그렸을때와는 
다른 표기를 사용했다는 점을 알려드리고 싶습니다. 

195
00:14:11,023 --> 00:14:14,205
그것은 x[0][0] w를 사용합니다.

196
00:14:14,205 --> 00:14:17,408
그리고 w 제곱과 같이 그 값을 쓰기보다는

197
00:14:17,408 --> 00:14:21,100
TensorFlow documentation 은 operation을 적는 편입니다.

198
00:14:21,100 --> 00:14:23,390
그러면 이것은, square operation이고

199
00:14:23,390 --> 00:14:28,520
이 2가지가 곱셈으로 합쳐지는 것과 같이 말이죠. 

200
00:14:28,520 --> 00:14:30,990
마지막으로 이것은 

201
00:14:30,990 --> 00:14:36,420
덧셈의 operation이구요 0에 x를 더하구요
최종값을 찾기 위해 말이죠.

202
00:14:36,420 --> 00:14:40,080
이번 해당 강의에서는 
여기 이 computation draft 에 대한 

203
00:14:40,080 --> 00:14:43,670
표기법이 조금 더 알아듣기 쉬웠을 것 같았습니다.

204
00:14:43,670 --> 00:14:47,010
하지만 TensorFlow documentation을 보면,

205
00:14:47,010 --> 00:14:51,840
computation drafts를 보면 

206
00:14:51,840 --> 00:14:55,630
표기법을 보통 값으로 표현하기보다는

207
00:14:55,630 --> 00:14:58,900
operation으로 표현하는 것을 볼 수 있습니다.

208
00:14:58,900 --> 00:15:01,380
하지만 2개 모두 

209
00:15:01,380 --> 00:15:04,270
동일한 computation draft를 대표합니다.

210
00:15:04,270 --> 00:15:07,730
그리고 한줄의 프로그래밍 프레임웍을 

211
00:15:07,730 --> 00:15:08,390
이용해서 할 수 있는 것이 많습니다.

212
00:15:08,390 --> 00:15:11,585
예를 들어, 만약 여러분이 GradientDescent쓰고 싶지 않으시면, 

213
00:15:11,585 --> 00:15:16,075
대신에 이 코드라인을 바꿔서 add-on Optimizer 사용하고 싶으시다면,

214
00:15:16,075 --> 00:15:21,685
신속히 바꿀 수 있습니다. 더 나은 최적화 알고리즘으로 말이죠. 

215
00:15:21,685 --> 00:15:25,905
그렇기 때문에 최신 딥러닝 프로그램 프레임웍은

216
00:15:25,905 --> 00:15:27,095
이런 것들을 지원하고

217
00:15:27,095 --> 00:15:32,360
이렇게 복잡한 신경망 네트워크 또한 쉽게 코드화시킬 수 있도록
해줍니다.

218
00:15:32,360 --> 00:15:33,730
저는 이 내용이 여러분께 도움이 되었으면 좋겠습니다.

219
00:15:33,730 --> 00:15:38,220
전형적인 TensorFlow 프로그램 구조에 대해 알 수 있었던 시간이였습니다.

220
00:15:38,220 --> 00:15:40,360
이번주의 내용을 복습하자면, 

221
00:15:40,360 --> 00:15:44,900
하이퍼 파라미터 서치 프로세스를 시스템적으로 조직화시키고 

222
00:15:44,900 --> 00:15:46,940
또한, 배치 정규화에 대해 이야기하고, 

223
00:15:46,940 --> 00:15:49,890
이것을 이용해서 신경망 네트워크를 빨리 트레이닝 시키는 방법을 배워보았습니다.

224
00:15:49,890 --> 00:15:53,580
마지막으로 딥러닝의 프로그래밍 프레임웍에대해 이야기해보았는데요.

225
00:15:53,580 --> 00:15:55,640
보신 것과 같이 훌륭한 프로그래밍 프레임웍이 많습니다.

226
00:15:55,640 --> 00:15:59,710
마지막 비디오는 TensorFlow에대해 집중적으로 다루어 보았습니다.

227
00:15:59,710 --> 00:16:03,420
그럼 여러분이 이번주 프로그래밍 연습학습을 흥미롭게 생각하셨길 바랍니다.

228
00:16:03,420 --> 00:16:06,870
이런 아이디어에 대해서 더 많이 친숙할 수 있게 해주는 내용이였습니다.