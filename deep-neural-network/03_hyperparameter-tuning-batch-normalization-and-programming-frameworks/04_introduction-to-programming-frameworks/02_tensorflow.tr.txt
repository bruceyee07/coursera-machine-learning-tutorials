Bu hafta için son videoya hoş geldiniz. Çok sayıda harika, 
derin öğrenme programlama yapıları var. Bunlardan biri TensorFlow. TensorFlow kullanmayı öğrenmeye başlamanıza
 yardımcı olmak için heyecanlıyım. Bu video da yapmak istediklerim
size temel yapıyı gösteriyor TensorFlow programı hakkında daha fazla ayrıntı öğrenmenize
 daha sonra pratik yapmanıza ve bu hafta ki problem alıştırmasını yapabilmenize yardımcı olacak. Bu hafta ki problem alıştırmasını yapmak biraz zaman alacak lütfen bunu yapmak için biraz fazla 
zaman ayırdığınızdan emin olun. Motivasyon problemi, En aza indirmek istediğiniz bir maliyet fonksiyonu
 J'ye sahip olduğunuzu varsayalım. Bu örnek için bunu çok basit bir şekilde kullanacağım maliyet fonksiyonu J(w)= w'nin karesi - 10 w + 25 Yani bu maliyet fonksiyonu. Bu fonksiyonun aslında (w-5)'in karesi olduğunu
 fark edebilirsiniz. Bu ifadeyi genişletirseniz yukarıda ki ifadeyi alırsınız 
ve bu nedenle bunu en aza indiren W değeri W=5'dir. Ama diyelim ki bunu bilmiyorduk ve siz sadece bu işleve sahipsiniz. Hadi bunu en aza indirmek için 
TensorFlow'da nasıl uygulayabileceğimizi görelim. Çünkü, çok benzer bir program yapısı sinir ağınızın tüm parametrelerine bağlı olarak 
karmaşık bir maliyet fonksiyonuna (J (w, b)) sahip olabileceğiniz sinir ağlarını eğitmek için kullanılabilir. Ve benzer şekilde, TensorFlow'u otomatik olarak kullanabileceksiniz böylece bu maliyet fonksiyonunu en aza indirecek 
şekilde w ve b değerlerini bulmaya deneyin. Ama soldaki daha basit örnekle başlayalım. Bunun için Jupyter Notebook'da Python'u çalıştırıyor
 ve TensorFlow'u başlatıyorum. İlk satır da import numpy as np ve 
import tensorflow as tf yazıyorum. Sonra, w parametresini tanımlayayım. Tensorflow'da bir parametreyi tanımlamak için
tf.Variable kullanacaksınız. Dtype = tf.float32. Ve sonra maliyet fonksiyonunun tanımlayalım. Maliyet fonksiyonunun w'nin karesi - 10 w +25 olduğunu hatırlayalım. Sonra tf.add kullanalım. Böylece w'nin karesi + tf.multiply sahip olacağım İkinci terimim -10 ,w. Ve sonra 25'i ekleyeceğim. Şuraya bir tane daha tf.add yazıyorum. Böylece sahip olduğumuz J maliyetini tanımlamış oluruz. Ve sonra, train= yazacağım. tf.train.GradientDescentOptimizer Şimdi 0.01'lik bir öğrenme oranını kullanalım, 
amaç maliyeti en aza indirmektir. Ve son olarak, aşağıdaki birkaç satır oldukça deyimseldir. Init = tf.global_variables_initializer ve sonra session= tf.Sessions. Bu bir TensorFlow oturumu başlatır. Global değişkenleri başlatmak için 
 init değişkenini Session.run içinde yazıyoruz. Ve sonra,TensorFlow'un değerlendirme değişkeni 
için Sess.run(w)'yi kullanacağız. Henüz bir şey yapmadık. Bu yüzden yukarıdaki bu satırla, w'yi sıfıra
 sıfırlayın ve bir maliyet fonksiyonu tanımlayın. Train'i maliyet fonksiyonumuzu en aza indirecek olan GradientDescentOptimizer (Gradyan Düşümü İyileştirici) olarak tanımlarız. Ama biz henüz öğrenme algoritmasını henüz çalıştırmadık, Session.run w'yi değerlendirmemi sağlar. print(session.run(w)) yazalım. Öyleyse bunu çalıştırsak, henüz bir bir şey çalıştırmadığımız için 0'a eşit olur. Şimdi, Bunun yapacağı şey GradientDescent(Gradyan Düşümü)'in bir adımının çalışmasıdır. Ve sonra, GradientDescent (Gradyan Düşümü)'in bir adımından 
sonra w'nin değerinin azaltalım ve yazdıralım. Bu yüzden bunu bir gradyan düşümü(gradient descent)
 setinden yapıyoruz, w şu an 0,1. Şimdi gradyan düşümü(gradient descent)'i 1000 iterasyon çalıştıralım. Ve daha sonra print(session.run w). Yani, Bu GradientDescent(Gradyan Düşümü)'in 1000 yineleme çalıştırabilir ve sonunda w 4.9999 olarak bulunur. Unutmayın, biz w-5 karesini en aza indirdiğimizi söyledik böylece w'nin mutlak değeri 5'tir ve buna çok yaklaşmıştır. Umarım bu, bir TensorFlow programının geniş yapısı 
hakkında size bir fikir verir. Ve aşağıdaki alıştırmayı yaptığınızda ve 
daha fazla TensorFlow kursuyla oynadığınızda, burada kullandığım bu işlevlerin bazıları daha tanıdık hale gelecektir. Bu konuda dikkat edilmesi gereken bazı şeyler, w optimize ettiğim bir parametredir, bu yüzden bunu bir değişken olarak ilan edeceğiz. Yapmamız gereken tek şey, bu ekleme ve çarpma işlevlerini kullanarak bir maliyet fonksiyonu tanımlamak. Ve TensorFlow, diğer işlevler gibi ekleme ve çoğaltma ile ilgili olarak türevlerin nasıl alınacağını otomatik olarak bilir. Bu nedenle temel olarak dört destek sistemi uygulamak 
zorunda kaldınız ve geri problemi veya not hesaplama işleminin nasıl yapılacağını anlayabiliyorsunuz. Çünkü bu zaten kare alma fonksiyonlarının yanı sıra toplama ve çarpma işlemlerine de eklenmiştir. Bu arada,sembolle gösterimi gerçekten kötü görüldüğü durumlarda Aslında TensorFlow normal artı, eksi vb. için ölçümleri aşırı yükledi. Bu nedenle, maliyet için bu güzel
 biçimi de yazabilir ve bunu tekrar yorumlayabilir,tekrar yayınlayabilir ve aynı sonucu alabilirsiniz. Yani bir kez TensorFlow değişkenine sahip
 olduğu beyan edildiğinde, kare alma, çarpma, toplama ve çıkarma işlemleri aşırı yüklenir. Yani yukarıda gördüğüm bu çirkin söz 
dizimini kullanmanıza gerek yok. Şimdi, size göstermek istediğim TensorFlow'un 
bir özelliği daha var, bu w örneğinin bir işlevini en aza indiriyor. En aza indirmek istediğimiz işlevlerden biriside eğitim setimizdir. Bu nedenle, bazı egzersiz verileriniz olsun, x ve nöral bir ağı eğitirken x eğitim verileri değişebilir. Öyleyse bir eğitim verisini TensorFlow 
programına nasıl verebilirsin? t ve X'i tanımlayacağım bunun için geçerli bir eğitim verisi olarak düşünürüm ya da 
x ve y ile gerçekten de eğitim verisini tanımlayacağım ama bu örnekte sadece x var. Yani x'i yer tutucuyla tanımlayacak ve bu bir tip float32 olacak ve bunu bir [3,1] dizisi yapalım. Ve yapacağım şey, buradaki maliyetin, bu ikinci derecedeki Bu ikinci derecedeki üç terimin önünde ki katsayılar 1w'nin karesi eksi 10*w +25. Bu sayıları 1-10 ve 25 arasında verilere dönüştürebiliriz. Öyleyse yapacağım şey maliyetin yerine with cost = x[0][0]*w'nin karesi + x[1][0]*w + x[2][0]. Peki,1 defa Öyleyse,Şimdi X bu 2.derecen fonksiyonun katsayılarını kontrol eden veri türü haline geliyor. Ve bu yer tutucu işlevi, TensorFlow'a x'in daha sonra değer verdiğiniz bir şey olduğunu söyler. Başka bir dizi tanımlayalım, coefficient =np.array, [1.], [-10.] ve evet kayıp değeri [25.] idi. X'e takacağımız veriler bu olacak. Sonuç olarak, bu dizi katsayılarını x değişkenine ve söz dizimini yapmak için bir yola ihtiyacımız var. X için sağlanacak olan değerlerin, Ben burada ayarlayacağım
feed_dict = x:coefficients Ve bunu değiştireceğim, kopyalayıp yapıştıracağım. Tamam, umarım bir söz dizimi hatası almam. Bunu tekrar çalıştırmayı deneyelim ve aynı sonuçları 
daha önce olduğu gibi elde ederiz. Ve şimdi, bu 2.dereceden fonksiyonun katsayılarını değiştirmek istiyorsanız, [-10]'u [-20] yaparak değiştir. Ve bunu 100 ile değiştirelim. Şimdi bu fonksiyon x-10'in karesi Ve eğer bunu tekrar çalıştırırsam, umarım x-10 karesini minimize eden değerin w = 10 olduğunu bulurum. Haydi görelim, güzel,harika ve GradientDescent (Gradyan Düşümü)'i 1000 yineleme çalıştırdıktan sonra 10 değerine çok yaklaştık. Yani, egzersiz yaptığınızda bunu daha çok 
gördüğünüzde, TensorFlow öğesindeki bir yer tutucunun, daha sonra atayacağınız
 bir değişken olduğu anlamına gelir. Ve bu, egzersiz verilerinizi maliyet fonksiyonuna getirmenin uygun bir yoludur. Ayrıca, verilerinizi maliyet işlevine getirme biçiminiz, x'in burada katsayılara eşit olmasını ayarlamak için feed_dict öğesini kullanmak üzere bir eğitim yinelemesi çalıştırdığınızda bu söz dizimi ile olur. Ve eğer her bir yinelemede mini batch GradientDescent(Gradyan Düşümü) yapıyorsanız farklı bir mini parça kullanmanız gerekir,
daha sonra farklı yinelemelerde öğrenme(training) setleriniz farklı alt kümelerini beslemek için feed_dict'i kullanırsınız. Maliyet fonksiyonu verileri görmeyi bekliyor. Umarım bu TensorFlow'un neler yapabileceğine dair bir fikir verir. Ve bunu bu kadar güçlü yapmak için yapmanız gereken tek şey maliyet fonksiyonunu nasıl hesaplayacağınızı belirtmektir. Ve sonra, türevleri alır ve hemen hemen bir veya iki satır kodla bir 
Gradyan (Gradient) iyileştirici veya bir eklenti iyileştirici veya başka bir iyileştirici uygulayabilir. İşte yine kod. Bunu biraz temizledim Ve bu işlevlerin veya değişkenlerin bir kısmı kullanım için biraz esrarengiz görünüyorsa da,birkaç problem alıştırması yaptıktan sonra daha aşina olacaktır. Bahsetmek istediğim son bir şey. Bu üç kod satırı Tensorflow'da oldukça deyimlidir ve bazı programcıların yapacağı şey bu alternatif biçimi kullanmaktır. Temelde aynı şeyi yapar. Oturumu başlatmak için tf.Session() değerini session olarak ayarlayın, sonra oturumu başlatmak için session.run(init)'i kullanın. ve daha sonra değerlendirmek için w'yi kullanıp sonucu yazdırın. Ancak bu yapı bir çok TensorFlow programında da kullanılmaktadır. Az ya da çok, soldaki şeyle aynı şey anlamına gelir. Ancak Python'daki komutla, bu iç döngüyü yürütürken istisnai bir hata durumunda temizleme konusunda biraz daha iyidir. Gördüğünüz gibi bu da aşağıdaki alıştırmadır. Peki bu kod gerçekten ne yapıyor? Bu denkleme odaklanalım. Bir TensorFlow programının kalbi, maliyetle hesaplanacak 
bir şeydir ve daha sonra TensorFlow, türevleri bu maliyetlerin nasıl en aza indirileceği konusunda otomatik olarak hesaplar. Yani bu denklem ya da bu kod 
satırının yaptığı şey TensorFlow'un bir hesaplama taslağı
 oluşturmasına izin vermesidir. Ve bir hesaplama taslağı aşağıdakileri yapar: x [0] [0] alır, w'yi alır ve sonra w'nin karesi olur. Ve sonra x [0] [0] w kare ile çarpılır, böylece x [0] [0] * w kare vb olur değil mi? Ve sonunda, bildiğiniz gibi, bu xw, x [0] [0] * w kare + x [1] [0] * w + vb. Ve sonuç olarak, maliyet fonksiyonunu elde edersiniz. Ve böylece maliyet fonksiyonuna eklenecek son terim x [2] [0] olurdu. Maliyet için başka format yazmayacağım. Ve TensorFlow ile ilgili güzel şey, bu hesaplama taslağı 
aracılığıyla temelde dört destekleyici uygulama uygulayarak hesaplanan maliyet, TensorFlow'da zaten yerleşik olarak bulunmasıdır. Gerekli tüm geriye dönük işlevler. Öyleyse, derin bir sinir ağının eğitiminin geriye doğru işlevler yerine nasıl bir ileri dizi işlevi olduğunu hatırlayın. TensorFlow gibi programlama çatıları zaten gerekli geriye dönük işlevlere sahiptir. Bu nedenle, ileri işlevi hesaplamak için yerleşik işlevleri kullanarak, çok karmaşık işlevlerle bile geri yayılımı uygulamak ve sizin için türevleri hesaplamak için otomatik olarak geriye dönük işlevleri gerçekleştirebilir. Bu yüzden, bu nedenle açık bir şekilde geri yayılım(backprop) uygulamanıza gerek yoktur. Bu, programlama çatılarını gerçekten verimli hale getirmenize yardımcı olan şeylerden biridir. TensorFlow'un dökümanlarına bakarsanız, Sadece TensorFlow belgelerinin, hesaplama taslağını çizmek için yaptığımdan biraz farklı bir gösterim kullandığını belirtmek istiyorum. Bu yüzden x[0][0] w kullanıyor. Ve sonra,w karesi gibi değeri yazmaktan ziyade, TensorFlow dokümantasyonu işlemi yazma eğilimindedir. Yani bu bir kare işlem olacak, ve daha sonra bu iki çarpım işleminde
 birleştirilecek ve böyle devam edecek. Ve sonra, son bir not, sanırım bu son değeri bulmak için x 0'a eklediğiniz bir ekleme işlemi olacaktır. Dolayısıyla, bu sınıfın amaçları için, hesaplama taslağı için bu gösterimin sizin anlamanız için daha kolay olacağını düşündüm. Fakat TensorFlow'un dökümanlarına bakarsanız, dökümanlarda ki hesaplama taslaklarına bakarsanız, notların, değerle değil, işlemlerle etiketlendiği bu alternatif yazıyı görürsünüz. Ancak bu gösterimlerin ikisi de temelde aynı hesaplama taslağını temsil etmektedir. Ve programlama çatılarında (frameworks) sadece bir satır kod ile yapabileceğiniz pek çok şey var. Örneğin, GradientDescent(Gradyan Düşümü) 'i kullanmak istemezseniz, bunun yerine bu kod satırını değiştirerek eklenti iyileştiriciyi'yi kullanmak istiyorsanız, bunu çok hızlı bir şekilde değiştirebilir, daha iyi bir en iyileştirme algoritmasında kullanabilirsiniz. Bu nedenle tüm modern derin öğrenme programlama yapıları bunun gibi şeyleri destekler ve oldukça karmaşık sinir ağlarını bile 
kodlamanızı gerçekten kolaylaştırır. Bu yüzden size bir TensorFlow programının tipik yapısını anlamanız için yardımcı olacağını umuyorum. Bu haftayı özetleyecek olursak,hiper parametre arama sürecini sistematik olarak nasıl organize edeceğinizi gördünüz. Ayrıca, toplu normalleştirme ve sinir ağlarınızın eğitimini hızlandırmak için bunu nasıl kullanabileceğinizi de konuştuk. Son olarak, derin öğrenme çatılarını(frameworks)
 programlamayı konuştuk. Birçok harika programlama yapısı var. Ve bu yapılardan TensorFlow'a odaklanan bu video vardı. Bununla birlikte, umarım bu haftaki programlama 
egzersizinden keyif aldınız ve bu da bu fikirlerle daha da aşinalık kazanmanıza yardımcı oluyor.