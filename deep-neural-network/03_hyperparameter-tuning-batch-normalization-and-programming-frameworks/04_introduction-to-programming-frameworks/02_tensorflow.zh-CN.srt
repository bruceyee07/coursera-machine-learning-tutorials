1
00:00:00,540 --> 00:00:02,320
欢迎来到这一周的最后一节课

2
00:00:02,320 --> 00:00:05,987
目前有很多非常好的深度学习编程框架

3
00:00:05,987 --> 00:00:07,900
其中一个就是TensorFlow

4
00:00:07,900 --> 00:00:11,430
我感到很激动 能帮助你开始学习使用TensorFlow

5
00:00:11,430 --> 00:00:14,760
在这节视频中 我将向你展示TensorFlow的基础结构

6
00:00:14,760 --> 00:00:18,970
然后在这一周的练习问题中

7
00:00:18,970 --> 00:00:22,150
你们将自己练习并学习更多的细节

8
00:00:22,150 --> 00:00:24,660
这一周的编程练习会需要一些时间

9
00:00:24,660 --> 00:00:27,622
所以请确保留出足够的时间来完成它

10
00:00:27,622 --> 00:00:29,270
让我们从一个动机问题开始吧

11
00:00:29,270 --> 00:00:34,210
比方说 这有个代价函数J 我们希望将其最小化

12
00:00:34,210 --> 00:00:39,857
在这个例子中 我们将使用一个很简单的代价函数

13
00:00:39,857 --> 00:00:45,033
J(w)=w^2-10w+25

14
00:00:45,033 --> 00:00:46,870
这就是代价函数

15
00:00:46,870 --> 00:00:53,751
你可能会注意到这个函数实际上是(w-5)的平方

16
00:00:53,751 --> 00:00:58,569
如果你展开这个二次项 你就会得到以上的表达式

17
00:00:58,569 --> 00:01:01,552
使得这个表达式最小的w的值为w=5

18
00:01:01,552 --> 00:01:05,770
但先假设我们并不知道 而且你只有这个表达式

19
00:01:05,770 --> 00:01:10,240
让我们看看如何在TensorFlow中加点东西
来最小化这个表达式

20
00:01:10,240 --> 00:01:13,960
有一个结构很相似的程序 可以用来训练神经网络

21
00:01:13,960 --> 00:01:18,337
而根据你的神经网络的所有参数

22
00:01:18,337 --> 00:01:22,934
你的神经网络可能会有非常复杂的代价函数J(w,b)

23
00:01:22,934 --> 00:01:26,910
同样的 你也可以使用TensorFlow

24
00:01:26,910 --> 00:01:33,010
自动地寻找w和b的值 来最小化代价函数的值

25
00:01:33,010 --> 00:01:36,140
让我们从左边这个更简单的例子开始

26
00:01:36,140 --> 00:01:41,690
我将在Jupyter Notebook上写Python程序
在启动TensorFlow时

27
00:01:41,690 --> 00:01:48,390
把numpy导入为np
把tensorflow导入成rf 是比较常用的写法

28
00:01:48,390 --> 00:01:52,979
接下来让我定义一下参数w

29
00:01:52,979 --> 00:01:58,556
在TensorFlow里
定义参数的方法是 rf.Variable

30
00:02:01,932 --> 00:02:06,396
类型是 dtype=tf.float32

31
00:02:08,480 --> 00:02:10,520
再让我们定义一下代价函数

32
00:02:10,520 --> 00:02:15,839
还记得代价函数是w的平方减10w加25

33
00:02:15,839 --> 00:02:19,239
所以我们用tf.add

34
00:02:19,239 --> 00:02:26,989
然后括号中是w的平方和rf.multiply

35
00:02:26,989 --> 00:02:31,642
第二项是-10乘以w

36
00:02:31,642 --> 00:02:35,491
然后我们再加上25

37
00:02:35,491 --> 00:02:40,500
我们再在这加上tf.add

38
00:02:40,500 --> 00:02:43,310
这就定义了我们的代价函数J

39
00:02:43,310 --> 00:02:48,067
然后我在这里写

40
00:02:48,067 --> 00:02:53,945
train=tf.train.GradientDescentOptimizer

41
00:02:53,945 --> 00:03:01,409
让我们用0.01作为学习速率
我们的目的是最小化代价函数

42
00:03:01,409 --> 00:03:05,850
最后 接下来的几行代码也是非常常用的

43
00:03:05,850 --> 00:03:13,560
init=tf.global_variables_initializer()

44
00:03:13,560 --> 00:03:18,404
接下来是
session=tf.Session()

45
00:03:18,404 --> 00:03:21,044
这就启动了一个TensorFlow的session

46
00:03:21,044 --> 00:03:25,890
用Session.run(init)来初始化全局变量

47
00:03:25,890 --> 00:03:33,135
要让TensorFlow对变量求值
就写session.run(w)

48
00:03:33,135 --> 00:03:34,320
我们现在还没有做任何事情

49
00:03:34,320 --> 00:03:39,540
上面这一行我们初始化w为0
然后定义了一个代价函数

50
00:03:39,540 --> 00:03:42,668
我们定义train为我们的学习算法

51
00:03:42,668 --> 00:03:46,206
它使用GradientDescentOptimizer
来最小化代价函数

52
00:03:46,206 --> 00:03:50,549
但我们目前还没有运行这个学习算法

53
00:03:50,549 --> 00:03:55,778
所以我们用session.run来对w求值
然后打印出session.run(w)

54
00:03:55,778 --> 00:03:56,852
所以 如果我们运行这个格子中的代码

55
00:03:56,852 --> 00:04:01,029
会得到对w求值的结果为0
因为我们还没有运行任何东西

56
00:04:01,029 --> 00:04:06,420
现在 让我们在train上进行session.run操作

57
00:04:06,420 --> 00:04:11,190
这将会运行一步梯度下降

58
00:04:11,190 --> 00:04:15,814
让我们在运行一步梯度下降后对w求值

59
00:04:15,814 --> 00:04:19,870
并将它打印出来

60
00:04:19,870 --> 00:04:25,690
我们在运行一步梯度下降后这么做
w的值现在变成了0.1

61
00:04:25,690 --> 00:04:33,057
让我们现在迭代运行1000次梯度下降
session.run(train)

62
00:04:35,543 --> 00:04:41,880
然后我们再打印出session.run(w)

63
00:04:41,880 --> 00:04:45,501
这将会迭代运行1000次梯度下降

64
00:04:45,501 --> 00:04:47,989
最后w的值将会是4.9999

65
00:04:47,989 --> 00:04:51,914
记住 我们的目的是最小化(w-5)的平方

66
00:04:51,914 --> 00:04:56,330
所以w的最优解5 这个结果已经非常接近了

67
00:04:56,330 --> 00:05:02,680
希望这能让你对TensorFlow程序的大致结构
有一个初步的印象

68
00:05:02,680 --> 00:05:08,038
随着你完成之后的练习
以及接触更多的TensorFlow代码

69
00:05:08,038 --> 00:05:12,653
你会对我这里使用的这些函数更加熟悉

70
00:05:12,653 --> 00:05:16,836
有一点需要注意的是
w是我在这里尝试优化的值

71
00:05:16,836 --> 00:05:19,760
所以我们将把它声明成一个tf.Variable

72
00:05:19,760 --> 00:05:24,400
同时注意我们要做的
是用这些add和multiply等方法

73
00:05:24,400 --> 00:05:26,654
来定义一个代价函数

74
00:05:26,654 --> 00:05:31,450
TensorFlow会自动知道
如何根据add和multiply等等方法

75
00:05:31,450 --> 00:05:34,370
分别求导

76
00:05:34,370 --> 00:05:38,640
这就是为什么你只需要定义前向传播函数

77
00:05:38,640 --> 00:05:43,560
它就会知道如何计算反向传播函数 或说是梯度

78
00:05:43,560 --> 00:05:48,070
因为它已经内置了加法

79
00:05:48,070 --> 00:05:50,920
乘法以及平方等方法的梯度的计算方法

80
00:05:50,920 --> 00:05:55,078
顺便说一句 如果你觉得写成这样很丑

81
00:05:55,078 --> 00:05:59,830
TensorFlow已经重载了加法减法等

82
00:05:59,830 --> 00:06:03,320
常见的计算

83
00:06:03,320 --> 00:06:07,480
这样你就可以把代价函数写成这个更优雅的形式

84
00:06:07,480 --> 00:06:11,600
把这个注释掉再重新跑一遍 就会得到一样的结果

85
00:06:11,600 --> 00:06:15,913
所以一旦w被声明为TensorFlow中的Variable
它的 平方 乘法 加法

86
00:06:15,913 --> 00:06:18,785
以及减法操作都会被重载

87
00:06:18,785 --> 00:06:22,575
这样你就不需要用我上面写的那种难看的语法

88
00:06:22,575 --> 00:06:27,848
现在我还有一个TensorFlow的功能需要告诉你们

89
00:06:27,848 --> 00:06:31,460
就是这个例子 最小化w的固定函数

90
00:06:31,460 --> 00:06:34,880
其中一个需要最小化的函数就是训练集的函数

91
00:06:34,880 --> 00:06:38,540
当给你一些训练数据x

92
00:06:38,540 --> 00:06:43,090
当你训练一个神经网络时 训练数据x会变化

93
00:06:43,090 --> 00:06:49,242
那你怎样才能把训练数据导入一个TensorFlow的程序呢?

94
00:06:49,242 --> 00:06:50,913
我将在这里定义x

95
00:06:50,913 --> 00:06:54,811
你可以把这个过程想象成正在训练数据

96
00:06:54,811 --> 00:07:00,008
或者更准确的说 是训练x和y 但我们这里只有x

97
00:07:00,008 --> 00:07:03,504
在这里把x定义成placeholder

98
00:07:03,504 --> 00:07:10,260
它的类型是float32 让我们把它设置成一个3行1列的矩阵

99
00:07:10,260 --> 00:07:15,489
这个代价函数三个变量前面的系数都是固定的

100
00:07:15,489 --> 00:07:21,308
这个二项式是1乘以w的平方减10乘以w加25

101
00:07:21,308 --> 00:07:26,821
我们可以把这些数字 1减10和25 都变成数据

102
00:07:26,821 --> 00:07:33,436
我将把这个代价函数替换成

103
00:07:33,436 --> 00:07:40,052
cost=x[0][0]*x的平方

104
00:07:40,052 --> 00:07:47,600
+x[1][0]*w+x[2][0]

105
00:07:47,600 --> 00:07:49,380
乘以1

106
00:07:49,380 --> 00:07:54,197
现在x就变成了一种

107
00:07:54,197 --> 00:07:59,148
能控制这个二项式系数的数据

108
00:07:59,148 --> 00:08:03,805
这个placeholder函数会告诉TensorFlow

109
00:08:03,805 --> 00:08:08,470
x就是你之后会赋值的东西

110
00:08:09,850 --> 00:08:16,669
让我们定义另一个矩阵
coefficient=np.array([[1.],[-10.],[25.]])

111
00:08:19,030 --> 00:08:23,514
让我们定义另一个矩阵
coefficient=np.array([[1.],[-10.],[25.]])

112
00:08:23,514 --> 00:08:27,554
loss的值是25

113
00:08:27,554 --> 00:08:31,120
这就是我们将赋给x的数据

114
00:08:32,300 --> 00:08:38,830
最后我们需要一种方法 来把这个矩阵中的系数赋给变量x

115
00:08:38,830 --> 00:08:43,170
这个的语法就是做一步训练的操作

116
00:08:43,170 --> 00:08:49,224
我们需要赋值给x

117
00:08:49,224 --> 00:08:56,302
我在这儿设置feed_dict=x:coefficients

118
00:08:58,005 --> 00:09:04,236
我把这里也改变一下 复制并粘贴到那里

119
00:09:04,236 --> 00:09:06,320
好 我希望我没有任何语法错误。

120
00:09:06,320 --> 00:09:13,330
让我们重新跑一遍 我们应该会得到和之前一样的结果

121
00:09:14,710 --> 00:09:19,540
现在 如果你想改变这个二项式的系数

122
00:09:19,540 --> 00:09:24,239
比如说 你想把这个[-10.]改成[-20.]

123
00:09:24,239 --> 00:09:26,823
我们再把这个改成100

124
00:09:26,823 --> 00:09:30,947
现在这个方程变成了x-10的平方

125
00:09:30,947 --> 00:09:34,068
如果我重新跑一遍

126
00:09:34,068 --> 00:09:39,810
一切正常的话 使得x-10的平方最小的值是w=10

127
00:09:39,810 --> 00:09:41,521
让我们看看 好的 没问题

128
00:09:41,521 --> 00:09:46,610
1000次梯度下降迭代之后 我们得到的w很接近10

129
00:09:46,610 --> 00:09:51,462
当你做了练习之后 你就会发现

130
00:09:51,462 --> 00:09:55,490
TensorFlow中的placeholder是你之后会赋值的变量

131
00:09:55,490 --> 00:10:02,297
这是把训练数据导入代价函数的一个很方便的办法

132
00:10:02,297 --> 00:10:07,011
这是你把数据导入代价函数的语法

133
00:10:07,011 --> 00:10:10,099
当你迭代训练的时候

134
00:10:10,099 --> 00:10:15,070
在feed_dict中 把x的值设置成coefficients

135
00:10:15,070 --> 00:10:18,930
当你在做最小批梯度下降的时候

136
00:10:18,930 --> 00:10:22,914
由于你要在每一次迭代的时候代入一个最小批

137
00:10:22,914 --> 00:10:26,676
所以你需要用feed_dict把训练集的不同的子集 不同的最小批

138
00:10:26,676 --> 00:10:31,940
代入需要数据的代价方程

139
00:10:31,940 --> 00:10:36,120
希望这让你们对TensorFlow能做的事情 有一个初步了解

140
00:10:36,120 --> 00:10:37,350
它之所以如此强大

141
00:10:37,350 --> 00:10:42,470
是因为你只需要指定如何计算代价函数

142
00:10:42,470 --> 00:10:44,380
然后 只需要一两行代码 它就能求导

143
00:10:44,380 --> 00:10:48,760
使用梯度优化器或者Adam优化器

144
00:10:48,760 --> 00:10:53,200
或者其它优化器

145
00:10:53,200 --> 00:10:55,980
让我们再看一眼那段代码

146
00:10:55,980 --> 00:10:57,840
我把它稍微处理了一下

147
00:10:57,840 --> 00:10:59,830
如果其中有一些函数或者变量看起来不好理解

148
00:10:59,830 --> 00:11:04,390
你也不用太担心 随着你多做几次练习

149
00:11:04,390 --> 00:11:09,830
你会对它们更加熟悉

150
00:11:09,830 --> 00:11:11,664
最后一件需要注意的是

151
00:11:11,664 --> 00:11:16,153
这三行代码在TensorFlow中是比较惯用的

152
00:11:16,153 --> 00:11:20,960
有一些程序员也会用这么一种版本

153
00:11:20,960 --> 00:11:22,880
这两个版本的功能是一样的

154
00:11:22,880 --> 00:11:25,460
把session设置成tf.Session()来启动新的进程

155
00:11:25,460 --> 00:11:28,050
然后用session.run(init)

156
00:11:28,050 --> 00:11:32,650
然后使用这个进程来对w求值 并打印结果。

157
00:11:32,650 --> 00:11:38,000
但是这个with结构也用在TensorFlow的其它一些程序中

158
00:11:38,000 --> 00:11:41,511
它和左边的代码是差不多的意思

159
00:11:41,511 --> 00:11:47,213
不过 当在执行这个内循环有异常出现的时候

160
00:11:47,213 --> 00:11:52,437
Python中的这个with命令 在清理的时候会好用一些

161
00:11:52,437 --> 00:11:55,792
所以 之后的练习中 你也会见到它

162
00:11:55,792 --> 00:11:58,890
那这段代码到底在做什么呢?

163
00:11:58,890 --> 00:12:02,230
让我们仔细看看这个等式

164
00:12:02,230 --> 00:12:07,885
TensorFlow程序的核心是计算代价函数

165
00:12:07,885 --> 00:12:13,410
之后TensorFlow会自动求导 并计算出如何最小化代价函数

166
00:12:13,410 --> 00:12:18,710
所以这个等式 或者说这段代码实际上做的是

167
00:12:18,710 --> 00:12:23,670
让TensorFlow构建一个计算草案

168
00:12:23,670 --> 00:12:26,840
计算图会执行以下操作 它会先取得x[0][0]

169
00:12:26,840 --> 00:12:32,310
再取得w并把w平方

170
00:12:33,840 --> 00:12:39,382
然后让x[0][0]和w的平方相乘

171
00:12:39,382 --> 00:12:45,160
这样你就得到了x[0][0]*w的平方等等 对吧?

172
00:12:45,160 --> 00:12:49,893
最终 这样就建立起了方程并计算xw

173
00:12:49,893 --> 00:12:54,650
x[0][0]*w^2+x[1][0]*w等等

174
00:12:54,650 --> 00:12:58,539
最终 得到代价函数

175
00:12:58,539 --> 00:13:03,404
最后一项是x[2][0]

176
00:13:03,404 --> 00:13:05,751
并把它加到代价函数里

177
00:13:05,751 --> 00:13:08,398
我这里就不把代价函数的另一种形式写出来了

178
00:13:08,398 --> 00:13:13,206
TensorFlow的好处是 正如像上面的图一样用前向传播

179
00:13:13,206 --> 00:13:17,937
来计算代价函数

180
00:13:17,937 --> 00:13:21,230
TensorFlow已经内置了

181
00:13:21,230 --> 00:13:24,420
所有必须的后向传播方程

182
00:13:24,420 --> 00:13:29,287
所以请记住训练神经网络有一套前向传播方程

183
00:13:29,287 --> 00:13:31,424
和后向传播方程

184
00:13:31,424 --> 00:13:35,661
像TensorFlow这样的编程框架已经内置

185
00:13:35,661 --> 00:13:37,240
必须的后向传播方程

186
00:13:37,240 --> 00:13:41,892
这就是为什么当你使用内置的函数来计算前向传播方程的时候

187
00:13:41,892 --> 00:13:46,129
即使是非常复杂的函数

188
00:13:46,129 --> 00:13:51,351
它也能自动使用后向传播函数来执行后向传播并求导

189
00:13:51,351 --> 00:13:55,180
这就是为什么你不需要显示地执行后向传播

190
00:13:55,180 --> 00:13:58,240
这是编程框架有助于提高你的效率

191
00:13:58,240 --> 00:14:00,140
的事情之一

192
00:14:00,140 --> 00:14:03,470
如果你看看TensorFlow的文档

193
00:14:03,470 --> 00:14:06,270
我只想指出一点 TensorFlow的文档的计算图中

194
00:14:06,270 --> 00:14:11,023
使用的标示 和我画的计算图中略有不同

195
00:14:11,023 --> 00:14:14,205
开始的结点中两者都是x[0][0]与w

196
00:14:14,205 --> 00:14:17,408
之后的结点中TensorFlow文档通常不写值 比如w^2

197
00:14:17,408 --> 00:14:21,100
TensorFlow文档中一般就写出运算符

198
00:14:21,100 --> 00:14:23,390
所以这里是平方的运算

199
00:14:23,390 --> 00:14:28,520
然后这两个用乘法运算结合在一起 等等

200
00:14:28,520 --> 00:14:30,990
最后这里 应该是加法运算

201
00:14:30,990 --> 00:14:36,420
在这里 把x和0相加等到最终值

202
00:14:36,420 --> 00:14:40,080
关于这节课的目的 我想这种计算图的标示

203
00:14:40,080 --> 00:14:43,670
会让你更容易理解

204
00:14:43,670 --> 00:14:47,010
但如果你看看 TensorFlow的文档

205
00:14:47,010 --> 00:14:51,840
如果你看看在文档中的计算图

206
00:14:51,840 --> 00:14:55,630
你会发现这么一种形式

207
00:14:55,630 --> 00:14:58,900
是用操作而不是值来标记的

208
00:14:58,900 --> 00:15:01,380
但这两种表示方法

209
00:15:01,380 --> 00:15:04,270
实际上代表同一种计算图

210
00:15:04,270 --> 00:15:07,730
还有很多事情
你只需要在编程框架中写一行代码就能完成

211
00:15:07,730 --> 00:15:08,390
还有很多事情 
你只需要在编程框架中写一行代码就能完成

212
00:15:08,390 --> 00:15:11,585
比如说 你不想使用梯度下降

213
00:15:11,585 --> 00:15:16,075
而想使用Adam优化器
只需要更改这一行代码

214
00:15:16,075 --> 00:15:21,685
你就能很快地换一种更好的优化算法

215
00:15:21,685 --> 00:15:25,905
所有现代深度学习的编程框架都支持

216
00:15:25,905 --> 00:15:27,095
像这样的东西

217
00:15:27,095 --> 00:15:32,360
这样哪怕是很复杂的神经网络
编程也会很轻松

218
00:15:32,360 --> 00:15:33,730
我希望这能帮助你

219
00:15:33,730 --> 00:15:38,220
对TensorFlow程序的典型结构有一些感觉

220
00:15:38,220 --> 00:15:40,360
回顾一下这周的内容

221
00:15:40,360 --> 00:15:44,900
你看到了如何系统地安排超参数搜索过程

222
00:15:44,900 --> 00:15:46,940
我们也谈到了批标准化

223
00:15:46,940 --> 00:15:49,890
以及你可以如何用它来加速训练神经网络

224
00:15:49,890 --> 00:15:53,580
最后 我们谈到了深度学习的编程框架

225
00:15:53,580 --> 00:15:55,640
有很多非常好的编程框架

226
00:15:55,640 --> 00:15:59,710
我们这最后一节课主要关注了TensorFlow

227
00:15:59,710 --> 00:16:03,420
我希望这些能让你喜欢这周的编程练习

228
00:16:03,420 --> 00:16:06,870
并让你对这些概念更加熟悉
GTC字幕组翻译