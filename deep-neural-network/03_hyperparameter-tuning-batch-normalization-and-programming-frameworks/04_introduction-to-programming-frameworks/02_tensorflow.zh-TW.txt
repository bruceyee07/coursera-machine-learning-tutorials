歡迎來到本週最後的影片 目前有很多很好的深度學習程式框架， 其中一個叫 TensorFlow 我很開心能幫助你、帶你入門 在這部影片我想採取的方式是， 示範 TensorFlow 程式基本的架構，
讓你自行練習，了解更多細節 最後在本周作業練習。 這禮拜的作業會需要比較長的時間， 所以請務必保留更多的時間。 以這個問題來出發， 假設你有某個成本函數 J，想要讓他變最小 在這個例子，我想用一個非常簡單的 成本函數做為例子 J(w) = w^2 - 10w + 25. 所以這個是成本函數 你可能會注意到這個函數實際上是 (w-5) 的平方 如果你展開這個二次式，你會得到上面這個方程式 所以讓其最小的 w 值會等於 5 但假設我們不知道這個答案；你就只有這個函數 讓我們瞧瞧如何用 TensorFlow 解決這個最小化 因為其類似架構的程式可以拿來訓練神經網路： 你會有一個複雜的成本函數 J(W, b) 由你神經網路所有的參數所決定 然後類似地，你可以拿 TensorFlow 來自動找到讓成本最小的 w 和 b 的值 不過，先讓我們從左邊比較簡單的例子來開始 那，我在我的 Jupyter notebook 上執行 Python，
要開始 TensorFlow 的話 你 "import numpy as np"，
然後這是慣例寫法 "import tensorflow as tf" 接下來，讓我定義參數 w 那在 TensorFlow，你會用 tf.Variable 來定義一個參數 dtype=tf.float32 接下來讓我們定義成本函數 還記得我們的成本函數是
 w^2 - 10w + 25 所以讓我用 tf.add 所以我們有 w 平方，加，tf.multiply 第二項是 -10 乘上 w 然後我把那個加上 25 所以讓我在這邊放另一個 tf.add 這樣就定義好我們的成本函數 J 了 然後我要寫 "train = tf.train.GradientDescentOptimizer" 讓我們用 0.01 做為學習率，
然後目標是最小化 (minimize) 成本 最後呢，以下這幾行算是慣例寫法 "init = tf.global_variables_initializer()" 然後 "session = tf.Session()" 所以這會啟動一個 TensorFlow 工作階段 (session) "session.run(init)" 來初始化全域變數 然後，為了讓 TensorFlow 執行這變數，
我們會用 "sess.run(w)" 我們目前還沒做什麼 所以上面這行，初始 w 為 0，然後定義成本函數 我們定義 train 為我們的學習演算法， 利用梯度下降優化器 (gradient descent optimizer)
把成本函數最小化 (minimize) 但是，我們尚未真正執行這個學習演算法 session.run，我們取值出 w；
讓我印出 session.run 的結果 所以如果我們執行這個 他算出 w 等於 0，因為我們還沒真正執行任何東西 現在呢，讓我們做 "session.run(train)" 所以這一行做的，是執行一步的梯度下降 然後在這一步梯度下降後， 讓我們再取值 w，然後印出來。 所以經過這一步的梯度下降後，w 現在是 0.1 我們現在跑個 1000 步的梯度下降，所以... .run(train) 然後來印個 print(session.run(w)) 所以這會跑一千次的梯度下降 而最後 w 變 4.99999 還記得我們要把 (w-5) 的平方最小化 所以 w 最佳值是 5，非常接近這個值。 希望這能給你一些對 TensorFlow 程式大致的架構一些感覺 而當你在做程式作業，自己玩過更多 TensorFlow 的程式後 這邊我用的一些函式會更眼熟的 這邊要注意的是，w 是我要去最佳化的參數 所以我們宣告他為一個變數 (Variable) 再注意到我們要做的，只是定義成本函數
— 利用這些加 (add)、 乘 (multiply) 等等的函式 TensorFlow 自己知道怎麼微分取導數
套用到這些「加」、 「乘」、還有其他函式 這就是為何我們只需實作正向傳播的部份 他自己能知道怎麼做反向傳播或是計算梯度 因為那些計算已經內建在加、乘 還有平方的函式 題外話，如果你覺得這堆英文很醜 實際上 TensorFlow 有把
這些運算子多載 (overload) 對於常見的加啦、減啦、等等 所以你也可以把這成本
寫成這種比較好看的形式， 把上一行註解掉，重跑，也會得到同樣結果 所以只要 w 被定義成一個 TensorFlow 變數，
那些平方、相乘、 加和減的運算子都被多載 所以你就不用我上面那樣醜醜的語法來寫。 還有，我想要示範另一個 TensorFlow 的功能： 在這個例子，最小化的是一個既定的 w 的函數 然而，有時你想最小化的
會是訓練集的函數 所以，萬一你有某些訓練資料 x， 而當訓練神經網路的時候，訓練資料 x 會變動 那麼，你要怎麼把訓練資料弄進 TensorFlow 程式中？ 所以我要定義 x 把這看作扮演著訓練集的角色 或者是有x和y的訓練集，只不過在這例子我們只有 x 所以這一行把 x 定義成一個 "placeholder" 而且會是 float32 的型態，而且讓這個是 3乘1 的矩陣 我現在要做的是：雖然這成本函數有固定的係數 就是二次方程式這三項前面的數字：
1 乘上w平方、減掉 10 乘w、再加 25 我們可以把這邊的數字，1, -10 和 25 換成資料 所以我這邊要做的是把成本換掉 用 cost 等於 x[0][0] 乘以w平方 加 x[1][0] 乘以 w，加 x[2][0] 嗯... 乘以 1 所以 x 現在變成像是 控制這二次方程式係數的資料 而這邊的 placeholder 函式告訴 TensorFlow 你之後會再把某個值給 x。 那我們再定義另一個陣列，coefficient = np.array( [1.], [-10.] 還有對，最後一個值是 [25.] 所以這就是我們等等會放入 x 的資料 那麼最後，我們需要有個方法把這個陣列 coefficients
餵進去給變數 x 做這個的語法呢，是在訓練的步驟 需要把值給 x，所以 我要在這邊設定，feed_dict = x 對應到 coefficients 我要把這改一下，我要把這個拷貝也貼到這邊 好的，希望我語法沒有錯誤 讓我們來重跑這個，希望得到和剛剛一樣的結果。 現在呢，如果你想改變這個二次方程式的係數 假設你把這個 [-10.] 改成 [-20.] 還有把這個改成 100 所以這是代表 (x-10)的平方的函數 那如果我現在重跑一次，希望 我找到讓 (x-10)平方最小的值會是 w=10 讓我們看看，讚 在1000步的梯度下降以後，我們得到的 w 很靠近 10。 所以你在程式作業會看到更多的例子： TensorFlow的placeholder是一個你
等一下會給值的變數 而這是讓你把訓練集放入成本函數
的一個方便的方法 而要把資料放入成本函數，就是用這個語法 當你在跑訓練迭代的步驟 利用 feed_dict，把 x 設成這邊的 coefficients 而如果你用的是小批次梯度下降，
其中每次的迭代 你必須塞入不同的小批資料，那麼在不同的迭代步驟 你就用 feed_dict 把訓練資料中不同的部份餵進去； 把不同的小批資料塞入
成本函數裡拿資料的地方。 希望這能給你一些概念，知道 TensorFlow 能做什麼 他強大之處 在於你只需要寫好怎麼算出成本函數 然後它就會做微分 還會套用梯度下降最佳化，或是Adam最佳化 或是其他最佳化 — 只需要一兩行的程式就夠了 所以再看一次程式碼 我稍微整理了一下 萬一這些函式 或變數看起來有點神秘，你會更熟悉的 — 當你開始做程式作業，用了他們幾回以後。 最後我想講一件事 這三行程式在 TensorFlow 裡面是很慣例的寫法， 然而有些人會用另一種寫法來代替 其實做的都一樣 把 session 設成 tf.Session() 以啟動這個 session 然後用這個 session 執行 init 然後用 session 來取值/執行 w，然後印出結果。 而這邊的 "with" 架構在 TensorFlow 程式內也很常用到 他和左邊這些意思上差不多 不過在 Python 用 with 這個指令，當執行 裡面這層的指令碰到錯誤的時候，他清理得比較好 所以你也會在程式作業裡看到這種寫法。 那麼，這一個程式到底在做什麼？ 讓我們專心看這個式子 TensorFlow 最重要的核心是成本函數的計算， 然後 TensorFlow 自動算出其導數，
來了解如何把成本變最小 所以這個方程式，或說是這行程式碼所作的 是讓 TensorFlow 能夠建造一個計算圖
(computation graph) 而計算圖做這樣的事：他拿著 x[0][0]， 他拿著 w，然後把 w 平方 然後 x[0][0] 和 w平方相乘 所以你得到 x[0][0] * w^2，依此類推 最終呢，你知道的，這一直建構下去，計算出 x[0][0]*w^2 + x[1][0]*w + ... 所以最後你就得到了成本函數 — 我想最後加的一項是 x[2][0] 這邊，加進去這個成本 我就省略不寫這成本其他的公式了。 還有，TensorFlow 一個好處是，你只要實作出 在計算圖上，正向傳播往前算出成本， TensorFlow 已經內建了 所有會用到的反向函式 還記得訓練神經網路時，會用一些正向的函數 還有一些反向的函數 像 TensorFlow 這種程式框架，早已內建了必要的 反向函式 這就是為什麼，當你用內建的函式計算正向的函數 他能夠自動執行反向的函數，來實踐反向傳播 即使函數再怎麼複雜，他都能幫你算出導數 這就是為何你不需要真的實作反向傳播。 這就是其中的一個例子，程式框架 能讓你更有效率。 如果你去讀 TensorFlow 的文件 我只是想提醒一下，TensowFlow 文件 在畫計算圖的時候，用了和我稍微不同的表示法。 他用 x[0][0]、w 然後他不是把值寫出來，例如 w**2， TensorFlow 文件裡通常只寫出運算子 所以這會是個「平方運算」 然後這兩個用「乘法運算」合併起來，依此類推 然後最後一個節點，我想會是 一個「加法運算」：你加上 x[2][0]，得到最後結果。 為了這門課，我覺得這一種表示法 會讓你比較容易理解計算圖 不過如果你去讀 TensorFlow 文件 如果你看文件裡面的計算圖 你會看到這另一種慣例，也就是節點標示的 是「運算」而不是「值」 不過呢，這兩種表示法 基本上都是代表同一個計算圖。 而在程式框架，你只要一行程式，就能做非常多 的事 例如，如果你不想用梯度下降法， 而是想用Adam優化法，你只消改這行程式 你就能很快換掉，
用另一個比較好的最佳化演算法來代替之。 所有現代的程式框架都支援 這樣的事情 讓你很容易寫出非常複雜的神經網路。 那麼，我希望這能有幫助 給你一些概念，了解 TensorFlow 程式典型的架構。 回顧一下這周的課程： 你見過怎麼有系統地尋找超參數 (hyperparameter)， 我們也談了批次標準化 (batch normalization) 並且要如何利用他來加速訓練， 最後呢，我們談到深度學習的程式框架 現今有很多很棒的程式框架 而最後這部影片，我們聚焦在 TensorFlow 上。 以上，我希望你能喜歡這禮拜的程式練習 幫助你更熟悉這些觀念