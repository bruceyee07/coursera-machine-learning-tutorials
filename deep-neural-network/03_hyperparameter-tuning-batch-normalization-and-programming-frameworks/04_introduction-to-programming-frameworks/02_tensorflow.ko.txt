이번주 마지막 강의에 오신 것을 환영합니다. 딥러닝에서는 훌륭한 러닝 프로그래밍 프레임웍이 많습니다. 그 중 한가지는 TensorFlow입니다. TensorFlow에 대해 배우는 법을 가르칠 수 있게되어 매우 기쁩니다. 이번 비디오에선 기본적인 TensorFlow 프로그램의 구조를 보여드리고 여러분이 배울 수 있고, 더 많이 상세적인 내용을 배우고, 직접 혼자 연습을 해볼 수 있도록
연습문제를 제공할 것입니다. 이번주 연습문제는 마치는데 시간이 다소 소요될 것입니다. 학습할 수 있도록 여유시간을 꼭 남겨두도록 하세요. 여러분의 흥미를 돋구기 위한 문제로 시작해보겠습니다, 여러분이 비용함수 J가 있다고 해보십시요. 최소화하고 싶은 값이지요. 이 예제에서는 아주 간단한 비용함수인 J(w) = w 제곱 - 10w + 25을 사용하겠습니다. 그것은 이 비용함수인데요, 그래서 이것이 바로 비용 함수입니다. 잘 보시면 이 함수는 (w- 5) 제곱이라는 것을 알텐데요. 이 2차함수를 확장해서 풀면 위와 같이 나타납니다. 그러므로, 이 값을 최소화시키는 값은 w의 값은 5입니다. 이제 이것을 몰랐다고하고, 이 함수만 있었다고 해보겠습니다. 이것을 최소화하기 위해 TensorFlow에 어떻게 도입시킬 수 있는지 알아보겠습니다. 아주 유사한 유형의 프로그램을 이용하여 신경망을 트레이닝 시키고 J(w, b)와 같은 복잡한 비용함수를 갖을 수 있기 때문입니다. 신경망의 파라미터에 따라서 말이죠. 비슷하게, TensorFlow를 이용하여 이 비용함수를 최소화하는 w, b의 값을 자동적으로 찾을 수 있습니다. 여기 왼쪽에 있는 것과 같이 조금 더 간단한 예제로 시작해보겠습니다. 저는 파이썬의 Jupiter notebook을 이용해서
TensorFlow 시작하는데요, 넘파이를 np로 import 시키고 tf를 이용해서
Tensorflow를 import 합니다. 다음으로, w 매개변수를 정의하겠습니다. TensorFlow에서는 tf 변수를 이용해서
파라미터를 정의할 것입니다. Dtype=tf.float32를 입력합니다. 그 다음으로 비용함수를 정의해보겠습니다. 기억하시겠지만 비용함수는 w 제곱- 10w + 25 였습니다. 저는 그럼 tf.add를 이용하겠습니다. w 제곱 + tf.multiply 를 치겠습니다. 두번째 항은 -10.w 였습니다. 그 후로, 25를 더합니다. 여기에 tf.add 다시한번 추가하겠습니다. 그럼 저희가 가지고 있는 비용함수J가 정의됩니다. 그 다음, train=tf.train.GradientDescentOptimizer를 입력할 것입니다. 러닝속도 0.01로 지정하고, 
목적은 비용을 최소화하는 것입니다. 마지막 라인은 꽤 자연스러운데요, Init = tf.global_variables_initializer , 그리고 그 다음으로 session = tf.Sessions입니다. 그러면 TensorFlow 세션이 시작됩니다. Session.run init 을 입력하여 global variables를 초기화시킵니다. 그 다음, TensorFlow의 evaluative variable은
sess.run w을 쓰도록 하겠습니다. 아직까지 한 것은 딱히 없습니다. 여기 위의 라인으로 w를 0으로 초기화하고, 비용함수를 정의하십시요. train이 러닝 알고리즘이 되도록 정의해줍니다. 이것은 GradientDescentOptimizer를 사용하고, 
비용함수를 최소화시켜 줍니다. 하지만 아직 러닝 알고리즘을 
실행시키지는 않았죠 session.run을 통해 w를 평가합니다.
제가 프린트해보겠습니다. 이것을 실행하면, w가 0이 나옵니다. 
아직 실행한 것이 없기 때문이죠. 이제 session.run train을 해보겠습니다. 이것이 하는 것은 
1개의 기울기 강하(gradient descent) 단계를 실행하고 이제는 w의 값을 1단게의 GradientDescent이루 평가해보겠습니다.
이제 프린트 해보겠습니다. 1단계의 gradient descent이후로는
w의 값이 0.1이 됩니다. 이제 1000개의 iteration을 수행시키고
그러면 .run(train)인데요 그 다음에 print(session.run w).을 쳐보겠습니다. 이것은 1000개의 gradient descent 업무를 실행할텐데요 그 이후로는 w는 4.9999가 됩니다. 기억하시겠지만 저희는 
w-5의 제곱을 최소화 시키는 것입니다. w의 절대값은 그러면 5입니다. 
그리고 여기 이 값과 매우 근접한 값이 나왔죠. 이것이 TensorFlow의 전반적인 구조에 대한 
내용을 제공했길 바랍니다. 여기 이어지는 연습문제를 통해 TensorFlow
코드를 본인이 직접 더 실습을 하면, 여기 제가 사용하고 있는 함수들이 익숙해질
것입니다. 여기에서 알만한 것은 
w는 최적화시키려고 하는 parameter입니다. 그렇기 때문에 여기 variable로 나타낼 것입니다. 여기 add와 multiply 함수 등등으로 
저희가 해야 했던 것은 바로 비용함수를 정의하는 것입니다. TensorFlow는 그리고 
어떻게 derivative를 갖는지 자동으로 압니다.
add와 multiply에 대해서 말이죠. 또 다른 함수에대해서도 말입니다. 그러므로 여러분은 기본적으로 forward prop만
도입하면 됐습니다. 그러면 그것이 후 방향전파을 할 줄 알고
gradient computation도 할 줄 알게 되기 때문입니다. 그 이유는 그것들이 이미
add 와 multiply, 그리고 제곱의 함수에 만들어져 있기 때문입니다. 참고로 표기법이 이상하게 생긴 경우, TensorFlow 는 사실 지금 
일반적인 플러스 마이너스 등등의 부호로 과부화되어 있습니다. 이것을 다시 이쁜 형식으로 적을 수 있습니다. cost 에서는 comment를 달고 이것을 재실행해서 
똑같은 결과값을 갖습니다. 그렇게하여 w가 TensorFlow 로 변수로 결정되는 경우,
제곱, 곱하기 더하기, 빼기 등의 운영이 과부하되어 있습니다. 그러므로 여기 못생긴 것을 쓸 필요가 없습니다. 또한가지 보여주고 싶은 TensorFlow 특성이 있는데요, 여기 minimize a fix function of w.라는 예제인데요, 여러분이 최소화하고 싶은 함수중에 하나는 
바로 트레이닝 세트의 함수입니다. 트레이닝 데이터 x가 있는 경우, 신경망에서 트레이닝을 시키는 경우,
이 트레이닝데이터 x는 변할 수 있습니다. 그러면 이런 트레이닝 데이터를 
어떻게TensorFlow program으로 
이동시킬까요? 이것을 t로 정의할 것인데요 이것을 트르이닝데이터의 
역할을 맞는다고 생각하십시요 x와 y값을 가진 트레이닝데이터 말이죠, 
하지만 이번 예제에서는 x 예시밖에 없습니다. x를 placeholder를 이용해서 정의할 것입니다. 이것은 type float 32 그리고 이것은 [3,1] array로 만들겠습니다. 그리고 여기서는 
비용이 원래 고정 계수가 2차함수에서 3개의 항에 있었는데 
1곱하기 w 제곱 - 10 곱하기 w 더하기 25인데요 여기 숫자 1-10 과 25를 데이터로 바꿀텐데요 저는 cost를 대체할 것인데요 cost = x[0][0]*w 제곱 + x[1][0]*w + x[2][0]로 바꿀 것인데요 곱하기 1이죠 그러면 이제 x는 2차함수의 계수를 조종하는 데이터가 될 것입니다. 그리고 여기 placeholder 함수는 
TensorFlow에게 x는 나중에 제공하는 값이라는 것을 알려줍니다. 그러면 이제 또 다른 array를 정의해보겠습니다.
coefficient = np.array입니다. [1.], [-10.] 이구요, 맞죠 loss 값은 [25.]였습니다. 그러면 이것이 우리가 x에 대입하는 
데이터인데요 그러면 마지막으로 우리는 여기 
array coefficients를 변수 x에 넣는 방법을 찾아야하는데요, 프로그래밍 언어로는 이것은 트레이닝 단계를
하는 것과 똑같습니다. x의 값이 제공되어야 할텐데요, 여기서 feed_dict = x: coefficients로 지정할 것입니다. 그리고 이것을 변경할 것인데요, 
복사해서 붙힐 것인데요 syntax오류가 없었길 바라겠습니다. 자 그럼 다시 실행해보겠습니다. 
이전과 똑같은 값이 나와야 할텐데요. 그러면 이제 여러분이 2차함수의 계수를 변경하고 싶으면, 예를 들어 여기 [-10.]을 [-20]으로 바꾼다 해보겠습니다. 그리고 이것을 100으로 바꾸겠습니다. 그러면 이제 이 함수는 x -10 의 제곱인데요 이것을 이제 재실행하면, x-10 제곱의 값을 최소화하는 값은 w=10이길 
바랄 것입니다. 한번 볼까여. 아 좋습니다. w의 값이 10과 아주 가까운 값으로 나왔는데요. 
1000개의 gradient descent iteration이후에 말이죠. 문제 연습을 많이 하면 볼 수 있는 것이, placeholder는 TensorFlow 의 변수인데요, 
그 값은 나중에 배정됩니다. 그리고 이것은 트레이닝 데이터를 비용함수로 이동시키는 
편안한 방법입니다. 그렇게 하는 방법은 이 syntax를 이용해서 가능한데요, 트레이닝 iteration을 실행하는 경우 feed_dict 를 이용하여 x를 여기 계수와 
동일하도록 지정해줍니다. 그리고 여러분이 한번의 ieteration마다,
다른 미니 배치를 입력해야하는 경우와 같은 미니 배치 기울기 겅하를 이용하는 경우 다른 iteration에서는 feed_dict 를 이용해서 
다른 일부의 트레이닝세트를 삽입합니다. 비용함수가 데이터를 볼것이라고 생각하는 곳에 
다른 미니배치를 삽입해주는 것입니다. 이것이 TensorFlow 가 하는 것에 대한 내용을 
제공해줬길 바랍니다. 이것이 굉장히 강력한 이유는 어떻게 비용함수를 산출할건지만 
명시하면 되기 때문입니다. 그 다음에, derivative의 값을 갖고, gradient optimizer 나 add-on optimizer 또는 한줄 또는 두줄의 코드로 이루어진 
또 다른 optimizer 를 적용합니다. 여기 또 다시 코드인데요, 이것을 조금 더 깨끗히 만들었습니다. 그리고 이런 함수들이나 변수들이 이해하기 힘든 경우에
여러분이 연습문제를 몇번 진행했을 경우 더 익숙해질 것입니다. 마지막으로 말하고 싶은 것은, 여기 코드라인은
꽤 자연스러운 관용으로 쓰이는데요, 어떤 프로그래머들은 이것과 같은
대안의 유형을 쓸 것입니다. 결론적으론 똑같습니다. 세션을 시작하기 위해서 tf.Session()로 합니다. 그리고 이 세션을 이용해 init, 을 실행합니다. 그 다음에 세션을 이용해 평가합니다. 
say, w 그 다음에 결과를 print합니다. 여기 with construction 은 몇개의 TensorFlow programs에서
쓰이기도 하는데요, 여기 왼쪽에 있는 것과 똑같은 뜻인데요, 파이썬의 with 명령이 조금더 error in exception의 경우 정리를 더 잘해줍니다.
inner loop를 실행할때 말이죠. 이것은 연습문제에서 볼 수 있을 것입니다. 그러면 여기 이코드는 무엇을 하는 것일까요? 여기 공식에 중점을 두겠습니다. TensorFlow program 의 핵심은 
cost로 어떤 것을 산출한 다음에, TensorFlow 가 자동적으로 이런 비용을 최소화 시킬 수 있는
derivative를 알려줍니다. 그러면 여기 이 공식이 또는
여기 코드라인이 하는 것은 TensorFlow 가 computation draft를 
만들 수 있도록 해줍니다. computation draft는 이런 것을 합니다.
x[0][0] 값을 갖고 w를 갖고 그 값이 제곱이 됩니다. 그런 다음 x[0][0] 은 w 제곱과 곱해집니다. 그러면 x[0][0]*w squared, 값을 갖게 되겠죠 맞죠? 마지막으로 이것이 만들어져 여기 xw을 산출하게 해줍니다. x[0][0]*w squared + x[1][0]*w + 등등 과 같이 말이죠. 그럼 결과적으로 비용함수를 갖게 되는데요, 마지막으로 더해질 항은 '
x [2][0] 인데요, 이 값은 비용으로 더해집니다. 비용의 포맷은 따로 적지 않겠습니다. 그리고 TensorFlow 의 장점은 
이런 computation draft를 통해 4개의 prop 어플을 도입하여 TensorFlow는 이미 있습니다. 모든 필요한 backward 함수말이죠. 기억하시겠지만 심층신경망을 트레이닝 시키는 것은 
forward 함수의 세트에서입니다. backward 함수가 아니라요. Tensor Flow와 같은 프로그래밍 프레임웍은 이미 빌트인 backward 함수가 있습니다. 그러므로 빌트인 함수를 통해 forward 함수를 
산출하면 자동적으로 backward 함수도 이용해서 후 방향전파을 도입해서 
derivative를 산출할 수 있습니다. 그렇기 때문에 후 방향전파을 도입할 필요가 없습니다. 이것이 프로그래밍 프레임웍이 효율적으로 만들어 줄 수 있는 것 중에 한가지
이유입니다. TensorFlow documentation을 보면, TensorFlow documentation은 조금 computation draft를 그렸을때와는 
다른 표기를 사용했다는 점을 알려드리고 싶습니다. 그것은 x[0][0] w를 사용합니다. 그리고 w 제곱과 같이 그 값을 쓰기보다는 TensorFlow documentation 은 operation을 적는 편입니다. 그러면 이것은, square operation이고 이 2가지가 곱셈으로 합쳐지는 것과 같이 말이죠. 마지막으로 이것은 덧셈의 operation이구요 0에 x를 더하구요
최종값을 찾기 위해 말이죠. 이번 해당 강의에서는 
여기 이 computation draft 에 대한 표기법이 조금 더 알아듣기 쉬웠을 것 같았습니다. 하지만 TensorFlow documentation을 보면, computation drafts를 보면 표기법을 보통 값으로 표현하기보다는 operation으로 표현하는 것을 볼 수 있습니다. 하지만 2개 모두 동일한 computation draft를 대표합니다. 그리고 한줄의 프로그래밍 프레임웍을 이용해서 할 수 있는 것이 많습니다. 예를 들어, 만약 여러분이 GradientDescent쓰고 싶지 않으시면, 대신에 이 코드라인을 바꿔서 add-on Optimizer 사용하고 싶으시다면, 신속히 바꿀 수 있습니다. 더 나은 최적화 알고리즘으로 말이죠. 그렇기 때문에 최신 딥러닝 프로그램 프레임웍은 이런 것들을 지원하고 이렇게 복잡한 신경망 네트워크 또한 쉽게 코드화시킬 수 있도록
해줍니다. 저는 이 내용이 여러분께 도움이 되었으면 좋겠습니다. 전형적인 TensorFlow 프로그램 구조에 대해 알 수 있었던 시간이였습니다. 이번주의 내용을 복습하자면, 하이퍼 파라미터 서치 프로세스를 시스템적으로 조직화시키고 또한, 배치 정규화에 대해 이야기하고, 이것을 이용해서 신경망 네트워크를 빨리 트레이닝 시키는 방법을 배워보았습니다. 마지막으로 딥러닝의 프로그래밍 프레임웍에대해 이야기해보았는데요. 보신 것과 같이 훌륭한 프로그래밍 프레임웍이 많습니다. 마지막 비디오는 TensorFlow에대해 집중적으로 다루어 보았습니다. 그럼 여러분이 이번주 프로그래밍 연습학습을 흥미롭게 생각하셨길 바랍니다. 이런 아이디어에 대해서 더 많이 친숙할 수 있게 해주는 내용이였습니다.