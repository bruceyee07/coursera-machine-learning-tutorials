上一节我们学习了softmax激活函数 softmax激活函数 这一节你将加深对softmax分类的理解 并学习如何训练采用了softmax层的模型 回想我们前面的示例 输出层是这样计算Z[L]的 即我们有4个类(classes) c=4则Z[L]是个(4*1)维的向量 然后计算t t是一个临时变量 它表示元素y的幂 最后 如果输出层的激活函数 g[L]是一个softmax激活函数 则输出是这样的 基本上它就是将临时变量t规范化使其和为1 得到a(L) 你应该注意到 z中最大的元素是5 a中最大概率值也依然是第一个元素 softmax的名字对应hardmax hardmax将矢量z映射到这个矢量 即 hardmax函数遍历Z中的元素 将Z中最大的元素对应的位置置1 其余位置置0 所以它实在简单粗暴(hard) 最大的元素得到输出1 其余的元素对应输出0 与之相对的是 softmax中Z到这些概率值的映射要平和些 所以 我不知道这是不是一个NB的名字 但是至少它体现了softmax背后的直观原因<br />都是来自于与hardmax的比较 另一件我没有真正展示但是有所暗示的事情是 softmax激活函数将logistic激活函数 从2分类推广到C分类 也就是说 如果C=2 那么C=2的softmax 实际上将简化成logistic回归 我不会在这个视频里给出证明 但是大致的证明思路是 当C=2时 如果你应用softmax 那么输出层a[L]将输出2个数 也许是0.842和0.158 这2个数的和总是1 因为这2个数的和必须为1 他们实际上是冗余的 也许你不需要费心计算2个值 而是只需要计算其中一个就可以了 你计算这个数的方法将会推导成 logistic回归的计算方法 即最后只需要计算一个输出 这个算不上什么证明 但是要记住的是 softmax回归是logistic回归从2分类到多分类的推广 现在让我们看看实际上训练一个 包含softmax输出层的神经网络要怎么做 所以具体地来说 先定义用于训练神经网络的损失函数 举个例子来看我们举个例子说明 这个例子中训练集的目标输出(target output) 即标签真实值是[0 1 0 1]' 也就是前一节视频提到的例子 这意味着这是一张猫的图 因为它落在类1上 现在假设你的神经网络的输出是y^ y^是一个概率矢量 其元素和为1 0.1 0.4...你可以验证他们的和为1 a[L]=y^ 由此可见这个神经网络对这个样例的预测并不好 因为它实际上是猫但是在猫这一分类只得到了20%概率 所以它在这个样例上表现并不好 那么你想要用怎样的损失函数来训练这个神经网络呢 softmax分类典型的损失计算 一般都会使用从1到4的的和的负值 实际上更一般的表示是从1到C 这里C=4 并取yj*log((y^)j) 我们通过一个具体的例子来更好地理解它 注意到这个例子中 y1=y3=y4=0 因为这些都是0 只有y2=1 所以这个和式中 所有yj=0的项都等于0 只剩下了 -y2*log((y^)2) 那么以j为索引求和 则所有的项最终都为0--除了j=2的项 因为y2=1 和式最后演算得到 -log((y^)2) 这意味着什么？ 如果你的学习算法试图将其变小 因为你会用梯度下降法来减少训练集的损失 那么唯一使其变小的方法是使这个变小 而唯一使它变小的方法是使(y^)2尽可能的大 因为这些都是概率值 所以其值<=1 但是它的确说明问题 因为这个样例确实是一张猫的图片 你当然希望它的输出概率值尽可能的大 所以更一般地 损失函数的功能是查看 训练集的真实分类值 并令其对应的 概率值尽可能的大 如果你熟悉最大似然估计统计学习方法<br />(maximum likelihood estimation statistics) 这实际上也是某种形式的最大似然估计<br />(maximum likelyhood estimation) 不过如果你不熟悉也没有关系 刚刚我们讨论过的那些直观上的理解就够用了 这只是单个训练样例的损失 整个训练集的损失要如何处理呢？ 基于某个参数集的损失<br />包括权重(weights) 偏差(bias)等等 它的定义正像你猜的那样 定义为整个训练集的损失的和 即所有训练样例的预测值与真实值所反映的损失 因此 你要做的是使用梯度下降法使损失(cost)最小化 最后要说的是一个实现上的细节 因为C=4 则y是一个4*1的矢量 y^也是一个4*1的矢量 那么如果你使用矢量化计算实现 则矩阵Y=(y(1),y(2),...y(m)) 表示为列矩阵 比如说 这里是你的第一个训练样例 则矩阵Y的第一列为[0 1 0 0]' 第二列的样例是狗的图片 第三个样例二者都不是 等等 则Y是一个4*m维矩阵 类似地 y^也是一个列矩阵 y^1到y^m 这实际上是y^1 也就是第一个训练样例的输出 则y^1为[0.3 0.2,...]' Y^也是一个4*m维的矩阵 最后让我们看看梯度下降法在 包含softmax输出层的神经网络上的实现 这个输出层计算z[L] 我们的例子中z[L]是4*1的矢量 然后应用softmax激活函数计算a[L] 即y^ 然后在此基础上计算损失 我们刚刚讨论的是如何实现前向传播 以获得输出并计算损失 那么反向传播也就是梯度下降法如何实现？ 实际上关键步骤或者说 关键方程式的初始化是这个表达式 它在最后一层对z求偏导 得到 y^-y y和y^都是维度为4*1的矢量 因为这个例子是4分类 更一般的情况是C*1维矢量 这就是dz通常的定义形式 是损失函数对z[L]的偏导 如果你特别熟悉微积分 你可以自己推导这个等式 如果你特别熟悉微积分
你可以自己试试推导这个等式 但是直接使用这个公式也可以 如果你有需要从头开始实现的话 有了这个公式 你就可以从计算dz[L]<br />开始你的反向传播过程 并遍历神经网络各层以计算你所需的各个导数 实际上本周的主要练习 我们将开始 使用一个深度学习的编程框架 而这些基本框架 通常都只需要你专注于正确地实现正向传播 只要你指定它为一个基本框架(primary framwork)<br />并制定前向传播过程 该框架将自动为你实现 反向传播过程 所以这个表达式值得你牢记 如果你需要从头开始实现softmax回归或softmax分类的话 尽管你在本周的主要练习中不会用到它 因为框架将会替你实现 这些导数计算 那么这就是softmax分类 你可以用它实现的不仅仅是二分类 还包括C分类 下一节我将向你展示一些深度学习的编程框架 它将使你的深度学习算法的实现更有效率 下一节见