현재까지, 분류 예제를 다뤘던 내용에서는 이진분류법에 대한 내용을 포함하고 있는데요,
여기서는 2가지 레이블링이 가능했습니다. 0또는 1과 같이 말이죠. 고양이인지 고양이가 아닌지 말이죠. 하지만 클래스가 여러개라고 하면 어떻게 할까요? Softmax regression이라고 하는 로지스틱 회귀의 일반화된 버전인데요, 이것은 1of C나 1of 복수의 클래스의 경우
예측할 수 있도록 가능케 해줍니다. 단지 2개를 인식하는 것과 달리 말이죠. 한번 보겠습니다. 한번 자세히 살펴볼까요. 여러분이 고양이만 인식하는 것이 아니라 고양이, 강아지, 그리고 병아리를 인식하고 싶다고 해보겠습니다. 그러면 저는 고양이를 class 1,
강아지를 class 2, 병아리를 class 3이라고 해보겠습니다. 만약 이에 해당하지 않을 경우,
class 0 이라고 하는 아무것도 해당하지 않는 클래스도 있습니다. 여기는 이미지의 예시가 있는데요
각각의 속하는 클래스도 있습니다. 이 사진은 병아리 사진이고
class 3입니다. 고양이는 class 1, 강아지는 class 2,
그리고 저것은 콜라로 보이는데요, 그러므로 아무것도 해당되지 않기 때문에,
class 0 이 될것이구요, 다음은 class 3 일 것이구요
이렇게 이어나갑니다. 그러면 저희가 사용할 표기법은
저는 여기 대문자 C를 이용해서 입력값의 카테고리화하려는 클래스 번호를 나타낼 것입니다. 이 경우에는, 4가지의 가능한
class가 있는데요, other 카테고리를 포함해서 말이죠. 이렇게 4개의 class가 있는 경우, 여러분의 class는 0에서부터
대문자 C 빼기 1일 것입니다. 다시 말해, 이것은 0이고 1,2,3,인 것입니다. 이 경우에는 신경망을 만들 것인데요
위층이 4인 경우로 말이죠. 이 경우는 C 결과값 유닛이 있는 경우 말이죠. N은 결과값 층의 유닛 개수인데요
이 값은 4가 될 것입니다. 아니면 일반적으로 이 값은 C가 될 것입니다. 그리고 저희가 원하는 것은
결과값 층이 4개의 class들의 확률을 알려주는 것입니다. 여기 첫번째 노드는 other class일 확률을 결과값으로 나타내길 바라는데요, x값에 대해서 말이죠, 그리고 이것은
고양이일 확률을 결과값으로 나타낼 것입니다. x값에 대해서 말이죠,
그리고 이것은 강아지일 확률을 결과값으로 나타내구요 x에 대해서 말이죠,
이것은 병아리일 확률을 나타낼 텐데요, 병아리는 여기서 bc로 줄여서 쓰겠습니다.
x값에 대해서 말이죠. 여기서는 결과값 레이블 ŷ이
4 x 1 차원의 벡터입니다. 그 이유는 4개의 결과값을 표츌해야하기 때문입니다.
여기 4개의 확률을 주면서 말이죠. 그리고 확률은 합해서 1이여야 하기 때문에,
여기 ŷ에 있는 숫자들도 합하면 1이 되어야 합니다. 여러분의 네트워크가 이렇게 할 수 있도록
쓰는 일반적인 모델은 Softmax layer라고 하는 것입니다.
결과값 층에서 이러한 결과값을 생성하기 위해서 말이죠. 제가 지도를 적어볼텐데요,
그러면 나중에 돌아와서 Softmax 가 무엇을 하는지
직관적인 부분을 알 수 있을 것입니다. 마지막 층의 신경망에서는 여기 층의 선형 부분을 산출할 것입니다. z 대문자 L은, 마지막 층의 z 변수인데요, 기억하시겠지만, 이 층은 대문자 L입니다. 보통은 이것을 wL 곱하기
첫번째 층의 activation 더하기 마지막 층의 bias입니다. 이제 z값을 구했으니 이제 Softmax activation 함수라고 하는 것을
적용해야 하는데요, 이 activation 함수는 조금 Softmax
레이어에서는 조금 일반적이진 않은데요, 이렇게 됩니다. 첫번째로, 우리는 일시적인 변수를 산출할 것입니다. 이 값을 t라고 부르겠습니다.
이 값은 e의 zl 승인데요 이것은 part element-wise입니다. 여기 zL은, 우리 예제에서
zL이 4 X 1 이 될 것입니다. 이것은 4차원 벡터입니다. t는 e의 zl승인데요 이것은
element wise exponentiation입니다. T는 또한 4 x 1차원 벡터입니다. 그리고 결과값 aL은 기본적으로 벡터 t이고
정규화 되어 합이 1이 될 것입니다. 그러면 aL은 e 의 zL승 나누기
J가 1에서 4까지인 합 공식입니다. 그 이유는 4개의 class가 있는데
이 class는 각각 t 첨자 i로 나타냅니다. 다시 말해, aL은 또한 4 x 1 벡터입니다. 여기 i 번째 4차원 벡터는 여기 적은대로 aL 아래첨자 i로 나타냅니다. 이것은 ti 나누기 ti의 합닙니다. 아시겠죠? 수학적인 부분이 확실하지 않으시면, 조금 후에 예제를 통해 보겠습니다.
그럼 더 확실해질 것입니다. 수학이 확실하지 않은 경우엔, 예를 통해 살펴보겠습니다. zL을 산출한다고 해보겠습니다. 그리고 zL은 4차원 벡터이구요
5,2,-1,-3이라고 해보겠습니다. 이제 할 것은 여기
element-wise exponentiation을
이용해서 여기 벡터 t를 산출할 것입니다. 그러면 t는 e의 5승일 것이고,
e의 2승, e의 -1승, e의 3승
이런식으로 될 것입니다. 그리고 이것을 계산기로 구하면,
값은 이렇게 됩니다. e의 5승은 1484,
e제곱은 약 7.4, e의 -1승은 0.4 , 그리고 e 세제곱은 20.1 그러므로 벡터t에서 벡터 aL로 가는 것은 여기 입력값을 정규화하여 합이 1이 되게 하는 것입니다. 그러면 t의 element들을 합하면 여기 값을 모두 더하면 176.3이 됩니다. 마지막으로
aL은 여기 벡터 t 나누기 176.3입니다. 예를 들어 여기 첫번째 노드는 이것은 e의 5승 나누기 176.3을 줄 것입니다. 그 값은 0.842입니다. 이 이미지에서는 이것이
z의 값이면 0이라 불릴 확률은 84.2퍼센트 입니다. 그리고 다음 노드는
e 제곱 나누기 176.3을 출력하는데요 이것은 그럼 0.042이고
4.2퍼센트의 확률입니다. 다음 것은 e의 -1승인데요 그 값은 0.042입니다. 마지막은 e의 세제곱 나누기 그것이고,
그 값은 0.114입니다. 그러면 이것은 11.4 퍼센트로
class 3일 확률을 갖습니다. 이것은 baby c class이죠 맞죠? 그러면 class 0일 확률, class 1, class2, class3
확률이 각각 있는데요, 그러면 신경망의 결과값은 aL,
이것은 또한 ŷ입니다. 이것은 4 x 1벡터입니다. 그리고 여기 4 x 1 벡터의 요소는 여기 4개의 숫자입니다. 그럼 이제 그냥 산출합니다. 여기 알고리즘은 zL 벡터 값을 갖고,
확률이 더해서 1이 됩니다. 그리고 저희가 zL 에서 aL로 매핑한 것을
요약하자면 여기 전체 산출 방법에서
exponentiation를 산출해서 일시적 변수 t를 가졌는데요,
그 다음에 normalizing을 통해 이것을 Softmax activation 함수로
요약할 수 있습니다. 그리고 aL = gl (zl) 로 표현할 수 있겠습니다. 이 activation 함수의 조금 특이한 부분은 여기 g activation함수가 4 x 1벡터를
입력값으로 갖고, 4 x 1 벡터를 결과값으로 출력한다는 것입니다. 이전에는 저희 activation 함수가
single row 입력값을 받았는데요, 예를 들어, 시그모이드 함수와 ReLu activation 함수가 실수를 입력값으로 갖고,
실수를 결과값으로 갖습니다. Softmax activation 함수의 특이한 부분은 다른 결과값들에 거쳐 정규화가 되야한다는 점인데요 그렇기 때문에 벡터의 값을 갖고,
결과값도 벡터로 가져야 한다는 것입니다. 그래서 Softmax cross layer가 나타낼 수 있는 것은 여기 입력값이 x1,x2인 경우의 예제를 보여드리겠습니다. 이것은 바로 3개 또는 4개 또는 더 많은 노드를 가지고 있는 Softmax layer로 삽입됩니다. 그리고 ŷ값을 결과값으로 가질텐데요. 이제 숨겨진 층이 없는 신경망을 보여드리겠습니다. 이것이 하는 것은 z1 = w1 곱하기 입력값 x
더하기 b입니다. 그리고 결과값 a는 a1 또는 ŷ 은 z1에 적용된 Softmax activation 함수입니다. 이렇게 숨겨진 층이 없는 신경망에서는 Softmax 함수가 대표할 수 있는 것들에 대한
내용을 알려줄텐데요, 이것은 raw 입력값 x1 과 x2를 가진 예제인데요, C=3 인 upper class 를 가진
Softmax layer가 이러한 decision boundaries를 나타낼 수 있습니다. 이러한 몇개의 선형 decision boundaries를 보실텐데요, 이런 것들은 데이터를 세가지
class로 나눌 수 있게 해줍니다. 이 그림에서와 같이 트레이닝세트를 가지고 여기 그림에서 보이듯이 그런 것이고, Softmax cross fire 를 upper label 과
트레이닝 시킵니다. 그리고 여기 색깔은 그리고 입력값을 3개중 가장 확률이 높을 것 같은것을
기준으로 색칠하는 것입니다. 그리고 입력값을 3개중 가장 확률이 높을 것 같은것을
기준으로 색칠하는 것입니다. 이것은 보시다시피 선형 decision boudary 가 있는 로지스틱 회귀분석과 비슷한데요 2개 이상의 class가 있는 것과 같이 말이죠.
class0 , 1,
class는 0,1 또는 2일 수 있습니다. 여기 Softmax cross fire가
대표하는 또 다른 decision boudary입니다. 3개의 class로 나타나는 경우 말이죠 여기 또 다른 것이 있구요
또 다른 직관적인 부분은 여기 2개의 class간의 decision boundary가
선형일 것입니다. 그렇기 때문에 여기 노란색과 다른 클래스들 사이에
선형 boundary, 보라색과 빨간색선 사이도 그렇고,
노란색과 다른 선형 boundary가 있습니다. 이런 선형 함수를 이용해서 공간을 3개의 다른 class로 나눌 수 있습니다. 그럼 더 많은 class가 있는 예시를 보겠습니다. 이것은 c=4인 예시입니다. 이것은 green class이고 Softmax가
지속적으로 이런 종류의 decision boundary를 복수의 class가 있는경우
나타내줄 수 있습니다. 여기 c=5인 예제도 있습니다. 마지막으로 c=6인 예제도 있습니다. 이것이 Softmax crossfire가 할 수 잇는 것을
보여주는데요 숨겨진 층이 없는 경우 말이죠,
당연히 더 깊은 심층신경망의 경우 어떤 수의 숨겨진 유닛 과
더 많을 수록 등등 의 경우면 더 복잡한 비선형 decision boundary를 통해
배울 수 있습니다. 다른 class들도 나눌수 있습니다. 제가 바라는 것은 이 강의가
신경망에서 Softmax layer 또는 a Softmax activation 함수가
무엇을 할 수 있는지 감을 주었길 바랍니다. 다음 비디오에서는 Softmax layer를 사용하는 신경망을 어떻게
트레이닝 시킬 수 있는지 알아보겠습니다.