1
00:00:00,990 --> 00:00:04,140
目前為止，我們討論過的分類問題都是

2
00:00:04,140 --> 00:00:08,410
二元分類：你有兩個可能的類別，0或1

3
00:00:08,410 --> 00:00:10,520
這是貓嗎？這不是貓嗎？

4
00:00:10,520 --> 00:00:13,050
如果我們有多個可能的類別，那會怎麼樣呢？

5
00:00:13,050 --> 00:00:17,140
有一種更廣義化的羅吉斯迴歸，叫做 Softmax 迴歸

6
00:00:17,140 --> 00:00:21,130
讓你能預測 C 個類別中的一個，

7
00:00:21,130 --> 00:00:26,280
或是多個類別中的一個，而非僅止於兩類

8
00:00:26,280 --> 00:00:26,915
讓我們看看讓我們看看

9
00:00:26,915 --> 00:00:31,264
假設我們不再光是辨認貓貓：
你要辨認貓、狗、

10
00:00:31,264 --> 00:00:31,984
和小雞

11
00:00:31,984 --> 00:00:38,050
那，我要稱貓是類別1，狗是類別2，小雞是類別3

12
00:00:38,050 --> 00:00:40,914
如果以上皆非，那就是其他

13
00:00:40,914 --> 00:00:44,406
或是以上皆非的類別，讓我稱之為類別0

14
00:00:44,406 --> 00:00:49,900
那麼這邊是一些圖片和其類別的例子

15
00:00:49,900 --> 00:00:52,680
這是小雞的圖片，所以是類別3

16
00:00:52,680 --> 00:00:57,395
貓是類別1，狗是類別2，我想
這是無尾熊，所以

17
00:00:57,395 --> 00:01:02,498
是以上皆非，也就是類別0。然後是類別3，以此類推。

18
00:01:02,498 --> 00:01:07,554
這邊我們所用的符號是，我要用大寫 C

19
00:01:07,554 --> 00:01:13,340
來表示你想分類類別的數量

20
00:01:13,340 --> 00:01:17,628
在這個例子，你有四個可能的類別，
包括「其他」

21
00:01:17,628 --> 00:01:19,298
或叫「以上皆非」的類別。

22
00:01:19,298 --> 00:01:23,921
所以當你有四個類別，你類別

23
00:01:23,921 --> 00:01:28,660
索引的數字會是 0 到 C-1，

24
00:01:28,660 --> 00:01:31,550
也就是說，是 0, 1, 2 或 3

25
00:01:31,550 --> 00:01:36,653
在這裡，我們會創一個神經網路，
其輸出層有 4 個

26
00:01:36,653 --> 00:01:40,870
或是說有 C 個輸出單元

27
00:01:43,140 --> 00:01:48,908
所以 n，輸出層，也就是第 L 層輸出單元的數量
會等於4

28
00:01:48,908 --> 00:01:51,807
或更廣義地說，等於 C

29
00:01:51,807 --> 00:01:56,619
而我們想要的，是讓輸出層的神經元

30
00:01:56,619 --> 00:02:00,860
告訴我們這四個類別，是其中某個類別的機率

31
00:02:00,860 --> 00:02:04,320
所以這邊第一個圓圈
應該要輸出

32
00:02:04,320 --> 00:02:08,110
或者說，想要他輸出「其他」類別的機率

33
00:02:08,110 --> 00:02:12,840
在我們輸入 x 的時候。
而這個則輸出是貓的機率

34
00:02:12,840 --> 00:02:16,980
當輸入x時。
這會輸出狗的機率

35
00:02:16,980 --> 00:02:20,170
當輸入x時。
那個則會輸出機率

36
00:02:20,170 --> 00:02:27,910
我把小雞縮寫成 "bc"，
所以是輸入x時，是 bc 的機率

37
00:02:29,160 --> 00:02:36,600
所以這邊，輸出的類別 y hat
會是 4乘1維度的向量

38
00:02:36,600 --> 00:02:41,760
因為他現在會輸出四個數字，給你這四個機率

39
00:02:42,850 --> 00:02:48,070
也因為機率加起來要等於一，
y hat 的這四個值

40
00:02:48,070 --> 00:02:48,980
加起來必須是 1

41
00:02:50,630 --> 00:02:55,390
在神經網路中，這樣的標準模型是用一個叫

42
00:02:55,390 --> 00:03:00,170
「Softmax 層」放到輸出層內，以產生出這些輸出

43
00:03:00,170 --> 00:03:02,040
讓我先寫數學式子，你可以之後回來看看

44
00:03:02,040 --> 00:03:04,680
對 Softmax 在做什麼有些概念

45
00:03:06,610 --> 00:03:08,940
那麼，在神經網路的最後一層

46
00:03:08,940 --> 00:03:13,360
和平常一樣，你計算這一層線性的部份

47
00:03:13,360 --> 00:03:17,800
所以 z[L]，這是最後一層的 z 變數

48
00:03:17,800 --> 00:03:21,980
還記得這是第 L 層

49
00:03:21,980 --> 00:03:26,619
所以照例地你算出 W[L] 乘前一層的

50
00:03:26,619 --> 00:03:32,170
啟動值，再加上最後一層的偏移項。

51
00:03:32,170 --> 00:03:33,180
在算完 z 之後，

52
00:03:33,180 --> 00:03:37,690
你要套用所謂的「Softmax 啟動函數」。

53
00:03:38,880 --> 00:03:43,340
在Softmax層，那個啟動函數和一般的不大一樣

54
00:03:43,340 --> 00:03:44,150
他是這樣做的：

55
00:03:45,380 --> 00:03:50,081
首先，我們計算出一個暫時的變數

56
00:03:50,081 --> 00:03:54,180
我們稱之 t，是 e 的 z[L] 次方

57
00:03:54,180 --> 00:03:56,119
這是套用在逐個元素上的

58
00:03:56,119 --> 00:04:00,824
在我們的例子，z[L] 是 4乘1

59
00:04:00,824 --> 00:04:03,470
是個四維的向量

60
00:04:03,470 --> 00:04:08,720
所以 t 本身，e 的 z[L] 次方，這是逐元素的次方，

61
00:04:08,720 --> 00:04:13,100
t 也會是一個 4乘1 的向量

62
00:04:13,100 --> 00:04:14,825
然後輸出的 a[L]

63
00:04:14,825 --> 00:04:20,415
會是向量 t，但是標準化至總和為 1

64
00:04:20,415 --> 00:04:28,673
所以 a[L] 是 e 的 z[L] 次方，除以 j=1到4 的 t_i 總和

65
00:04:28,673 --> 00:04:33,994
因為我們有四個類別。

66
00:04:33,994 --> 00:04:40,082
所以這也表示，a[L] 也是一個 4乘1 的向量

67
00:04:40,082 --> 00:04:44,780
而且這個四維向量的第 i 個元素

68
00:04:44,780 --> 00:04:50,885
寫做 a[L] 下標 i，這會

69
00:04:50,885 --> 00:04:56,660
等於 t_i 除以 t_i 的總和

70
00:04:56,660 --> 00:04:58,690
萬一這數學公式不夠清楚

71
00:04:58,690 --> 00:05:02,320
我們等等就拿個例子讓這件事更清楚

72
00:05:02,320 --> 00:05:03,835
萬一這數學公式不夠清楚

73
00:05:03,835 --> 00:05:06,775
讓我們跑過實際的例子來釐清之

74
00:05:06,775 --> 00:05:10,905
假設你算出了 z[L]，

75
00:05:10,905 --> 00:05:18,277
z[L] 是個四維向量，假設是 5, 2, -1, 3

76
00:05:18,277 --> 00:05:22,256
我們要做的是用逐元素的指數

77
00:05:22,256 --> 00:05:23,665
來算出向量 t

78
00:05:23,665 --> 00:05:29,465
所以 t 會是 e^5, e^2, e^-1 e^3

79
00:05:29,465 --> 00:05:32,529
如果你按按計算機，這會是你得到的

80
00:05:32,529 --> 00:05:38,750
e^5 是 148.4，e^2 大概是 7.4

81
00:05:38,750 --> 00:05:44,697
e^-1 是 0.4，而 e^3 是 20.1

82
00:05:44,697 --> 00:05:49,519
那我們從向量 t 轉到向量 a[L] 的方式是

83
00:05:49,519 --> 00:05:52,910
把這些元素標準化，使得總和為 1

84
00:05:52,910 --> 00:05:56,808
所以如果你把 t 的元素加起來

85
00:05:56,808 --> 00:06:03,232
把這四個數字加起來，你會得到 176.3

86
00:06:03,232 --> 00:06:09,565
所以最後，a[L] 會是這個向量 t

87
00:06:09,565 --> 00:06:14,515
作為一個向量，除以 176.3

88
00:06:14,515 --> 00:06:18,580
所以舉例，這邊第一個點

89
00:06:18,580 --> 00:06:23,885
他會輸出e的5次方除以 176.3

90
00:06:23,885 --> 00:06:27,777
也就是 0.842

91
00:06:27,777 --> 00:06:32,675
所以是，對於這個圖片，如果你得到的 z 值長這樣

92
00:06:32,675 --> 00:06:36,434
他是類別0的機率是 84.2%

93
00:06:36,434 --> 00:06:42,192
然後下一個點輸出e平方除以176.3

94
00:06:42,192 --> 00:06:48,200
這會是 0.042，所以會是 4.2% 的機率

95
00:06:48,200 --> 00:06:53,449
下一個是 e^-1 除以這個

96
00:06:53,449 --> 00:06:56,891
等於 0.002

97
00:06:56,891 --> 00:07:04,235
最後一個是 e 的三次方除以這個，等於 0.114

98
00:07:04,235 --> 00:07:08,312
所以有 11.4% 的機率這會是類別3

99
00:07:08,312 --> 00:07:10,683
也就是小雞，對吧

100
00:07:10,683 --> 00:07:15,568
所以這就是他是類別0，類別1，類別2，類別3的機率

101
00:07:15,568 --> 00:07:21,930
所以這神經網路的輸出 a[L]，也就是 y hat

102
00:07:21,930 --> 00:07:25,170
這是 4乘1 的向量，其中

103
00:07:25,170 --> 00:07:29,800
四個元素會是這邊的四個數字

104
00:07:29,800 --> 00:07:31,060
就是剛剛算的

105
00:07:31,060 --> 00:07:38,077
所以這個演算法拿 z[L] 向量
轉成四個相加為一的機率

106
00:07:38,077 --> 00:07:43,013
把剛剛把z[L]轉成a[L]的數學式整理一下

107
00:07:43,013 --> 00:07:47,741
從算指數開始、得到暫時的變數 t、

108
00:07:47,741 --> 00:07:52,469
然後標準化，這全部的計算

109
00:07:52,469 --> 00:07:57,827
我們可以把整個流程
總結為 Softmax 啟動函數

110
00:07:57,827 --> 00:08:03,625
就是 a[L] 等於啟動函數 g 套用在向量 z[L] 上

111
00:08:03,625 --> 00:08:08,379
這個啟動函數特別的地方在於

112
00:08:08,379 --> 00:08:12,654
這啟動函數 g，他吃一個 4乘1 的向量

113
00:08:12,654 --> 00:08:15,060
然後輸出一個4乘1的向量

114
00:08:15,060 --> 00:08:19,280
我們之前看到的啟動函數是拿單一一個數字作為輸入

115
00:08:19,280 --> 00:08:20,875
例如S型函數和

116
00:08:20,875 --> 00:08:24,840
ReLU啟動函數，輸入一個實數然後輸出一個實數

117
00:08:24,840 --> 00:08:28,255
Softmax啟動函數特別的是

118
00:08:28,255 --> 00:08:32,262
因為他要把不同的輸出整個做標準化

119
00:08:32,262 --> 00:08:35,365
他必須拿一個向量，然後輸出一個向量

120
00:08:35,365 --> 00:08:39,219
那麼，Softmax 分類器能描繪的

121
00:08:39,219 --> 00:08:43,383
我來舉一個例子：你有輸入 x1, x2

122
00:08:43,383 --> 00:08:48,435
然後這些直接餵進一個 Softmax 層，有三個、

123
00:08:48,435 --> 00:08:53,500
四個、或多個輸出的節點，來輸出 y hat

124
00:08:53,500 --> 00:08:58,801
所以我給你看的是一個沒有隱藏層的神經網路

125
00:08:58,801 --> 00:09:04,777
他唯一做的是計算 z[1] 等於 W[1] 乘以輸入 x，加上 b

126
00:09:04,777 --> 00:09:07,359
然後輸出的 a[1]，

127
00:09:07,359 --> 00:09:13,210
或說是 y hat，只是 Softmax 啟動函數套用在 z[1] 上

128
00:09:13,210 --> 00:09:15,615
所以這個沒有隱藏層的神經網路

129
00:09:15,615 --> 00:09:20,260
應該能給你一些感覺
Softmax函數能表示出什麼樣的東西

130
00:09:20,260 --> 00:09:23,677
這邊是一個例子，輸入只有 x1, x2，

131
00:09:23,677 --> 00:09:28,662
一個 C=3 個輸出類別的 Softmax 層

132
00:09:28,662 --> 00:09:31,661
可以表現出這樣的決策邊界
(decision boundaries)

133
00:09:31,661 --> 00:09:35,289
注意這可看作有多個線性的決策邊界

134
00:09:35,289 --> 00:09:39,223
然而這已能把資料分成三個類別

135
00:09:39,223 --> 00:09:44,126
關於這張圖，實際上我們所做的是把

136
00:09:44,126 --> 00:09:47,335
圖裡面顯示的訓練資料

137
00:09:47,335 --> 00:09:52,079
訓練出有三個類別的 Softmax 分類器

138
00:09:52,079 --> 00:09:54,750
而這圖的顏色顯示了

139
00:09:54,750 --> 00:09:59,330
Softmax 分類器輸出的臨界點，
著色是根據

140
00:09:59,330 --> 00:10:03,790
對於某個輸入，其輸出的三個機率哪一個最高來著色

141
00:10:03,790 --> 00:10:07,917
所以某種角度，我們可以把這個看成
廣義的羅吉斯迴歸

142
00:10:07,917 --> 00:10:11,182
其中有多個線性的決策邊界，

143
00:10:11,182 --> 00:10:16,065
而類別不再是兩個 0, 1，
而是比兩個類別還多：0, 1 或 2。

144
00:10:16,065 --> 00:10:20,238
這邊是另一個 Softmax 分類器決策邊界的例子

145
00:10:20,238 --> 00:10:23,625
訓練在有三個類別的資料上

146
00:10:23,625 --> 00:10:28,731
這邊又是另一個例子。這邊有個觀念是

147
00:10:28,731 --> 00:10:34,211
任兩個類別中間的決策邊界會是直線

148
00:10:34,211 --> 00:10:38,325
這就是為什麼你在這邊看到，

149
00:10:38,325 --> 00:10:42,312
在黃色和紅色類別中間的邊界是直的

150
00:10:42,312 --> 00:10:46,949
紫色和紅色中間，直線的邊界，
紫色和黃色，另一個線性的決策邊界

151
00:10:46,949 --> 00:10:49,729
他能利用各種不同的線性函數

152
00:10:49,729 --> 00:10:52,660
來把這個空間分成三個類別

153
00:10:52,660 --> 00:10:55,460
讓我們來看看有更多類別的例子

154
00:10:55,460 --> 00:10:58,199
那麼這是C=4的例子

155
00:10:58,199 --> 00:11:03,096
所以這綠色的類別和 Softmax 依然表現出

156
00:11:03,096 --> 00:11:07,280
這樣子多類別之間的線性決策邊界

157
00:11:07,280 --> 00:11:11,796
這邊是更多的例子，C=5

158
00:11:11,796 --> 00:11:15,190
這最後的例子是 C=6

159
00:11:15,190 --> 00:11:20,184
所以，這邊示範了當 Softmax 分類器沒有隱藏層時，

160
00:11:20,184 --> 00:11:24,545
能夠做到的事情。當然以更深的神經網路，有x

161
00:11:24,545 --> 00:11:28,860
然後一些隱藏神經元，然後更多的隱藏神經元，等等等

162
00:11:28,860 --> 00:11:32,850
這樣你就能學到更複雜的，
非線性的決策邊界來分開

163
00:11:32,850 --> 00:11:34,065
各種不同的類別。

164
00:11:35,240 --> 00:11:37,990
那麼，我希望這能給你一些概念，
在神經網路裡面

165
00:11:37,990 --> 00:11:41,820
Softmax 層或者是 Softmax 啟動函數能做到什麼

166
00:11:41,820 --> 00:11:42,650
在下一個影片中

167
00:11:42,650 --> 00:11:46,940
讓我們瞧瞧你怎麼去訓練一個
有Softmax層的神經網路