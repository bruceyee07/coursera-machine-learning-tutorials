在上部影片，你學到了 Softmax 層 和 Softmax 啟動函數 在這部影片，你會更深入了解 Softmax 分類 也會學到如何訓練有 Softmax 層的模型 回憶一下之前的例子，輸出層算出了這些 z[L] 所以我們有四個類別 C=4，然後 z[L] 是4乘1的向量。
還有我們提到計算 t， 他是一個暫時的變數，做的是逐元素的指數。 最後呢，如果你輸出層的啟動函數 g[L] 是個 Softmax 啟動函數，那你的輸出就會是這個 基本上是拿那個暫時變數 t，標準化至總和為 1 所以這就變成 a[L]。 那你注意到向量 z 裡面，最大的元素是 5 而這邊最高的機率，就是這第一個機率。 "Softmax" 之所以叫這個名字，
是相對於所謂的 "hard max"， 也就是把向量 z 變成這一種向量。 所以 hard max 函數會看過 z 的所有元素
然後放一個 1 在 z 最大元素的那個位置；而其他地方放 0 所以這是個很硬派的max，其中最大的元素得到輸出1 然後其他的都拿到輸出0 相較之下， soft max 比較溫柔，把 z 對應到這邊的機率。 所以，我不確定這命名好不好，至少直覺上 這是為什麼我們叫他 softmax，是對比於 hard max。 還有另一件事，雖然我沒示範，但簡單提過，
就是 Softmax 迴歸 或說 Softmax 啟動函數將羅吉斯啟動函數
更廣義化 至 C 個類別，而不僅只於兩個類別， 而實際上如果 C=2，那 softmax 在C=2時，會簡化成羅吉斯迴歸分析。 在這部影片裡我不會去證明，不過大意是 如果 C=2，然後你套用了 softmax， 那輸出層 a[L]，會輸出兩個數字，因為 C=2， 所以例如他輸出 0.842 和 0.158，對吧 這兩個數字加起來一定為 1 而因為這兩個數字加起來都是 1，
實際上算重複了 所以你不用費心把兩個都算出來 也許你只要算出其中一個就好 最終，你算出那一個數字的方式，可以簡化成 羅吉斯迴歸計算出那單一輸出的方法。 這不算是真的證明，不過重點是 softmax 迴歸是羅吉斯迴歸
擴充到比兩個類別還多的情況。 那麼，讓我們看看實際上要怎麼訓練含有 softmax 輸出層的神經網路 特別是 我們來定義訓練這個神經網路所用的損失函數 我們來舉個例子 舉例來說，你的訓練資料裡面可能有一筆，他的輸出目標 他真正的標籤是 0, 1, 0, 0 如果用前一部影片的例子 這表示這是張貓貓的圖，因為他是類別1 那假設你的神經網路現在輸出的 y hat y hat 會是加總為1，代表機率的向量 ...0.1, 0.4，你檢查一下就知道這些加起來是1。這個是 a[L] 所以，對於這筆資料，這網路表現不是很好，因為 這張圖實際上是一隻貓，但只被認為有 20% 機率是貓 所以在這例子表現不好 那麼，在訓練這個神經網路，你會想用哪種損失函數呢？ 在 softmax 分類問題 通常在用的損失函數是，負的 j=1~4 的總和 —其實一般情況下，這總和是從 1 到 C 我們在這裡就用4— y_j log y_j hat 所以我們來看我們上面這筆例子，來更了解這在做什麼 注意到在這個例子 y1 = y3 = y4 = 0，因為這些都是 0，只有 y2 是 1 所以你看這個總和 所有那些 y_j 是 0 的項都會是 0 你剩下唯一的一項是 -y_2 log y_2 hat 因為對於所有索引 j 相加 除了 j 是 2 的時候，其他全部的數字都會是0， 也因為 y_2 = 1，所以這項就只是 -log y_2 hat。 所以這個意思是， 如果你的學習演算法想讓這個是小的 因為你用梯度下降法，試著減少你訓練集的損失 要讓這個變小唯一的方式，是讓這個變小 而要如此唯一的方法是，讓 y_2 hat 盡可能地大 而這些是機率，所以不能比 1 還大 這某種角度還滿合理的，因為在這例子 x 是貓的圖片 而你想讓那一個輸出的機率越大越好 所以更一般來說，這個損失函數所做的是看你的訓練資料 裡面真實的類別是什麼，他試著讓那個 類別相對應的機率盡可能地高。 如果你熟最大近似估計 (maximum likelihood estimation) 這其實就是一種最大近似估計 不過如果你不知道的話也不用擔心 知道我們剛剛講的概念就夠了。 那麼，這是一筆訓練資料的損失值， 如果是全部的訓練資料的成本 J 呢？ 所以，這一組參數的成本，這邊全部的權重 和偏移項，大概就是照你想的那樣來定義： 全部訓練資料的損失值的總和 你的學習演算法的預測，這邊，加過所有的訓練資料 那麼 你要做的是利用梯度下降法，來讓這個成本最小化。 最後呢，另一個實作上的細節 注意到因為 C=4，y 是 4乘1 的向量， 而 y hat 也是一個 4乘1 的向量 所以如果你用向量化的實作，這個矩陣大寫 Y 會是 y(1), y(2), 到 y(m)，水平的串起來。 舉例來說，如果上面這例子是第一筆訓練資料 那這矩陣Y的第一欄會是0, 1, 0, 0，
然後你的第二筆資料 也許是隻狗，然後第三個可能是以上皆非，依此類推 那這個矩陣Y最後會是 4乘m 的矩陣 同樣道理，Y hat 會是 y hat(1)，水平的過去 一直到 y hat(m)。所以這一個是 y hat(1) 第一筆訓練資料的輸出，這樣的話 Y hat 就會是 0.3, 0.2, 0.1, 0.4，等等等 而 Y hat 本身也會是個 4乘m 的矩陣。 最後呢，讓我們瞧瞧你要怎麼實作梯度下降法 當你有個 softmax 輸出層的時候 所以這個輸出層會計算 C乘1 的 z[L]，
在我們的例子是 4乘1 然後你套用 softmax 啟動函數以得到 a[L]，或者說 y hat， 然後這樣就可以算出損失值 那麼，我們已經講過如何實作正向傳播 以得到這些輸出，來算出損失值 那麼反向傳播呢？或者說梯度下降呢？ 其實最關鍵的一步， 或說最關鍵的公式，讓你能啟動反向傳播，是這個式子 在損失層對於z[L]的導數，這個其實是 你可以用 y hat, 4乘1的向量，減掉 y，4乘1的向量 所以注意到這些都是4乘1的向量， 因為你有四個類別；一般的情況下則是 C乘1 這邊的 dz 根據我們慣例 這是我們的成本函數對z[L]的偏導數 如果你是微積分專家，你可以自己導一遍 如果你是微積分專家，你可以自己導一遍，不過 直接套用這個公式也可以 如果你需要從無到有實作的話。 不過藉由這個，你就可以計算 dz[L] 然後開始反向傳播的程序 來算出你的神經網路裡所有你需要的導數 不過呢，在這個禮拜的程式作業，我們將會利用眾多 深度學習程式框架的其中一個，
而在程式框架 通常你只要專注於把正向傳播弄正確 而只要你在程式框架中指定好正向傳播， 程式框架會弄清楚要怎麼執行反向傳播， 來幫你做反向的傳播。 所以這個公式值得一記—如果你需要從無到有 實作 softmax 迴歸，或是 softmax 分類— 雖然實際上在這個禮拜的程式作業，你並不會用到他， 因為你所使用的程式框架會幫你處理好 這個導數的計算。 那麼，這就是 softmax 分類了。 藉由他，你現在可以實作學習演算法
來把輸入資料歸類 成不只兩個類別中的一個，而是眾多C個類別中的一個。 接下來呢，我想向你展示一些深度學習的程式框架， 他能讓你在實作深度學習演算法時更有效率。 讓我們繼續看下去