1
00:00:00,570 --> 00:00:02,937
在上部影片，你學到了 Softmax 層

2
00:00:02,937 --> 00:00:04,900
和 Softmax 啟動函數

3
00:00:04,900 --> 00:00:08,760
在這部影片，你會更深入了解 Softmax 分類

4
00:00:08,760 --> 00:00:13,230
也會學到如何訓練有 Softmax 層的模型

5
00:00:13,230 --> 00:00:18,434
回憶一下之前的例子，輸出層算出了這些 z[L]

6
00:00:18,434 --> 00:00:19,915
所以我們有四個類別

7
00:00:19,915 --> 00:00:24,573
C=4，然後 z[L] 是4乘1的向量。
還有我們提到計算 t，

8
00:00:24,573 --> 00:00:30,160
他是一個暫時的變數，做的是逐元素的指數。

9
00:00:30,160 --> 00:00:34,778
最後呢，如果你輸出層的啟動函數

10
00:00:34,778 --> 00:00:40,320
g[L] 是個 Softmax 啟動函數，那你的輸出就會是這個

11
00:00:40,320 --> 00:00:45,570
基本上是拿那個暫時變數 t，標準化至總和為 1

12
00:00:45,570 --> 00:00:49,328
所以這就變成 a[L]。

13
00:00:49,328 --> 00:00:53,304
那你注意到向量 z 裡面，最大的元素是 5

14
00:00:53,304 --> 00:00:57,650
而這邊最高的機率，就是這第一個機率。

15
00:00:57,650 --> 00:01:02,653
"Softmax" 之所以叫這個名字，
是相對於所謂的 "hard max"，

16
00:01:02,653 --> 00:01:07,848
也就是把向量 z 變成這一種向量。

17
00:01:07,848 --> 00:01:12,869
所以 hard max 函數會看過 z 的所有元素
然後放一個 1

18
00:01:12,869 --> 00:01:18,182
在 z 最大元素的那個位置；而其他地方放 0

19
00:01:18,182 --> 00:01:23,259
所以這是個很硬派的max，其中最大的元素得到輸出1

20
00:01:23,259 --> 00:01:25,850
然後其他的都拿到輸出0

21
00:01:25,850 --> 00:01:27,626
相較之下，

22
00:01:27,626 --> 00:01:33,500
soft max 比較溫柔，把 z 對應到這邊的機率。

23
00:01:33,500 --> 00:01:37,980
所以，我不確定這命名好不好，至少直覺上

24
00:01:37,980 --> 00:01:42,040
這是為什麼我們叫他 softmax，是對比於 hard max。

25
00:01:43,060 --> 00:01:47,930
還有另一件事，雖然我沒示範，但簡單提過，
就是 Softmax 迴歸

26
00:01:47,930 --> 00:01:52,450
或說 Softmax 啟動函數將羅吉斯啟動函數
更廣義化

27
00:01:52,450 --> 00:01:56,330
至 C 個類別，而不僅只於兩個類別，

28
00:01:56,330 --> 00:02:01,586
而實際上如果 C=2，那 softmax

29
00:02:01,586 --> 00:02:07,940
在C=2時，會簡化成羅吉斯迴歸分析。

30
00:02:07,940 --> 00:02:13,680
在這部影片裡我不會去證明，不過大意是

31
00:02:13,680 --> 00:02:18,087
如果 C=2，然後你套用了 softmax，

32
00:02:18,087 --> 00:02:23,929
那輸出層 a[L]，會輸出兩個數字，因為 C=2，

33
00:02:23,929 --> 00:02:28,872
所以例如他輸出 0.842 和 0.158，對吧

34
00:02:28,872 --> 00:02:31,056
這兩個數字加起來一定為 1

35
00:02:31,056 --> 00:02:34,221
而因為這兩個數字加起來都是 1，
實際上算重複了

36
00:02:34,221 --> 00:02:37,077
所以你不用費心把兩個都算出來

37
00:02:37,077 --> 00:02:39,193
也許你只要算出其中一個就好

38
00:02:39,193 --> 00:02:43,999
最終，你算出那一個數字的方式，可以簡化成

39
00:02:43,999 --> 00:02:48,895
羅吉斯迴歸計算出那單一輸出的方法。

40
00:02:48,895 --> 00:02:53,835
這不算是真的證明，不過重點是

41
00:02:53,835 --> 00:02:58,468
softmax 迴歸是羅吉斯迴歸
擴充到比兩個類別還多的情況。

42
00:02:58,468 --> 00:03:02,187
那麼，讓我們看看實際上要怎麼訓練含有

43
00:03:02,187 --> 00:03:04,157
softmax 輸出層的神經網路

44
00:03:04,157 --> 00:03:04,966
特別是

45
00:03:04,966 --> 00:03:08,437
我們來定義訓練這個神經網路所用的損失函數

46
00:03:08,437 --> 00:03:09,427
我們來舉個例子

47
00:03:09,427 --> 00:03:15,018
舉例來說，你的訓練資料裡面可能有一筆，他的輸出目標

48
00:03:15,018 --> 00:03:17,881
他真正的標籤是 0, 1, 0, 0

49
00:03:17,881 --> 00:03:20,661
如果用前一部影片的例子

50
00:03:20,661 --> 00:03:25,500
這表示這是張貓貓的圖，因為他是類別1

51
00:03:25,500 --> 00:03:31,096
那假設你的神經網路現在輸出的 y hat

52
00:03:31,096 --> 00:03:35,083
y hat 會是加總為1，代表機率的向量

53
00:03:35,083 --> 00:03:42,870
...0.1, 0.4，你檢查一下就知道這些加起來是1。這個是 a[L]

54
00:03:42,870 --> 00:03:46,320
所以，對於這筆資料，這網路表現不是很好，因為

55
00:03:46,320 --> 00:03:49,670
這張圖實際上是一隻貓，但只被認為有 20% 機率是貓

56
00:03:49,670 --> 00:03:51,180
所以在這例子表現不好

57
00:03:52,290 --> 00:03:56,700
那麼，在訓練這個神經網路，你會想用哪種損失函數呢？

58
00:03:56,700 --> 00:03:58,762
在 softmax 分類問題

59
00:03:58,762 --> 00:04:03,310
通常在用的損失函數是，負的 j=1~4 的總和

60
00:04:03,310 --> 00:04:07,269
—其實一般情況下，這總和是從 1 到 C

61
00:04:07,269 --> 00:04:14,620
我們在這裡就用4— y_j log y_j hat

62
00:04:14,620 --> 00:04:20,040
所以我們來看我們上面這筆例子，來更了解這在做什麼

63
00:04:20,040 --> 00:04:24,209
注意到在這個例子

64
00:04:24,209 --> 00:04:32,730
y1 = y3 = y4 = 0，因為這些都是 0，只有 y2 是 1

65
00:04:32,730 --> 00:04:35,447
所以你看這個總和

66
00:04:35,447 --> 00:04:39,726
所有那些 y_j 是 0 的項都會是 0

67
00:04:39,726 --> 00:04:44,412
你剩下唯一的一項是 -y_2 log y_2 hat

68
00:04:44,412 --> 00:04:47,802
因為對於所有索引 j 相加

69
00:04:47,802 --> 00:04:52,622
除了 j 是 2 的時候，其他全部的數字都會是0，

70
00:04:52,622 --> 00:04:58,440
也因為 y_2 = 1，所以這項就只是 -log y_2 hat。

71
00:04:58,440 --> 00:05:00,190
所以這個意思是，

72
00:05:00,190 --> 00:05:04,510
如果你的學習演算法想讓這個是小的

73
00:05:04,510 --> 00:05:09,040
因為你用梯度下降法，試著減少你訓練集的損失

74
00:05:09,040 --> 00:05:12,550
要讓這個變小唯一的方式，是讓這個變小

75
00:05:12,550 --> 00:05:17,390
而要如此唯一的方法是，讓 y_2 hat 盡可能地大

76
00:05:18,430 --> 00:05:20,846
而這些是機率，所以不能比 1 還大

77
00:05:20,846 --> 00:05:26,286
這某種角度還滿合理的，因為在這例子 x 是貓的圖片

78
00:05:26,286 --> 00:05:31,170
而你想讓那一個輸出的機率越大越好

79
00:05:31,170 --> 00:05:35,590
所以更一般來說，這個損失函數所做的是看你的訓練資料

80
00:05:35,590 --> 00:05:39,640
裡面真實的類別是什麼，他試著讓那個

81
00:05:39,640 --> 00:05:42,640
類別相對應的機率盡可能地高。

82
00:05:42,640 --> 00:05:46,126
如果你熟最大近似估計 (maximum likelihood estimation)

83
00:05:46,126 --> 00:05:49,153
這其實就是一種最大近似估計

84
00:05:49,153 --> 00:05:51,790
不過如果你不知道的話也不用擔心

85
00:05:51,790 --> 00:05:53,770
知道我們剛剛講的概念就夠了。

86
00:05:54,970 --> 00:05:57,460
那麼，這是一筆訓練資料的損失值，

87
00:05:57,460 --> 00:06:00,857
如果是全部的訓練資料的成本 J 呢？

88
00:06:00,857 --> 00:06:05,717
所以，這一組參數的成本，這邊全部的權重

89
00:06:05,717 --> 00:06:09,767
和偏移項，大概就是照你想的那樣來定義：

90
00:06:09,767 --> 00:06:12,926
全部訓練資料的損失值的總和

91
00:06:12,926 --> 00:06:18,374
你的學習演算法的預測，這邊，加過所有的訓練資料

92
00:06:18,374 --> 00:06:18,924
那麼

93
00:06:18,924 --> 00:06:23,830
你要做的是利用梯度下降法，來讓這個成本最小化。

94
00:06:23,830 --> 00:06:26,370
最後呢，另一個實作上的細節

95
00:06:26,370 --> 00:06:30,949
注意到因為 C=4，y 是 4乘1 的向量，

96
00:06:30,949 --> 00:06:33,160
而 y hat 也是一個 4乘1 的向量

97
00:06:34,450 --> 00:06:39,565
所以如果你用向量化的實作，這個矩陣大寫 Y

98
00:06:39,565 --> 00:06:45,711
會是 y(1), y(2), 到 y(m)，水平的串起來。

99
00:06:45,711 --> 00:06:50,903
舉例來說，如果上面這例子是第一筆訓練資料

100
00:06:50,903 --> 00:06:56,428
那這矩陣Y的第一欄會是0, 1, 0, 0，
然後你的第二筆資料

101
00:06:56,428 --> 00:07:01,730
也許是隻狗，然後第三個可能是以上皆非，依此類推

102
00:07:01,730 --> 00:07:08,580
那這個矩陣Y最後會是 4乘m 的矩陣

103
00:07:08,580 --> 00:07:13,838
同樣道理，Y hat 會是 y hat(1)，水平的過去

104
00:07:13,838 --> 00:07:18,284
一直到 y hat(m)。所以這一個是 y hat(1)

105
00:07:19,590 --> 00:07:25,403
第一筆訓練資料的輸出，這樣的話 Y hat 就會是 0.3,

106
00:07:25,403 --> 00:07:29,120
0.2, 0.1, 0.4，等等等

107
00:07:29,120 --> 00:07:33,290
而 Y hat 本身也會是個 4乘m 的矩陣。

108
00:07:33,290 --> 00:07:37,382
最後呢，讓我們瞧瞧你要怎麼實作梯度下降法

109
00:07:37,382 --> 00:07:38,942
當你有個 softmax 輸出層的時候

110
00:07:38,942 --> 00:07:46,161
所以這個輸出層會計算 C乘1 的 z[L]，
在我們的例子是 4乘1

111
00:07:46,161 --> 00:07:52,670
然後你套用 softmax 啟動函數以得到 a[L]，或者說 y hat，

112
00:07:53,740 --> 00:07:58,310
然後這樣就可以算出損失值

113
00:07:58,310 --> 00:08:02,168
那麼，我們已經講過如何實作正向傳播

114
00:08:02,168 --> 00:08:07,070
以得到這些輸出，來算出損失值

115
00:08:07,070 --> 00:08:10,650
那麼反向傳播呢？或者說梯度下降呢？

116
00:08:10,650 --> 00:08:11,990
其實最關鍵的一步，

117
00:08:11,990 --> 00:08:16,240
或說最關鍵的公式，讓你能啟動反向傳播，是這個式子

118
00:08:16,240 --> 00:08:20,460
在損失層對於z[L]的導數，這個其實是

119
00:08:20,460 --> 00:08:26,160
你可以用 y hat, 4乘1的向量，減掉 y，4乘1的向量

120
00:08:26,160 --> 00:08:30,150
所以注意到這些都是4乘1的向量，

121
00:08:30,150 --> 00:08:33,220
因為你有四個類別；一般的情況下則是 C乘1

122
00:08:34,250 --> 00:08:37,180
這邊的 dz 根據我們慣例

123
00:08:37,180 --> 00:08:42,660
這是我們的成本函數對z[L]的偏導數

124
00:08:42,660 --> 00:08:47,570
如果你是微積分專家，你可以自己導一遍

125
00:08:47,570 --> 00:08:50,690
如果你是微積分專家，你可以自己導一遍，不過

126
00:08:50,690 --> 00:08:52,514
直接套用這個公式也可以

127
00:08:52,514 --> 00:08:54,548
如果你需要從無到有實作的話。

128
00:08:54,548 --> 00:08:59,405
不過藉由這個，你就可以計算 dz[L] 然後開始反向傳播的程序

129
00:08:59,405 --> 00:09:05,310
來算出你的神經網路裡所有你需要的導數

130
00:09:05,310 --> 00:09:09,286
不過呢，在這個禮拜的程式作業，我們將會利用眾多

131
00:09:09,286 --> 00:09:13,143
深度學習程式框架的其中一個，
而在程式框架

132
00:09:13,143 --> 00:09:17,830
通常你只要專注於把正向傳播弄正確

133
00:09:17,830 --> 00:09:21,803
而只要你在程式框架中指定好正向傳播，

134
00:09:21,803 --> 00:09:24,845
程式框架會弄清楚要怎麼執行反向傳播，

135
00:09:24,845 --> 00:09:26,730
來幫你做反向的傳播。

136
00:09:27,890 --> 00:09:32,700
所以這個公式值得一記—如果你需要從無到有

137
00:09:32,700 --> 00:09:35,524
實作 softmax 迴歸，或是 softmax 分類—

138
00:09:35,524 --> 00:09:39,579
雖然實際上在這個禮拜的程式作業，你並不會用到他，

139
00:09:39,579 --> 00:09:42,739
因為你所使用的程式框架會幫你處理好

140
00:09:42,739 --> 00:09:43,888
這個導數的計算。

141
00:09:43,888 --> 00:09:46,783
那麼，這就是 softmax 分類了。

142
00:09:46,783 --> 00:09:51,715
藉由他，你現在可以實作學習演算法
來把輸入資料歸類

143
00:09:51,715 --> 00:09:56,920
成不只兩個類別中的一個，而是眾多C個類別中的一個。

144
00:09:56,920 --> 00:10:01,410
接下來呢，我想向你展示一些深度學習的程式框架，

145
00:10:01,410 --> 00:10:05,570
他能讓你在實作深度學習演算法時更有效率。

146
00:10:05,570 --> 00:10:07,550
讓我們繼續看下去