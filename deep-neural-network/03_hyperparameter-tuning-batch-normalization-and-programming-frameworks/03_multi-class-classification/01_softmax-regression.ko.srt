1
00:00:00,990 --> 00:00:04,140
현재까지, 분류 예제를 다뤘던 내용에서는

2
00:00:04,140 --> 00:00:08,410
이진분류법에 대한 내용을 포함하고 있는데요,
여기서는 2가지 레이블링이 가능했습니다. 0또는 1과 같이 말이죠.

3
00:00:08,410 --> 00:00:10,520
고양이인지 고양이가 아닌지 말이죠.

4
00:00:10,520 --> 00:00:13,050
하지만 클래스가 여러개라고 하면 어떻게 할까요?

5
00:00:13,050 --> 00:00:17,140
Softmax regression이라고 하는 로지스틱 회귀의 일반화된 버전인데요,

6
00:00:17,140 --> 00:00:21,130
이것은 1of C나 1of 복수의 클래스의 경우
예측할 수 있도록 가능케 해줍니다.

7
00:00:21,130 --> 00:00:26,280
단지 2개를 인식하는 것과 달리 말이죠. 한번 보겠습니다.

8
00:00:26,280 --> 00:00:26,915
한번 자세히 살펴볼까요.

9
00:00:26,915 --> 00:00:31,264
여러분이 고양이만 인식하는 것이 아니라 고양이, 강아지, 그리고

10
00:00:31,264 --> 00:00:31,984
병아리를 인식하고 싶다고 해보겠습니다.

11
00:00:31,984 --> 00:00:38,050
그러면 저는 고양이를 class 1,
강아지를 class 2, 병아리를 class 3이라고 해보겠습니다.

12
00:00:38,050 --> 00:00:40,914
만약 이에 해당하지 않을 경우,
class 0 이라고 하는

13
00:00:40,914 --> 00:00:44,406
아무것도 해당하지 않는 클래스도 있습니다.

14
00:00:44,406 --> 00:00:49,900
여기는 이미지의 예시가 있는데요
각각의 속하는 클래스도 있습니다.

15
00:00:49,900 --> 00:00:52,680
이 사진은 병아리 사진이고
class 3입니다.

16
00:00:52,680 --> 00:00:57,395
고양이는 class 1, 강아지는 class 2,
그리고 저것은 콜라로 보이는데요,

17
00:00:57,395 --> 00:01:02,498
그러므로 아무것도 해당되지 않기 때문에,
class 0 이 될것이구요, 다음은 class 3 일 것이구요
이렇게 이어나갑니다.

18
00:01:02,498 --> 00:01:07,554
그러면 저희가 사용할 표기법은
저는 여기 대문자 C를 이용해서

19
00:01:07,554 --> 00:01:13,340
입력값의 카테고리화하려는 클래스 번호를 나타낼 것입니다.

20
00:01:13,340 --> 00:01:17,628
이 경우에는, 4가지의 가능한
class가 있는데요, other 카테고리를

21
00:01:17,628 --> 00:01:19,298
포함해서 말이죠.

22
00:01:19,298 --> 00:01:23,921
이렇게 4개의 class가 있는 경우,

23
00:01:23,921 --> 00:01:28,660
여러분의 class는 0에서부터
대문자 C 빼기 1일 것입니다.

24
00:01:28,660 --> 00:01:31,550
다시 말해, 이것은 0이고 1,2,3,인 것입니다.

25
00:01:31,550 --> 00:01:36,653
이 경우에는 신경망을 만들 것인데요
위층이 4인 경우로 말이죠.

26
00:01:36,653 --> 00:01:40,870
이 경우는 C 결과값 유닛이 있는 경우 말이죠.

27
00:01:43,140 --> 00:01:48,908
N은 결과값 층의 유닛 개수인데요
이 값은 4가 될 것입니다.

28
00:01:48,908 --> 00:01:51,807
아니면 일반적으로 이 값은 C가 될 것입니다.

29
00:01:51,807 --> 00:01:56,619
그리고 저희가 원하는 것은
결과값 층이

30
00:01:56,619 --> 00:02:00,860
4개의 class들의 확률을 알려주는 것입니다.

31
00:02:00,860 --> 00:02:04,320
여기 첫번째 노드는

32
00:02:04,320 --> 00:02:08,110
other class일 확률을 결과값으로 나타내길 바라는데요,

33
00:02:08,110 --> 00:02:12,840
x값에 대해서 말이죠, 그리고 이것은
고양이일 확률을 결과값으로 나타낼 것입니다.

34
00:02:12,840 --> 00:02:16,980
x값에 대해서 말이죠,
그리고 이것은 강아지일 확률을 결과값으로 나타내구요

35
00:02:16,980 --> 00:02:20,170
x에 대해서 말이죠,
이것은 병아리일 확률을 나타낼 텐데요,

36
00:02:20,170 --> 00:02:27,910
병아리는 여기서 bc로 줄여서 쓰겠습니다.
x값에 대해서 말이죠.

37
00:02:29,160 --> 00:02:36,600
여기서는 결과값 레이블 ŷ이
4 x 1 차원의 벡터입니다.

38
00:02:36,600 --> 00:02:41,760
그 이유는 4개의 결과값을 표츌해야하기 때문입니다.
여기 4개의 확률을 주면서 말이죠.

39
00:02:42,850 --> 00:02:48,070
그리고 확률은 합해서 1이여야 하기 때문에,
여기 ŷ에 있는 숫자들도

40
00:02:48,070 --> 00:02:48,980
합하면 1이 되어야 합니다.

41
00:02:50,630 --> 00:02:55,390
여러분의 네트워크가 이렇게 할 수 있도록
쓰는 일반적인 모델은

42
00:02:55,390 --> 00:03:00,170
Softmax layer라고 하는 것입니다.
결과값 층에서 이러한 결과값을 생성하기 위해서 말이죠.

43
00:03:00,170 --> 00:03:02,040
제가 지도를 적어볼텐데요,
그러면 나중에 돌아와서

44
00:03:02,040 --> 00:03:04,680
Softmax 가 무엇을 하는지
직관적인 부분을 알 수 있을 것입니다.

45
00:03:06,610 --> 00:03:08,940
마지막 층의 신경망에서는

46
00:03:08,940 --> 00:03:13,360
여기 층의 선형 부분을 산출할 것입니다.

47
00:03:13,360 --> 00:03:17,800
z 대문자 L은, 마지막 층의 z 변수인데요,

48
00:03:17,800 --> 00:03:21,980
기억하시겠지만, 이 층은 대문자 L입니다.

49
00:03:21,980 --> 00:03:26,619
보통은 이것을 wL 곱하기
첫번째 층의

50
00:03:26,619 --> 00:03:32,170
activation 더하기 마지막 층의 bias입니다.

51
00:03:32,170 --> 00:03:33,180
이제 z값을 구했으니

52
00:03:33,180 --> 00:03:37,690
이제 Softmax activation 함수라고 하는 것을
적용해야 하는데요,

53
00:03:38,880 --> 00:03:43,340
이 activation 함수는 조금 Softmax
레이어에서는 조금 일반적이진 않은데요,

54
00:03:43,340 --> 00:03:44,150
이렇게 됩니다.

55
00:03:45,380 --> 00:03:50,081
첫번째로, 우리는 일시적인 변수를 산출할 것입니다.

56
00:03:50,081 --> 00:03:54,180
이 값을 t라고 부르겠습니다.
이 값은 e의 zl 승인데요

57
00:03:54,180 --> 00:03:56,119
이것은 part element-wise입니다.

58
00:03:56,119 --> 00:04:00,824
여기 zL은, 우리 예제에서
zL이 4 X 1 이 될 것입니다.

59
00:04:00,824 --> 00:04:03,470
이것은 4차원 벡터입니다.

60
00:04:03,470 --> 00:04:08,720
t는 e의 zl승인데요 이것은
element wise exponentiation입니다.

61
00:04:08,720 --> 00:04:13,100
T는 또한 4 x 1차원 벡터입니다.

62
00:04:13,100 --> 00:04:14,825
그리고 결과값 aL은

63
00:04:14,825 --> 00:04:20,415
기본적으로 벡터 t이고
정규화 되어 합이 1이 될 것입니다.

64
00:04:20,415 --> 00:04:28,673
그러면 aL은 e 의 zL승 나누기
J가 1에서 4까지인 합 공식입니다.

65
00:04:28,673 --> 00:04:33,994
그 이유는 4개의 class가 있는데
이 class는 각각 t 첨자 i로 나타냅니다.

66
00:04:33,994 --> 00:04:40,082
다시 말해, aL은 또한 4 x 1 벡터입니다.

67
00:04:40,082 --> 00:04:44,780
여기 i 번째 4차원 벡터는

68
00:04:44,780 --> 00:04:50,885
여기 적은대로 aL 아래첨자 i로 나타냅니다.

69
00:04:50,885 --> 00:04:56,660
이것은 ti 나누기 ti의 합닙니다. 아시겠죠?

70
00:04:56,660 --> 00:04:58,690
수학적인 부분이 확실하지 않으시면,

71
00:04:58,690 --> 00:05:02,320
조금 후에 예제를 통해 보겠습니다.
그럼 더 확실해질 것입니다.

72
00:05:02,320 --> 00:05:03,835
수학이 확실하지 않은 경우엔,

73
00:05:03,835 --> 00:05:06,775
예를 통해 살펴보겠습니다.

74
00:05:06,775 --> 00:05:10,905
zL을 산출한다고 해보겠습니다.

75
00:05:10,905 --> 00:05:18,277
그리고 zL은 4차원 벡터이구요
5,2,-1,-3이라고 해보겠습니다.

76
00:05:18,277 --> 00:05:22,256
이제 할 것은 여기
element-wise exponentiation을
이용해서

77
00:05:22,256 --> 00:05:23,665
여기 벡터 t를 산출할 것입니다.

78
00:05:23,665 --> 00:05:29,465
그러면 t는 e의 5승일 것이고,
e의 2승, e의 -1승, e의 3승
이런식으로 될 것입니다.

79
00:05:29,465 --> 00:05:32,529
그리고 이것을 계산기로 구하면,
값은 이렇게 됩니다.

80
00:05:32,529 --> 00:05:38,750
e의 5승은 1484,
e제곱은 약 7.4,

81
00:05:38,750 --> 00:05:44,697
e의 -1승은 0.4 , 그리고 e 세제곱은 20.1

82
00:05:44,697 --> 00:05:49,519
그러므로 벡터t에서 벡터 aL로 가는 것은

83
00:05:49,519 --> 00:05:52,910
여기 입력값을 정규화하여 합이 1이 되게 하는 것입니다.

84
00:05:52,910 --> 00:05:56,808
그러면 t의 element들을 합하면

85
00:05:56,808 --> 00:06:03,232
여기 값을 모두 더하면 176.3이 됩니다.

86
00:06:03,232 --> 00:06:09,565
마지막으로
aL은 여기 벡터 t

87
00:06:09,565 --> 00:06:14,515
나누기 176.3입니다.

88
00:06:14,515 --> 00:06:18,580
예를 들어 여기 첫번째 노드는

89
00:06:18,580 --> 00:06:23,885
이것은 e의 5승 나누기 176.3을 줄 것입니다.

90
00:06:23,885 --> 00:06:27,777
그 값은 0.842입니다.

91
00:06:27,777 --> 00:06:32,675
이 이미지에서는 이것이
z의 값이면

92
00:06:32,675 --> 00:06:36,434
0이라 불릴 확률은 84.2퍼센트 입니다.

93
00:06:36,434 --> 00:06:42,192
그리고 다음 노드는
e 제곱 나누기 176.3을 출력하는데요

94
00:06:42,192 --> 00:06:48,200
이것은 그럼 0.042이고
4.2퍼센트의 확률입니다.

95
00:06:48,200 --> 00:06:53,449
다음 것은 e의 -1승인데요

96
00:06:53,449 --> 00:06:56,891
그 값은 0.042입니다.

97
00:06:56,891 --> 00:07:04,235
마지막은 e의 세제곱 나누기 그것이고,
그 값은 0.114입니다.

98
00:07:04,235 --> 00:07:08,312
그러면 이것은 11.4 퍼센트로
class 3일 확률을 갖습니다.

99
00:07:08,312 --> 00:07:10,683
이것은 baby c class이죠 맞죠?

100
00:07:10,683 --> 00:07:15,568
그러면 class 0일 확률, class 1, class2, class3
확률이 각각 있는데요,

101
00:07:15,568 --> 00:07:21,930
그러면 신경망의 결과값은 aL,
이것은 또한 ŷ입니다.

102
00:07:21,930 --> 00:07:25,170
이것은 4 x 1벡터입니다. 그리고 여기

103
00:07:25,170 --> 00:07:29,800
4 x 1 벡터의 요소는 여기 4개의 숫자입니다.

104
00:07:29,800 --> 00:07:31,060
그럼 이제 그냥 산출합니다.

105
00:07:31,060 --> 00:07:38,077
여기 알고리즘은 zL 벡터 값을 갖고,
확률이 더해서 1이 됩니다.

106
00:07:38,077 --> 00:07:43,013
그리고 저희가 zL 에서 aL로 매핑한 것을
요약하자면

107
00:07:43,013 --> 00:07:47,741
여기 전체 산출 방법에서
exponentiation를 산출해서

108
00:07:47,741 --> 00:07:52,469
일시적 변수 t를 가졌는데요,
그 다음에 normalizing을 통해

109
00:07:52,469 --> 00:07:57,827
이것을 Softmax activation 함수로
요약할 수 있습니다.

110
00:07:57,827 --> 00:08:03,625
그리고 aL = gl (zl) 로 표현할 수 있겠습니다.

111
00:08:03,625 --> 00:08:08,379
이 activation 함수의 조금 특이한 부분은

112
00:08:08,379 --> 00:08:12,654
여기 g activation함수가 4 x 1벡터를
입력값으로 갖고,

113
00:08:12,654 --> 00:08:15,060
4 x 1 벡터를 결과값으로 출력한다는 것입니다.

114
00:08:15,060 --> 00:08:19,280
이전에는 저희 activation 함수가
single row 입력값을 받았는데요,

115
00:08:19,280 --> 00:08:20,875
예를 들어, 시그모이드 함수와

116
00:08:20,875 --> 00:08:24,840
ReLu activation 함수가 실수를 입력값으로 갖고,
실수를 결과값으로 갖습니다.

117
00:08:24,840 --> 00:08:28,255
Softmax activation 함수의 특이한 부분은

118
00:08:28,255 --> 00:08:32,262
다른 결과값들에 거쳐 정규화가 되야한다는 점인데요

119
00:08:32,262 --> 00:08:35,365
그렇기 때문에 벡터의 값을 갖고,
결과값도 벡터로 가져야 한다는 것입니다.

120
00:08:35,365 --> 00:08:39,219
그래서 Softmax cross layer가 나타낼 수 있는 것은

121
00:08:39,219 --> 00:08:43,383
여기 입력값이 x1,x2인 경우의 예제를 보여드리겠습니다.

122
00:08:43,383 --> 00:08:48,435
이것은 바로 3개 또는 4개 또는 더 많은 노드를 가지고 있는

123
00:08:48,435 --> 00:08:53,500
Softmax layer로 삽입됩니다. 그리고 ŷ값을 결과값으로 가질텐데요.

124
00:08:53,500 --> 00:08:58,801
이제 숨겨진 층이 없는 신경망을 보여드리겠습니다.

125
00:08:58,801 --> 00:09:04,777
이것이 하는 것은 z1 = w1 곱하기 입력값 x
더하기 b입니다.

126
00:09:04,777 --> 00:09:07,359
그리고 결과값 a는 a1 또는

127
00:09:07,359 --> 00:09:13,210
ŷ 은 z1에 적용된 Softmax activation 함수입니다.

128
00:09:13,210 --> 00:09:15,615
이렇게 숨겨진 층이 없는 신경망에서는

129
00:09:15,615 --> 00:09:20,260
Softmax 함수가 대표할 수 있는 것들에 대한
내용을 알려줄텐데요,

130
00:09:20,260 --> 00:09:23,677
이것은 raw 입력값 x1 과 x2를 가진 예제인데요,

131
00:09:23,677 --> 00:09:28,662
C=3 인 upper class 를 가진
Softmax layer가

132
00:09:28,662 --> 00:09:31,661
이러한 decision boundaries를 나타낼 수 있습니다.

133
00:09:31,661 --> 00:09:35,289
이러한 몇개의 선형 decision boundaries를 보실텐데요,

134
00:09:35,289 --> 00:09:39,223
이런 것들은 데이터를 세가지
class로 나눌 수 있게 해줍니다.

135
00:09:39,223 --> 00:09:44,126
이 그림에서와 같이 트레이닝세트를 가지고

136
00:09:44,126 --> 00:09:47,335
여기 그림에서 보이듯이 그런 것이고,

137
00:09:47,335 --> 00:09:52,079
Softmax cross fire 를 upper label 과
트레이닝 시킵니다.

138
00:09:52,079 --> 00:09:54,750
그리고 여기 색깔은

139
00:09:54,750 --> 00:09:59,330
그리고 입력값을 3개중 가장 확률이 높을 것 같은것을
기준으로 색칠하는 것입니다.

140
00:09:59,330 --> 00:10:03,790
그리고 입력값을 3개중 가장 확률이 높을 것 같은것을
기준으로 색칠하는 것입니다.

141
00:10:03,790 --> 00:10:07,917
이것은 보시다시피 선형 decision boudary 가 있는

142
00:10:07,917 --> 00:10:11,182
로지스틱 회귀분석과 비슷한데요

143
00:10:11,182 --> 00:10:16,065
2개 이상의 class가 있는 것과 같이 말이죠.
class0 , 1,
class는 0,1 또는 2일 수 있습니다.

144
00:10:16,065 --> 00:10:20,238
여기 Softmax cross fire가
대표하는 또 다른 decision boudary입니다.

145
00:10:20,238 --> 00:10:23,625
3개의 class로 나타나는 경우 말이죠

146
00:10:23,625 --> 00:10:28,731
여기 또 다른 것이 있구요
또 다른 직관적인 부분은

147
00:10:28,731 --> 00:10:34,211
여기 2개의 class간의 decision boundary가
선형일 것입니다.

148
00:10:34,211 --> 00:10:38,325
그렇기 때문에 여기 노란색과

149
00:10:38,325 --> 00:10:42,312
다른 클래스들 사이에
선형 boundary, 보라색과

150
00:10:42,312 --> 00:10:46,949
빨간색선 사이도 그렇고,
노란색과 다른 선형 boundary가 있습니다.

151
00:10:46,949 --> 00:10:49,729
이런 선형 함수를 이용해서

152
00:10:49,729 --> 00:10:52,660
공간을 3개의 다른 class로 나눌 수 있습니다.

153
00:10:52,660 --> 00:10:55,460
그럼 더 많은 class가 있는 예시를 보겠습니다.

154
00:10:55,460 --> 00:10:58,199
이것은 c=4인 예시입니다.

155
00:10:58,199 --> 00:11:03,096
이것은 green class이고 Softmax가
지속적으로 이런 종류의

156
00:11:03,096 --> 00:11:07,280
decision boundary를 복수의 class가 있는경우
나타내줄 수 있습니다.

157
00:11:07,280 --> 00:11:11,796
여기 c=5인 예제도 있습니다.

158
00:11:11,796 --> 00:11:15,190
마지막으로 c=6인 예제도 있습니다.

159
00:11:15,190 --> 00:11:20,184
이것이 Softmax crossfire가 할 수 잇는 것을
보여주는데요

160
00:11:20,184 --> 00:11:24,545
숨겨진 층이 없는 경우 말이죠,
당연히 더 깊은 심층신경망의 경우

161
00:11:24,545 --> 00:11:28,860
어떤 수의 숨겨진 유닛 과
더 많을 수록 등등 의 경우면

162
00:11:28,860 --> 00:11:32,850
더 복잡한 비선형 decision boundary를 통해
배울 수 있습니다.

163
00:11:32,850 --> 00:11:34,065
다른 class들도 나눌수 있습니다.

164
00:11:35,240 --> 00:11:37,990
제가 바라는 것은 이 강의가
신경망에서 Softmax layer 또는

165
00:11:37,990 --> 00:11:41,820
a Softmax activation 함수가
무엇을 할 수 있는지 감을 주었길 바랍니다.

166
00:11:41,820 --> 00:11:42,650
다음 비디오에서는

167
00:11:42,650 --> 00:11:46,940
Softmax layer를 사용하는 신경망을 어떻게
트레이닝 시킬 수 있는지 알아보겠습니다.