이전 비디오에서는 soft max 에관해 배웠는데요, softmax activation 함수 말이죠 이번 비디오에서는 softmax 분류법에 대한
이해도를 넓히고 여러분의 모델을 트레이닝 하는
방법에 대해 배우겠습니다. 결과값 층이 z[L]을 이렇게 산출하는
이전의 예제를 보겠습니다. 저희는 4개의 class 가 있는데요 c = 4면 z[L]은 4,1 벡터이고
우리는 t를 계산한다고 했습니다. 일시벽인 변수라고 했죠.
element wise exponentiation를 수행하는 변수 말이죠 그리고 마지막으로 결과값 층에 대한
activation 함수 g[L]이고 이것이 softmax activation 함수인 경우
결과값은 그러면 이렇게 될 것입니다. 기본적으로 일시적 변수 t를 가지고,
정교화하뎌 합이 1이 되게 하는 것입니다. 그러면 이것은 a(L)이 됩니다. 그러면 z 벡터에서
가장 큰 값은 5였는뎅, 가장 큰 확률은 여기 첫번때 확률입니다. softmax 라는 이름은 hard max라는 것과 비교한다는 의미에서 만들어졌는데요,
벡터 z와 비교하여 여기 벡터에 맞출 것입니다. hard max 함수와 같은 경우
z의 요소들을 봐서 가장 큰 요소에 1을 넣고 나머지는 0으로 채울 것입니다. 이것은 매우 hard max 인데요,
가장 큰 요소는 결과값 1을 갖습니다. 나머지는 모두 결과값 0을 갖게됩니다. 반면에 그에 반해 softmax는 조금 더 젠틀한 매핑인데요,
z에서 여기 확률까지 말이죠. 저는 이게 가장 훌륭한 이름인지 모르겠으나
적어도 그게 왜 softmax라고 불렸는지 이 모든 것은 hard max에 대한 것의 비교되는 것입니아. 제가 보여드리지 않고 언급만 한 것중에 하나는
softmax regression 또는 softmax regression activation 함수가
로지스틱 회귀분석을 C clss로 일반화한다는 것입니다.
2개의 class로 하기 보다 말이죠. 그리하여 C = 2인 경우,
c=2 인 경우릐 softmax인 경우, 로지스틱회귀분석으로 줄어듭니다. 이 비디오에서는 증병하지 않을 것이지만
대략적인 증명 가이드라인은 만약 c=2인 경우,
softmax 를 적용하면 a[L]이라고 하는 결과값 층은
2개의 숫자를 줄 것입니다. c가 2일때 말이죠. 그러면 0.842와 0.158를 결과값으로 줄 수 있겠습니다. 여기 2개의 숫자는 항상 합해서 1이 되어야 합니다. 그리고 여기 2개의 숫자가 합해서 1이 되어야 하기 때문에,
이것들은 중복입니다. 이 2개를 구지 계산을 할 필요가 없을 수도 있으나 1개 정도는 계산해야 할 수도 맀습니다. 그렇기 때문에 결과적으로 숫자를 산출하는 과정에 있어 여기 한개의 결과값을 가지도록
로지스틱 회귀분석 처리가 됩니다. 이것은 증명은 아니었습니다만
여기서 기억할 것은 softmax regression은 로지스틱 회귀분석의
일반화라는 것입니다. 2개의 이상의 class에 대해서 말이죠. 그러면 이제 실제로
softmax 결과값 층으로 어떻게 신경망을 트레이닝 시키는지
한번 보겠습니다. 특히 신경망을 트레이닝 하는데 쓰이는
loss 함수를 정의해보겠습니다.
예제를 한번 보겠습니다. 예를 들어보겠습니다. 트레이닝세트의 예제를 한번 보겠는데요,
목표가 ground truth 이리블이 0 1 0 0 인것을 보겠습니다. 이전 비디오에서 본 예제인데요, 이 뜻은 이것이 class 1이기 때문에 고양이 이미지라는 것입니다. 그리고 여러분의 신경망이 ŷ의 결과값을 ŷ인 벡터 확률이고 합해서 1이 될텐데요, 여기 0.1,0.4,, 등등 이것의 합이 1이라는 것을
확인할 수 있습니다.그리고 이것은 a[L]이 됩니다. 여기 신경망은 그닥 잘하는 것이 아닌게, 이것이 고양이 부문인데,
오직 고양이일 확률이 오직 20퍼센트이기 때문입니다. 그러므로 여기 예제에서는 잘하지 않았습니다. 그러면 어떤 loss함수를 이용해서 이 신경망을
트레이닝 시켜야 하는 것일까요? softmax classification에서는 주로 사용하는 loss는
마이너스 j가 1에서 4까지의 합인데요 더 일반적인 케이스에서는
c가 1에서 4까지의 경우인데, 여기서는 4를 사용하고
yj log ŷ 의 j를 사용하겠습니다. 그러면 하나의 예시를 통해서 어떻게 되는 것인지
더 자세히 보겠습니다. 여기 예시에서 보면 y1 = y3 = y4 = 0 인데요, 그 이유는 이것이 전부 0이고,
오로지 y2만 1이기 때문입니다. 여기 합을 보면 여기 모든 항이 0인 yj들의 값은 0이였습니다. 그럼 마지막으로 남는 항은 -y2 log ŷ 2인데요 j의 지수에 대한 합을 이용하기 때문입니다. 모든 항은 0이 되고,
유일하게 j가 2인 경우는 y2 = 1이기 때문에 이 경우
-log ŷ 2가 됩니다. 이것이 뜻하는 바는, 여러분의 러닝 알고리즘이
이 값을, 기울기 강하를 사용해서 트레이닝 세트의
loss를 줄인려고 한다면 이 값을 작게 만드는 유일한 방법은, 이것이 유일한 방법인데요,
ŷ 2 를 최대한 크게 만드는 것입니다. 그리고 이것은 확률이기 때문에
절대로 1의 값을 넘을 수 없습니다. 이것은 말이되죠. 왜냐면 이 예제의
x 는 고양이의 사진인 경우, 이 결과값 확률이 최대한 크길 바랄 것이기 때문입니다. 그러므로 더 일반적으로는
여기 loss 함수가 하는 것은 여러분의 트레이닝 세트에서
ground truth class가 어떻던지 간에
그에 상응하는 class의 확률값을 최대한 높게 만들려고
하는 것입니다. 여러분이 maximum likelihood estimation
statistics와 익숙하시다고 하면, 이것이 maximum likelihood estimation유형에
해당하는 것입니다. 이것이 무슨 뜻인지 잘 모르는 경우엔
걱정하지 마십시요. 지금 설명드린 직관적인 부분이면
충분할 것입니다. 이것은 한개의 트레이닝 예시에 대한
loss 내용입니다. 그러면 전체 트레이닝 세트에서의 j 비용은
어떨까요? 즉, 여기 이런 파라미터들을 세팅하는데 드는
비용을 모든 방법과 bias를 계산하는 방법 중에서, 이것을 여러분이 예상하는 값으로,
모든 트레이닝 세트의 loss의 합을 여러분의 러닝 알고리즘의 예측치를
트레이닝 예시에 거쳐 더한 값입니다. 그러므로 밀어내는 것이란 핵심적인 연관성에
촛점을 맞추는 것입니다 여러분이 하는 것은 기울기 강하를 이용해서
여기 비용함수를 최소화하려고 하는 것입니다. 마지막으로 실행부분에서의 한가지 내용입니다. c가 4이기 때문에, y 는 4 x 1벡터인데요, ŷ도 또한 4 x 1 벡터입니다. 그러므로 여러분이 vectorized limitation을 이용하는 경우,
대문자 Y 매트릭스는 y(1), y(2), ,, y(m)까지 가로로 쌓인 것이고, 예를 들어, 이것이 여러분의 첫번때 트레이닝 예시인 경우, 첫번째 메트랙스의 세로부분은 0 1 0 0일 것이고
두번째 예시는 강아지, 세번째 예시는 아무 해당사항 없음 class일 것입니다. 그럼 여기 Y 매트릭스는
4 x m 차원의 매트릭스가 될 것입니다. 비슷하게 ŷ은 h hat 1 ,, 등등 ŷ m까지 가로고 쌓아질 것인데요,
이것은 사실 ŷ 1입니다. 그러면 첫번째 트레이닝 예시에서 모든 결과값은
그러면 ŷ 0.3, 0.2 , 0.1 그리고 0.4 등등이 될 것입니다. 그리하여 ŷ 또한
4 x m 차원의 매트릭스가 됩니다. 마지막으로 softmax 결과값 층의 경우
어떻게 기울기 강하를 도입하는지 한번 살펴보겠습니다. 보면 여기 결과값 층은 z[L]을 산출할텐데요,
C x 1 인데요, 저희 예제에서는 4 by 1 인데요
softmax activation 함수를 적용하여
a[L] 또는 ŷ을 갖고, 그 것은 결과적으로 loss를 산출할 수 있게
해줍니다. 저희는 신경망의 전 방향전파 step을 도입해서 여기 결과값을 구하고
loss를 산출하는 방법을 배웠는데요, 후 방향전파 스텝 또는 기울기 강하는
어떨까요? 여기 expression에서 초기화해야하는 핵심 절차 또는 공식은 loss 층에서의 z의 derivative입니다. 이것을 통해 ŷ을 구하고,
4 x 1벡터 빼기 4 x 1벡터인 y를 해서 구합니다., 여기 모든 값이 4 x 1 벡터일 텐데요 4 class를 갖고, 더 일반적인 겨웅
c x 1일 것입니다. 이것이 그러면 보통 dz의 정의일텐데요. z[L]에 대한 비용함수의 partial derivative입니다. 여러분이 미적분에 익숙한 경우,
이것을 한번 직접 계산해볼 수 있습니다. 직접 공식을 계산해볼 수 있지만, 여기 공식을 바로 이용해도 됩니다. 처음부터 바로 도입해야 되는 경우에 말이죠. 이것을 이용해서 이제는 dz[L]계산하고
후 방향전파절차를 시작해서 모든 필요한 신경망의 derivative값을 구합니다. 이번 주 연습문제에서 보겠디만, 딥러닝 프로그램 프레임웍 중에 하나를 사용할텐데요,
이런 프레임웍에서는 그냥 전 방향전파가 올바르게 되도록 집중하는 것이
중요합니다. 이것을 그리고 주요 프레임웍으로 규명하는 한, 주요 프레임웍은 후 방향전파하는 방법도 알아낼 것입니다. backward pass를 하는 방법을 말이죠. 이 표현은 알아두는 것이 좋습니다.
softmax regression을 도입해야하는 경우에는 말이죠, 또는 처음부터 softmax classification을 도입해야하는 경우 말이죠. 물론 여러분이 이번주 연습문제에는 이 것이 필요없습니다만, 여러분이 사용하는 primary framework이 여기 derivative 산출을 맡아줄 것입니다. softmax classification에 대한 내용은 이것이 전부입니다. 이것을 이용해서 이제 여러분은
러닝 알고리즘을 캐릭터화된 입력값에 도입시킬 수 있습니다. 2개중 1개의 class 뿐만 아니라
복수의 c class에서 1개를 말이죠. 다음으로는 딥러닝 프로그래밍 프레임웍을 보여드리겠습니다. 이 것은 딥러닝 알고리즘을 도입시키는 과정에서
효율성을 높혀줄 것입니다. 그럼 다음 비디오로 넘어가서 이야기해보겠습니다.