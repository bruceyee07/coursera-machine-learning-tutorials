1
00:00:00,570 --> 00:00:02,937
上一节我们学习了softmax激活函数

2
00:00:02,937 --> 00:00:04,900
softmax激活函数

3
00:00:04,900 --> 00:00:08,760
这一节你将加深对softmax分类的理解

4
00:00:08,760 --> 00:00:13,230
并学习如何训练采用了softmax层的模型

5
00:00:13,230 --> 00:00:18,434
回想我们前面的示例 输出层是这样计算Z[L]的

6
00:00:18,434 --> 00:00:19,915
即我们有4个类(classes)

7
00:00:19,915 --> 00:00:24,573
c=4则Z[L]是个(4*1)维的向量 然后计算t

8
00:00:24,573 --> 00:00:30,160
t是一个临时变量 它表示元素y的幂

9
00:00:30,160 --> 00:00:34,778
最后 如果输出层的激活函数

10
00:00:34,778 --> 00:00:40,320
g[L]是一个softmax激活函数 则输出是这样的

11
00:00:40,320 --> 00:00:45,570
基本上它就是将临时变量t规范化使其和为1

12
00:00:45,570 --> 00:00:49,328
得到a(L)

13
00:00:49,328 --> 00:00:53,304
你应该注意到 z中最大的元素是5

14
00:00:53,304 --> 00:00:57,650
a中最大概率值也依然是第一个元素

15
00:00:57,650 --> 00:01:02,653
softmax的名字对应hardmax

16
00:01:02,653 --> 00:01:07,848
hardmax将矢量z映射到这个矢量

17
00:01:07,848 --> 00:01:12,869
即 hardmax函数遍历Z中的元素

18
00:01:12,869 --> 00:01:18,182
将Z中最大的元素对应的位置置1 其余位置置0

19
00:01:18,182 --> 00:01:23,259
所以它实在简单粗暴(hard) 最大的元素得到输出1

20
00:01:23,259 --> 00:01:25,850
其余的元素对应输出0

21
00:01:25,850 --> 00:01:27,626
与之相对的是

22
00:01:27,626 --> 00:01:33,500
softmax中Z到这些概率值的映射要平和些

23
00:01:33,500 --> 00:01:37,980
所以 我不知道这是不是一个NB的名字

24
00:01:37,980 --> 00:01:42,040
但是至少它体现了softmax背后的直观原因<br />都是来自于与hardmax的比较

25
00:01:43,060 --> 00:01:47,930
另一件我没有真正展示但是有所暗示的事情是

26
00:01:47,930 --> 00:01:52,450
softmax激活函数将logistic激活函数

27
00:01:52,450 --> 00:01:56,330
从2分类推广到C分类

28
00:01:56,330 --> 00:02:01,586
也就是说 如果C=2 那么C=2的softmax

29
00:02:01,586 --> 00:02:07,940
实际上将简化成logistic回归

30
00:02:07,940 --> 00:02:13,680
我不会在这个视频里给出证明 但是大致的证明思路是

31
00:02:13,680 --> 00:02:18,087
当C=2时 如果你应用softmax

32
00:02:18,087 --> 00:02:23,929
那么输出层a[L]将输出2个数

33
00:02:23,929 --> 00:02:28,872
也许是0.842和0.158

34
00:02:28,872 --> 00:02:31,056
这2个数的和总是1

35
00:02:31,056 --> 00:02:34,221
因为这2个数的和必须为1 他们实际上是冗余的

36
00:02:34,221 --> 00:02:37,077
也许你不需要费心计算2个值

37
00:02:37,077 --> 00:02:39,193
而是只需要计算其中一个就可以了

38
00:02:39,193 --> 00:02:43,999
你计算这个数的方法将会推导成

39
00:02:43,999 --> 00:02:48,895
logistic回归的计算方法 即最后只需要计算一个输出

40
00:02:48,895 --> 00:02:53,835
这个算不上什么证明 但是要记住的是

41
00:02:53,835 --> 00:02:58,468
softmax回归是logistic回归从2分类到多分类的推广

42
00:02:58,468 --> 00:03:02,187
现在让我们看看实际上训练一个

43
00:03:02,187 --> 00:03:04,157
包含softmax输出层的神经网络要怎么做

44
00:03:04,157 --> 00:03:04,966
所以具体地来说

45
00:03:04,966 --> 00:03:08,437
先定义用于训练神经网络的损失函数

46
00:03:08,437 --> 00:03:09,427
举个例子来看我们举个例子说明

47
00:03:09,427 --> 00:03:15,018
这个例子中训练集的目标输出(target output)

48
00:03:15,018 --> 00:03:17,881
即标签真实值是[0 1 0 1]'

49
00:03:17,881 --> 00:03:20,661
也就是前一节视频提到的例子

50
00:03:20,661 --> 00:03:25,500
这意味着这是一张猫的图 因为它落在类1上

51
00:03:25,500 --> 00:03:31,096
现在假设你的神经网络的输出是y^

52
00:03:31,096 --> 00:03:35,083
y^是一个概率矢量 其元素和为1

53
00:03:35,083 --> 00:03:42,870
0.1 0.4...你可以验证他们的和为1 a[L]=y^

54
00:03:42,870 --> 00:03:46,320
由此可见这个神经网络对这个样例的预测并不好

55
00:03:46,320 --> 00:03:49,670
因为它实际上是猫但是在猫这一分类只得到了20%概率

56
00:03:49,670 --> 00:03:51,180
所以它在这个样例上表现并不好

57
00:03:52,290 --> 00:03:56,700
那么你想要用怎样的损失函数来训练这个神经网络呢

58
00:03:56,700 --> 00:03:58,762
softmax分类典型的损失计算

59
00:03:58,762 --> 00:04:03,310
一般都会使用从1到4的的和的负值

60
00:04:03,310 --> 00:04:07,269
实际上更一般的表示是从1到C

61
00:04:07,269 --> 00:04:14,620
这里C=4 并取yj*log((y^)j)

62
00:04:14,620 --> 00:04:20,040
我们通过一个具体的例子来更好地理解它

63
00:04:20,040 --> 00:04:24,209
注意到这个例子中

64
00:04:24,209 --> 00:04:32,730
y1=y3=y4=0 因为这些都是0 只有y2=1

65
00:04:32,730 --> 00:04:35,447
所以这个和式中

66
00:04:35,447 --> 00:04:39,726
所有yj=0的项都等于0

67
00:04:39,726 --> 00:04:44,412
只剩下了 -y2*log((y^)2)

68
00:04:44,412 --> 00:04:47,802
那么以j为索引求和

69
00:04:47,802 --> 00:04:52,622
则所有的项最终都为0--除了j=2的项

70
00:04:52,622 --> 00:04:58,440
因为y2=1 和式最后演算得到 -log((y^)2)

71
00:04:58,440 --> 00:05:00,190
这意味着什么？

72
00:05:00,190 --> 00:05:04,510
如果你的学习算法试图将其变小

73
00:05:04,510 --> 00:05:09,040
因为你会用梯度下降法来减少训练集的损失

74
00:05:09,040 --> 00:05:12,550
那么唯一使其变小的方法是使这个变小

75
00:05:12,550 --> 00:05:17,390
而唯一使它变小的方法是使(y^)2尽可能的大

76
00:05:18,430 --> 00:05:20,846
因为这些都是概率值 所以其值<=1

77
00:05:20,846 --> 00:05:26,286
但是它的确说明问题 因为这个样例确实是一张猫的图片

78
00:05:26,286 --> 00:05:31,170
你当然希望它的输出概率值尽可能的大

79
00:05:31,170 --> 00:05:35,590
所以更一般地 损失函数的功能是查看

80
00:05:35,590 --> 00:05:39,640
训练集的真实分类值 并令其对应的

81
00:05:39,640 --> 00:05:42,640
概率值尽可能的大

82
00:05:42,640 --> 00:05:46,126
如果你熟悉最大似然估计统计学习方法<br />(maximum likelihood estimation statistics)

83
00:05:46,126 --> 00:05:49,153
这实际上也是某种形式的最大似然估计<br />(maximum likelyhood estimation)

84
00:05:49,153 --> 00:05:51,790
不过如果你不熟悉也没有关系

85
00:05:51,790 --> 00:05:53,770
刚刚我们讨论过的那些直观上的理解就够用了

86
00:05:54,970 --> 00:05:57,460
这只是单个训练样例的损失

87
00:05:57,460 --> 00:06:00,857
整个训练集的损失要如何处理呢？

88
00:06:00,857 --> 00:06:05,717
基于某个参数集的损失<br />包括权重(weights) 偏差(bias)等等

89
00:06:05,717 --> 00:06:09,767
它的定义正像你猜的那样

90
00:06:09,767 --> 00:06:12,926
定义为整个训练集的损失的和

91
00:06:12,926 --> 00:06:18,374
即所有训练样例的预测值与真实值所反映的损失

92
00:06:18,374 --> 00:06:18,924
因此

93
00:06:18,924 --> 00:06:23,830
你要做的是使用梯度下降法使损失(cost)最小化

94
00:06:23,830 --> 00:06:26,370
最后要说的是一个实现上的细节

95
00:06:26,370 --> 00:06:30,949
因为C=4 则y是一个4*1的矢量

96
00:06:30,949 --> 00:06:33,160
y^也是一个4*1的矢量

97
00:06:34,450 --> 00:06:39,565
那么如果你使用矢量化计算实现

98
00:06:39,565 --> 00:06:45,711
则矩阵Y=(y(1),y(2),...y(m)) 表示为列矩阵

99
00:06:45,711 --> 00:06:50,903
比如说 这里是你的第一个训练样例

100
00:06:50,903 --> 00:06:56,428
则矩阵Y的第一列为[0 1 0 0]'

101
00:06:56,428 --> 00:07:01,730
第二列的样例是狗的图片 第三个样例二者都不是 等等

102
00:07:01,730 --> 00:07:08,580
则Y是一个4*m维矩阵

103
00:07:08,580 --> 00:07:13,838
类似地 y^也是一个列矩阵

104
00:07:13,838 --> 00:07:18,284
y^1到y^m 这实际上是y^1

105
00:07:19,590 --> 00:07:25,403
也就是第一个训练样例的输出

106
00:07:25,403 --> 00:07:29,120
则y^1为[0.3 0.2,...]'

107
00:07:29,120 --> 00:07:33,290
Y^也是一个4*m维的矩阵

108
00:07:33,290 --> 00:07:37,382
最后让我们看看梯度下降法在

109
00:07:37,382 --> 00:07:38,942
包含softmax输出层的神经网络上的实现

110
00:07:38,942 --> 00:07:46,161
这个输出层计算z[L] 我们的例子中z[L]是4*1的矢量

111
00:07:46,161 --> 00:07:52,670
然后应用softmax激活函数计算a[L] 即y^

112
00:07:53,740 --> 00:07:58,310
然后在此基础上计算损失

113
00:07:58,310 --> 00:08:02,168
我们刚刚讨论的是如何实现前向传播

114
00:08:02,168 --> 00:08:07,070
以获得输出并计算损失

115
00:08:07,070 --> 00:08:10,650
那么反向传播也就是梯度下降法如何实现？

116
00:08:10,650 --> 00:08:11,990
实际上关键步骤或者说

117
00:08:11,990 --> 00:08:16,240
关键方程式的初始化是这个表达式

118
00:08:16,240 --> 00:08:20,460
它在最后一层对z求偏导 得到

119
00:08:20,460 --> 00:08:26,160
y^-y

120
00:08:26,160 --> 00:08:30,150
y和y^都是维度为4*1的矢量

121
00:08:30,150 --> 00:08:33,220
因为这个例子是4分类 更一般的情况是C*1维矢量

122
00:08:34,250 --> 00:08:37,180
这就是dz通常的定义形式

123
00:08:37,180 --> 00:08:42,660
是损失函数对z[L]的偏导

124
00:08:42,660 --> 00:08:47,570
如果你特别熟悉微积分 你可以自己推导这个等式

125
00:08:47,570 --> 00:08:50,690
如果你特别熟悉微积分
你可以自己试试推导这个等式

126
00:08:50,690 --> 00:08:52,514
但是直接使用这个公式也可以

127
00:08:52,514 --> 00:08:54,548
如果你有需要从头开始实现的话

128
00:08:54,548 --> 00:08:59,405
有了这个公式 你就可以从计算dz[L]<br />开始你的反向传播过程

129
00:08:59,405 --> 00:09:05,310
并遍历神经网络各层以计算你所需的各个导数

130
00:09:05,310 --> 00:09:09,286
实际上本周的主要练习 我们将开始

131
00:09:09,286 --> 00:09:13,143
使用一个深度学习的编程框架 而这些基本框架

132
00:09:13,143 --> 00:09:17,830
通常都只需要你专注于正确地实现正向传播

133
00:09:17,830 --> 00:09:21,803
只要你指定它为一个基本框架(primary framwork)<br />并制定前向传播过程

134
00:09:21,803 --> 00:09:24,845
该框架将自动为你实现

135
00:09:24,845 --> 00:09:26,730
反向传播过程

136
00:09:27,890 --> 00:09:32,700
所以这个表达式值得你牢记

137
00:09:32,700 --> 00:09:35,524
如果你需要从头开始实现softmax回归或softmax分类的话

138
00:09:35,524 --> 00:09:39,579
尽管你在本周的主要练习中不会用到它

139
00:09:39,579 --> 00:09:42,739
因为框架将会替你实现

140
00:09:42,739 --> 00:09:43,888
这些导数计算

141
00:09:43,888 --> 00:09:46,783
那么这就是softmax分类

142
00:09:46,783 --> 00:09:51,715
你可以用它实现的不仅仅是二分类

143
00:09:51,715 --> 00:09:56,920
还包括C分类

144
00:09:56,920 --> 00:10:01,410
下一节我将向你展示一些深度学习的编程框架

145
00:10:01,410 --> 00:10:05,570
它将使你的深度学习算法的实现更有效率

146
00:10:05,570 --> 00:10:07,550
下一节见