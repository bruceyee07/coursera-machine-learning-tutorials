1
00:00:00,570 --> 00:00:02,937
이전 비디오에서는 soft max 에관해 배웠는데요,

2
00:00:02,937 --> 00:00:04,900
softmax activation 함수 말이죠

3
00:00:04,900 --> 00:00:08,760
이번 비디오에서는 softmax 분류법에 대한
이해도를 넓히고

4
00:00:08,760 --> 00:00:13,230
여러분의 모델을 트레이닝 하는
방법에 대해 배우겠습니다.

5
00:00:13,230 --> 00:00:18,434
결과값 층이 z[L]을 이렇게 산출하는
이전의 예제를 보겠습니다.

6
00:00:18,434 --> 00:00:19,915
저희는 4개의 class 가 있는데요

7
00:00:19,915 --> 00:00:24,573
c = 4면 z[L]은 4,1 벡터이고
우리는 t를 계산한다고 했습니다.

8
00:00:24,573 --> 00:00:30,160
일시벽인 변수라고 했죠.
element wise exponentiation를 수행하는 변수 말이죠

9
00:00:30,160 --> 00:00:34,778
그리고 마지막으로 결과값 층에 대한
activation 함수 g[L]이고

10
00:00:34,778 --> 00:00:40,320
이것이 softmax activation 함수인 경우
결과값은 그러면 이렇게 될 것입니다.

11
00:00:40,320 --> 00:00:45,570
기본적으로 일시적 변수 t를 가지고,
정교화하뎌 합이 1이 되게 하는 것입니다.

12
00:00:45,570 --> 00:00:49,328
그러면 이것은 a(L)이 됩니다.

13
00:00:49,328 --> 00:00:53,304
그러면 z 벡터에서
가장 큰 값은 5였는뎅,

14
00:00:53,304 --> 00:00:57,650
가장 큰 확률은 여기 첫번때 확률입니다.

15
00:00:57,650 --> 00:01:02,653
softmax 라는 이름은 hard max라는 것과

16
00:01:02,653 --> 00:01:07,848
비교한다는 의미에서 만들어졌는데요,
벡터 z와 비교하여 여기 벡터에 맞출 것입니다.

17
00:01:07,848 --> 00:01:12,869
hard max 함수와 같은 경우
z의 요소들을 봐서 가장 큰 요소에

18
00:01:12,869 --> 00:01:18,182
1을 넣고 나머지는 0으로 채울 것입니다.

19
00:01:18,182 --> 00:01:23,259
이것은 매우 hard max 인데요,
가장 큰 요소는 결과값 1을 갖습니다.

20
00:01:23,259 --> 00:01:25,850
나머지는 모두 결과값 0을 갖게됩니다. 반면에

21
00:01:25,850 --> 00:01:27,626
그에 반해

22
00:01:27,626 --> 00:01:33,500
softmax는 조금 더 젠틀한 매핑인데요,
z에서 여기 확률까지 말이죠.

23
00:01:33,500 --> 00:01:37,980
저는 이게 가장 훌륭한 이름인지 모르겠으나
적어도 그게 왜 softmax라고 불렸는지

24
00:01:37,980 --> 00:01:42,040
이 모든 것은 hard max에 대한 것의 비교되는 것입니아.

25
00:01:43,060 --> 00:01:47,930
제가 보여드리지 않고 언급만 한 것중에 하나는
softmax regression 또는

26
00:01:47,930 --> 00:01:52,450
softmax regression activation 함수가
로지스틱 회귀분석을

27
00:01:52,450 --> 00:01:56,330
C clss로 일반화한다는 것입니다.
2개의 class로 하기 보다 말이죠.

28
00:01:56,330 --> 00:02:01,586
그리하여 C = 2인 경우,
c=2 인 경우릐 softmax인 경우,

29
00:02:01,586 --> 00:02:07,940
로지스틱회귀분석으로 줄어듭니다.

30
00:02:07,940 --> 00:02:13,680
이 비디오에서는 증병하지 않을 것이지만
대략적인 증명 가이드라인은

31
00:02:13,680 --> 00:02:18,087
만약 c=2인 경우,
softmax 를 적용하면

32
00:02:18,087 --> 00:02:23,929
a[L]이라고 하는 결과값 층은
2개의 숫자를 줄 것입니다. c가 2일때 말이죠.

33
00:02:23,929 --> 00:02:28,872
그러면 0.842와 0.158를 결과값으로 줄 수 있겠습니다.

34
00:02:28,872 --> 00:02:31,056
여기 2개의 숫자는 항상 합해서 1이 되어야 합니다.

35
00:02:31,056 --> 00:02:34,221
그리고 여기 2개의 숫자가 합해서 1이 되어야 하기 때문에,
이것들은 중복입니다.

36
00:02:34,221 --> 00:02:37,077
이 2개를 구지 계산을 할 필요가 없을 수도 있으나

37
00:02:37,077 --> 00:02:39,193
1개 정도는 계산해야 할 수도 맀습니다.

38
00:02:39,193 --> 00:02:43,999
그렇기 때문에 결과적으로 숫자를 산출하는 과정에 있어

39
00:02:43,999 --> 00:02:48,895
여기 한개의 결과값을 가지도록
로지스틱 회귀분석 처리가 됩니다.

40
00:02:48,895 --> 00:02:53,835
이것은 증명은 아니었습니다만
여기서 기억할 것은

41
00:02:53,835 --> 00:02:58,468
softmax regression은 로지스틱 회귀분석의
일반화라는 것입니다. 2개의 이상의 class에 대해서 말이죠.

42
00:02:58,468 --> 00:03:02,187
그러면 이제 실제로
softmax 결과값 층으로

43
00:03:02,187 --> 00:03:04,157
어떻게 신경망을 트레이닝 시키는지
한번 보겠습니다.

44
00:03:04,157 --> 00:03:04,966
특히

45
00:03:04,966 --> 00:03:08,437
신경망을 트레이닝 하는데 쓰이는
loss 함수를 정의해보겠습니다.
예제를 한번 보겠습니다.

46
00:03:08,437 --> 00:03:09,427
예를 들어보겠습니다.

47
00:03:09,427 --> 00:03:15,018
트레이닝세트의 예제를 한번 보겠는데요,
목표가

48
00:03:15,018 --> 00:03:17,881
ground truth 이리블이 0 1 0 0 인것을 보겠습니다.

49
00:03:17,881 --> 00:03:20,661
이전 비디오에서 본 예제인데요,

50
00:03:20,661 --> 00:03:25,500
이 뜻은 이것이 class 1이기 때문에 고양이 이미지라는 것입니다.

51
00:03:25,500 --> 00:03:31,096
그리고 여러분의 신경망이 ŷ의 결과값을

52
00:03:31,096 --> 00:03:35,083
ŷ인 벡터 확률이고 합해서 1이 될텐데요,

53
00:03:35,083 --> 00:03:42,870
여기 0.1,0.4,, 등등 이것의 합이 1이라는 것을
확인할 수 있습니다.그리고 이것은 a[L]이 됩니다.

54
00:03:42,870 --> 00:03:46,320
여기 신경망은 그닥 잘하는 것이 아닌게,

55
00:03:46,320 --> 00:03:49,670
이것이 고양이 부문인데,
오직 고양이일 확률이 오직 20퍼센트이기 때문입니다.

56
00:03:49,670 --> 00:03:51,180
그러므로 여기 예제에서는 잘하지 않았습니다.

57
00:03:52,290 --> 00:03:56,700
그러면 어떤 loss함수를 이용해서 이 신경망을
트레이닝 시켜야 하는 것일까요?

58
00:03:56,700 --> 00:03:58,762
softmax classification에서는

59
00:03:58,762 --> 00:04:03,310
주로 사용하는 loss는
마이너스 j가 1에서 4까지의 합인데요

60
00:04:03,310 --> 00:04:07,269
더 일반적인 케이스에서는
c가 1에서 4까지의 경우인데,

61
00:04:07,269 --> 00:04:14,620
여기서는 4를 사용하고
yj log ŷ 의 j를 사용하겠습니다.

62
00:04:14,620 --> 00:04:20,040
그러면 하나의 예시를 통해서 어떻게 되는 것인지
더 자세히 보겠습니다.

63
00:04:20,040 --> 00:04:24,209
여기 예시에서 보면

64
00:04:24,209 --> 00:04:32,730
y1 = y3 = y4 = 0 인데요, 그 이유는 이것이 전부 0이고,
오로지 y2만 1이기 때문입니다.

65
00:04:32,730 --> 00:04:35,447
여기 합을 보면

66
00:04:35,447 --> 00:04:39,726
여기 모든 항이 0인 yj들의 값은 0이였습니다.

67
00:04:39,726 --> 00:04:44,412
그럼 마지막으로 남는 항은 -y2 log ŷ 2인데요

68
00:04:44,412 --> 00:04:47,802
j의 지수에 대한 합을 이용하기 때문입니다.

69
00:04:47,802 --> 00:04:52,622
모든 항은 0이 되고,
유일하게 j가 2인 경우는

70
00:04:52,622 --> 00:04:58,440
y2 = 1이기 때문에 이 경우
-log ŷ 2가 됩니다.

71
00:04:58,440 --> 00:05:00,190
이것이 뜻하는 바는,

72
00:05:00,190 --> 00:05:04,510
여러분의 러닝 알고리즘이
이 값을,

73
00:05:04,510 --> 00:05:09,040
기울기 강하를 사용해서 트레이닝 세트의
loss를 줄인려고 한다면

74
00:05:09,040 --> 00:05:12,550
이 값을 작게 만드는 유일한 방법은,

75
00:05:12,550 --> 00:05:17,390
이것이 유일한 방법인데요,
ŷ 2 를 최대한 크게 만드는 것입니다.

76
00:05:18,430 --> 00:05:20,846
그리고 이것은 확률이기 때문에
절대로 1의 값을 넘을 수 없습니다.

77
00:05:20,846 --> 00:05:26,286
이것은 말이되죠. 왜냐면 이 예제의
x 는 고양이의 사진인 경우,

78
00:05:26,286 --> 00:05:31,170
이 결과값 확률이 최대한 크길 바랄 것이기 때문입니다.

79
00:05:31,170 --> 00:05:35,590
그러므로 더 일반적으로는
여기 loss 함수가 하는 것은

80
00:05:35,590 --> 00:05:39,640
여러분의 트레이닝 세트에서
ground truth class가 어떻던지 간에
그에 상응하는

81
00:05:39,640 --> 00:05:42,640
class의 확률값을 최대한 높게 만들려고
하는 것입니다.

82
00:05:42,640 --> 00:05:46,126
여러분이 maximum likelihood estimation
statistics와 익숙하시다고 하면,

83
00:05:46,126 --> 00:05:49,153
이것이 maximum likelihood estimation유형에
해당하는 것입니다.

84
00:05:49,153 --> 00:05:51,790
이것이 무슨 뜻인지 잘 모르는 경우엔
걱정하지 마십시요.

85
00:05:51,790 --> 00:05:53,770
지금 설명드린 직관적인 부분이면
충분할 것입니다.

86
00:05:54,970 --> 00:05:57,460
이것은 한개의 트레이닝 예시에 대한
loss 내용입니다.

87
00:05:57,460 --> 00:06:00,857
그러면 전체 트레이닝 세트에서의 j 비용은
어떨까요?

88
00:06:00,857 --> 00:06:05,717
즉, 여기 이런 파라미터들을 세팅하는데 드는
비용을 모든 방법과 bias를 계산하는

89
00:06:05,717 --> 00:06:09,767
방법 중에서, 이것을 여러분이

90
00:06:09,767 --> 00:06:12,926
예상하는 값으로,
모든 트레이닝 세트의 loss의 합을

91
00:06:12,926 --> 00:06:18,374
여러분의 러닝 알고리즘의 예측치를
트레이닝 예시에 거쳐 더한 값입니다. 그러므로

92
00:06:18,374 --> 00:06:18,924
밀어내는 것이란 핵심적인 연관성에
촛점을 맞추는 것입니다

93
00:06:18,924 --> 00:06:23,830
여러분이 하는 것은 기울기 강하를 이용해서
여기 비용함수를 최소화하려고 하는 것입니다.

94
00:06:23,830 --> 00:06:26,370
마지막으로 실행부분에서의 한가지 내용입니다.

95
00:06:26,370 --> 00:06:30,949
c가 4이기 때문에, y 는 4 x 1벡터인데요,

96
00:06:30,949 --> 00:06:33,160
ŷ도 또한 4 x 1 벡터입니다.

97
00:06:34,450 --> 00:06:39,565
그러므로 여러분이 vectorized limitation을 이용하는 경우,
대문자 Y 매트릭스는

98
00:06:39,565 --> 00:06:45,711
y(1), y(2), ,, y(m)까지 가로로 쌓인 것이고,

99
00:06:45,711 --> 00:06:50,903
예를 들어, 이것이 여러분의 첫번때 트레이닝 예시인 경우,

100
00:06:50,903 --> 00:06:56,428
첫번째 메트랙스의 세로부분은 0 1 0 0일 것이고
두번째 예시는

101
00:06:56,428 --> 00:07:01,730
강아지, 세번째 예시는 아무 해당사항 없음 class일 것입니다.

102
00:07:01,730 --> 00:07:08,580
그럼 여기 Y 매트릭스는
4 x m 차원의 매트릭스가 될 것입니다.

103
00:07:08,580 --> 00:07:13,838
비슷하게 ŷ은 h hat 1 ,, 등등

104
00:07:13,838 --> 00:07:18,284
ŷ m까지 가로고 쌓아질 것인데요,
이것은 사실 ŷ 1입니다.

105
00:07:19,590 --> 00:07:25,403
그러면 첫번째 트레이닝 예시에서 모든 결과값은
그러면 ŷ 0.3, 0.2 , 0.1 그리고

106
00:07:25,403 --> 00:07:29,120
0.4 등등이 될 것입니다.

107
00:07:29,120 --> 00:07:33,290
그리하여 ŷ 또한
4 x m 차원의 매트릭스가 됩니다.

108
00:07:33,290 --> 00:07:37,382
마지막으로 softmax 결과값 층의 경우
어떻게 기울기 강하를

109
00:07:37,382 --> 00:07:38,942
도입하는지 한번 살펴보겠습니다.

110
00:07:38,942 --> 00:07:46,161
보면 여기 결과값 층은 z[L]을 산출할텐데요,
C x 1 인데요,

111
00:07:46,161 --> 00:07:52,670
저희 예제에서는 4 by 1 인데요
softmax activation 함수를 적용하여
a[L] 또는 ŷ을 갖고,

112
00:07:53,740 --> 00:07:58,310
그 것은 결과적으로 loss를 산출할 수 있게
해줍니다.

113
00:07:58,310 --> 00:08:02,168
저희는 신경망의 전 방향전파

114
00:08:02,168 --> 00:08:07,070
step을 도입해서 여기 결과값을 구하고
loss를 산출하는 방법을 배웠는데요,

115
00:08:07,070 --> 00:08:10,650
후 방향전파 스텝 또는 기울기 강하는
어떨까요?

116
00:08:10,650 --> 00:08:11,990
여기 expression에서 초기화해야하는

117
00:08:11,990 --> 00:08:16,240
핵심 절차 또는 공식은

118
00:08:16,240 --> 00:08:20,460
loss 층에서의 z의 derivative입니다.

119
00:08:20,460 --> 00:08:26,160
이것을 통해 ŷ을 구하고,
4 x 1벡터 빼기 4 x 1벡터인 y를 해서 구합니다.,

120
00:08:26,160 --> 00:08:30,150
여기 모든 값이 4 x 1 벡터일 텐데요

121
00:08:30,150 --> 00:08:33,220
4 class를 갖고, 더 일반적인 겨웅
c x 1일 것입니다.

122
00:08:34,250 --> 00:08:37,180
이것이 그러면 보통 dz의 정의일텐데요.

123
00:08:37,180 --> 00:08:42,660
z[L]에 대한 비용함수의 partial derivative입니다.

124
00:08:42,660 --> 00:08:47,570
여러분이 미적분에 익숙한 경우,
이것을 한번 직접 계산해볼 수 있습니다.

125
00:08:47,570 --> 00:08:50,690
직접 공식을 계산해볼 수 있지만,

126
00:08:50,690 --> 00:08:52,514
여기 공식을 바로 이용해도 됩니다.

127
00:08:52,514 --> 00:08:54,548
처음부터 바로 도입해야 되는 경우에 말이죠.

128
00:08:54,548 --> 00:08:59,405
이것을 이용해서 이제는 dz[L]계산하고
후 방향전파절차를 시작해서

129
00:08:59,405 --> 00:09:05,310
모든 필요한 신경망의 derivative값을 구합니다.

130
00:09:05,310 --> 00:09:09,286
이번 주 연습문제에서 보겠디만,

131
00:09:09,286 --> 00:09:13,143
딥러닝 프로그램 프레임웍 중에 하나를 사용할텐데요,
이런 프레임웍에서는

132
00:09:13,143 --> 00:09:17,830
그냥 전 방향전파가 올바르게 되도록 집중하는 것이
중요합니다.

133
00:09:17,830 --> 00:09:21,803
이것을 그리고 주요 프레임웍으로 규명하는 한,

134
00:09:21,803 --> 00:09:24,845
주요 프레임웍은 후 방향전파하는 방법도 알아낼 것입니다.

135
00:09:24,845 --> 00:09:26,730
backward pass를 하는 방법을 말이죠.

136
00:09:27,890 --> 00:09:32,700
이 표현은 알아두는 것이 좋습니다.
softmax regression을 도입해야하는 경우에는

137
00:09:32,700 --> 00:09:35,524
말이죠, 또는 처음부터 softmax classification을 도입해야하는 경우 말이죠.

138
00:09:35,524 --> 00:09:39,579
물론 여러분이 이번주 연습문제에는 이 것이 필요없습니다만,

139
00:09:39,579 --> 00:09:42,739
여러분이 사용하는 primary framework이

140
00:09:42,739 --> 00:09:43,888
여기 derivative 산출을 맡아줄 것입니다.

141
00:09:43,888 --> 00:09:46,783
softmax classification에 대한 내용은 이것이 전부입니다.

142
00:09:46,783 --> 00:09:51,715
이것을 이용해서 이제 여러분은
러닝 알고리즘을 캐릭터화된 입력값에 도입시킬 수 있습니다.

143
00:09:51,715 --> 00:09:56,920
2개중 1개의 class 뿐만 아니라
복수의 c class에서 1개를 말이죠.

144
00:09:56,920 --> 00:10:01,410
다음으로는 딥러닝 프로그래밍 프레임웍을 보여드리겠습니다.

145
00:10:01,410 --> 00:10:05,570
이 것은 딥러닝 알고리즘을 도입시키는 과정에서
효율성을 높혀줄 것입니다.

146
00:10:05,570 --> 00:10:07,550
그럼 다음 비디오로 넘어가서 이야기해보겠습니다.