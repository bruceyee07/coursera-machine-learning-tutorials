到目前为止，我们已经讨论过的分类示例使用了 二分类方法,也就是当你只有两类标签的情况 这是猫,或者不是猫 如果我们有多种可能的分类目标呢? 有一种更普遍的逻辑回归的方法叫做softmax回归 这种方法能够让你试图预测一个 多种分类中的类别时做出预测,而不是识别两类中的类别 让我们一起来看看让我们一起来看看 假设你不单需要识别猫,你还需要识别狗 和小鸡 因此我将会把这个猫叫做类别1,狗就是类别2,小鸡就是类别3 并且如果一个类别不属于上面这三种 那么我们把它们分类成类别0 这里显示的图片和他们对应的分类就是一个例子 这一张是小鸡 因此它属于类别3 猫是类别1,狗是类别2,而这个就是一只考拉 它不属于任何一类 所以分为类别0
这是类别3 依次类推 我们将使用一些缩写 大写的C表示你将要预测的总的类别数 这种情况下 你共有4个类别 其中包括了一类 不属于3类任何一类 那么当你有4个类别时 类别序号就是 从0到C-1 换句话说,就是0,1,2,3 这种情况下,我们就需要构建一个新的神经网络
其中输出层是4个 或者说有C个输出单元 因此输出层L的单元数N等于4 或者说等于C 我们想得到的是输出层的单元告诉我们 每个类别的概率 因此这里的第一个单元应该输出的 概率就是"其他"类别的概率 给定输入x 网络这个节点就会输出概率为猫的概率 这个就会输出x是狗的概率 这个就会输出x是小鸡的概率 这里我把小鸡简写为bc 所以这里输出的层y^就是一个4*1维度的向量 因为他有四个输出,代表四个概率 由于输出的概率加和应该是1 因此y^输出层的总和就应该是1 标准化的做法就是使你的网络使用 这里的softmax层,以及生成这些输出的层 我们先把网络画下来 等下回来再看的时候 就会对softmax的作用有一个直观的理解了 在网络的最后一层 你需要像往常那样计算每层的线性部分 因此这个就是最后一层的变量 请记住这个大写的L层 像往常一样你计算w*L 再乘以上一层输出的激活层加上一个偏差 现在有了z 现在你需要把这个softmax激活函数用起来 这个激活函数和softmax层有一点不同 但它就是这样 首先我们计算一个临时的值 这里我们把它叫做t 他是我写的这样的 这适用于每个元素 所以这里的zL就是4*1维度的 这里是一个4维的向量 所以这个t的公式就是对所有元素求幂 t就是一个4*1的向量 这个输出层a^[L] 基本上就是t 但是需要归一化到1 于是al就需要图中这样计算出来 由于我们有四个类别 也就是说我们想要aL层也是一个4*1层的向量 第i个元素的向量 写出来就是 需要如图中这样计算 为了避免数学公式不够清晰 我们接下来将展示一个例子来解除大家的疑惑 为了避免大家不明白 现在让我们来看一个特殊的例子 假设你计算出了zL 而zL是一个4维的向量 我们将要使用这个针对每个元素计算的幂 计算这个向量t 所以t就是这几个e的指数形式 如果你使用计算器计算一下,你就得到了想要的值 e^5就是148.4 e^2就是7.4 e^-1就是0.4 e^3就是20.1 于是根据向量t得到aL的方法就是 把输入的归一化到1 如果你把t的值加起来 大概就是176.3 最后aL就是这个向量t 整体除以176.3 举个例子,这里的第一个数就是 e^5除以176.3 结果就是0.842 也就是说加入这个是z的值 那么也就是说图片属于类别0的概率是84.2％ 下一个节点就是e^2除以176.3 结果就是0.042 所以就是4.2％的概率 下一个就是e^-1除以这个数 就是0.002 最后一个就是e^3除以这个数就是0.0.114 也就是11.4%的概率是类别3 也就是小鸡的概率 所以我们就得到了类别0,1,2,3的概率 神经网络aL的输出 也就是y^ 就是4*1维的向量 其中的元素就是这里的4个数 然后我们来计算它然后我们来计算它 这个算法通过向量zL计算出总和为1的四个概率 并且如果我们总结一下从zL到aL的计算过程 这整个的计算过程从计算幂 到得出临时变量 再做归一化 我们可以把这个过程总结为一个softmax激活函数 假设aL等于向量zL的激活函数g 这个激活函数不同之处在于 这个函数g需要输入一个4*1的向量 也会输出一个4*1的向量 以前我们的激活通常是接收单行输入 比如 sigmoid函数和ReLU函数就是接收一个实数输入 然后输出一个实数的输出 softmax函数的不同之处就是 由于它需要把输出归一化 以及输入输出都是向量 因此softmax分类器还能代表什么呢? 现在我将向你们展示一些例子 其中输入是x1和x2 这些输入直接送入一个softmax层 它有三四个或者更多的输出节点 所以我将会向你展示一个没有隐藏层的网络 他的作用就是计算z1=w1*x+b 输出的a1 或者说y^就是在z1上使用softmax激活函数 所以这个没有隐藏层的神经网络 应该可以让你有一个关于softmax函数意义的直观感觉 这里是一个例子:输入x1和x2 一个输出类别C=3的softmax层可以代表 这种形式的决策边界 请注意这是一种线性决策边界 但是它可以把我们的数据区分为三类 这个图中我们所做的就是 选择图中展示的训练数据 使用数据的三种标签训练这个softmax分类器 图中的颜色显示了 分类器输出的阈值 输入的颜色是基于三种输出中概率最高的那种 这就是一种泛化的逻辑回归 它使用一类似种线性的决策边界 分类结果不只是只有0和1的两类结果 还可以是0,1,2 这是softmax可以代表决策边界的另一个例子 使用三个分类 这里还有另一种 就像图中右边这样 但是直觉告诉我们两类之间的决策边界是线性的 这就是为什么你看的例子中 黄色和其他分类之间的分类界面是线性的 紫色和红色直接也是线性的 黄色和紫色之间也是线性的 但是它能使用这照片不同的线性函数 来把数据区分为不同类别 我们来看一个有更多分类的例子 这是一个有分类数C=4的例子 其中绿色的类别和softmax仍然可以代表线 不同类别直接的线性决策平面 这是另一个例子 其中C=5 这是另一个例子 其中C=6 这些图展示了softmax分类器在没有隐藏层的时候 可以做什么 当然有的网络会更深 有更多的隐藏层 以及更多的隐藏单元 然后你就可以发现更复杂的非线性决策平面来区分 这些不同的类别 我希望这部分的讲解可以让你直观的理解在网络中的 softmax激活函数的工作原理 这里这个 下个视频我们来看一下如何训练
一个带有softmax层的神经网络