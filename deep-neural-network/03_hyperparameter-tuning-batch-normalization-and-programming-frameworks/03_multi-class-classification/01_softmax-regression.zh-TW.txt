目前為止，我們討論過的分類問題都是 二元分類：你有兩個可能的類別，0或1 這是貓嗎？這不是貓嗎？ 如果我們有多個可能的類別，那會怎麼樣呢？ 有一種更廣義化的羅吉斯迴歸，叫做 Softmax 迴歸 讓你能預測 C 個類別中的一個， 或是多個類別中的一個，而非僅止於兩類 讓我們看看讓我們看看 假設我們不再光是辨認貓貓：
你要辨認貓、狗、 和小雞 那，我要稱貓是類別1，狗是類別2，小雞是類別3 如果以上皆非，那就是其他 或是以上皆非的類別，讓我稱之為類別0 那麼這邊是一些圖片和其類別的例子 這是小雞的圖片，所以是類別3 貓是類別1，狗是類別2，我想
這是無尾熊，所以 是以上皆非，也就是類別0。然後是類別3，以此類推。 這邊我們所用的符號是，我要用大寫 C 來表示你想分類類別的數量 在這個例子，你有四個可能的類別，
包括「其他」 或叫「以上皆非」的類別。 所以當你有四個類別，你類別 索引的數字會是 0 到 C-1， 也就是說，是 0, 1, 2 或 3 在這裡，我們會創一個神經網路，
其輸出層有 4 個 或是說有 C 個輸出單元 所以 n，輸出層，也就是第 L 層輸出單元的數量
會等於4 或更廣義地說，等於 C 而我們想要的，是讓輸出層的神經元 告訴我們這四個類別，是其中某個類別的機率 所以這邊第一個圓圈
應該要輸出 或者說，想要他輸出「其他」類別的機率 在我們輸入 x 的時候。
而這個則輸出是貓的機率 當輸入x時。
這會輸出狗的機率 當輸入x時。
那個則會輸出機率 我把小雞縮寫成 "bc"，
所以是輸入x時，是 bc 的機率 所以這邊，輸出的類別 y hat
會是 4乘1維度的向量 因為他現在會輸出四個數字，給你這四個機率 也因為機率加起來要等於一，
y hat 的這四個值 加起來必須是 1 在神經網路中，這樣的標準模型是用一個叫 「Softmax 層」放到輸出層內，以產生出這些輸出 讓我先寫數學式子，你可以之後回來看看 對 Softmax 在做什麼有些概念 那麼，在神經網路的最後一層 和平常一樣，你計算這一層線性的部份 所以 z[L]，這是最後一層的 z 變數 還記得這是第 L 層 所以照例地你算出 W[L] 乘前一層的 啟動值，再加上最後一層的偏移項。 在算完 z 之後， 你要套用所謂的「Softmax 啟動函數」。 在Softmax層，那個啟動函數和一般的不大一樣 他是這樣做的： 首先，我們計算出一個暫時的變數 我們稱之 t，是 e 的 z[L] 次方 這是套用在逐個元素上的 在我們的例子，z[L] 是 4乘1 是個四維的向量 所以 t 本身，e 的 z[L] 次方，這是逐元素的次方， t 也會是一個 4乘1 的向量 然後輸出的 a[L] 會是向量 t，但是標準化至總和為 1 所以 a[L] 是 e 的 z[L] 次方，除以 j=1到4 的 t_i 總和 因為我們有四個類別。 所以這也表示，a[L] 也是一個 4乘1 的向量 而且這個四維向量的第 i 個元素 寫做 a[L] 下標 i，這會 等於 t_i 除以 t_i 的總和 萬一這數學公式不夠清楚 我們等等就拿個例子讓這件事更清楚 萬一這數學公式不夠清楚 讓我們跑過實際的例子來釐清之 假設你算出了 z[L]， z[L] 是個四維向量，假設是 5, 2, -1, 3 我們要做的是用逐元素的指數 來算出向量 t 所以 t 會是 e^5, e^2, e^-1 e^3 如果你按按計算機，這會是你得到的 e^5 是 148.4，e^2 大概是 7.4 e^-1 是 0.4，而 e^3 是 20.1 那我們從向量 t 轉到向量 a[L] 的方式是 把這些元素標準化，使得總和為 1 所以如果你把 t 的元素加起來 把這四個數字加起來，你會得到 176.3 所以最後，a[L] 會是這個向量 t 作為一個向量，除以 176.3 所以舉例，這邊第一個點 他會輸出e的5次方除以 176.3 也就是 0.842 所以是，對於這個圖片，如果你得到的 z 值長這樣 他是類別0的機率是 84.2% 然後下一個點輸出e平方除以176.3 這會是 0.042，所以會是 4.2% 的機率 下一個是 e^-1 除以這個 等於 0.002 最後一個是 e 的三次方除以這個，等於 0.114 所以有 11.4% 的機率這會是類別3 也就是小雞，對吧 所以這就是他是類別0，類別1，類別2，類別3的機率 所以這神經網路的輸出 a[L]，也就是 y hat 這是 4乘1 的向量，其中 四個元素會是這邊的四個數字 就是剛剛算的 所以這個演算法拿 z[L] 向量
轉成四個相加為一的機率 把剛剛把z[L]轉成a[L]的數學式整理一下 從算指數開始、得到暫時的變數 t、 然後標準化，這全部的計算 我們可以把整個流程
總結為 Softmax 啟動函數 就是 a[L] 等於啟動函數 g 套用在向量 z[L] 上 這個啟動函數特別的地方在於 這啟動函數 g，他吃一個 4乘1 的向量 然後輸出一個4乘1的向量 我們之前看到的啟動函數是拿單一一個數字作為輸入 例如S型函數和 ReLU啟動函數，輸入一個實數然後輸出一個實數 Softmax啟動函數特別的是 因為他要把不同的輸出整個做標準化 他必須拿一個向量，然後輸出一個向量 那麼，Softmax 分類器能描繪的 我來舉一個例子：你有輸入 x1, x2 然後這些直接餵進一個 Softmax 層，有三個、 四個、或多個輸出的節點，來輸出 y hat 所以我給你看的是一個沒有隱藏層的神經網路 他唯一做的是計算 z[1] 等於 W[1] 乘以輸入 x，加上 b 然後輸出的 a[1]， 或說是 y hat，只是 Softmax 啟動函數套用在 z[1] 上 所以這個沒有隱藏層的神經網路 應該能給你一些感覺
Softmax函數能表示出什麼樣的東西 這邊是一個例子，輸入只有 x1, x2， 一個 C=3 個輸出類別的 Softmax 層 可以表現出這樣的決策邊界
(decision boundaries) 注意這可看作有多個線性的決策邊界 然而這已能把資料分成三個類別 關於這張圖，實際上我們所做的是把 圖裡面顯示的訓練資料 訓練出有三個類別的 Softmax 分類器 而這圖的顏色顯示了 Softmax 分類器輸出的臨界點，
著色是根據 對於某個輸入，其輸出的三個機率哪一個最高來著色 所以某種角度，我們可以把這個看成
廣義的羅吉斯迴歸 其中有多個線性的決策邊界， 而類別不再是兩個 0, 1，
而是比兩個類別還多：0, 1 或 2。 這邊是另一個 Softmax 分類器決策邊界的例子 訓練在有三個類別的資料上 這邊又是另一個例子。這邊有個觀念是 任兩個類別中間的決策邊界會是直線 這就是為什麼你在這邊看到， 在黃色和紅色類別中間的邊界是直的 紫色和紅色中間，直線的邊界，
紫色和黃色，另一個線性的決策邊界 他能利用各種不同的線性函數 來把這個空間分成三個類別 讓我們來看看有更多類別的例子 那麼這是C=4的例子 所以這綠色的類別和 Softmax 依然表現出 這樣子多類別之間的線性決策邊界 這邊是更多的例子，C=5 這最後的例子是 C=6 所以，這邊示範了當 Softmax 分類器沒有隱藏層時， 能夠做到的事情。當然以更深的神經網路，有x 然後一些隱藏神經元，然後更多的隱藏神經元，等等等 這樣你就能學到更複雜的，
非線性的決策邊界來分開 各種不同的類別。 那麼，我希望這能給你一些概念，
在神經網路裡面 Softmax 層或者是 Softmax 啟動函數能做到什麼 在下一個影片中 讓我們瞧瞧你怎麼去訓練一個
有Softmax層的神經網路