當您試圖改進神經網路時 您需要選擇的其中一項是 使用什麼啟動函數在隱藏層 跟輸出單元在您的 神經網路, 目前為止我們僅 使用S型啟動函數 但有時候其他選擇可以 做得更好, 讓我們來看 一些選項, 在正向傳播 的步驟對於神經網路 我們有兩個步驟我們都使用 S型函數
所以這個S型函數 稱為啟動函數
而 G 是熟悉的S型函數 a 等於 1 除以 1 加 e 的 -z 次方 一般情況下我們可以 用不同的函數 G of z, 
我們寫成這樣 而 G 可以是非線性 函數
也許不是S型函數 舉個例子
S型函數 從 0 到 1
一個 啟動函數幾乎總是 做得比S型函數更好 的是 tanh 函數
或者稱為雙曲 正切函數
所以這是 z 這是 a 等於 tanh of z 而這個 從 正 1 到 負 1
這個公式 對於 tanh 函數是
e 的 z次方 減 e 的 -z次方
除以它們的和 實際上是一個 S型函數的數學轉移版本 所以您知道
S型函數像這樣 但移動它
讓它經過 原點
然後放大比例 讓它從 負 1 到 正 1 實際上對於隱藏單元
如果您讓 函數 G of z 等於 tanh of z
這幾乎總是作用得 比 S型函數
因為 介於正 1 跟負 1 的 啟動值平均數 從隱藏層出來時會趨近 於 0 平均值
也就是 有時候當您訓練一個學習 演算法 您或許會集中您的資料 讓資料有 0 平均值, 使用 tanh
而不是 用S型函數
有一種 集中您的資料的影響
所以 資料的平均值會趨近於 0 而不是 0.5
而這 實際上讓下一層的學習 容易一些
我們會在 第二個課程談到這些
當我們 談到最佳化演算法時 但一個重點是我 幾乎不再用S型啟動 函數 tanh 函數幾乎總是 比較好
一個例外是 在於輸出層
因為如果 y 不是 0 就是 1 的話
那對於 y-hat 應該是一個數字 輸出介於 0 跟 1
而不是 介於 負1 跟 正1
所以一個 例外是我會使用S型 啟動函數是
當您使用 二元分類時您 也許會用S型啟動 函數在輸出層
所以 G of z[2] 這裡會等於
S形函數 of z[2] 您看到的這個例子 您或許會用 tanh 啟動 函數在隱藏層 S型函數在輸出層, 所以 啟動函數可能不同 在不同層, 而有時候 為了記啟動函數 在不同層時有所不同
我們 會使用上標方括號 來指示 G of 方 括號 1
也許不同於 G of 方 括號 2
上標方括號1 指的是這一層 上標方括號 2
指的是 輸出層 對於 S型函數跟 tanh 函數
的一個共同缺點是 如果 z 很大或是 很小時
那函數梯度 或者說斜率 會變得很小
所以 z 很大或者 z 很小時
函數的斜率 幾乎趨近於 0, 所以這會讓
梯度下降法慢下來 一種很大眾化的選擇 在機器學習是 稱為線性整流函數單元 這個線性整流函數像這樣 公式是 a 等於 max of 0, z 當 z 是正值時導數為 1 當 z 是負值時導數 為 0
如果您 建置這個時
技術上而言 當 z 為 0 時導數是 沒有定義的
但當您使用 電腦來建置時
通常您 得到的值是 0000000 很小
您不用去 擔心它
實作上您可以 假設導數當 z 等於 0 時您可以假設為 1 或 0 您還是可以作用得很好
即使是 不能微分的事實
這裡有 一些準則來 選擇啟動函數
如果您的 輸出是 0 跟 1的值
如果您用 二元分類
那S型 啟動函數是很自然的選擇在 輸出層
而其他的 單元
ReLU 或者 線性整流函數 單元越來越是預設選擇的 啟動函數
如果您不 確定使用什麼在您的隱藏層 我會直接使用 ReLU 啟動 函數
這是大部分的人 在這個時期常使用的
雖然有時候 人們也會用到 tanh 啟動 函數 ReLU 的一個缺點是 當 z 為負的時候導數為 0 實作上這還是可行的 但有另一種版本的 ReLU 稱為滲漏線性整流函數(Leaky ReLU) 會在下一張投影片給您公式 與其當 z 負的時候為 0 它會用很小的斜率 像這樣
所以稱為 滲漏 線性整流函數
通常這會 比線性整流啟動函數好
雖然 在實作上比較少用到 兩者皆可 如果您必須只選一個
我 通常只是使用
線性整流函數 線性整流函數跟
滲漏線性整流函數的優點是 很多 z 的空間 啟動函數的導數 啟動函數的斜率 是非常異於 0 在實作上使用 ReLU (線性整流函數) 啟動函數您的神經網路 通常學習比使用 tanh 或者S型函數快很多 主要的原因是 比較不被 函數斜率趨近於 0
會緩慢 整個學習的影響
我知道 ReLU 一半 z 值的斜率為 0 實作時足夠多的 隱藏單元會讓 z 大於 0 學習還是可以很快 對於大部分的訓練例子
讓我們 很快整理一下利與弊 對於不同的啟動函數 這是S型啟動函數我會說 不要用它
除非是輸出 層如果您做二元分類 或者也許不會用到 我幾乎從來不用它的原因是 因為 tanh 就是 比較好
所以 tanh 啟動函數是這樣
然後 最常用的預設選擇 啟動函數是 ReLU 也就是這個
所以當您不確定 用什麼時, 就用這個
或許您 高興的話也可以用 滲漏線性整流函數
也許 0.01z 逗點 z
所以 a 是最大值 of 0.01 乘 z 跟 z
所以這給您 的函數扳一點點彎曲
您 也許會說為什麼用常數 0.01
您也可以將它視為另一個 學習演算法的參數 有些人說這樣做會更好 但我極少看到人們這樣做 但如果您想試著在您的 應用上這樣使用
請隨您意 您可以看它的作用 跟作用得如何 然後繼續用它
如果它給您好的結果 我希望給您一點點感覺對於 一些您可以使用的啟動函數的選擇 在您的神經網路中
一個 我們會看到在深度學習的主題是
您 通常有很多不同的選擇在於 您如何建造您的神經網路從 隱藏單元的數目到 選擇啟動函數
到您如何 初始您的權值
我們會 在以後看到很多選擇像這樣 實際上有時候很難有 好的指南
對於什麼確實 作用得最好
在您的問題上 在這些課程中
我會持續給您 一些感覺對於我看到在這個 產業裡哪些是比較 常用的
但對於您的應用 您應用的綜合性 確實很難事先知道 哪一種作用最佳
所以 一點建議是如果您不確定 哪一種啟動 函數最好
那就全部試試看 然後用保持 驗證集或者開發集來驗證 我們以後會談到
然後看 哪一種作用得最好就用它 我想利用測試這些 不同的選項
對於您的應用 您會為未來準備比較好的 神經網路架構相對於 您問題的綜合性跟 演算法的演進
而不是 如果我告訴您永遠 使用 ReLU 啟動函數
而不用 其他的
這或許會或許不會 適用於不管是什麼您處理的問題 您知道在 最近或者以後 所以這是選擇 啟動函數
您看到 最受歡迎的一些啟動函數 有一個問題 有時候被問到的是
為什麼您 甚至需要用到 啟動函數
為什麼不丟掉它 讓我們來 談一談 在下一段影片中
您會看到為什麼 神經網路確實需要 一些非線形啟動函數