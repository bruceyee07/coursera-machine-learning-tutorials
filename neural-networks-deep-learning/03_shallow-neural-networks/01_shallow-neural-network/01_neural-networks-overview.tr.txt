Tekrar hoş geldiniz. Bu hafta yapay sinir ağını kurmayı teknik detaylara girmeden öğrendiniz. 
İstediğim bu video size bu haftanın videolarıyla ilgili genel bir bakış açısı verecek. eğer tüm detayları takip etmezseniz endişelenmeyin, ileriki videolarda teknik detayları derinlemesine ele alacağız. Ama şimdi sinir ağınızı nasıl kuracağınıza dair küçük bir ön izleme yapalım.
 Geçen hafta lojistik regresyon hakkında konuştuk ve bu modelin ekranda gördüğünüz hesaplama grafiğine nasıl karşılık geldiğini gördük. Girdi olarak her bir özellik X ve parametrelerimiz w ile b'yi kullanarak z'yi hesapladınız. Daha sonra z'yi kullanarak a'yı hesapladınız. y^ yani çıktı ile aynı terime karşılık gelen a'yı kullanarak kayıp fonksiyonu L'i hesaplayabilirsiniz. Sinirsel ağ böyle gözükür ve daha önce kastettiğim gibi bir çok sigmoid birimini bu şekilde üst üste dizerek bir sinirsel ağ kurabilirsiniz. Daha önce dediğimiz gibi, bu düğüm iki hesap aşamasını gösteriyor ilk hesap z değerini ikinci hesap ise a değerini belirliyor. Bu sinirsel ağda bu düğüm yığını Z gibi bir hesaplamaya ve aynı zamanda şu kısım a benzeri bir hesaplamaya denk geliyor. Ve bu düğüm diğer bir z ve diğer bir a'ya karşılık geliyor. Dolayısıyla ileride kullanacağımız notasyon şöyle olacak: öncelikle girdilerimiz X ve beraberinde bazı parametrelerimiz W ve b bunlar Z1'i hesaplamanızı sağlayacak Kullanacağımız yeni notasyon şöyle: üstindis köşeli parantez içinde 1 ile bu düğüm yığınıyla yani katman ile alakalı değerleri göstereceğiz. Sonra üstindis köşeli parantez içinde 2 ile ile bu düğüm yığınındaki değerleri yani sinir ağının diğer bir katmanındaki değerleri
göstereceğiz. Bu üstindise eklediğimiz köşeli parantezler buradaki gibi, eğitim setindeki her bir örneği tanımlamak için kullandığımız normal parantezler ile karıştırılmamalıdır. Yani X üstindis normal parantez ile i'inci eğitim seti örneğinden bahsederken üstindis köşeli parantez 1 ve 2 her bir katmana karşılık geliyor. Sinir ağının birinci ve ikinci katmanları. Devam edelim, Z1'i hesapladıktan sonra lojistik regresyonda olduğu gibi a1'i hesaplayacağız. z1'in sigmoid fonksiyonundan geçirilmiş hali. Diğer bir lineer denklem kullanarak a2'yi hesaplayacağız.
a2 sinirsel ağın nihai çıktısı ve aynı zamanda y^ile eş anlamlı olarak kullanılacak. Biliyorum çok fazla detay oldu ancak alınacak esas ders şu ki lojistik regresyon için Z ve müteakiben a hesaplamalarımız vardı. Sinirsel ağda ise bunu birden fazla sayıda
yapıyoruz. z'yi takip eden bir a hesaplaması ve tekrar z ve ardından a ve daha sonra son olarak sondaki kaybı hesaplıyorsunuz ve yapısal bağıntı(logistic regression)'nın türevlerini hesaplamak için (da,dz vs.) burada gördüğünüz gibi geriye yönelik hesaplama yapıyoruz dolayısıyla aynı mantıkla bir sinir ağı, buna benzeyen şekilde da2,dz2 yi hesapladığınız ve bunların dw2 db2 yi hesaplamada kullanılacağı sağdan sola doğru hesaplamaları kırmızı okla belirttiğim şekilde yapacağınız geriye yönelik hesaplama yapar ve böylece, bunlar size sinir ağının ne yaptığı ile ilgili fikir verecektir basitçe yapısal bağlanımı(logictic regression) alıyor ve iki kez tekrar ediyor. Çok fazla yeni kavram ve detay olduğunu biliyorum fakat eğer hepsini takip edemediyseniz bunlar hakkında çok fazla endişelenmeyin daha sonraki videolarda bu detayların üzerinden daha yavaş bir şekilde geçeceğiz hadi, sinir ağlarının nasıl temsil edildiği ile alakalı olan bir sonraki videoya geçelim