歡迎回來，這個禮拜您將會學到 建置一個神經網路，在進入 技術細節之前我想用 這段影片給您一點概述 有關您會在這個禮拜看到的影片 如果您跟不上 這段影片所有的細節, 不用擔心 我們會進入技術的細節 在下幾段影片, 但現在讓我們 給一個快速的瀏覽您如何 建置您的神經網路, 上週我們 談過羅吉斯迴歸分析 我們看到這個模型如何對應到 下面的計算圖形您 把特徵 x 跟參數 w 跟 b 讓您計算 z 然後用來計算 a, 我們 用 a 跟這個輸出 y-hat 通用然後您可以計算損失函數 l 一個神經網路像這樣 前面暗示過 您可以形成一個神經網路用 很多小的S形函數疊在一起 而在前面 這個節點表示兩個步驟 的計算首先計算 z 值第二計算 a 值 在這個神經網路裡這一疊的 節點會相對於一個 z 像這樣 計算跟一個 a 如此計算而那一個 節點會相對於另一個 z 跟 另一個 a 的計算所以 我們以後用的記號會 像這樣 首先輸入特徵 X 跟一些參數 w 跟 b 這會讓您計算 z[1] 新引進一個記號是 我們用上標方括號 1 來標示這個量關聯於 這一疊的節點稱為層 稍後我們會用上標方 括號 2 來標示量 關聯於那個節點也是 稱為這個網路的另一層 這個這裡的上標方括號 不要跟 上標圓括號搞混了，那是 我們用來標示個別訓練 例子所以 x 上標圓括號 i 指的是 第 i 個訓練例子上標方 括號指的是 不同的層, 層 1, 層 2 在這個網路, 我們繼續 計算完 z[1] 後類似羅吉斯 迴歸分析會有一個計算 來計算 a[1] 而那只是 S形函數 of z[1], 然後您計算 z[2] 使用另一個線性方程式, 然後 計算 a[2], 而 a[2] 是最終輸出 對於這個神經網路, 也會 跟 y-hat 通用, 我 知道有很多的細節但是 重要的直觀是 對於羅吉斯迴歸分析我們有 這個 z 跟 a 的計算 這個神經網路我們只是 重複做很多次, z 然後跟著 a 計算，然後 z 接著 a 計算然後最後計算 最後的損失而您記得 對於羅吉斯迴歸分析我們有 一些反向計算為了 計算導數, 像是計算 da, dz 等等同樣的 在神經網路我們也會做 反向計算像是 這樣您會計算 da[2] dz[2], 讓您計算 dw[2] db[2] 等等用這種從右 到左反向計算, 也就是 用紅色箭頭表示的 這給您 快速瀏覽一個神經網路 的樣子, 基本上您拿羅吉斯 迴歸分析重複做兩次, 我知道 有很多的新的記號很多 細節不用擔心您 跟不上我們會進入 細節在下面的影片用最慢的速度 讓我們進入下一段影片 我們將開始談到有關神經 網路的表達方式