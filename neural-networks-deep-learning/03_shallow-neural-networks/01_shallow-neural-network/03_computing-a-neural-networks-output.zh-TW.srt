1
00:00:00,006 --> 00:00:04,743
上一段影片您看到了一個單一
隱藏層神經網路的樣子

2
00:00:04,743 --> 00:00:08,175
這段影片讓我們更進一步
談論細節, 真的如何讓這個

3
00:00:08,175 --> 00:00:10,361
神經網路計算它的輸出

4
00:00:10,361 --> 00:00:15,533
您將看到的會是像羅吉斯
迴歸分析， 只是重複很多次

5
00:00:15,533 --> 00:00:16,423
讓我們看看讓我們看看

6
00:00:16,423 --> 00:00:19,364
這是一個兩層
神經網路

7
00:00:19,364 --> 00:00:23,973
讓我們更進一步看
這個神經網路計算什麼

8
00:00:23,973 --> 00:00:26,653
我們之前談過
羅吉斯迴歸分析

9
00:00:26,653 --> 00:00:31,035
在羅吉斯迴歸分析中的一個圓圈, 實際
代表計算的兩個步驟

10
00:00:31,035 --> 00:00:34,498
首先您計算 z 像這樣
再來

11
00:00:34,498 --> 00:00:37,754
您計算這個啟動為
S型函數 of z

12
00:00:37,754 --> 00:00:40,536
神經網路只是
重複做這樣很多次

13
00:00:40,536 --> 00:00:43,953
讓我們專注於從隱藏層一個
節點開始

14
00:00:43,953 --> 00:00:46,320
讓我們看在隱藏層第一個
節點

15
00:00:46,320 --> 00:00:48,079
我先將其他節點變成灰色

16
00:00:48,079 --> 00:00:50,820
類似在左邊的羅吉斯
迴歸分析

17
00:00:50,820 --> 00:00:54,391
這個在隱藏層的節點
做這兩個步驟的計算

18
00:00:54,391 --> 00:00:58,418
第一步, 您想成是
這個節點的左半部

19
00:00:58,418 --> 00:01:02,754
它計算 z = w轉置x + b
而我們使用的符號是

20
00:01:02,754 --> 00:01:08,253
這些量都是相關於
第一個隱藏層, 所以這是為什麼我們有

21
00:01:08,253 --> 00:01:13,458
一堆方括號在這裡, 這個
是在隱藏層第一個節點

22
00:01:13,458 --> 00:01:16,597
所以這是為什麼在這裡我們有
下標 1 

23
00:01:16,597 --> 00:01:18,424
所以先做這個

24
00:01:18,424 --> 00:01:24,419
然後第二個步驟是計算
a[1], 1 = sigmoid of z[1],1 像這樣

25
00:01:24,419 --> 00:01:29,013
對於 z 跟 a , 約定
記號為 a[l], i

26
00:01:29,013 --> 00:01:33,770
l 是上標方
括號是您的第幾層數字

27
00:01:33,770 --> 00:01:37,720
而下標 i 指的是在這一層第幾個節點

28
00:01:37,720 --> 00:01:42,344
所以我們正在看的節點是
第一層, 也就是隱藏層, 第一個節點

29
00:01:42,344 --> 00:01:45,878
這是為什麼上標跟
下標都是 1, 1

30
00:01:45,878 --> 00:01:49,965
所以這個小圈圈, 第一個
節點在神經網路代表著

31
00:01:49,965 --> 00:01:52,579
進行這兩個
步驟的計算

32
00:01:52,579 --> 00:01:58,399
現在，讓我們看第二個節點
在這個神經網路的隱藏層

33
00:01:58,399 --> 00:02:01,482
類似於在左邊的
羅吉斯迴歸分析

34
00:02:01,482 --> 00:02:04,781
下面這個小圓圈代表
兩個計算步驟

35
00:02:04,781 --> 00:02:08,733
第一個步驟計算 z
這還是在第一層, 但是

36
00:02:08,733 --> 00:02:12,996
現在是第二個節點
等於 w[1], 2轉置X + b[1], 2, 
而

37
00:02:12,996 --> 00:02:17,880
a[1], 2 = sigmoid of z[1], 2
再次提醒， 隨時

38
00:02:17,880 --> 00:02:23,071
暫停影片如果您覺得需要的話, 但您可以
仔細檢查上標跟

39
00:02:23,071 --> 00:02:28,453
下標符號是跟
上面用紫色寫的公式一致的

40
00:02:28,453 --> 00:02:32,831
我們已經談過前面兩個
隱藏單元在神經網路

41
00:02:32,831 --> 00:02:36,940
隱藏單元 3 跟 
4 代表類似的計算

42
00:02:36,940 --> 00:02:39,778
現在
讓我拿著這對方程式

43
00:02:39,778 --> 00:02:44,169
跟這部分的方程式
複製它們到下一張投影片

44
00:02:44,169 --> 00:02:48,921
這是我們的神經網路
這是第一個，這是第二個

45
00:02:48,921 --> 00:02:54,050
方程式我們剛剛前面做的
對於第一個跟第二個隱藏層

46
00:02:54,050 --> 00:02:59,022
如果您用同樣方式寫下
對應方程式對於第三跟

47
00:02:59,022 --> 00:03:02,093
第四隱藏單元
您會得到如下

48
00:03:02,093 --> 00:03:06,550
只是要確認這些符號是您已理解的
這是向量 w[1], 1

49
00:03:06,550 --> 00:03:09,430
這是一個向量
轉置乘 X, 好吧

50
00:03:09,430 --> 00:03:13,460
這是上標 Ｔ 在這裡的意思
代表著是向量的轉置

51
00:03:13,460 --> 00:03:17,585
也許您已經猜到, 如果您
實際建置一個神經網路

52
00:03:17,585 --> 00:03:20,209
用一個迴圈作這些
似乎很無效率

53
00:03:20,209 --> 00:03:25,174
我們將拿這
四個方程式做向量化

54
00:03:25,174 --> 00:03:29,348
首先開始從如何
計算 z 用向量的方式而其實

55
00:03:29,348 --> 00:03:30,859
您可以這樣做

56
00:03:30,859 --> 00:03:34,843
讓我們拿這些 w 
將他們疊成一個矩陣

57
00:03:34,843 --> 00:03:38,767
您有w[1], 1 轉置
所以是一個行向量, 或者說

58
00:03:38,767 --> 00:03:42,231
這是一個列向量
因為轉置給您一個行向量

59
00:03:42,231 --> 00:03:48,494
然後 w[1], 2 轉置, w[1],
3 轉置, w[1], 4 轉置

60
00:03:48,494 --> 00:03:54,499
將這四個 w 向量疊起來
您會得到一個矩陣

61
00:03:54,499 --> 00:03:59,204
另一種理解的方式是, 我們
這裡有羅吉斯迴歸分析單元

62
00:03:59,204 --> 00:04:03,913
而每一個羅吉斯迴歸分析單元
有相對應的參數向量 w

63
00:04:03,913 --> 00:04:06,535
而將這
四個向量疊在一起

64
00:04:06,535 --> 00:04:08,842
您會得到這個 (4, 3) 矩陣

65
00:04:08,842 --> 00:04:14,281
如果您拿這個矩陣
乘上您的輸入特徵

66
00:04:14,281 --> 00:04:19,806
x1, x2, x3 您會得到
使用矩陣乘法

67
00:04:19,806 --> 00:04:24,546
您會得到 w1, [1]轉置 x,
w2, [1]轉置 X

68
00:04:24,546 --> 00:04:30,995
w3, [1]轉置 x, w4,[1]轉置 x 
然後不要忘記 b

69
00:04:30,995 --> 00:04:35,997
我們加上這個向量 b[1], 1
b[1], 2

70
00:04:35,997 --> 00:04:40,811
b[1], 3, b[1], 4 基本上是這個

71
00:04:40,811 --> 00:04:45,654
然後這個是 b[1],1, b[1], 2, b[1], 3, b[1], 4

72
00:04:45,654 --> 00:04:50,579
您看到結果這四行的每一行
相對應於正好是

73
00:04:50,579 --> 00:04:55,772
這四行
每一個都是我們上面的方程式

74
00:04:55,772 --> 00:05:00,899
換句話說, 我們剛剛證明了
這個東西等於 z[1],

75
00:05:00,899 --> 00:05:05,303
1, z[1], 2, z[1], 3, z[1], 4 
像這裡定義的

76
00:05:05,303 --> 00:05:10,289
也許不意外的我們將
這些稱為向量 z[1]

77
00:05:10,289 --> 00:05:15,097
也就是把這些
每一個 z 疊起來變成一個列向量

78
00:05:15,097 --> 00:05:19,524
當我們做向量化, 一個準則
來幫助您引導這些

79
00:05:19,524 --> 00:05:23,966
是當您有不同的節點
在這一層時, 我們將它們垂直疊起來

80
00:05:23,966 --> 00:05:27,656
這是為什麼
當您有 z[1],1, 到 z[1],4

81
00:05:27,656 --> 00:05:31,852
這相對於四個不同的
節點在隱藏層

82
00:05:31,852 --> 00:05:36,481
我們將這四個數字
垂直疊起來變成向量 z[1]

83
00:05:36,481 --> 00:05:40,457
再多用一個符號
這個 (4,3) 矩陣

84
00:05:40,457 --> 00:05:45,233
我們用
小寫的 w[1],1, w[1],2 等等疊起來的

85
00:05:45,233 --> 00:05:49,860
我們稱這個為矩陣
 W[1], 同樣的這個向量

86
00:05:49,860 --> 00:05:54,623
我們稱這個為 B 上標[1]
所以這是一個 (4,1) 向量

87
00:05:54,623 --> 00:05:59,584
現在, 我們使用
這個向量矩陣的符號來計算 Z

88
00:05:59,584 --> 00:06:03,535
最後一件事是我們也需要
計算這些 a 的值

89
00:06:03,535 --> 00:06:08,195
或許不意外的您看到
我們將定義 a[1] 為

90
00:06:08,195 --> 00:06:13,019
將這些啟動
值a[1], 1, 直到 a[1], 4 疊起來

91
00:06:13,019 --> 00:06:18,202
就是拿這四個值
疊起來成為一個向量稱為 a[1]

92
00:06:18,202 --> 00:06:21,122
這將會是 sigmoid of z[1]

93
00:06:21,122 --> 00:06:25,794
而現在這是
這個 S型函數 的建置也就是拿

94
00:06:25,794 --> 00:06:30,761
z 的四個值然後應用
這個S型函數逐元素作用在它們上

95
00:06:30,761 --> 00:06:36,750
回顧一下，我們
找出 z[1] = W[1]

96
00:06:36,750 --> 00:06:41,883
X + b[1], 而 a[1] 是 sigmoid of z[1]

97
00:06:41,883 --> 00:06:47,321
讓我們複製這寫到下一張投影片
我們看到的是

98
00:06:47,321 --> 00:06:52,156
對於神經網路的第一層
給予輸入 x

99
00:06:52,156 --> 00:06:56,286
我們有 z[1] = W[1]x + b[1]

100
00:06:56,286 --> 00:07:01,526
a[1] = sigmoid(z[1])
而這個維度

101
00:07:01,526 --> 00:07:06,563
是 (4,1) = (4,3) x (3,1) + (4,1)

102
00:07:06,563 --> 00:07:11,297
這是(4,1) 
一樣的維度

103
00:07:11,297 --> 00:07:16,793
記得我們說 x = a[0]
且 y-hat = a[2]

104
00:07:16,793 --> 00:07:21,560
如果您要的話, 您可以將這個
x 取代為 a[0], 因為 a[0]

105
00:07:21,560 --> 00:07:25,417
如果您要的話, 只是
輸入特徵 x 的別名

106
00:07:25,417 --> 00:07:30,968
用類似的推導, 您可以
找出

107
00:07:30,968 --> 00:07:35,972
下一層的表示法可以寫成
類似的, 而輸出層做的是

108
00:07:35,972 --> 00:07:40,770
它也是有
對應的參數 w[2] 跟 b[2]

109
00:07:40,770 --> 00:07:44,549
w[2] 在這種情況下
是 (1,4)矩陣而

110
00:07:44,549 --> 00:07:47,529
b[2] 就只是一個實數 (1,1)

111
00:07:47,529 --> 00:07:51,982
z[2] 會是一個實數
寫成 (1,1) 矩陣

112
00:07:51,982 --> 00:07:57,267
這會是一個 (1,4) 
乘這個是 (4,1) 加上 b 是 (1,1)

113
00:07:57,267 --> 00:08:02,397
所以這給您一個實數
如果您將最後這個輸出單元想成

114
00:08:02,397 --> 00:08:07,787
就只是一個類似羅吉斯
迴歸分析，有參數 w 跟 b

115
00:08:07,787 --> 00:08:12,517
W 扮演類似
w[2]轉置的角色, 或者說

116
00:08:12,517 --> 00:08:16,675
w2 實際是 w 轉置
b 等於 b[2]

117
00:08:16,675 --> 00:08:21,665
如果您將左邊
這個網路蓋起來, 先忽略他們

118
00:08:21,665 --> 00:08:26,434
這個最後的輸出單元
就像是羅吉斯迴歸分析

119
00:08:26,434 --> 00:08:30,010
除了與其寫成
參數 w 跟 b

120
00:08:30,010 --> 00:08:35,784
我們寫成 w[2] 跟 b[2]
維度是 (1,4) 跟 (1,1)

121
00:08:35,784 --> 00:08:39,765
回顧一下
對於羅吉斯迴歸分析

122
00:08:39,765 --> 00:08:44,620
要實現輸出, 或者
實現預估

123
00:08:44,620 --> 00:08:51,143
您會計算 z = w轉置
x + b, 而 y-hat = a = sigmoid of (z)

124
00:08:51,143 --> 00:08:55,499
當您有一個神經網路單元
有一個隱藏層,您需要

125
00:08:55,499 --> 00:09:00,131
執行計算這個輸出只要
這四個方程式，而您可以想像

126
00:09:00,131 --> 00:09:04,902
這是向量化建置來
先計算這四個

127
00:09:04,902 --> 00:09:09,329
羅吉斯迴歸分析單元在隱藏
層, 這是這些做的

128
00:09:09,329 --> 00:09:13,867
然後這是
輸出層的羅吉斯迴歸分析, 以就是這些做的

129
00:09:13,867 --> 00:09:18,401
我希望這些描述是合理的
重點是計算

130
00:09:18,401 --> 00:09:22,001
神經網路的輸出
您需要的是這四行程式

131
00:09:22,001 --> 00:09:25,706
現在您看到
給予一個輸入特徵 x

132
00:09:25,706 --> 00:09:30,278
您如何用四行程式來
計算神經網路的輸出

133
00:09:30,278 --> 00:09:34,575
類似我們在
羅吉斯迴歸分析中做的,我們也想要向量化到

134
00:09:34,575 --> 00:09:39,002
我們全部的訓練例子
我們將會看到, 將這三個例子疊起來

135
00:09:39,002 --> 00:09:43,653
在矩陣不同的列上, 
只要一點點修正

136
00:09:43,653 --> 00:09:47,396
您會見到, 類似於
您在羅吉斯迴歸分析看到的

137
00:09:47,396 --> 00:09:50,514
能夠計算
這個神經網路的輸出

138
00:09:50,514 --> 00:09:55,114
不只是一次一個例子, 而是
同時整個訓練集

139
00:09:55,114 --> 00:09:57,939
讓我們在下一段影片看這些細節