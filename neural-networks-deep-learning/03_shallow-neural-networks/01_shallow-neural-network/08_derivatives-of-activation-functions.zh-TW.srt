1
00:00:00,302 --> 00:00:04,340
當您建置您的反向傳播
對於您的神經網路時, 您需要能夠

2
00:00:04,340 --> 00:00:07,813
計算斜率或者說導數
對於這些啟動函數

3
00:00:07,813 --> 00:00:11,333
所以讓我們看看我們的
啟動函數選擇

4
00:00:11,333 --> 00:00:14,025
您如何可以計算
這些函數的斜率

5
00:00:14,025 --> 00:00:17,239
這是您熟悉的S型函數
啟動函數

6
00:00:17,239 --> 00:00:22,252
所以給任何一個z值
也許是這個z值, 這個函數

7
00:00:22,252 --> 00:00:27,184
會有一些斜率, 或者導數
相對於, 如果您畫

8
00:00:27,184 --> 00:00:32,389
一小條線在這裡, 這個高除以
寬在這個小三角形

9
00:00:32,389 --> 00:00:39,823
所以如果g(z) 是S型函數, 那
這個函數的斜率是 d/dz g(z)

10
00:00:39,823 --> 00:00:47,034
從微積分我們知道
這是 g(z) 在 z 的斜率

11
00:00:47,034 --> 00:00:52,513
如果您熟悉微積分
知道如何求導數

12
00:00:52,513 --> 00:00:56,074
如果您求
S狀函數的導數

13
00:00:56,074 --> 00:01:00,300
這可以證明
是這個公式

14
00:01:00,300 --> 00:01:02,380
再次說明
我不做這些微積分的步驟

15
00:01:02,380 --> 00:01:04,839
但如果您熟悉微積分

16
00:01:04,839 --> 00:01:08,465
歡迎隨時暫停影片
試著自己證明

17
00:01:08,465 --> 00:01:16,583
所以
這也是等於 g(z) 乘 (1 - g(z))

18
00:01:16,583 --> 00:01:20,923
讓我們檢查
這個公式是合理的

19
00:01:20,923 --> 00:01:26,343
首先, 如果 z 很大, 假設 z = 10

20
00:01:26,343 --> 00:01:30,315
g(z) 會趨近於 1, 所以

21
00:01:30,315 --> 00:01:38,758
這個公式告訴我們
d/dz g(z) 趨近於 g(z) (1 - g(z))

22
00:01:38,758 --> 00:01:42,979
也就是等於 1 乘 (1 - 1)

23
00:01:42,979 --> 00:01:46,851
也就是趨近於 0

24
00:01:46,851 --> 00:01:51,267
而這是正確的, 因為當 z 
很大時斜率會趨近於 0

25
00:01:51,267 --> 00:01:58,885
相反地, 如果 z = -10, 
同樣的方式, g(z) 趨近於 0

26
00:01:58,885 --> 00:02:01,765
所以在左邊的公式告訴我們

27
00:02:01,765 --> 00:02:07,333
d/dz g(z) 會趨近於 g(z)
也就是 0 乘 (1 - 0)

28
00:02:07,333 --> 00:02:10,005
也是趨近於 0
這個也是正確的

29
00:02:10,005 --> 00:02:16,889
最後如果 z = 0 那 g(z) = 1/2

30
00:02:16,889 --> 00:02:18,983
就是用這個S形函數

31
00:02:18,983 --> 00:02:24,040
所以導數
等於 1/2 乘 (1 -

32
00:02:24,040 --> 00:02:28,459
1/2), 等於 1/4

33
00:02:28,459 --> 00:02:33,135
而這個實際上
是正確的導數或者說

34
00:02:33,135 --> 00:02:35,831
這個函數的斜率當 z = 0時

35
00:02:35,831 --> 00:02:38,839
最後, 再介紹
 一個符號

36
00:02:38,839 --> 00:02:42,321
有時候, 與其用這個項目

37
00:02:42,321 --> 00:02:46,087
導數的簡寫
是 g 一撇 of z

38
00:02:46,087 --> 00:02:52,167
所以 g 一撇 of z 在微積分
這在上面的這一撇稱 prime

39
00:02:52,167 --> 00:02:55,283
g prime of z 
是微積分的簡寫, 對於

40
00:02:55,283 --> 00:03:00,455
g 函數的導數
相對應於輸入變數 z

41
00:03:00,455 --> 00:03:07,513
在神經網路, 我們有 a = g(z)

42
00:03:07,513 --> 00:03:11,351
等於這個

43
00:03:11,351 --> 00:03:17,145
這個公式也可以
簡化成為 a 乘上 (1 - a)

44
00:03:17,145 --> 00:03:20,109
有時候在建置時

45
00:03:20,109 --> 00:03:25,256
您或許會看到
g prime of z 等於 a 乘上 (1 - a)

46
00:03:25,256 --> 00:03:29,040
而這指的是
觀察 g prime

47
00:03:29,040 --> 00:03:33,428
意思是導數
等於這一個項目

48
00:03:33,428 --> 00:03:38,397
這個公式的好處是，如果你已經計算過的

49
00:03:38,397 --> 00:03:43,078
a 值, 使用這個公式
您可以很快地計算

50
00:03:43,078 --> 00:03:45,038
g prime 斜率的值, 

51
00:03:45,038 --> 00:03:47,632
好的
這是有關於 S型啟動函數

52
00:03:47,632 --> 00:03:51,172
讓我們看 tanh
啟動函數

53
00:03:51,172 --> 00:03:56,311
類似於我們前面做的
d/dz g(z) 的定義

54
00:03:56,311 --> 00:04:01,155
是  g(z) 的斜率
在一個特定點 z 上

55
00:04:01,155 --> 00:04:07,747
如果您看這個
雙曲正切函數公式

56
00:04:07,747 --> 00:04:13,066
如果您熟微積分
您可以求導數

57
00:04:13,066 --> 00:04:17,359
證明可以簡化成這個公式

58
00:04:20,813 --> 00:04:23,997
像之前我們用的簡寫

59
00:04:23,997 --> 00:04:27,019
我們稱這個為
g prime of z

60
00:04:27,019 --> 00:04:30,941
如果您要的話, 您可以檢查
這個公式是合理的

61
00:04:30,941 --> 00:04:37,005
舉個例子如果z=10
tanh(z) 趨近於 1

62
00:04:37,005 --> 00:04:41,309
這個從+1 到-1

63
00:04:41,309 --> 00:04:45,183
而 g prime of z 
根據這個公式

64
00:04:45,183 --> 00:04:48,148
會大約是 1 - 1 的平方，所以是 0

65
00:04:48,148 --> 00:04:53,930
所以當 z 很大時
斜率趨近於 0

66
00:04:53,930 --> 00:04:58,760
相反地, 如果 z 很小, 假設 z = -10

67
00:04:58,760 --> 00:05:02,440
那 tanh(z) 會趨近於 -1

68
00:05:02,440 --> 00:05:07,871
所以 g prime of z 會趨近於
1 減 -1 的平方

69
00:05:07,871 --> 00:05:12,792
所以趨近於 1 - 1
也是趨近於 0

70
00:05:12,792 --> 00:05:18,421
最後, 如果 z = 0, 那 tanh(z) = 0

71
00:05:18,421 --> 00:05:22,165
那這個斜率等於 1

72
00:05:22,165 --> 00:05:26,433
也正是
當 z 等於 0 時的斜率

73
00:05:26,433 --> 00:05:33,001
總結一下, 如果 a=g(z) 
也就是如果 a 等於 tanh(z)

74
00:05:33,001 --> 00:05:38,300
這個導數 g prime 
of z 等於 1 - a 的平方

75
00:05:38,300 --> 00:05:42,000
再一次
如果您已經計算 a 的值

76
00:05:42,000 --> 00:05:46,522
您可以用這個公式很
快的計算導數

77
00:05:46,522 --> 00:05:49,618
最後這是我們如何計算
ReLU 跟

78
00:05:49,618 --> 00:05:51,338
Leaky ReLU 啟動函數的導數

79
00:05:51,338 --> 00:05:57,866
對於 ReLU, g(z) 等於 max(0, z)

80
00:05:57,866 --> 00:06:03,330
所以導數是等於 0

81
00:06:03,330 --> 00:06:09,066
如果 z 小於 0
1, 如果 z 大於 0

82
00:06:09,066 --> 00:06:15,498
實際上 a 是未定義
如果 z 剛好為 0

83
00:06:15,498 --> 00:06:18,339
但如果您在軟體中建置

84
00:06:18,339 --> 00:06:21,398
也許不是百分之百
正確的數學

85
00:06:21,398 --> 00:06:25,571
但還是可以運作
如果 z 真的是 0

86
00:06:25,571 --> 00:06:30,501
如果您設導數
等於 1, 或者設為 0

87
00:06:30,501 --> 00:06:31,781
其實無所謂

88
00:06:31,781 --> 00:06:33,578
如果你 [聽不清] 優化

89
00:06:33,578 --> 00:06:37,109
技術上而言 g prime 變成
稱為子梯度

90
00:06:37,109 --> 00:06:41,360
在啟動函數 g(z)
這是為什麼梯度下降仍然可行

91
00:06:41,360 --> 00:06:47,327
但您可以想像成
z 是 0.000000, 它太

92
00:06:47,327 --> 00:06:52,805
小, 小到幾乎無關緊要
不管您設導數為何

93
00:06:52,805 --> 00:06:54,303
當 z 為 0 時

94
00:06:54,303 --> 00:06:59,155
在實踐上, 這是人們
用來建置 z 的導數

95
00:06:59,155 --> 00:07:03,712
最後
如果您訓練神經網路使用

96
00:07:03,712 --> 00:07:06,882
Leaky ReLU 啟動函數

97
00:07:06,882 --> 00:07:12,244
g(z) 會是 max of 
(0.001z, z)

98
00:07:12,244 --> 00:07:15,247
所以 g prime of z 等於 0.001

99
00:07:15,247 --> 00:07:19,074
如果 z 小於 0, 等於 1
如果 z 大於 0

100
00:07:19,074 --> 00:07:20,477
再一次

101
00:07:20,477 --> 00:07:26,403
技術上而言這個梯度是未定義
當 z 剛好為 0 時

102
00:07:26,403 --> 00:07:31,479
但如果您建置一些
程式, 設這個導數或者說

103
00:07:31,479 --> 00:07:38,353
設 g prime 為 0.001 或者1 
無論哪種方式都沒差

104
00:07:38,353 --> 00:07:41,499
當
 z 真的是 0 時您的
程式還是可行

105
00:07:41,499 --> 00:07:45,594
所以，帶著這些公式，您應該能夠計算斜率，或

106
00:07:45,594 --> 00:07:48,400
導數
對於您的啟動函數

107
00:07:48,400 --> 00:07:50,553
現在您有了這些建構基石

108
00:07:50,553 --> 00:07:54,502
準備好去看到如何實現
梯度下降神經網路

109
00:07:54,502 --> 00:07:57,309
我們下一段影片見