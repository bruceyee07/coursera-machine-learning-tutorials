上一段影片您看到了一個單一
隱藏層神經網路的樣子 這段影片讓我們更進一步
談論細節, 真的如何讓這個 神經網路計算它的輸出 您將看到的會是像羅吉斯
迴歸分析， 只是重複很多次 讓我們看看讓我們看看 這是一個兩層
神經網路 讓我們更進一步看
這個神經網路計算什麼 我們之前談過
羅吉斯迴歸分析 在羅吉斯迴歸分析中的一個圓圈, 實際
代表計算的兩個步驟 首先您計算 z 像這樣
再來 您計算這個啟動為
S型函數 of z 神經網路只是
重複做這樣很多次 讓我們專注於從隱藏層一個
節點開始 讓我們看在隱藏層第一個
節點 我先將其他節點變成灰色 類似在左邊的羅吉斯
迴歸分析 這個在隱藏層的節點
做這兩個步驟的計算 第一步, 您想成是
這個節點的左半部 它計算 z = w轉置x + b
而我們使用的符號是 這些量都是相關於
第一個隱藏層, 所以這是為什麼我們有 一堆方括號在這裡, 這個
是在隱藏層第一個節點 所以這是為什麼在這裡我們有
下標 1 所以先做這個 然後第二個步驟是計算
a[1], 1 = sigmoid of z[1],1 像這樣 對於 z 跟 a , 約定
記號為 a[l], i l 是上標方
括號是您的第幾層數字 而下標 i 指的是在這一層第幾個節點 所以我們正在看的節點是
第一層, 也就是隱藏層, 第一個節點 這是為什麼上標跟
下標都是 1, 1 所以這個小圈圈, 第一個
節點在神經網路代表著 進行這兩個
步驟的計算 現在，讓我們看第二個節點
在這個神經網路的隱藏層 類似於在左邊的
羅吉斯迴歸分析 下面這個小圓圈代表
兩個計算步驟 第一個步驟計算 z
這還是在第一層, 但是 現在是第二個節點
等於 w[1], 2轉置X + b[1], 2, 
而 a[1], 2 = sigmoid of z[1], 2
再次提醒， 隨時 暫停影片如果您覺得需要的話, 但您可以
仔細檢查上標跟 下標符號是跟
上面用紫色寫的公式一致的 我們已經談過前面兩個
隱藏單元在神經網路 隱藏單元 3 跟 
4 代表類似的計算 現在
讓我拿著這對方程式 跟這部分的方程式
複製它們到下一張投影片 這是我們的神經網路
這是第一個，這是第二個 方程式我們剛剛前面做的
對於第一個跟第二個隱藏層 如果您用同樣方式寫下
對應方程式對於第三跟 第四隱藏單元
您會得到如下 只是要確認這些符號是您已理解的
這是向量 w[1], 1 這是一個向量
轉置乘 X, 好吧 這是上標 Ｔ 在這裡的意思
代表著是向量的轉置 也許您已經猜到, 如果您
實際建置一個神經網路 用一個迴圈作這些
似乎很無效率 我們將拿這
四個方程式做向量化 首先開始從如何
計算 z 用向量的方式而其實 您可以這樣做 讓我們拿這些 w 
將他們疊成一個矩陣 您有w[1], 1 轉置
所以是一個行向量, 或者說 這是一個列向量
因為轉置給您一個行向量 然後 w[1], 2 轉置, w[1],
3 轉置, w[1], 4 轉置 將這四個 w 向量疊起來
您會得到一個矩陣 另一種理解的方式是, 我們
這裡有羅吉斯迴歸分析單元 而每一個羅吉斯迴歸分析單元
有相對應的參數向量 w 而將這
四個向量疊在一起 您會得到這個 (4, 3) 矩陣 如果您拿這個矩陣
乘上您的輸入特徵 x1, x2, x3 您會得到
使用矩陣乘法 您會得到 w1, [1]轉置 x,
w2, [1]轉置 X w3, [1]轉置 x, w4,[1]轉置 x 
然後不要忘記 b 我們加上這個向量 b[1], 1
b[1], 2 b[1], 3, b[1], 4 基本上是這個 然後這個是 b[1],1, b[1], 2, b[1], 3, b[1], 4 您看到結果這四行的每一行
相對應於正好是 這四行
每一個都是我們上面的方程式 換句話說, 我們剛剛證明了
這個東西等於 z[1], 1, z[1], 2, z[1], 3, z[1], 4 
像這裡定義的 也許不意外的我們將
這些稱為向量 z[1] 也就是把這些
每一個 z 疊起來變成一個列向量 當我們做向量化, 一個準則
來幫助您引導這些 是當您有不同的節點
在這一層時, 我們將它們垂直疊起來 這是為什麼
當您有 z[1],1, 到 z[1],4 這相對於四個不同的
節點在隱藏層 我們將這四個數字
垂直疊起來變成向量 z[1] 再多用一個符號
這個 (4,3) 矩陣 我們用
小寫的 w[1],1, w[1],2 等等疊起來的 我們稱這個為矩陣
 W[1], 同樣的這個向量 我們稱這個為 B 上標[1]
所以這是一個 (4,1) 向量 現在, 我們使用
這個向量矩陣的符號來計算 Z 最後一件事是我們也需要
計算這些 a 的值 或許不意外的您看到
我們將定義 a[1] 為 將這些啟動
值a[1], 1, 直到 a[1], 4 疊起來 就是拿這四個值
疊起來成為一個向量稱為 a[1] 這將會是 sigmoid of z[1] 而現在這是
這個 S型函數 的建置也就是拿 z 的四個值然後應用
這個S型函數逐元素作用在它們上 回顧一下，我們
找出 z[1] = W[1] X + b[1], 而 a[1] 是 sigmoid of z[1] 讓我們複製這寫到下一張投影片
我們看到的是 對於神經網路的第一層
給予輸入 x 我們有 z[1] = W[1]x + b[1] a[1] = sigmoid(z[1])
而這個維度 是 (4,1) = (4,3) x (3,1) + (4,1) 這是(4,1) 
一樣的維度 記得我們說 x = a[0]
且 y-hat = a[2] 如果您要的話, 您可以將這個
x 取代為 a[0], 因為 a[0] 如果您要的話, 只是
輸入特徵 x 的別名 用類似的推導, 您可以
找出 下一層的表示法可以寫成
類似的, 而輸出層做的是 它也是有
對應的參數 w[2] 跟 b[2] w[2] 在這種情況下
是 (1,4)矩陣而 b[2] 就只是一個實數 (1,1) z[2] 會是一個實數
寫成 (1,1) 矩陣 這會是一個 (1,4) 
乘這個是 (4,1) 加上 b 是 (1,1) 所以這給您一個實數
如果您將最後這個輸出單元想成 就只是一個類似羅吉斯
迴歸分析，有參數 w 跟 b W 扮演類似
w[2]轉置的角色, 或者說 w2 實際是 w 轉置
b 等於 b[2] 如果您將左邊
這個網路蓋起來, 先忽略他們 這個最後的輸出單元
就像是羅吉斯迴歸分析 除了與其寫成
參數 w 跟 b 我們寫成 w[2] 跟 b[2]
維度是 (1,4) 跟 (1,1) 回顧一下
對於羅吉斯迴歸分析 要實現輸出, 或者
實現預估 您會計算 z = w轉置
x + b, 而 y-hat = a = sigmoid of (z) 當您有一個神經網路單元
有一個隱藏層,您需要 執行計算這個輸出只要
這四個方程式，而您可以想像 這是向量化建置來
先計算這四個 羅吉斯迴歸分析單元在隱藏
層, 這是這些做的 然後這是
輸出層的羅吉斯迴歸分析, 以就是這些做的 我希望這些描述是合理的
重點是計算 神經網路的輸出
您需要的是這四行程式 現在您看到
給予一個輸入特徵 x 您如何用四行程式來
計算神經網路的輸出 類似我們在
羅吉斯迴歸分析中做的,我們也想要向量化到 我們全部的訓練例子
我們將會看到, 將這三個例子疊起來 在矩陣不同的列上, 
只要一點點修正 您會見到, 類似於
您在羅吉斯迴歸分析看到的 能夠計算
這個神經網路的輸出 不只是一次一個例子, 而是
同時整個訓練集 讓我們在下一段影片看這些細節