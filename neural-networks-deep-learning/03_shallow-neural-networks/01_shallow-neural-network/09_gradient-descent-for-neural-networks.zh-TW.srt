1
00:00:00,000 --> 00:00:03,840
我想這是令人興奮的影片

2
00:00:01,800 --> 00:00:06,240
在這段影片您將會看到如何去

3
00:00:03,840 --> 00:00:08,730
建置梯度下降對於您的

4
00:00:06,240 --> 00:00:10,530
神經網路使用一個隱藏層, 

5
00:00:08,730 --> 00:00:12,809
在這段影片, 我將只給您

6
00:00:10,530 --> 00:00:14,639
您建置時需要的方程式

7
00:00:12,809 --> 00:00:17,039
為了讓反向傳播的

8
00:00:14,639 --> 00:00:19,410
梯度下降可行然後在

9
00:00:17,039 --> 00:00:21,510
下一段影片我會給一些

10
00:00:19,410 --> 00:00:24,150
直觀也關於這個特別的

11
00:00:21,510 --> 00:00:26,070
方程式是準確的方程式或

12
00:00:24,150 --> 00:00:27,630
正確的方程式來計算

13
00:00:26,070 --> 00:00:28,289
您需要的梯度對於您的神經

14
00:00:27,630 --> 00:00:30,090
網路

15
00:00:28,289 --> 00:00:32,520
您的神經網路有一個

16
00:00:30,090 --> 00:00:39,930
單一隱藏層有

17
00:00:32,520 --> 00:00:44,760
參數 w1, b1, w2 跟 b2

18
00:00:39,930 --> 00:00:50,399
記得如果您有 nx, 或者說

19
00:00:44,760 --> 00:00:56,640
n0 輸入特徵跟 n1 隱藏

20
00:00:50,399 --> 00:00:59,149
單元跟 n2 輸出單元在我們

21
00:00:56,640 --> 00:01:05,670
例子, 目前為止 n2 等於 1

22
00:00:59,149 --> 00:01:08,880
這個矩陣 w1 會是 n1 乘 n0, b1

23
00:01:05,670 --> 00:01:11,250
會是 n1 維度向量

24
00:01:08,880 --> 00:01:13,350
您可以寫成 n1 乘 1

25
00:01:11,250 --> 00:01:16,500
維度矩陣實際上是列

26
00:01:13,350 --> 00:01:20,750
向量, w2 維度為 n2

27
00:01:16,500 --> 00:01:26,759
乘 n1 而 b2 維度會是

28
00:01:20,750 --> 00:01:28,590
n2 乘 1, 目前看到

29
00:01:26,759 --> 00:01:30,930
的例子都只是 n2 等於

30
00:01:28,590 --> 00:01:36,930
1 您只有一個

31
00:01:30,930 --> 00:01:39,570
隱藏單元, 您也會有成本

32
00:01:36,930 --> 00:01:41,340
函數對於神經網路, 而現在

33
00:01:39,570 --> 00:01:44,220
我假設您做的是

34
00:01:41,340 --> 00:01:48,659
二元分類問題, 所以在

35
00:01:44,220 --> 00:01:51,740
這情況下您參數的成本如下

36
00:01:48,659 --> 00:01:57,090
將會是 1/m of 

37
00:01:51,740 --> 00:01:59,969
平均對於損失函數, 所以 

38
00:01:57,090 --> 00:02:03,420
這裡的 L 是您神經網路的損失當

39
00:01:59,969 --> 00:02:06,240
預測 y-hat 這應該是 a2

40
00:02:03,420 --> 00:02:07,649
當真正的值是 y時

41
00:02:06,240 --> 00:02:09,629
如果您做二元

42
00:02:07,649 --> 00:02:12,510
分類時損失函數可以是

43
00:02:09,629 --> 00:02:15,030
您使用過的羅吉斯迴歸分析

44
00:02:12,510 --> 00:02:18,420
為了訓練參數, 您的

45
00:02:15,030 --> 00:02:21,450
演算法需要執行梯度

46
00:02:18,420 --> 00:02:23,189
下降法, 當訓練神經網路時

47
00:02:21,450 --> 00:02:25,379
重要的是初始這些

48
00:02:23,189 --> 00:02:26,129
參數為隨機的而不是都

49
00:02:25,379 --> 00:02:28,140
為 0

50
00:02:26,129 --> 00:02:30,030
我們稍後會解釋為什麼這樣

51
00:02:28,140 --> 00:02:32,069
經過初始這些參數為

52
00:02:30,030 --> 00:02:34,140
一些值每個迴圈的梯度下降

53
00:02:32,069 --> 00:02:36,780
會計算預測值

54
00:02:34,140 --> 00:02:42,359
您基本上會計算 y-hat

55
00:02:36,780 --> 00:02:44,519
i 從 1 到 m, 然後

56
00:02:42,359 --> 00:02:49,440
您需要計算導數

57
00:02:44,519 --> 00:02:51,420
您需要計算 dw1 , 那是

58
00:02:49,440 --> 00:02:54,359
成本函數的導數

59
00:02:51,420 --> 00:02:56,489
相對於參數 w1, 您

60
00:02:54,359 --> 00:02:59,220
需要計算另一個變數

61
00:02:56,489 --> 00:03:00,870
稱為 db1 也是

62
00:02:59,220 --> 00:03:04,109
成本函數的導數或者說斜率

63
00:03:00,870 --> 00:03:07,349
相對於變數 b1

64
00:03:04,109 --> 00:03:10,170
同樣的對於其它

65
00:03:07,349 --> 00:03:12,629
參數 w2 跟 b2, 最後

66
00:03:10,170 --> 00:03:17,879
梯度下降更新會是

67
00:03:12,629 --> 00:03:22,709
更新 w1 為 w1 - alpha

68
00:03:17,879 --> 00:03:26,129
學習率 乘 dw1, b1 更新

69
00:03:22,709 --> 00:03:31,620
為 b1 減學習率

70
00:03:26,129 --> 00:03:34,739
乘 db1, 同樣的對於 w2 跟 b2

71
00:03:31,620 --> 00:03:36,299
有時後我用冒號等號

72
00:03:34,739 --> 00:03:38,489
也時候用等號, 兩種

73
00:03:36,299 --> 00:03:40,829
符號都可以, 所以這會是

74
00:03:38,489 --> 00:03:42,510
一次遞迭的梯度下降

75
00:03:40,829 --> 00:03:44,280
然後您可以重複幾次

76
00:03:42,510 --> 00:03:46,079
直到您的參數看起來

77
00:03:44,280 --> 00:03:48,150
已經收斂, 在前面的影片

78
00:03:46,079 --> 00:03:50,099
我們談到如何計算

79
00:03:48,150 --> 00:03:51,629
預估如何計算輸出

80
00:03:50,099 --> 00:03:54,060
我們看到如何用

81
00:03:51,629 --> 00:03:56,269
向量化來做這些, 所以關鍵是

82
00:03:54,060 --> 00:04:00,180
知道如何計算這些偏

83
00:03:56,269 --> 00:04:04,079
導數項目, dw1, db1 跟

84
00:04:00,180 --> 00:04:06,780
導數 dw2 跟 db2

85
00:04:04,079 --> 00:04:09,419
我要做的是只給您

86
00:04:06,780 --> 00:04:12,150
您需要的方程式為了計算

87
00:04:09,419 --> 00:04:14,699
這些導數而我會延遲到

88
00:04:12,150 --> 00:04:17,430
下一段影片, 那是一段可選的影片

89
00:04:14,699 --> 00:04:19,090
一個好的機會來檢查我們如何

90
00:04:17,430 --> 00:04:21,400
找出這些公式

91
00:04:19,090 --> 00:04:26,169
讓我總結一下這些

92
00:04:21,400 --> 00:04:33,250
正向傳播方程式, 您有

93
00:04:26,169 --> 00:04:37,900
z1 等於 w1 X + b1 然後

94
00:04:33,250 --> 00:04:41,680
a1 等於啟動函數在

95
00:04:37,900 --> 00:04:49,690
那一層, 應用逐元素到 z1

96
00:04:41,680 --> 00:04:53,530
z2 等於 w2 A1 + b2

97
00:04:49,690 --> 00:04:55,180
最後， 這些都是

98
00:04:53,530 --> 00:05:01,210
向量化在您所有的訓練集

99
00:04:55,180 --> 00:05:02,740
a2 等於 g2 of z2, 再次

100
00:05:01,210 --> 00:05:04,870
提醒我們假設您做的是二元分類

101
00:05:02,740 --> 00:05:06,610
這個啟動

102
00:05:04,870 --> 00:05:08,560
函數實在應該是S型函數

103
00:05:06,610 --> 00:05:11,080
我就直接寫在這裡

104
00:05:08,560 --> 00:05:13,870
所以這是正向傳播或者說

105
00:05:11,080 --> 00:05:15,729
左到右正向計算對於

106
00:05:13,870 --> 00:05:18,430
您的神經網路, 下一步讓我們計算

107
00:05:15,729 --> 00:05:24,750
導數, 所以這是反向

108
00:05:18,430 --> 00:05:30,750
傳播步驟它計算 dz2

109
00:05:24,750 --> 00:05:33,610
等於 a2 減真正 y 值

110
00:05:30,750 --> 00:05:36,580
提醒一下所有這些都是

111
00:05:33,610 --> 00:05:41,289
向量化對於所有例子, 所以矩陣 Y

112
00:05:36,580 --> 00:05:45,280
是 1 乘 m 矩陣列出所有

113
00:05:41,289 --> 00:05:51,370
您的 m 個例子用列水平疊起來

114
00:05:45,280 --> 00:05:55,330
原來 dw2 等於這個, 實際上

115
00:05:51,370 --> 00:05:58,870
這前面三個方程式是非常

116
00:05:55,330 --> 00:06:00,900
類似於梯度下降對於羅吉斯

117
00:05:58,870 --> 00:06:00,900
迴歸分析

118
00:06:00,960 --> 00:06:12,610
axis 等於 1, keepdims 等於

119
00:06:07,419 --> 00:06:15,580
True, 只是一點小細節, 這個 np

120
00:06:12,610 --> 00:06:18,070
.sum 是 Python Numpy 指令

121
00:06:15,580 --> 00:06:21,100
來總和經過矩陣的一個維度

122
00:06:18,070 --> 00:06:24,810
在這個例子是水平總和

123
00:06:21,100 --> 00:06:27,600
而 keepdims 做的是避免

124
00:06:24,810 --> 00:06:31,230
Python 輸出一種有趣的

125
00:06:27,600 --> 00:06:34,650
等級 1 陣列, 而這種陣列的維度

126
00:06:31,230 --> 00:06:37,010
是 (n,) 用 keepdims 

127
00:06:34,650 --> 00:06:41,280
等於 True, 這會保證

128
00:06:37,010 --> 00:06:44,580
Python 輸出 db2 是一個向量

129
00:06:41,280 --> 00:06:47,820
是 n 乘 1, 技術上而言這會是

130
00:06:44,580 --> 00:06:50,130
n2 乘 1 在這裡

131
00:06:47,820 --> 00:06:53,520
只是 1 乘 1 數字, 也許

132
00:06:50,130 --> 00:06:56,790
無關緊要, 但等一下我們會見到

133
00:06:53,520 --> 00:06:58,500
當它真的要緊時, 所以目前為止

134
00:06:56,790 --> 00:07:01,320
我們做得很類似羅吉斯

135
00:06:58,500 --> 00:07:03,919
迴歸分析, 但當您繼續計算

136
00:07:01,320 --> 00:07:14,370
跑反向傳播您

137
00:07:03,919 --> 00:07:19,380
會計算這個 dz2 乘上 g1 prime

138
00:07:14,370 --> 00:07:20,880
of z1, 所以這個 g1 prime 是

139
00:07:19,380 --> 00:07:22,919
導數對於任何您用的

140
00:07:20,880 --> 00:07:25,770
啟動函數使用在

141
00:07:22,919 --> 00:07:27,030
隱藏層而對於輸出層,我

142
00:07:25,770 --> 00:07:29,400
假設您做的是二元

143
00:07:27,030 --> 00:07:30,780
分類使用S型函數

144
00:07:29,400 --> 00:07:34,620
這已經放入

145
00:07:30,780 --> 00:07:39,090
dz2 的公式, 而這個相乘是

146
00:07:34,620 --> 00:07:43,050
逐元素相乘, 所以這裡這個

147
00:07:39,090 --> 00:07:46,950
會是 n1 乘 m 矩陣

148
00:07:43,050 --> 00:07:48,990
這個逐元素

149
00:07:46,950 --> 00:07:52,680
導數也將是

150
00:07:48,990 --> 00:07:54,720
n1 乘 m 矩陣，所以這個相乘

151
00:07:52,680 --> 00:07:59,669
是逐元素相乘對於這兩個

152
00:07:54,720 --> 00:08:08,490
矩陣, 最後 dw1 等於

153
00:07:59,669 --> 00:08:18,950
這個, db1 等於這個 np.

154
00:08:08,490 --> 00:08:21,900
sum dz1 axis 等於 1, keepdims 

155
00:08:18,950 --> 00:08:23,430
等於 True, 在

156
00:08:21,900 --> 00:08:27,210
前面 keepdims 也許

157
00:08:23,430 --> 00:08:28,590
比較無關緊要如果 n2 等於 1, 所以這只是

158
00:08:27,210 --> 00:08:35,729
1 乘 1 也就是一個實數

159
00:08:28,590 --> 00:08:38,370
這裡 db1 會是 n1 乘 1

160
00:08:35,729 --> 00:08:40,110
向量, 所以您要 Python np

161
00:08:38,370 --> 00:08:43,110
dot sum 輸出一種這個

162
00:08:40,110 --> 00:08:46,529
維度而不是有趣的等級1

163
00:08:43,110 --> 00:08:48,360
陣列那個維度可能會

164
00:08:46,529 --> 00:08:50,580
搞砸你以後一些的

165
00:08:48,360 --> 00:08:53,310
計算, 另一種方式是

166
00:08:50,580 --> 00:08:56,910
不用 keepdims 參數但

167
00:08:53,310 --> 00:09:00,060
明顯的呼叫 reshape函數, 來重塑

168
00:08:56,910 --> 00:09:04,400
np.sum 的輸出變成

169
00:09:00,060 --> 00:09:08,310
您希望 db 的這個維度, 所以

170
00:09:04,400 --> 00:09:11,339
這是正向傳播我想是

171
00:09:08,310 --> 00:09:14,310
四個方程式跟反向傳播我猜

172
00:09:11,339 --> 00:09:16,680
六個方程式, 我知道我只寫下

173
00:09:14,310 --> 00:09:18,870
這些方程式, 但在下一段

174
00:09:16,680 --> 00:09:22,050
可選影片讓我們看一些

175
00:09:18,870 --> 00:09:23,940
直觀的如何推導出者六個方程式

176
00:09:22,050 --> 00:09:25,830
在反向傳播演算法

177
00:09:23,940 --> 00:09:27,750
隨您的意願看不看

178
00:09:25,830 --> 00:09:30,000
下一段影片, 但不管如何如果您建置

179
00:09:27,750 --> 00:09:32,730
這些演算法您會有正確的

180
00:09:30,000 --> 00:09:34,650
正向跟反向傳播

181
00:09:32,730 --> 00:09:36,750
您能夠計算

182
00:09:34,650 --> 00:09:39,029
您需要的導數為了應用在

183
00:09:36,750 --> 00:09:41,520
梯度下降來學習參數

184
00:09:39,029 --> 00:09:43,680
在您的神經網路, 這是可能的

185
00:09:41,520 --> 00:09:45,209
建置這個演算法然後執行它

186
00:09:43,680 --> 00:09:47,130
而不深入瞭解這些

187
00:09:45,209 --> 00:09:50,520
微積分很多成功的

188
00:09:47,130 --> 00:09:52,320
深度學習人員已經這麼做, 但如果您

189
00:09:50,520 --> 00:09:54,180
要的話您可以看下一段影片

190
00:09:52,320 --> 00:09:56,580
來獲得更多的直觀有關於

191
00:09:54,180 --> 00:09:58,820
這些導數在這些

192
00:09:56,580 --> 00:09:58,820
方程式