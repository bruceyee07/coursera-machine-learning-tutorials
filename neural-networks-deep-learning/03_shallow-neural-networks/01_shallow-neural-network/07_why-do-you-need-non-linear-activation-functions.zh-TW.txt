為什麼神經網路需要非線性啟動函數呢？ 實際上為了使您的神經網路計算有趣的函數 您真的需要用非線性啟動函數 讓我們看看為什麼, 這是神經網路的正向傳播 我們為什麼不去掉這個? 去掉 g 函數而直接設 a1 等於 z1 或是說, 您可以說 g of z 等於 z, 對吧? 有時候稱為線性啟動函數 也許更好的名字會是 恆等啟動函數因為它的輸出會是永遠等於輸入 為了這個緣故 如果說 a2 等於 z2 事實上如果您這樣做 那這個模型會只是計算 y 或者 y-hat 用線性函數處理輸入特徵 x 拿前面兩個方程式 如果您用 a1 等於 z1 等於 w1 x + b, 然後讓 a2 等於 z2 等於 w2 a1 + b, 如果您拿 a1 的定義插入這個公式 您發現 a2 等於 w2 乘 w1 x + b1, 對吧? 這是 a1 + b2 所以這簡化了 w2, w1, x + w2 b1 + b2 所以這只是 讓我們稱這項為 w', b' 這只是等於 w'x + b' 如果您使用線性啟動函數 或者我們稱它為恆等啟動函數 那這神經網路只是將輸入用線性函數輸出 我們會在以後談深度網路 神經網路有著很多 很多層, 很多很多隱藏層 它會變成, 如果您使用線性啟動函數, 或者換句話說 如果您不用啟動函數那 不論您的神經網路有幾層 它只會計算線性啟動函數 所以您有隱藏層跟沒有一樣 有一些情況需要簡要的提及 如果您用 線性啟動函數這裡跟S型函數這裡 那這個模型不就是 標準羅吉斯迴歸分析沒有隱藏層 我不去證明它但您可以試著
自己去證明如果您想的話 但重要的是 一個線性隱藏層幾乎是沒有用處的 因為兩個線性函數的組合還是線性函數 除非您丟一個非線性去那裡 那您不會計算出 更有趣的函數即便您用更深入的網路 只有一個地方您或許會用到線性啟動函數 g of z 等於 z 如果您做機器學習在迴歸分析問題上 如果 y 是實數 舉個例子, 如果您試著預估房價 所以 y 不是 0 或 1, 是一個實數 房價從 0 開始到很貴的房子 我猜或許, 貴的房子要好幾百萬美金 不管如何, 有很多房價的資料在您的資料集 如果 y 用這些實數 也許這種情況下可以用線性啟動函數 您的輸出 y-hat 也是實數從負的無窮大到正的無窮大 但這些隱藏單元不應該用線性啟動函數 它們可以用 ReLU 或 tanh 或 Leaky ReLU 或者其他函數 您或許會用到線性啟動函數的地方會是在輸出層 但除了那裡 使用線性啟動函數在 隱藏層除了很特別的情況下 跟壓縮有關我們之前談過的 使用線性啟動函數是相當少的 當然, 如果您預測房價像是在 第一週影片中看到的是非負值 您可以使用 ReLU 啟動函數 您的輸出 y-hat 會大於等於 0 我希望給您一點感覺為什麼 非線性啟動函數在神經網路很重要 下一步, 我將開始談到梯度下降, 為了做到那些 要設置梯度下降的議題, 在下一段影片 我要先示範如何去預估 如何計算斜率或者說導數對於每種啟動函數 讓我們進入下一段影片