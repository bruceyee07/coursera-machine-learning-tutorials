1
00:00:00,000 --> 00:00:04,360
為什麼神經網路需要非線性啟動函數呢？

2
00:00:04,360 --> 00:00:07,425
實際上為了使您的神經網路計算有趣的函數

3
00:00:07,425 --> 00:00:10,335
您真的需要用非線性啟動函數

4
00:00:10,335 --> 00:00:15,830
讓我們看看為什麼, 這是神經網路的正向傳播

5
00:00:15,830 --> 00:00:17,770
我們為什麼不去掉這個?

6
00:00:17,770 --> 00:00:22,315
去掉 g 函數而直接設 a1 等於 z1

7
00:00:22,315 --> 00:00:27,690
或是說, 您可以說 g of z 等於 z, 對吧?

8
00:00:27,690 --> 00:00:31,813
有時候稱為線性啟動函數

9
00:00:31,813 --> 00:00:33,205
也許更好的名字會是

10
00:00:33,205 --> 00:00:37,800
恆等啟動函數因為它的輸出會是永遠等於輸入

11
00:00:37,800 --> 00:00:39,600
為了這個緣故

12
00:00:39,600 --> 00:00:43,310
如果說 a2 等於 z2

13
00:00:43,310 --> 00:00:45,183
事實上如果您這樣做

14
00:00:45,183 --> 00:00:53,620
那這個模型會只是計算 y 或者 y-hat 用線性函數處理輸入特徵 x

15
00:00:53,620 --> 00:00:55,940
拿前面兩個方程式

16
00:00:55,940 --> 00:01:04,547
如果您用 a1 等於 z1 等於 w1

17
00:01:04,547 --> 00:01:15,965
x + b, 然後讓 a2 等於 z2 等於 w2

18
00:01:15,965 --> 00:01:25,370
a1 + b, 如果您拿 a1 的定義插入這個公式

19
00:01:25,370 --> 00:01:32,585
您發現 a2 等於 w2 乘 w1

20
00:01:32,585 --> 00:01:35,695
x + b1, 對吧?

21
00:01:35,695 --> 00:01:40,985
這是 a1 + 

22
00:01:40,985 --> 00:01:47,460
b2 所以這簡化了 w2,

23
00:01:47,460 --> 00:01:53,120
w1, x + w2

24
00:01:53,120 --> 00:01:57,966
b1 + b2

25
00:01:57,966 --> 00:02:01,930
所以這只是

26
00:02:01,930 --> 00:02:06,726
讓我們稱這項為 w', b'

27
00:02:06,726 --> 00:02:10,935
這只是等於 w'x + b'

28
00:02:10,935 --> 00:02:13,720
如果您使用線性啟動函數

29
00:02:13,720 --> 00:02:17,095
或者我們稱它為恆等啟動函數

30
00:02:17,095 --> 00:02:23,335
那這神經網路只是將輸入用線性函數輸出

31
00:02:23,335 --> 00:02:26,260
我們會在以後談深度網路

32
00:02:26,260 --> 00:02:27,460
神經網路有著很多

33
00:02:27,460 --> 00:02:29,167
很多層, 很多很多隱藏層

34
00:02:29,167 --> 00:02:34,460
它會變成, 如果您使用線性啟動函數, 或者換句話說

35
00:02:34,460 --> 00:02:36,760
如果您不用啟動函數那

36
00:02:36,760 --> 00:02:39,250
不論您的神經網路有幾層

37
00:02:39,250 --> 00:02:42,970
它只會計算線性啟動函數

38
00:02:42,970 --> 00:02:45,905
所以您有隱藏層跟沒有一樣

39
00:02:45,905 --> 00:02:49,335
有一些情況需要簡要的提及

40
00:02:49,335 --> 00:02:50,880
如果您用

41
00:02:50,880 --> 00:02:55,170
線性啟動函數這裡跟S型函數這裡

42
00:02:55,170 --> 00:02:58,275
那這個模型不就是

43
00:02:58,275 --> 00:03:02,505
標準羅吉斯迴歸分析沒有隱藏層

44
00:03:02,505 --> 00:03:05,910
我不去證明它但您可以試著
自己去證明如果您想的話

45
00:03:05,910 --> 00:03:07,465
但重要的是

46
00:03:07,465 --> 00:03:11,265
一個線性隱藏層幾乎是沒有用處的

47
00:03:11,265 --> 00:03:17,130
因為兩個線性函數的組合還是線性函數

48
00:03:17,130 --> 00:03:19,950
除非您丟一個非線性去那裡

49
00:03:19,950 --> 00:03:21,235
那您不會計算出

50
00:03:21,235 --> 00:03:25,350
更有趣的函數即便您用更深入的網路

51
00:03:25,350 --> 00:03:29,820
只有一個地方您或許會用到線性啟動函數

52
00:03:29,820 --> 00:03:36,810
g of z 等於 z 如果您做機器學習在迴歸分析問題上

53
00:03:36,810 --> 00:03:39,420
如果 y 是實數

54
00:03:39,420 --> 00:03:42,675
舉個例子, 如果您試著預估房價

55
00:03:42,675 --> 00:03:46,935
所以 y 不是 0 或 1, 是一個實數

56
00:03:46,935 --> 00:03:54,660
房價從 0 開始到很貴的房子

57
00:03:54,660 --> 00:03:58,640
我猜或許, 貴的房子要好幾百萬美金

58
00:03:58,640 --> 00:04:04,580
不管如何, 有很多房價的資料在您的資料集

59
00:04:04,580 --> 00:04:09,705
如果 y 用這些實數

60
00:04:09,705 --> 00:04:14,700
也許這種情況下可以用線性啟動函數

61
00:04:14,700 --> 00:04:17,805
您的輸出 y-hat 

62
00:04:17,805 --> 00:04:24,215
也是實數從負的無窮大到正的無窮大

63
00:04:24,215 --> 00:04:28,700
但這些隱藏單元不應該用線性啟動函數

64
00:04:28,700 --> 00:04:34,380
它們可以用 ReLU 或 tanh 或 Leaky ReLU 或者其他函數

65
00:04:34,380 --> 00:04:39,995
您或許會用到線性啟動函數的地方會是在輸出層

66
00:04:39,995 --> 00:04:41,595
但除了那裡

67
00:04:41,595 --> 00:04:44,730
使用線性啟動函數在

68
00:04:44,730 --> 00:04:50,135
隱藏層除了很特別的情況下

69
00:04:50,135 --> 00:04:52,320
跟壓縮有關我們之前談過的

70
00:04:52,320 --> 00:04:56,250
使用線性啟動函數是相當少的

71
00:04:56,250 --> 00:04:59,130
當然, 如果您預測房價像是在

72
00:04:59,130 --> 00:05:03,795
第一週影片中看到的是非負值

73
00:05:03,795 --> 00:05:07,060
您可以使用 ReLU 啟動函數

74
00:05:07,060 --> 00:05:11,580
您的輸出 y-hat 會大於等於 0

75
00:05:11,580 --> 00:05:13,980
我希望給您一點感覺為什麼

76
00:05:13,980 --> 00:05:19,290
非線性啟動函數在神經網路很重要

77
00:05:19,290 --> 00:05:23,945
下一步, 我將開始談到梯度下降, 為了做到那些

78
00:05:23,945 --> 00:05:27,440
要設置梯度下降的議題, 在下一段影片

79
00:05:27,440 --> 00:05:29,230
我要先示範如何去預估

80
00:05:29,230 --> 00:05:34,105
如何計算斜率或者說導數對於每種啟動函數

81
00:05:34,105 --> 00:05:35,600
讓我們進入下一段影片