1
00:00:00,390 --> 00:00:04,350
當您試圖改進神經網路時

2
00:00:02,580 --> 00:00:06,720
您需要選擇的其中一項是

3
00:00:04,350 --> 00:00:09,599
使用什麼啟動函數在隱藏層

4
00:00:06,720 --> 00:00:11,490
跟輸出單元在您的

5
00:00:09,599 --> 00:00:13,139
神經網路, 目前為止我們僅

6
00:00:11,490 --> 00:00:16,080
使用S型啟動函數

7
00:00:13,139 --> 00:00:18,720
但有時候其他選擇可以

8
00:00:16,080 --> 00:00:20,939
做得更好, 讓我們來看

9
00:00:18,720 --> 00:00:23,279
一些選項, 在正向傳播

10
00:00:20,939 --> 00:00:26,099
的步驟對於神經網路

11
00:00:23,279 --> 00:00:28,710
我們有兩個步驟我們都使用

12
00:00:26,099 --> 00:00:32,610
S型函數
所以這個S型函數

13
00:00:28,710 --> 00:00:37,590
稱為啟動函數
而 G 

14
00:00:32,610 --> 00:00:40,680
是熟悉的S型函數 a 等於 1

15
00:00:37,590 --> 00:00:42,600
除以 1 加 e 的 -z 次方

16
00:00:40,680 --> 00:00:49,739
一般情況下我們可以

17
00:00:42,600 --> 00:00:53,309
用不同的函數 G of z, 
我們寫成這樣

18
00:00:49,739 --> 00:00:56,010
而 G 可以是非線性

19
00:00:53,309 --> 00:00:59,250
函數
也許不是S型函數

20
00:00:56,010 --> 00:01:01,879
舉個例子
S型函數

21
00:00:59,250 --> 00:01:04,290
從 0 到 1
一個

22
00:01:01,879 --> 00:01:06,900
啟動函數幾乎總是

23
00:01:04,290 --> 00:01:10,320
做得比S型函數更好

24
00:01:06,900 --> 00:01:14,189
的是 tanh 函數
或者稱為雙曲

25
00:01:10,320 --> 00:01:19,979
正切函數
所以這是 z

26
00:01:14,189 --> 00:01:25,710
這是 a 等於 tanh of z 而這個

27
00:01:19,979 --> 00:01:31,079
從 正 1 到 負 1
這個公式

28
00:01:25,710 --> 00:01:37,799
對於 tanh 函數是
e 的 z次方 減

29
00:01:31,079 --> 00:01:40,140
e 的 -z次方
除以它們的和

30
00:01:37,799 --> 00:01:43,890
實際上是一個

31
00:01:40,140 --> 00:01:46,350
S型函數的數學轉移版本

32
00:01:43,890 --> 00:01:49,860
所以您知道
S型函數像這樣

33
00:01:46,350 --> 00:01:52,079
但移動它
讓它經過

34
00:01:49,860 --> 00:01:54,570
原點
然後放大比例

35
00:01:52,079 --> 00:01:58,530
讓它從 負 1 到 正 1

36
00:01:54,570 --> 00:02:05,340
實際上對於隱藏單元
如果您讓

37
00:01:58,530 --> 00:02:09,910
函數 G of z 等於

38
00:02:05,340 --> 00:02:12,490
tanh of z
這幾乎總是作用得

39
00:02:09,910 --> 00:02:14,020
比 S型函數
因為

40
00:02:12,490 --> 00:02:16,930
介於正 1 跟負 1 的

41
00:02:14,020 --> 00:02:19,000
啟動值平均數

42
00:02:16,930 --> 00:02:21,550
從隱藏層出來時會趨近

43
00:02:19,000 --> 00:02:23,020
於 0 平均值
也就是

44
00:02:21,550 --> 00:02:23,590
有時候當您訓練一個學習

45
00:02:23,020 --> 00:02:25,690
演算法

46
00:02:23,590 --> 00:02:29,709
您或許會集中您的資料

47
00:02:25,690 --> 00:02:31,510
讓資料有 0 平均值, 使用 tanh
而不是

48
00:02:29,709 --> 00:02:34,750
用S型函數
有一種

49
00:02:31,510 --> 00:02:36,880
集中您的資料的影響
所以

50
00:02:34,750 --> 00:02:39,610
資料的平均值會趨近於

51
00:02:36,880 --> 00:02:41,410
0 而不是 0.5
而這

52
00:02:39,610 --> 00:02:43,510
實際上讓下一層的學習

53
00:02:41,410 --> 00:02:45,820
容易一些
我們會在

54
00:02:43,510 --> 00:02:47,380
第二個課程談到這些
當我們

55
00:02:45,820 --> 00:02:50,739
談到最佳化演算法時

56
00:02:47,380 --> 00:02:52,480
但一個重點是我

57
00:02:50,739 --> 00:02:54,250
幾乎不再用S型啟動

58
00:02:52,480 --> 00:02:56,410
函數

59
00:02:54,250 --> 00:02:59,560
tanh 函數幾乎總是

60
00:02:56,410 --> 00:03:03,550
比較好
一個例外是

61
00:02:59,560 --> 00:03:07,420
在於輸出層
因為如果 y 

62
00:03:03,550 --> 00:03:10,570
不是 0 就是 1 的話
那對於

63
00:03:07,420 --> 00:03:13,989
y-hat 應該是一個數字

64
00:03:10,570 --> 00:03:16,570
輸出介於 0 跟 1
而不是

65
00:03:13,989 --> 00:03:19,360
介於 負1 跟 正1
所以一個

66
00:03:16,570 --> 00:03:21,430
例外是我會使用S型

67
00:03:19,360 --> 00:03:24,670
啟動函數是
當您使用

68
00:03:21,430 --> 00:03:26,350
二元分類時您

69
00:03:24,670 --> 00:03:29,709
也許會用S型啟動

70
00:03:26,350 --> 00:03:35,170
函數在輸出層
所以 G of z[2]

71
00:03:29,709 --> 00:03:37,180
這裡會等於
S形函數 of z[2]

72
00:03:35,170 --> 00:03:40,299
您看到的這個例子

73
00:03:37,180 --> 00:03:43,920
您或許會用 tanh 啟動

74
00:03:40,299 --> 00:03:47,769
函數在隱藏層

75
00:03:43,920 --> 00:03:49,299
S型函數在輸出層, 所以

76
00:03:47,769 --> 00:03:51,670
啟動函數可能不同

77
00:03:49,299 --> 00:03:53,709
在不同層, 而有時候

78
00:03:51,670 --> 00:03:55,690
為了記啟動函數

79
00:03:53,709 --> 00:03:58,510
在不同層時有所不同
我們

80
00:03:55,690 --> 00:04:02,230
會使用上標方括號

81
00:03:58,510 --> 00:04:04,540
來指示 G of 方

82
00:04:02,230 --> 00:04:06,940
括號 1
也許不同於 G of 方

83
00:04:04,540 --> 00:04:09,340
括號 2
上標方括號1

84
00:04:06,940 --> 00:04:11,470
指的是這一層

85
00:04:09,340 --> 00:04:12,879
上標方括號 2
指的是

86
00:04:11,470 --> 00:04:15,680
輸出層

87
00:04:12,879 --> 00:04:18,109
對於

88
00:04:15,680 --> 00:04:20,780
S型函數跟 tanh 函數
的一個共同缺點是

89
00:04:18,109 --> 00:04:22,910
如果 z 很大或是

90
00:04:20,780 --> 00:04:24,460
很小時
那函數梯度

91
00:04:22,910 --> 00:04:27,560
或者說斜率

92
00:04:24,460 --> 00:04:30,139
會變得很小
所以 z 很大或者

93
00:04:27,560 --> 00:04:33,169
z 很小時
函數的斜率

94
00:04:30,139 --> 00:04:35,270
幾乎趨近於

95
00:04:33,169 --> 00:04:38,360
0, 所以這會讓
梯度下降法慢下來

96
00:04:35,270 --> 00:04:41,810
一種很大眾化的選擇

97
00:04:38,360 --> 00:04:44,900
在機器學習是

98
00:04:41,810 --> 00:04:50,720
稱為線性整流函數單元

99
00:04:44,900 --> 00:04:57,110
這個線性整流函數像這樣

100
00:04:50,720 --> 00:05:00,500
公式是 a 等於 max of 0, z

101
00:04:57,110 --> 00:05:03,530
當 z 是正值時導數為 1

102
00:05:00,500 --> 00:05:05,990
當 z 是負值時導數

103
00:05:03,530 --> 00:05:07,580
為 0
如果您

104
00:05:05,990 --> 00:05:10,190
建置這個時
技術上而言

105
00:05:07,580 --> 00:05:12,349
當 z 為 0 時導數是

106
00:05:10,190 --> 00:05:14,210
沒有定義的
但當您使用

107
00:05:12,349 --> 00:05:18,770
電腦來建置時
通常您

108
00:05:14,210 --> 00:05:21,229
得到的值是 0000000

109
00:05:18,770 --> 00:05:22,940
很小
您不用去

110
00:05:21,229 --> 00:05:25,610
擔心它
實作上您可以

111
00:05:22,940 --> 00:05:29,659
假設導數當 z 等於

112
00:05:25,610 --> 00:05:32,270
0 時您可以假設為 1 或 0

113
00:05:29,659 --> 00:05:35,479
您還是可以作用得很好
即使是

114
00:05:32,270 --> 00:05:37,430
不能微分的事實
這裡有

115
00:05:35,479 --> 00:05:40,010
一些準則來

116
00:05:37,430 --> 00:05:43,280
選擇啟動函數
如果您的

117
00:05:40,010 --> 00:05:45,620
輸出是 0 跟 1的值
如果您用

118
00:05:43,280 --> 00:05:47,539
二元分類
那S型

119
00:05:45,620 --> 00:05:50,479
啟動函數是很自然的選擇在

120
00:05:47,539 --> 00:05:59,419
輸出層
而其他的

121
00:05:50,479 --> 00:06:04,460
單元
ReLU 或者 線性整流函數

122
00:05:59,419 --> 00:06:07,190
單元越來越是預設選擇的

123
00:06:04,460 --> 00:06:10,280
啟動函數
如果您不

124
00:06:07,190 --> 00:06:13,849
確定使用什麼在您的隱藏層

125
00:06:10,280 --> 00:06:15,289
我會直接使用 ReLU 啟動

126
00:06:13,849 --> 00:06:17,570
函數
這是大部分的人

127
00:06:15,289 --> 00:06:20,120
在這個時期常使用的
雖然有時候

128
00:06:17,570 --> 00:06:21,350
人們也會用到 tanh 啟動

129
00:06:20,120 --> 00:06:23,150
函數

130
00:06:21,350 --> 00:06:26,270
ReLU 的一個缺點是

131
00:06:23,150 --> 00:06:28,640
當 z 為負的時候導數為 0

132
00:06:26,270 --> 00:06:31,700
實作上這還是可行的

133
00:06:28,640 --> 00:06:33,890
但有另一種版本的

134
00:06:31,700 --> 00:06:35,420
ReLU 稱為滲漏線性整流函數(Leaky ReLU)

135
00:06:33,890 --> 00:06:38,690
會在下一張投影片給您公式

136
00:06:35,420 --> 00:06:40,520
與其當 z 負的時候為 0

137
00:06:38,690 --> 00:06:42,940
它會用很小的斜率

138
00:06:40,520 --> 00:06:47,900
像這樣
所以稱為 滲漏

139
00:06:42,940 --> 00:06:51,170
線性整流函數
通常這會

140
00:06:47,900 --> 00:06:53,900
比線性整流啟動函數好
雖然

141
00:06:51,170 --> 00:06:54,860
在實作上比較少用到

142
00:06:53,900 --> 00:06:56,770
兩者皆可

143
00:06:54,860 --> 00:06:59,330
如果您必須只選一個
我

144
00:06:56,770 --> 00:07:01,460
通常只是使用
線性整流函數

145
00:06:59,330 --> 00:07:04,460
線性整流函數跟
滲漏線性整流函數的優點是

146
00:07:01,460 --> 00:07:06,500
很多 z 的空間

147
00:07:04,460 --> 00:07:08,150
啟動函數的導數

148
00:07:06,500 --> 00:07:11,870
啟動函數的斜率

149
00:07:08,150 --> 00:07:13,970
是非常異於 0

150
00:07:11,870 --> 00:07:15,920
在實作上使用 ReLU (線性整流函數)

151
00:07:13,970 --> 00:07:18,590
啟動函數您的神經網路

152
00:07:15,920 --> 00:07:20,810
通常學習比使用

153
00:07:18,590 --> 00:07:23,840
tanh 或者S型函數快很多

154
00:07:20,810 --> 00:07:26,420
主要的原因是

155
00:07:23,840 --> 00:07:28,700
比較不被

156
00:07:26,420 --> 00:07:31,580
函數斜率趨近於 0
會緩慢

157
00:07:28,700 --> 00:07:33,950
整個學習的影響
我知道

158
00:07:31,580 --> 00:07:36,710
ReLU 一半 z 值的斜率為 0

159
00:07:33,950 --> 00:07:39,050
實作時足夠多的

160
00:07:36,710 --> 00:07:41,120
隱藏單元會讓 z 大於 0

161
00:07:39,050 --> 00:07:43,700
學習還是可以很快

162
00:07:41,120 --> 00:07:45,800
對於大部分的訓練例子
讓我們

163
00:07:43,700 --> 00:07:47,600
很快整理一下利與弊

164
00:07:45,800 --> 00:07:50,030
對於不同的啟動函數

165
00:07:47,600 --> 00:07:52,790
這是S型啟動函數我會說

166
00:07:50,030 --> 00:07:54,410
不要用它
除非是輸出

167
00:07:52,790 --> 00:07:56,330
層如果您做二元分類

168
00:07:54,410 --> 00:07:59,540
或者也許不會用到

169
00:07:56,330 --> 00:08:02,720
我幾乎從來不用它的原因是

170
00:07:59,540 --> 00:08:05,060
因為 tanh 就是

171
00:08:02,720 --> 00:08:12,080
比較好
所以 tanh 

172
00:08:05,060 --> 00:08:13,430
啟動函數是這樣
然後

173
00:08:12,080 --> 00:08:15,490
最常用的預設選擇

174
00:08:13,430 --> 00:08:19,100
啟動函數是 ReLU

175
00:08:15,490 --> 00:08:23,660
也就是這個
所以當您不確定

176
00:08:19,100 --> 00:08:26,600
用什麼時, 就用這個
或許您

177
00:08:23,660 --> 00:08:31,930
高興的話也可以用 

178
00:08:26,600 --> 00:08:36,659
滲漏線性整流函數
也許

179
00:08:31,930 --> 00:08:40,390
0.01z 逗點 z
所以 a 是最大值 of

180
00:08:36,659 --> 00:08:43,810
0.01 乘 z 跟 z
所以這給您

181
00:08:40,390 --> 00:08:46,200
的函數扳一點點彎曲
您

182
00:08:43,810 --> 00:08:51,670
也許會說為什麼用常數

183
00:08:46,200 --> 00:08:53,380
0.01
您也可以將它視為另一個

184
00:08:51,670 --> 00:08:54,670
學習演算法的參數

185
00:08:53,380 --> 00:08:58,480
有些人說這樣做會更好

186
00:08:54,670 --> 00:08:59,649
但我極少看到人們這樣做

187
00:08:58,480 --> 00:09:01,360
但如果您想試著在您的

188
00:08:59,649 --> 00:09:03,430
應用上這樣使用
請隨您意

189
00:09:01,360 --> 00:09:05,800
您可以看它的作用

190
00:09:03,430 --> 00:09:08,290
跟作用得如何

191
00:09:05,800 --> 00:09:09,880
然後繼續用它
如果它給您好的結果

192
00:09:08,290 --> 00:09:11,620
我希望給您一點點感覺對於

193
00:09:09,880 --> 00:09:13,870
一些您可以使用的啟動函數的選擇

194
00:09:11,620 --> 00:09:15,940
在您的神經網路中
一個

195
00:09:13,870 --> 00:09:18,130
我們會看到在深度學習的主題是
您

196
00:09:15,940 --> 00:09:20,110
通常有很多不同的選擇在於

197
00:09:18,130 --> 00:09:22,089
您如何建造您的神經網路從

198
00:09:20,110 --> 00:09:24,430
隱藏單元的數目到

199
00:09:22,089 --> 00:09:25,839
選擇啟動函數
到您如何

200
00:09:24,430 --> 00:09:28,480
初始您的權值
我們會

201
00:09:25,839 --> 00:09:30,880
在以後看到很多選擇像這樣

202
00:09:28,480 --> 00:09:33,279
實際上有時候很難有

203
00:09:30,880 --> 00:09:35,649
好的指南
對於什麼確實

204
00:09:33,279 --> 00:09:37,270
作用得最好
在您的問題上

205
00:09:35,649 --> 00:09:39,070
在這些課程中
我會持續給您

206
00:09:37,270 --> 00:09:40,839
一些感覺對於我看到在這個

207
00:09:39,070 --> 00:09:43,450
產業裡哪些是比較

208
00:09:40,839 --> 00:09:45,520
常用的
但對於您的應用

209
00:09:43,450 --> 00:09:46,930
您應用的綜合性

210
00:09:45,520 --> 00:09:49,450
確實很難事先知道

211
00:09:46,930 --> 00:09:51,400
哪一種作用最佳
所以

212
00:09:49,450 --> 00:09:52,930
一點建議是如果您不確定

213
00:09:51,400 --> 00:09:54,940
哪一種啟動

214
00:09:52,930 --> 00:09:57,700
函數最好
那就全部試試看

215
00:09:54,940 --> 00:10:00,010
然後用保持

216
00:09:57,700 --> 00:10:02,529
驗證集或者開發集來驗證

217
00:10:00,010 --> 00:10:04,480
我們以後會談到
然後看

218
00:10:02,529 --> 00:10:08,350
哪一種作用得最好就用它

219
00:10:04,480 --> 00:10:10,180
我想利用測試這些

220
00:10:08,350 --> 00:10:13,510
不同的選項
對於您的應用

221
00:10:10,180 --> 00:10:16,240
您會為未來準備比較好的

222
00:10:13,510 --> 00:10:18,130
神經網路架構相對於

223
00:10:16,240 --> 00:10:20,550
您問題的綜合性跟

224
00:10:18,130 --> 00:10:23,440
演算法的演進
而不是

225
00:10:20,550 --> 00:10:25,630
如果我告訴您永遠

226
00:10:23,440 --> 00:10:27,339
使用 ReLU 啟動函數
而不用

227
00:10:25,630 --> 00:10:29,440
其他的
這或許會或許不會

228
00:10:27,339 --> 00:10:30,790
適用於不管是什麼您處理的問題

229
00:10:29,440 --> 00:10:32,410
您知道在

230
00:10:30,790 --> 00:10:36,220
最近或者以後

231
00:10:32,410 --> 00:10:37,870
所以這是選擇

232
00:10:36,220 --> 00:10:39,310
啟動函數
您看到

233
00:10:37,870 --> 00:10:41,459
最受歡迎的一些啟動函數

234
00:10:39,310 --> 00:10:44,260
有一個問題

235
00:10:41,459 --> 00:10:45,160
有時候被問到的是
為什麼您

236
00:10:44,260 --> 00:10:46,959
甚至需要用到

237
00:10:45,160 --> 00:10:49,240
啟動函數
為什麼不丟掉它

238
00:10:46,959 --> 00:10:49,779
讓我們來

239
00:10:49,240 --> 00:10:52,240
談一談

240
00:10:49,779 --> 00:10:54,430
在下一段影片中
您會看到為什麼

241
00:10:52,240 --> 00:10:58,259
神經網路確實需要

242
00:10:54,430 --> 00:10:58,259
一些非線形啟動函數