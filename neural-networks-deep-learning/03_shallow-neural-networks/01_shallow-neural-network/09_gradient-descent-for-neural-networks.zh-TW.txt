我想這是令人興奮的影片 在這段影片您將會看到如何去 建置梯度下降對於您的 神經網路使用一個隱藏層, 在這段影片, 我將只給您 您建置時需要的方程式 為了讓反向傳播的 梯度下降可行然後在 下一段影片我會給一些 直觀也關於這個特別的 方程式是準確的方程式或 正確的方程式來計算 您需要的梯度對於您的神經 網路 您的神經網路有一個 單一隱藏層有 參數 w1, b1, w2 跟 b2 記得如果您有 nx, 或者說 n0 輸入特徵跟 n1 隱藏 單元跟 n2 輸出單元在我們 例子, 目前為止 n2 等於 1 這個矩陣 w1 會是 n1 乘 n0, b1 會是 n1 維度向量 您可以寫成 n1 乘 1 維度矩陣實際上是列 向量, w2 維度為 n2 乘 n1 而 b2 維度會是 n2 乘 1, 目前看到 的例子都只是 n2 等於 1 您只有一個 隱藏單元, 您也會有成本 函數對於神經網路, 而現在 我假設您做的是 二元分類問題, 所以在 這情況下您參數的成本如下 將會是 1/m of 平均對於損失函數, 所以  這裡的 L 是您神經網路的損失當 預測 y-hat 這應該是 a2 當真正的值是 y時 如果您做二元 分類時損失函數可以是 您使用過的羅吉斯迴歸分析 為了訓練參數, 您的 演算法需要執行梯度 下降法, 當訓練神經網路時 重要的是初始這些 參數為隨機的而不是都 為 0 我們稍後會解釋為什麼這樣 經過初始這些參數為 一些值每個迴圈的梯度下降 會計算預測值 您基本上會計算 y-hat i 從 1 到 m, 然後 您需要計算導數 您需要計算 dw1 , 那是 成本函數的導數 相對於參數 w1, 您 需要計算另一個變數 稱為 db1 也是 成本函數的導數或者說斜率 相對於變數 b1 同樣的對於其它 參數 w2 跟 b2, 最後 梯度下降更新會是 更新 w1 為 w1 - alpha 學習率 乘 dw1, b1 更新 為 b1 減學習率 乘 db1, 同樣的對於 w2 跟 b2 有時後我用冒號等號 也時候用等號, 兩種 符號都可以, 所以這會是 一次遞迭的梯度下降 然後您可以重複幾次 直到您的參數看起來 已經收斂, 在前面的影片 我們談到如何計算 預估如何計算輸出 我們看到如何用 向量化來做這些, 所以關鍵是 知道如何計算這些偏 導數項目, dw1, db1 跟 導數 dw2 跟 db2 我要做的是只給您 您需要的方程式為了計算 這些導數而我會延遲到 下一段影片, 那是一段可選的影片 一個好的機會來檢查我們如何 找出這些公式 讓我總結一下這些 正向傳播方程式, 您有 z1 等於 w1 X + b1 然後 a1 等於啟動函數在 那一層, 應用逐元素到 z1 z2 等於 w2 A1 + b2 最後， 這些都是 向量化在您所有的訓練集 a2 等於 g2 of z2, 再次 提醒我們假設您做的是二元分類 這個啟動 函數實在應該是S型函數 我就直接寫在這裡 所以這是正向傳播或者說 左到右正向計算對於 您的神經網路, 下一步讓我們計算 導數, 所以這是反向 傳播步驟它計算 dz2 等於 a2 減真正 y 值 提醒一下所有這些都是 向量化對於所有例子, 所以矩陣 Y 是 1 乘 m 矩陣列出所有 您的 m 個例子用列水平疊起來 原來 dw2 等於這個, 實際上 這前面三個方程式是非常 類似於梯度下降對於羅吉斯 迴歸分析 axis 等於 1, keepdims 等於 True, 只是一點小細節, 這個 np .sum 是 Python Numpy 指令 來總和經過矩陣的一個維度 在這個例子是水平總和 而 keepdims 做的是避免 Python 輸出一種有趣的 等級 1 陣列, 而這種陣列的維度 是 (n,) 用 keepdims 等於 True, 這會保證 Python 輸出 db2 是一個向量 是 n 乘 1, 技術上而言這會是 n2 乘 1 在這裡 只是 1 乘 1 數字, 也許 無關緊要, 但等一下我們會見到 當它真的要緊時, 所以目前為止 我們做得很類似羅吉斯 迴歸分析, 但當您繼續計算 跑反向傳播您 會計算這個 dz2 乘上 g1 prime of z1, 所以這個 g1 prime 是 導數對於任何您用的 啟動函數使用在 隱藏層而對於輸出層,我 假設您做的是二元 分類使用S型函數 這已經放入 dz2 的公式, 而這個相乘是 逐元素相乘, 所以這裡這個 會是 n1 乘 m 矩陣 這個逐元素 導數也將是 n1 乘 m 矩陣，所以這個相乘 是逐元素相乘對於這兩個 矩陣, 最後 dw1 等於 這個, db1 等於這個 np. sum dz1 axis 等於 1, keepdims 等於 True, 在 前面 keepdims 也許 比較無關緊要如果 n2 等於 1, 所以這只是 1 乘 1 也就是一個實數 這裡 db1 會是 n1 乘 1 向量, 所以您要 Python np dot sum 輸出一種這個 維度而不是有趣的等級1 陣列那個維度可能會 搞砸你以後一些的 計算, 另一種方式是 不用 keepdims 參數但 明顯的呼叫 reshape函數, 來重塑 np.sum 的輸出變成 您希望 db 的這個維度, 所以 這是正向傳播我想是 四個方程式跟反向傳播我猜 六個方程式, 我知道我只寫下 這些方程式, 但在下一段 可選影片讓我們看一些 直觀的如何推導出者六個方程式 在反向傳播演算法 隨您的意願看不看 下一段影片, 但不管如何如果您建置 這些演算法您會有正確的 正向跟反向傳播 您能夠計算 您需要的導數為了應用在 梯度下降來學習參數 在您的神經網路, 這是可能的 建置這個演算法然後執行它 而不深入瞭解這些 微積分很多成功的 深度學習人員已經這麼做, 但如果您 要的話您可以看下一段影片 來獲得更多的直觀有關於 這些導數在這些 方程式