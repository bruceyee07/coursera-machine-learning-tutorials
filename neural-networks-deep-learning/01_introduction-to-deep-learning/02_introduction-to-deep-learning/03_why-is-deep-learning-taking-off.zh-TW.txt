如果基本技術思維在 深度學習，神經網路背後已經 幾十年了, 為什麼它們 只在最近起飛呢? 在這段影片中 讓我們來看一些主要驅動力 在深度學習背後, 因為 我想這會幫助您專注於 在您自己組織內最佳的機會 來應用這些, 在 最近幾年很多人問我 Andrew 為什麼深度學習 突然間作用的這麼好, 而當我 回答這問題時, 通常我們 畫這個圖形給他們, 假設說我們 畫一個圖在橫軸 我們畫資料的數量
在一件任務上 假設說在縱軸 我們畫 學習演算法的表現, 像是正確性 對於垃圾郵件分類器
或者廣告點擊預測 或者我們的神經 網路對於找出其他 車子的自動車的正確性 實際上如果您畫 傳統學習演算法的表現像是 支援向量機或者羅吉斯 迴歸分析當作是
您有的資料量的函數 您也許得到一個曲線 像這樣, 他的表現改進一段時間
當您 加入更多的資料
但在經過一段時間 他的表現變得 平緩， 假設這應該是 水平線, 畫得不是很好, 總是這樣 他們不知道如何來處理 大量的資料, 而發生在 我們社群這十幾年來 很多的問題我們從 相對小量的資料 到通常相當大量的 資料而這些資料 來自於社群的數位化 很多的人類活動現在都 在數位化領域，我們花很多時間 在電腦在網站上在手機 app, 很多活動在電子設備上 產生資料而來自 便宜相機興起內置於我們的 手機上加速器等等各式各樣 感測器在物聯網上， 我們 也收集了越來越多 的資料, 在過去二十年來 很多的應用程式我們 累積了 很多的資料, 多過於傳統 學習演算法能夠 有效地利用他們而 神經網路實際上如果您 訓練在小的神經網路時 它的表現也許像這樣 如果您訓練大一點的神經網路 讓我們稱它為中型神經網路 他的表現會好一點 如果您訓練一個大型的神經網路 它的表現通常會保持 越來越好, 所以一些 觀察，第一如果您想到達 這樣高的表現 您需要兩件事, 第一您需要 能夠訓練足夠大的神經網路 為了能夠充分利用 大量的資料, 而第二您 需要在 x 軸這裡, 您 需要很多資料， 所以我們通常說 規模驅動了深度學習的 進步, 而規模我的意思是 同時是神經網路的大小我們需要 很多隱藏單元的網路 很多的參數很多的連結 以及資料的規模, 實際上 在今天最值得信賴的方式來 獲得好的表現在神經網路 通常是要不是訓練一個 大一點的網路或者丟入更多的資料給它 而這只可行到一定程度 因為最終您用完資料 或者最終您的網路太大了 花太多時間來訓練但 僅改進規模已經 讓我們在深度學習的世界走得很辛苦 為了讓這個圖更 技術上精確，讓我來加入 一些東西, 我寫資料量 在 x 軸上技術上這是 標籤資料量，而所謂標籤資料 我的意思是訓練例子我們有 輸入 x 跟標籤 y, 我想 介紹一些符號 我們在後面課程用到我們 用小寫字母 m 來 記為我訓練集的大小 或者訓練例子的數目 這小寫 m 所以這是 水平軸, 一些細節 在這個圖形上 在這個小量的訓練集區域 這相對的演算法排名 實際上不是那麼好定義, 如果 所以如果您沒有很多的訓練集 通常您的手工 打造的特徵來決定 表現，所以很有可能 有人訓練支援向量機 (SVM) 是更 傾向於手工打造特徵而 有些人訓練大型的神經網路 也許在這個小量訓練集 區域 支援向量機可以做得更好 所以在這個地區的左邊 的圖形中 至於演算法間的相對排序則沒有清楚的定義 而效能多半取決於 您在手工工程的技術 以及演算法的低階細節 只有在這個大數據的區域 很大的訓練集 在右邊的大的 m 區域 比較一致的看到大的神經網路 領先其他的演算法 如果您的朋友問起，為什麼 神經網路會起飛，我會 鼓勵您畫這幅圖給他們看 我會講說在 當今深度學習的興起 早期的時候 因為有了大規模的資料 大規模的計算力，剛好給了我們能夠 訓練大型的神經網路 在 CPU 或者 GPU 上，讓我們 有極大的進展 逐漸地，特別是最近幾年 我們看到大量的 演算法創新，我也 不想低估這部分 有趣的是很多的演算法 創新試著讓 神經網路跑得更快 一個具體的例子 一個重大突破在神經網路是 從S型函數像這樣 切換成為 ReLU 函數 我們在早先的影片提過 長得像這樣 如果您不了解這些細節 不用擔心 實際上使用 S型函數在機器學習時的問題是 在這段區域中 這裡函數的 斜率 也就是梯度，會幾乎為 0,  導致學習 變得十分緩慢，因為當你 建置梯度下降時，當梯度為 0 時，參數的改變會非常 緩慢，學習也就十分緩慢 而當我們改變稱為 神經網路的啟動函數 使用這個函數稱為 ReLU 函數 (Rectified Linear Unit) 函數 R,E,L,U, 梯度等於 1 對於所有輸入值為正數 所以梯度比較不會 逐漸縮為 0 而這邊的梯度，這一條線的斜率 為 0 在左半部，實際上 只要將S型函數切換為 ReLU 函數會讓 演算法做梯度下降時 作用得更快，所以這是 一個例子，或許相對簡單 的演算法創新，但是最終 這個演算法創新的影響是 它真的幫助了計算力，所以 其實有相當多的例子像這樣 我們改變了演算法 因為這會讓我們的程式 跑得更快，讓我們可以訓練 更大的神經網路，或者在 合理的時間內跑完，即使我們有 相當大的網路，大量的資料 另一個高速計算很重要的原因是 實際上在整個 訓練神經網路的過程是 一個循環，通常您有一個想法 對於神經網路的架構，所以 您建置您的想法在程式裡 建立您的想法然後讓您跑 一個實驗，然後告訴您， 您個神經網路效果如何，然後 看著它，回頭改變一些 神經網路的細節 經過這樣的循環，一次又一次 而當您的神經網路花太長的時間 來訓練，就會花太長的時間來 跑一次循環，這就會有巨大的 不同生產力在於建立有效的 神經網路，當您有一個 想法，試試看，十分鐘後看著它 的效果，或者最多一天 相對於如果您訓練一個神經 網路花了一個月，這 有時候會發生 因為您在 十分鐘或者一天內得知結果 您就可以試更多的想法， 更能夠找到一個 神經網路可以在您的應用上作用得很好 所以快速的計算 幫助了在 您可以更快獲得實驗結果 而這真的 幫助了神經網路的參與者 跟研究人員 更快速地深度學習循環 更快改進您的想法 所以所有這些變成 一個大的爆炸對於整個深度學習 研究社群，一直 很不可思議的發展 新的演算法，創造不停止的 進展，這些就是 激勵了深度學習的興起的動力 好消息是這些 動力還在激勵讓 深度學習更棒，技術資料 社群還一直丟出更多的 數位資料，或者讓計算力 特別的硬體像是 GPU 跟更快的網路的興起，很多的 硬體，我真的很有信心 我們有能力來做很大型的 神經網路，從計算力的角度來看 會越來越好，而 演算法方面，整個深度學習 研究社群持續地 在驚人的創新 演算法上，因為這樣 我們可以樂觀地回答 深度學習會 持續的越來越好在 未來幾年 讓我們進入最後一段影片 我們想談一談 目前為止在這個課程 您學到了什麼