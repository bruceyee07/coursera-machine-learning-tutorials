1
00:00:00,329 --> 00:00:05,700
如果基本技術思維在

2
00:00:03,060 --> 00:00:07,470
深度學習，神經網路背後已經

3
00:00:05,700 --> 00:00:09,870
幾十年了, 為什麼它們

4
00:00:07,470 --> 00:00:12,090
只在最近起飛呢? 在這段影片中

5
00:00:09,870 --> 00:00:14,130
讓我們來看一些主要驅動力

6
00:00:12,090 --> 00:00:16,170
在深度學習背後, 因為

7
00:00:14,130 --> 00:00:18,090
我想這會幫助您專注於

8
00:00:16,170 --> 00:00:20,850
在您自己組織內最佳的機會

9
00:00:18,090 --> 00:00:22,439
來應用這些, 在

10
00:00:20,850 --> 00:00:24,240
最近幾年很多人問我

11
00:00:22,439 --> 00:00:26,820
Andrew 為什麼深度學習

12
00:00:24,240 --> 00:00:28,949
突然間作用的這麼好, 而當我

13
00:00:26,820 --> 00:00:31,109
回答這問題時, 通常我們

14
00:00:28,949 --> 00:00:33,210
畫這個圖形給他們, 假設說我們

15
00:00:31,109 --> 00:00:36,180
畫一個圖在橫軸

16
00:00:33,210 --> 00:00:39,270
我們畫資料的數量
在一件任務上

17
00:00:36,180 --> 00:00:42,570
假設說在縱軸

18
00:00:39,270 --> 00:00:44,430
我們畫

19
00:00:42,570 --> 00:00:48,180
學習演算法的表現, 像是正確性

20
00:00:44,430 --> 00:00:51,960
對於垃圾郵件分類器
或者廣告點擊預測

21
00:00:48,180 --> 00:00:53,969
或者我們的神經

22
00:00:51,960 --> 00:00:56,399
網路對於找出其他

23
00:00:53,969 --> 00:00:58,440
車子的自動車的正確性

24
00:00:56,399 --> 00:01:00,270
實際上如果您畫

25
00:00:58,440 --> 00:01:02,460
傳統學習演算法的表現像是

26
00:01:00,270 --> 00:01:04,710
支援向量機或者羅吉斯

27
00:01:02,460 --> 00:01:07,619
迴歸分析當作是
您有的資料量的函數

28
00:01:04,710 --> 00:01:09,720
您也許得到一個曲線

29
00:01:07,619 --> 00:01:11,670
像這樣, 

30
00:01:09,720 --> 00:01:14,280
他的表現改進一段時間
當您

31
00:01:11,670 --> 00:01:16,200
加入更多的資料
但在經過一段時間

32
00:01:14,280 --> 00:01:18,630
他的表現變得

33
00:01:16,200 --> 00:01:21,180
平緩， 假設這應該是

34
00:01:18,630 --> 00:01:25,320
水平線, 畫得不是很好, 總是這樣

35
00:01:21,180 --> 00:01:28,140
他們不知道如何來處理

36
00:01:25,320 --> 00:01:30,689
大量的資料, 而發生在

37
00:01:28,140 --> 00:01:32,850
我們社群這十幾年來

38
00:01:30,689 --> 00:01:34,820
很多的問題我們從

39
00:01:32,850 --> 00:01:38,610
相對小量的資料

40
00:01:34,820 --> 00:01:40,979
到通常相當大量的

41
00:01:38,610 --> 00:01:43,979
資料而這些資料

42
00:01:40,979 --> 00:01:46,979
來自於社群的數位化

43
00:01:43,979 --> 00:01:48,720
很多的人類活動現在都

44
00:01:46,979 --> 00:01:51,180
在數位化領域，我們花很多時間

45
00:01:48,720 --> 00:01:54,320
在電腦在網站上在手機

46
00:01:51,180 --> 00:01:57,960
app, 很多活動在電子設備上

47
00:01:54,320 --> 00:02:00,360
產生資料而來自

48
00:01:57,960 --> 00:02:02,369
便宜相機興起內置於我們的

49
00:02:00,360 --> 00:02:05,909
手機上加速器等等各式各樣

50
00:02:02,369 --> 00:02:07,890
感測器在物聯網上， 我們

51
00:02:05,909 --> 00:02:11,129
也收集了越來越多

52
00:02:07,890 --> 00:02:12,870
的資料, 在過去二十年來

53
00:02:11,129 --> 00:02:13,560
很多的應用程式我們

54
00:02:12,870 --> 00:02:16,319
累積了

55
00:02:13,560 --> 00:02:17,550
很多的資料, 多過於傳統

56
00:02:16,319 --> 00:02:20,520
學習演算法能夠

57
00:02:17,550 --> 00:02:22,560
有效地利用他們而

58
00:02:20,520 --> 00:02:26,310
神經網路實際上如果您

59
00:02:22,560 --> 00:02:28,470
訓練在小的神經網路時

60
00:02:26,310 --> 00:02:31,349
它的表現也許像這樣

61
00:02:28,470 --> 00:02:34,590
如果您訓練大一點的神經網路

62
00:02:31,349 --> 00:02:36,330
讓我們稱它為中型神經網路

63
00:02:34,590 --> 00:02:39,900
他的表現會好一點

64
00:02:36,330 --> 00:02:42,180
如果您訓練一個大型的神經網路

65
00:02:39,900 --> 00:02:44,580
它的表現通常會保持

66
00:02:42,180 --> 00:02:46,890
越來越好, 所以一些

67
00:02:44,580 --> 00:02:49,410
觀察，第一如果您想到達

68
00:02:46,890 --> 00:02:52,620
這樣高的表現

69
00:02:49,410 --> 00:02:54,420
您需要兩件事, 第一您需要

70
00:02:52,620 --> 00:02:57,360
能夠訓練足夠大的神經網路

71
00:02:54,420 --> 00:02:59,670
為了能夠充分利用

72
00:02:57,360 --> 00:03:02,010
大量的資料, 而第二您

73
00:02:59,670 --> 00:03:05,430
需要在 x 軸這裡, 您

74
00:03:02,010 --> 00:03:07,799
需要很多資料， 所以我們通常說

75
00:03:05,430 --> 00:03:10,860
規模驅動了深度學習的

76
00:03:07,799 --> 00:03:12,900
進步, 而規模我的意思是

77
00:03:10,860 --> 00:03:15,150
同時是神經網路的大小我們需要

78
00:03:12,900 --> 00:03:17,069
很多隱藏單元的網路

79
00:03:15,150 --> 00:03:21,480
很多的參數很多的連結

80
00:03:17,069 --> 00:03:23,910
以及資料的規模, 實際上

81
00:03:21,480 --> 00:03:25,440
在今天最值得信賴的方式來

82
00:03:23,910 --> 00:03:27,390
獲得好的表現在神經網路

83
00:03:25,440 --> 00:03:29,940
通常是要不是訓練一個

84
00:03:27,390 --> 00:03:31,829
大一點的網路或者丟入更多的資料給它

85
00:03:29,940 --> 00:03:33,359
而這只可行到一定程度

86
00:03:31,829 --> 00:03:35,640
因為最終您用完資料

87
00:03:33,359 --> 00:03:37,769
或者最終您的網路太大了

88
00:03:35,640 --> 00:03:40,200
花太多時間來訓練但

89
00:03:37,769 --> 00:03:42,690
僅改進規模已經

90
00:03:40,200 --> 00:03:45,810
讓我們在深度學習的世界走得很辛苦

91
00:03:42,690 --> 00:03:48,060
為了讓這個圖更

92
00:03:45,810 --> 00:03:49,920
技術上精確，讓我來加入

93
00:03:48,060 --> 00:03:53,040
一些東西, 我寫資料量

94
00:03:49,920 --> 00:03:57,900
在 x 軸上技術上這是

95
00:03:53,040 --> 00:04:00,180
標籤資料量，而所謂標籤資料

96
00:03:57,900 --> 00:04:03,630
我的意思是訓練例子我們有

97
00:04:00,180 --> 00:04:05,910
輸入 x 跟標籤 y, 我想

98
00:04:03,630 --> 00:04:07,709
介紹一些符號

99
00:04:05,910 --> 00:04:10,769
我們在後面課程用到我們

100
00:04:07,709 --> 00:04:12,540
用小寫字母 m 來

101
00:04:10,769 --> 00:04:13,739
記為我訓練集的大小

102
00:04:12,540 --> 00:04:15,690
或者訓練例子的數目

103
00:04:13,739 --> 00:04:18,989
這小寫 m 所以這是

104
00:04:15,690 --> 00:04:20,310
水平軸, 一些細節

105
00:04:18,989 --> 00:04:23,340
在這個圖形上

106
00:04:20,310 --> 00:04:26,970
在這個小量的訓練集區域

107
00:04:23,340 --> 00:04:29,700
這相對的演算法排名

108
00:04:26,970 --> 00:04:31,590
實際上不是那麼好定義, 如果

109
00:04:29,700 --> 00:04:34,500
所以如果您沒有很多的訓練集

110
00:04:31,590 --> 00:04:36,510
通常您的手工

111
00:04:34,500 --> 00:04:39,090
打造的特徵來決定

112
00:04:36,510 --> 00:04:41,910
表現，所以很有可能

113
00:04:39,090 --> 00:04:44,070
有人訓練支援向量機 (SVM) 是更

114
00:04:41,910 --> 00:04:46,320
傾向於手工打造特徵而

115
00:04:44,070 --> 00:04:48,300
有些人訓練大型的神經網路

116
00:04:46,320 --> 00:04:50,730
也許在這個小量訓練集

117
00:04:48,300 --> 00:04:53,130
區域 支援向量機可以做得更好

118
00:04:50,730 --> 00:04:55,020
所以在這個地區的左邊

119
00:04:53,130 --> 00:04:57,090
的圖形中

120
00:04:55,020 --> 00:04:59,550
至於演算法間的相對排序則沒有清楚的定義

121
00:04:57,090 --> 00:05:01,919
而效能多半取決於

122
00:04:59,550 --> 00:05:03,389
您在手工工程的技術

123
00:05:01,919 --> 00:05:05,970
以及演算法的低階細節

124
00:05:03,389 --> 00:05:08,850
只有在這個大數據的區域

125
00:05:05,970 --> 00:05:12,000
很大的訓練集

126
00:05:08,850 --> 00:05:14,669
在右邊的大的 m 區域

127
00:05:12,000 --> 00:05:17,639
比較一致的看到大的神經網路

128
00:05:14,669 --> 00:05:19,560
領先其他的演算法

129
00:05:17,639 --> 00:05:21,600
如果您的朋友問起，為什麼

130
00:05:19,560 --> 00:05:23,700
神經網路會起飛，我會

131
00:05:21,600 --> 00:05:26,729
鼓勵您畫這幅圖給他們看

132
00:05:23,700 --> 00:05:28,890
我會講說在

133
00:05:26,729 --> 00:05:29,310
當今深度學習的興起

134
00:05:28,890 --> 00:05:32,070
早期的時候

135
00:05:29,310 --> 00:05:34,919
因為有了大規模的資料

136
00:05:32,070 --> 00:05:36,330
大規模的計算力，剛好給了我們能夠

137
00:05:34,919 --> 00:05:39,479
訓練大型的神經網路

138
00:05:36,330 --> 00:05:41,850
在 CPU 或者 GPU 上，讓我們

139
00:05:39,479 --> 00:05:43,590
有極大的進展

140
00:05:41,850 --> 00:05:45,800
逐漸地，特別是最近幾年

141
00:05:43,590 --> 00:05:48,360
我們看到大量的

142
00:05:45,800 --> 00:05:50,539
演算法創新，我也

143
00:05:48,360 --> 00:05:53,700
不想低估這部分

144
00:05:50,539 --> 00:05:56,940
有趣的是很多的演算法

145
00:05:53,700 --> 00:06:01,139
創新試著讓

146
00:05:56,940 --> 00:06:03,510
神經網路跑得更快

147
00:06:01,139 --> 00:06:05,310
一個具體的例子

148
00:06:03,510 --> 00:06:08,729
一個重大突破在神經網路是

149
00:06:05,310 --> 00:06:12,330
從S型函數像這樣

150
00:06:08,729 --> 00:06:14,760
切換成為 ReLU 函數

151
00:06:12,330 --> 00:06:18,479
我們在早先的影片提過

152
00:06:14,760 --> 00:06:20,190
長得像這樣

153
00:06:18,479 --> 00:06:22,260
如果您不了解這些細節

154
00:06:20,190 --> 00:06:24,389
不用擔心

155
00:06:22,260 --> 00:06:26,010
實際上使用

156
00:06:24,389 --> 00:06:27,870
S型函數在機器學習時的問題是

157
00:06:26,010 --> 00:06:29,520
在這段區域中

158
00:06:27,870 --> 00:06:30,280
這裡函數的

159
00:06:29,520 --> 00:06:32,920
斜率

160
00:06:30,280 --> 00:06:35,350
也就是梯度，會幾乎為 0,  導致學習

161
00:06:32,920 --> 00:06:37,060
變得十分緩慢，因為當你

162
00:06:35,350 --> 00:06:39,639
建置梯度下降時，當梯度為

163
00:06:37,060 --> 00:06:41,470
0 時，參數的改變會非常

164
00:06:39,639 --> 00:06:44,740
緩慢，學習也就十分緩慢

165
00:06:41,470 --> 00:06:46,450
而當我們改變稱為

166
00:06:44,740 --> 00:06:48,600
神經網路的啟動函數

167
00:06:46,450 --> 00:06:52,060
使用這個函數稱為

168
00:06:48,600 --> 00:06:54,970
ReLU 函數 (Rectified Linear Unit) 函數

169
00:06:52,060 --> 00:06:57,070
R,E,L,U, 梯度等於

170
00:06:54,970 --> 00:07:00,220
1 對於所有輸入值為正數

171
00:06:57,070 --> 00:07:03,100
所以梯度比較不會

172
00:07:00,220 --> 00:07:04,750
逐漸縮為 0

173
00:07:03,100 --> 00:07:07,300
而這邊的梯度，這一條線的斜率

174
00:07:04,750 --> 00:07:09,520
為 0 在左半部，實際上

175
00:07:07,300 --> 00:07:12,580
只要將S型函數切換為

176
00:07:09,520 --> 00:07:14,410
ReLU 函數會讓

177
00:07:12,580 --> 00:07:16,960
演算法做梯度下降時

178
00:07:14,410 --> 00:07:19,169
作用得更快，所以這是

179
00:07:16,960 --> 00:07:22,030
一個例子，或許相對簡單

180
00:07:19,169 --> 00:07:23,860
的演算法創新，但是最終

181
00:07:22,030 --> 00:07:27,520
這個演算法創新的影響是

182
00:07:23,860 --> 00:07:29,080
它真的幫助了計算力，所以

183
00:07:27,520 --> 00:07:31,240
其實有相當多的例子像這樣

184
00:07:29,080 --> 00:07:33,340
我們改變了演算法

185
00:07:31,240 --> 00:07:35,140
因為這會讓我們的程式

186
00:07:33,340 --> 00:07:37,479
跑得更快，讓我們可以訓練

187
00:07:35,140 --> 00:07:39,520
更大的神經網路，或者在

188
00:07:37,479 --> 00:07:42,250
合理的時間內跑完，即使我們有

189
00:07:39,520 --> 00:07:45,810
相當大的網路，大量的資料

190
00:07:42,250 --> 00:07:48,610
另一個高速計算很重要的原因是

191
00:07:45,810 --> 00:07:51,070
實際上在整個

192
00:07:48,610 --> 00:07:53,710
訓練神經網路的過程是

193
00:07:51,070 --> 00:07:56,350
一個循環，通常您有一個想法

194
00:07:53,710 --> 00:07:58,020
對於神經網路的架構，所以

195
00:07:56,350 --> 00:08:01,060
您建置您的想法在程式裡

196
00:07:58,020 --> 00:08:02,830
建立您的想法然後讓您跑

197
00:08:01,060 --> 00:08:05,050
一個實驗，然後告訴您，

198
00:08:02,830 --> 00:08:07,510
您個神經網路效果如何，然後

199
00:08:05,050 --> 00:08:10,030
看著它，回頭改變一些

200
00:08:07,510 --> 00:08:12,930
神經網路的細節

201
00:08:10,030 --> 00:08:15,880
經過這樣的循環，一次又一次

202
00:08:12,930 --> 00:08:18,550
而當您的神經網路花太長的時間

203
00:08:15,880 --> 00:08:21,400
來訓練，就會花太長的時間來

204
00:08:18,550 --> 00:08:24,039
跑一次循環，這就會有巨大的

205
00:08:21,400 --> 00:08:26,740
不同生產力在於建立有效的

206
00:08:24,039 --> 00:08:29,560
神經網路，當您有一個

207
00:08:26,740 --> 00:08:34,169
想法，試試看，十分鐘後看著它

208
00:08:29,560 --> 00:08:36,370
的效果，或者最多一天

209
00:08:34,169 --> 00:08:39,490
相對於如果您訓練一個神經

210
00:08:36,370 --> 00:08:40,590
網路花了一個月，這

211
00:08:39,490 --> 00:08:42,570
有時候會發生

212
00:08:40,590 --> 00:08:44,670
因為您在

213
00:08:42,570 --> 00:08:47,250
十分鐘或者一天內得知結果

214
00:08:44,670 --> 00:08:49,170
您就可以試更多的想法，

215
00:08:47,250 --> 00:08:50,610
更能夠找到一個

216
00:08:49,170 --> 00:08:53,720
神經網路可以在您的應用上作用得很好

217
00:08:50,610 --> 00:08:57,900
所以快速的計算

218
00:08:53,720 --> 00:08:59,730
幫助了在

219
00:08:57,900 --> 00:09:02,610
您可以更快獲得實驗結果

220
00:08:59,730 --> 00:09:05,400
而這真的

221
00:09:02,610 --> 00:09:07,550
幫助了神經網路的參與者

222
00:09:05,400 --> 00:09:10,650
跟研究人員

223
00:09:07,550 --> 00:09:13,320
更快速地深度學習循環

224
00:09:10,650 --> 00:09:16,589
更快改進您的想法

225
00:09:13,320 --> 00:09:18,570
所以所有這些變成

226
00:09:16,589 --> 00:09:21,029
一個大的爆炸對於整個深度學習

227
00:09:18,570 --> 00:09:23,370
研究社群，一直

228
00:09:21,029 --> 00:09:25,620
很不可思議的發展

229
00:09:23,370 --> 00:09:28,920
新的演算法，創造不停止的

230
00:09:25,620 --> 00:09:30,990
進展，這些就是

231
00:09:28,920 --> 00:09:33,570
激勵了深度學習的興起的動力

232
00:09:30,990 --> 00:09:36,000
好消息是這些

233
00:09:33,570 --> 00:09:38,490
動力還在激勵讓

234
00:09:36,000 --> 00:09:41,130
深度學習更棒，技術資料

235
00:09:38,490 --> 00:09:43,800
社群還一直丟出更多的

236
00:09:41,130 --> 00:09:45,660
數位資料，或者讓計算力

237
00:09:43,800 --> 00:09:48,300
特別的硬體像是

238
00:09:45,660 --> 00:09:50,940
GPU 跟更快的網路的興起，很多的

239
00:09:48,300 --> 00:09:53,250
硬體，我真的很有信心

240
00:09:50,940 --> 00:09:55,140
我們有能力來做很大型的

241
00:09:53,250 --> 00:09:57,320
神經網路，從計算力的角度來看

242
00:09:55,140 --> 00:10:00,360
會越來越好，而

243
00:09:57,320 --> 00:10:02,880
演算法方面，整個深度學習

244
00:10:00,360 --> 00:10:05,070
研究社群持續地

245
00:10:02,880 --> 00:10:07,680
在驚人的創新

246
00:10:05,070 --> 00:10:09,839
演算法上，因為這樣

247
00:10:07,680 --> 00:10:11,370
我們可以樂觀地回答

248
00:10:09,839 --> 00:10:13,650
深度學習會

249
00:10:11,370 --> 00:10:14,120
持續的越來越好在

250
00:10:13,650 --> 00:10:17,100
未來幾年

251
00:10:14,120 --> 00:10:18,540
讓我們進入最後一段影片

252
00:10:17,100 --> 00:10:20,280
我們想談一談

253
00:10:18,540 --> 00:10:22,610
目前為止在這個課程

254
00:10:20,280 --> 00:10:22,610
您學到了什麼