1
00:00:00,060 --> 00:00:03,750
在上一段影片您看到如何去

2
00:00:01,890 --> 00:00:05,819
計算導數跟建置

3
00:00:03,750 --> 00:00:07,500
梯度下降相對於只有

4
00:00:05,819 --> 00:00:09,929
一個訓練例子的羅吉斯

5
00:00:07,500 --> 00:00:12,450
迴歸分析現在我們想用在 m 個

6
00:00:09,929 --> 00:00:14,429
訓練例子, 開始前我們

7
00:00:12,450 --> 00:00:17,460
先提醒自己一下, 

8
00:00:14,429 --> 00:00:19,380
成本函數J 的定義, 成本函數 w b

9
00:00:17,460 --> 00:00:22,699
是您關心的這個平均值 

10
00:00:19,380 --> 00:00:25,350
1 / m 總和從 i 等於 1 到 m

11
00:00:22,699 --> 00:00:29,519
的損失當您的演算法

12
00:00:25,350 --> 00:00:33,510
輸出 a i 在 y 例子上, 而

13
00:00:29,519 --> 00:00:36,120
a i 是預測值在我的

14
00:00:33,510 --> 00:00:40,620
訓練例子也就是 Sigma of z i

15
00:00:36,120 --> 00:00:46,800
也相等於 Sigma of W轉置 X i

16
00:00:40,620 --> 00:00:48,510
加 B, 我們在

17
00:00:46,800 --> 00:00:51,600
前一張投影片顯示過對於單一

18
00:00:48,510 --> 00:00:55,620
訓練例子如何計算

19
00:00:51,600 --> 00:01:00,180
導數, 當您只有一個

20
00:00:55,620 --> 00:01:03,809
訓練例子像 d w1 d w2 跟

21
00:01:00,180 --> 00:01:06,689
db 現在加上上標 i 來

22
00:01:03,809 --> 00:01:08,369
記為對應的值

23
00:01:06,689 --> 00:01:10,799
如果您做了我們在

24
00:01:08,369 --> 00:01:15,030
前一張投影片只用於單一

25
00:01:10,799 --> 00:01:17,850
訓練例子 xi yi 我用同樣的 i 

26
00:01:15,030 --> 00:01:20,759
在那邊, 所以現在您

27
00:01:17,850 --> 00:01:22,530
注意到全部成本函數是一個總和

28
00:01:20,759 --> 00:01:26,220
或者其實是平均數因為 1 

29
00:01:22,530 --> 00:01:29,369
/ m 項目,於每一個損失, 所以

30
00:01:26,220 --> 00:01:32,810
實際上這個導數相對於

31
00:01:29,369 --> 00:01:38,600
w1 的整體成本函數

32
00:01:32,810 --> 00:01:42,600
也會是平均數的

33
00:01:38,600 --> 00:01:46,170
導數相對於

34
00:01:42,600 --> 00:01:48,240
每一個損失項目, 但先前我們

35
00:01:46,170 --> 00:01:54,119
已經展示如何計算這個

36
00:01:48,240 --> 00:01:55,890
項目為 d w1, i 我們

37
00:01:54,119 --> 00:01:57,659
在前面的投影片顯示如何去

38
00:01:55,890 --> 00:02:00,450
計算在單一訓練例子

39
00:01:57,659 --> 00:02:03,119
所以您需要做的是計算

40
00:02:00,450 --> 00:02:04,680
這些導數像我們在

41
00:02:03,119 --> 00:02:07,350
前面顯示的訓練例子然後

42
00:02:04,680 --> 00:02:10,379
取平均值這會給您

43
00:02:07,350 --> 00:02:10,830
整個梯度您可以用來

44
00:02:10,379 --> 00:02:12,870
建置

45
00:02:10,830 --> 00:02:15,390
梯度下降, 我知道這裡有

46
00:02:12,870 --> 00:02:17,730
很多的細節但讓我們重新全部拿出來

47
00:02:15,390 --> 00:02:19,920
包成一個具體的

48
00:02:17,730 --> 00:02:21,690
演算法您可以用來建置

49
00:02:19,920 --> 00:02:24,960
羅吉斯迴歸分析使用

50
00:02:21,690 --> 00:02:28,350
梯度下降法, 您可以

51
00:02:24,960 --> 00:02:37,770
這樣做, 首先初始化 J 等於 0

52
00:02:28,350 --> 00:02:40,140
dw1 等於 0, dw2 等於 0 , db 等於

53
00:02:37,770 --> 00:02:43,190
0, 我們要做的是使用一個

54
00:02:40,140 --> 00:02:45,690
迴圈在這訓練集然後

55
00:02:43,190 --> 00:02:47,670
計算導數相對於每ㄧ個

56
00:02:45,690 --> 00:02:49,020
訓練例子然後加起來

57
00:02:47,670 --> 00:02:51,480
我們這樣做 for i

58
00:02:49,020 --> 00:02:54,360
等於 1 到 m, m 是訓練例子的數目

59
00:02:51,480 --> 00:02:57,090
我們計算Z i 等於 W

60
00:02:54,360 --> 00:03:00,360
轉置X i 加 B, 

61
00:02:57,090 --> 00:03:04,020
預測值 a i 等於 Sigma of Z i

62
00:03:00,360 --> 00:03:09,120
再來讓我們加總 J, J

63
00:03:04,020 --> 00:03:12,360
加等於 y i log a i 加 1 減

64
00:03:09,120 --> 00:03:14,010
y i log 1 減 a i , 然後放

65
00:03:12,360 --> 00:03:15,959
負號在整個前面

66
00:03:14,010 --> 00:03:20,580
然後我們看先前我們有

67
00:03:15,959 --> 00:03:28,500
d zi 等於 a i 減 y i 而

68
00:03:20,580 --> 00:03:33,180
dw 加等於 x1 i d z i, 而 d w2 

69
00:03:28,500 --> 00:03:35,280
加等於 x i 2 d z i, 我做這樣的

70
00:03:33,180 --> 00:03:37,680
計算假設您只有

71
00:03:35,280 --> 00:03:41,070
兩個特徵, 所以 n 等於 2

72
00:03:37,680 --> 00:03:45,480
要不然您要做 d w1, d w2, d w3 ...

73
00:03:41,070 --> 00:03:47,430
等等, 而 db 加等於 d z i 我想

74
00:03:45,480 --> 00:03:49,350
這是這個迴圈的終點

75
00:03:47,430 --> 00:03:51,900
最後做完這些對於所有 m

76
00:03:49,350 --> 00:03:54,959
訓練例子您會需要

77
00:03:51,900 --> 00:03:56,880
除以等於 m 因為我們計算

78
00:03:54,959 --> 00:04:01,920
平均值, 所以, d w1

79
00:03:56,880 --> 00:04:04,260
除以等於 m, d w2 除以等於 m, db

80
00:04:01,920 --> 00:04:07,019
除以等於 m, 所有計算

81
00:04:04,260 --> 00:04:09,060
平均值而有了這所有的這些

82
00:04:07,019 --> 00:04:11,160
計算, 您剛計算了

83
00:04:09,060 --> 00:04:14,250
這個成本函數的導數

84
00:04:11,160 --> 00:04:17,010
相對於每一個參數 w1, w2

85
00:04:14,250 --> 00:04:22,079
跟 b, 一點點細節的註解

86
00:04:17,010 --> 00:04:25,020
我們在做 d w1, d w2 跟 d b

87
00:04:22,079 --> 00:04:28,169
當作累加器, 所以在這

88
00:04:25,020 --> 00:04:31,500
計算之後您知道 d w1 等於

89
00:04:28,169 --> 00:04:33,509
您整個成本函數的導數

90
00:04:31,500 --> 00:04:36,780
相對於 w1, 而

91
00:04:33,509 --> 00:04:39,720
同樣的對於 d w2 跟 d b, 注意到

92
00:04:36,780 --> 00:04:41,520
d w1 跟 d w2 並沒有上標 i

93
00:04:39,720 --> 00:04:43,379
因為我們用他們在程式中當作

94
00:04:41,520 --> 00:04:45,690
累加器來加總這個

95
00:04:43,379 --> 00:04:48,960
訓練集而相對的 d z i 

96
00:04:45,690 --> 00:04:51,539
這是 d z 相對於

97
00:04:48,960 --> 00:04:53,490
單一訓練例子所以

98
00:04:51,539 --> 00:04:55,740
這是為什麼會有上標 i 來指到

99
00:04:53,490 --> 00:04:58,379
一個訓練例子 i 

100
00:04:55,740 --> 00:05:00,960
在計算中, 所以做完這些所有

101
00:04:58,379 --> 00:05:03,449
計算要來建置一步驟的

102
00:05:00,960 --> 00:05:06,360
梯度下降您建置了 w1

103
00:05:03,449 --> 00:05:10,710
更新為 w1 減 學習率

104
00:05:06,360 --> 00:05:13,740
乘上 d w1, w2 更新為 w2 減

105
00:05:10,710 --> 00:05:17,190
學習率乘 d w2, 而 b 

106
00:05:13,740 --> 00:05:21,000
更新為 b 減 學習率乘 db

107
00:05:17,190 --> 00:05:23,879
而 d w1, d w2 跟 d b 是這樣

108
00:05:21,000 --> 00:05:27,000
計算出來的, 而最後 J 這個

109
00:05:23,879 --> 00:05:28,590
會是正確的值對於您成本

110
00:05:27,000 --> 00:05:31,050
函數, 所以所有這個投影片

111
00:05:28,590 --> 00:05:33,060
建立了單一步驟的

112
00:05:31,050 --> 00:05:35,699
梯度下降所以您需要

113
00:05:33,060 --> 00:05:37,680
重複這個投影片每一個步驟許多次

114
00:05:35,699 --> 00:05:40,469
為了要做多次

115
00:05:37,680 --> 00:05:41,819
梯度下降法, 假設這些細節

116
00:05:40,469 --> 00:05:43,830
太過複雜

117
00:05:41,819 --> 00:05:45,960
現在不要擔心它

118
00:05:43,830 --> 00:05:48,599
希望所有這些會變得比較清楚

119
00:05:45,960 --> 00:05:50,520
當您真正實作在

120
00:05:48,599 --> 00:05:54,120
程式作業時, 但實際上

121
00:05:50,520 --> 00:05:57,300
會有兩個弱點在

122
00:05:54,120 --> 00:05:59,729
建置這裡時的運算

123
00:05:57,300 --> 00:06:01,440
也就是建立

124
00:05:59,729 --> 00:06:03,960
羅吉斯迴歸分析用這種方式您需要

125
00:06:01,440 --> 00:06:05,490
用兩個迴圈, 第一個迴圈

126
00:06:03,960 --> 00:06:07,770
是小迴圈經過所有 m 訓練

127
00:06:05,490 --> 00:06:10,919
例子而第二個迴圈是

128
00:06:07,770 --> 00:06:13,139
經過所有特徵這裡

129
00:06:10,919 --> 00:06:15,930
所以在這個例子我們只用兩個

130
00:06:13,139 --> 00:06:17,879
特徵所以 n 是 2,或者說 nx 等於 2

131
00:06:15,930 --> 00:06:21,000
但如果您有更多的特徵

132
00:06:17,879 --> 00:06:23,099
您最終會有d w1, d w2, 

133
00:06:21,000 --> 00:06:25,979
您有類似的計算對於 d w3

134
00:06:23,099 --> 00:06:29,009
等等直到  d wn, 所以似乎您

135
00:06:25,979 --> 00:06:31,279
需要有一個迴圈來過

136
00:06:29,009 --> 00:06:33,199
所有這 n 個特徵

137
00:06:31,279 --> 00:06:36,049
當您建置深度學習

138
00:06:33,199 --> 00:06:38,419
演算法時您發現用明顯的

139
00:06:36,049 --> 00:06:41,839
使用迴圈在您的程式會使得

140
00:06:38,419 --> 00:06:44,149
您的演算法效率比較不好,所以在

141
00:06:41,839 --> 00:06:46,669
深度學習中會邁向

142
00:06:44,149 --> 00:06:48,649
越來越大的資料集所以

143
00:06:46,669 --> 00:06:50,779
建置您的演算法

144
00:06:48,649 --> 00:06:52,969
不使用明顯的迴圈是

145
00:06:50,779 --> 00:06:55,129
很重要的會幫助您

146
00:06:52,969 --> 00:06:56,719
放大到大的資料集所以

147
00:06:55,129 --> 00:06:58,129
實際上有一種

148
00:06:56,719 --> 00:07:01,159
技巧稱為向量化

149
00:06:58,129 --> 00:07:03,559
技巧能讓您去除

150
00:07:01,159 --> 00:07:06,169
明顯的迴圈在程式中, 我

151
00:07:03,559 --> 00:07:08,199
想在神經網路時代以前

152
00:07:06,169 --> 00:07:11,239
也就是在神經網路興起之前

153
00:07:08,199 --> 00:07:13,159
向量化是可有可無, 您

154
00:07:11,239 --> 00:07:15,589
可能有時候這樣做來加速您的程式

155
00:07:13,159 --> 00:07:17,749
有時候不這樣做, 但在深度

156
00:07:15,589 --> 00:07:20,029
學習時代向量化也就是

157
00:07:17,749 --> 00:07:22,699
去除迴圈像這樣跟

158
00:07:20,029 --> 00:07:25,039
這樣變成很重要

159
00:07:22,699 --> 00:07:26,989
因為我們將訓練越來越

160
00:07:25,039 --> 00:07:29,239
大的資料集所以您真的

161
00:07:26,989 --> 00:07:31,209
需要讓您的程式很有效率

162
00:07:29,239 --> 00:07:34,219
在下幾個影片中我們將談論到

163
00:07:31,209 --> 00:07:37,339
向量化跟如何建置所有

164
00:07:34,219 --> 00:07:40,879
這些甚至不需要一個迴圈

165
00:07:37,339 --> 00:07:43,069
所以有了這些我希望您有點概念

166
00:07:40,879 --> 00:07:44,299
如何去建置羅吉斯迴歸分析

167
00:07:43,069 --> 00:07:46,339
或者說梯度下降法在羅吉斯

168
00:07:44,299 --> 00:07:47,959
迴歸分析, 事情會變得更清晰

169
00:07:46,339 --> 00:07:50,299
當您建立了程式作業

170
00:07:47,959 --> 00:07:51,829
但在真正做程式

171
00:07:50,299 --> 00:07:54,079
作業之前我們先談談

172
00:07:51,829 --> 00:07:56,419
向量化所以您可以建置

173
00:07:54,079 --> 00:07:58,369
這整個東西用一個單一

174
00:07:56,419 --> 00:08:01,479
遞迴的梯度下降而並不

175
00:07:58,369 --> 00:08:01,479
使用任何的迴圈