在上一段影片您看到如何去 計算導數跟建置 梯度下降相對於只有 一個訓練例子的羅吉斯 迴歸分析現在我們想用在 m 個 訓練例子, 開始前我們 先提醒自己一下, 成本函數J 的定義, 成本函數 w b 是您關心的這個平均值 1 / m 總和從 i 等於 1 到 m 的損失當您的演算法 輸出 a i 在 y 例子上, 而 a i 是預測值在我的 訓練例子也就是 Sigma of z i 也相等於 Sigma of W轉置 X i 加 B, 我們在 前一張投影片顯示過對於單一 訓練例子如何計算 導數, 當您只有一個 訓練例子像 d w1 d w2 跟 db 現在加上上標 i 來 記為對應的值 如果您做了我們在 前一張投影片只用於單一 訓練例子 xi yi 我用同樣的 i 在那邊, 所以現在您 注意到全部成本函數是一個總和 或者其實是平均數因為 1 / m 項目,於每一個損失, 所以 實際上這個導數相對於 w1 的整體成本函數 也會是平均數的 導數相對於 每一個損失項目, 但先前我們 已經展示如何計算這個 項目為 d w1, i 我們 在前面的投影片顯示如何去 計算在單一訓練例子 所以您需要做的是計算 這些導數像我們在 前面顯示的訓練例子然後 取平均值這會給您 整個梯度您可以用來 建置 梯度下降, 我知道這裡有 很多的細節但讓我們重新全部拿出來 包成一個具體的 演算法您可以用來建置 羅吉斯迴歸分析使用 梯度下降法, 您可以 這樣做, 首先初始化 J 等於 0 dw1 等於 0, dw2 等於 0 , db 等於 0, 我們要做的是使用一個 迴圈在這訓練集然後 計算導數相對於每ㄧ個 訓練例子然後加起來 我們這樣做 for i 等於 1 到 m, m 是訓練例子的數目 我們計算Z i 等於 W 轉置X i 加 B, 預測值 a i 等於 Sigma of Z i 再來讓我們加總 J, J 加等於 y i log a i 加 1 減 y i log 1 減 a i , 然後放 負號在整個前面 然後我們看先前我們有 d zi 等於 a i 減 y i 而 dw 加等於 x1 i d z i, 而 d w2 加等於 x i 2 d z i, 我做這樣的 計算假設您只有 兩個特徵, 所以 n 等於 2 要不然您要做 d w1, d w2, d w3 ... 等等, 而 db 加等於 d z i 我想 這是這個迴圈的終點 最後做完這些對於所有 m 訓練例子您會需要 除以等於 m 因為我們計算 平均值, 所以, d w1 除以等於 m, d w2 除以等於 m, db 除以等於 m, 所有計算 平均值而有了這所有的這些 計算, 您剛計算了 這個成本函數的導數 相對於每一個參數 w1, w2 跟 b, 一點點細節的註解 我們在做 d w1, d w2 跟 d b 當作累加器, 所以在這 計算之後您知道 d w1 等於 您整個成本函數的導數 相對於 w1, 而 同樣的對於 d w2 跟 d b, 注意到 d w1 跟 d w2 並沒有上標 i 因為我們用他們在程式中當作 累加器來加總這個 訓練集而相對的 d z i 這是 d z 相對於 單一訓練例子所以 這是為什麼會有上標 i 來指到 一個訓練例子 i 在計算中, 所以做完這些所有 計算要來建置一步驟的 梯度下降您建置了 w1 更新為 w1 減 學習率 乘上 d w1, w2 更新為 w2 減 學習率乘 d w2, 而 b 更新為 b 減 學習率乘 db 而 d w1, d w2 跟 d b 是這樣 計算出來的, 而最後 J 這個 會是正確的值對於您成本 函數, 所以所有這個投影片 建立了單一步驟的 梯度下降所以您需要 重複這個投影片每一個步驟許多次 為了要做多次 梯度下降法, 假設這些細節 太過複雜 現在不要擔心它 希望所有這些會變得比較清楚 當您真正實作在 程式作業時, 但實際上 會有兩個弱點在 建置這裡時的運算 也就是建立 羅吉斯迴歸分析用這種方式您需要 用兩個迴圈, 第一個迴圈 是小迴圈經過所有 m 訓練 例子而第二個迴圈是 經過所有特徵這裡 所以在這個例子我們只用兩個 特徵所以 n 是 2,或者說 nx 等於 2 但如果您有更多的特徵 您最終會有d w1, d w2, 您有類似的計算對於 d w3 等等直到  d wn, 所以似乎您 需要有一個迴圈來過 所有這 n 個特徵 當您建置深度學習 演算法時您發現用明顯的 使用迴圈在您的程式會使得 您的演算法效率比較不好,所以在 深度學習中會邁向 越來越大的資料集所以 建置您的演算法 不使用明顯的迴圈是 很重要的會幫助您 放大到大的資料集所以 實際上有一種 技巧稱為向量化 技巧能讓您去除 明顯的迴圈在程式中, 我 想在神經網路時代以前 也就是在神經網路興起之前 向量化是可有可無, 您 可能有時候這樣做來加速您的程式 有時候不這樣做, 但在深度 學習時代向量化也就是 去除迴圈像這樣跟 這樣變成很重要 因為我們將訓練越來越 大的資料集所以您真的 需要讓您的程式很有效率 在下幾個影片中我們將談論到 向量化跟如何建置所有 這些甚至不需要一個迴圈 所以有了這些我希望您有點概念 如何去建置羅吉斯迴歸分析 或者說梯度下降法在羅吉斯 迴歸分析, 事情會變得更清晰 當您建立了程式作業 但在真正做程式 作業之前我們先談談 向量化所以您可以建置 這整個東西用一個單一 遞迴的梯度下降而並不 使用任何的迴圈