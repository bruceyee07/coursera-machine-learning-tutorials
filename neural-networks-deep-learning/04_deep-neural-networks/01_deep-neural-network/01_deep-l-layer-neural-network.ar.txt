مرحبا بكم في الأسبوع الرابع من هذه الدورة التدريبية. قد تعلمتم الآن الانتشار
الأمامي والانتشار الخلفي فيما يتعلق الشبكة العصبية مع طبقة واحدة
مخفية، بالإضافة إلى الانحدار اللوجستي وقد تعرفتم على الطريقة المتجهية و متى يكون من الضروري
تهيئة الطرق عشوائيًا. وإذا قمتم بأداء الواجب المنزلي
للأسبوعين السابقين، فقد طبقتم ورأيتم بعض هذه
الأفكار بأنفسكم. فالآن لقد رأيتم فعليا معظم الأفكار التي
تحتاجونها لتطبيق شبكة عصبية عميقة. وما سنفعله هذا الأسبوع هو أننا
سنأخذ هذه الأفكار ونجمعها معًا كي كي تتمكنوا من تطبيق
شبكتكم العصبية العميقة. لأن تمرين هذا الأسبوع أطول ويتضمن بذل مجهود أكثر
وسأجعل مقاطع الفيديو لهذا الأسبوع أقصر كي تتمكنوا
من مشاهدتها بسرعة و ومن ثم سيكون لديكم وقت
أطول للعمل على التمرين الذي آمل أن يترككم مع أفكار عن الشبكة
العصبية العميقة تفخرون بها. إذن ما هي الشبكة العصبية العميقة؟ رأيتم هذه الصورة
في الانحدار اللوجستي ورأيتم أيضًا شبكات عصبية
بطبقة مخفية واحدة. لذا إليكم مثال عن شبكة
عصبية بطبقتين مخفيتين وشبكة عصبية بخمس طبقات مخفية. نعرِّف الانحدار اللوجستي
كنموذج "سطحي" للغاية. بينما هذا النموذج
هنا أكثر عمقًا و والتضاد بين السطحي والعميق
أمر يعتمد على الدرجة. إذن شبكة عصبية
بطبقة مخفية واحدة. وستكون هذه شبكة عصبية بطبقتين. تذكروا أنه عندما نعد الطبقات في
شبكة عصبية فلا نعد طبقة الإدخال. نعد فقط الطبقات المخفية
كما كانت طبقة الإخراج. إذن ما زالت هذه الشبكة العصبية
ذات الطبقتين سطحية إلى حد ما. لكن ليست بسطحية الانحدار اللوجستي. تقنيًا الانحدار اللوجستي هو
شبكة عصبية بطبقة واحدة. لكن خلال الأعوام السابقة
أدرك الذكاء الاصطناعي بمجتمع التعلم الآلي
 أن هناك دالات تستطيع الشبكات العصبية العميقة تعلمها
بينما تفشل النماذج الأكثر سطحية عن ذلك. على الرغم من أنه يصعب في
أي تمرين التنبؤ مسبقًا إلى أي تحتاج للتعمق في شبكتك. لذا سيكون من الأفضل تجربة
الانحدار اللوجستي. شبكة بطبقة واحدة، ثم طبقتين وتحديد
عدد الطبقات المخفية كمعلمة مفرطة أخرى يمكنك
تجربة عدة قيم لها و وتقييم كل ما يتوافق مع بيانات
التحقق من الصحة أو development set. وسنتكلم عن ذلك أكثر لاحقًا. لنتكلم الآن عن الترميز الذي استخدمناه
لوصف الشبكات العصبية العميقة. هذه شبكة عصبية مكونة
من واحد، اثنان، ثلاث، أربع طبقات. مع ثلاث طبقات مخفية وأظن
أن عدد الوحدات في هذه الطبقات المخفية هو 5، 5، 3، ثم
هناك وحدة واحدة علوية. لذا الترميز الذي سنستخدمه هو سنستخدم L بحرف كبير
للتعبير عن عدد الطبقات في الشبكة. إذن في هذه الحالة
L = 4 وكذلك عدد الطبقات و وسنستخدم N أس [l]
للتعبير عن عدد العقد أو عدد الوحدات في حالة
حرف l الصغير للطبقات. إذا دللنا على هذا إذن،
فالإدخال كالطبقة رقم "0". فهذه هي الطبقة رقم 1 وهذه
رقم 2 وهذه رقم 3 وهذه رقم 4. ثم سيكون لدينا على سبيل
المثال، n[1] أي هذا أول طبقة وستساوي 5
لأن لدينا 5 طبقات مخفية. أما لهذه فلدينا n[2] أي عدد الوحدات في
الطبقة المخفية الثانية والذي يساوي أيضًا 5، وn[3] = 3 و n[4] = n[L]. عدد
الوحدات العلوية هو 1 لأن حرف L الكبير يساوي أربعة. وسيكون لدينا هنا لطبقة الإدخال n[0] = nx = 3. إذن هذا هو الترميز الذي سنستخدمه
لوصف عدد العقد التي لدينا في الطبقات المختلفة. لكل L طبقة، سنستخدم أيضًا a[l] للتعبير عن المنشطات في l طبقة. سنرى لاحقًا أنه في الانتشار سننتهي بحساب a[l]
كالمنشط g(z[l]) وربما المنشط مرفوع
بالأس l للطبقة أيضًا ثم سنستخدم W[l]
للتعبير عن الأوزان لاحتساب القيمة z[l] في l للطبقة وبنفس الطريقة تستخدم b[l] لاحتساب z [l]. وأخيرًا لتلخيص الترميز،
تسمى سمات الإدخال x لكن x أيضًا هي المنشطات
للطبقة صفر. إذن a[0] = x والمنشط للطبقة الأخيرة
هو a[L] = y-hat. إذن a[L] تساوي الإخراج المتوقع
للتوقع y-hat للشبكة العصبية. الآن تعرفون كيف يبدو
شكل الشبكة العصبية العميقة. وأيضًا الترميز المستخدم لوصف
الشبكات العميقة واحتسابها. أعرف أننا شرحنا الكثير من الرموز
في هذا الفيديو، لكن إذا نسيتم يومًا ما الذي تعنيه بعض الرموز، فقد نشرناها
على موقع الدورة في ورقة للترميز أو دليل ترميز يمكنكم استخدامه
للبحث عما تعنيه هذه الرموز المختلفة. أما تاليًا، فأود شرح كيف يبدو
الانتشار الأمامي في هذا النوع من الشبكات. لننتقل إلى الفيديو التالي.