1
00:00:03,182 --> 00:00:06,580
欢迎Rus！很高兴今天你可以来。

2
00:00:06,580 --> 00:00:08,370
谢谢Andrew。

3
00:00:08,370 --> 00:00:11,696
你现在是苹果研究院的一个总监，

4
00:00:11,696 --> 00:00:16,720
同时你也是卡耐基梅隆大学（CMU）的教授，

5
00:00:16,720 --> 00:00:20,050
我想听你讲一下你的个人经历。

6
00:00:20,050 --> 00:00:24,350
你是如何最终走上了现在的道路的呢?

7
00:00:24,350 --> 00:00:27,930
噢，这，从某种程度上，

8
00:00:27,930 --> 00:00:32,040
我开始从事深度学习工作是比较偶然的。

9
00:00:32,040 --> 00:00:35,710
在多伦多大学完成了硕士学位后
我离开学校了一年

10
00:00:35,710 --> 00:00:37,860
那一年里，我在金融行业工作。

11
00:00:37,860 --> 00:00:40,161
我自己想起来也有点惊讶。

12
00:00:40,161 --> 00:00:44,047
在那个时候,我不太确定是否要去攻读博士学位
或者做点什么其他。

13
00:00:44,047 --> 00:00:46,110
然后发生了一件事情，一年意外的事情

14
00:00:46,110 --> 00:00:50,641
一天早上我要去上班,我碰见了Geoff Hinton

15
00:00:50,641 --> 00:00:55,153
Geoff对我说：“我有一个很好的点子“

16
00:00:55,153 --> 00:00:56,810
“到我办公室来,我给你讲讲。“

17
00:00:56,810 --> 00:01:01,205
然后，我们就一边走一边聊
他开始跟我聊起

18
00:01:01,205 --> 00:01:06,400
波尔兹曼机，对比分歧

19
00:01:06,400 --> 00:01:09,080
还有一些我当时不太明白的技术

20
00:01:10,300 --> 00:01:15,320
但是我听了后，对那些东西非常感兴趣。

21
00:01:15,320 --> 00:01:20,340
然后，基本上,在三月之内
我开始跟Geoff攻读博士学位。

22
00:01:21,410 --> 00:01:28,743
就是那样开始的
那是在2005，2006年前后

23
00:01:28,743 --> 00:01:32,766
就是那个时候，好些最初的深度学习的算法

24
00:01:32,766 --> 00:01:37,810
例如受限波尔兹曼机（RBM）
无监督的预训练等开始出现了

25
00:01:37,810 --> 00:01:41,969
我就是这样开始进入深度学习这个领域的。

26
00:01:41,969 --> 00:01:46,181
那个偶遇Geoff的早上

27
00:01:46,181 --> 00:01:48,990
完完全全地改变了我未来的事业方向。

28
00:01:48,990 --> 00:01:52,374
然后事实上你是

29
00:01:52,374 --> 00:01:55,070
最早的使得神经网络和深度学习

30
00:01:55,070 --> 00:02:00,430
重新受到关注的
关于受限玻尔兹曼机的论文的一个作者。

31
00:02:00,430 --> 00:02:03,359
请多告诉我一点
从事那个有深远影响的研究是什么样的感觉

32
00:02:03,359 --> 00:02:06,217
对，这确实是

33
00:02:06,217 --> 00:02:10,992
非常令人兴奋的。那是我的读博的第一年

34
00:02:10,992 --> 00:02:11,960
Geoff和我

35
00:02:11,960 --> 00:02:17,505
尝试去探索使用受限玻尔兹曼机和预训练技巧

36
00:02:17,505 --> 00:02:21,680
来训练多层网络的一些想法。

37
00:02:21,680 --> 00:02:25,934
特别是我们试图集中精力研究自编码器

38
00:02:25,934 --> 00:02:29,880
还有如何有效的对PCA做非线性的扩展

39
00:02:29,880 --> 00:02:34,416
这很令人兴奋
因为有了这些系统我们可以工作了

40
00:02:34,416 --> 00:02:37,296
这是令人兴奋的,但接下来我们需要做的是

41
00:02:37,296 --> 00:02:42,062
我们是否真的可以将这些模型扩展到处理人脸。

42
00:02:42,062 --> 00:02:45,069
我记得我们有一个Olivetti人脸数据集

43
00:02:45,069 --> 00:02:48,000
然后我们又开始看我们是否能做图像压缩

44
00:02:48,000 --> 00:02:52,576
我们开始看各种不同的数据

45
00:02:52,576 --> 00:02:57,152
实数、正整数、二进制，这些都是

46
00:02:57,152 --> 00:03:03,672
在我的读博第一年之内做的
对我而言确实是一个很重要的学习经验

47
00:03:03,672 --> 00:03:06,310
但是其实就在6、7个月的时间内

48
00:03:06,310 --> 00:03:11,079
我们就已经取得了很有趣的结果
我的意思是，很好的结果。

49
00:03:11,079 --> 00:03:14,765
我们可以训练这些非常多层的自编码器

50
00:03:14,765 --> 00:03:19,195
而那些是当时你用传统的

51
00:03:19,195 --> 00:03:22,077
优化技术所无法做到的。

52
00:03:22,077 --> 00:03:26,965
那一段时间最后证实了
确实是我们的一段非常辉煌的时期。

53
00:03:27,970 --> 00:03:33,520
那是非常让我兴奋的时光，因为我学到很多

54
00:03:33,520 --> 00:03:38,230
同时，对于是我们想要达到的结果而言，

55
00:03:38,230 --> 00:03:40,710
研究结果也是非常让人瞩目。

56
00:03:42,210 --> 00:03:45,360
就是说，在深度学习的早期，

57
00:03:45,360 --> 00:03:49,760
很多的研究活动是以受限波尔兹曼机为中心的

58
00:03:49,760 --> 00:03:52,340
然后就是深度波尔兹曼机

59
00:03:52,340 --> 00:03:54,842
当然，还有很多其他让人振奋的研究

60
00:03:54,842 --> 00:03:58,228
包括你们小组的一些研究
但是，到底后来波尔兹曼机

61
00:03:58,228 --> 00:03:59,663
和受限波尔兹曼机出现了什么情况呢？

62
00:03:59,663 --> 00:04:00,900
这是一个很好的问题

63
00:04:00,900 --> 00:04:01,710
我想，

64
00:04:01,710 --> 00:04:07,032
在深度学习的早期，
我们使用受限波尔兹曼机的方式是

65
00:04:07,032 --> 00:04:10,940
你基本生可以想象
就是训练多层的受限波尔兹曼机

66
00:04:10,940 --> 00:04:14,715
那可以使你有效的一层一层的学习

67
00:04:14,715 --> 00:04:17,928
这背后有一个很好的理论证明
当你添加一个特定的层时,

68
00:04:17,928 --> 00:04:21,010
在一定条件下,它可以被证明是具有变分上限的，等等。

69
00:04:21,010 --> 00:04:24,414
因此,这后面是有很好的理论根据的,并且这些模型

70
00:04:24,414 --> 00:04:28,697
可以用来预训练的深度学习系统，
在当时是相当好用的

71
00:04:28,697 --> 00:04:35,013
但是，在大约2009，2010年左右，
当以GPU引入的计算能力开始突然发展

72
00:04:35,013 --> 00:04:41,618
我们很多人开始认识到，事实上直接优化

73
00:04:41,618 --> 00:04:47,760
这些深度神经网络可以得到相似甚至更好的结果。

74
00:04:47,760 --> 00:04:50,336
那就是说可以不需要做预训练或者使用受限波尔兹曼机，

75
00:04:50,336 --> 00:04:51,824
直接做标准的反向传播。

76
00:04:51,824 --> 00:04:52,700
你说的没错

77
00:04:52,700 --> 00:04:56,180
而且那只经过了3到4年到时间
那就是为什么

78
00:04:56,180 --> 00:04:58,050
整个学界都感到很兴奋
因为人们感觉到：Wow

79
00:04:58,050 --> 00:05:02,460
你原来可以用那些预训练机制来训练深度模型，

80
00:05:02,460 --> 00:05:06,930
然后随着计算能力的增强，人们开始认识到

81
00:05:06,930 --> 00:05:10,580
你基本上可以直接做基本的反向传播，
那是在2005或者2004年所无法做到的

82
00:05:11,660 --> 00:05:17,240
因为如果使用传统CPU来做，会需要数月的时间

83
00:05:17,240 --> 00:05:20,330
所以那是一个巨大的变化。

84
00:05:20,330 --> 00:05:24,050
另外，我认为我们还没有真的想出

85
00:05:24,050 --> 00:05:28,108
如何去用好波尔兹曼机和深度波尔兹曼机

86
00:05:28,108 --> 00:05:29,650
我相信他们是非常强大的模型，

87
00:05:29,650 --> 00:05:32,010
因为你可以把他们想象是一种生成模型

88
00:05:32,010 --> 00:05:36,200
他们试图对数据的耦合分布建模，但

89
00:05:36,200 --> 00:05:40,440
当我们开始看学习算法的时候，现在的学习算法

90
00:05:40,440 --> 00:05:45,000
他们需要使用马尔可夫链蒙特卡罗方法和变分学习等

91
00:05:45,000 --> 00:05:50,170
这些方法并没有反向传播算法那样的可扩展性

92
00:05:50,170 --> 00:05:55,480
因此,我们还需要找出更有效的方法来训练这些模型,

93
00:05:55,480 --> 00:05:57,920
还有卷积的使用,

94
00:05:57,920 --> 00:06:02,830
这是相当难以整合到这些模型的东西。

95
00:06:02,830 --> 00:06:07,500
我记得你的一些使用概率最大池化

96
00:06:07,500 --> 00:06:12,020
来构建不同对象的生成模型的研究工作

97
00:06:12,020 --> 00:06:16,970
和使用这些卷积的思想也非常令人兴奋

98
00:06:16,970 --> 00:06:20,630
但是，同样的，训练这些模型仍然非常困难,

99
00:06:20,630 --> 00:06:21,488
所以这些东西行得通的可能性是多大？

100
00:06:21,488 --> 00:06:22,720
是的，行得通的可能性是多大，对吧？

101
00:06:22,720 --> 00:06:24,990
所以我们还得继续弄清楚。

102
00:06:27,750 --> 00:06:31,185
在另一方面，例如
一些最近的研究工作使用变分自编码

103
00:06:31,185 --> 00:06:35,260
可以将其视为玻尔兹曼机的交互式版本。

104
00:06:35,260 --> 00:06:40,491
我们已经找到了训练这些模块的方法
那是Max Welling和

105
00:06:40,491 --> 00:06:44,800
Diederik Kingma
关于使用再参数化（reparameterization）技术的研究

106
00:06:44,800 --> 00:06:50,720
现在我们可以在随机系统中使用反向算法,

107
00:06:50,720 --> 00:06:53,630
现在正推动着很多研究进步

108
00:06:53,630 --> 00:06:59,199
但是,在玻尔兹曼机的情况下,我们还没有想到如何那样做

109
00:06:59,199 --> 00:07:04,606
所以这实际上是一个我以前不了解的非常有趣的观点

110
00:07:04,606 --> 00:07:09,382
那就是，在早期的计算机速度较慢的时代,RBM，

111
00:07:09,382 --> 00:07:11,969
预训练等技术对于深度学习系统是非常的重要

112
00:07:11,969 --> 00:07:17,920
只有更快的计算驱动我们切换到标准的反向传播。

113
00:07:17,920 --> 00:07:21,370
至于深度学习社区的思维的演变和

114
00:07:21,370 --> 00:07:24,712
其他话题,
我知道你花了很多时间思考这个问题,

115
00:07:24,712 --> 00:07:28,870
生成的、无监督的与监督的方法的对比

116
00:07:28,870 --> 00:07:32,718
你是否可以分享一下你对此的想法是如何随着时间演变的？

117
00:07:32,718 --> 00:07:37,407
我觉得这是一个非常重要的话题,

118
00:07:37,407 --> 00:07:41,727
特别是如果我们考虑无监督,半监督或

119
00:07:41,727 --> 00:07:46,493
生成模型,因为在某种程度上
最近我们已经看到了很多的成功

120
00:07:46,493 --> 00:07:50,812
是来自于有监督的学习,而在早期的时候,

121
00:07:50,812 --> 00:07:55,803
无监督学习主要被视为无监督的预训练

122
00:07:55,803 --> 00:07:59,850
因为我们不知道怎样训练这些多层系统。

123
00:07:59,850 --> 00:08:04,320
即使在今天,如果你要解决的问题中有

124
00:08:04,320 --> 00:08:08,488
大量未标记的数据和一小部分标记的样例,

125
00:08:08,488 --> 00:08:11,371
这些无监督的预训练模型

126
00:08:11,371 --> 00:08:15,721
建立这些生成模型,可以对有监督学习有所帮助

127
00:08:15,721 --> 00:08:20,863
所以我认为，当我开始读博士的时候
我们社区中的很多人，

128
00:08:20,863 --> 00:08:25,838
都深信生成模型，并试图学习

129
00:08:25,838 --> 00:08:30,900
一系列的这些模型,因为这是我们训练这些系统的唯一途径。

130
00:08:30,900 --> 00:08:35,720
如今,有很多关于生成建模的研究工作

131
00:08:35,720 --> 00:08:38,530
如果你看一下生成对抗网络（GAN），

132
00:08:38,530 --> 00:08:40,397
如果你看一下变分自编码,

133
00:08:40,397 --> 00:08:45,620
深度能量模型，
这也是我的实验室正在研究的,

134
00:08:45,620 --> 00:08:50,660
我认为这是非常令人兴奋的研究
但也许我们还没有完全搞通,

135
00:08:50,660 --> 00:08:55,420
同样,对于你们中的许多
正在考虑进入深入的学习领域的人而言，

136
00:08:55,420 --> 00:08:59,340
我希望，在不久的将来，这是一个

137
00:08:59,340 --> 00:09:01,187
我们会取得很大的进展的领域,

138
00:09:01,187 --> 00:09:02,214
无监督学习

139
00:09:02,214 --> 00:09:04,281
对，无监督学习

140
00:09:04,281 --> 00:09:07,751
或者你可以把它看成是这样的无监督的学习,或者

141
00:09:07,751 --> 00:09:12,886
半监督学习，
你有一些提示或者大概表示不同含义的样本

142
00:09:12,886 --> 00:09:18,240
然后还有大量的无标记的样例

143
00:09:18,240 --> 00:09:21,506
实际上，在深度学习较早期，
计算机还没有现在这么快的时候，

144
00:09:21,506 --> 00:09:23,929
一个非常重要的观察是，

145
00:09:23,929 --> 00:09:27,763
我们需要通过受限玻尔兹曼机
和深度玻尔兹曼机

146
00:09:27,763 --> 00:09:31,257
来初始化神经网络的权重，
然后计算机变得越来越快

147
00:09:31,257 --> 00:09:34,700
直接使用反向传播开始得到更好的效果

148
00:09:34,700 --> 00:09:39,220
我知道你花了很多时间思考的
另外一个课题是

149
00:09:39,220 --> 00:09:45,342
监督学习和生成模型、
无监督学习方法的比较

150
00:09:45,342 --> 00:09:46,619
那你可不可以

151
00:09:46,619 --> 00:09:51,920
告诉我们你对于这个辩论的看法

152
00:09:51,920 --> 00:09:56,780
我想我们都相信我们应该能够在那里取得进展。

153
00:09:56,780 --> 00:10:03,990
那些玻尔兹曼机，变分自编码，GAN

154
00:10:03,990 --> 00:10:08,556
这些大都被人们认为是生成模型，但是

155
00:10:08,556 --> 00:10:13,595
我们还没有弄清楚如何有效的运用它们

156
00:10:13,595 --> 00:10:16,654
如何使用大的矩

157
00:10:16,654 --> 00:10:21,797
甚至我在IT行业中经常看到，
很多公司有很多

158
00:10:21,797 --> 00:10:26,463
很多的数据，无标签的数据，
这些公司很多的

159
00:10:26,463 --> 00:10:30,848
做法是通过给数据加标注,
因为这是现在唯一的

160
00:10:30,848 --> 00:10:33,350
可以让我们往前推进的办法

161
00:10:33,350 --> 00:10:36,400
并且因为我们有大量的无标注的数据，

162
00:10:36,400 --> 00:10:40,200
看起来我们应该能够利用它们，但是

163
00:10:40,200 --> 00:10:42,190
事实上我们还没有搞清楚
如何可以很好地使用这些数据

164
00:10:44,020 --> 00:10:48,300
你提到，对于想进入深度学习研究的人，

165
00:10:48,300 --> 00:10:50,920
无监督学习是令人兴奋的领域。

166
00:10:50,920 --> 00:10:54,240
今天有很多人想进入深度学习领域,

167
00:10:54,240 --> 00:10:57,688
做研究或者应用性的工作，
那对于这样的一个全球性的社区

168
00:10:57,688 --> 00:11:01,490
无论是研究还是应用工作,你会有什么建议？

169
00:11:01,490 --> 00:11:06,680
我认为其中一个关键的建议

170
00:11:06,680 --> 00:11:10,620
我应该对想要进入这个领域的人说的

171
00:11:10,620 --> 00:11:14,210
我会鼓励他们尝试不同的东西,

172
00:11:14,210 --> 00:11:18,280
不要害怕尝试新事物,也不要害怕尝试创新。

173
00:11:18,280 --> 00:11:20,135
我可以举一个例子

174
00:11:20,135 --> 00:11:24,975
当我还是个研究生的时候,我们正在研究神经网络,

175
00:11:24,975 --> 00:11:29,680
这些都是高度的非凸（non-convex）的系统,
很难优化。

176
00:11:29,680 --> 00:11:33,780
我记得和我在优化领域中的朋友聊天

177
00:11:33,780 --> 00:11:38,350
我听到的总是，啊，你无法解决这些问题

178
00:11:38,350 --> 00:11:41,600
因为这些都不是凸问题,我们也不太懂优化,

179
00:11:41,600 --> 00:11:46,470
不像是做凸优化，
你怎么可以优化这些非凸的问题呢？

180
00:11:46,470 --> 00:11:51,225
但令人惊讶的是,因为在我们的实验室,
我们从来没有真正

181
00:11:51,225 --> 00:11:55,520
关注那些具体的问题是不是凸问题

182
00:11:55,520 --> 00:11:58,005
我们只是思考如何才能优化和

183
00:11:58,005 --> 00:11:59,960
我们是否能得到有趣的结果。

184
00:11:59,960 --> 00:12:04,150
真是那样的精神在有效地推动社区的进步，

185
00:12:04,150 --> 00:12:09,038
我们不害怕,也许在某种程度上是因为我们

186
00:12:09,038 --> 00:12:13,444
缺乏对优化背后的理论的知识。

187
00:12:13,444 --> 00:12:16,123
但我会鼓励人们勇敢地尝试

188
00:12:16,123 --> 00:12:19,200
不要害怕去尝试解决困难的问题。

189
00:12:19,200 --> 00:12:22,616
是的,我记得你曾经说过,
不要只是学习如何写深度学习框架层面上的代码，

190
00:12:22,616 --> 00:12:25,740
而要实际上了解深度学习的理论。

191
00:12:25,740 --> 00:12:26,370
是，对的。

192
00:12:26,370 --> 00:12:30,992
我尝试做的一件事情是,
当我教深度学习的课程的时候，

193
00:12:30,992 --> 00:12:35,182
其中的一个作业，我要求学生去写代码实现

194
00:12:35,182 --> 00:12:39,323
卷积神经网络中的反向传播算法

195
00:12:39,323 --> 00:12:43,379
它是痛苦的,但同时,如果你做过一次,

196
00:12:43,379 --> 00:12:48,510
您将真正了解这些系统的工作方式。

197
00:12:49,540 --> 00:12:53,223
以及如何有效地在GPU上实现它们,以及

198
00:12:53,223 --> 00:12:58,266
我想，这对你进入研究或工业应用来说，都是重要的。

199
00:12:58,266 --> 00:13:03,013
如果你对这些系统是如何工作有很好的理解。

200
00:13:03,013 --> 00:13:05,450
所以我认为这很重要。

201
00:13:05,450 --> 00:13:09,160
因为你既有教授的学术经验,同时

202
00:13:09,160 --> 00:13:13,730
也有公司工作的经验,
我很好奇,如果有人想进入深入学习,

203
00:13:13,730 --> 00:13:18,290
做博士与加入公司有什么优缺点？

204
00:13:18,290 --> 00:13:21,290
这其实是一个很好的问题。

205
00:13:22,660 --> 00:13:25,780
在我的实验室里,我有很多不同的学生

206
00:13:25,780 --> 00:13:28,850
有些学生想去走学术路线，

207
00:13:28,850 --> 00:13:32,041
一些学生想去企业工作

208
00:13:32,041 --> 00:13:38,290
它变得非常有挑战性,因为你可以在企业里做很好的研究,

209
00:13:38,290 --> 00:13:41,910
你也可以在学术界做很好的研究。

210
00:13:41,910 --> 00:13:46,480
至于利弊，在学术界，

211
00:13:46,480 --> 00:13:53,180
我觉得你有更多的自由可以投入到长期的研究，或者你

212
00:13:53,180 --> 00:13:59,150
可以投入到一个疯狂的想法的研究中，
所以说你选择研究方向的自由度更高

213
00:13:59,150 --> 00:14:03,940
同时,你在工业界做的研究也很令人兴奋

214
00:14:03,940 --> 00:14:08,920
因为在许多情况下,如果你开发出一个核心的AI技术，

215
00:14:08,920 --> 00:14:14,470
你的研究可以影响数以百万计的用户。

216
00:14:14,470 --> 00:14:19,473
显然,在工业界内,你有很多

217
00:14:19,473 --> 00:14:26,120
在计算方面的资源,并能够做真正惊人的事情。

218
00:14:26,120 --> 00:14:30,260
所以两者都有优缺点,怎么选择主要取决于你想做什么。

219
00:14:30,260 --> 00:14:32,630
现在是一个很有趣环境

220
00:14:32,630 --> 00:14:36,860
很多人从学术界转向工业届,

221
00:14:36,860 --> 00:14:40,450
然后,也有少数人从工业界转移到学术界。

222
00:14:40,450 --> 00:14:45,756
因此,这是非常激动人心的时期。

223
00:14:45,756 --> 00:14:49,244
听起来好像学术届的机器学习也好，工业界的机器学习也好

224
00:14:49,244 --> 00:14:52,800
最重要的事情只是去做，对吧

225
00:14:52,800 --> 00:14:54,070
学术研究也好，工业研发也好，只管去做

226
00:14:54,070 --> 00:14:58,870
它真的取决于你的喜好,因为在那两个地方

227
00:14:58,870 --> 00:14:59,800
你都可以做惊人的研究

228
00:14:59,800 --> 00:15:03,301
你提到无监督学习是一个激动人心的

229
00:15:03,301 --> 00:15:04,260
研究前沿

230
00:15:04,260 --> 00:15:08,850
是否有其他领域,你认为令人兴奋的研究前沿？

231
00:15:08,850 --> 00:15:09,700
—不用谢

232
00:15:09,700 --> 00:15:12,520
我想现在在社区里看到的

233
00:15:12,520 --> 00:15:16,010
尤其是在深度学习的领域中,有几个趋势。

234
00:15:17,400 --> 00:15:20,390
一个我认为是非常令人兴奋的方向是

235
00:15:20,390 --> 00:15:22,909
是深度强化学习的领域

236
00:15:24,110 --> 00:15:28,940
因为我们能够搞懂
如何在虚拟世界中训练智能主体

237
00:15:28,940 --> 00:15:33,633
这是在短短的几年里,你看到了很多

238
00:15:33,633 --> 00:15:38,251
很多进步,我们如何扩展这些系统,我们如何能开发

239
00:15:38,251 --> 00:15:42,643
新的算法，如何可以使得智能主体可以相互沟通

240
00:15:42,643 --> 00:15:46,731
我认为这个防线是,总的来说,

241
00:15:46,731 --> 00:15:52,004
与环境交互的问题是超级激动人心的。

242
00:15:52,004 --> 00:15:55,230
另外一个我认为是真正令人兴奋的领域

243
00:15:55,230 --> 00:16:00,720
是推理和自然语言理解

244
00:16:00,720 --> 00:16:03,810
我们能建立对话系统吗？

245
00:16:03,810 --> 00:16:09,120
我们可以建立可以推理的系统吗？可以读取文本和

246
00:16:09,120 --> 00:16:12,730
能够智能地回答问题吗？

247
00:16:12,730 --> 00:16:17,670
我认为这是目前很多研究的重点。

248
00:16:17,670 --> 00:16:21,832
然后还有另外一些子领域

249
00:16:21,832 --> 00:16:26,382
是关于可以从少量的例子中学习

250
00:16:26,382 --> 00:16:31,210
一般人认为它是一次（one-shot）学习或迁移学习,

251
00:16:31,210 --> 00:16:36,970
这种学习方式学习到一些关于这个世界的东西

252
00:16:36,970 --> 00:16:41,500
然后我给你一个新的任务的时候,
你可以很快地解决这个任务。

253
00:16:41,500 --> 00:16:46,770
就像人类的学习一样,不需要大量的有标签的样例

254
00:16:46,770 --> 00:16:52,051
这些就是我们这个社区里很多人都在试图

255
00:16:52,051 --> 00:16:58,010
找出如何可以做到的事情，
如何可以使机器学习更加接近人类的学习能力

256
00:16:58,010 --> 00:17:00,790
谢谢你,Rus,分享所有的评论和见解。

257
00:17:00,790 --> 00:17:02,205
听到你的故事

258
00:17:02,205 --> 00:17:04,870
以及你早期的深度学习工作，真是非常有意思

259
00:17:04,870 --> 00:17:05,660
谢谢,Andrew

260
00:17:07,100 --> 00:17:07,800
谢谢你们邀请我来
GTC字幕组 翻译