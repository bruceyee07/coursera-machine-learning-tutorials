1
00:00:03,182 --> 00:00:06,580
歡迎魯斯，很高興
今天您可以加入我們的討論

2
00:00:06,580 --> 00:00:08,370
>> 謝謝您

3
00:00:08,370 --> 00:00:11,696
>> 今天你是蘋果公司的研究總監

4
00:00:11,696 --> 00:00:16,720
你還是一個教授,
在卡內基-梅隆大學

5
00:00:16,720 --> 00:00:20,050
跟大家分享一下您個人的故事

6
00:00:20,050 --> 00:00:24,350
您怎麼會從事這項深度學習的工作？

7
00:00:24,350 --> 00:00:27,930
是啊，某種程度上

8
00:00:27,930 --> 00:00:32,040
某種程度上是靠運氣
開始深度學習的

9
00:00:32,040 --> 00:00:35,710
我在多倫多完成碩士學位, 
然後我休假了一年

10
00:00:35,710 --> 00:00:37,860
實際上，
我是在金融部門工作

11
00:00:37,860 --> 00:00:40,161
有點意外

12
00:00:40,161 --> 00:00:44,047
在那個時候, 
我不太確定是否要去攻讀博士學位

13
00:00:44,047 --> 00:00:46,110
然後發生了一些意外的事情

14
00:00:46,110 --> 00:00:50,641
一天早上我要去上班, 
我碰見了 Geoff Hinton

15
00:00:50,641 --> 00:00:55,153
傑夫告訴我, 嘿, 
我有一個絕妙的主意

16
00:00:55,153 --> 00:00:56,810
到我辦公室來, 
我來告訴你

17
00:00:56,810 --> 00:01:01,205
所以，我們一起走，
然後他開始告訴我有關

18
00:01:01,205 --> 00:01:06,400
玻爾茲曼機和對比的分歧, 
和一些技巧

19
00:01:06,400 --> 00:01:09,080
當時我不太明白他在說什麼

20
00:01:10,300 --> 00:01:15,320
但是, 真的, 真的很興奮, 
真的讓我興奮

21
00:01:15,320 --> 00:01:20,340
然後基本上, 在三月內, 
我跟隨傑夫開始了我的博士學位

22
00:01:21,410 --> 00:01:28,743
所以就是這樣開始的, 
那是早在 2005年, 2006年

23
00:01:28,743 --> 00:01:32,766
是一些原始的深度學習演算法

24
00:01:32,766 --> 00:01:37,810
使用受限玻爾茲曼機，
非監督式預先訓練，開始出現

25
00:01:37,810 --> 00:01:41,969
這就是我的開始點，真的

26
00:01:41,969 --> 00:01:46,181
那個特別的早上，我遇見傑夫，

27
00:01:46,181 --> 00:01:48,990
完全改變了
我的未來事業生涯

28
00:01:48,990 --> 00:01:52,374
>> 實際上, 你是一個非常早的共同作者

29
00:01:52,374 --> 00:01:55,070
在受限玻爾茲曼機器的論文

30
00:01:55,070 --> 00:02:00,430
真的幫助了神經網路
跟深度學習的再造

31
00:02:00,430 --> 00:02:03,359
告訴我們，那個時候的工作
您的感覺如何？

32
00:02:03,359 --> 00:02:06,217
>> 那真的是很

33
00:02:06,217 --> 00:02:10,992
令人興奮，那是我的第一年博士學位學生

34
00:02:10,992 --> 00:02:11,960
傑夫跟

35
00:02:11,960 --> 00:02:17,505
我試著探索使用
受限玻爾茲曼機的想法

36
00:02:17,505 --> 00:02:21,680
使用預先訓練技巧來
訓練多層（深度學習）

37
00:02:21,680 --> 00:02:25,934
特別是我們試著
專注於自動編碼器

38
00:02:25,934 --> 00:02:29,880
我們如何有效地做非線性的 
PCA (Principle Component Analysis)

39
00:02:29,880 --> 00:02:34,416
那是很令人興奮的, 
因為我們讓系統作用在 MNIST digit

40
00:02:34,416 --> 00:02:37,296
很令人興奮，但下一步當我們

41
00:02:37,296 --> 00:02:42,062
真的看是否可以
擴展這個模型到人臉辨識

42
00:02:42,062 --> 00:02:45,069
我記得我們用 Olivetti 人臉資料集

43
00:02:45,069 --> 00:02:48,000
然後我們開始看
可不可以做文件的壓縮

44
00:02:48,000 --> 00:02:52,576
我們開始看所有這些不同的資料

45
00:02:52,576 --> 00:02:57,152
實數，二進位，看了一年

46
00:02:57,152 --> 00:03:03,672
當時我是第一年的博士生，
是我很大的學習經驗

47
00:03:03,672 --> 00:03:06,310
但僅僅不到六到七個月

48
00:03:06,310 --> 00:03:11,079
我們已經可以得到很有趣的結果，
我的意思是很好的結果

49
00:03:11,079 --> 00:03:14,765
我想我們可以訓練
很深的自動編碼器

50
00:03:14,765 --> 00:03:19,195
在當時，您無法使用

51
00:03:19,195 --> 00:03:22,077
傳統的最佳化技術能夠做到的

52
00:03:22,077 --> 00:03:26,965
然後，那真的真的
令人非常興奮的時期

53
00:03:27,970 --> 00:03:33,520
那時真的超級興奮，
因為我學到很多

54
00:03:33,520 --> 00:03:38,230
同時，那些結果也真的

55
00:03:38,230 --> 00:03:40,710
非常令人印象深刻，
對於我們做的部分

56
00:03:42,210 --> 00:03:45,360
>> 所以在早期這些深度學習的研究中

57
00:03:45,360 --> 00:03:49,760
許多活動集中在受限玻爾茲曼機上,

58
00:03:49,760 --> 00:03:52,340
然後是深度玻爾茲曼機

59
00:03:52,340 --> 00:03:54,842
之後還有很多的令人興奮的研究

60
00:03:54,842 --> 00:03:58,228
包含了您的小組，後來玻爾茲曼機

61
00:03:58,228 --> 00:03:59,663
跟受限玻爾茲曼機怎麼了？

62
00:03:59,663 --> 00:04:00,900
>> 這是一個好問題

63
00:04:00,900 --> 00:04:01,710
我想

64
00:04:01,710 --> 00:04:07,032
早期，我們使用受限玻爾茲曼機的方式是

65
00:04:07,032 --> 00:04:10,940
您可以想像訓練
疊起來的受限玻爾茲曼機

66
00:04:10,940 --> 00:04:14,715
這會讓您有效地一次訓練一層

67
00:04:14,715 --> 00:04:17,928
而有好的理論在
當您加入一個特別的層的背後

68
00:04:17,928 --> 00:04:21,010
在一定條件下, 它可以證明變化的約束等等

69
00:04:21,010 --> 00:04:24,414
因此, 有一些理論的原因, 這些模型

70
00:04:24,414 --> 00:04:28,697
可以作用得很棒在
預先訓練這些系統時

71
00:04:28,697 --> 00:04:35,013
然後在大約 2009, 2010時，
一旦運算力開始展現

72
00:04:35,013 --> 00:04:41,618
GPU, 我們很多人開始理解，
實際上這個運算力直接

73
00:04:41,618 --> 00:04:47,760
最佳化神經網路，
給予類似的結果，甚至更好的結果

74
00:04:47,760 --> 00:04:50,336
>> 所以只是標準的反向傳播，
沒有預先訓鍊或

75
00:04:50,336 --> 00:04:51,824
或者受限玻爾茲曼機

76
00:04:51,824 --> 00:04:52,700
>> 對對沒錯

77
00:04:52,700 --> 00:04:56,180
經過三，四年後，

78
00:04:56,180 --> 00:04:58,050
整個社群都很興奮，因為人們覺得，哇

79
00:04:58,050 --> 00:05:02,460
您可以真的訓練這些深度模型，
使用這些預先訓練機制

80
00:05:02,460 --> 00:05:06,930
然後，越多的運算力，人們開始理解您可以

81
00:05:06,930 --> 00:05:10,580
基本上做標準的反向傳播，那是我們無法在

82
00:05:11,660 --> 00:05:17,240
2005, 2004 的時候做，因為
會花幾個月的時間在 CPU 上做

83
00:05:17,240 --> 00:05:20,330
所以那是一個很大的改變

84
00:05:20,330 --> 00:05:24,050
另一件事我想
我們還沒發現到

85
00:05:24,050 --> 00:05:28,108
玻爾茲曼機跟深度玻爾茲曼機要做什麼？

86
00:05:28,108 --> 00:05:29,650
我相信那是很強大的模型

87
00:05:29,650 --> 00:05:32,010
因為您可以將
他們想成生成模型

88
00:05:32,010 --> 00:05:36,200
他們試圖在資料中耦合分佈, 但

89
00:05:36,200 --> 00:05:40,440
當我們開始看學習演算法，
現在的學習演算法

90
00:05:40,440 --> 00:05:45,000
他們用到馬可夫鍊，
蒙地卡羅跟變分學習等等

91
00:05:45,000 --> 00:05:50,170
它們不像反向傳播那樣可以擴充

92
00:05:50,170 --> 00:05:55,480
我們還在尋找更有效的方式
來訓練這些模型

93
00:05:55,480 --> 00:05:57,920
我們也用到卷積

94
00:05:57,920 --> 00:06:02,830
但是很難將它融入這些模型

95
00:06:02,830 --> 00:06:07,500
我記得您曾經用過機率最大池

96
00:06:07,500 --> 00:06:12,020
類似建立這些不同物件的生成模型

97
00:06:12,020 --> 00:06:16,970
使用卷積的概念
也是非常令人興奮

98
00:06:16,970 --> 00:06:20,630
同時，還是很難來訓練這些模型

99
00:06:20,630 --> 00:06:21,488
>> 可行性如何？

100
00:06:21,488 --> 00:06:22,720
>> 可行性如何？是吧

101
00:06:22,720 --> 00:06:24,990
我們還得弄清楚

102
00:06:27,750 --> 00:06:31,185
另一方面，一些人
最近利用變分自動編碼器

103
00:06:31,185 --> 00:06:35,260
舉個例子，可以看成是
玻爾茲曼機的互動版

104
00:06:35,260 --> 00:06:40,491
我們已經找到方法來訓練這些模型，
Max Welling 

105
00:06:40,491 --> 00:06:44,800
跟 Diederik Kingma 使用重參數技巧

106
00:06:44,800 --> 00:06:50,720
現在我們可以使用反向傳播
演算法在隨機系統上

107
00:06:50,720 --> 00:06:53,630
現在有很多的進展

108
00:06:53,630 --> 00:06:59,199
但我們還沒發現到如何
作用在玻爾茲曼機上

109
00:06:59,199 --> 00:07:04,606
>> 實際上是一個非常有趣的觀點,
 我並不知道

110
00:07:04,606 --> 00:07:09,382
在早期電腦比較慢，
RBM （受限玻爾茲曼機）

111
00:07:09,382 --> 00:07:11,969
預先訓練是
真的很重要

112
00:07:11,969 --> 00:07:17,920
只是在更快的運算力時，
驅動成為標準的反向傳播

113
00:07:17,920 --> 00:07:21,370
在社群中所想的深度學習演化

114
00:07:21,370 --> 00:07:24,712
換另一個主題，
我知道您花很多時間在上面

115
00:07:24,712 --> 00:07:28,870
生成，非監督式跟監督式的方法

116
00:07:28,870 --> 00:07:32,718
您可以分享一下您的想法
關於那個的演進

117
00:07:32,718 --> 00:07:37,407
>> 是的, 我認為這是一個非常, 
我覺得是一個非常重要的話題

118
00:07:37,407 --> 00:07:41,727
特別是如果我們考慮到
非監督式, 半監督式或

119
00:07:41,727 --> 00:07:46,493
生成模型，因為某種程度上，最近的成功

120
00:07:46,493 --> 00:07:50,812
都是在監督式學習上，回到以前

121
00:07:50,812 --> 00:07:55,803
非監督式學習主要被視為
非監督式預先訓練,

122
00:07:55,803 --> 00:07:59,850
因為我們不知道
如何訓練多層系統

123
00:07:59,850 --> 00:08:04,320
即使在今天，如果您在設置的時候有

124
00:08:04,320 --> 00:08:08,488
很多未標籤的資料跟少數標籤的例子

125
00:08:08,488 --> 00:08:11,371
這些非監督式預先訓練模型

126
00:08:11,371 --> 00:08:15,721
建立在這些生成模型，
可以幫助監督式

127
00:08:15,721 --> 00:08:20,863
我想有很多在社群中的人，相信這樣

128
00:08:20,863 --> 00:08:25,838
當我開始我的博士學位時，
都是有關於生成模型，試著學習

129
00:08:25,838 --> 00:08:30,900
這些疊起來的模型，因為那是
唯一的方式我們來訓練系統

130
00:08:30,900 --> 00:08:35,720
在今天，有很多的工作在生成模型

131
00:08:35,720 --> 00:08:38,530
如果您看生成對抗網路

132
00:08:38,530 --> 00:08:40,397
如果你看變分自動編碼器

133
00:08:40,397 --> 00:08:45,620
深度能量模型是
我的實驗室正在研究的

134
00:08:45,620 --> 00:08:50,660
我想都是很令人興奮的研究，
但或許我們還沒發展出來

135
00:08:50,660 --> 00:08:55,420
再一次，對於很多想要
進入深度學習領域的人

136
00:08:55,420 --> 00:08:59,340
這一個領域是我想我們
將會獲得重大的進展

137
00:08:59,340 --> 00:09:01,187
希望在不久的將來

138
00:09:01,187 --> 00:09:02,214
>> 因此, 非監督學習

139
00:09:02,214 --> 00:09:04,281
>> 非監督學習, 對

140
00:09:04,281 --> 00:09:07,751
或者您可以想成非監督式學習

141
00:09:07,751 --> 00:09:12,886
或者半監督式學習，您會有，
我給您一些暗示或者例子

142
00:09:12,886 --> 00:09:18,240
不同的東西意味著什麼, 
我拋出了很多未標籤的資料

143
00:09:18,240 --> 00:09:21,506
>> 所以實際上這是一個非常重要的洞見, 
在早期的時代

144
00:09:21,506 --> 00:09:23,929
對於深度學習, 電腦的速度較慢,

145
00:09:23,929 --> 00:09:27,763
受限玻爾茲曼機和
深度玻爾茲曼機需要用來

146
00:09:27,763 --> 00:09:31,257
初始神經網路權重，但當電腦越來越快

147
00:09:31,257 --> 00:09:34,700
直接反向傳播
已經作用得比較好

148
00:09:34,700 --> 00:09:39,220
換另一個主題，
我知道您花很多時間在想

149
00:09:39,220 --> 00:09:45,342
監督學習與
生成模型, 非監督學習方法。

150
00:09:45,342 --> 00:09:46,619
您如何

151
00:09:46,619 --> 00:09:51,920
告訴我您的看法是如何
對於這種辯論，隨著時間推移而演變的？

152
00:09:51,920 --> 00:09:56,780
>> 我想我們都相信我們
應該能夠在那裡取得進展

153
00:09:56,780 --> 00:10:03,990
所有的工作在於玻爾茲曼機，
變分自動編碼器，GAN (生成對抗網路）

154
00:10:03,990 --> 00:10:08,556
您想有很多的這些生成模型

155
00:10:08,556 --> 00:10:13,595
我們只是還沒有弄清楚
如何真正讓他們可行

156
00:10:13,595 --> 00:10:16,654
如何使用大量的資料

157
00:10:16,654 --> 00:10:21,797
即使，我看到很多 IT 
有關的公司，有很多

158
00:10:21,797 --> 00:10:26,463
大量的資料，大量非標籤資料，很多的努力

159
00:10:26,463 --> 00:10:30,848
來做標註，因為那是唯一的方式

160
00:10:30,848 --> 00:10:33,350
在今日讓我們可以有所進展

161
00:10:33,350 --> 00:10:36,400
似乎我們應該能夠使用

162
00:10:36,400 --> 00:10:40,200
非標籤資料，因為它就是很大量

163
00:10:40,200 --> 00:10:42,190
而我們還沒發現如何去用它

164
00:10:44,020 --> 00:10:48,300
>> 所以你提到的如果
有人想進入深度學習研究

165
00:10:48,300 --> 00:10:50,920
非監督式學習
是令人興奮的領域

166
00:10:50,920 --> 00:10:54,240
今天有很多人想
進入深度學習

167
00:10:54,240 --> 00:10:57,688
無論是研究還是應用工作, 
對於這個全球社群

168
00:10:57,688 --> 00:11:01,490
無論是研究還是應用工作, 
你會有什麼建議？

169
00:11:01,490 --> 00:11:06,680
>> 是的, 我認為其中一個
關鍵的建議, 是我想

170
00:11:06,680 --> 00:11:10,620
人們想進入這個領域

171
00:11:10,620 --> 00:11:14,210
我會鼓勵他們
只是嘗試不同的東西,

172
00:11:14,210 --> 00:11:18,280
不要害怕嘗試新事物, 
也不要害怕嘗試創新

173
00:11:18,280 --> 00:11:20,135
我可以舉一個例子

174
00:11:20,135 --> 00:11:24,975
當我還是個研究生的時候, 
我們正在研究神經網路

175
00:11:24,975 --> 00:11:29,680
這些都是高度非凸系統, 
很難去做最佳化

176
00:11:29,680 --> 00:11:33,780
我記得在最佳化社區群裡
和我的朋友聊天

177
00:11:33,780 --> 00:11:38,350
和回饋總是, 嗯, 
您沒有辦法可以解決這些問題

178
00:11:38,350 --> 00:11:41,600
因為這些都是非凸，
我們不知道如何去做最佳化

179
00:11:41,600 --> 00:11:46,470
與做凸形函數最佳化相比, 
你還能做什麼呢？

180
00:11:46,470 --> 00:11:51,225
令人驚訝的是，
因為在我們實驗室中

181
00:11:51,225 --> 00:11:55,520
從不關心這樣
特定的問題

182
00:11:55,520 --> 00:11:58,005
我們只是想著
如何做最佳化

183
00:11:58,005 --> 00:11:59,960
是否我們可以獲得有趣的結果

184
00:11:59,960 --> 00:12:04,150
而這有效地推動社群, 使得

185
00:12:04,150 --> 00:12:09,038
我們不害怕, 
也許在某種程度上, 因為我們

186
00:12:09,038 --> 00:12:13,444
實際上缺乏最佳化背後的理論

187
00:12:13,444 --> 00:12:16,123
但我會鼓勵人們只是嘗試

188
00:12:16,123 --> 00:12:19,200
不要害怕去嘗試解決困難的問題

189
00:12:19,200 --> 00:12:22,616
>> 是的, 我記得你曾經說過, 不要學高階

190
00:12:22,616 --> 00:12:25,740
深度學習框架的程式，但要真的了解深度學習

191
00:12:25,740 --> 00:12:26,370
>>是，對

192
00:12:26,370 --> 00:12:30,992
我想這是我想嘗試的一件事是，
當我教深度學習課程時

193
00:12:30,992 --> 00:12:35,182
其中一項作業，我要求學生實際去寫

194
00:12:35,182 --> 00:12:39,323
反向傳播演算法的程式，
對於卷積神經網路

195
00:12:39,323 --> 00:12:43,379
那是很痛苦的，但同時，
如果您做過一次

196
00:12:43,379 --> 00:12:48,510
您會真的了解系統如何作用，
它們的工作方式

197
00:12:49,540 --> 00:12:53,223
您可以如何有效地
建立它們在 GPU 上

198
00:12:53,223 --> 00:12:58,266
我想對您而言，
當您進入研究或者應用領域

199
00:12:58,266 --> 00:13:03,013
您會有相當好的理解
對於系統如何作用

200
00:13:03,013 --> 00:13:05,450
所以這很重要, 我想

201
00:13:05,450 --> 00:13:09,160
>> 因為你有教授的學術經驗,

202
00:13:09,160 --> 00:13:13,730
跟公司的經驗, 我很好奇, 
如果有人想進入深度學習,

203
00:13:13,730 --> 00:13:18,290
做博士研究跟加入公司
各有什麼優缺點？

204
00:13:18,290 --> 00:13:21,290
>> 這是一個好問題

205
00:13:22,660 --> 00:13:25,780
在我的實驗室中，
我有混合的學生

206
00:13:25,780 --> 00:13:28,850
有些學生想要
進入學術路線

207
00:13:28,850 --> 00:13:32,041
一些學生想進入產業界

208
00:13:32,041 --> 00:13:38,290
這變得很挑戰，
因為您可以在產業界做驚人的研究

209
00:13:38,290 --> 00:13:41,910
您也可以在
學術界做驚人的研究

210
00:13:41,910 --> 00:13:46,480
但在利與弊之間，在學術界

211
00:13:46,480 --> 00:13:53,180
我覺得您比較能自由地
從事長期的問題，或者您想

212
00:13:53,180 --> 00:13:59,150
一些瘋狂的問題，您可以做它們，
所以您可以比較自由

213
00:13:59,150 --> 00:14:03,940
同時, 你在產業界做的研究
也很令人興奮

214
00:14:03,940 --> 00:14:08,920
因為在許多情況下, 你的研究可以影響

215
00:14:08,920 --> 00:14:14,470
數以百萬計的使用者, 
如果你開發的是核心 AI 技術

216
00:14:14,470 --> 00:14:19,473
很顯然的, 在產業界, 你有很多

217
00:14:19,473 --> 00:14:26,120
資源，像是運算力，
能夠做驚人的事情

218
00:14:26,120 --> 00:14:30,260
所以有優缺點, 
這取決於你想做什麼

219
00:14:30,260 --> 00:14:32,630
現在變得很有趣

220
00:14:32,630 --> 00:14:36,860
非常有趣的環境, 學術界轉向產業界

221
00:14:36,860 --> 00:14:40,450
然後, 從產業界轉移到學術界, 
但沒有那麼多

222
00:14:40,450 --> 00:14:45,756
因此, 這是非常激動人心的時刻

223
00:14:45,756 --> 00:14:49,244
>> 它聽起來像學術界的機器學習是很棒的，
產業界的機器

224
00:14:49,244 --> 00:14:52,800
學習也是很棒的，
最重要的是要跳進去

225
00:14:52,800 --> 00:14:54,070
任何一個，只要跳進去

226
00:14:54,070 --> 00:14:58,870
>> 它真的取決於你的喜好, 
因為你可以做驚人的研究

227
00:14:58,870 --> 00:14:59,800
在這兩種地方都可

228
00:14:59,800 --> 00:15:03,301
>> 所以你提到非監督式學習
是一個激動人心

229
00:15:03,301 --> 00:15:04,260
研究的前沿

230
00:15:04,260 --> 00:15:08,850
是否還有其他領域, 
你認為令人興奮的研究前沿？

231
00:15:08,850 --> 00:15:09,700
>> 好啊，當然

232
00:15:09,700 --> 00:15:12,520
我想現在在社群裡看到的

233
00:15:12,520 --> 00:15:16,010
特別是在深度學習社群中，
有一些趨勢

234
00:15:17,400 --> 00:15:20,390
一個特別的領域，我想很令人興奮

235
00:15:20,390 --> 00:15:22,909
的是深度強化學習

236
00:15:24,110 --> 00:15:28,940
因為我們能夠知道如何在
虛擬世界中訓練代理

237
00:15:28,940 --> 00:15:33,633
這是在短短的幾年裡, 
你看到了很多,

238
00:15:33,633 --> 00:15:38,251
很多進步, 我們如何擴展這些系統, 
我們如何能發展

239
00:15:38,251 --> 00:15:42,643
新的演算法, 我們怎樣
才能讓代理彼此溝通

240
00:15:42,643 --> 00:15:46,731
我想那個領域是，一般而言

241
00:15:46,731 --> 00:15:52,004
設定來讓您與環境互動，
那是超級令人興奮的

242
00:15:52,004 --> 00:15:55,230
其他領與我想也是令人興奮的是

243
00:15:55,230 --> 00:16:00,720
在推理領域跟
自然語言理解部分

244
00:16:00,720 --> 00:16:03,810
我們可以建立對話系統嗎？

245
00:16:03,810 --> 00:16:09,120
我們可以建立系統來做推理，
可以讀文字

246
00:16:09,120 --> 00:16:12,730
然後可以
智慧地回答問題

247
00:16:12,730 --> 00:16:17,670
我認為這是目前很多研究的重點

248
00:16:17,670 --> 00:16:21,832
然後還有另一種子領域也是

249
00:16:21,832 --> 00:16:26,382
這個領域可以從幾個例子中學習

250
00:16:26,382 --> 00:16:31,210
所以一般人認為它是
一次學習或轉移學習

251
00:16:31,210 --> 00:16:36,970
一種您可以從這個世界學習的設定

252
00:16:36,970 --> 00:16:41,500
然後我丟給您一個任務，
您可以很快解決這個任務

253
00:16:41,500 --> 00:16:46,770
像是人類在做的，不需要很多很多的標籤例子

254
00:16:46,770 --> 00:16:52,051
所以這些事, 我們在社群上很多人都在試圖找出

255
00:16:52,051 --> 00:16:58,010
我們如何才能做到這一點, 
我們如何能夠更接近于人的學習能力

256
00:16:58,010 --> 00:17:00,790
>> 非常感謝, 魯斯, 分享這些意見、洞見和忠告

257
00:17:00,790 --> 00:17:02,205
這真有趣的看到

258
00:17:02,205 --> 00:17:04,870
聽到你的早期的故事,

259
00:17:04,870 --> 00:17:05,660
>> [歡笑]是的

260
00:17:07,100 --> 00:17:07,800
感謝您的邀請