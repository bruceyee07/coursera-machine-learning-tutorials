欢迎Rus！很高兴今天你可以来。 谢谢Andrew。 你现在是苹果研究院的一个总监， 同时你也是卡耐基梅隆大学（CMU）的教授， 我想听你讲一下你的个人经历。 你是如何最终走上了现在的道路的呢? 噢，这，从某种程度上， 我开始从事深度学习工作是比较偶然的。 在多伦多大学完成了硕士学位后
我离开学校了一年 那一年里，我在金融行业工作。 我自己想起来也有点惊讶。 在那个时候,我不太确定是否要去攻读博士学位
或者做点什么其他。 然后发生了一件事情，一年意外的事情 一天早上我要去上班,我碰见了Geoff Hinton Geoff对我说：“我有一个很好的点子“ “到我办公室来,我给你讲讲。“ 然后，我们就一边走一边聊
他开始跟我聊起 波尔兹曼机，对比分歧 还有一些我当时不太明白的技术 但是我听了后，对那些东西非常感兴趣。 然后，基本上,在三月之内
我开始跟Geoff攻读博士学位。 就是那样开始的
那是在2005，2006年前后 就是那个时候，好些最初的深度学习的算法 例如受限波尔兹曼机（RBM）
无监督的预训练等开始出现了 我就是这样开始进入深度学习这个领域的。 那个偶遇Geoff的早上 完完全全地改变了我未来的事业方向。 然后事实上你是 最早的使得神经网络和深度学习 重新受到关注的
关于受限玻尔兹曼机的论文的一个作者。 请多告诉我一点
从事那个有深远影响的研究是什么样的感觉 对，这确实是 非常令人兴奋的。那是我的读博的第一年 Geoff和我 尝试去探索使用受限玻尔兹曼机和预训练技巧 来训练多层网络的一些想法。 特别是我们试图集中精力研究自编码器 还有如何有效的对PCA做非线性的扩展 这很令人兴奋
因为有了这些系统我们可以工作了 这是令人兴奋的,但接下来我们需要做的是 我们是否真的可以将这些模型扩展到处理人脸。 我记得我们有一个Olivetti人脸数据集 然后我们又开始看我们是否能做图像压缩 我们开始看各种不同的数据 实数、正整数、二进制，这些都是 在我的读博第一年之内做的
对我而言确实是一个很重要的学习经验 但是其实就在6、7个月的时间内 我们就已经取得了很有趣的结果
我的意思是，很好的结果。 我们可以训练这些非常多层的自编码器 而那些是当时你用传统的 优化技术所无法做到的。 那一段时间最后证实了
确实是我们的一段非常辉煌的时期。 那是非常让我兴奋的时光，因为我学到很多 同时，对于是我们想要达到的结果而言， 研究结果也是非常让人瞩目。 就是说，在深度学习的早期， 很多的研究活动是以受限波尔兹曼机为中心的 然后就是深度波尔兹曼机 当然，还有很多其他让人振奋的研究 包括你们小组的一些研究
但是，到底后来波尔兹曼机 和受限波尔兹曼机出现了什么情况呢？ 这是一个很好的问题 我想， 在深度学习的早期，
我们使用受限波尔兹曼机的方式是 你基本生可以想象
就是训练多层的受限波尔兹曼机 那可以使你有效的一层一层的学习 这背后有一个很好的理论证明
当你添加一个特定的层时, 在一定条件下,它可以被证明是具有变分上限的，等等。 因此,这后面是有很好的理论根据的,并且这些模型 可以用来预训练的深度学习系统，
在当时是相当好用的 但是，在大约2009，2010年左右，
当以GPU引入的计算能力开始突然发展 我们很多人开始认识到，事实上直接优化 这些深度神经网络可以得到相似甚至更好的结果。 那就是说可以不需要做预训练或者使用受限波尔兹曼机， 直接做标准的反向传播。 你说的没错 而且那只经过了3到4年到时间
那就是为什么 整个学界都感到很兴奋
因为人们感觉到：Wow 你原来可以用那些预训练机制来训练深度模型， 然后随着计算能力的增强，人们开始认识到 你基本上可以直接做基本的反向传播，
那是在2005或者2004年所无法做到的 因为如果使用传统CPU来做，会需要数月的时间 所以那是一个巨大的变化。 另外，我认为我们还没有真的想出 如何去用好波尔兹曼机和深度波尔兹曼机 我相信他们是非常强大的模型， 因为你可以把他们想象是一种生成模型 他们试图对数据的耦合分布建模，但 当我们开始看学习算法的时候，现在的学习算法 他们需要使用马尔可夫链蒙特卡罗方法和变分学习等 这些方法并没有反向传播算法那样的可扩展性 因此,我们还需要找出更有效的方法来训练这些模型, 还有卷积的使用, 这是相当难以整合到这些模型的东西。 我记得你的一些使用概率最大池化 来构建不同对象的生成模型的研究工作 和使用这些卷积的思想也非常令人兴奋 但是，同样的，训练这些模型仍然非常困难, 所以这些东西行得通的可能性是多大？ 是的，行得通的可能性是多大，对吧？ 所以我们还得继续弄清楚。 在另一方面，例如
一些最近的研究工作使用变分自编码 可以将其视为玻尔兹曼机的交互式版本。 我们已经找到了训练这些模块的方法
那是Max Welling和 Diederik Kingma
关于使用再参数化（reparameterization）技术的研究 现在我们可以在随机系统中使用反向算法, 现在正推动着很多研究进步 但是,在玻尔兹曼机的情况下,我们还没有想到如何那样做 所以这实际上是一个我以前不了解的非常有趣的观点 那就是，在早期的计算机速度较慢的时代,RBM， 预训练等技术对于深度学习系统是非常的重要 只有更快的计算驱动我们切换到标准的反向传播。 至于深度学习社区的思维的演变和 其他话题,
我知道你花了很多时间思考这个问题, 生成的、无监督的与监督的方法的对比 你是否可以分享一下你对此的想法是如何随着时间演变的？ 我觉得这是一个非常重要的话题, 特别是如果我们考虑无监督,半监督或 生成模型,因为在某种程度上
最近我们已经看到了很多的成功 是来自于有监督的学习,而在早期的时候, 无监督学习主要被视为无监督的预训练 因为我们不知道怎样训练这些多层系统。 即使在今天,如果你要解决的问题中有 大量未标记的数据和一小部分标记的样例, 这些无监督的预训练模型 建立这些生成模型,可以对有监督学习有所帮助 所以我认为，当我开始读博士的时候
我们社区中的很多人， 都深信生成模型，并试图学习 一系列的这些模型,因为这是我们训练这些系统的唯一途径。 如今,有很多关于生成建模的研究工作 如果你看一下生成对抗网络（GAN）， 如果你看一下变分自编码, 深度能量模型，
这也是我的实验室正在研究的, 我认为这是非常令人兴奋的研究
但也许我们还没有完全搞通, 同样,对于你们中的许多
正在考虑进入深入的学习领域的人而言， 我希望，在不久的将来，这是一个 我们会取得很大的进展的领域, 无监督学习 对，无监督学习 或者你可以把它看成是这样的无监督的学习,或者 半监督学习，
你有一些提示或者大概表示不同含义的样本 然后还有大量的无标记的样例 实际上，在深度学习较早期，
计算机还没有现在这么快的时候， 一个非常重要的观察是， 我们需要通过受限玻尔兹曼机
和深度玻尔兹曼机 来初始化神经网络的权重，
然后计算机变得越来越快 直接使用反向传播开始得到更好的效果 我知道你花了很多时间思考的
另外一个课题是 监督学习和生成模型、
无监督学习方法的比较 那你可不可以 告诉我们你对于这个辩论的看法 我想我们都相信我们应该能够在那里取得进展。 那些玻尔兹曼机，变分自编码，GAN 这些大都被人们认为是生成模型，但是 我们还没有弄清楚如何有效的运用它们 如何使用大的矩 甚至我在IT行业中经常看到，
很多公司有很多 很多的数据，无标签的数据，
这些公司很多的 做法是通过给数据加标注,
因为这是现在唯一的 可以让我们往前推进的办法 并且因为我们有大量的无标注的数据， 看起来我们应该能够利用它们，但是 事实上我们还没有搞清楚
如何可以很好地使用这些数据 你提到，对于想进入深度学习研究的人， 无监督学习是令人兴奋的领域。 今天有很多人想进入深度学习领域, 做研究或者应用性的工作，
那对于这样的一个全球性的社区 无论是研究还是应用工作,你会有什么建议？ 我认为其中一个关键的建议 我应该对想要进入这个领域的人说的 我会鼓励他们尝试不同的东西, 不要害怕尝试新事物,也不要害怕尝试创新。 我可以举一个例子 当我还是个研究生的时候,我们正在研究神经网络, 这些都是高度的非凸（non-convex）的系统,
很难优化。 我记得和我在优化领域中的朋友聊天 我听到的总是，啊，你无法解决这些问题 因为这些都不是凸问题,我们也不太懂优化, 不像是做凸优化，
你怎么可以优化这些非凸的问题呢？ 但令人惊讶的是,因为在我们的实验室,
我们从来没有真正 关注那些具体的问题是不是凸问题 我们只是思考如何才能优化和 我们是否能得到有趣的结果。 真是那样的精神在有效地推动社区的进步， 我们不害怕,也许在某种程度上是因为我们 缺乏对优化背后的理论的知识。 但我会鼓励人们勇敢地尝试 不要害怕去尝试解决困难的问题。 是的,我记得你曾经说过,
不要只是学习如何写深度学习框架层面上的代码， 而要实际上了解深度学习的理论。 是，对的。 我尝试做的一件事情是,
当我教深度学习的课程的时候， 其中的一个作业，我要求学生去写代码实现 卷积神经网络中的反向传播算法 它是痛苦的,但同时,如果你做过一次, 您将真正了解这些系统的工作方式。 以及如何有效地在GPU上实现它们,以及 我想，这对你进入研究或工业应用来说，都是重要的。 如果你对这些系统是如何工作有很好的理解。 所以我认为这很重要。 因为你既有教授的学术经验,同时 也有公司工作的经验,
我很好奇,如果有人想进入深入学习, 做博士与加入公司有什么优缺点？ 这其实是一个很好的问题。 在我的实验室里,我有很多不同的学生 有些学生想去走学术路线， 一些学生想去企业工作 它变得非常有挑战性,因为你可以在企业里做很好的研究, 你也可以在学术界做很好的研究。 至于利弊，在学术界， 我觉得你有更多的自由可以投入到长期的研究，或者你 可以投入到一个疯狂的想法的研究中，
所以说你选择研究方向的自由度更高 同时,你在工业界做的研究也很令人兴奋 因为在许多情况下,如果你开发出一个核心的AI技术， 你的研究可以影响数以百万计的用户。 显然,在工业界内,你有很多 在计算方面的资源,并能够做真正惊人的事情。 所以两者都有优缺点,怎么选择主要取决于你想做什么。 现在是一个很有趣环境 很多人从学术界转向工业届, 然后,也有少数人从工业界转移到学术界。 因此,这是非常激动人心的时期。 听起来好像学术届的机器学习也好，工业界的机器学习也好 最重要的事情只是去做，对吧 学术研究也好，工业研发也好，只管去做 它真的取决于你的喜好,因为在那两个地方 你都可以做惊人的研究 你提到无监督学习是一个激动人心的 研究前沿 是否有其他领域,你认为令人兴奋的研究前沿？ —不用谢 我想现在在社区里看到的 尤其是在深度学习的领域中,有几个趋势。 一个我认为是非常令人兴奋的方向是 是深度强化学习的领域 因为我们能够搞懂
如何在虚拟世界中训练智能主体 这是在短短的几年里,你看到了很多 很多进步,我们如何扩展这些系统,我们如何能开发 新的算法，如何可以使得智能主体可以相互沟通 我认为这个防线是,总的来说, 与环境交互的问题是超级激动人心的。 另外一个我认为是真正令人兴奋的领域 是推理和自然语言理解 我们能建立对话系统吗？ 我们可以建立可以推理的系统吗？可以读取文本和 能够智能地回答问题吗？ 我认为这是目前很多研究的重点。 然后还有另外一些子领域 是关于可以从少量的例子中学习 一般人认为它是一次（one-shot）学习或迁移学习, 这种学习方式学习到一些关于这个世界的东西 然后我给你一个新的任务的时候,
你可以很快地解决这个任务。 就像人类的学习一样,不需要大量的有标签的样例 这些就是我们这个社区里很多人都在试图 找出如何可以做到的事情，
如何可以使机器学习更加接近人类的学习能力 谢谢你,Rus,分享所有的评论和见解。 听到你的故事 以及你早期的深度学习工作，真是非常有意思 谢谢,Andrew 谢谢你们邀请我来
GTC字幕组 翻译