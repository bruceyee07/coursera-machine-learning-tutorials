深度学习中最有力的方法之一，是有时你可以把在一个任务中神经网络 学习到的东西，应用到另一个任务中去。 比如，你可以让神经网络 学习去识别物体，比如猫，然后用学习到的 （一部分）知识 来帮助你更好地识别X射线的结果。 这就是所谓的迁移学习。让我们来看看。 假如你现在已经训练了一个图像识别的神经网络， 首先你设计一个神经网络并使用X-Y对来对其进行训练 其中，X代表一张图片，Y代表某个物体（的标签） 图片可以是一只猫，狗，鸟或者其他的东西 如果你想要将这个已经经过学习的神经网络提取出来并适用到 或者我们说，迁移到 另外一个不同的任务中 例如放射扫描结果的诊断过程 即真实地读取X光扫描图像 你可以做的就是，取出这个神经网络的最后一层 并移除掉这一层以及其相关的权重 然后为最后一层神经网络创建一个新的随机初始化的权重 使用新建的这个输出层来进行放射结果的诊断 所以，具体来说，在训练模型的第一阶段， 当你在做图像识别的任务时， 你训练了所有常用的神经网络的参数，所有权值 所有层，然后这个模型 现在能够做出图像识别的预测。 训练得出了这样一个神经网络之后， 要实现迁移学习，你现在需要把数据集X和Y 设定为放射影像。 现在的Y是你想要预测的诊断结果， 你要做的是初始化最后一层的权值 我们叫这个为WL 和PL。 然后现在，重新在新的数据集上训练这个神经网络， 就是新的放射数据集上。 你有几种方法来重新训练这个放射数据的神经网络。 如果你只有一个小的放射数据集， 你可以只重新训练最后一层的权值，就是WL 和PL，同时保留其它所有参数。 如果你有足够的数据， 你也可以重新训练神经网络的其余所有层。 我们的经验就是如果你有一个小规模的数据集， 那么就去重新训练最后一层和输出层的神经网络， 或者你也可以训练最后一到两层的神经网络。 但是如果你有大量的数据， 你或许可以对这个神经网络的所有参数都进行重新训练。 要是你对神经网络的所有参数进行重新训练， 那么这样训练的初始化阶段 有时候被我们叫做预训练（pre-training） 原因是，你在是使用图像识别的数据 来预初始化（pre-initialize）或者说预训练神经网络的权重。 然后如果你在之后对所有的权重进行更新， 那么在放射扫描的数据上的训练有时候被我们叫做微调（fine tuning) 所以有时候你会在深度学习领域听到预训练（pre-training）和微调（fine tuning）这些词， 上面我所说的就是他们在迁移学习中 表达的真正含义。 你在这个例子中已经做的 是把已经从图像识别中学到的知识信息， 应用到，或者说迁移到了放射扫描诊断中。 之所以这样的过程是有用的， 是因为从从大规模的图像识别数据集中 学习到的边界检测，曲线检测，明暗对象检测等 低层次的信息， 或许能够帮助你的学习算法更好地去进行放射扫描结果的诊断。 它会在图像的组织结构和自然特性中学习到很多信息， 其中一些信息会非常的有帮助。 所以当神经网络学会了图像识别 意味着它可能学习到了以下信息： 关于不同图片的 点，线，曲面等等信息， 在不同图片中看起来是什么样子的。 或许关于一个物体对象的很小的细节， 都能够帮助你 在放射信息诊断的神经网络中，学习得更快一些或者减少学习需要的数据。 来看另一个例子下面再举另一个例子 假若你现在已经训练好了一个语音识别系统， 其中，X为你的语音或者语音片段的输入， 而Y是一些对应的文字片段。 就是说你已经训练好了一个语音识别系统来输出对应的文字。 现在假设你想要构建一个“唤醒词” 或者一个“触发词”检测系统。 想一想一个唤醒词或者一个触发词的含义是， 为了唤醒我们的语音控制设备，我们会说 “Alexa”来唤醒Amazon echo，或者”Ok Google“来唤醒我们谷歌设备，又或者 我们说“hey, Siri”来唤醒苹果设备，说“你好，百度”来唤醒百度设备。 为了实现这些功能， 你可以同样去除神经网络的最后一层， 然后创建一个新的输出节点。 但是有时候，另外一个你可以做的事儿是，<br /> 你可以不仅仅去创建一个单一的输出， 而是创建好几层新的神经网络来让你的神经网络 来给你的语音唤醒检测系统打上对应的Y标签。 接下来，根据你拥有的数据集规模 你可能仅对你的神经网络中新建的几层进行训练， 或者你也可以重新训练其中更多的几层神经网络。 那么迁移学习在什么时候有用呢 当你在你的被迁移的模型中拥有大量的数据， 而你在你需要解决的问题上拥有相对较少的数据时， 迁移学习是适用的。 例如，假设你在一个图像识别任务中拥有一百万个样本。 这就意味着大量数据中的 低层次特征信息或者大量的 有帮助的特征信息在神经网络的前几层被学习到了。 但是对于放射扫描结果的诊断任务， 你或许仅仅拥有几百个样本。 所以，你在放射诊疗这个问题上就只拥有非常小的数据， 或许仅仅是100个X光扫描样本。 所以你从图像识别中学习到的大量的信息可以被用于迁移并且 这些信息会有效地帮助你处理好 放射诊疗，哪怕你没有足够多的数据来开展放射诊疗任务。 又比如语音识别，假定或许你使用10000小时的数据 来训练好了一个语音识别系统。 那么，你已经从人类的语音中学习到了很多的信息， 10000个小时的数据真的好多好多啦。 但是，在你的语音唤醒系统中， 或许你仅仅有一个小时的数据。 所以，这并不是我们说的用大量的数据来解出大量的参数的问题， 那么在这个例子中，你从人类说话中学习到的大量信息更像是， 人类语音中的组成信息等等， 而这在构建一个语音唤醒系统中是真正有用的， 尽管，你在这个任务中仅有一个相对小规模的数据集，或者说 至少是一个相对唤醒词检测来说及其小量的数据集。 所以总结一下就是，在这两个例子中， 你把一个有大量数据的问题模型， 迁移到了一个相对之下仅有很少数据的问题模型中。 一个迁移学习不适用的例子是， （上面讲到的）数据规模被颠倒过来的时候。 所以，如果你有一百个图像样本来做图像识别， 然后你有100个，<br /> 或者甚至是1000个样本来做放射诊疗任务。 做这个事儿的时候，<br /> 你可能首先想到的是把放射诊断任务做好， 假设你真地想要把这个诊断任务做好， 那么放射扫描图像是比猫，狗等等的图片更加地有价值。 所以在这个地方的样本<br />就比在这个地方的样本更有价值， 至少，在构建一个放射诊断系统时是这样的。 那么，如果你已经在放射诊断上有更多的数据时， 那么100张随机的 猫，狗或者汽车等等的图片并没有什么用 因为在你猫和狗的图像识别任务中一张图片的价值 就是比用于构建一个良好的放射诊断系统时 用到的X光照片更加地低。 小结一下，这就是一个迁移学习的反例。 这样子做不会对模型造成损失，
但是我也不能看出这样做有什么实际的意义。 相似地，如果你使用10小时的语音数据来构建一个语音识别系统， 但是实际上你有10小时甚至更多的， 假设50个小时的数据来构建一个语音唤醒系统， 这或许不会造成损失， 或许你将这10个小时的数据添加到迁移学习中不会造成什么损失， 但是你也并不要期望这会给你什么有意义的增益。 总结一下，什么时候迁移学习是适用的呢? 如果你尝试 从某个任务A中学习到的信息迁移到某个任务B中， 那么当任务A和任务B在有相同的输入时，迁移学习才能适用。 在第一个例子中， A和B都以图片作为输入。 在第二个例子中， 它们都以语音片段作为输入。 必须在任务A比任务B有更多的数据时，迁移学习才适用。 所有条件都是在想要把任务B做好的前提下 因为在任务B中的数据比在任务A中更加地有价值， 通常你需要为任务A准备更多的数据，因为 在任务A中的样本比任务B中的样本价值更低。 最后，迁移学习更加适用的场景 是当你认为，任务A中的低层次特征会帮助任务B达成目标。 在上面我们提到的例子中， 或许图像识别任务教给了你足够的信息 用迁移学习来进行一个放射诊断， 或许语音识别的例子教给你了 如何把语音识别系统迁移到语音唤醒系统中。 继续总结，迁移学习必须是 你尝试把任务B中的任务做好时才会有效， 即通常情况下，你要把这个拥有更少数据的任务做好时。 例如，在放射诊断中， 获取大量的X光扫描 来进行一个良好的放射诊断系统是很难的。 在这个例子中，你或许发现一个相关但又不同的任务， 例如图像识别， 在这样的任务中，你或许可以获取到百万级的数据
并且从中学习到很多低层次的特征。 如此以来你就能够通过迁移学习
来尝试把任务B即放射诊断任务做得很好， 尽管你并没有那么足够多的数据。 那，什么时候迁移学习会是适用的呢？ 迁移学习确确实实会提高你所进行的任务的表现。 但是我也有遇到过，迁移学习在应用于 任务A实际上拥有比任务B更少的数据的时候，
在这样的情况下， 你并不能得到很大的增益。 那么，这就是迁移学习， 一种你把从一个任务中学到的信息迁移到另外一个不同的任务中的方式。 还有另外一种关于 从多个任务中学习的版本叫做多任务学习， 这是一种当你尝试同时从多个任务中学习的方式， 而不是序列化的先从一个任务学习， 然后迁移到另外一个任务的学习。 下个视频再见 我们将会讨论一下多任务学习。