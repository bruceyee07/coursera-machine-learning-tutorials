transfer learning 에서는 
순차적인 절차가 있었는 반면에, A업무를 배우고 B업무로 
넘어가는 절차 말이죠, milti-task 러닝에서는,
동시에 시작합니다. 신경망이 여러가지 일을 할 수 있게
만드는 것이죠. 그러면 바라건대 여기 각각의 업무가 
다른 업무들도 도와주는 역할을 하는 것이죠. 
예제를 보도록 하겠습니다. 예시를 봐볼까요. 여러분이 자율주행차를 만든다고 해보겠습니다. 그러면 이러한 자율주행차는
보행자나 다른 차량, 정비 팻말등 여러가지 부분을 잘 감지해야
할텐데요, 또한 신호나 다른 것들도 감지해야하는 
중요한 요소가 될 것입니다. 예를 들어, 여기 왼쪽을 보시면, 
여기 정지 sign이 있는데요, 그리고 여기 이미지에는 챠량이 있습니다.
하지만 보행자나 신호등은 따로 없죠. 그러면 이 이미지가 example x(i)의 입력값이라고 하면, 하나의 y(i)의 레이블을 갖는 것이 아니라
4개의 레이블이 필요할 것입니다. 이 예제에서는, 
보행자는 없고, 차량은 있으며, 정지 sign이 있고, 신호등은 없습니다. 그리고 여러분이 다른 요소를 감지하려고 하는 경우, y(i)는 더 많은 dimension을 가질 수 있겠죠. 하지만 현재로써는 4개로 유지하도록 하겠습니다. 그러면 y(i)는 4 x 1 벡터입니다. 트레이닝 테스트 레이블 전체를 보시면, 이전과 비슷하게, 트레이닝 데이터를 가로로 쌓을 것입니다. 이와같이 y(1)에서 
y(m)까지 말이죠. 하지만 이제는 y(i)는 4 x 1 벡터이고,
그러므로 여기는 긴 세로줄 벡터입니다. 여기 y 매트릭스는 이제 
4 x m 매트릭스입니다. 이전에는 y가 single 실수였는데요, 
여기가 1 x m 매트릭스가 됐었었겠죠 그럼. 이제 그럼 여러분이 할 수 있는 것은, 
여기 y값을 예측하기 위해, 신경망을 트레이닝 시키는 것입니다. 그러면 신경망 입력값 x를 가지고, 결과값은 y의 4차원의 값을 가질 수 있습니다. 여기서 보이다시피, 결과값으로 4개의 노드를 그렸습니다. 첫번째 노드를 통해 예측하려고 하는 것은 여기 사진에 있는 보행자 입니다. 두번째 결과값은 차량에 대한 예측을할 것입니다. 이것은 정비 sign이 있는지 예측하고, 
이것은 신호등이 있는지 예측할 것입니다. 그러므로 여기 y hat은 4차원입니다. 여기 신경망을 트레이닝 시키기 위해서는
신경망 네트워크의 loss를 정의해야 합니다. 그러므로 예측 결과값 y hat은 
4 x 1차원인데요, 이 값에 대해서, 전체 트레이닝세트에서의
평균 loss는 1 나누기 m 의 합, i가 1에서 m까지와, 그리고 개인별 예측의 loss에 대해서 j가 1에서 4까지의 합입니다. 그러므로 이 절차는 4가지의 요소에서, 보행자, 자동차, 장지 sign, 신호등에 대해서 그 값을 더하는 것입니다. 그리고 여기 L값은 보통 로지스틱 loss를 나타냅니다. 이것을 적자면, -yj i 로그 y hat ji - 1-y log 1 - y hat 인데요, 이전에 다루었던 이진분류법과 두드러지는 차이점은 이제 여러분은 j의 값이 1 에서 4 사이인 값을
합한다는 것입니다. 그리고 이것과 softmax regression의 주요 차이점은, softmax regression은 single example에 대해서
single label을 부여했는데요, 이런 것과는 달리, 여기 이 이미지는 복수의 레이블을 가질 수 있습니다. 그러므로 여러분은 이미지가 
보행자의 사진이라거나, 자동차의 사진이라거나, 정지 sign의 이미지라거나,
신호등 사진이라고 하는 것이 아닙니다. 한 사진에 대해서, 보행자가 있는지, 
챠량이 있는지, stop sign이 있는지, 신호등이 있는지, 이렇게 복수의 
물체가 같은 그림에 있을 수 있다는 것입니다. 이전 슬라이드에서 다루었던 예제를 보면, 차량도 있었고, stop sign도 있었습니다.
보행자나 신호등은 없었구요. 그러므로 여러분은 이 이미지에
1개의 레이블을 부여하는 것이 아닙니다. 복수 클래스들을 통해 각각의 클래스를 상대로 물어보고,
이미지에 어떤 물체가 표출되는지 확인하는 것입니다. 그렇기 때문에 이러한 설정을 통해 하나의 이미지가 복수의 레이블을 가질 수 있다고 설명드리는 것입니다. 여러분이 비용함수를 최소화 시키기위해 신경망을 트레이닝 한다면, 여러분은 multi-task 러닝을 실행하는 것입니다. 그럼 여러분이 하는것은 
각각의 이미지를 볼 수 있도록 하나의 신경망을 만들고,
이것을 통해 4개의 문제를 푸는 것과 같습니다. 이것은 하나의 이미지가 4개의 물체를 포함하고 있는지 
알려주는 역할을 합니다. 여러분이 할 수 있는 또 다른 방법은
4개의 개별 신경망은 트레이닝 시키는 것입니다. 하나의 네트워크가 4개를 실행하도록 하는 대신에
말이죠, 하지만 신경망의 초반 특성들의 일부가 다양한 물체들과 공유될 수 있다고하면, 한개의 신경망을 트레이닝 시켜 
4개의 일을 할 수 있도록 하는 것이 보통 더 나은 성능을 갖게 해줍니다. 4 개의 신경망을
완전히 따로 트레이닝 시키는 것보다 말이죠. 이것이 바로 multi-task 러닝의 힘입니다. 또 다른 상세 내용으로는, 이제까지는 알고리즘이 각각의 이미지가 모든 레이블을 
포함한 것처럼 설명했지만, multi-task 러닝은 어떤 이미지가 일부 물체만 레이블하더라고 잘 구현됩니다. 첫번째 트레이닝 example에서, 
어떤 사람이, 레이블하는 사람이 보행자가 있다고 알려주고, 차량은 없는데 stop sign과 신호등의 유무여부는 
귀찮아서 레이블하지 않았다고 해보겠습니다. 두번째 예제에서는, 보행자가 있고, 차량이 있는데, 첫번째와 마찬가지로, 레이블하는 사람이 
이미지를 보았음에도, stop sign이 있는지, 신호등이 있는지 등등의 정보를
레이블하지 않았다고 해보겠습니다. 또 다른 예제에서는 완전히 레이블 처리 되었을 수 있겠죠. 또 다른 예제에서는, 차량에 대한 유무여부만 레이블해서 물음표가 있는 경우도 있겠죠. 이런 데이터의 경우엔, 어떤 이미지들이 일부 레이블밖에 없는 경우에도, 나머지 레이블은 물음표이거나 신경 안쓰는 
레이블이라더라도 알고리즘을 트레이닝 시킬 수 있습니다. 알고리즘을 트레이닝 시키는 방식은, 이런 일부의 레이블이 의문점을 갖게하는 것이더라도 또는 레이블이 전혀 안 되었더라도
여기 j값이 1에서 4인 경우, 합이 j의 값이 0 또는 1의 레이블을 갖는 것만
합니다. 그러므로 물음표가 있는 경우엔, 
이 합에서 해당 항을 제외하고 레이블이 있는 것만 더합니다. 그러면 이러한 데이터세트도 이용할 수 있게 해줍니다. 그럼 언제 multi-task 러닝이 말이 될까요? 그럼 언제 multi-task 러닝이 말이 될까요? 3가지 경우가 성립할 때, 말이 된다고 봅니다. 첫번째로는, shared low-level features로부터 이득을 볼 수 있는 업무들을 트레이닝 시킬때, 자율주행차 같은 경우엔, 신호등,보행자, 차량을 인식하는 것이 stop sign과 같은 것을 인식하는 것에 도움을 줄 수 있는 
특성과 비슷할 것입니다. 이것들은 모두 도로의 특성을 가지고 있기 때문입니다. 두번째로, 필수 규칙은 아닌데요, 이것이 항상 옳지는 않습니다. 제가 보는 여러개의 성공적인 milti-task 러닝
환경에서는 각가의 업무에 있는 데이터량은 
서로 비슷합니다. transfer learning에서 봤듯이, 
업무 A에서 배우고 업무 B로 넘기는데요, 만약 백만개의 A업무 example이 있고, 1000개의 B업무 example이 있으면, 
여기 백만개의 예제에서 배운 지식이 훨씬 더 작은 B 업무의 데이터세트를 
증가시키는데 도움을 줄 수 있습니다. 그러면 multi-task 러닝은 어떨까요? multi-task 러닝의 경우에는, 
보통 2대의 업무보다 더 많이 있습니다. 그러면 이전에는 4개의 업무가 있었는데, 
이제 100개의 업무가 있다고 해보겠습니다. 그리고나서 multi-task 러닝을 진행해서
100개의 다른 물체를 한번에 인식하려고 합니다. 이런 경우, 
한 업무당 1000개의 example이 있을 수 있기 때문에, 하나의 업무 성능에만 집중하는 경우, 100번째 업무에 한번 중점을 두겠습니다.
이것은 A100이라고 부르겠습니다. 이런 최종 업무를 분리시켜 수행하려고 하면, 이 한가지의 업무를 트레이닝 시키기 위해
1000개의 example만이 있었을 것입니다. 이것은 100개의 업무고, 
99개의 다른 업무를 트레이닝 시킴으로꺼 나왔을 것입니다. 이것들의 합계는 99,000개의 트레이닝 example 들로
큰 부스트 효과가 있습니다. 그리고, 이런 비교적 작은 1000개의 샘플을 가지고 있는 A100 업무에서 
많은 지식을 제공함으로써 증가시킬 수 있습니다. 그리고 시스템적으로 또 다른 99개의 다른 업무들이
어떠한 데이터를 제공하거나 지식을 제공하여 100개의 업무에게 
도움을 줄 수 있습니다. 그렇기 때문에 두번째 부분에 대한 부분은 
엄격한 규칙이 적용되는 것은 아니지만, 제가 보는 것은, 어떤 업무에 집중할 때,
이것이 multi-task 러닝에서 부스트 효과가 있으려면, 다른 업무의 총 데이터 합산 양이
다른 업무에 비해 훨씬 더 많아야 한다는 것입니다. 이것을 만족시킬 수 있는 방법은
이런 오른쪽과 같은 example에서의 경우, 그리고 각각의 업무에서의
데이터량이 비슷한 경우, 정말로 중요한 것은, 만약 이미 1000개의 example이 
하나의 업무에서 있다고 하면, 다른 모든 업무에서는 1000개보다 훨씬 더 많아야 할
것입니다. 이 마지막 업무에서 성능이 잘 발휘될 수 있게
도와주려면 말이죠. 마지막으로 multi-task 러닝에은 업무에서 잘 할 수 있도록 큰 신경망을 트레이닝 시키는 경우에
더 잘 되는 편입니다. 그렇기 때문에 multi-task 러닝의 대안은 각각의 업무에 대해서 개별적으로 신경망을 트레이닝 시키는 것입니다. 보행자,차량,stop sign, 그리고 신호등 감지에 관해 1개의 신경망을 트레이닝 시키는 대신에, 개별적으로 보행자에 대해 1개 신경망을 트레이닝 시키고,
또 다른 망으로 챠량을 트레이닝 시키고, stop sign을 트레이닝 시키고, 또 개별적인 망에서 
신호등을 트레이닝 시킵니다. 그리하여 Rich Carona라는 리서치 연구원이 발견한 것은 개별적으로 신경망을 트레이닝 시키는 것보다 multi-task 러닝이 성능에 효과적이지 못한 부분은
신경망 네트워크가 충분히 크지 못한 경우입니다. 하지만 충분히 큰 신경망을 트레이닝 시킬 수 있는 경우, 
multi-task 러닝이 성능을 저하시키는 경우가 거의 없을 것입니다. 바라건대, 이런 다른 업무들을 따로 신경망에서 트레이닝 시키는 것보다 
성능을 증가시키는 것에 도움을 줄 것입니다. 이것이 멀티태스크 러닝에 대한 모든 내용인데요, 실제로는, multi-task 러닝이 
transfer 러닝보다는 훨씬 더 적게 쓰입니다. 저는 transfer learning이 데이터량이 적은데서 문제를 해결하기 위해 많이 어플에서 쓰이는 경우를
자주 봅니다. 그러므로 관련된 문제에서 많은 양의 데이터가 
있는 경우, 그 내용을 배우고, 새로운 문제에 transfer를 시킵니다. 그러나 multi-task 러닝은 여러분이 잘하고 싶어하는 업무의 양이 굉장히 큰 경우가 더 드뭅니다. 그 모든 업무를 한번에 트레이닝 시킬 수 있습니다. computer vision이 그 예가 될 수 있겠죠. 물체 감지 분야에서, 
신경망이 많은 여러가지 물체를 한번에 감지하려고 하는 multi-task 어플을 봅니다 이 경우, 물체를 감지하는데 있어, 따로 신경망을 트레이닝 시키는 것보다
더 잘 작동합니다. 하지만 평균적으로, transfer learning이 오늘날
더 많이 쓰입니다. 하지만 이 2개의 방법 모두 여러분이 쓸 수 있는 
도구들입니다. 요약하자면, multi-task 러닝은 여러분이 1개의 신경망이 
여러업무를 처리할 수 있도록 해줍니다. 그리고 이러한 특성은 따로 업무를 진행하는 것보다 여러분이 더 좋은 성능을 
가질 수 있도록 해줍니다. 주의하실 사항이 있는데요, 실제로는 transfer learning 이 multi-task learning보다 더 자주 쓰이는 것을 
보실 것입니다. 저는 그래서, 머신 러닝 문제를 풀고 싶은 경우에, 비교적 데이터세트의 크기가 작은 경우, 
transfer learning이 큰 도움을 줄 것이라 생각합니다. 데이터세트 크기가 훨씬 더 큰 문제를 찾으면 그 시점에서부터 신경망에서 트레이닝 시킬 수 있습니다. 그 다음에 데이터량이 적은 문제로 transfer 시킬 수 있죠. 결론적으로 transfer learning은 오늘날 매우 자주 쓰입니다. 어떤 어플에서는 transfer multi-task learning이 
쓰이는 경우도 있는데요, 제 생각에는 multi-task learning 은
transfer learning보다는 훨씬 덜 쓰이긴 합니다. 예외적인 분야는 computer vision object detection인데요, 여기서는 많은 수의 다른 물체들을 감지하는데 신경망을 트레이닝 시키는 경우입니다. 이런 경우에는, 신경망을 개별적으로 트레이닝 시키는 것보다 더 잘 작동합니다. 하지만 평균적으로 
transfer learning 과 multi-task learning 은 비슷하게 보여지는 경우가 
많을텐데요, 실제로는 multi-task learning보다 
transfer learning의 적용사례를 훨씬 더 많이 봤습니다. 제 생각에는 각각의 다른 업무들을 일일히 세팅하거나 찾는 일이 굉장히 어렵기 때문에 그런 것 같습니다. 다시 말씀드리지만, computer vision에서는 물체 감지 예제가 가장 두드러지는 예외 적용 사례입니다. 멀티태스크 러닝에 대한 내용은 이것으로 모두 마쳤는데요. 여러분이 사용할 수 있는 것들 중, 멀티태스크 러닝과 transfer 러닝은 모두
중요한 도구입니다. 마지막으로, end-to-end 딥러닝에 대해 
이야기하고 싶은데요. 그러면 다음 비디오로 넘어가서, 
end-to-end에 대한 내용을 다루도록 하겠습니다.