1
00:00:00,000 --> 00:00:05,250
深度学习中最有力的方法之一，是有时你可以把在一个任务中神经网络

2
00:00:05,250 --> 00:00:10,560
学习到的东西，应用到另一个任务中去。

3
00:00:10,560 --> 00:00:12,815
比如，你可以让神经网络

4
00:00:12,815 --> 00:00:15,720
学习去识别物体，比如猫，然后用学习到的

5
00:00:15,720 --> 00:00:18,085
（一部分）知识

6
00:00:18,085 --> 00:00:21,300
来帮助你更好地识别X射线的结果。

7
00:00:21,300 --> 00:00:24,920
这就是所谓的迁移学习。让我们来看看。

8
00:00:24,920 --> 00:00:30,180
假如你现在已经训练了一个图像识别的神经网络，

9
00:00:30,180 --> 00:00:34,395
首先你设计一个神经网络并使用X-Y对来对其进行训练

10
00:00:34,395 --> 00:00:37,700
其中，X代表一张图片，Y代表某个物体（的标签）

11
00:00:37,700 --> 00:00:41,340
图片可以是一只猫，狗，鸟或者其他的东西

12
00:00:41,340 --> 00:00:43,960
如果你想要将这个已经经过学习的神经网络提取出来并适用到

13
00:00:43,960 --> 00:00:45,465
或者我们说，迁移到

14
00:00:45,465 --> 00:00:47,670
另外一个不同的任务中

15
00:00:47,670 --> 00:00:51,750
例如放射扫描结果的诊断过程

16
00:00:51,750 --> 00:00:54,780
即真实地读取X光扫描图像

17
00:00:54,780 --> 00:00:59,160
你可以做的就是，取出这个神经网络的最后一层

18
00:00:59,160 --> 00:01:03,885
并移除掉这一层以及其相关的权重

19
00:01:03,885 --> 00:01:08,880
然后为最后一层神经网络创建一个新的随机初始化的权重

20
00:01:08,880 --> 00:01:15,000
使用新建的这个输出层来进行放射结果的诊断

21
00:01:15,000 --> 00:01:17,160
所以，具体来说，在训练模型的第一阶段，

22
00:01:17,160 --> 00:01:19,795
当你在做图像识别的任务时，

23
00:01:19,795 --> 00:01:23,710
你训练了所有常用的神经网络的参数，所有权值

24
00:01:23,710 --> 00:01:27,150
所有层，然后这个模型

25
00:01:27,150 --> 00:01:32,680
现在能够做出图像识别的预测。

26
00:01:32,680 --> 00:01:35,370
训练得出了这样一个神经网络之后，

27
00:01:35,370 --> 00:01:41,540
要实现迁移学习，你现在需要把数据集X和Y

28
00:01:41,540 --> 00:01:47,120
设定为放射影像。

29
00:01:47,120 --> 00:01:50,580
现在的Y是你想要预测的诊断结果，

30
00:01:50,580 --> 00:01:58,340
你要做的是初始化最后一层的权值

31
00:01:58,340 --> 00:02:00,250
我们叫这个为WL

32
00:02:00,250 --> 00:02:02,360
和PL。

33
00:02:02,360 --> 00:02:07,175
然后现在，重新在新的数据集上训练这个神经网络，

34
00:02:07,175 --> 00:02:09,320
就是新的放射数据集上。

35
00:02:09,320 --> 00:02:14,260
你有几种方法来重新训练这个放射数据的神经网络。

36
00:02:14,260 --> 00:02:16,905
如果你只有一个小的放射数据集，

37
00:02:16,905 --> 00:02:20,647
你可以只重新训练最后一层的权值，就是WL

38
00:02:20,647 --> 00:02:22,620
和PL，同时保留其它所有参数。

39
00:02:22,620 --> 00:02:23,975
如果你有足够的数据，

40
00:02:23,975 --> 00:02:27,400
你也可以重新训练神经网络的其余所有层。

41
00:02:27,400 --> 00:02:32,535
我们的经验就是如果你有一个小规模的数据集，

42
00:02:32,535 --> 00:02:35,560
那么就去重新训练最后一层和输出层的神经网络，

43
00:02:35,560 --> 00:02:37,070
或者你也可以训练最后一到两层的神经网络。

44
00:02:37,070 --> 00:02:38,755
但是如果你有大量的数据，

45
00:02:38,755 --> 00:02:42,490
你或许可以对这个神经网络的所有参数都进行重新训练。

46
00:02:42,490 --> 00:02:45,775
要是你对神经网络的所有参数进行重新训练，

47
00:02:45,775 --> 00:02:49,270
那么这样训练的初始化阶段

48
00:02:49,270 --> 00:02:53,938
有时候被我们叫做预训练（pre-training）

49
00:02:53,938 --> 00:02:57,355
原因是，你在是使用图像识别的数据

50
00:02:57,355 --> 00:03:01,745
来预初始化（pre-initialize）或者说预训练神经网络的权重。

51
00:03:01,745 --> 00:03:04,545
然后如果你在之后对所有的权重进行更新，

52
00:03:04,545 --> 00:03:09,885
那么在放射扫描的数据上的训练有时候被我们叫做微调（fine tuning)

53
00:03:09,885 --> 00:03:15,185
所以有时候你会在深度学习领域听到预训练（pre-training）和微调（fine tuning）这些词，

54
00:03:15,185 --> 00:03:17,530
上面我所说的就是他们在迁移学习中

55
00:03:17,530 --> 00:03:21,050
表达的真正含义。

56
00:03:21,050 --> 00:03:22,585
你在这个例子中已经做的

57
00:03:22,585 --> 00:03:25,435
是把已经从图像识别中学到的知识信息，

58
00:03:25,435 --> 00:03:31,285
应用到，或者说迁移到了放射扫描诊断中。

59
00:03:31,285 --> 00:03:33,490
之所以这样的过程是有用的，

60
00:03:33,490 --> 00:03:36,570
是因为从从大规模的图像识别数据集中

61
00:03:36,570 --> 00:03:39,400
学习到的边界检测，曲线检测，明暗对象检测等

62
00:03:39,400 --> 00:03:43,045
低层次的信息，

63
00:03:43,045 --> 00:03:47,736
或许能够帮助你的学习算法更好地去进行放射扫描结果的诊断。

64
00:03:47,736 --> 00:03:51,730
它会在图像的组织结构和自然特性中学习到很多信息，

65
00:03:51,730 --> 00:03:56,465
其中一些信息会非常的有帮助。

66
00:03:56,465 --> 00:03:58,545
所以当神经网络学会了图像识别

67
00:03:58,545 --> 00:04:00,910
意味着它可能学习到了以下信息：

68
00:04:00,910 --> 00:04:03,135
关于不同图片的

69
00:04:03,135 --> 00:04:05,880
点，线，曲面等等信息，

70
00:04:05,880 --> 00:04:07,725
在不同图片中看起来是什么样子的。

71
00:04:07,725 --> 00:04:09,555
或许关于一个物体对象的很小的细节，

72
00:04:09,555 --> 00:04:10,950
都能够帮助你

73
00:04:10,950 --> 00:04:15,910
在放射信息诊断的神经网络中，学习得更快一些或者减少学习需要的数据。

74
00:04:15,910 --> 00:04:17,545
来看另一个例子下面再举另一个例子

75
00:04:17,545 --> 00:04:20,730
假若你现在已经训练好了一个语音识别系统，

76
00:04:20,730 --> 00:04:24,398
其中，X为你的语音或者语音片段的输入，

77
00:04:24,398 --> 00:04:27,545
而Y是一些对应的文字片段。

78
00:04:27,545 --> 00:04:34,200
就是说你已经训练好了一个语音识别系统来输出对应的文字。

79
00:04:34,200 --> 00:04:39,435
现在假设你想要构建一个“唤醒词”

80
00:04:39,435 --> 00:04:45,345
或者一个“触发词”检测系统。

81
00:04:45,345 --> 00:04:49,580
想一想一个唤醒词或者一个触发词的含义是，

82
00:04:49,580 --> 00:04:54,100
为了唤醒我们的语音控制设备，我们会说

83
00:04:54,100 --> 00:04:58,610
“Alexa”来唤醒Amazon echo，或者”Ok Google“来唤醒我们谷歌设备，又或者

84
00:04:58,610 --> 00:05:03,590
我们说“hey, Siri”来唤醒苹果设备，说“你好，百度”来唤醒百度设备。

85
00:05:03,590 --> 00:05:05,120
为了实现这些功能，

86
00:05:05,120 --> 00:05:09,080
你可以同样去除神经网络的最后一层，

87
00:05:09,080 --> 00:05:13,435
然后创建一个新的输出节点。

88
00:05:13,435 --> 00:05:18,995
但是有时候，另外一个你可以做的事儿是，<br /> 你可以不仅仅去创建一个单一的输出，

89
00:05:18,995 --> 00:05:23,120
而是创建好几层新的神经网络来让你的神经网络

90
00:05:23,120 --> 00:05:28,215
来给你的语音唤醒检测系统打上对应的Y标签。

91
00:05:28,215 --> 00:05:30,425
接下来，根据你拥有的数据集规模

92
00:05:30,425 --> 00:05:34,400
你可能仅对你的神经网络中新建的几层进行训练，

93
00:05:34,400 --> 00:05:38,925
或者你也可以重新训练其中更多的几层神经网络。

94
00:05:38,925 --> 00:05:42,150
那么迁移学习在什么时候有用呢

95
00:05:42,150 --> 00:05:46,845
当你在你的被迁移的模型中拥有大量的数据，

96
00:05:46,845 --> 00:05:49,110
而你在你需要解决的问题上拥有相对较少的数据时，

97
00:05:49,110 --> 00:05:52,430
迁移学习是适用的。

98
00:05:52,430 --> 00:05:58,030
例如，假设你在一个图像识别任务中拥有一百万个样本。

99
00:05:58,030 --> 00:06:00,605
这就意味着大量数据中的

100
00:06:00,605 --> 00:06:03,095
低层次特征信息或者大量的

101
00:06:03,095 --> 00:06:06,385
有帮助的特征信息在神经网络的前几层被学习到了。

102
00:06:06,385 --> 00:06:08,240
但是对于放射扫描结果的诊断任务，

103
00:06:08,240 --> 00:06:12,005
你或许仅仅拥有几百个样本。

104
00:06:12,005 --> 00:06:15,650
所以，你在放射诊疗这个问题上就只拥有非常小的数据，

105
00:06:15,650 --> 00:06:17,530
或许仅仅是100个X光扫描样本。

106
00:06:17,530 --> 00:06:23,070
所以你从图像识别中学习到的大量的信息可以被用于迁移并且

107
00:06:23,070 --> 00:06:24,560
这些信息会有效地帮助你处理好

108
00:06:24,560 --> 00:06:29,360
放射诊疗，哪怕你没有足够多的数据来开展放射诊疗任务。

109
00:06:29,360 --> 00:06:31,800
又比如语音识别，假定或许你使用10000小时的数据

110
00:06:31,800 --> 00:06:35,110
来训练好了一个语音识别系统。

111
00:06:35,110 --> 00:06:37,700
那么，你已经从人类的语音中学习到了很多的信息，

112
00:06:37,700 --> 00:06:41,270
10000个小时的数据真的好多好多啦。

113
00:06:41,270 --> 00:06:43,220
但是，在你的语音唤醒系统中，

114
00:06:43,220 --> 00:06:45,735
或许你仅仅有一个小时的数据。

115
00:06:45,735 --> 00:06:48,800
所以，这并不是我们说的用大量的数据来解出大量的参数的问题，

116
00:06:48,800 --> 00:06:53,215
那么在这个例子中，你从人类说话中学习到的大量信息更像是，

117
00:06:53,215 --> 00:06:56,450
人类语音中的组成信息等等，

118
00:06:56,450 --> 00:07:00,300
而这在构建一个语音唤醒系统中是真正有用的，

119
00:07:00,300 --> 00:07:03,220
尽管，你在这个任务中仅有一个相对小规模的数据集，或者说

120
00:07:03,220 --> 00:07:08,005
至少是一个相对唤醒词检测来说及其小量的数据集。

121
00:07:08,005 --> 00:07:09,440
所以总结一下就是，在这两个例子中，

122
00:07:09,440 --> 00:07:11,500
你把一个有大量数据的问题模型，

123
00:07:11,500 --> 00:07:15,610
迁移到了一个相对之下仅有很少数据的问题模型中。

124
00:07:15,610 --> 00:07:19,480
一个迁移学习不适用的例子是，

125
00:07:19,480 --> 00:07:22,330
（上面讲到的）数据规模被颠倒过来的时候。

126
00:07:22,330 --> 00:07:27,560
所以，如果你有一百个图像样本来做图像识别，

127
00:07:27,560 --> 00:07:34,120
然后你有100个，<br /> 或者甚至是1000个样本来做放射诊疗任务。

128
00:07:34,120 --> 00:07:38,395
做这个事儿的时候，<br /> 你可能首先想到的是把放射诊断任务做好，

129
00:07:38,395 --> 00:07:41,830
假设你真地想要把这个诊断任务做好，

130
00:07:41,830 --> 00:07:47,670
那么放射扫描图像是比猫，狗等等的图片更加地有价值。

131
00:07:47,670 --> 00:07:52,060
所以在这个地方的样本<br />就比在这个地方的样本更有价值，

132
00:07:52,060 --> 00:07:55,935
至少，在构建一个放射诊断系统时是这样的。

133
00:07:55,935 --> 00:07:58,810
那么，如果你已经在放射诊断上有更多的数据时，

134
00:07:58,810 --> 00:08:01,955
那么100张随机的

135
00:08:01,955 --> 00:08:06,310
猫，狗或者汽车等等的图片并没有什么用

136
00:08:06,310 --> 00:08:12,130
因为在你猫和狗的图像识别任务中一张图片的价值

137
00:08:12,130 --> 00:08:15,430
就是比用于构建一个良好的放射诊断系统时

138
00:08:15,430 --> 00:08:19,870
用到的X光照片更加地低。

139
00:08:19,870 --> 00:08:22,925
小结一下，这就是一个迁移学习的反例。

140
00:08:22,925 --> 00:08:27,515
这样子做不会对模型造成损失，
但是我也不能看出这样做有什么实际的意义。

141
00:08:27,515 --> 00:08:31,030
相似地，如果你使用10小时的语音数据来构建一个语音识别系统，

142
00:08:31,030 --> 00:08:34,660
但是实际上你有10小时甚至更多的，

143
00:08:34,660 --> 00:08:38,330
假设50个小时的数据来构建一个语音唤醒系统，

144
00:08:38,330 --> 00:08:40,505
这或许不会造成损失，

145
00:08:40,505 --> 00:08:44,010
或许你将这10个小时的数据添加到迁移学习中不会造成什么损失，

146
00:08:44,010 --> 00:08:47,350
但是你也并不要期望这会给你什么有意义的增益。

147
00:08:47,350 --> 00:08:51,220
总结一下，什么时候迁移学习是适用的呢?

148
00:08:51,220 --> 00:08:53,200
如果你尝试

149
00:08:53,200 --> 00:09:00,830
从某个任务A中学习到的信息迁移到某个任务B中，

150
00:09:00,830 --> 00:09:07,825
那么当任务A和任务B在有相同的输入时，迁移学习才能适用。

151
00:09:07,825 --> 00:09:10,285
在第一个例子中，

152
00:09:10,285 --> 00:09:12,455
A和B都以图片作为输入。

153
00:09:12,455 --> 00:09:13,585
在第二个例子中，

154
00:09:13,585 --> 00:09:17,260
它们都以语音片段作为输入。

155
00:09:17,260 --> 00:09:22,460
必须在任务A比任务B有更多的数据时，迁移学习才适用。

156
00:09:22,460 --> 00:09:27,345
所有条件都是在想要把任务B做好的前提下

157
00:09:27,345 --> 00:09:32,023
因为在任务B中的数据比在任务A中更加地有价值，

158
00:09:32,023 --> 00:09:36,765
通常你需要为任务A准备更多的数据，因为

159
00:09:36,765 --> 00:09:43,605
在任务A中的样本比任务B中的样本价值更低。

160
00:09:43,605 --> 00:09:47,740
最后，迁移学习更加适用的场景

161
00:09:47,740 --> 00:09:52,300
是当你认为，任务A中的低层次特征会帮助任务B达成目标。

162
00:09:52,300 --> 00:09:54,395
在上面我们提到的例子中，

163
00:09:54,395 --> 00:09:57,010
或许图像识别任务教给了你足够的信息

164
00:09:57,010 --> 00:09:59,800
用迁移学习来进行一个放射诊断，

165
00:09:59,800 --> 00:10:02,200
或许语音识别的例子教给你了

166
00:10:02,200 --> 00:10:06,000
如何把语音识别系统迁移到语音唤醒系统中。

167
00:10:06,000 --> 00:10:08,350
继续总结，迁移学习必须是

168
00:10:08,350 --> 00:10:11,560
你尝试把任务B中的任务做好时才会有效，

169
00:10:11,560 --> 00:10:14,480
即通常情况下，你要把这个拥有更少数据的任务做好时。

170
00:10:14,480 --> 00:10:16,910
例如，在放射诊断中，

171
00:10:16,910 --> 00:10:18,310
获取大量的X光扫描

172
00:10:18,310 --> 00:10:21,990
来进行一个良好的放射诊断系统是很难的。

173
00:10:21,990 --> 00:10:25,270
在这个例子中，你或许发现一个相关但又不同的任务，

174
00:10:25,270 --> 00:10:26,645
例如图像识别，

175
00:10:26,645 --> 00:10:30,850
在这样的任务中，你或许可以获取到百万级的数据
并且从中学习到很多低层次的特征。

176
00:10:30,850 --> 00:10:34,180
如此以来你就能够通过迁移学习
来尝试把任务B即放射诊断任务做得很好，

177
00:10:34,180 --> 00:10:38,166
尽管你并没有那么足够多的数据。

178
00:10:38,166 --> 00:10:40,305
那，什么时候迁移学习会是适用的呢？

179
00:10:40,305 --> 00:10:43,690
迁移学习确确实实会提高你所进行的任务的表现。

180
00:10:43,690 --> 00:10:47,865
但是我也有遇到过，迁移学习在应用于

181
00:10:47,865 --> 00:10:52,360
任务A实际上拥有比任务B更少的数据的时候，
在这样的情况下，

182
00:10:52,360 --> 00:10:55,285
你并不能得到很大的增益。

183
00:10:55,285 --> 00:10:57,900
那么，这就是迁移学习，

184
00:10:57,900 --> 00:11:00,895
一种你把从一个任务中学到的信息迁移到另外一个不同的任务中的方式。

185
00:11:00,895 --> 00:11:02,585
还有另外一种关于

186
00:11:02,585 --> 00:11:05,080
从多个任务中学习的版本叫做多任务学习，

187
00:11:05,080 --> 00:11:07,510
这是一种当你尝试同时从多个任务中学习的方式，

188
00:11:07,510 --> 00:11:10,845
而不是序列化的先从一个任务学习，

189
00:11:10,845 --> 00:11:14,170
然后迁移到另外一个任务的学习。

190
00:11:14,170 --> 00:11:15,450
下个视频再见

191
00:11:15,450 --> 00:11:17,340
我们将会讨论一下多任务学习。