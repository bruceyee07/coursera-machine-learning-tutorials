1
00:00:00,000 --> 00:00:02,880
近期，深度学习最令人兴奋的进展之一

2
00:00:02,880 --> 00:00:05,765
是端到端深度学习的崛起

3
00:00:05,765 --> 00:00:07,570
那么，什么是“端到端“的深度学习呢？

4
00:00:07,570 --> 00:00:10,510
简单地说，我们有一些数据处理系统

5
00:00:10,510 --> 00:00:13,880
或者是由多个阶段组成的学习系统

6
00:00:13,880 --> 00:00:15,330
端到端的深度学习做的

7
00:00:15,330 --> 00:00:17,625
就是它可以捕获所有的阶段

8
00:00:17,625 --> 00:00:20,968
并且，通常可以将其替代为单个神经网络

9
00:00:20,968 --> 00:00:24,170
也就是说运行速度更快

10
00:00:24,170 --> 00:00:26,460
以语音识别为例

11
00:00:26,460 --> 00:00:30,975
其目标是接收输入音频片段X

12
00:00:30,975 --> 00:00:33,015
将其转换为该音频剪辑对应的

13
00:00:33,015 --> 00:00:37,230
脚本输出Y。

14
00:00:37,230 --> 00:00:41,550
传统的语音识别分为多个处理阶段。

15
00:00:41,550 --> 00:00:44,085
首先，需要提取音频的一些特征，

16
00:00:44,085 --> 00:00:46,045
一些人工设计的音频特征。

17
00:00:46,045 --> 00:00:48,885
或许你听说过MFCC

18
00:00:48,885 --> 00:00:53,570
这种算法（MFCC），
用于提取一组特定的人工设计的音频特征。

19
00:00:53,570 --> 00:00:55,845
提取了低级特征之后，

20
00:00:55,845 --> 00:00:58,260
可以应用机器学习算法

21
00:00:58,260 --> 00:01:01,530
从音频剪辑中查找音素

22
00:01:01,530 --> 00:01:04,390
音素是声音的基本单位

23
00:01:04,390 --> 00:01:07,810
比如说，单词"cat"由三个音构成

24
00:01:07,810 --> 00:01:10,690
Cu,Ah和Tu，算法会提取出这三个音素

25
00:01:10,690 --> 00:01:13,630
然后把音素串在一起，形成单词

26
00:01:13,630 --> 00:01:19,356
再将这些单词串在一起，构成音频剪辑的脚本。

27
00:01:19,356 --> 00:01:23,365
不同于上述由多个阶段组成的途径（管道）

28
00:01:23,365 --> 00:01:24,940
端到端的深度学习

29
00:01:24,940 --> 00:01:28,735
可以训练一个庞大的神经网络，只需输入音频片段，

30
00:01:28,735 --> 00:01:32,670
然后直接输出脚本。

31
00:01:32,670 --> 00:01:35,800
人工智能的一个有趣的社会学效应是

32
00:01:35,800 --> 00:01:39,085
在端到端的深度学习开始更好地工作的同时，

33
00:01:39,085 --> 00:01:41,545
有一些研究人员花费了

34
00:01:41,545 --> 00:01:44,830
数年的时间设计管道的各个阶段

35
00:01:44,830 --> 00:01:50,249
在不同的领域都有一些研究人员

36
00:01:50,249 --> 00:01:52,360
不只是语音识别，可能是计算机视觉，或者其它领域，

37
00:01:52,360 --> 00:01:53,945
已经花费了很多时间，

38
00:01:53,945 --> 00:01:57,332
写了多篇论文，甚至在他们的职业生涯中创建了很多

39
00:01:57,332 --> 00:02:00,270
工程特征或其它管道部件。

40
00:02:00,270 --> 00:02:02,990
而端到端的深度学习只是

41
00:02:02,990 --> 00:02:06,475
从最新的训练数据集中，直接学习从x到y的映射，

42
00:02:06,475 --> 00:02:09,720
真的绕过了很多中间步骤，

43
00:02:09,720 --> 00:02:13,300
对于一些领域来说

44
00:02:13,300 --> 00:02:17,255
接受这种构建人工智能系统的替代方法是一个挑战。

45
00:02:17,255 --> 00:02:20,110
因为它在某些情况下，确实淘汰了

46
00:02:20,110 --> 00:02:23,665
很多年对于（管道）中间部件的研究

47
00:02:23,665 --> 00:02:27,070
结果表明，端到端深度学习的一个挑战是

48
00:02:27,070 --> 00:02:30,670
它需要大量的数据才能取得很好的结果

49
00:02:30,670 --> 00:02:33,030
例如，如果用3000小时的

50
00:02:33,030 --> 00:02:35,935
训练数据构建一个语音识别系统，

51
00:02:35,935 --> 00:02:37,600
传统的方法（Pipeline）

52
00:02:37,600 --> 00:02:40,675
传统的完整管道方法（Pipeline）
能取得很好的结果。

53
00:02:40,675 --> 00:02:42,745
只有当你有一个非常大的数据集，

54
00:02:42,745 --> 00:02:45,290
比如说10,000小时的数据，

55
00:02:45,290 --> 00:02:49,000
也许增加到100,000小时的数据时，

56
00:02:49,000 --> 00:02:53,350
端到端的方法才突然开始工作得很好。

57
00:02:53,350 --> 00:02:55,000
因此当你的数据集较小时，

58
00:02:55,000 --> 00:02:58,990
传统的管道方法实际上同样有效，

59
00:02:58,990 --> 00:03:00,700
有时甚至更好。

60
00:03:00,700 --> 00:03:06,855
你需要一个大的数据集以便端到端的方法真正突显其作用。

61
00:03:06,855 --> 00:03:08,850
如果你有中等量的数据，

62
00:03:08,850 --> 00:03:12,365
可以采用折衷的方法：输入音频，

63
00:03:12,365 --> 00:03:16,420
绕过这些特征，只学习输出神经网络的音素，

64
00:03:16,420 --> 00:03:17,765
然后继续其它阶段。

65
00:03:17,765 --> 00:03:19,420
因此这是端到端学习的一个步骤，

66
00:03:19,420 --> 00:03:29,065
不是彻底的端到端学习。

67
00:03:29,065 --> 00:03:34,855
这张照片显示的是旋转闸门人脸识别，

68
00:03:34,855 --> 00:03:36,600
由百度的研究者林元庆开发，

69
00:03:36,600 --> 00:03:41,050
这个摄像头对着接近大门的人，

70
00:03:41,050 --> 00:03:43,135
如果它识别出那个人，

71
00:03:43,135 --> 00:03:46,645
旋转闸门自动让他们通过。

72
00:03:46,645 --> 00:03:51,170
因此人们进门时不再需要刷RFID徽章，

73
00:03:51,170 --> 00:03:53,410
在中国，越来越多的办公室使用这种方式

74
00:03:53,410 --> 00:03:56,270
希望也有越来越多其它国家

75
00:03:56,270 --> 00:03:59,485
你只需要接近旋转门，如果它识别出你的脸，

76
00:03:59,485 --> 00:04:04,155
就会让你通过，你不需要携带RFID徽章。

77
00:04:04,155 --> 00:04:07,385
那么如何构建一个这样的系统呢？

78
00:04:07,385 --> 00:04:12,590
嗯，你可以做的一件事情是看看摄像头捕捉到的图像。

79
00:04:12,590 --> 00:04:14,645
对吗？我图画得不好。

80
00:04:14,645 --> 00:04:16,190
设想这是一张摄像头拍下的照片。

81
00:04:16,190 --> 00:04:19,390
你知道，有人接近旋转门，

82
00:04:19,390 --> 00:04:23,810
因此，这可能是摄像头捕捉的图像X。

83
00:04:23,810 --> 00:04:26,950
你可以试着学习一个函数，

84
00:04:26,950 --> 00:04:31,470
直接建立图像X与个人Y的映射。

85
00:04:31,470 --> 00:04:34,580
事实证明，这不是最好的方法。

86
00:04:34,580 --> 00:04:36,395
其中一个问题是

87
00:04:36,395 --> 00:04:39,835
人们可以从不同的方向接近旋转门。

88
00:04:39,835 --> 00:04:41,945
因此他们可能在绿色的位置，

89
00:04:41,945 --> 00:04:43,195
或者在蓝色的位置。

90
00:04:43,195 --> 00:04:45,245
有时候他们离摄像头很近，

91
00:04:45,245 --> 00:04:47,020
因此在图像中显得很大。

92
00:04:47,020 --> 00:04:49,225
有时他们非常接近摄像头，

93
00:04:49,225 --> 00:04:51,600
以至于脸显得非常大。

94
00:04:51,600 --> 00:04:54,205
因此，旋转门应用实际上

95
00:04:54,205 --> 00:04:56,350
不只是采集原始影像，

96
00:04:56,350 --> 00:04:59,315
把它注入神经网络，试图推断一个人的身份。

97
00:04:59,315 --> 00:05:02,005
相反，迄今为止最好的方法

98
00:05:02,005 --> 00:05:05,140
似乎是一个多步骤的方法。首先，

99
00:05:05,140 --> 00:05:09,210
运行一个软件来检测此人的脸。

100
00:05:09,210 --> 00:05:12,820
所以这个第一个探测器计算这个人的脸的位置。

101
00:05:12,820 --> 00:05:14,785
检测到人脸之后，

102
00:05:14,785 --> 00:05:16,930
放大图片中的这部分，

103
00:05:16,930 --> 00:05:24,540
并进行裁剪，

104
00:05:24,540 --> 00:05:29,340
使人脸位于图像中心。

105
00:05:29,340 --> 00:05:34,757
这是我图中我画的红色的部分，

106
00:05:34,757 --> 00:05:36,080
然后将其送入神经网络，

107
00:05:36,080 --> 00:05:38,010
试着去学习，

108
00:05:38,010 --> 00:05:40,485
或者估测此人的身份。

109
00:05:40,485 --> 00:05:42,150
研究者们发现

110
00:05:42,150 --> 00:05:45,615
比起一步学到所有的东西更加有效的是

111
00:05:45,615 --> 00:05:48,780
把这个问题拆分为两个步骤：

112
00:05:48,780 --> 00:05:51,490
首先找到脸的位置，

113
00:05:51,490 --> 00:05:54,890
再推断这是谁的脸。

114
00:05:54,890 --> 00:05:58,010
第二种方法使得学习算法（事实上有两个学习算法）

115
00:05:58,010 --> 00:06:03,560
得以解决两个比较简单的任务，从而总体上获得更好地效果。

116
00:06:03,560 --> 00:06:05,930
顺便说一下，如果你想知道

117
00:06:05,930 --> 00:06:08,975
我简化了讨论。

118
00:06:08,975 --> 00:06:11,865
顺便说一下，如果你想知道第二步如何工作，

119
00:06:11,865 --> 00:06:13,605
我简化了讨论。

120
00:06:13,605 --> 00:06:15,470
实际上第二步的训练方式是

121
00:06:15,470 --> 00:06:16,730
当你训练网络时，

122
00:06:16,730 --> 00:06:18,920
输入两个图像。

123
00:06:18,920 --> 00:06:22,230
你的网络所做的是

124
00:06:22,230 --> 00:06:29,553
告诉你这两个输入图像，是不是同一个人。

125
00:06:29,553 --> 00:06:34,700
所以，如果你有1万个员工的ID的文件，

126
00:06:34,700 --> 00:06:36,430
你可以拿这个红色的图像，

127
00:06:36,430 --> 00:06:38,545
然后将它与

128
00:06:38,545 --> 00:06:41,795
1万个员工的ID比较，试图找出

129
00:06:41,795 --> 00:06:44,810
这个红色的图片是否对应一万个员工之一

130
00:06:44,810 --> 00:06:48,640
可以进入某个设施或你的办公楼

131
00:06:48,640 --> 00:06:51,770
这是个旋转闸门限制员工出入工作场所（的场景）

132
00:06:51,770 --> 00:06:55,970
那么为什么第二个方法效果更好呢？

133
00:06:55,970 --> 00:06:58,605
实际上由两个原因。

134
00:06:58,605 --> 00:07:02,020
一方面是因为这两个子问题每个和原问题比都简单很多，

135
00:07:02,020 --> 00:07:10,360
更重要的是，你有很多数据可供这两个子问题使用。

136
00:07:10,360 --> 00:07:16,950
特别是你可以获得很多数据用于人脸检测，

137
00:07:16,950 --> 00:07:18,720
即第一个子问题:

138
00:07:18,720 --> 00:07:20,820
检查一张图片，

139
00:07:20,820 --> 00:07:23,415
找到人脸在这个图片中的位置

140
00:07:23,415 --> 00:07:25,695
所以有很多数据，

141
00:07:25,695 --> 00:07:27,610
很多有标签的数据（X, Y)

142
00:07:27,610 --> 00:07:31,420
其中X是一张图片，而y代表人脸在这个图片中的位置。

143
00:07:31,420 --> 00:07:35,100
所以你能够建立一个神经网络很好的处理第一个子问题。

144
00:07:35,100 --> 00:07:37,440
同样的，有很多数据可以用于第二个子问题。

145
00:07:37,440 --> 00:07:41,710
今天，处于行业领先地位的公司拥有，比如说，

146
00:07:41,710 --> 00:07:44,525
成千上万张人脸图片。

147
00:07:44,525 --> 00:07:46,845
输入一张沿着脸剪裁的图片，

148
00:07:46,845 --> 00:07:49,275
比如说这张红色图片或者这张下方的图片，

149
00:07:49,275 --> 00:07:51,555
今天,处于行业领先地位的人脸识别团队拥有

150
00:07:51,555 --> 00:07:53,880
至少数亿的图像可以使用，

151
00:07:53,880 --> 00:07:55,980
从而比较两张图片然后尝试着

152
00:07:55,980 --> 00:07:58,765
识别出人的身份或者两张图片是否是同一个人。

153
00:07:58,765 --> 00:08:02,310
同样的，也有很多数据可以用于第二个子问题。

154
00:08:02,310 --> 00:08:07,155
然而，如果你尝试着让一个模型同时学习所有的步骤，

155
00:08:07,155 --> 00:08:10,680
只能获取很少数据（X，Y）。

156
00:08:10,680 --> 00:08:13,175
其中X是如图所示的旋转栅图片，

157
00:08:13,175 --> 00:08:16,390
Y是人的身份。

158
00:08:16,390 --> 00:08:21,610
因为没有足够的数据去解决端到端的学习问题，

159
00:08:21,610 --> 00:08:27,410
却有足够的数据去解决两个子问题，所以在实际应用中，

160
00:08:27,410 --> 00:08:29,740
把整个问题分成两个子问题来解决会

161
00:08:29,740 --> 00:08:34,260
比完全端到端的深度学习方案得到更好的性能。

162
00:08:34,260 --> 00:08:37,575
尽管如果你有足够的数据，

163
00:08:37,575 --> 00:08:40,470
也许端到端的方案会更加好，

164
00:08:40,470 --> 00:08:44,240
但是目前来说在实际应用中这不是最好的方法。

165
00:08:44,240 --> 00:08:46,886
让我们来看看另一些例子

166
00:08:46,886 --> 00:08:49,000
以机器翻译作为例子，

167
00:08:49,000 --> 00:08:54,390
传统上机器翻译系统也是一个非常冗长和复杂的流程，

168
00:08:54,390 --> 00:08:56,230
首先获取，比如说，英文的

169
00:08:56,230 --> 00:08:58,955
文本然后做文本分析。

170
00:08:58,955 --> 00:09:01,990
简单来说,从文本中抽取一组特征等，

171
00:09:01,990 --> 00:09:04,630
然后经过很多步骤后，最终，比如说，

172
00:09:04,630 --> 00:09:07,900
得到一个从英文到法文的文本翻译。

173
00:09:07,900 --> 00:09:10,030
对于机器翻译来说，

174
00:09:10,030 --> 00:09:13,990
你可以获取很多英文和法文对应的文本对，

175
00:09:13,990 --> 00:09:17,640
所以端到端深度学习在机器翻译领域非常有效。

176
00:09:17,640 --> 00:09:20,140
那是因为今天，

177
00:09:20,140 --> 00:09:24,350
收集大量（X,Y）英文和法文对应的文本对的数据集

178
00:09:24,350 --> 00:09:28,780
英文和法文对应的文本对的数据集是可行的。

179
00:09:28,780 --> 00:09:29,955
在这个例子里

180
00:09:29,955 --> 00:09:32,320
端到端的深度学习效果很好。

181
00:09:32,320 --> 00:09:35,200
最后一个例子,假设你

182
00:09:35,200 --> 00:09:38,220
想通过查看儿童手掌的x光图片

183
00:09:38,220 --> 00:09:40,215
来估计这个儿童的年龄。

184
00:09:40,215 --> 00:09:41,830
你知道吗，我第一次听说这个问题的时候，

185
00:09:41,830 --> 00:09:45,460
我以为这是一个非常酷的犯罪场景调查的问题。

186
00:09:45,460 --> 00:09:48,610
你可能悲剧性地发现了一个的孩子的骨架，

187
00:09:48,610 --> 00:09:50,980
想分析出这个孩子的年龄。

188
00:09:50,980 --> 00:09:54,445
事实上，通过手掌的x光图片估计儿童的年龄

189
00:09:54,445 --> 00:09:57,010
的典型应用场景

190
00:09:57,010 --> 00:09:59,995
比我想象的犯罪场景调查要普通很多。

191
00:09:59,995 --> 00:10:02,890
事实上，儿科医生使用

192
00:10:02,890 --> 00:10:06,925
这个工具去评估一个儿童是否健康成长。

193
00:10:06,925 --> 00:10:09,460
如果使用非端到端的方法解决这个问题，

194
00:10:09,460 --> 00:10:14,045
是从一个图片中分割出或识别出骨骼，

195
00:10:14,045 --> 00:10:16,730
就是尝试着识别这个骨头部分在哪里，

196
00:10:16,730 --> 00:10:17,849
另外一个骨头部分在哪里，

197
00:10:17,849 --> 00:10:20,290
还有一个骨头部分在哪里等等。

198
00:10:20,290 --> 00:10:22,285
基于不同骨骼的长度，

199
00:10:22,285 --> 00:10:26,380
你可以大概从表中查找到儿童手掌的平均骨骼长度

200
00:10:26,380 --> 00:10:30,615
然后用这个信息来估计这个儿童的年龄。

201
00:10:30,615 --> 00:10:32,800
这样的方法实际上很有效。

202
00:10:32,800 --> 00:10:37,960
相反,如果你想直接从图片联系到孩子的年龄,

203
00:10:37,960 --> 00:10:43,030
你需要很多数据，而且基于我的了解，

204
00:10:43,030 --> 00:10:45,850
这种方法现今不太可行，就是

205
00:10:45,850 --> 00:10:50,515
因为没有足够的数据去支持端到端的训练。

206
00:10:50,515 --> 00:10:56,326
相反，你能够想象其实我们可以把这个问题分成两步。

207
00:10:56,326 --> 00:10:58,780
第一步是相对简单的方法，

208
00:10:58,780 --> 00:11:00,345
也许你不需要特别多的数据，

209
00:11:00,345 --> 00:11:03,455
也许你不需要特别多的X光图片数据
去分割出骨骼

210
00:11:03,455 --> 00:11:08,225
然后第二步，收集统计儿童手掌的信息，

211
00:11:08,225 --> 00:11:11,280
你也能基于不多的数据，得到比较准确的评估。

212
00:11:11,280 --> 00:11:14,125
所以，这种多步的方法看起来很有前景，

213
00:11:14,125 --> 00:11:16,420
可能比端到端的方法更可行

214
00:11:16,420 --> 00:11:20,635
除非你可以获取支持端到端学习的更多的数据。

215
00:11:20,635 --> 00:11:22,840
所以，如果端到端深度学习可行，

216
00:11:22,840 --> 00:11:26,650
可以非常有效和简化系统，且

217
00:11:26,650 --> 00:11:30,875
不用建立很多手工设计的单个组件。

218
00:11:30,875 --> 00:11:32,773
但是它不是万能的，

219
00:11:32,773 --> 00:11:34,315
它并不总是有效。

220
00:11:34,315 --> 00:11:35,650
这里这个

221
00:11:35,650 --> 00:11:39,530
我想与大家分享，更系统地讨论，什么时候你应该

222
00:11:39,530 --> 00:11:42,820
以及什么时候也许你不应该使用端到端的深度学习，还有如何

223
00:11:42,820 --> 00:11:47,000
把这些复杂的机器学习系统拼凑起来。