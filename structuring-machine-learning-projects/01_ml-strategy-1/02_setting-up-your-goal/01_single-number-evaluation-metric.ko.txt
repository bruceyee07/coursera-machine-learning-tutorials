여러분이 하이퍼 파라미터를 튜닝하거나, 알고리즘 교육에 다른 아이디어를 발휘하는 경우이거나 머신러닝 시스템을 만드는데 있어 다른 방법을 찾는 경우 실수 평가 측정 지표가 있을 시, 진행속도가 훨씬 빨라지는 것을 보게 될 것입니다. 이 지표는 새롭게 시도한 것들이 더 잘 작동하는지 더 좋지 않은 결과를 주는지 알려주기 때문입니다. 그래서 보통 팀들이 머신러닝 프로젝트를 시작할 때, 실수 평가 측정 지표를 사용하라고 권장합니다. 예시를 봐볼까요. 저번에도 제가 얘기 했듯이, applied 머신 러닝은 실증적 증거에 기반한 절차입니다. 어떠한 특정 발상이 있으면, 그것을 코드화하고, 실험을 하여 결과를 획득하고, 결과를 이용하여 아이디어를 다듬어 나갑니다. 이러한 절차를 계속 루프처럼 반복하여 알고리즘을 계속 개선해 나갑니다. 여러분의 classifier A라는 것이 있다고 해봅시다. 하이퍼 파라미터와 트레이닝 세트에 변화를 줌으로써, 또는 다른 변수에 변화를 주어 새로운 classifier B를 트레이닝 했다고 해봅시다. 한가지 합리적인 성능 평가 방법은 정밀도와 재현율을 보는 것입니다. 정확히 정밀도와 재현율이 어떻게 되어 있는지 보는 것은 이번 예시에서 사실 그리 중요하지 않습니다 간략히 정밀도의 정의는, 고양이를 인식하는 classifier의 예를 들자면, 몇 퍼센트가 고양이인지? 의 정확도를 나타내는 것을 이야기합니다. 만약 classifier A가 95퍼센트의 정확도를 보이면, classifier A는 고양이가 맞는다고 올바르게 판별하는 확률이 95 퍼센트라는 뜻입니다. 재현율은 고양이 이미지에서 실제로 얼마나 classifier를 통해 올바르게 인식이 되었는가? 를 뜻합니다. 실제 몇 퍼센트의 고양이가 올바르게 분류가 되었는가? 그렇다면 만약 classifier A가 90 퍼센트 재현율 이라고 하면 모든 dev set의 이미지에서 실제로 고양이인 이미지가 90 퍼센트 정확도로 인식했다는 뜻입니다. 그러니 정밀도와 재현율의 정의에 너무 목 메일 필요 없습니다. 정밀도와 재현율의 절충이 있다고 보는데요 둘 다 신경 쓸 수 밖에 없죠. 만약 classifier가 어떤 것이 고양이라고 하면 그럴 확률이 매우 높습니다. 그렇지만 모든 것이 고양이의 이미지라고 하면 classifier가 이 이미지들의 대부분을 올바르게 고양이라고 인식하길 바랄 것입니다. 그렇기 때문에 classifier를 정밀도와 재현율의 기준에서 평가를 하는 것이 합리적인 것이라고 할 수 있습니다. 평가 측정 지표로 정밀도와 재현율을 사용하는 문제점은 여기에서 그렇듯이 classifier A가 재현율에서 더 잘 발휘하고, classifier B는 정밀도에 더 강한데요, 그렇기 때문에 어떤 classifier가 더 좋은지 알기 어려울 수 있습니다. 여러가지 아이디어와, 여러 가지 하이퍼 파라미터를 시도하면서 두 가지의 classifier만 시도할 것이 아니라 약 12가지의 classifier를 테스트해봐서 가장 좋은 것을 고르는 것이 좋습니다. 그 이후로는 계속 반복할 수 있기 때문이죠. 또한, 두 가지의 평가 지표로는 두 가지 중 한가지를 선택하거나, 열 가지 중 한가지를 선택하는 것이 어렵습니다. 그래서 classifier를 고르는 과정에서 제가 추천 드리는 것은 두 가지의 숫자, 정밀도, 재현율을 사용하기보다는 정밀도와 재현율을 결합시킨 새로운 평가 지표를 찾는 것입니다. 머신러닝 세계에서는, 정밀도와 재현율을 결합시키는데 있어 F1 score라는 것을 쓰는 게 정석입니다. F1 score의 상세내용은 그리 중요하지 않습니다, 그렇지만 대략적으로 P라는 정밀도 값과 R이라는 재현율값의 평균수치라고 생각하면 됩니다. 공식적으로는, F1 score가 식으로 정도 되는데요. 공식은 2/ 1/P + 1/R입니다. 수학에서는 이 공식을 Harmonic mean of Precision P and recall R이라고 합니다. 비공식적으로는, 정밀도와 재현율의 평균값을 구하는 것이라고 생각하시면 편합니다. 산술적인 평균값을 갖기보다는, 조화 평균값을 공식과 같이 갖게 됩니다. 정밀도와 재현율의 균형에 있어서 장점이 있는데요. 이번 예시를 통해, classifier A가 더 나은 F1 score를 갖는 사실을 바로 볼 수 있습니다. F1 score가 정밀도와 재현율을 결합하는데 합리적인 방법이라고 가정하면, classifier A가 classifier B 보다 낫겠죠. 여러 머신러닝 팀을 보면서 느낀 것은 많은 팀들이 잘 정의된 dev set를 갖추었는데, 즉, 정밀도와 재현율을 잘 측정하도록 구축한 경우를 말하는데요, single number 평가 지표와 함께 사용하는 것을 보았습니다. 가끔씩 single row number라고 표현하기도 합니다. 평가 지표는 classifier A 또는 classifier B가 더 좋은지 빠르게 판단할 수 있게 해주고, 결과적으로 dev set와 single number 평가 지표를 구축하여 반복하는데 속도를 높일 수 있습니다. 반복적인 절차를 빠르게 처리할 수 있게 해줌으로써 머신러닝 알고리즘을 개선하는데 빠른 처리를 도와줍니다. 다른 예제를 보시죠. 고양이를 좋아하는 사람들을 위하여 4대륙에 고양이 어플을 만든다고 해봅시다. 미국, 중국, 인도, 그리고 나머지 국가로 말이죠. 그리고 2가지 classifier가 다른 종류의 에러를 갖게 된다고 해봅시다. 대륙 별로 각각 다른 데이터에서 오류가 발생했다고 해봅시다. 알고리즘 A는 미국에서 제출한 사진으로부터 3퍼센트 에러를 갖고 나머지도 이런 식으로 된다고 해봅시다. 그럼 기록을 유지하는 것이 합리적일 수 있는데요, 이러한 다른 시장 또는 다른 대륙에서 classifier가 얼마나 잘 발휘하는지 말이죠. 하지만 4가지 숫자 기록을 유지하면서 알고리즘 A가 나은지, 알고리즘 B가 더 나은지 결정하기는 쉽지 않습니다. 그리고 여러 가지 classifier로 테스팅을 하게 되면, 이러한 많은 숫자들을 보면서 한가지를 고르는 것은 너무 어렵습니다. 그래서 제가 이번 예시를 통해 추천 드리는 것은, 대륙 별 성과 기록을 유지하는 것 외에, 평균을 계산하는 것입니다. 평균 성과가 합리적인 single real number 평가 지표라고 하면 평균값을 산출함으로써, 알고리즘 C가 가장 낮은 평균 에러 값을 가지고 있다는 것을 빨리
확인할 수 있습니다. 그럼 바로 이 것을 선택할 수 있겠죠. 지속적으로 반복할 알고리즘을 잘 골라야 합니다. 머신러닝의 절차는 아이디어가 있고, 그 아이디어를 도입하여 시도하고, 그 아이디어가 도움이 되었는지 확인하는 절차를 따릅니다. 이 비디오에서 보여드렸다시피, single number 평가 지표를 이용함으로써, 전체적인 효율성 높일 수 있고, 프로젝트 팀이 효율적인 의사결정을 내릴 수 있도록 도와줍니다. 효과적인 평가 지표를 구축하는 방법에 대한 토론이 완전히 끝난 것은 아닙니다. 다음 비디오에서는 최적화의 만족스러운 매트릭스를 설정하는 방법에 대해서 공유하도록 하겠습니다. 자 그럼 다음 비디오를 보도록 하겠습니다.