无论你是选择超参调优 或是选择不同的机器学习算法 还是在构建机器学习系统时 尝试不同的配置项 你都会发现 如果你有单一的量化评估指标 可以让你知道新的方法比上一次是更好还是更糟 那么整个进程会加快很多 所以当团队在启动一个机器学习项目时 我常常建议 设置一个单一的量化评估指标 我们来看个例子让我们来看一个例子 我之前有说过应用机器学习 有一个非常经典的流程 我们通常是先有一个想法 用代码实现它
再实验验证它的效果 然后通过验证的结果来改进之前的想法 不断的在这个循环中改进你的算法 让我们举个例子 你构建了某个分类器A 通过调整超参和训练集或者其他什么方法 你训练得到了一个新的分类器B 这时一个合理的评估你分类器好坏的方法 就是去看它的精确率和召回率 不用太关心究竟什么是精准率和召回率 这个例子 简单地说 对于你那个分类器 精准率就是 它识别出是猫的集合中有多少百分比真正是猫 因此分类器A有95%的精准率 就意味着 A如果把一张图片分类为猫 那么95%可能它就是猫 而召回率就是 对于所有是猫的图片 你的分类器有多少百分比可以正确的识别出来 即多少比率的真猫被正确的识别出了 因此如果分类器A的召回率是90% 意味着 你验证集中真猫的图片 分类器A正确分出了90% 不用太在意精准率和召回率的定义 事实证明你常常要在这两者之间做权衡 需要同时关注这两个指标 你要的是 当分类器说它是猫时 它有很高的可能性是猫 而对于所有真正是猫的图片 你同样希望分类器可以把绝大多数都识别出来 所以使用精准率和召回率来评估分类器 看起来应该是合理的 问题就在于把他们作为评估指标时 如果分类器A的召回率比较好 像这里 而分类器B的精准率更好些
你就不能确定究竟哪个分类器更好了 并且如果你尝试许多不同的算法 就会有许多超参 你就不止从两个分类器中选择 可能需要从十几个中快速地选出最好的那个 以便你可以在这个基础上继续迭代 当有两个评估指标时 快速找出最好的那个会很困难 因此我的建议是与其使用两个数字 精准率和召回率来选择分类器 不如找一个新的评估指标 它兼顾了精准率和召回率 在机器学习领域 标准的方法是 使用 F1分数 F1分数的细节并不重要 简单的说 你可以把它理解为精准率P和召回率R的一种平均值 F1分数的计算公式是 2 / （1/P + 1/R） 在数学里 这个方程被称为 精准率P和召回率R的调和平均数 但通俗一些的说 你还是可以把这理解为精准率和召回率的某种平均 只是我们不使用算术平均数 而是使用像上面公式那样的调和平均数 它在权衡精准率和召回率时是很好用的 像这个例子 你可以立刻发现分类器A的F1分数更高 如果F1分数是一个整合精准率和召回率的合理方法 你就可以很快的在这里选择分类器A 我发现 许多机器学习的团队都有一个很好的验证集 来评估精准率和召回率 和一个单一量化评估指标 有时候我们会称之为single row number 这个评估指标可以帮你快速判断哪个分类器更好 所以一个好的验证集和单一量化评估指标 可以提高迭代的效率 它加速整个流程 
让你的机器学习算法不断改进 从这里开始 增加θ 似乎这也是我希望得到的 也就是 假如你做了一个关于猫的应用
给四个主要地区的猫的爱好者 美国 中国 印度和其他 你的两个分类器在这四个地区的数据上 有不同的错误率 算法A在美国用户提交的图片上达到了3%的错误率 等等 可能追踪每个不同地区上你分类器的效果 是有意义的 但是同时考量4个数字 让你快速的确定 分类器A好还是B好会很困难 并且如果你同时测试许多不同的分类器 那将会更加困难 因此在这个例子中我建议 除了追踪 4个不同地区的表现 应该同时关注它们的平均值 平均表现是一个合理的单一量化评估指标 通过计算平均值 你可以很快的看出似乎算法C的错误率最低 然后你就可以继续对于它做改进了 你必须选择一个算法并且不断的对它进行迭代 在进行机器学习类工作的时候 常常先有一个想法 然后实现它 然后你想要知道你的想法是否有效 所以在这节课中你学到找一个单一量化评估指标 可以很好的提高你的效率 让你的团队可以更快速的做决策 到目前为止 我们还没有讨论完如何效率的设置评估指标 这里这个 我将和你分享如何设置优化和满意度矩阵 让我们来看下一个视频