1
00:00:00,252 --> 00:00:03,417
无论你是选择超参调优 或是选择不同的机器学习算法

2
00:00:03,417 --> 00:00:06,047
还是在构建机器学习系统时

3
00:00:06,047 --> 00:00:07,764
尝试不同的配置项

4
00:00:07,764 --> 00:00:12,016
你都会发现 如果你有单一的量化评估指标

5
00:00:12,016 --> 00:00:16,064
可以让你知道新的方法比上一次是更好还是更糟

6
00:00:16,064 --> 00:00:20,260
那么整个进程会加快很多

7
00:00:20,260 --> 00:00:24,710
所以当团队在启动一个机器学习项目时 我常常建议

8
00:00:24,710 --> 00:00:29,570
设置一个单一的量化评估指标

9
00:00:29,570 --> 00:00:30,600
我们来看个例子让我们来看一个例子

10
00:00:32,400 --> 00:00:35,244
我之前有说过应用机器学习

11
00:00:35,244 --> 00:00:36,165
有一个非常经典的流程

12
00:00:36,165 --> 00:00:40,360
我们通常是先有一个想法 用代码实现它
再实验验证它的效果

13
00:00:40,360 --> 00:00:44,100
然后通过验证的结果来改进之前的想法

14
00:00:44,100 --> 00:00:48,590
不断的在这个循环中改进你的算法

15
00:00:48,590 --> 00:00:54,124
让我们举个例子 你构建了某个分类器A

16
00:00:54,124 --> 00:00:58,036
通过调整超参和训练集或者其他什么方法

17
00:00:58,036 --> 00:01:02,032
你训练得到了一个新的分类器B

18
00:01:02,032 --> 00:01:06,866
这时一个合理的评估你分类器好坏的方法

19
00:01:06,866 --> 00:01:08,680
就是去看它的精确率和召回率

20
00:01:08,680 --> 00:01:12,804
不用太关心究竟什么是精准率和召回率

21
00:01:12,804 --> 00:01:13,650
这个例子

22
00:01:13,650 --> 00:01:16,594
简单地说 对于你那个分类器

23
00:01:16,594 --> 00:01:20,207
精准率就是

24
00:01:23,068 --> 00:01:26,741
它识别出是猫的集合中有多少百分比真正是猫

25
00:01:32,341 --> 00:01:37,045
因此分类器A有95%的精准率 就意味着

26
00:01:37,045 --> 00:01:41,830
A如果把一张图片分类为猫 那么95%可能它就是猫

27
00:01:41,830 --> 00:01:45,878
而召回率就是 对于所有是猫的图片

28
00:01:45,878 --> 00:01:50,731
你的分类器有多少百分比可以正确的识别出来

29
00:01:50,731 --> 00:01:57,110
即多少比率的真猫被正确的识别出了

30
00:02:04,331 --> 00:02:08,986
因此如果分类器A的召回率是90% 意味着

31
00:02:08,986 --> 00:02:11,010
你验证集中真猫的图片

32
00:02:11,010 --> 00:02:13,987
分类器A正确分出了90%

33
00:02:13,987 --> 00:02:19,049
不用太在意精准率和召回率的定义

34
00:02:19,049 --> 00:02:23,933
事实证明你常常要在这两者之间做权衡

35
00:02:23,933 --> 00:02:26,845
需要同时关注这两个指标

36
00:02:26,845 --> 00:02:29,455
你要的是 当分类器说它是猫时

37
00:02:29,455 --> 00:02:31,765
它有很高的可能性是猫

38
00:02:31,765 --> 00:02:33,475
而对于所有真正是猫的图片

39
00:02:33,475 --> 00:02:37,905
你同样希望分类器可以把绝大多数都识别出来

40
00:02:37,905 --> 00:02:40,865
所以使用精准率和召回率来评估分类器

41
00:02:40,865 --> 00:02:44,685
看起来应该是合理的

42
00:02:44,685 --> 00:02:49,728
问题就在于把他们作为评估指标时

43
00:02:49,728 --> 00:02:54,926
如果分类器A的召回率比较好 像这里

44
00:02:54,926 --> 00:02:59,840
而分类器B的精准率更好些
你就不能确定究竟哪个分类器更好了

45
00:03:03,481 --> 00:03:06,976
并且如果你尝试许多不同的算法 就会有许多超参

46
00:03:06,976 --> 00:03:11,076
你就不止从两个分类器中选择

47
00:03:11,076 --> 00:03:14,932
可能需要从十几个中快速地选出最好的那个

48
00:03:14,932 --> 00:03:17,010
以便你可以在这个基础上继续迭代

49
00:03:19,850 --> 00:03:23,570
当有两个评估指标时

50
00:03:23,570 --> 00:03:27,380
快速找出最好的那个会很困难

51
00:03:29,170 --> 00:03:33,220
因此我的建议是与其使用两个数字

52
00:03:33,220 --> 00:03:35,870
精准率和召回率来选择分类器

53
00:03:35,870 --> 00:03:40,440
不如找一个新的评估指标 它兼顾了精准率和召回率

54
00:03:41,740 --> 00:03:45,205
在机器学习领域 标准的方法是

55
00:03:45,205 --> 00:03:47,028
使用 F1分数

56
00:03:47,028 --> 00:03:52,777
F1分数的细节并不重要 简单的说

57
00:03:52,777 --> 00:03:58,541
你可以把它理解为精准率P和召回率R的一种平均值

58
00:03:58,541 --> 00:04:04,574
F1分数的计算公式是

59
00:04:04,574 --> 00:04:07,670
2 / （1/P + 1/R）

60
00:04:07,670 --> 00:04:12,240
在数学里 这个方程被称为

61
00:04:12,240 --> 00:04:16,860
精准率P和召回率R的调和平均数

62
00:04:16,860 --> 00:04:17,850
但通俗一些的说

63
00:04:17,850 --> 00:04:21,721
你还是可以把这理解为精准率和召回率的某种平均

64
00:04:22,840 --> 00:04:25,190
只是我们不使用算术平均数

65
00:04:25,190 --> 00:04:28,800
而是使用像上面公式那样的调和平均数

66
00:04:28,800 --> 00:04:33,410
它在权衡精准率和召回率时是很好用的

67
00:04:33,410 --> 00:04:34,953
像这个例子

68
00:04:34,953 --> 00:04:39,853
你可以立刻发现分类器A的F1分数更高

69
00:04:39,853 --> 00:04:43,825
如果F1分数是一个整合精准率和召回率的合理方法

70
00:04:43,825 --> 00:04:47,000
你就可以很快的在这里选择分类器A

71
00:04:48,100 --> 00:04:48,880
我发现

72
00:04:48,880 --> 00:04:52,401
许多机器学习的团队都有一个很好的验证集

73
00:04:52,401 --> 00:04:57,598
来评估精准率和召回率 和一个单一量化评估指标

74
00:04:57,598 --> 00:05:03,430
有时候我们会称之为single row number

75
00:05:04,580 --> 00:05:09,147
这个评估指标可以帮你快速判断哪个分类器更好

76
00:05:09,147 --> 00:05:13,971
所以一个好的验证集和单一量化评估指标

77
00:05:13,971 --> 00:05:18,301
可以提高迭代的效率

78
00:05:21,551 --> 00:05:26,980
它加速整个流程 
让你的机器学习算法不断改进

79
00:05:26,980 --> 00:05:28,010
从这里开始 增加θ 似乎这也是我希望得到的 也就是

80
00:05:29,130 --> 00:05:35,390
假如你做了一个关于猫的应用
给四个主要地区的猫的爱好者

81
00:05:35,390 --> 00:05:40,490
美国 中国 印度和其他

82
00:05:40,490 --> 00:05:43,940
你的两个分类器在这四个地区的数据上

83
00:05:45,370 --> 00:05:48,400
有不同的错误率

84
00:05:48,400 --> 00:05:54,280
算法A在美国用户提交的图片上达到了3%的错误率 等等

85
00:05:56,100 --> 00:05:59,140
可能追踪每个不同地区上你分类器的效果

86
00:05:59,140 --> 00:06:03,260
是有意义的

87
00:06:03,260 --> 00:06:06,770
但是同时考量4个数字 让你快速的确定

88
00:06:06,770 --> 00:06:10,890
分类器A好还是B好会很困难

89
00:06:10,890 --> 00:06:13,370
并且如果你同时测试许多不同的分类器

90
00:06:13,370 --> 00:06:17,590
那将会更加困难

91
00:06:17,590 --> 00:06:22,390
因此在这个例子中我建议 除了追踪

92
00:06:22,390 --> 00:06:26,450
4个不同地区的表现 应该同时关注它们的平均值

93
00:06:26,450 --> 00:06:30,874
平均表现是一个合理的单一量化评估指标

94
00:06:30,874 --> 00:06:33,799
通过计算平均值

95
00:06:33,799 --> 00:06:38,530
你可以很快的看出似乎算法C的错误率最低

96
00:06:38,530 --> 00:06:40,555
然后你就可以继续对于它做改进了

97
00:06:40,555 --> 00:06:44,490
你必须选择一个算法并且不断的对它进行迭代

98
00:06:44,490 --> 00:06:47,573
在进行机器学习类工作的时候 常常先有一个想法

99
00:06:47,573 --> 00:06:51,970
然后实现它 然后你想要知道你的想法是否有效

100
00:06:51,970 --> 00:06:56,760
所以在这节课中你学到找一个单一量化评估指标

101
00:06:56,760 --> 00:06:58,980
可以很好的提高你的效率

102
00:06:58,980 --> 00:07:02,340
让你的团队可以更快速的做决策

103
00:07:02,340 --> 00:07:03,240
到目前为止

104
00:07:03,240 --> 00:07:07,510
我们还没有讨论完如何效率的设置评估指标

105
00:07:07,510 --> 00:07:08,430
这里这个

106
00:07:08,430 --> 00:07:13,880
我将和你分享如何设置优化和满意度矩阵

107
00:07:13,880 --> 00:07:15,480
让我们来看下一个视频