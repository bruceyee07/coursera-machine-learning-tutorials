1
00:00:00,360 --> 00:00:01,805
你们已经学过正交化

2
00:00:01,805 --> 00:00:06,561
如何设置开发集和测试集<br />用人类水平代表贝叶斯误差

3
00:00:06,561 --> 00:00:11,110
以及如何估计可避免偏差和方差

4
00:00:11,110 --> 00:00:14,121
让我们把这些结合成一套准则

5
00:00:14,121 --> 00:00:17,473
用于改进你的学习算法

6
00:00:17,473 --> 00:00:22,149
我认为一个有监督学习算法能发挥作用

7
00:00:22,149 --> 00:00:24,656
基本上意味着<br />希望或者预设你可以做到两件事

8
00:00:24,656 --> 00:00:30,015
第一是你可以很好地拟合训练集

9
00:00:30,015 --> 00:00:37,670
你可以大致认为你能得到较低的可避免偏差

10
00:00:38,830 --> 00:00:42,992
需要做好的第二点是

11
00:00:42,992 --> 00:00:47,369
训练集的结果可以很好地<br />推广到开发集或者测试集

12
00:00:47,369 --> 00:00:50,488
这就是说 方差不太大

13
00:00:50,488 --> 00:00:53,717
从正交化角度来说

14
00:00:53,717 --> 00:00:58,779
你看到的是 存在一组旋钮<br />可以修正可避免偏差的问题

15
00:00:58,779 --> 00:01:03,245
比如训练更大的神经网络<br />或者训练更长时间

16
00:01:03,245 --> 00:01:08,635
并且有另一组独立的方法<br />可以用来处理方差的问题

17
00:01:08,635 --> 00:01:12,369
比如正则化或者获取更多训练数据

18
00:01:12,369 --> 00:01:16,543
总结一下之前几个视频里展示的方法

19
00:01:16,543 --> 00:01:21,190
如果想要改善你的机器学习系统

20
00:01:21,190 --> 00:01:26,234
我建议看看你的训练误差<br />和贝叶斯误差估计值间的差距

21
00:01:26,234 --> 00:01:31,163
这能让你估计可避免偏差

22
00:01:31,163 --> 00:01:35,297
换句话说 就是你需要试着<br />在训练集优化到什么程度

23
00:01:35,297 --> 00:01:39,366
然后再看看你的开发集误差

24
00:01:39,366 --> 00:01:41,382
和训练集误差间的差距

25
00:01:41,382 --> 00:01:43,871
来估计你的方差问题有多大

26
00:01:43,871 --> 00:01:44,711
换句话说

27
00:01:44,711 --> 00:01:48,671
你需要付出多大的努力

28
00:01:48,671 --> 00:01:52,392
来把你训练集的结果推广到开发集<br />那些没有直接用于训练的数据上

29
00:02:04,393 --> 00:02:09,201
无论你想把可避免偏差<br />降低到什么程度

30
00:02:09,201 --> 00:02:13,386
我会尝试用一些策略<br />比如 训练一个更大的模型

31
00:02:13,386 --> 00:02:18,124
这样你的训练集结果会更好<br />又或者 训练更长的时间

32
00:02:18,124 --> 00:02:21,196
用更好的优化算法 比如

33
00:02:24,005 --> 00:02:27,433
加入Momentum或者RMS Prop

34
00:02:27,433 --> 00:02:32,060
或者用更好的算法 比如Adam算法

35
00:02:34,874 --> 00:02:39,894
又或者 你可以试试寻找

36
00:02:39,894 --> 00:02:45,220
更好的神经网络架构<br />或者更准确来说 超参数

37
00:02:45,220 --> 00:02:50,187
这可能包括从改变激活函数 到<br />改变层数或者隐含单元数的各个方面

38
00:02:50,187 --> 00:02:55,341
虽然你这么做可能会增大模型的规模

39
00:02:55,341 --> 00:03:00,654
因为要训练其他模型 或者模型架构

40
00:03:00,654 --> 00:03:06,500
比如循环神经网络(RNN)或者卷积神经网络(CNN)<br />我们会在之后的课程里学到

41
00:03:06,500 --> 00:03:09,520
一个新的神经网络架构<br />能否更好地拟合你的训练集

42
00:03:09,520 --> 00:03:12,800
有时候很难提前判断

43
00:03:12,800 --> 00:03:16,570
但有时候用一个更好的架构<br />可以得到好得多的结果

44
00:03:18,500 --> 00:03:20,941
下一步你会发现方差是主要问题

45
00:03:20,941 --> 00:03:26,417
有很多方法你可以尝试 包括

46
00:03:26,417 --> 00:03:30,762
尝试获得更多的数据<br />因为用更多的数据来训练

47
00:03:30,762 --> 00:03:35,437
可以帮助你更好地把结果推广到<br />开发集那些你没有遇到过的数据上

48
00:03:35,437 --> 00:03:37,759
你可以试试正则化

49
00:03:37,759 --> 00:03:43,000
包括L2正则化 随机失活法(dropout)

50
00:03:43,000 --> 00:03:50,501
或者我们之前讲过的数据集扩增

51
00:03:50,501 --> 00:03:55,187
又或者 同样地<br />你还可以试试不同的神经网络架构

52
00:03:55,187 --> 00:03:58,467
或者超参数搜索 看看是否能帮你

53
00:03:58,467 --> 00:04:02,390
找到一个更好的构架<br />更适合这个问题

54
00:04:03,810 --> 00:04:07,430
我想这些偏差或者说可避免偏差<br />以及方差的概念

55
00:04:07,430 --> 00:04:12,150
是很容易学但很难精通的东西

56
00:04:12,150 --> 00:04:16,090
在这周的视频中<br />我们系统地应用了这些概念

57
00:04:16,090 --> 00:04:20,244
你实际上已经比很多机器学习团队

58
00:04:20,244 --> 00:04:24,734
更高效 更系统 更有策略了

59
00:04:24,734 --> 00:04:28,567
从系统地改善他们<br />机器学习系统的角度来说

60
00:04:28,567 --> 00:04:32,982
这周的作业会让你练习实践

61
00:04:32,982 --> 00:04:36,832
你对这些概念的理解

62
00:04:36,832 --> 00:04:38,950
祝你们好运

63
00:04:38,950 --> 00:04:42,463
并且期待在下周的视频中<br />继续见到你们

64
00:06:19,757 --> 00:06:20,701
方差