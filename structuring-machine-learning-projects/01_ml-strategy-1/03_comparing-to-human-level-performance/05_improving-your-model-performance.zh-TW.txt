你聽過了正交法 (orthogonalization)、 設置開發/測試集的方法、
用人類水準來估計 Bayes error、 以及如何估計可避免的偏差和差異。 讓我們整合成一套準則 以改善你的演算法學習的成效 那麼，我覺得要讓監督式學習演算法有效 意味著你要能作到兩件事 第一，你很能配適訓練集 (fit the training set) 你可以看成，能夠讓「可避免的偏差」很低 第二，你在訓練集上能做好的 也能推廣到開發集或測試集上做得好 某種角度代表「變異」不差 以正交法的精神而言 這代表有一些旋鈕可以解決「可避免偏差」 例如訓練更大的網路、訓練更久 而有另外一些東西能解決「變異」 例如正則化、蒐集更多訓練資料 統整過去的影片提到的流程 如果你想改善你的機器學習系統 我會建議你觀察訓練錯誤和 Bayes error 估計值之間的差距，讓你有概念可避免的偏差多大 換句話說，你覺得要在訓練資料上進步多少 然後，觀察你的開發錯誤和訓練錯誤 之間的差距， 來估計變異的問題有多嚴重 換句話說 你要花多少力氣在把訓練資料的成效 推廣到開發資料
 — 尤其你並不會直接拿開發資料做訓練 所以無論你想減少「可避免的偏差」多少 我會試著訓練比較大的模型 所以你可以在訓練集上做更好。或者訓練更久、 用更好的最佳化演算法 例如動量法或 RMSprop 或者更好的 Adam 抑或，你可以試著找尋更好的神經網路架構、 或是更好的超參數，這包含很多事 不同的啟動函數、不同隱藏單元的層數 — 雖然這和增大網路一樣 還有嘗試不同模型、架構，例如 Recurrent Neural Network, Convolutional Neural Network
我們會在之後的課程看到 一個新的神經網路架構，能否更適合 (fit) 你的訓練資料，這其實很難事先知道 不過有時用了更好的架構可以得到更好的結果 接下來，你可能發現「變異」是個問題 有很多技巧，以下列出幾個你可以試試 你可以試著蒐集更多資料，因為訓練更多資料 能幫助你更能推廣 (generalize) 至沒看過的開發資料 你可以試正則化 (regularization) 包括像是 L2正則化、dropout 或是以前提到的資料擴增 (data augmentation) 或者一樣，試不同的神經網路架構、 搜尋好的超參數，看看能否為你的問題 找到一個適合的架構 我想這種「可避免的偏差」和「變異」的觀念 很容易學、但很難駕馭 如果你能有系統地套用這禮拜談到的概念 你就能更有效率、更有系統、更有策略 比很多團隊還來得強，能有系統地 改善機器學習系統的成效 那麼，這禮拜的作業，能讓你練習 更加實踐，讓你更了解這些概念 祝你好運 也期望在下禮拜的影片相見 .