1
00:00:02,400 --> 00:00:07,760
我们之前讨论过 如何使学习算法在训练集上表现良好

2
00:00:07,760 --> 00:00:10,085
但是有时候你并不希望它好过头

3
00:00:10,085 --> 00:00:12,765
如果你知道了人类的水准是什么程度的话

4
00:00:12,765 --> 00:00:15,070
那你就能够知道 对于训练集来说

5
00:00:15,070 --> 00:00:18,250
怎样的表现算是好但没有过头

6
00:00:18,250 --> 00:00:19,392
我来展示一下我的意思

7
00:00:19,392 --> 00:00:24,320
猫分类问题我们讨论过很多次了 就是给定一幅图

8
00:00:24,320 --> 00:00:32,195
我们假定人类的分类准确率是近乎完美 错误率低到只有1%

9
00:00:32,195 --> 00:00:34,475
在这个情况下 如果你的学习算法

10
00:00:34,475 --> 00:00:38,915
在训练集上错误率为8% 开发集上为10%的话

11
00:00:38,915 --> 00:00:44,500
那你可能希望它在训练集上的表现能再好一些

12
00:00:44,500 --> 00:00:49,510
也就是说 因为在你的算法的表现和人类的表现之间

13
00:00:49,510 --> 00:00:52,010
存在着巨大的差距

14
00:00:52,010 --> 00:00:55,625
这就意味着你的算法甚至都没法良好拟合训练集

15
00:00:55,625 --> 00:00:59,210
所以 在消除偏差和方差的工具两者之中

16
00:00:59,210 --> 00:01:03,835
我推荐在这种情况下重点消除偏差

17
00:01:03,835 --> 00:01:09,410
可以尝试训练更大的神经网络 或者训练更长时间

18
00:01:09,410 --> 00:01:12,003
从而提升模型在训练集上的性能

19
00:01:12,003 --> 00:01:15,050
然后 我们固定训练集和开发集上的误差

20
00:01:15,050 --> 00:01:19,340
再想象人类的水准并不是1%的错误率了

21
00:01:19,340 --> 00:01:22,120
所以这个例子结束了 但是

22
00:01:22,120 --> 00:01:25,170
假设在一个不同的程序 或是不同的数据集上

23
00:01:25,170 --> 00:01:30,180
人类的错误率其实有7.5%

24
00:01:30,180 --> 00:01:33,890
这或许是因为数据集中的图片如此模糊

25
00:01:33,890 --> 00:01:37,917
以至于人类都分不出图中是否有猫

26
00:01:37,917 --> 00:01:41,090
这个情况可能有点为举例而举例的意思

27
00:01:41,090 --> 00:01:44,525
因为人类实际上很擅长通过看来判断图中有没有猫

28
00:01:44,525 --> 00:01:46,490
但就这个例子来说

29
00:01:46,490 --> 00:01:48,270
如果你的数据集中的图片

30
00:01:48,270 --> 00:01:54,680
实在是太模糊了 以至于即使是人类的错误率也高达7.5%

31
00:01:54,680 --> 00:01:56,720
在这个情况下

32
00:01:56,720 --> 00:02:00,305
虽然训练集和开发集中的错误率与前例一样高

33
00:02:00,305 --> 00:02:04,016
但是你可以认为它在训练集上的表现已经比较好了

34
00:02:04,016 --> 00:02:07,980
它只比人类的水准稍微差一点点

35
00:02:07,980 --> 00:02:10,010
在这第二个例子中

36
00:02:10,010 --> 00:02:14,295
你可能希望重点减小方差这一项

37
00:02:14,295 --> 00:02:19,390
就是你的学习算法中的方差

38
00:02:19,390 --> 00:02:21,650
例如 可以尝试使用正则化的办法

39
00:02:21,650 --> 00:02:25,600
来使开发集上的误差向训练集上的误差靠近

40
00:02:25,600 --> 00:02:29,790
其实在之前的课对偏差和方差的讨论中

41
00:02:29,790 --> 00:02:36,900
我们曾有一个主要的假设是 贝叶斯误差接近零

42
00:02:36,900 --> 00:02:39,280
在刚才的两个例子中

43
00:02:39,280 --> 00:02:42,150
也就是猫分类的问题中

44
00:02:42,150 --> 00:02:47,510
我们可以将人类水准的错误率

45
00:02:47,510 --> 00:02:56,030
看成是贝叶斯最优误差的代理变量 或是估计值

46
00:02:56,030 --> 00:02:58,660
在计算机视觉任务中

47
00:02:58,660 --> 00:03:02,450
这个代理变量的选取是合理的 因为人类擅长视觉的任务

48
00:03:02,450 --> 00:03:08,750
所以人类能达到的水准也许离贝叶斯误差相差不大

49
00:03:08,750 --> 00:03:11,930
从定义上说 人类误差必然是

50
00:03:11,930 --> 00:03:14,840
要大于贝叶斯误差的 因为没有任何事物

51
00:03:14,840 --> 00:03:19,665
能小于贝叶斯误差 但是人类可以做到与贝叶斯误差相比差距不大

52
00:03:19,665 --> 00:03:25,145
所以这里有个惊奇的发现是 根据每个任务中人类误差的不同

53
00:03:25,145 --> 00:03:31,214
或说 根据贝叶斯误差的估计值或假设值的不同

54
00:03:31,214 --> 00:03:35,273
总之 这是我们认为可以达到的错误率

55
00:03:35,273 --> 00:03:40,970
虽然两次实验中的训练集误差和开发集误差都相同

56
00:03:40,970 --> 00:03:47,510
但是改进的侧重点的可能会不同 可能是减小偏差或是方差

57
00:03:47,510 --> 00:03:51,710
所以在左边的例子中

58
00:03:51,710 --> 00:03:55,850
8%的训练集错误已经是非常高了

59
00:03:55,850 --> 00:04:01,310
因为只要采取减小偏差的策略 就能把错误降低到1%

60
00:04:01,310 --> 00:04:02,885
而在右边的例子中

61
00:04:02,885 --> 00:04:07,140
如果贝叶斯误差高达7.5%

62
00:04:07,140 --> 00:04:12,265
我们在这里用人类的误差作为代理变量来估算贝叶斯误差

63
00:04:12,265 --> 00:04:13,622
如果你认为贝叶斯误差

64
00:04:13,622 --> 00:04:15,860
确实是接近7.5%的话 那你就会知道

65
00:04:15,860 --> 00:04:20,195
训练集上的误差降低的空间就不会太多

66
00:04:20,195 --> 00:04:24,710
而你也不希望错误率低于7.5%太多 因为

67
00:04:24,710 --> 00:04:29,780
有可能只有过拟合才会达成那种结果

68
00:04:29,780 --> 00:04:32,910
相反 不妨就接受这个数据集上表现一般的事实

69
00:04:32,910 --> 00:04:36,380
把重点放在开发集与训练集的 2% 的差距上

70
00:04:36,380 --> 00:04:38,660
然后针对它尝试一些

71
00:04:38,660 --> 00:04:43,370
消除方差的方法 譬如正则化 或者获取更多训练数据

72
00:04:43,370 --> 00:04:47,463
我想给这个问题取一个名字

73
00:04:47,463 --> 00:04:50,490
这个名字现在还不是广泛使用的术语

74
00:04:50,490 --> 00:04:54,075
但是我觉得它对于思考问题有帮助

75
00:04:54,075 --> 00:04:58,380
所以我把贝叶斯误差 或是对其的近似

76
00:04:58,380 --> 00:05:05,670
其与训练集上的误差 称为 可避免的偏差

77
00:05:05,670 --> 00:05:11,830
所以 你可能想改进模型在训练集上的表现

78
00:05:11,830 --> 00:05:14,020
直到误差小到与贝叶斯误差齐平

79
00:05:14,020 --> 00:05:16,565
但是你并不想小于贝叶斯误差

80
00:05:16,565 --> 00:05:20,740
想小于贝叶斯误差是不可能的 除非过拟合

81
00:05:20,740 --> 00:05:24,879
而这时模型在训练集与开发集上的误差之差

82
00:05:24,879 --> 00:05:29,775
则是对模型中存在的方差问题的度量

83
00:05:29,775 --> 00:05:35,350
而 可回避的误差 这个术语就表现出了

84
00:05:35,350 --> 00:05:38,140
训练时能达成的误差的下限

85
00:05:38,140 --> 00:05:42,975
也就是贝叶斯误差 即7.5%

86
00:05:42,975 --> 00:05:46,885
你大概不希望训练集上的误差比它小

87
00:05:46,885 --> 00:05:50,650
所以 与其说 在训练集上的误差是8%

88
00:05:50,650 --> 00:05:53,427
也就是说在这个例子中测量出的偏差是8%

89
00:05:53,427 --> 00:06:01,520
不如说 可回避的偏差是0.5% 或说 偏差的度量

90
00:06:01,520 --> 00:06:06,220
是0.5% 同时 方差的度量是2%

91
00:06:06,220 --> 00:06:11,378
所以在方差上有2%的进步空间 比偏差的0.5%要大许多

92
00:06:11,378 --> 00:06:14,384
而在左边的例子中

93
00:06:14,384 --> 00:06:20,055
可回避的偏差的度量高达7%

94
00:06:20,055 --> 00:06:24,275
而可回避的方差的度量只有2%

95
00:06:24,275 --> 00:06:25,960
所以在左边的例子中

96
00:06:25,960 --> 00:06:31,789
偏差上的进步空间要大很多

97
00:06:31,789 --> 00:06:33,310
在这个例子里

98
00:06:33,310 --> 00:06:35,845
如果能够理解人类水准的误差

99
00:06:35,845 --> 00:06:38,220
就能对贝叶斯误差进行估计

100
00:06:38,220 --> 00:06:42,420
也就能使得你在不同情况下 对症下药

101
00:06:42,420 --> 00:06:45,970
根据情况 采取削减偏差或削减方差的策略

102
00:06:45,970 --> 00:06:48,820
还有很多微妙之处是你在通过考虑人类水准的误差

103
00:06:48,820 --> 00:06:53,800
来选择偏重的策略时需要注意的

104
00:06:53,800 --> 00:06:55,970
所以在下一个视频中 我们会更深入探讨

105
00:06:55,970 --> 00:06:59,460
理解人类水准的误差的意义