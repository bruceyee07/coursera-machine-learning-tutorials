我们之前讨论过 如何使学习算法在训练集上表现良好 但是有时候你并不希望它好过头 如果你知道了人类的水准是什么程度的话 那你就能够知道 对于训练集来说 怎样的表现算是好但没有过头 我来展示一下我的意思 猫分类问题我们讨论过很多次了 就是给定一幅图 我们假定人类的分类准确率是近乎完美 错误率低到只有1% 在这个情况下 如果你的学习算法 在训练集上错误率为8% 开发集上为10%的话 那你可能希望它在训练集上的表现能再好一些 也就是说 因为在你的算法的表现和人类的表现之间 存在着巨大的差距 这就意味着你的算法甚至都没法良好拟合训练集 所以 在消除偏差和方差的工具两者之中 我推荐在这种情况下重点消除偏差 可以尝试训练更大的神经网络 或者训练更长时间 从而提升模型在训练集上的性能 然后 我们固定训练集和开发集上的误差 再想象人类的水准并不是1%的错误率了 所以这个例子结束了 但是 假设在一个不同的程序 或是不同的数据集上 人类的错误率其实有7.5% 这或许是因为数据集中的图片如此模糊 以至于人类都分不出图中是否有猫 这个情况可能有点为举例而举例的意思 因为人类实际上很擅长通过看来判断图中有没有猫 但就这个例子来说 如果你的数据集中的图片 实在是太模糊了 以至于即使是人类的错误率也高达7.5% 在这个情况下 虽然训练集和开发集中的错误率与前例一样高 但是你可以认为它在训练集上的表现已经比较好了 它只比人类的水准稍微差一点点 在这第二个例子中 你可能希望重点减小方差这一项 就是你的学习算法中的方差 例如 可以尝试使用正则化的办法 来使开发集上的误差向训练集上的误差靠近 其实在之前的课对偏差和方差的讨论中 我们曾有一个主要的假设是 贝叶斯误差接近零 在刚才的两个例子中 也就是猫分类的问题中 我们可以将人类水准的错误率 看成是贝叶斯最优误差的代理变量 或是估计值 在计算机视觉任务中 这个代理变量的选取是合理的 因为人类擅长视觉的任务 所以人类能达到的水准也许离贝叶斯误差相差不大 从定义上说 人类误差必然是 要大于贝叶斯误差的 因为没有任何事物 能小于贝叶斯误差 但是人类可以做到与贝叶斯误差相比差距不大 所以这里有个惊奇的发现是 根据每个任务中人类误差的不同 或说 根据贝叶斯误差的估计值或假设值的不同 总之 这是我们认为可以达到的错误率 虽然两次实验中的训练集误差和开发集误差都相同 但是改进的侧重点的可能会不同 可能是减小偏差或是方差 所以在左边的例子中 8%的训练集错误已经是非常高了 因为只要采取减小偏差的策略 就能把错误降低到1% 而在右边的例子中 如果贝叶斯误差高达7.5% 我们在这里用人类的误差作为代理变量来估算贝叶斯误差 如果你认为贝叶斯误差 确实是接近7.5%的话 那你就会知道 训练集上的误差降低的空间就不会太多 而你也不希望错误率低于7.5%太多 因为 有可能只有过拟合才会达成那种结果 相反 不妨就接受这个数据集上表现一般的事实 把重点放在开发集与训练集的 2% 的差距上 然后针对它尝试一些 消除方差的方法 譬如正则化 或者获取更多训练数据 我想给这个问题取一个名字 这个名字现在还不是广泛使用的术语 但是我觉得它对于思考问题有帮助 所以我把贝叶斯误差 或是对其的近似 其与训练集上的误差 称为 可避免的偏差 所以 你可能想改进模型在训练集上的表现 直到误差小到与贝叶斯误差齐平 但是你并不想小于贝叶斯误差 想小于贝叶斯误差是不可能的 除非过拟合 而这时模型在训练集与开发集上的误差之差 则是对模型中存在的方差问题的度量 而 可回避的误差 这个术语就表现出了 训练时能达成的误差的下限 也就是贝叶斯误差 即7.5% 你大概不希望训练集上的误差比它小 所以 与其说 在训练集上的误差是8% 也就是说在这个例子中测量出的偏差是8% 不如说 可回避的偏差是0.5% 或说 偏差的度量 是0.5% 同时 方差的度量是2% 所以在方差上有2%的进步空间 比偏差的0.5%要大许多 而在左边的例子中 可回避的偏差的度量高达7% 而可回避的方差的度量只有2% 所以在左边的例子中 偏差上的进步空间要大很多 在这个例子里 如果能够理解人类水准的误差 就能对贝叶斯误差进行估计 也就能使得你在不同情况下 对症下药 根据情况 采取削减偏差或削减方差的策略 还有很多微妙之处是你在通过考虑人类水准的误差 来选择偏重的策略时需要注意的 所以在下一个视频中 我们会更深入探讨 理解人类水准的误差的意义