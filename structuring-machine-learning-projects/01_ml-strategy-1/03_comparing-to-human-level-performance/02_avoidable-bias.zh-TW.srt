1
00:00:02,400 --> 00:00:07,760
我們曾說，你想讓你的演算法
在訓練集上學得很好

2
00:00:07,760 --> 00:00:10,085
但有時候又不希望他學太好

3
00:00:10,085 --> 00:00:12,765
而知道人類的表現水準在哪

4
00:00:12,765 --> 00:00:15,070
就能夠告訴你

5
00:00:15,070 --> 00:00:18,250
在訓練集上「好但不要太好」究竟是多好

6
00:00:18,250 --> 00:00:19,392
讓我來解釋一下

7
00:00:19,392 --> 00:00:24,320
我們用了很多貓貓分類的例子：給一張照片

8
00:00:24,320 --> 00:00:32,195
假設人類有近乎完美的準確度，
其錯誤率是 1%

9
00:00:32,195 --> 00:00:34,475
在這種情況下，如果你的學習演算法達到

10
00:00:34,475 --> 00:00:38,915
8% 的訓練錯誤率和 10% 的開發錯誤率

11
00:00:38,915 --> 00:00:44,500
那麼可能，你要在訓練集上學得更好

12
00:00:44,500 --> 00:00:49,510
如果你的演算法在訓練集上的表現

13
00:00:49,510 --> 00:00:52,010
和人類有一段很大的差距

14
00:00:52,010 --> 00:00:55,625
表示你的演算法連訓練集都合不來

15
00:00:55,625 --> 00:00:59,210
我們已學了一些減少偏差或變異的手段

16
00:00:59,210 --> 00:01:03,835
在這情況我會覺得要減少偏差 (bias)

17
00:01:03,835 --> 00:01:09,410
例如用更大的網路訓練、
讓梯度下降跑更久

18
00:01:09,410 --> 00:01:12,003
專心把訓練集做更好

19
00:01:12,003 --> 00:01:15,050
那麼現在，再假設同樣的訓練/開發錯誤率

20
00:01:15,050 --> 00:01:19,340
但假設人類水準並不是 1%

21
00:01:19,340 --> 00:01:22,120
— 把這複製過來，不過假設

22
00:01:22,120 --> 00:01:25,170
在不一樣的應用，
或者不一樣的資料

23
00:01:25,170 --> 00:01:30,180
假設人類水準的錯誤率其實是 7.5%

24
00:01:30,180 --> 00:01:33,890
或許你資料裡面的圖片太糊了，就算是人

25
00:01:33,890 --> 00:01:37,917
也無法知道裡面有沒有貓

26
00:01:37,917 --> 00:01:41,090
— 這例子有點故意，因為人類

27
00:01:41,090 --> 00:01:44,525
其實非常擅長於看圖認貓

28
00:01:44,525 --> 00:01:46,490
不過為了舉例

29
00:01:46,490 --> 00:01:48,270
我們假設你的圖片集

30
00:01:48,270 --> 00:01:54,680
很模糊、解析度很低，就算是人也會錯 7.5%

31
00:01:54,680 --> 00:01:56,720
在這種情況下，

32
00:01:56,720 --> 00:02:00,305
就算你的訓練錯誤、開發錯誤和前個例子一樣

33
00:02:00,305 --> 00:02:04,016
然而你在訓練資料上已經做得很好了

34
00:02:04,016 --> 00:02:07,980
和人類表現只差了一點點

35
00:02:07,980 --> 00:02:10,010
在這第二個例子

36
00:02:10,010 --> 00:02:14,295
你可能會想要減少這部份

37
00:02:14,295 --> 00:02:19,390
減少演算法的變異 (variance)

38
00:02:19,390 --> 00:02:21,650
所以你可以試試正則化 (regularization)

39
00:02:21,650 --> 00:02:25,600
試著讓開發錯誤和訓練錯誤更接近。

40
00:02:25,600 --> 00:02:29,790
那麼，在早先講到偏差/變異的課程

41
00:02:29,790 --> 00:02:36,900
我們是假設 Bayes error 近乎為零

42
00:02:36,900 --> 00:02:39,280
那麼現在這邊的情況

43
00:02:39,280 --> 00:02:42,150
在我們貓貓分類的例子

44
00:02:42,150 --> 00:02:47,510
把人類水準的錯誤想成

45
00:02:47,510 --> 00:02:56,030
一種 Bayes optimal error 的近似、估計

46
00:02:56,030 --> 00:02:58,660
— 在電腦視覺的任務

47
00:02:58,660 --> 00:03:02,450
這樣的估計還滿合理的，
因為人類真的很擅長

48
00:03:02,450 --> 00:03:08,750
電腦視覺，所以人類能達到的和 Bayes error 相去不遠

49
00:03:08,750 --> 00:03:11,930
根據定義，人類水準會比 Bayes error 差

50
00:03:11,930 --> 00:03:14,840
因為沒有東西能比 Bayes error 好

51
00:03:14,840 --> 00:03:19,665
不過人類大概也不會差 Bayes error 太多

52
00:03:19,665 --> 00:03:25,145
那麼讓人意外的是，根據不同的人類水準

53
00:03:25,145 --> 00:03:31,214
或者我們認為大約是 Bayes error 的估計

54
00:03:31,214 --> 00:03:35,273
總之取決於我們認為能達成的水準

55
00:03:35,273 --> 00:03:40,970
就算在這兩個例子，有同樣的訓練/開發錯誤

56
00:03:40,970 --> 00:03:47,510
我們會有不同決定：
把心力放在減少偏差、或減少變異

57
00:03:47,510 --> 00:03:51,710
在左邊的情況是

58
00:03:51,710 --> 00:03:55,850
8% 訓練錯誤「很高」 
— 如果你覺得可以降到 1%

59
00:03:55,850 --> 00:04:01,310
而減少偏差 (bias) 能幫你作到這點

60
00:04:01,310 --> 00:04:02,885
然而在右邊的例子

61
00:04:02,885 --> 00:04:07,140
如果你認為 Bayes error 為 7.5%

62
00:04:07,140 --> 00:04:12,265
— 這裡我們用人類水平來當作 Bayes error 的近似值

63
00:04:12,265 --> 00:04:13,622
如果你認為 Bayes error

64
00:04:13,622 --> 00:04:15,860
很接近 7.5%，那你知道的

65
00:04:15,860 --> 00:04:20,195
沒有太多空間能再降低訓練錯誤

66
00:04:20,195 --> 00:04:24,710
你不希望訓練到比 7.5% 還要好，因為要這樣

67
00:04:24,710 --> 00:04:29,780
你只能藉由 overfit (過適) 訓練集來達到

68
00:04:29,780 --> 00:04:32,910
然而，你還有很多空間

69
00:04:32,910 --> 00:04:36,380
對這裡的 2% 的差異求進步

70
00:04:36,380 --> 00:04:38,660
試著減低這差距

71
00:04:38,660 --> 00:04:43,370
利用減少變異 (variance) 的手段：像是正則化、收集更多訓練資料

72
00:04:43,370 --> 00:04:47,463
那麼，把剛剛講的這些取個名字

73
00:04:47,463 --> 00:04:50,490
這並不是大家在用的術語

74
00:04:50,490 --> 00:04:54,075
不過我發現這還滿有用、也容易聯想：

75
00:04:54,075 --> 00:04:58,380
對於「Bayes error」或其估計，
與「訓練錯誤」之間的差距

76
00:04:58,380 --> 00:05:05,670
我要叫它做 "avoidable bias" (可避免的偏差)

77
00:05:05,670 --> 00:05:11,830
因此，你想要的或許是不斷增進你的訓練成效

78
00:05:11,830 --> 00:05:14,020
直到你達到 Bayes error 為止。不過呢

79
00:05:14,020 --> 00:05:16,565
你不會想比 Bayes error 還要好

80
00:05:16,565 --> 00:05:20,740
因為除非你 overfit，
否則沒辦法比 Bayes error 好

81
00:05:20,740 --> 00:05:24,879
而這邊，訓練錯誤和開發錯誤的差距

82
00:05:24,879 --> 00:05:29,775
你的演算法仍然會有變異的問題

83
00:05:29,775 --> 00:05:35,350
有「可避免的偏差」這詞，
意味著也存在某種偏差

84
00:05:35,350 --> 00:05:38,140
或者某種最小等級的錯誤

85
00:05:38,140 --> 00:05:42,975
是你無法再改進的：如果 Bayes error 是 7.5%

86
00:05:42,975 --> 00:05:46,885
你不會真的想比它還低

87
00:05:46,885 --> 00:05:50,650
所以，與其於說訓練錯誤是 8%

88
00:05:50,650 --> 00:05:53,427
也就是在這例子「偏差」測出來是 8%

89
00:05:53,427 --> 00:06:01,520
不如說「可避免的偏差」是 0.5%；
avoidable bias 測出來

90
00:06:01,520 --> 00:06:06,220
是 0.5%，而變異 (variance) 測出來是 2%

91
00:06:06,220 --> 00:06:11,378
因此，減少這 2% 和這邊的 0.5% 相比，較有進步空間

92
00:06:11,378 --> 00:06:14,384
相反地，如果是左邊的例子

93
00:06:14,384 --> 00:06:20,055
可避免的偏差是 7%

94
00:06:20,055 --> 00:06:24,275
而你有的變異是 2%

95
00:06:24,275 --> 00:06:25,960
所以在左邊的例子

96
00:06:25,960 --> 00:06:31,789
專心減低 avoidable bias 會比較有可能

97
00:06:31,789 --> 00:06:33,310
在這個例子中

98
00:06:33,310 --> 00:06:35,845
了解人類錯誤的水準

99
00:06:35,845 --> 00:06:38,220
也就是了解 Bayes error 的估計

100
00:06:38,220 --> 00:06:42,420
讓你能在不同的情況採取不同的策略

101
00:06:42,420 --> 00:06:45,970
— 要減低偏差、抑或減少變異。

102
00:06:45,970 --> 00:06:48,820
而在決定專注於哪種策略時

103
00:06:48,820 --> 00:06:53,800
要怎麼把人類表現的水準考慮進去，
還有很多眉眉角角

104
00:06:53,800 --> 00:06:55,970
在下部影片，讓我們更深入

105
00:06:55,970 --> 00:06:59,460
了解到底「人類表現的水準」是什麼意思