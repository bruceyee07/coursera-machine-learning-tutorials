우리는 여러분이 어떻게 러닝 알고리즘이 트레이닝세트에서 잘 되기를 바라는지에 대해 이야기 했었는데요. 가끔씩은 너무 잘하는 것이 별로 좋지 않을 수도 있습니다. 인간레벨성능을 알면 그것에 따라 얼마나 성능이 발휘되야 하는지 너무 잘하면 좋지 않을 수도 있으니 적당한 레벨로 알고리즘이 트레이닝세트에서 발휘할 수 있도록 가능케 할 것입니다. 어떤 말인지 자세히 설명해드리도록 하겠습니다. 고양이 인식 프로그램을 봤었는데요, 이 사진을 보면 인간은 거의 완벽에 가까운 정확도를 가지고 있고 인간오류는 1퍼센트라고 해봅시다. 이 경우, 러닝 알고리즘이 8퍼센트 트레이닝 오류, 10퍼센트 dev 오류 그리소 트레이닝세트에서는 조금 잘 하고 싶으셨을 수도 있죠. 그러므로 트레이닝 세트에서의 알고리즘의 성능과 인간의 차이가 매우 크기 때문에 이것은 해당 알고리즘이 트레이닝세트에 잘 피팅되지 않고 있음을 보여줍니다. 그러므로, 바이어스 와 편차를 줄이는 방면에서, 이번 경우에는 바이어스를 줄이는데 중점을 둘 것 같습니다. 더 큰 신경망을 트레이닝 한다거나 더 오랜 시간 동안 트레이닝세트를 운영하는 방법이 있겠습니다. 트레이닝세트에서 더 잘하기 위함이죠. 이번에는 동일한 트레이닝 오류와 dev error를 봅시다. 그리고 인간레벨성능이 1퍼센트가 아니라고 가정해봅시다. 이것을 이쪽으로 복사해보겠습니다, 다른 어플 또는 다른 데이터세트에서는 인간 오류가 7.5퍼센트라고 해봅시다. 데이터세트에 있는 이미지가 너무 흐려서 사람이 판단하기에도 고양이인지 여부를 알 수 없는 경우가 있을 수 있겠죠. 이 사례는 약간 부자연스러운 사례일 수 있긴 합니다. 왜냐면, 인간은 사진을 보고 고양이인지 이미지를 분간하는 게 굉장히 뛰어나기 때문입니다. 하지만 예시인 만큼, 한번 보도록 하겠습니다. 그럼 데이터세트에 있는 이미지가 너무 흐리거나 화질이 너무 좋지 않아서 인간도 7.5퍼센트의 오류를 범한다고 가정해봅시다. 이런 경우, 트레이닝 오류와 dev error가 다른 예제들과 동일하긴 하지만, 트레이닝세트에서는 잘 하고 있는 것을 보실 수 있습니다. 인간레벨성능에서 조금 못 미치는 수준을 보이고 있습니다. 두 번째는, 이 부분을 조금 줄이고 싶을 수 있겠죠. 러닝 알고리즘의 편차를 줄이는 겁니다. 그렇게 하기 위해 Regularization (일반화)를 시도하여 dev error를 트레이닝 오류와 비슷한 수준으로 만드는 것이죠. 이전 코스에서 다루었던 바이어스와 편차에 대한 내용에서는, 주로 Bayes error가 0에 가까운 업무를 다루었습니다. 그러므로 여기서 일어난 것을 설명 드리자면, 고양이인식 프로그램 예로 들자면, 인간레벨 오류를 Bayes error의 추정치로 생각하거나 Bayes optimal error의 추정 수치로 생각합니다. 컴퓨터의 인식 영역에서는, 꽤 합리적인 프록시 값입니다. 이유인 즉 슨, 사람은 computer vision영역에서 굉장히 능숙하고 사람이 할 수 있는 부분이 Bayes error와 크게 다르지 않기 때문입니다. 인간레벨 오류는 정의로만 봤을 때, Bayes error보다 못합니다. 그 이유는 그 어떤 것도 Bayes error보다 더 나을 순 없기 때문이죠. 하지만 인간레벨 오류는 Bayes error에서 크게 떨어지지 않을 수 있습니다. 여기서 놀라운 사실을 발견했는데요, 인간레벨오류가 얼마인지에 따라 또는, 이 값은 사실 Bayes error와 동일합니다만, 어떤 것을 이룰 수 있느냐에 따라 다르지만, 동일한 트레이닝 오류와 dev error인 경우, 이 2가지의 사례와 같이, 바이어스를 줄이는 기술이나 편차를 줄이는 방법에 중점을 두기로 했습니다. 왼쪽 예제에서는, 8퍼센트의 트레이닝은 1퍼센트까지 줄일 수 있다고 하면 굉장히 큰 값입니다. 바이어스를 줄이는 기술이 가능케 하죠. 오른쪽 예시는, Bayes error가 7.5퍼센트입니다. 여기서는 인간레벨오류를 Bayes error를 프로시로 사용합니다. 만약 Bayes error가 7.5퍼세트와 가깝다고 생각되는 경우, 트레이닝 오류를 더 이상 줄일 수 있는 공간이 많지 않습니다. 7.5퍼센트보다 더 많이 뛰어나길 바라지 않습니다. 어차피 그렇게 되기 위해선 교육 량을 늘려야 할 것이기 때문이죠. 대신에, 여기 2퍼센트 갭을 줄이는 방안을 생각해보면, 일반화 과 같은 또는 트레이닝 데이터를 더 수집하는 방안으로 편차를 줄이는 기술을 생각해 볼 것입니다. 몇 가지 이름을 부여하자면, 자주 쓰이는 용어는 아니지만, 개인적으로 유용한 용어였는데요, 생각하는 사고방식이 유용합니다. Bayes error 또는 Bayes error의 근사치와 트레이닝 오류의 차이를 avoidable bias라고 할 것입니다. 여러분이 할 수 잇는 것은 트레이닝 성능을 Bayes error 값으로 내려갈 때까지 계속 개선시키는 것입니다. 하지만 Bayes error보다 잘하면 안되겠죠. 사실 overfitting을 하지 않는 이상 Bayes error보다 잘할 수는 없습니다. 그리고 이것, 트레이닝 오류와 dev error의 차이는 알고리즘 편차 문제의 수치입니다. ‘avoidable bias’라는 용어는 최소의 오류 값이나 특정 오류가 있다는 것을 인정하는 셈이고, Bayes error가 7.5퍼센트인 경우 이 이하로 내려갈 수 없다는 뜻도 내포하고 있습니다. 이 오류 수치 밑으로는 내려가면 좋지 않습니다. 그러므로, 트레이닝 오류가 8퍼센트라고 이야기하기 보다는, 8퍼센트는 바이어스의 측정수치이며, 해당 예시에서는 avoidable bias가 0.5퍼센트이거나, 2퍼센트는 편차 수치입니다. 그러므로 2퍼센트를 줄일 수 있는 공간은 0.5퍼센트 줄이는 공간보다 훨씬 더 많습니다. 반대로 왼쪽 예시는, 7퍼센트는 avoidable bias 이고, 2퍼센트가 편차입니다. 왼쪽 사례에서는, avoidable bias를 줄이면 더욱 큰 효과를 얻을 수 있겠죠. 이번 예시에서는, 인간레벨 오류에 대해 다뤘는데요, Bayes error에 대한 추정치를 이해하면 시나리오마다 각각 다른 전술로 접근하여, bias avoidance 기술을 이용하거나, variance avoidance 기술을 이용할 수 있습니다. 본인이 어느 곳에 중점을 둘지 여부와 관련하여 의사결정을 내리는데 있어, 인간레벨성능이 미묘하게 관여하는 부분이 있는데요, 다음 비디오에선, 인간레벨성능이 구체적으로 어떤 것을 이야기하는지 자세히 알아보겠습니다.