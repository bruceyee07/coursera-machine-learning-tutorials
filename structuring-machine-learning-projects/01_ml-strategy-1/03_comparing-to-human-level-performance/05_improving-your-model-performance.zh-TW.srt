1
00:00:00,360 --> 00:00:01,805
你聽過了正交法 (orthogonalization)、

2
00:00:01,805 --> 00:00:06,561
設置開發/測試集的方法、
用人類水準來估計 Bayes error、

3
00:00:06,561 --> 00:00:11,110
以及如何估計可避免的偏差和差異。

4
00:00:11,110 --> 00:00:14,121
讓我們整合成一套準則

5
00:00:14,121 --> 00:00:17,473
以改善你的演算法學習的成效

6
00:00:17,473 --> 00:00:22,149
那麼，我覺得要讓監督式學習演算法有效

7
00:00:22,149 --> 00:00:24,656
意味著你要能作到兩件事

8
00:00:24,656 --> 00:00:30,015
第一，你很能配適訓練集 (fit the training set)

9
00:00:30,015 --> 00:00:37,670
你可以看成，能夠讓「可避免的偏差」很低

10
00:00:38,830 --> 00:00:42,992
第二，你在訓練集上能做好的

11
00:00:42,992 --> 00:00:47,369
也能推廣到開發集或測試集上做得好

12
00:00:47,369 --> 00:00:50,488
某種角度代表「變異」不差

13
00:00:50,488 --> 00:00:53,717
以正交法的精神而言

14
00:00:53,717 --> 00:00:58,779
這代表有一些旋鈕可以解決「可避免偏差」

15
00:00:58,779 --> 00:01:03,245
例如訓練更大的網路、訓練更久

16
00:01:03,245 --> 00:01:08,635
而有另外一些東西能解決「變異」

17
00:01:08,635 --> 00:01:12,369
例如正則化、蒐集更多訓練資料

18
00:01:12,369 --> 00:01:16,543
統整過去的影片提到的流程

19
00:01:16,543 --> 00:01:21,190
如果你想改善你的機器學習系統

20
00:01:21,190 --> 00:01:26,234
我會建議你觀察訓練錯誤和 Bayes error

21
00:01:26,234 --> 00:01:31,163
估計值之間的差距，讓你有概念可避免的偏差多大

22
00:01:31,163 --> 00:01:35,297
換句話說，你覺得要在訓練資料上進步多少

23
00:01:35,297 --> 00:01:39,366
然後，觀察你的開發錯誤和訓練錯誤

24
00:01:39,366 --> 00:01:41,382
之間的差距，

25
00:01:41,382 --> 00:01:43,871
來估計變異的問題有多嚴重

26
00:01:43,871 --> 00:01:44,711
換句話說

27
00:01:44,711 --> 00:01:48,671
你要花多少力氣在把訓練資料的成效

28
00:01:48,671 --> 00:01:52,392
推廣到開發資料
 — 尤其你並不會直接拿開發資料做訓練

29
00:02:04,393 --> 00:02:09,201
所以無論你想減少「可避免的偏差」多少

30
00:02:09,201 --> 00:02:13,386
我會試著訓練比較大的模型

31
00:02:13,386 --> 00:02:18,124
所以你可以在訓練集上做更好。或者訓練更久、

32
00:02:18,124 --> 00:02:21,196
用更好的最佳化演算法

33
00:02:24,005 --> 00:02:27,433
例如動量法或 RMSprop

34
00:02:27,433 --> 00:02:32,060
或者更好的 Adam

35
00:02:34,874 --> 00:02:39,894
抑或，你可以試著找尋更好的神經網路架構、

36
00:02:39,894 --> 00:02:45,220
或是更好的超參數，這包含很多事

37
00:02:45,220 --> 00:02:50,187
不同的啟動函數、不同隱藏單元的層數

38
00:02:50,187 --> 00:02:55,341
— 雖然這和增大網路一樣

39
00:02:55,341 --> 00:03:00,654
還有嘗試不同模型、架構，例如

40
00:03:00,654 --> 00:03:06,500
Recurrent Neural Network, Convolutional Neural Network
我們會在之後的課程看到

41
00:03:06,500 --> 00:03:09,520
一個新的神經網路架構，能否更適合 (fit)

42
00:03:09,520 --> 00:03:12,800
你的訓練資料，這其實很難事先知道

43
00:03:12,800 --> 00:03:16,570
不過有時用了更好的架構可以得到更好的結果

44
00:03:18,500 --> 00:03:20,941
接下來，你可能發現「變異」是個問題

45
00:03:20,941 --> 00:03:26,417
有很多技巧，以下列出幾個你可以試試

46
00:03:26,417 --> 00:03:30,762
你可以試著蒐集更多資料，因為訓練更多資料

47
00:03:30,762 --> 00:03:35,437
能幫助你更能推廣 (generalize) 至沒看過的開發資料

48
00:03:35,437 --> 00:03:37,759
你可以試正則化 (regularization)

49
00:03:37,759 --> 00:03:43,000
包括像是 L2正則化、dropout

50
00:03:43,000 --> 00:03:50,501
或是以前提到的資料擴增 (data augmentation)

51
00:03:50,501 --> 00:03:55,187
或者一樣，試不同的神經網路架構、

52
00:03:55,187 --> 00:03:58,467
搜尋好的超參數，看看能否為你的問題

53
00:03:58,467 --> 00:04:02,390
找到一個適合的架構

54
00:04:03,810 --> 00:04:07,430
我想這種「可避免的偏差」和「變異」的觀念

55
00:04:07,430 --> 00:04:12,150
很容易學、但很難駕馭

56
00:04:12,150 --> 00:04:16,090
如果你能有系統地套用這禮拜談到的概念

57
00:04:16,090 --> 00:04:20,244
你就能更有效率、更有系統、更有策略

58
00:04:20,244 --> 00:04:24,734
比很多團隊還來得強，能有系統地

59
00:04:24,734 --> 00:04:28,567
改善機器學習系統的成效

60
00:04:28,567 --> 00:04:32,982
那麼，這禮拜的作業，能讓你練習

61
00:04:32,982 --> 00:04:36,832
更加實踐，讓你更了解這些概念

62
00:04:36,832 --> 00:04:38,950
祝你好運

63
00:04:38,950 --> 00:04:42,463
也期望在下禮拜的影片相見

64
00:06:19,757 --> 00:06:20,701
.