1
00:00:00,360 --> 00:00:03,130
在最后的视频中, 你了解到了

2
00:00:03,130 --> 00:00:06,480
学习不同单词的特征化表示的意义。

3
00:00:06,480 --> 00:00:10,154
在本视频中, 你可以看到如何利用这些表示

4
00:00:10,154 --> 00:00:12,140
将它们应用到 NLP 中。

5
00:00:12,140 --> 00:00:13,699
让我们从一个例子开始

6
00:00:13,699 --> 00:00:16,763
继续以人名识别为例,

7
00:00:16,763 --> 00:00:19,260
假设你想检测出人的名字。

8
00:00:19,260 --> 00:00:24,588
给出一个句子，如
‘Sally Johnson is⏎an orange farmer 
萨莉约翰逊是一个橙色的农夫‘，希望

9
00:00:24,588 --> 00:00:30,400
你能指导, 萨莉约翰逊是人的名字,
因此, 输出这样的1。

10
00:00:30,400 --> 00:00:34,410
确保萨莉约翰逊必须是个人，

11
00:00:34,410 --> 00:00:39,520
而不是公司名字的一种方法就是
你知道orange farmer橙农民是个人。

12
00:00:39,520 --> 00:00:44,599
之前, 我们谈到了 one hot 的表示法,
来表示这些单词,

13
00:00:44,599 --> 00:00:46,462
x (1), x (2), 等等。

14
00:00:46,462 --> 00:00:50,239
但如果你现在使用特征化表示法,

15
00:00:50,239 --> 00:00:54,429
即在上个视频中所谈到的嵌入向量。

16
00:00:54,429 --> 00:00:59,292
模型在训练使用单词嵌入作为输入之后,

17
00:00:59,292 --> 00:01:03,008
如果现在看到一个新的输入,
‘ Robert Lin is an apple farmer.
罗伯特.林是一个种苹果的农夫’。

18
00:01:03,008 --> 00:01:07,599
已知橙子和苹果非常相似，这将使

19
00:01:07,599 --> 00:01:12,587
学习算法更好地推广, 以找出Robert Lin罗伯特.林是

20
00:01:12,587 --> 00:01:15,480
人, 也是人的名字。

21
00:01:15,480 --> 00:01:20,404
最有趣的情况之一是,
如果在测试集中, 你看到的

22
00:01:20,404 --> 00:01:25,196
不是‘Robert Lin is an apple farmer
罗伯特. 林是个苹果农夫’,
而是不那么常见的词语呢？

23
00:01:25,196 --> 00:01:29,124
如果是‘Robert Lin⏎is a durian cultivator
罗伯特. 林是种榴莲的耕种者’呢？

24
00:01:31,600 --> 00:01:38,780
榴莲是一种罕见的水果,
在新加坡和其他少数节国家很流行。

25
00:01:38,780 --> 00:01:43,510
但如果你为人名识别任务
设置了一个小的标签训练集,

26
00:01:43,510 --> 00:01:45,900
你可能根本没有在你的训练集中看到

27
00:01:45,900 --> 00:01:49,460
单词’durian榴莲’或’cultivator耕种者

28
00:01:49,460 --> 00:01:53,560
我想严格来说,应该是a durian cultivator
一位种榴莲的耕种者。

29
00:01:53,560 --> 00:01:58,900
但如果你学了一个单词嵌入,
它告诉你榴莲是一种水果,

30
00:01:58,900 --> 00:02:04,700
它就像橙子一样,
并且告诉你耕种者和农民类似,

31
00:02:04,700 --> 00:02:09,240
然后,你可能在训练集中见过
orange farmer橙子果民

32
00:02:09,240 --> 00:02:14,670
从而得知durian⏎cultivator
榴莲耕种者可能也是个人。

33
00:02:14,670 --> 00:02:18,190
因此，单词嵌入能够做到这一点的原因之一是

34
00:02:18,190 --> 00:02:23,400
学习单词嵌入的算法
可以检查大量的文本主体,

35
00:02:23,400 --> 00:02:24,630
也许是在网上找到的这些文本

36
00:02:24,630 --> 00:02:29,204
所以你可以检查非常大的数据集,
也许会达到10亿字,

37
00:02:29,204 --> 00:02:33,876
甚至多达1000亿字，这也是相当合理的。

38
00:02:33,876 --> 00:02:37,430
大量的只含未标记文本的训练集

39
00:02:38,980 --> 00:02:44,200
通过检查大量的未标记的文本,
这些文本是可以免费下载的

40
00:02:44,200 --> 00:02:49,350
你可以弄清楚,橙子和榴莲是类似的。

41
00:02:49,350 --> 00:02:52,780
农夫和耕种者是类似的,因此,

42
00:02:52,780 --> 00:02:56,050
学习嵌入向量,把他们组合在一起。

43
00:02:56,050 --> 00:02:58,370
现在通过阅读大量的互联网文本发现,

44
00:02:58,370 --> 00:03:04,230
橙子和榴莲都是水果,

45
00:03:04,230 --> 00:03:08,206
你可以做的是,采取这个词嵌入,
并将其应用到你的命名

46
00:03:08,206 --> 00:03:12,420
识别任务中,你的训练集可能会小一些,

47
00:03:12,420 --> 00:03:16,140
也许只有10万字,甚至更小。

48
00:03:17,770 --> 00:03:21,870
并且这可以让你进行学习迁移,

49
00:03:21,870 --> 00:03:25,520
你可以利用来自文本的信息，

50
00:03:25,520 --> 00:03:29,540
这些大量未标记的文本可从网络上免费得到

51
00:03:29,540 --> 00:03:34,470
去得出橘子、苹果和榴莲都是水果。

52
00:03:35,660 --> 00:03:40,430
然后将该知识迁移到任务上,
如命名实体识别,

53
00:03:40,430 --> 00:03:45,172
对于这个任务，你可能有
较小的已标记的训练集。

54
00:03:45,172 --> 00:03:49,783
当然,简单起见,
我只画出单向RNN。

55
00:03:49,783 --> 00:03:54,358
如果你确实要执行命名识别任务,则应该

56
00:03:54,358 --> 00:03:58,820
使用双向RNN, 而不是我画的简单RNN。

57
00:03:58,820 --> 00:04:00,296
但总而言之,

58
00:04:00,296 --> 00:04:05,920
这就是如何使用单词嵌入来进行学习迁移

59
00:04:05,920 --> 00:04:10,970
第一步是从大量的文本语料库学习单词嵌入,

60
00:04:10,970 --> 00:04:16,490
或者可以从网上下载已经训练好的单词嵌入。

61
00:04:16,490 --> 00:04:19,710
你可以在网上找到几种单词嵌入

62
00:04:19,710 --> 00:04:23,130
在有许可证的情况下。

63
00:04:23,130 --> 00:04:26,021
然后你可以把这些单词嵌入迁移

64
00:04:26,021 --> 00:04:30,219
到有着更小的已标记训练集的任务上。

65
00:04:30,219 --> 00:04:35,560
然后用这个300维的词嵌入,来代表单词。

66
00:04:35,560 --> 00:04:40,150
还有一点好处是你现在可以使用

67
00:04:40,150 --> 00:04:43,340
相对较低维的特征向量。

68
00:04:43,340 --> 00:04:48,490
不用1万维的one hot向量,

69
00:04:48,490 --> 00:04:53,471
你现在可以改用300维向量。

70
00:04:53,471 --> 00:04:57,951
虽然one hot矢量很快，
而用于嵌入的300维向量

71
00:04:57,951 --> 00:05:01,390
是一个密集的向量。

72
00:05:01,390 --> 00:05:06,430
最后,当你在新任务上训练你的模型时,

73
00:05:06,430 --> 00:05:10,487
比如，在较小标签数据集的命名识别任务上,

74
00:05:10,487 --> 00:05:14,530
你可以选择性的去继续微调参数,

75
00:05:14,530 --> 00:05:20,060
继续用新数据调整单词嵌入。

76
00:05:20,060 --> 00:05:25,589
实战中，只有任务2具有相当大的数据集时,
才会执行此操作。

77
00:05:25,589 --> 00:05:30,650
如果第二步带标签的数据集很小,
那么通常,

78
00:05:30,650 --> 00:05:35,740
我不会继续微调单词嵌入。

79
00:05:35,740 --> 00:05:39,580
所以单词嵌入发挥最大影响的时候,是在你的任务

80
00:05:39,580 --> 00:05:44,990
有一个相对较小的训练集时。

81
00:05:44,990 --> 00:05:47,063
因此它对于许多 NLP 任务是有用的。

82
00:05:47,063 --> 00:05:47,845
在这里我只说出几个。

83
00:05:47,845 --> 00:05:50,500
如果你不知道这些术语,没关系。

84
00:05:50,500 --> 00:05:54,855
在命名实体识别,文本摘要中可以使用,

85
00:05:54,855 --> 00:05:57,262
指代消解，用于语法分析

86
00:05:57,262 --> 00:06:00,810
这些都是非常标准的 NLP 任务。

87
00:06:00,810 --> 00:06:04,360
在语言建模、机器翻译上也有用处，

88
00:06:04,360 --> 00:06:08,141
尤其是在执行语言建模或机器翻译任务时

89
00:06:08,141 --> 00:06:11,970
你的大量数据专用于这些任务。

90
00:06:11,970 --> 00:06:15,241
因此, 如同在其他迁移学习设置中看到的,

91
00:06:15,241 --> 00:06:19,307
如果您正在尝试
从某个任务 a 转移到某个任务 B,

92
00:06:19,307 --> 00:06:24,014
迁移学习过程在这种情况下最有用的,
就是当你碰巧

93
00:06:24,014 --> 00:06:28,955
对A有很多数据集，B有相对更少的数据集。

94
00:06:28,955 --> 00:06:33,149
对于许多 NLP 任务, 这是正确的，然而对于

95
00:06:33,149 --> 00:06:38,020
一些语言建模和机器翻译，就不一定了。

96
00:06:38,020 --> 00:06:42,434
最后,单词嵌入与人脸编码有着有趣的关联

97
00:06:42,434 --> 00:06:46,398
就是你在上节课中了解到的编码思想,

98
00:06:46,398 --> 00:06:50,360
如果你上了卷积神经网络课程的话。

99
00:06:50,360 --> 00:06:53,579
你就会记得,对于人脸识别,

100
00:06:53,579 --> 00:06:57,743
我们训练这个 Siamese 网络来学习,

101
00:06:57,743 --> 00:07:02,155
一个128维向量来表示不同的面孔。

102
00:07:02,155 --> 00:07:07,128
然后你可以比较这些编码,为了识别出

103
00:07:07,128 --> 00:07:10,920
这两张照片是不是同一张脸。

104
00:07:10,920 --> 00:07:16,510
编码和嵌入的意思相近。

105
00:07:16,510 --> 00:07:22,020
所以在人脸识别中, 人们也使用术语

106
00:07:22,020 --> 00:07:27,960
编码来代表这些向量, f (x (i)) 和 f (x (j))。

107
00:07:27,960 --> 00:07:30,600
人脸识别与

108
00:07:30,600 --> 00:07:34,590
在 word 嵌入中所做的区别是,
 对于人脸的识别来说,

109
00:07:34,590 --> 00:07:40,350
你想训练一个神经网络
可以以任何面孔图片作为输入,

110
00:07:40,350 --> 00:07:42,140
甚至一张你从未见过的照片,

111
00:07:42,140 --> 00:07:46,280
神经网络来计算这张新图片的编码。

112
00:07:46,280 --> 00:07:50,260
而我们要做什么,
你会更清楚, 在我们学习

113
00:07:50,260 --> 00:07:54,960
接下来的几个视频时,
为了学习词嵌入,我们要做的是

114
00:07:54,960 --> 00:07:58,700
有一个固定的词汇库, 比方说, 1万词汇的词汇库。

115
00:07:58,700 --> 00:08:02,282
我们将学习向量 e1 到,

116
00:08:02,282 --> 00:08:06,335
e10,000, 这只是学习一个固定的编码

117
00:08:06,335 --> 00:08:12,420
或者说固定的单词嵌入，
对于词汇中的每一个词来说。

118
00:08:12,420 --> 00:08:17,354
所以，这是和人脸识别中的想法
的一个区别

119
00:08:17,354 --> 00:08:21,557
这个算法就是
在接下来的视频中要讨论的。

120
00:08:21,557 --> 00:08:26,330
但术语编码和嵌入有时被互换使用。

121
00:08:26,330 --> 00:08:30,485
所以, 我刚才描述的差异不是

122
00:08:30,485 --> 00:08:31,496
术语表述的差异.

123
00:08:31,496 --> 00:08:36,326
区别如何使用这些算法,
在人脸识别中

124
00:08:36,326 --> 00:08:40,450
是任何在未来可能看到的图片，

125
00:08:40,450 --> 00:08:45,720
而在自然语言处理中,
可能只是一个固定的词汇表,

126
00:08:45,720 --> 00:08:49,170
和其他的词汇, 
我们只是将其作为一个未知的词。

127
00:08:50,310 --> 00:08:54,580
所以在这个视频中,
你看到了如何使用单词嵌入

128
00:08:54,580 --> 00:08:56,710
去实施这种方式的转移学习。

129
00:08:56,710 --> 00:09:01,350
以及如何, 通过替换 one hot 向量,
我们使用之前的嵌入向量

130
00:09:01,350 --> 00:09:05,110
你可以更好的推广你的算法,

131
00:09:05,110 --> 00:09:07,267
或者可以学习更少的标签数据。

132
00:09:07,267 --> 00:09:11,462
接下来, 我想向您展示
这些单词嵌入的一些属性。

133
00:09:11,462 --> 00:09:14,026
然后, 我们将讨论

134
00:09:14,026 --> 00:09:16,450
真正学习这些单词嵌入的算法。

135
00:09:16,450 --> 00:09:18,023
让我们继续下一个视频,

136
00:09:18,023 --> 00:09:22,150
你可以看到单词嵌入，
如何帮助推理的类比。