在最后的视频中, 你了解到了 学习不同单词的特征化表示的意义。 在本视频中, 你可以看到如何利用这些表示 将它们应用到 NLP 中。 让我们从一个例子开始 继续以人名识别为例, 假设你想检测出人的名字。 给出一个句子，如
‘Sally Johnson is⏎an orange farmer 
萨莉约翰逊是一个橙色的农夫‘，希望 你能指导, 萨莉约翰逊是人的名字,
因此, 输出这样的1。 确保萨莉约翰逊必须是个人， 而不是公司名字的一种方法就是
你知道orange farmer橙农民是个人。 之前, 我们谈到了 one hot 的表示法,
来表示这些单词, x (1), x (2), 等等。 但如果你现在使用特征化表示法, 即在上个视频中所谈到的嵌入向量。 模型在训练使用单词嵌入作为输入之后, 如果现在看到一个新的输入,
‘ Robert Lin is an apple farmer.
罗伯特.林是一个种苹果的农夫’。 已知橙子和苹果非常相似，这将使 学习算法更好地推广, 以找出Robert Lin罗伯特.林是 人, 也是人的名字。 最有趣的情况之一是,
如果在测试集中, 你看到的 不是‘Robert Lin is an apple farmer
罗伯特. 林是个苹果农夫’,
而是不那么常见的词语呢？ 如果是‘Robert Lin⏎is a durian cultivator
罗伯特. 林是种榴莲的耕种者’呢？ 榴莲是一种罕见的水果,
在新加坡和其他少数节国家很流行。 但如果你为人名识别任务
设置了一个小的标签训练集, 你可能根本没有在你的训练集中看到 单词’durian榴莲’或’cultivator耕种者 我想严格来说,应该是a durian cultivator
一位种榴莲的耕种者。 但如果你学了一个单词嵌入,
它告诉你榴莲是一种水果, 它就像橙子一样,
并且告诉你耕种者和农民类似, 然后,你可能在训练集中见过
orange farmer橙子果民 从而得知durian⏎cultivator
榴莲耕种者可能也是个人。 因此，单词嵌入能够做到这一点的原因之一是 学习单词嵌入的算法
可以检查大量的文本主体, 也许是在网上找到的这些文本 所以你可以检查非常大的数据集,
也许会达到10亿字, 甚至多达1000亿字，这也是相当合理的。 大量的只含未标记文本的训练集 通过检查大量的未标记的文本,
这些文本是可以免费下载的 你可以弄清楚,橙子和榴莲是类似的。 农夫和耕种者是类似的,因此, 学习嵌入向量,把他们组合在一起。 现在通过阅读大量的互联网文本发现, 橙子和榴莲都是水果, 你可以做的是,采取这个词嵌入,
并将其应用到你的命名 识别任务中,你的训练集可能会小一些, 也许只有10万字,甚至更小。 并且这可以让你进行学习迁移, 你可以利用来自文本的信息， 这些大量未标记的文本可从网络上免费得到 去得出橘子、苹果和榴莲都是水果。 然后将该知识迁移到任务上,
如命名实体识别, 对于这个任务，你可能有
较小的已标记的训练集。 当然,简单起见,
我只画出单向RNN。 如果你确实要执行命名识别任务,则应该 使用双向RNN, 而不是我画的简单RNN。 但总而言之, 这就是如何使用单词嵌入来进行学习迁移 第一步是从大量的文本语料库学习单词嵌入, 或者可以从网上下载已经训练好的单词嵌入。 你可以在网上找到几种单词嵌入 在有许可证的情况下。 然后你可以把这些单词嵌入迁移 到有着更小的已标记训练集的任务上。 然后用这个300维的词嵌入,来代表单词。 还有一点好处是你现在可以使用 相对较低维的特征向量。 不用1万维的one hot向量, 你现在可以改用300维向量。 虽然one hot矢量很快，
而用于嵌入的300维向量 是一个密集的向量。 最后,当你在新任务上训练你的模型时, 比如，在较小标签数据集的命名识别任务上, 你可以选择性的去继续微调参数, 继续用新数据调整单词嵌入。 实战中，只有任务2具有相当大的数据集时,
才会执行此操作。 如果第二步带标签的数据集很小,
那么通常, 我不会继续微调单词嵌入。 所以单词嵌入发挥最大影响的时候,是在你的任务 有一个相对较小的训练集时。 因此它对于许多 NLP 任务是有用的。 在这里我只说出几个。 如果你不知道这些术语,没关系。 在命名实体识别,文本摘要中可以使用, 指代消解，用于语法分析 这些都是非常标准的 NLP 任务。 在语言建模、机器翻译上也有用处， 尤其是在执行语言建模或机器翻译任务时 你的大量数据专用于这些任务。 因此, 如同在其他迁移学习设置中看到的, 如果您正在尝试
从某个任务 a 转移到某个任务 B, 迁移学习过程在这种情况下最有用的,
就是当你碰巧 对A有很多数据集，B有相对更少的数据集。 对于许多 NLP 任务, 这是正确的，然而对于 一些语言建模和机器翻译，就不一定了。 最后,单词嵌入与人脸编码有着有趣的关联 就是你在上节课中了解到的编码思想, 如果你上了卷积神经网络课程的话。 你就会记得,对于人脸识别, 我们训练这个 Siamese 网络来学习, 一个128维向量来表示不同的面孔。 然后你可以比较这些编码,为了识别出 这两张照片是不是同一张脸。 编码和嵌入的意思相近。 所以在人脸识别中, 人们也使用术语 编码来代表这些向量, f (x (i)) 和 f (x (j))。 人脸识别与 在 word 嵌入中所做的区别是,
 对于人脸的识别来说, 你想训练一个神经网络
可以以任何面孔图片作为输入, 甚至一张你从未见过的照片, 神经网络来计算这张新图片的编码。 而我们要做什么,
你会更清楚, 在我们学习 接下来的几个视频时,
为了学习词嵌入,我们要做的是 有一个固定的词汇库, 比方说, 1万词汇的词汇库。 我们将学习向量 e1 到, e10,000, 这只是学习一个固定的编码 或者说固定的单词嵌入，
对于词汇中的每一个词来说。 所以，这是和人脸识别中的想法
的一个区别 这个算法就是
在接下来的视频中要讨论的。 但术语编码和嵌入有时被互换使用。 所以, 我刚才描述的差异不是 术语表述的差异. 区别如何使用这些算法,
在人脸识别中 是任何在未来可能看到的图片， 而在自然语言处理中,
可能只是一个固定的词汇表, 和其他的词汇, 
我们只是将其作为一个未知的词。 所以在这个视频中,
你看到了如何使用单词嵌入 去实施这种方式的转移学习。 以及如何, 通过替换 one hot 向量,
我们使用之前的嵌入向量 你可以更好的推广你的算法, 或者可以学习更少的标签数据。 接下来, 我想向您展示
这些单词嵌入的一些属性。 然后, 我们将讨论 真正学习这些单词嵌入的算法。 让我们继续下一个视频, 你可以看到单词嵌入，
如何帮助推理的类比。