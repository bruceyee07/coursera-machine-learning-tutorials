1
00:00:00,870 --> 00:00:05,535
机器学习和AI算法越来越受人信任

2
00:00:05,535 --> 00:00:07,590
可以用来帮人做出非常重要的决策

3
00:00:07,590 --> 00:00:11,352
所以我们需要尽可能确保

4
00:00:11,352 --> 00:00:16,550
算法中没有我们不希望看到的偏见，如性别偏见、种族偏见等

5
00:00:16,550 --> 00:00:20,690
这节课我想教你一些方法

6
00:00:20,690 --> 00:00:24,415
来减少或消除在词嵌入中的这些形式的偏差

7
00:00:24,415 --> 00:00:28,630
当我用术语偏差时，我指的不是“偏差”这个变量

8
00:00:28,630 --> 00:00:33,667
不是说数据偏离，我指的是性别、种族、性取向的偏差

9
00:00:33,667 --> 00:00:38,353
这两者之间的意思是不同的，这里就指的不是我在技术方面提到的偏差

10
00:00:38,353 --> 00:00:40,330
也就是在机器学习方面

11
00:00:40,330 --> 00:00:43,430
而关于词偏差方面的问题，我们要讲的是

12
00:00:43,430 --> 00:00:47,700
如何让词嵌入学习相似性，如男人对应女人，国王对应皇后

13
00:00:47,700 --> 00:00:52,583
但如果你说男人对应程序猿，那么女人对应什么呢？

14
00:00:52,583 --> 00:00:56,244
所以这篇论文的作者：Tolga Bolukbasi

15
00:00:56,244 --> 00:01:00,428
Kai Wei Chang，James Zou，Venkatesh Saligrama和

16
00:01:00,428 --> 00:01:06,098
Adam Kalai找到一种很奇特的方法，使得词嵌入可以做到

17
00:01:06,098 --> 00:01:11,008
这样的输出：男人对应程序猿，女人对应家庭主妇

18
00:01:11,008 --> 00:01:17,145
而这看起来还是错的，因为这导致了一个不正常的性别刻板印象

19
00:01:17,145 --> 00:01:20,500
比较好的算法应该是可以得出：男人对应程序猿

20
00:01:20,500 --> 00:01:22,815
而女人也对应程序猿

21
00:01:22,815 --> 00:01:27,618
同时他们还发现，父亲对应医生，那母亲对应什么呢？

22
00:01:27,618 --> 00:01:32,534
不幸的是，一些训练好的词嵌入会产生这样的输出：

23
00:01:32,534 --> 00:01:34,530
母亲对应护士

24
00:01:34,530 --> 00:01:38,300
所以词嵌入会反应性别、种族、年龄

25
00:01:38,300 --> 00:01:42,798
性取向，以及其他的一些偏差用来训练模型

26
00:01:42,798 --> 00:01:48,520
其中有一个我特别感兴趣的是社会经济学地位的偏差

27
00:01:48,520 --> 00:01:52,140
我认为每个人，无论你是来自富有的家庭

28
00:01:52,140 --> 00:01:54,743
还是低收入的家庭，或者之间

29
00:01:54,743 --> 00:01:57,554
我认为每个人都应该有很大的机会来发展自己

30
00:01:57,554 --> 00:02:02,537
并且因为机器学习算法能够用来做出许多重要的决定

31
00:02:02,537 --> 00:02:03,375
决定

32
00:02:03,375 --> 00:02:08,388
这些算法能影响每件事，从学校申请到求职

33
00:02:08,388 --> 00:02:13,464
再到申请贷款，无论你申请贷款有没有被批准

34
00:02:13,464 --> 00:02:17,710
再到司法系统里，来量刑审判罪犯

35
00:02:17,710 --> 00:02:22,560
学习算法都在做出非常重要的决策，所以我认为

36
00:02:22,560 --> 00:02:28,130
尽量消除算法中的这些偏差是很重要的

37
00:02:28,130 --> 00:02:33,940
更明确来讲，是消除这些不想看到的偏差结果

38
00:02:33,940 --> 00:02:38,770
目前的单词嵌入可以找出偏差的文本用来

39
00:02:38,770 --> 00:02:42,410
训练模型，而这些找出来的偏差

40
00:02:42,410 --> 00:02:48,460
是由人的行为所影响的

41
00:02:48,460 --> 00:02:50,450
几十年间，几个世纪间

42
00:02:50,450 --> 00:02:55,085
我认为人类在减少这类偏差方面有了很大的进步

43
00:02:55,085 --> 00:02:59,355
对AI而言幸运的是，我们有更好的方法

44
00:02:59,355 --> 00:03:02,085
来更快地减少由AI造成的偏差

45
00:03:02,085 --> 00:03:05,305
而且相比人造成的偏差更快

46
00:03:05,305 --> 00:03:09,690
尽管我们从未说AI已经发展得很好了

47
00:03:09,690 --> 00:03:12,080
仍然有许多研究要做

48
00:03:12,080 --> 00:03:16,110
我们需要很多操作
来减少这些在我们算法中的这些偏差

49
00:03:16,110 --> 00:03:20,621
但在这个视频中我想分享一个例子

50
00:03:20,621 --> 00:03:24,973
这个例子参考了Bolukbasi

51
00:03:24,973 --> 00:03:28,241
和其他一些人在减少词嵌入误差方面的研究

52
00:03:30,356 --> 00:03:32,210
接下来我们来学习其中的原理

53
00:03:32,210 --> 00:03:38,063
假设我们已经得到一个训练好的词嵌入模型

54
00:03:38,063 --> 00:03:43,803
babysister在这，doctor在这

55
00:03:43,803 --> 00:03:48,403
grandmother在这

56
00:03:48,403 --> 00:03:52,403
grandfather在这

57
00:03:52,403 --> 00:03:56,860
可能girl在这，boy在这

58
00:03:56,860 --> 00:04:01,830
she在这 he在这

59
00:04:01,830 --> 00:04:07,060
因此,我们首先要识别

60
00:04:07,060 --> 00:04:11,750
需要减少或消除的特定偏见的方向

61
00:04:11,750 --> 00:04:15,760
这里我将用性别偏见为例

62
00:04:15,760 --> 00:04:18,450
但这些思路可以应用到其他所有

63
00:04:18,450 --> 00:04:21,050
我在之前ppt上提到的偏见类型

64
00:04:26,450 --> 00:04:30,840
那如何才可以明确对应偏见的方向呢？

65
00:04:30,840 --> 00:04:36,010
对性别而言，我们可以将词向量he

66
00:04:36,010 --> 00:04:41,580
减去词向量she，因为他们是按性别来区分不同的

67
00:04:41,580 --> 00:04:46,230
符号为e_male-e_female

68
00:04:46,230 --> 00:04:48,720
取相类似的，然后取平均值对吧

69
00:04:48,720 --> 00:04:51,920
并采取一些这些差异,基本上对他们去平均数。

70
00:04:51,920 --> 00:04:56,490
这能让你明白在这个案例中应该得出一个怎样的结果

71
00:04:56,490 --> 00:05:01,170
这个方向是性别的方向，即偏差方向

72
00:05:02,640 --> 00:05:10,650
然而这个方向和我们希望得到的偏差方向不一致

73
00:05:10,650 --> 00:05:16,580
所以这个是无偏差方向

74
00:05:16,580 --> 00:05:22,013
按这个例子中，把这个偏差方向视为1维的子空间

75
00:05:22,013 --> 00:05:27,047
然而一个无偏差方向，即299维的子空间

76
00:05:27,047 --> 00:05:31,520
我将简要的描述

77
00:05:31,520 --> 00:05:34,504
偏差方向可以比1维要高

78
00:05:34,504 --> 00:05:38,426
而不是取平均，这里实际上会用到

79
00:05:38,426 --> 00:05:42,597
一个更加复杂的叫SVU算法，
即singular value decomposition

80
00:05:42,597 --> 00:05:44,566
这个和PCA很类似

81
00:05:44,566 --> 00:05:48,429
如果你熟悉主成分分析

82
00:05:48,429 --> 00:05:53,729
它使用的方法类似于这个PCA

83
00:05:53,729 --> 00:05:57,320
在此之后，下一步就是中立化

84
00:05:57,320 --> 00:06:02,710
对于每一个没有定义的词，通过映射来摆脱偏差

85
00:06:02,710 --> 00:06:07,350
有些词本来就带有性别的含义

86
00:06:07,350 --> 00:06:08,400
所以像grandmother

87
00:06:08,400 --> 00:06:13,680
grandfather，girl，boy，she，he，
都是本来就有性别相关的定义

88
00:06:13,680 --> 00:06:16,431
然而像其他的词，如doctor

89
00:06:16,431 --> 00:06:19,341
babysitter，这些词是性别中性词

90
00:06:19,341 --> 00:06:24,008
在很多情况下，你也许希望doctor

91
00:06:24,008 --> 00:06:28,601
babysitter是中性的或者说是
性别导向为中立的

92
00:06:28,601 --> 00:06:32,989
我们现在以性别为例

93
00:06:32,989 --> 00:06:35,733
到目前为止每个词都是没有定义的

94
00:06:35,733 --> 00:06:39,646
这意味着不会有像grandmother和grandfather

95
00:06:39,646 --> 00:06:44,450
这类词，它们有包含性别的组成部分

96
00:06:44,450 --> 00:06:48,190
因为grandmother是女性，grandfather是男性

97
00:06:48,190 --> 00:06:51,253
所以对像doctor和babysitter这类词

98
00:06:51,253 --> 00:06:56,330
让我们把他们映射到这个轴来减少
他们的组成部分

99
00:06:56,330 --> 00:07:00,636
或者消除他们在偏差方向的组成部分

100
00:07:00,636 --> 00:07:04,330
所以要在这个水平方向上减少组成部分

101
00:07:06,050 --> 00:07:08,311
这是第二步的“中立”步骤

102
00:07:08,311 --> 00:07:13,192
最后一步叫均匀化

103
00:07:13,192 --> 00:07:17,861
你有一对词，比如grandmother和

104
00:07:17,861 --> 00:07:22,740
grandfather或girl和boy，你希望

105
00:07:22,740 --> 00:07:27,790
词嵌入中唯一的差别是性别

106
00:07:27,790 --> 00:07:30,410
那为什么你想这么做呢？

107
00:07:30,410 --> 00:07:33,910
在这个例子中，距离也就是相似度

108
00:07:33,910 --> 00:07:38,500
在babysitter和grandmother之间的距离

109
00:07:38,500 --> 00:07:42,650
比babysitter和grandfather之间的要小

110
00:07:42,650 --> 00:07:46,130
这会加强一种不健康

111
00:07:46,130 --> 00:07:50,830
不希望看到的偏差，
即：grandmother之于babysitter
多余grandermother之于grandfather

112
00:07:50,830 --> 00:07:53,960
所以在最后平均化这一步中

113
00:07:53,960 --> 00:07:57,170
我们希望确保的是像grandmother，
grandfather这类词

114
00:07:57,170 --> 00:08:01,900
有相似性，有相同的距离

115
00:08:01,900 --> 00:08:06,720
比如都是性别中立的，
比如babysitter之于doctor

116
00:08:06,720 --> 00:08:09,102
对这问题有一些线性代数的方法来解决

117
00:08:09,102 --> 00:08:13,616
基本要做的是移动grandmother

118
00:08:13,616 --> 00:08:20,705
和grandfather分别到一个点，
使得两点到中间轴的距离相等

119
00:08:20,705 --> 00:08:24,894
效果是babysitter

120
00:08:24,894 --> 00:08:28,240
到这两点的距离相等

121
00:08:29,480 --> 00:08:33,716
通常有许多对这样的词，比如

122
00:08:33,716 --> 00:08:38,724
grandmother-grandfather，
boy-girl，sorority-fraternity

123
00:08:38,724 --> 00:08:44,326
再比如girlhood-boyhood,
sister-brother,⏎niece-nephew, daughter-son

124
00:08:44,326 --> 00:08:49,760
你可能需要对这些都完成平均化

125
00:08:49,760 --> 00:08:54,600
最后一个细节是，你如何决定什么词需要中立化？

126
00:08:54,600 --> 00:08:59,550
比如，doctor就是应该性别中立的一个词

127
00:08:59,550 --> 00:09:04,270
使其没有性别特征或种族特征

128
00:09:04,270 --> 00:09:06,360
而grandmother

129
00:09:06,360 --> 00:09:09,900
应该有性别特征

130
00:09:09,900 --> 00:09:12,950
而像beard这类词

131
00:09:12,950 --> 00:09:16,470
只是一个统计事实，男性比

132
00:09:16,470 --> 00:09:21,510
女性可能有更多胡须，
所以也许胡须应该更加接近男性

133
00:09:21,510 --> 00:09:26,463
而作者做的是训练一个分类器来试着明白

134
00:09:26,463 --> 00:09:29,173
哪些词需要定义

135
00:09:29,173 --> 00:09:34,690
哪些词需要性别特征，哪些不要

136
00:09:34,690 --> 00:09:39,200
得出的结论是大多数英语单词不需要定义

137
00:09:39,200 --> 00:09:41,770
这意味着性别是不需要定义的

138
00:09:41,770 --> 00:09:47,391
这一小部分有相关性的词，
如grandmother-grandfather

139
00:09:47,391 --> 00:09:52,360
girl-boy, sorority-fraternity不应该中立化

140
00:09:53,370 --> 00:09:58,160
所以一个线性分类器能告诉你什么词要经历

141
00:09:58,160 --> 00:10:02,774
中立化步骤来映射偏差方向

142
00:10:02,774 --> 00:10:08,022
来映射到这个299维的子空间

143
00:10:08,022 --> 00:10:12,118
有多少对词需要平均化

144
00:10:12,118 --> 00:10:17,608
事实上这些词相对较少，
至少对性别这个例子而言只这样的

145
00:10:17,608 --> 00:10:22,428
这是很灵活的，
来手动找出你想平均化的几对词

146
00:10:22,428 --> 00:10:26,750
其实整个算法
是比我现在所陈述的更为复杂的

147
00:10:26,750 --> 00:10:29,990
你可以看看论文里的所有细节

148
00:10:29,990 --> 00:10:32,740
你也会用到这些方法

149
00:10:32,740 --> 00:10:35,510
在之后的编程练习中

150
00:10:35,510 --> 00:10:39,950
总结一下，我认为减少或消除算法中的偏差

151
00:10:39,950 --> 00:10:44,190
是非常重要的问题，因为这些算法是用来

152
00:10:44,190 --> 00:10:48,175
帮我们做出决策的

153
00:10:48,175 --> 00:10:51,350
在这个视频中，我只是分享了一个想法

154
00:10:51,350 --> 00:10:53,990
如何试着说出这个问题

155
00:10:53,990 --> 00:11:00,180
但这仍然是一个正在继续进行的研究方向

156
00:11:00,180 --> 00:11:03,458
所以这周我们就先讲到这里

157
00:11:03,458 --> 00:11:06,170
祝你这周的编程练习顺利

158
00:11:06,170 --> 00:11:07,660
我期待下周还能见到你