这节课中，你会开始学习 一些具体的词嵌入学习算法 在深度学习的发展历史中，学习词嵌入的过程也一样 人们其实是从相对复杂的算法开始的 随着时间的推移, 研究人员发现他们可以使用 更简单的算法，并且仍然 可以得到很好的结果，尤其是对大数据集来说 但事实是 现在最流行的一些算法 它们是如此的简单以至于如果我先提出来 看起来似乎有点不可思议 为什么这么简单的算法也可以发挥作用 因此我将会从一些 稍微更复杂的算法开始，因为我认为它们其实 更容易直观地理解它们可以工作的原因 然后我们会继续简化这些算法并展示 一些虽然简单却可以获得很好结果的算法 那我们开始吧。 假设你正在建立 一个语言模型并且你使用神经网络来建 因此，在训练的过程中，你可能会想让你的神经网络做一些类似输入之类的工作 我想要一杯橙汁 然后预测序列中的下一个单词 在每个单词下面 我还在不同单词的词汇表上写下了索引 所以事实上 构建神经网络语言模型是学习嵌入集合小巧的方式 我在这张幻灯片上提出的想法源于 Yoshua Bengio Rejean Ducharme, Pascals Vincent,和Christian Jauvin 这也是如何通过建立一个神经网络来预测序列中的下一个词 让我把这些单词列出来 我想要一杯橙汁 我们从第一个单词 I 开始 所以我要构造一个对应于单词 I 的添加向量 所以有一个添加向量与1在位置4343 这将会是一个10,000维的向量 我们接下来要做的是有一个参数矩阵 E 然后用 E 和 O 相乘来得到一个嵌入向量 e4343 这一步真正意味着 e4343是由矩阵E和一个添加向量43相乘得到的 然后,我们会对所有其他的单词进行同样的操作 所以这个词想要的, 是9665，一个添加向量的地方 乘以E来得到嵌入向量 对于其他单词也类似 A 是字典里的第一个单词 在字母表中最先出现, 所以是O1，得到这个E1 对于这个句子中的其他单词也类似 现在你有了一些三维的嵌入 所以每一个都是一个300维的嵌入向量 然后我们能做的 就是把这些全部填入一个神经网络,这里就是神经网络层 然后把这个神经网络接到一个softmax层 它同样也有自己的参数 而且softmax会在词汇表1万种可能的输出种分类出 那些我们试图预测出的最终单词 因此，如果在训练过程中我们看到单词juice 那么softmax重复培训的目标应该是预测 紧接着的另一个单词就是juice 所以这个隐含单词会有自己的参数 所以我将用W1和B1来表示 softmax自有的参数用W2和B2表示 他们使用的是300维的词嵌入 这里我们有六个词 也就是6乘以300 所以这层或者这个输入就会是1800维的向量 由6个嵌入向量堆叠在一起得到 实际上, 更常见的做法是拥有一个固定的历史窗口 比如，你可能决定总是想预测 下一个单词，譬如基于前四个单词 这里四就是算法的超参数 这就是你如何调整算法来应对 很长或者很短的句子，或者你决定 总是只看前四个单词 所以你说, 我还是会用那四个单词 所以, 我们先不看这些 因此如果你总是使用包含四个单词的历史 这意味着你的神经网络总是输入一个1200维的特征向量 进入这一层 然后有一个softmax来试图预测输出 再次, 各种选择 使用一个固定的历史，只意味着你可以处理任意 长度的句子，因为输入大小总是固定的 因此,这个模型的参数就是矩阵E 然后对于所有的单词使用相同的矩阵E 所以你没有 前面四个单词不同位置的不同矩阵 是相同的矩阵 E。然后 这些权值就是算法的参数 你可以使用这批单词来进行梯度计算 使得你的训练集的出现可能性最大化 来重复地基于序列中给定的四个单词进行预测 你的文本语料库中的下一个单词是什么 结果表明,这个算法我们将学习到相当恰当的词嵌入 原因是, 如果你还记得我们的橙汁, 苹果汁的例子 是在算法的激励下学习 对于橙和苹果来说非常相似的词嵌入 因为这样做可以使它更好地拟合 训练集，因为偶尔会看到橙汁 或者苹果汁, 所以 如果你只有一个300维的特征向量来表示所有的单词 算法会发现它可以很快地拟合训练集 如果苹果,橙,葡萄和梨, 等等并且甚至一种很少见的水果榴莲 它们都有相似的特征向量 所以这就是一种较早的相当成功的算法 来学习词嵌入 以及学习矩阵E。但现在让我们 泛化这个算法来看看我们怎样得到更简单的算法 所以, 我想说明其他算法 用一个更复杂的句子作为我们的例子 假设在你的训练集中 你有这样一个更长的句子 我想要一杯橙汁作为我的麦片伴侣 在前一张幻灯片我们看到的是 这个算法的工作是预测一些单词juice 我们把它叫做目标词 并且给定了一些上下文也就是最后四个单词 因此如果你的目标是学习 嵌入，对此我已经实验了许多不同类型的上下文的研究人员的嵌入 如果我们要建立一个语言模型那么 很自然地我们会用目标词前面的几个单词作为上下文 但是如果你的目标是学习语言模型本身 那么你可以选择其他的上下文 例如,你可以构造一个学习问题 其中上下文是左右两边的四个单词 所以你就可以把左右四个单词作为上下文 这也就意味着我们构造了一个学习问题 会把左边的四个单词提供给算法 也就是a glass of orange 以及右边的四个单词 也就是to go along with 这里需要预测的是中间的单词 构造一个像这样的学习算法,也就是你有 左边四个单词以及右边四个单词的词嵌入输入到神经网络中 和你在之前的幻灯片中看到的类似 试图预测中间的单词 试着把目标单词放在中间 这也可以用来学习词嵌入 或者你想用一个更简单的上下文 也许你只用最后一个词 那么只给你单词orange 什么单词会接在orange之后呢 所以这就会是不同的学习问题,你告诉它一个单词 orange 然后说好吧 你认为下一个单词是什么 你可以建立一个神经网络只输入一个单词 前一个字或嵌入 到神经网络中，当你试图预测下一个单词时 或者,一件令人惊讶的事情是只取相邻的一个单词 有些可能会告诉你,那么好吧 取单词glass 邻近的一个单词 有些可能会说,我看 单词glass然后有另一个词靠近glass 你认为这个词是什么 那么将会使用相邻的一个词作为上下文 我们会在下一个视频中正式说明这个，但这就是Skip-Gram模型的想法 只是一个简单的算法的例子, 其中的上下文现在更简单 只有一个单词而不是之前的四个 但是这个效果也很好 所以研究人员发现, 如果你真的想建立一个语言模型 用最后几个单词作为上下文语境是很自然的 但是, 如果你的主要目标是真正学习词嵌入 那么你可以使用所有这些其他的上下文并且他们 也会产生非常有意义的词嵌入 在接下来的视频中我会详细介绍具体细节 我们会讨论沃尔特 VEC 模型 综上所述, 在这个视频中你们看到了语言建模问题 这导致机器学习问题的不同角度 你输入像最后四个单词这样的上下文, 然后预测一些目标词 如何看待这个问题让你可以学习输入词嵌入 下节课 你们将看到如何使用更简单的上下文 甚至更简单的学习算法来标记从上下文到目标词 并基于此训练出一个有用的词嵌入 下一节, 我们将讨论沃尔特 VEC模型
GTC字幕组翻译