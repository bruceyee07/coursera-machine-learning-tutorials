你已经学到了计算词嵌入的几种算法 另一种在自然语言处理中
具有一定势头的算法是GloVe算法 它并不像Word2Vec或者Skip-Gram算法那样被广泛使用 但它也有一定拥趸 在我看来，是因为它的简洁性 让我们一起来看看 GloVe算法是由Jeffrey Pennington, Richard Socher和Chris Manning发明的 GloVe的意思是用于词汇表征的全局矢量 在前面，我们成对抽样词语 即上下文词语和目标词语对的时候 采用的方法是选取两个在我们的文本中距离相近的词语 而GloVe算法所做的是 让这个过程更加明确 假设 X_ij表示次数就是i 在 j的上下文中出现的次数 因此，这里的 i和 j扮演了 t和 c的角色 你可以把 X_ij想象成 X_tc 但是，你可以遍历你的训练文本集，并且数出 词语 i在另一个词语 j的上下文中出现了多少次 或者说词语 t在另一个词语 c的上下文里出现了多少次 这取决于你对上下文词语和目标词语的定义 你也可以说 X_ij就是 X_ji 实际上，如果你通过它们是否出现在彼此前后10个单词以内 来定义上下文和目标词语的话 这就是一个对称的关系 尽管来说，如果你对上下文词语的选择 永远是目标词语前面紧邻的那一个 那么 X_ij和 X_ji就不会这样对称了 但在GloVe算法里 我们可以定义上下文和目标词语为 彼此出现在对方附近 例如彼此前后10个单词的范围内 因此， X_ij表示了词语 i和 j同时出现 或者说彼此接近的频繁程度 因此，GloVe算法所做的实际上是 如下的优化问题 我们将最小化 theta_i转置乘以 e_j 和 X_ij对数之间差的平方 一会儿我将填充这些部分 记住，把 i和 j想象成之前的 t和 c 因此，这就和之前看到的 theta_t转置乘以 e_c类似 你的目标，就是让这个表达式告诉你这两个词关联有多大 单词 t和 c关联有多大 或者单词 i和 j关联有多大，即它们一起出现的频率有多大 而这受到 X_ij的影响 因此，我们要做的就是 利用梯度下降法解出参数 theta和 e，从而最小化 这个差值 i从1到10,000、j从1到10,000的和 因此，你就是要训练这些向量 使得它们的内积能够很好地预测两个词语同时出现的频率 现在，再说一点额外的细节 假如 X_ij等于0 0取对数是没有定义的，或者说是负无穷 因此，我们要做的 我们对那些 X_ij为0的项取合 因此，我们要做的 是加上额外的权重项 因此这将是一个权重项 当 X_ij等于0时，权重项也为0 我们将采用 0乘以 0的对数等于 0的约定 因此，这就意味着 假如 X_ij等于0 就不要理会那对 X_ij再取合 所以这个 0的对数项就变得无关紧要了 因此这意味着和运算只在 那些至少作为一对上下文-目标同时出现的词语上进行 F( X_ij)实现的另一件事是 有些词在英语中出现十分频繁 例如“这”、“是”、“的”、“一个“等等 有时我们将这些词称为停用词 但实际上常用词和罕见词之间并没有绝对的界限 另外也有一些罕见词，例如“榴莲” 你实际上是想把它们考虑进来的 但不像一些更加常用词那样频繁 因此，权重因子可以作为一个函数 即使对“榴莲”这样不太常见的词 也给出一个有意义的权重 而对“这”、“是”、“的”、“一个”这样特别常见的词语 给出更大、但也并不过度的权重 因此，有许多启发式方法来选择这个权重函数 F 使得它既不给这些词语过高的权重 也不给罕见词过低的权重 如果你想知道启发式选取 F的细节 你可以去读一读上一页幻灯片中引用的 GloVe论文 接下来，关于这个算法最后一个有趣的事情是 theta和 e的作用是完全对称的 因此， theta_i和 e_j是对称的 就是说从数学的角度 它们的作用是一样的，
你可以把它们反过来或者对调它们 实际上得到的将是相同的目标优化函数 因此训练这个算法的一种方式是随机均匀地 初始化 theta和 e，运用梯度下降法来最小化目标函数 然后当你对每个单词完成这个过程后 取平均值 因此对给定的单词 w 最终的 e等于 通过这个梯度下降法得到的嵌入 e，加上 通过梯度下降法得到的 theta，再除以2 因为在这个式子中，theta和 e扮演对称的角色 而不像之前视频中看到的模型那样 theta和 e的作用不一样，
不能只是这样取平均值 好了，这就是GloVe算法 我认为这个算法中一个让人不解的地方在于 如果你看看这个等式，它看起来太简单了 怎么可能仅仅靠最小化一个 平方损失函数就训练出一个有效的词嵌入呢？ 但实际上，这是可行的 这个算法的发明者们最终能得到这种算法 是基于一系列更加复杂的算法 例如神经语言模型 后来的Word2Vec Skip-Gram模型 再后来才有了这种算法 我们确实希望能简化所有之前的模型 在结束对于词嵌入算法的讨论前 我们应该讨论一下它们的某个属性 是什么呢？我们从这个特征化的视角出发 作为我们训练词向量的动机 我们可以说：“或许嵌入向量的第一个元素代表了性别” “第二个元素代表了皇室属性” “然后是年龄，是否属于食物，等等” 但是当你用我们已经见过的某个算法
例如上一张幻灯片中的GloVe算法 来训练一个词嵌入的时候 常出现的情况是，你无法确保 嵌入向量的单个元素是可以解释的 为什么呢？我们假设有一个空间 第一个坐标轴是性别，第二个是皇室属性 你无法确保嵌入向量的坐标 刚好和这些“性别”、“皇室属性”、
“年龄”、“食物”坐标对齐 尤其是 学习算法可能把这个选作第一维坐标轴 因此，给定一段文字 或许第一维可能是这个坐标轴，
第二维或许是这个 或许他们根本就不正交 或许还会有第二个不正交的坐标轴 代表着你训练所得词嵌入的第二个元素 如果你对线性代数有一定了解 理解这个的一种方法就是 如果有一个可逆矩阵 A 那么这个部分可以轻易地替换为 A乘以 theta_i，转置，乘以 A逆转置再乘以 e_j 因为如果我们把这个展开 这就等于 theta_i转置乘以 A转置乘以 A逆转置再乘以 e_j 因此中间的项消去了 剩下 theta_i转置乘以 e_j，就和之前一样 如果你不太清楚线性代数，没关系 这只是一个简短的证明，
说明了用这样一种算法 你无法保证用来表示特征的那些坐标 可以轻易地和人类可以
容易理解的特征坐标联系起来 尤其是，第一个特征可能是包括了性别、 皇室属性、年龄、食物、 价格、尺寸、 是名词还是动词 等等这些特征的一个组合 因此要给单个元素、嵌入矩阵的单个行 一个人性化解释是非常困难的 但尽管有这种线性变换 我们在描述类比的时候用到的 平行四边形图，仍然是有效的 因此，尽管有这种特征间可能的任意的线性变换 表征类比的平行四边形图仍然是有效的 好了，这就是训练词嵌入的全部内容了 你现在已经见识了许多训练词嵌入的算法了 在这周的编程练习中你将更多地接触它们 接下来，我想向你们展示 如何用这些算法进行情绪分类 我们下一节再见