1
00:00:00,400 --> 00:00:01,220
上个视频中

2
00:00:01,220 --> 00:00:06,400
你看到了如何利用Skip-Gram模型构建一个监督学习任务

3
00:00:06,400 --> 00:00:09,030
使得我们建立从上下文到目标词语的映射

4
00:00:09,030 --> 00:00:12,330
并基于此训练出一个有用的词嵌入

5
00:00:12,330 --> 00:00:16,890
但是这种方法的弊端在于Softmax函数训练起来比较慢

6
00:00:16,890 --> 00:00:21,338
在这个视频中，你将看到一种改进的学习方法，
叫做“负采样法”

7
00:00:21,338 --> 00:00:25,913
它使你达到和刚才看到的Skip-Gram模型类似的结果

8
00:00:25,913 --> 00:00:28,570
但采用一种更加有效的学习算法

9
00:00:28,570 --> 00:00:31,138
让我们看看如何做到这一点

10
00:00:31,138 --> 00:00:35,665
这段视频中介绍的想法大多数基于
Tomas Mikolov, Ilya Sutskever

11
00:00:35,665 --> 00:00:37,911
Kai Chen, Greg Corrado, 以及Jeff Dean的成果

12
00:00:37,911 --> 00:00:40,761
在这个算法中，我们要做的就是

13
00:00:40,761 --> 00:00:43,540
建立一个新的监督学习问题

14
00:00:43,540 --> 00:00:49,720
这个问题就是，给定一对词语，
例如“橙子"和"果汁"

15
00:00:51,160 --> 00:00:56,030
我们要预测这是否是一对上下文和目标词语

16
00:00:56,030 --> 00:01:00,290
在这个例子中，“橙子果汁”是一个正样本

17
00:01:00,290 --> 00:01:04,255
那么“橙子”和“国王”呢？

18
00:01:04,255 --> 00:01:09,350
对，这是一个负样本，
因此我们在目标这一栏写上0

19
00:01:09,350 --> 00:01:13,570
那么我们要做的就是采样一个上下文

20
00:01:13,570 --> 00:01:14,810
以及一个目标词语

21
00:01:14,810 --> 00:01:17,410
在这个例子中，我们采样的是“橙子”和“果汁”

22
00:01:17,410 --> 00:01:23,310
并且我们给这对词语标记为1
我们把中间这一栏记为“词语”

23
00:01:23,310 --> 00:01:27,270
那么，用和我们之前视频中一样的方法

24
00:01:27,270 --> 00:01:31,620
生成一个正样本之后

25
00:01:31,620 --> 00:01:35,762
即采样一个上下文词语，然后在附近的
一个窗口，例如前后10个词语

26
00:01:35,762 --> 00:01:36,845
选取一个目标词语

27
00:01:36,845 --> 00:01:41,590
那么这就是你生成这个表格第一行的方法

28
00:01:41,590 --> 00:01:45,456
接下来，要生成一个负样本
你首先选取和之前一样的上下文词语

29
00:01:45,456 --> 00:01:48,102
然后从字典中随机抽取一个词语

30
00:01:48,102 --> 00:01:54,260
在这个例子中，我随机选取了“国王”这个词语
并且将这对词语标记为0

31
00:01:54,260 --> 00:01:58,186
然后，我们仍然选取“橙子”
并且从字典中再随机抽取一个词语

32
00:01:58,186 --> 00:02:01,484
我们假设了
如果我们确实是随机抽取了一个词语

33
00:02:01,484 --> 00:02:07,245
它很大可能不会和“橙子”这个词语相关
那么记为“橙子”、“书”、0

34
00:02:07,245 --> 00:02:11,731
让我们再选取一些其他的，首先“橙子”，或许因为偶然

35
00:02:11,731 --> 00:02:14,335
你选到了“这”这个词，记为0

36
00:02:14,335 --> 00:02:18,945
再来，“橙子”，或许再次偶然地，我们选到了“的”

37
00:02:18,945 --> 00:02:20,440
我们标记为0

38
00:02:20,440 --> 00:02:25,645
注意到，尽管“的”被标记为0

39
00:02:25,645 --> 00:02:30,538
但“的”实际上也会出现在“橙子”的后面

40
00:02:30,538 --> 00:02:37,421
总结下来，我们生成这个数据集的方法是
我们选择一个上下文词语

41
00:02:37,421 --> 00:02:42,408
然后选取一个目标词语
这就是这个表格的第一行

42
00:02:42,408 --> 00:02:45,161
这就给我们一个正样本

43
00:02:45,161 --> 00:02:50,670
那么上下文、目标，然后标记为1

44
00:02:50,670 --> 00:02:55,466
然后我们要做的是
对于某个次数，k

45
00:02:55,466 --> 00:02:59,976
我们选取同样的上下文词语，
然后从字典中随机抽取词语

46
00:02:59,976 --> 00:03:04,630
“国王”、“书”、“这”、“的”，
只要是从字典中随机抽取的就行

47
00:03:04,630 --> 00:03:08,380
把这些全部标记为0，这样就得到了我们的负样本

48
00:03:10,450 --> 00:03:15,284
即使我们从字典中随机抽取的某个词

49
00:03:15,284 --> 00:03:18,431
恰好出现在上下文词语附近的一个窗口

50
00:03:18,431 --> 00:03:23,210
前后十个词语内，例如紧接着上下文词语“橙汁”
也是没关系的

51
00:03:23,210 --> 00:03:27,950
接下来，我们要设计一个监督学习问题

52
00:03:27,950 --> 00:03:32,687
在这个问题中，这个学习算法输入为x，输入这对词语

53
00:03:32,687 --> 00:03:37,440
然后它通过预测这个标记，来预测输出y

54
00:03:38,740 --> 00:03:42,461
那么这个问题就是，给定一对词语，
例如“橙子”和“果汁”

55
00:03:42,461 --> 00:03:44,430
你认为它们出现在一起吗？

56
00:03:44,430 --> 00:03:49,229
你认为要得到这对词语，
我是通过采样两个彼此接近的词语？

57
00:03:49,229 --> 00:03:52,515
还是通过从文本中选取一个词

58
00:03:52,515 --> 00:03:55,585
然后从字典中随机再选取一个词？

59
00:03:55,585 --> 00:04:00,182
实际上就是要努力区分这两种

60
00:04:00,182 --> 00:04:02,420
生成一对词语的分布

61
00:04:03,600 --> 00:04:06,130
这就是你生成训练集的方法

62
00:04:07,240 --> 00:04:10,690
至于如何选取k，Mikolov等人

63
00:04:10,690 --> 00:04:16,330
建议对于小的数据集，或许k介于5到20

64
00:04:16,330 --> 00:04:20,950
如果你有一个很大的数据集，
那么选取较小的k

65
00:04:20,950 --> 00:04:27,009
也就是，对于大的数据集，k介于2到5

66
00:04:27,009 --> 00:04:33,620
对于小的数据集选取更大的k值

67
00:04:33,620 --> 00:04:37,194
好了，在这个例子中，我们选取k=4

68
00:04:38,500 --> 00:04:42,430
接下来我们要描述这个监督学习算法

69
00:04:42,430 --> 00:04:44,524
从x映射到y

70
00:04:44,524 --> 00:04:49,630
这是我们从之前视频中学习的Softmax模型

71
00:04:49,630 --> 00:04:54,156
这是我们从前一张幻灯片中得到的训练集

72
00:04:54,156 --> 00:04:56,267
再强调一下，这将会是新的输入x

73
00:04:56,267 --> 00:04:59,910
这将是你想要预测的y值

74
00:04:59,910 --> 00:05:04,696
那么要定义这个模型，我将用c来表示

75
00:05:04,696 --> 00:05:09,079
上下文词语，用t来表示可能的目标词语

76
00:05:09,079 --> 00:05:14,925
用y来表示0或1，这就是一对上下文-目标词语

77
00:05:14,925 --> 00:05:19,015
接下来我们要做的是定义一个逻辑回归模型

78
00:05:19,015 --> 00:05:24,292
例如，在给定c、t的条件下，y=1的概率，

79
00:05:24,292 --> 00:05:29,677
我们要把这个建模为一个逻辑回归模型

80
00:05:29,677 --> 00:05:34,429
但具体我们采用的公式是Sigmoid函数，应用在

81
00:05:34,429 --> 00:05:38,387
theta_t，转置，乘以e_c

82
00:05:38,387 --> 00:05:41,788
这些参数和以前差不多

83
00:05:41,788 --> 00:05:47,341
对每一个可能的目标词语，
你有一个参数向量，theta

84
00:05:47,341 --> 00:05:52,242
以及对于每一个上下文词语，另一个参数向量

85
00:05:52,242 --> 00:05:54,390
实际上也就是词嵌入向量

86
00:05:54,390 --> 00:05:59,308
我们将用这个公式来估计

87
00:05:59,308 --> 00:06:00,506
y=1的概率

88
00:06:00,506 --> 00:06:07,357
假设你有k个样本，
那么你可以看成是有

89
00:06:07,357 --> 00:06:12,480
一个k1的负样本-正样本比率

90
00:06:12,480 --> 00:06:16,160
对每一个正样本，你有

91
00:06:16,160 --> 00:06:20,920
k个负样本来训练这个类似逻辑回归的模型

92
00:06:21,970 --> 00:06:28,525
那么把这个转换成一个神经网络，
如果输入词语是“橙子”

93
00:06:31,120 --> 00:06:35,017
也就是第6257个词语，你要做的是

94
00:06:35,017 --> 00:06:39,121
输入这个one-hot向量

95
00:06:39,121 --> 00:06:44,470
传递给e，利用向量乘法，得到嵌入向量，e_6275

96
00:06:44,470 --> 00:06:49,217
那么你实际上得到了10,000个可能的

97
00:06:49,217 --> 00:06:53,950
逻辑回归分类问题

98
00:06:53,950 --> 00:06:59,930
其中一个就是对应于

99
00:06:59,930 --> 00:07:04,120
目标词语是否是“果汁”的分类器

100
00:07:04,120 --> 00:07:06,934
接下来还会有其他的词语，例如

101
00:07:06,934 --> 00:07:11,425
可能在这里某个地方，
你要预测这个词语是不是“国王”

102
00:07:11,425 --> 00:07:15,460
这些都是你字典中的词语

103
00:07:15,460 --> 00:07:20,522
那么，想象这是有10,000个二进制逻辑回归分类器

104
00:07:20,522 --> 00:07:24,787
但是，并非要在每次迭代中
训练全部10,000个分类器

105
00:07:24,787 --> 00:07:27,270
我们只训练其中的5个

106
00:07:27,270 --> 00:07:32,760
也就是对应于实际的目标词语的那个

107
00:07:32,760 --> 00:07:36,813
以及对应于其他4个
随机抽取的负样本的分类器

108
00:07:36,813 --> 00:07:39,660
这就是k等于4时的情形

109
00:07:39,660 --> 00:07:44,835
那么，不同于训练一个非常难于训练的

110
00:07:44,835 --> 00:07:50,115
巨大的具有10,000个可能结果的Softmax模型，
我们将它转换为

111
00:07:50,115 --> 00:07:55,185
10,000个二元分类问题

112
00:07:55,185 --> 00:07:58,787
它们每一个计算起来都特别容易

113
00:07:58,787 --> 00:08:03,778
并且在每次迭代中，我们只训练其中的5个，
或者更一般地说

114
00:08:03,778 --> 00:08:08,290
其中的k+1个，k个负样本，1个正样本

115
00:08:08,290 --> 00:08:12,812
这就是为什么这个算法的计算成本显著更低

116
00:08:12,812 --> 00:08:15,825
因为你在训练k+1个逻辑回归模型

117
00:08:15,825 --> 00:08:18,364
或者说k+1个二元分类问题

118
00:08:18,364 --> 00:08:22,836
它们在每次迭代中计算起来都相对容易

119
00:08:22,836 --> 00:08:26,539
而不是训练一个具有10,000个
可能结果的Softmax分类器

120
00:08:26,539 --> 00:08:30,643
在这周的编程练习中，你将有机会

121
00:08:30,643 --> 00:08:32,720
来熟悉这种算法

122
00:08:32,720 --> 00:08:37,327
这种技巧叫做负采样法，因为你做的正是

123
00:08:37,327 --> 00:08:40,690
得到一个正样本，”橙子“，然后”果汁“

124
00:08:40,690 --> 00:08:45,447
然后你会故意地生成一些负样本

125
00:08:45,447 --> 00:08:49,180
这就是为什么它叫做”负采样法“

126
00:08:49,180 --> 00:08:52,860
你将用这些负样本来训练4个二元分类器

127
00:08:54,940 --> 00:08:58,640
在每次迭代中，你选择4个不同的

128
00:08:58,640 --> 00:09:02,080
随机负样本，并用它们来训练你的算法

129
00:09:02,080 --> 00:09:06,090
现在，在我们总结之前，
对于这个算法很重要的一个细节就是

130
00:09:06,090 --> 00:09:09,170
你如何选取这些负样本？

131
00:09:09,170 --> 00:09:12,176
在选取上下文词语“橙子”后

132
00:09:12,176 --> 00:09:16,220
你要如何抽取这些词语来生成负样本呢？

133
00:09:18,090 --> 00:09:23,480
一个可能的做法是，抽取中间的词语

134
00:09:23,480 --> 00:09:26,236
作为候选目标词语

135
00:09:26,236 --> 00:09:30,686
一个可能的做法是，根据你的文本中

136
00:09:30,686 --> 00:09:32,064
词语的经验频率分布来抽样

137
00:09:32,064 --> 00:09:36,810
也就是根据词语出现的频率来抽样

138
00:09:36,810 --> 00:09:40,845
这么做的问题在于，你会得到很多例如

139
00:09:40,845 --> 00:09:44,855
“这”、“的”、“和”之类的词语

140
00:09:44,855 --> 00:09:48,677
另一个极端就是，你根据字典词汇量的倒数

141
00:09:48,677 --> 00:09:51,943
均匀地抽取负样本，但是这么做

142
00:09:51,943 --> 00:09:56,620
也很不能够代表英语中单词的分布

143
00:09:56,620 --> 00:10:01,738
因此，作者Mikolov等人表示，
根据经验，他们认为最好的方法是

144
00:10:01,738 --> 00:10:06,424
采用这种启发式的值，也就是介于

145
00:10:06,424 --> 00:10:11,401
从经验频率分布中采样，即

146
00:10:11,401 --> 00:10:15,910
观察到的英语文本中的分布，
到从均匀分布中采样

147
00:10:15,910 --> 00:10:20,516
他们所做的，就是依词频的3/4次幂

148
00:10:20,516 --> 00:10:24,830
来抽样

149
00:10:24,830 --> 00:10:30,542
那么例如f(w_i)表示某个观察到的
英语中的词频

150
00:10:30,542 --> 00:10:36,338
或者说你训练集中的词频，
那么取它的3/4次幂

151
00:10:36,338 --> 00:10:41,812
这样它就介于取均匀分布

152
00:10:41,812 --> 00:10:45,821
和取经验分布的

153
00:10:45,821 --> 00:10:47,020
两个极端之间

154
00:10:48,670 --> 00:10:52,598
我不确定这是否在理论上站得住脚，但是

155
00:10:52,598 --> 00:10:56,950
许多研究者现在正在用这种启发式方法，
并且似乎效果非常不错

156
00:10:56,950 --> 00:11:01,145
那么，总结一下，
你学到了如何用Softmax分类器
来训练词向量

157
00:11:01,145 --> 00:11:03,406
但这样计算上成本很高

158
00:11:03,406 --> 00:11:06,953
在这个视频中，你学到了如何把它变为一系列

159
00:11:06,953 --> 00:11:11,105
二元分类问题，并且非常高效地训练词向量

160
00:11:11,105 --> 00:11:15,750
如果你运行这种算法，你会得到非常好的词向量

161
00:11:15,750 --> 00:11:19,031
当然，就像深度学习中的其他领域一样

162
00:11:19,031 --> 00:11:21,024
有开源的实现

163
00:11:21,024 --> 00:11:24,886
也有预训练过的词向量，
这些都是其他人训练过，

164
00:11:24,886 --> 00:11:27,320
并且通过许可证的方式分享到网上的

165
00:11:27,320 --> 00:11:32,280
因此如果你想快速上手一个自然语言处理问题，

166
00:11:32,280 --> 00:11:37,740
合理的做法是下载其他人的词向量，
并以此作为出发点

167
00:11:37,740 --> 00:11:41,371
这样我们就讲完了Skip-Gram模型

168
00:11:41,371 --> 00:11:44,648
在下一个视频中，我将与你们分享另外一种

169
00:11:44,648 --> 00:11:49,360
可能比你现在看到的更加简单的词嵌入算法

170
00:11:49,360 --> 00:11:52,430
在下个视频中，我们将学习GloVe算法