1
00:00:00,000 --> 00:00:04,337
你已经学到了计算词嵌入的几种算法

2
00:00:04,337 --> 00:00:10,615
另一种在自然语言处理中
具有一定势头的算法是GloVe算法

3
00:00:10,615 --> 00:00:14,520
它并不像Word2Vec或者Skip-Gram算法那样被广泛使用

4
00:00:14,520 --> 00:00:16,884
但它也有一定拥趸

5
00:00:16,884 --> 00:00:19,865
在我看来，是因为它的简洁性

6
00:00:19,865 --> 00:00:21,133
让我们一起来看看

7
00:00:21,133 --> 00:00:24,085
GloVe算法是由Jeffrey Pennington,

8
00:00:24,085 --> 00:00:25,720
Richard Socher和Chris Manning发明的

9
00:00:25,720 --> 00:00:30,570
GloVe的意思是用于词汇表征的全局矢量

10
00:00:30,570 --> 00:00:33,630
在前面，我们成对抽样词语

11
00:00:33,630 --> 00:00:35,625
即上下文词语和目标词语对的时候

12
00:00:35,625 --> 00:00:42,180
采用的方法是选取两个在我们的文本中距离相近的词语

13
00:00:42,180 --> 00:00:43,380
而GloVe算法所做的是

14
00:00:43,380 --> 00:00:46,721
让这个过程更加明确

15
00:00:46,721 --> 00:00:52,515
假设 X_ij表示次数就是i

16
00:00:52,515 --> 00:01:01,510
在 j的上下文中出现的次数

17
00:01:01,510 --> 00:01:09,095
因此，这里的 i和 j扮演了 t和 c的角色

18
00:01:09,095 --> 00:01:13,970
你可以把 X_ij想象成 X_tc

19
00:01:13,970 --> 00:01:17,420
但是，你可以遍历你的训练文本集，并且数出

20
00:01:17,420 --> 00:01:22,330
词语 i在另一个词语 j的上下文中出现了多少次

21
00:01:22,330 --> 00:01:27,170
或者说词语 t在另一个词语 c的上下文里出现了多少次

22
00:01:27,170 --> 00:01:33,100
这取决于你对上下文词语和目标词语的定义

23
00:01:33,100 --> 00:01:37,240
你也可以说 X_ij就是 X_ji

24
00:01:37,240 --> 00:01:40,940
实际上，如果你通过它们是否出现在彼此前后10个单词以内

25
00:01:40,940 --> 00:01:44,600
来定义上下文和目标词语的话

26
00:01:44,600 --> 00:01:47,225
这就是一个对称的关系

27
00:01:47,225 --> 00:01:50,880
尽管来说，如果你对上下文词语的选择

28
00:01:50,880 --> 00:01:54,795
永远是目标词语前面紧邻的那一个

29
00:01:54,795 --> 00:01:59,595
那么 X_ij和 X_ji就不会这样对称了

30
00:01:59,595 --> 00:02:02,745
但在GloVe算法里

31
00:02:02,745 --> 00:02:05,630
我们可以定义上下文和目标词语为

32
00:02:05,630 --> 00:02:09,155
彼此出现在对方附近

33
00:02:09,155 --> 00:02:13,755
例如彼此前后10个单词的范围内

34
00:02:13,755 --> 00:02:22,750
因此， X_ij表示了词语 i和 j同时出现

35
00:02:22,750 --> 00:02:24,650
或者说彼此接近的频繁程度

36
00:02:24,650 --> 00:02:26,480
因此，GloVe算法所做的实际上是

37
00:02:26,480 --> 00:02:28,616
如下的优化问题

38
00:02:28,616 --> 00:02:36,745
我们将最小化 theta_i转置乘以 e_j

39
00:02:36,745 --> 00:02:45,565
和 X_ij对数之间差的平方

40
00:02:45,565 --> 00:02:48,488
一会儿我将填充这些部分

41
00:02:48,488 --> 00:02:54,300
记住，把 i和 j想象成之前的 t和 c

42
00:02:54,300 --> 00:03:00,428
因此，这就和之前看到的 theta_t转置乘以 e_c类似

43
00:03:00,428 --> 00:03:05,125
你的目标，就是让这个表达式告诉你这两个词关联有多大

44
00:03:05,125 --> 00:03:07,135
单词 t和 c关联有多大

45
00:03:07,135 --> 00:03:12,795
或者单词 i和 j关联有多大，即它们一起出现的频率有多大

46
00:03:12,795 --> 00:03:17,635
而这受到 X_ij的影响

47
00:03:17,635 --> 00:03:20,775
因此，我们要做的就是

48
00:03:20,775 --> 00:03:28,320
利用梯度下降法解出参数 theta和 e，从而最小化

49
00:03:28,320 --> 00:03:36,500
这个差值 i从1到10,000、j从1到10,000的和

50
00:03:36,500 --> 00:03:38,340
因此，你就是要训练这些向量

51
00:03:38,340 --> 00:03:44,565
使得它们的内积能够很好地预测两个词语同时出现的频率

52
00:03:44,565 --> 00:03:47,386
现在，再说一点额外的细节

53
00:03:47,386 --> 00:03:49,556
假如 X_ij等于0

54
00:03:49,556 --> 00:03:52,505
0取对数是没有定义的，或者说是负无穷

55
00:03:52,505 --> 00:03:53,740
因此，我们要做的

56
00:03:53,740 --> 00:03:59,050
我们对那些 X_ij为0的项取合

57
00:03:59,050 --> 00:04:00,910
因此，我们要做的

58
00:04:00,910 --> 00:04:03,590
是加上额外的权重项

59
00:04:03,590 --> 00:04:07,895
因此这将是一个权重项

60
00:04:07,895 --> 00:04:17,925
当 X_ij等于0时，权重项也为0

61
00:04:17,925 --> 00:04:26,135
我们将采用 0乘以 0的对数等于 0的约定

62
00:04:26,135 --> 00:04:27,152
因此，这就意味着

63
00:04:27,152 --> 00:04:29,195
假如 X_ij等于0

64
00:04:29,195 --> 00:04:32,433
就不要理会那对 X_ij再取合

65
00:04:32,433 --> 00:04:35,315
所以这个 0的对数项就变得无关紧要了

66
00:04:35,315 --> 00:04:40,190
因此这意味着和运算只在

67
00:04:40,190 --> 00:04:46,220
那些至少作为一对上下文-目标同时出现的词语上进行

68
00:04:46,220 --> 00:04:48,650
F( X_ij)实现的另一件事是

69
00:04:48,650 --> 00:04:52,350
有些词在英语中出现十分频繁

70
00:04:52,350 --> 00:04:55,005
例如“这”、“是”、“的”、“一个“等等

71
00:04:55,005 --> 00:04:57,110
有时我们将这些词称为停用词

72
00:04:57,110 --> 00:04:59,810
但实际上常用词和罕见词之间并没有绝对的界限

73
00:04:59,810 --> 00:05:02,665
另外也有一些罕见词，例如“榴莲”

74
00:05:02,665 --> 00:05:04,460
你实际上是想把它们考虑进来的

75
00:05:04,460 --> 00:05:09,480
但不像一些更加常用词那样频繁

76
00:05:09,480 --> 00:05:12,620
因此，权重因子可以作为一个函数

77
00:05:12,620 --> 00:05:16,976
即使对“榴莲”这样不太常见的词

78
00:05:16,976 --> 00:05:20,090
也给出一个有意义的权重

79
00:05:20,090 --> 00:05:25,128
而对“这”、“是”、“的”、“一个”这样特别常见的词语

80
00:05:25,128 --> 00:05:28,745
给出更大、但也并不过度的权重

81
00:05:28,745 --> 00:05:33,960
因此，有许多启发式方法来选择这个权重函数 F

82
00:05:33,960 --> 00:05:37,110
使得它既不给这些词语过高的权重

83
00:05:37,110 --> 00:05:41,020
也不给罕见词过低的权重

84
00:05:41,020 --> 00:05:43,725
如果你想知道启发式选取 F的细节

85
00:05:43,725 --> 00:05:45,141
你可以去读一读上一页幻灯片中引用的

86
00:05:45,141 --> 00:05:51,720
GloVe论文

87
00:05:51,720 --> 00:05:56,070
接下来，关于这个算法最后一个有趣的事情是

88
00:05:56,070 --> 00:06:01,215
theta和 e的作用是完全对称的

89
00:06:01,215 --> 00:06:07,595
因此， theta_i和 e_j是对称的

90
00:06:07,595 --> 00:06:08,912
就是说从数学的角度

91
00:06:08,912 --> 00:06:12,403
它们的作用是一样的，
你可以把它们反过来或者对调它们

92
00:06:12,403 --> 00:06:17,293
实际上得到的将是相同的目标优化函数

93
00:06:17,293 --> 00:06:21,571
因此训练这个算法的一种方式是随机均匀地

94
00:06:21,571 --> 00:06:26,180
初始化 theta和 e，运用梯度下降法来最小化目标函数

95
00:06:26,180 --> 00:06:30,567
然后当你对每个单词完成这个过程后

96
00:06:30,567 --> 00:06:31,660
取平均值

97
00:06:31,660 --> 00:06:33,580
因此对给定的单词 w

98
00:06:33,580 --> 00:06:36,910
最终的 e等于

99
00:06:36,910 --> 00:06:41,410
通过这个梯度下降法得到的嵌入 e，加上

100
00:06:41,410 --> 00:06:46,999
通过梯度下降法得到的 theta，再除以2

101
00:06:46,999 --> 00:06:50,770
因为在这个式子中，theta和 e扮演对称的角色

102
00:06:50,770 --> 00:06:54,910
而不像之前视频中看到的模型那样

103
00:06:54,910 --> 00:06:59,890
theta和 e的作用不一样，
不能只是这样取平均值

104
00:06:59,890 --> 00:07:02,320
好了，这就是GloVe算法

105
00:07:02,320 --> 00:07:04,450
我认为这个算法中一个让人不解的地方在于

106
00:07:04,450 --> 00:07:07,073
如果你看看这个等式，它看起来太简单了

107
00:07:07,073 --> 00:07:09,040
怎么可能仅仅靠最小化一个

108
00:07:09,040 --> 00:07:12,767
平方损失函数就训练出一个有效的词嵌入呢？

109
00:07:12,767 --> 00:07:15,675
但实际上，这是可行的

110
00:07:15,675 --> 00:07:18,480
这个算法的发明者们最终能得到这种算法

111
00:07:18,480 --> 00:07:19,865
是基于一系列更加复杂的算法

112
00:07:19,865 --> 00:07:23,643
例如神经语言模型

113
00:07:23,643 --> 00:07:28,340
后来的Word2Vec Skip-Gram模型

114
00:07:28,340 --> 00:07:30,625
再后来才有了这种算法

115
00:07:30,625 --> 00:07:34,960
我们确实希望能简化所有之前的模型

116
00:07:34,960 --> 00:07:41,200
在结束对于词嵌入算法的讨论前

117
00:07:41,200 --> 00:07:47,215
我们应该讨论一下它们的某个属性

118
00:07:47,215 --> 00:07:50,935
是什么呢？我们从这个特征化的视角出发

119
00:07:50,935 --> 00:07:54,633
作为我们训练词向量的动机

120
00:07:54,633 --> 00:07:58,803
我们可以说：“或许嵌入向量的第一个元素代表了性别”

121
00:07:58,803 --> 00:08:02,147
“第二个元素代表了皇室属性”

122
00:08:02,147 --> 00:08:06,175
“然后是年龄，是否属于食物，等等”

123
00:08:06,175 --> 00:08:10,780
但是当你用我们已经见过的某个算法
例如上一张幻灯片中的GloVe算法

124
00:08:10,780 --> 00:08:15,580
来训练一个词嵌入的时候

125
00:08:15,580 --> 00:08:18,910
常出现的情况是，你无法确保

126
00:08:18,910 --> 00:08:22,933
嵌入向量的单个元素是可以解释的

127
00:08:22,933 --> 00:08:27,190
为什么呢？我们假设有一个空间

128
00:08:27,190 --> 00:08:32,188
第一个坐标轴是性别，第二个是皇室属性

129
00:08:32,188 --> 00:08:37,545
你无法确保嵌入向量的坐标

130
00:08:37,545 --> 00:08:43,210
刚好和这些“性别”、“皇室属性”、
“年龄”、“食物”坐标对齐

131
00:08:43,210 --> 00:08:46,480
尤其是

132
00:08:46,480 --> 00:08:51,850
学习算法可能把这个选作第一维坐标轴

133
00:08:51,850 --> 00:08:54,343
因此，给定一段文字

134
00:08:54,343 --> 00:08:58,993
或许第一维可能是这个坐标轴，
第二维或许是这个

135
00:08:58,993 --> 00:09:00,895
或许他们根本就不正交

136
00:09:00,895 --> 00:09:05,420
或许还会有第二个不正交的坐标轴

137
00:09:05,420 --> 00:09:09,230
代表着你训练所得词嵌入的第二个元素

138
00:09:09,230 --> 00:09:10,530
如果你对线性代数有一定了解

139
00:09:10,530 --> 00:09:17,105
理解这个的一种方法就是

140
00:09:17,105 --> 00:09:19,850
如果有一个可逆矩阵 A

141
00:09:19,850 --> 00:09:26,330
那么这个部分可以轻易地替换为 A乘以

142
00:09:26,330 --> 00:09:35,604
theta_i，转置，乘以 A逆转置再乘以 e_j

143
00:09:35,604 --> 00:09:36,790
因为如果我们把这个展开

144
00:09:36,790 --> 00:09:44,008
这就等于 theta_i转置乘以 A转置乘以 A逆转置再乘以 e_j

145
00:09:44,008 --> 00:09:46,215
因此中间的项消去了

146
00:09:46,215 --> 00:09:49,530
剩下 theta_i转置乘以 e_j，就和之前一样

147
00:09:49,530 --> 00:09:51,993
如果你不太清楚线性代数，没关系

148
00:09:51,993 --> 00:09:56,540
这只是一个简短的证明，
说明了用这样一种算法

149
00:09:56,540 --> 00:10:00,965
你无法保证用来表示特征的那些坐标

150
00:10:00,965 --> 00:10:06,975
可以轻易地和人类可以
容易理解的特征坐标联系起来

151
00:10:06,975 --> 00:10:10,205
尤其是，第一个特征可能是包括了性别、

152
00:10:10,205 --> 00:10:11,565
皇室属性、年龄、食物、

153
00:10:11,565 --> 00:10:14,575
价格、尺寸、

154
00:10:14,575 --> 00:10:16,781
是名词还是动词

155
00:10:16,781 --> 00:10:17,975
等等这些特征的一个组合

156
00:10:17,975 --> 00:10:21,620
因此要给单个元素、嵌入矩阵的单个行

157
00:10:21,620 --> 00:10:27,670
一个人性化解释是非常困难的

158
00:10:27,670 --> 00:10:30,710
但尽管有这种线性变换

159
00:10:30,710 --> 00:10:33,620
我们在描述类比的时候用到的

160
00:10:33,620 --> 00:10:36,660
平行四边形图，仍然是有效的

161
00:10:36,660 --> 00:10:42,950
因此，尽管有这种特征间可能的任意的线性变换

162
00:10:42,950 --> 00:10:51,220
表征类比的平行四边形图仍然是有效的

163
00:10:51,220 --> 00:10:54,170
好了，这就是训练词嵌入的全部内容了

164
00:10:54,170 --> 00:10:57,860
你现在已经见识了许多训练词嵌入的算法了

165
00:10:57,860 --> 00:11:01,802
在这周的编程练习中你将更多地接触它们

166
00:11:01,802 --> 00:11:03,680
接下来，我想向你们展示

167
00:11:03,680 --> 00:11:06,640
如何用这些算法进行情绪分类

168
00:11:06,640 --> 00:11:08,000
我们下一节再见