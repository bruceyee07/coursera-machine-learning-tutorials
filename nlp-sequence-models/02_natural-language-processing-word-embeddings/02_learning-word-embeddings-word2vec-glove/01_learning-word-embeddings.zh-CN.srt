1
00:00:00,000 --> 00:00:01,965
这节课中，你会开始学习

2
00:00:01,965 --> 00:00:04,440
一些具体的词嵌入学习算法

3
00:00:04,440 --> 00:00:09,645
在深度学习的发展历史中，学习词嵌入的过程也一样

4
00:00:09,645 --> 00:00:13,210
人们其实是从相对复杂的算法开始的

5
00:00:13,210 --> 00:00:14,490
随着时间的推移,

6
00:00:14,490 --> 00:00:16,200
研究人员发现他们可以使用

7
00:00:16,200 --> 00:00:18,540
更简单的算法，并且仍然

8
00:00:18,540 --> 00:00:21,755
可以得到很好的结果，尤其是对大数据集来说

9
00:00:21,755 --> 00:00:23,205
但事实是

10
00:00:23,205 --> 00:00:26,006
现在最流行的一些算法

11
00:00:26,006 --> 00:00:28,680
它们是如此的简单以至于如果我先提出来

12
00:00:28,680 --> 00:00:30,980
看起来似乎有点不可思议

13
00:00:30,980 --> 00:00:33,200
为什么这么简单的算法也可以发挥作用

14
00:00:33,200 --> 00:00:35,580
因此我将会从一些

15
00:00:35,580 --> 00:00:38,310
稍微更复杂的算法开始，因为我认为它们其实

16
00:00:38,310 --> 00:00:41,605
更容易直观地理解它们可以工作的原因

17
00:00:41,605 --> 00:00:44,820
然后我们会继续简化这些算法并展示

18
00:00:44,820 --> 00:00:48,630
一些虽然简单却可以获得很好结果的算法

19
00:00:48,630 --> 00:00:50,885
那我们开始吧。

20
00:00:50,885 --> 00:01:00,205
假设你正在建立

21
00:01:00,205 --> 00:01:04,205
一个语言模型并且你使用神经网络来建

22
00:01:04,205 --> 00:01:09,000
因此，在训练的过程中，你可能会想让你的神经网络做一些类似输入之类的工作

23
00:01:09,000 --> 00:01:10,330
我想要一杯橙汁

24
00:01:10,330 --> 00:01:13,330
然后预测序列中的下一个单词

25
00:01:13,330 --> 00:01:15,010
在每个单词下面

26
00:01:15,010 --> 00:01:21,435
我还在不同单词的词汇表上写下了索引

27
00:01:21,435 --> 00:01:22,975
所以事实上

28
00:01:22,975 --> 00:01:28,880
构建神经网络语言模型是学习嵌入集合小巧的方式

29
00:01:28,880 --> 00:01:31,743
我在这张幻灯片上提出的想法源于 Yoshua Bengio

30
00:01:31,743 --> 00:01:36,305
Rejean Ducharme, Pascals Vincent,和Christian Jauvin

31
00:01:36,305 --> 00:01:43,530
这也是如何通过建立一个神经网络来预测序列中的下一个词

32
00:01:43,530 --> 00:01:44,865
让我把这些单词列出来

33
00:01:44,865 --> 00:01:46,510
我想要一杯橙汁

34
00:01:46,510 --> 00:01:49,635
我们从第一个单词 I 开始

35
00:01:49,635 --> 00:01:53,625
所以我要构造一个对应于单词 I 的添加向量

36
00:01:53,625 --> 00:01:59,085
所以有一个添加向量与1在位置4343

37
00:01:59,085 --> 00:02:02,932
这将会是一个10,000维的向量

38
00:02:02,932 --> 00:02:07,525
我们接下来要做的是有一个参数矩阵 E

39
00:02:07,525 --> 00:02:15,580
然后用 E 和 O 相乘来得到一个嵌入向量 e4343

40
00:02:15,580 --> 00:02:17,425
这一步真正意味着

41
00:02:17,425 --> 00:02:25,025
e4343是由矩阵E和一个添加向量43相乘得到的

42
00:02:25,025 --> 00:02:28,420
然后,我们会对所有其他的单词进行同样的操作

43
00:02:28,420 --> 00:02:32,534
所以这个词想要的, 是9665，一个添加向量的地方

44
00:02:32,534 --> 00:02:35,445
乘以E来得到嵌入向量

45
00:02:35,445 --> 00:02:38,030
对于其他单词也类似

46
00:02:38,030 --> 00:02:39,650
A 是字典里的第一个单词

47
00:02:39,650 --> 00:02:43,690
在字母表中最先出现, 所以是O1，得到这个E1

48
00:02:43,690 --> 00:02:51,007
对于这个句子中的其他单词也类似

49
00:02:51,007 --> 00:02:54,400
现在你有了一些三维的嵌入

50
00:02:54,400 --> 00:03:00,274
所以每一个都是一个300维的嵌入向量

51
00:03:00,274 --> 00:03:01,360
然后我们能做的

52
00:03:01,360 --> 00:03:07,490
就是把这些全部填入一个神经网络,这里就是神经网络层

53
00:03:07,490 --> 00:03:12,920
然后把这个神经网络接到一个softmax层

54
00:03:12,920 --> 00:03:17,144
它同样也有自己的参数

55
00:03:17,144 --> 00:03:21,520
而且softmax会在词汇表1万种可能的输出种分类出

56
00:03:21,520 --> 00:03:26,235
那些我们试图预测出的最终单词

57
00:03:26,235 --> 00:03:29,525
因此，如果在训练过程中我们看到单词juice

58
00:03:29,525 --> 00:03:32,882
那么softmax重复培训的目标应该是预测

59
00:03:32,882 --> 00:03:36,600
紧接着的另一个单词就是juice

60
00:03:36,600 --> 00:03:39,340
所以这个隐含单词会有自己的参数

61
00:03:39,340 --> 00:03:43,108
所以我将用W1和B1来表示

62
00:03:43,108 --> 00:03:45,265
softmax自有的参数用W2和B2表示

63
00:03:45,265 --> 00:03:51,985
他们使用的是300维的词嵌入

64
00:03:51,985 --> 00:03:54,080
这里我们有六个词

65
00:03:54,080 --> 00:03:58,880
也就是6乘以300

66
00:03:58,880 --> 00:04:03,515
所以这层或者这个输入就会是1800维的向量

67
00:04:03,515 --> 00:04:09,915
由6个嵌入向量堆叠在一起得到

68
00:04:09,915 --> 00:04:15,110
实际上, 更常见的做法是拥有一个固定的历史窗口

69
00:04:15,110 --> 00:04:18,080
比如，你可能决定总是想预测

70
00:04:18,080 --> 00:04:21,635
下一个单词，譬如基于前四个单词

71
00:04:21,635 --> 00:04:24,220
这里四就是算法的超参数

72
00:04:24,220 --> 00:04:25,835
这就是你如何调整算法来应对

73
00:04:25,835 --> 00:04:28,640
很长或者很短的句子，或者你决定

74
00:04:28,640 --> 00:04:31,370
总是只看前四个单词

75
00:04:31,370 --> 00:04:34,550
所以你说, 我还是会用那四个单词

76
00:04:34,550 --> 00:04:37,850
所以, 我们先不看这些

77
00:04:37,850 --> 00:04:41,785
因此如果你总是使用包含四个单词的历史

78
00:04:41,785 --> 00:04:48,890
这意味着你的神经网络总是输入一个1200维的特征向量

79
00:04:48,890 --> 00:04:50,000
进入这一层

80
00:04:50,000 --> 00:04:52,925
然后有一个softmax来试图预测输出

81
00:04:52,925 --> 00:04:56,105
再次, 各种选择

82
00:04:56,105 --> 00:04:59,630
使用一个固定的历史，只意味着你可以处理任意

83
00:04:59,630 --> 00:05:04,745
长度的句子，因为输入大小总是固定的

84
00:05:04,745 --> 00:05:08,712
因此,这个模型的参数就是矩阵E

85
00:05:08,712 --> 00:05:11,775
然后对于所有的单词使用相同的矩阵E

86
00:05:11,775 --> 00:05:13,310
所以你没有

87
00:05:13,310 --> 00:05:16,640
前面四个单词不同位置的不同矩阵

88
00:05:16,640 --> 00:05:19,505
是相同的矩阵 E。然后

89
00:05:19,505 --> 00:05:23,270
这些权值就是算法的参数

90
00:05:23,270 --> 00:05:27,050
你可以使用这批单词来进行梯度计算

91
00:05:27,050 --> 00:05:30,200
使得你的训练集的出现可能性最大化

92
00:05:30,200 --> 00:05:34,620
来重复地基于序列中给定的四个单词进行预测

93
00:05:34,620 --> 00:05:38,180
你的文本语料库中的下一个单词是什么

94
00:05:38,180 --> 00:05:43,330
结果表明,这个算法我们将学习到相当恰当的词嵌入

95
00:05:43,330 --> 00:05:49,040
原因是, 如果你还记得我们的橙汁, 苹果汁的例子

96
00:05:49,040 --> 00:05:51,855
是在算法的激励下学习

97
00:05:51,855 --> 00:05:55,205
对于橙和苹果来说非常相似的词嵌入

98
00:05:55,205 --> 00:05:57,860
因为这样做可以使它更好地拟合

99
00:05:57,860 --> 00:06:01,290
训练集，因为偶尔会看到橙汁

100
00:06:01,290 --> 00:06:05,475
或者苹果汁, 所以

101
00:06:05,475 --> 00:06:10,020
如果你只有一个300维的特征向量来表示所有的单词

102
00:06:10,020 --> 00:06:13,290
算法会发现它可以很快地拟合训练集

103
00:06:13,290 --> 00:06:15,870
如果苹果,橙,葡萄和梨,

104
00:06:15,870 --> 00:06:18,455
等等并且甚至一种很少见的水果榴莲

105
00:06:18,455 --> 00:06:22,040
它们都有相似的特征向量

106
00:06:22,040 --> 00:06:25,455
所以这就是一种较早的相当成功的算法

107
00:06:25,455 --> 00:06:27,200
来学习词嵌入

108
00:06:27,200 --> 00:06:30,870
以及学习矩阵E。但现在让我们

109
00:06:30,870 --> 00:06:35,160
泛化这个算法来看看我们怎样得到更简单的算法

110
00:06:35,160 --> 00:06:37,380
所以, 我想说明其他算法

111
00:06:37,380 --> 00:06:41,455
用一个更复杂的句子作为我们的例子

112
00:06:41,455 --> 00:06:43,390
假设在你的训练集中

113
00:06:43,390 --> 00:06:44,910
你有这样一个更长的句子

114
00:06:44,910 --> 00:06:48,017
我想要一杯橙汁作为我的麦片伴侣

115
00:06:48,017 --> 00:06:51,150
在前一张幻灯片我们看到的是

116
00:06:51,150 --> 00:06:54,600
这个算法的工作是预测一些单词juice

117
00:06:54,600 --> 00:06:57,375
我们把它叫做目标词

118
00:06:57,375 --> 00:07:05,150
并且给定了一些上下文也就是最后四个单词

119
00:07:05,150 --> 00:07:07,995
因此如果你的目标是学习

120
00:07:07,995 --> 00:07:13,160
嵌入，对此我已经实验了许多不同类型的上下文的研究人员的嵌入

121
00:07:13,160 --> 00:07:16,050
如果我们要建立一个语言模型那么

122
00:07:16,050 --> 00:07:20,095
很自然地我们会用目标词前面的几个单词作为上下文

123
00:07:20,095 --> 00:07:23,175
但是如果你的目标是学习语言模型本身

124
00:07:23,175 --> 00:07:25,480
那么你可以选择其他的上下文

125
00:07:25,480 --> 00:07:27,990
例如,你可以构造一个学习问题

126
00:07:27,990 --> 00:07:31,500
其中上下文是左右两边的四个单词

127
00:07:31,500 --> 00:07:36,440
所以你就可以把左右四个单词作为上下文

128
00:07:36,440 --> 00:07:38,985
这也就意味着我们构造了一个学习问题

129
00:07:38,985 --> 00:07:42,530
会把左边的四个单词提供给算法

130
00:07:42,530 --> 00:07:44,670
也就是a glass of orange

131
00:07:44,670 --> 00:07:47,000
以及右边的四个单词

132
00:07:47,000 --> 00:07:49,440
也就是to go along with

133
00:07:49,440 --> 00:07:52,395
这里需要预测的是中间的单词

134
00:07:52,395 --> 00:07:56,210
构造一个像这样的学习算法,也就是你有

135
00:07:56,210 --> 00:08:00,720
左边四个单词以及右边四个单词的词嵌入输入到神经网络中

136
00:08:00,720 --> 00:08:03,810
和你在之前的幻灯片中看到的类似

137
00:08:03,810 --> 00:08:06,000
试图预测中间的单词

138
00:08:06,000 --> 00:08:07,995
试着把目标单词放在中间

139
00:08:07,995 --> 00:08:11,010
这也可以用来学习词嵌入

140
00:08:11,010 --> 00:08:13,320
或者你想用一个更简单的上下文

141
00:08:13,320 --> 00:08:15,320
也许你只用最后一个词

142
00:08:15,320 --> 00:08:17,760
那么只给你单词orange

143
00:08:17,760 --> 00:08:19,785
什么单词会接在orange之后呢

144
00:08:19,785 --> 00:08:23,425
所以这就会是不同的学习问题,你告诉它一个单词

145
00:08:23,425 --> 00:08:24,580
orange 然后说好吧

146
00:08:24,580 --> 00:08:26,350
你认为下一个单词是什么

147
00:08:26,350 --> 00:08:29,580
你可以建立一个神经网络只输入一个单词

148
00:08:29,580 --> 00:08:32,790
前一个字或嵌入

149
00:08:32,790 --> 00:08:36,555
到神经网络中，当你试图预测下一个单词时

150
00:08:36,555 --> 00:08:42,800
或者,一件令人惊讶的事情是只取相邻的一个单词

151
00:08:42,800 --> 00:08:44,820
有些可能会告诉你,那么好吧

152
00:08:44,820 --> 00:08:45,990
取单词glass

153
00:08:45,990 --> 00:08:47,220
邻近的一个单词

154
00:08:47,220 --> 00:08:48,690
有些可能会说,我看

155
00:08:48,690 --> 00:08:52,080
单词glass然后有另一个词靠近glass

156
00:08:52,080 --> 00:08:53,330
你认为这个词是什么

157
00:08:53,330 --> 00:08:57,135
那么将会使用相邻的一个词作为上下文

158
00:08:57,135 --> 00:09:03,775
我们会在下一个视频中正式说明这个，但这就是Skip-Gram模型的想法

159
00:09:03,775 --> 00:09:08,960
只是一个简单的算法的例子, 其中的上下文现在更简单

160
00:09:08,960 --> 00:09:11,850
只有一个单词而不是之前的四个

161
00:09:11,850 --> 00:09:13,821
但是这个效果也很好

162
00:09:13,821 --> 00:09:17,305
所以研究人员发现, 如果你真的想建立一个语言模型

163
00:09:17,305 --> 00:09:20,790
用最后几个单词作为上下文语境是很自然的

164
00:09:20,790 --> 00:09:25,425
但是, 如果你的主要目标是真正学习词嵌入

165
00:09:25,425 --> 00:09:28,350
那么你可以使用所有这些其他的上下文并且他们

166
00:09:28,350 --> 00:09:31,710
也会产生非常有意义的词嵌入

167
00:09:31,710 --> 00:09:33,450
在接下来的视频中我会详细介绍具体细节

168
00:09:33,450 --> 00:09:35,535
我们会讨论沃尔特 VEC 模型

169
00:09:35,535 --> 00:09:42,210
综上所述, 在这个视频中你们看到了语言建模问题

170
00:09:42,210 --> 00:09:45,365
这导致机器学习问题的不同角度

171
00:09:45,365 --> 00:09:49,455
你输入像最后四个单词这样的上下文, 然后预测一些目标词

172
00:09:49,455 --> 00:09:51,485
如何看待这个问题让你可以学习输入词嵌入

173
00:09:51,485 --> 00:09:54,810
下节课

174
00:09:54,810 --> 00:09:57,660
你们将看到如何使用更简单的上下文

175
00:09:57,660 --> 00:10:01,595
甚至更简单的学习算法来标记从上下文到目标词

176
00:10:01,595 --> 00:10:03,840
并基于此训练出一个有用的词嵌入

177
00:10:03,840 --> 00:10:08,440
下一节, 我们将讨论沃尔特 VEC模型
GTC字幕组翻译