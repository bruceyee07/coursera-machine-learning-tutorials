単語埋め込みを計算するアルゴリズムを
いくつか学びました NLPコミュニティで注目されているものに
GloVeアルゴリズムがあります これはスキップグラムのWord2Vecほどは
使われていませんが 一定数のファンがいます その簡潔さが理由かもしれません 次の例をみてみましょう GloVeアルゴリズムは Jeffrey Pennington, Richard Socher, Chris Manningによって
生み出されました GloVeは "global vectors for word representation" 
(単語表現のためのグローバル・ベクトル) を表します 以前は単語ペアをサンプリングするのに つまりコンテクストワードとターゲットワードのペアを 文章コーパス内で近接して現れる2単語として
選び出していました ではGloVeアルゴリズムが何をしているのか 明らかにしていきたいと思います まず Xij を 単語 i がコンテクスト j で
現れる回数とします ここで i と j は t と c に
成り代わっていると考えてください つまり Xij を Xtc と
みなしてかまわないのですが トレーニングセットコーパスを
最初から終わりまで読んで ある単語 i が別の単語 j のコンテクストで
出現する回数 つまり ある単語 t が別の単語 c のコンテクストで
出現する回数を数え上げていきます またコンテクストワードとターゲットワードの
定義によっては Xij イコール Xji であることもあります 実際 コンテクストとターゲットを プラスマイナス10単語内に同時出現するかどうか
という定義にすれば 対称な関係性になります 一方で コンテクストの選び方として 必ずターゲットワードの直前の単語
という風にすると Xij と Xji は前の例のように
対称にはなりません ここでは GloVeアルゴリズムの目的を考え コンテクストとターゲットを 近接して現れる2単語
と定義しましょう 例えばある単語の前後10単語以内に
もう一方が現れる ということです さて Xij は単語 i j が
どのくらいの頻度で共起するか つまり 近くにあるか を表す値です GloVeモデルが何をするのか というと 次のようなことを最適化します 以下のような差分を最小化しようとします θi の転置 ej 引く log Xij の2乗 後ほどこの式に少し追加をします i, j が t, c の役割を果たしているということを
再度念頭に置いておいてください 前に出てきた "θt の転置 ec"
にすこし似ていますね つまり その2単語がどの程度関連しているのかを
これによって表したいというわけです 単語 t と c は どの程度関連しているのか？ 単語 i と j は どの程度関連しているのか？
それをその単語が共起する頻度で測定します つまりこの Xij によって決まります ここで何をやろうとしているのかというと 最急降下法で ある値を最小化するような
パラメタ θ と e を求めることです その値とは
この差分を i = 1~10,000 と j = 1~10,000
全てに渡って取った和です つまり 内積を計算すると それがその2単語の共起頻度に近づくように
ベクトルを学習しようとしています ここで少し細かい点を補足しておくと もし Xij が 0 に等しい場合 log 0 は 負の無限大で定義できません とはいえ Xij が 0 の時を含めて和を取りたいわけです どうすればいいかというと 重み項を一つ追加します この f(Xij) が重み項です Xij が 0 に等しい時
f(Xij) も 0 になります 数学の約束事として 0 log 0 はイコール 0 です これにより Xij が 0 のときでも 和を取ることが出来るようになります この log 0 を気にしなくて良くなります つまり この和は
コンテクスト-ターゲット関係として 少なくとも1回以上共起する単語のペア
のみの和 ということになります f(Xij) の別の役割について説明します 英語には非常に高頻度で登場する単語が
幾つかありますね this, is, of, a などです ストップワードと呼ばれることもありますが
実際には 高頻出単語と低頻出単語には
連続性があります 低頻出単語の中には
たとえば durion (ドリアン) のように 他の一般的な単語程は頻出しないけれども 考慮には入れておきたい単語があります そこで 重み項は以下のような関数として
機能します durion のような低頻出単語にも ある程度の値を与え this, is, of, a などの高頻出単語には より大きな重み ただし大きすぎない値を
与えるような関数です この重み関数が
高頻出単語に過大な重みを与えないように また 低頻出単語に過小な重みを与えないように 関数の選び方において
様々な工夫があります 前のスライドにGloVeの論文への 参照がありますので どんな f を選べばうまくいくか詳しく知りたい方は
ご覧いただければと思います 最後に このアルゴリズムに関する
面白い事実として θ と e の役割が完全に対称だということがあります θi と ej は対称です 数学的に見れば その2つは同じ働きをしていますので
入れ替えたり並べ替えたりしても 結局 同じ最適化目標関数に
なることが分かります このアルゴリズムを学習するには θ と e 両方を一様に乱数初期化し
最急降下法を実行して目標関数を最小化します そして 全ての単語について終わったら 平均を取ります つまり ある単語 w に対し ew(final) を次のように求めます 最急降下法で一通り学習した ew と 最急降下法で一通り学習した θw を
足し2で割ります なぜなら この式における θ と e は
対称な働きをしているからです これまでのビデオで見たような以前のモデルでは θ と e は違う働きをしていたので
こういった平均を取ることは出来ませんでした GloVeアルゴリズムについては以上です このアルゴリズムの不思議なところは この式が異様に簡素に見えることです どうして こんな単純な 二乗誤差関数を最小化するだけで
意味のある単語埋め込みを学習できるのだろう？ と しかし これでうまくいくことが分かります このアルゴリズムの発明者たちが
ここにたどり着くまでには これより遥かに複雑なアルゴリズムを 構築してきた歴史があるのです 新しい言語モデルに続いて
Word2Vecスキップグラムが生み出され その後GloVeが生まれました それまでのアルゴリズムを全て
単純化しようとしてきた結果です 単語埋め込みを学習するための
アルゴリズムの話を終える前に 検討しておきたい性質が
もう一つあります 私たちは 単語ベクトル学習の動機づけとして この特徴量化した表から始めたのでした 「埋め込みベクトルの最初の要素は性別を」 「2つ目は高貴さを」 「年齢、食べ物かどうか・・・などを表現している」
と言っていましたね しかし これまでに見たアルゴリズム
例えばGloVeアルゴリズムなどを使って 単語埋め込みを学習すると
どうなるのかというと 実は 埋め込みベクトルの各要素に対し 必ずしも 何らかの説明がつく保証はないのです どうしてでしょうか
例えば 第1軸が性別 第2軸が高貴さ
であるような空間があるとしましょう 埋め込みベクトルの第1軸を これら性別 高貴さ 年齢 食べ物 の意味の軸の中に
はめ込むことが出来る ということは保証できます 具体的には 例えば 学習アルゴリズムがこの矢印を
第1次元の軸に選ぶかもしれません 単語のコンテクストが与えられて 第1次元はこう
第2軸はこうなるかもしれませんね もしくは直交すら
していないかもしれません この非直交軸が第2軸で 学習した単語埋め込みベクトルの
第2要素となります このとき 線形代数のことをよくご存知であれば
わかると思いますが もし正則行列 A があれば この部分が（θi の転置掛ける ej）が
簡単に次のように書き換えられます A 掛ける θi の転置
A の逆転置 掛けるej なぜなら これを展開すれば θi の転置  A の転置  A の逆転置  掛ける  ej 真ん中の項が打ち消し合って θi の転置 ej が残り
前と同じになりますね もし線形代数がよくわからなくても
心配しなくて構いませんが こういったアルゴリズムにおいて 特徴量を表すのに使われている軸は
人間が解釈しやすい軸に 上手く変換できることが保証されている
ということの簡単な証明をお見せしています 具体的には 学習された特徴量は
色々なものの組み合わせの可能性があります 性別 高貴さ 年齢 食べ物 コスト サイズ 名詞か否か 動作動詞かどうか その他色々な特徴量がありえます 埋め込み行列の各要素 つまり各行を見て それに人間の解釈を
直接当てはめるのは非常に難しいのですが この手の線形変換でないとしても アナロジーとして記述する上では 平行四辺形座標が上手く機能するのです つまり 学習語獲得した特徴量を
どのように線形変換しようが 類推のための平行四辺形座標が
依然機能するのです 単語埋め込み学習については以上です これら単語埋め込みを学習するための
様々なアルゴリズムを見てきました 今週のプログラミング演習でも
もう少し取り扱うことになっています 次回は 感情分析をする上で これらのアルゴリズムを
どう使っていくかについてお見せしていきたいと思います 次のビデオに進みましょう