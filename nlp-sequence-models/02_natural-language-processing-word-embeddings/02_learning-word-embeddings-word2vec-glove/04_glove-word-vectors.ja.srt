1
00:00:00,000 --> 00:00:04,337
単語埋め込みを計算するアルゴリズムを
いくつか学びました

2
00:00:04,337 --> 00:00:10,615
NLPコミュニティで注目されているものに
GloVeアルゴリズムがあります

3
00:00:10,615 --> 00:00:14,520
これはスキップグラムのWord2Vecほどは
使われていませんが

4
00:00:14,520 --> 00:00:16,884
一定数のファンがいます

5
00:00:16,884 --> 00:00:19,865
その簡潔さが理由かもしれません

6
00:00:19,865 --> 00:00:21,133
次の例をみてみましょう

7
00:00:21,133 --> 00:00:24,085
GloVeアルゴリズムは Jeffrey Pennington,

8
00:00:24,085 --> 00:00:25,720
Richard Socher, Chris Manningによって
生み出されました

9
00:00:25,720 --> 00:00:30,570
GloVeは "global vectors for word representation" 
(単語表現のためのグローバル・ベクトル) を表します

10
00:00:30,570 --> 00:00:33,630
以前は単語ペアをサンプリングするのに

11
00:00:33,630 --> 00:00:35,625
つまりコンテクストワードとターゲットワードのペアを

12
00:00:35,625 --> 00:00:42,180
文章コーパス内で近接して現れる2単語として
選び出していました

13
00:00:42,180 --> 00:00:43,380
ではGloVeアルゴリズムが何をしているのか

14
00:00:43,380 --> 00:00:46,721
明らかにしていきたいと思います

15
00:00:46,721 --> 00:00:52,515
まず Xij を

16
00:00:52,515 --> 00:01:01,510
単語 i がコンテクスト j で
現れる回数とします

17
00:01:01,510 --> 00:01:09,095
ここで i と j は t と c に
成り代わっていると考えてください

18
00:01:09,095 --> 00:01:13,970
つまり Xij を Xtc と
みなしてかまわないのですが

19
00:01:13,970 --> 00:01:17,420
トレーニングセットコーパスを
最初から終わりまで読んで

20
00:01:17,420 --> 00:01:22,330
ある単語 i が別の単語 j のコンテクストで
出現する回数 つまり

21
00:01:22,330 --> 00:01:27,170
ある単語 t が別の単語 c のコンテクストで
出現する回数を数え上げていきます

22
00:01:27,170 --> 00:01:33,100
またコンテクストワードとターゲットワードの
定義によっては

23
00:01:33,100 --> 00:01:37,240
Xij イコール Xji であることもあります

24
00:01:37,240 --> 00:01:40,940
実際 コンテクストとターゲットを

25
00:01:40,940 --> 00:01:44,600
プラスマイナス10単語内に同時出現するかどうか
という定義にすれば

26
00:01:44,600 --> 00:01:47,225
対称な関係性になります

27
00:01:47,225 --> 00:01:50,880
一方で コンテクストの選び方として

28
00:01:50,880 --> 00:01:54,795
必ずターゲットワードの直前の単語
という風にすると

29
00:01:54,795 --> 00:01:59,595
Xij と Xji は前の例のように
対称にはなりません

30
00:01:59,595 --> 00:02:02,745
ここでは GloVeアルゴリズムの目的を考え

31
00:02:02,745 --> 00:02:05,630
コンテクストとターゲットを

32
00:02:05,630 --> 00:02:09,155
近接して現れる2単語
と定義しましょう

33
00:02:09,155 --> 00:02:13,755
例えばある単語の前後10単語以内に
もう一方が現れる ということです

34
00:02:13,755 --> 00:02:22,750
さて Xij は単語 i j が
どのくらいの頻度で共起するか

35
00:02:22,750 --> 00:02:24,650
つまり 近くにあるか を表す値です

36
00:02:24,650 --> 00:02:26,480
GloVeモデルが何をするのか というと

37
00:02:26,480 --> 00:02:28,616
次のようなことを最適化します

38
00:02:28,616 --> 00:02:36,745
以下のような差分を最小化しようとします

39
00:02:36,745 --> 00:02:45,565
θi の転置 ej 引く log Xij の2乗

40
00:02:45,565 --> 00:02:48,488
後ほどこの式に少し追加をします

41
00:02:48,488 --> 00:02:54,300
i, j が t, c の役割を果たしているということを
再度念頭に置いておいてください

42
00:02:54,300 --> 00:03:00,428
前に出てきた "θt の転置 ec"
にすこし似ていますね

43
00:03:00,428 --> 00:03:05,125
つまり その2単語がどの程度関連しているのかを
これによって表したいというわけです

44
00:03:05,125 --> 00:03:07,135
単語 t と c は どの程度関連しているのか？

45
00:03:07,135 --> 00:03:12,795
単語 i と j は どの程度関連しているのか？
それをその単語が共起する頻度で測定します

46
00:03:12,795 --> 00:03:17,635
つまりこの Xij によって決まります

47
00:03:17,635 --> 00:03:20,775
ここで何をやろうとしているのかというと

48
00:03:20,775 --> 00:03:28,320
最急降下法で ある値を最小化するような
パラメタ θ と e を求めることです

49
00:03:28,320 --> 00:03:36,500
その値とは
この差分を i = 1~10,000 と j = 1~10,000
全てに渡って取った和です

50
00:03:36,500 --> 00:03:38,340
つまり 内積を計算すると

51
00:03:38,340 --> 00:03:44,565
それがその2単語の共起頻度に近づくように
ベクトルを学習しようとしています

52
00:03:44,565 --> 00:03:47,386
ここで少し細かい点を補足しておくと

53
00:03:47,386 --> 00:03:49,556
もし Xij が 0 に等しい場合

54
00:03:49,556 --> 00:03:52,505
log 0 は 負の無限大で定義できません

55
00:03:52,505 --> 00:03:53,740
とはいえ

56
00:03:53,740 --> 00:03:59,050
Xij が 0 の時を含めて和を取りたいわけです

57
00:03:59,050 --> 00:04:00,910
どうすればいいかというと

58
00:04:00,910 --> 00:04:03,590
重み項を一つ追加します

59
00:04:03,590 --> 00:04:07,895
この f(Xij) が重み項です

60
00:04:07,895 --> 00:04:17,925
Xij が 0 に等しい時
f(Xij) も 0 になります

61
00:04:17,925 --> 00:04:26,135
数学の約束事として 0 log 0 はイコール 0 です

62
00:04:26,135 --> 00:04:27,152
これにより

63
00:04:27,152 --> 00:04:29,195
Xij が 0 のときでも

64
00:04:29,195 --> 00:04:32,433
和を取ることが出来るようになります

65
00:04:32,433 --> 00:04:35,315
この log 0 を気にしなくて良くなります

66
00:04:35,315 --> 00:04:40,190
つまり この和は
コンテクスト-ターゲット関係として

67
00:04:40,190 --> 00:04:46,220
少なくとも1回以上共起する単語のペア
のみの和 ということになります

68
00:04:46,220 --> 00:04:48,650
f(Xij) の別の役割について説明します

69
00:04:48,650 --> 00:04:52,350
英語には非常に高頻度で登場する単語が
幾つかありますね

70
00:04:52,350 --> 00:04:55,005
this, is, of, a などです

71
00:04:55,005 --> 00:04:57,110
ストップワードと呼ばれることもありますが
実際には

72
00:04:57,110 --> 00:04:59,810
高頻出単語と低頻出単語には
連続性があります

73
00:04:59,810 --> 00:05:02,665
低頻出単語の中には
たとえば durion (ドリアン) のように

74
00:05:02,665 --> 00:05:04,460
他の一般的な単語程は頻出しないけれども

75
00:05:04,460 --> 00:05:09,480
考慮には入れておきたい単語があります

76
00:05:09,480 --> 00:05:12,620
そこで 重み項は以下のような関数として
機能します

77
00:05:12,620 --> 00:05:16,976
durion のような低頻出単語にも

78
00:05:16,976 --> 00:05:20,090
ある程度の値を与え

79
00:05:20,090 --> 00:05:25,128
this, is, of, a などの高頻出単語には

80
00:05:25,128 --> 00:05:28,745
より大きな重み ただし大きすぎない値を
与えるような関数です

81
00:05:28,745 --> 00:05:33,960
この重み関数が
高頻出単語に過大な重みを与えないように

82
00:05:33,960 --> 00:05:37,110
また 低頻出単語に過小な重みを与えないように

83
00:05:37,110 --> 00:05:41,020
関数の選び方において
様々な工夫があります

84
00:05:41,020 --> 00:05:43,725
前のスライドにGloVeの論文への

85
00:05:43,725 --> 00:05:45,141
参照がありますので

86
00:05:45,141 --> 00:05:51,720
どんな f を選べばうまくいくか詳しく知りたい方は
ご覧いただければと思います

87
00:05:51,720 --> 00:05:56,070
最後に このアルゴリズムに関する
面白い事実として

88
00:05:56,070 --> 00:06:01,215
θ と e の役割が完全に対称だということがあります

89
00:06:01,215 --> 00:06:07,595
θi と ej は対称です

90
00:06:07,595 --> 00:06:08,912
数学的に見れば

91
00:06:08,912 --> 00:06:12,403
その2つは同じ働きをしていますので
入れ替えたり並べ替えたりしても

92
00:06:12,403 --> 00:06:17,293
結局 同じ最適化目標関数に
なることが分かります

93
00:06:17,293 --> 00:06:21,571
このアルゴリズムを学習するには

94
00:06:21,571 --> 00:06:26,180
θ と e 両方を一様に乱数初期化し
最急降下法を実行して目標関数を最小化します

95
00:06:26,180 --> 00:06:30,567
そして 全ての単語について終わったら

96
00:06:30,567 --> 00:06:31,660
平均を取ります

97
00:06:31,660 --> 00:06:33,580
つまり ある単語 w に対し

98
00:06:33,580 --> 00:06:36,910
ew(final) を次のように求めます

99
00:06:36,910 --> 00:06:41,410
最急降下法で一通り学習した ew と

100
00:06:41,410 --> 00:06:46,999
最急降下法で一通り学習した θw を
足し2で割ります

101
00:06:46,999 --> 00:06:50,770
なぜなら この式における θ と e は
対称な働きをしているからです

102
00:06:50,770 --> 00:06:54,910
これまでのビデオで見たような以前のモデルでは

103
00:06:54,910 --> 00:06:59,890
θ と e は違う働きをしていたので
こういった平均を取ることは出来ませんでした

104
00:06:59,890 --> 00:07:02,320
GloVeアルゴリズムについては以上です

105
00:07:02,320 --> 00:07:04,450
このアルゴリズムの不思議なところは

106
00:07:04,450 --> 00:07:07,073
この式が異様に簡素に見えることです

107
00:07:07,073 --> 00:07:09,040
どうして こんな単純な

108
00:07:09,040 --> 00:07:12,767
二乗誤差関数を最小化するだけで
意味のある単語埋め込みを学習できるのだろう？ と

109
00:07:12,767 --> 00:07:15,675
しかし これでうまくいくことが分かります

110
00:07:15,675 --> 00:07:18,480
このアルゴリズムの発明者たちが
ここにたどり着くまでには

111
00:07:18,480 --> 00:07:19,865
これより遥かに複雑なアルゴリズムを

112
00:07:19,865 --> 00:07:23,643
構築してきた歴史があるのです

113
00:07:23,643 --> 00:07:28,340
新しい言語モデルに続いて
Word2Vecスキップグラムが生み出され

114
00:07:28,340 --> 00:07:30,625
その後GloVeが生まれました

115
00:07:30,625 --> 00:07:34,960
それまでのアルゴリズムを全て
単純化しようとしてきた結果です

116
00:07:34,960 --> 00:07:41,200
単語埋め込みを学習するための
アルゴリズムの話を終える前に

117
00:07:41,200 --> 00:07:47,215
検討しておきたい性質が
もう一つあります

118
00:07:47,215 --> 00:07:50,935
私たちは 単語ベクトル学習の動機づけとして

119
00:07:50,935 --> 00:07:54,633
この特徴量化した表から始めたのでした

120
00:07:54,633 --> 00:07:58,803
「埋め込みベクトルの最初の要素は性別を」

121
00:07:58,803 --> 00:08:02,147
「2つ目は高貴さを」

122
00:08:02,147 --> 00:08:06,175
「年齢、食べ物かどうか・・・などを表現している」
と言っていましたね

123
00:08:06,175 --> 00:08:10,780
しかし これまでに見たアルゴリズム
例えばGloVeアルゴリズムなどを使って

124
00:08:10,780 --> 00:08:15,580
単語埋め込みを学習すると
どうなるのかというと

125
00:08:15,580 --> 00:08:18,910
実は 埋め込みベクトルの各要素に対し

126
00:08:18,910 --> 00:08:22,933
必ずしも 何らかの説明がつく保証はないのです

127
00:08:22,933 --> 00:08:27,190
どうしてでしょうか
例えば 第1軸が性別

128
00:08:27,190 --> 00:08:32,188
第2軸が高貴さ
であるような空間があるとしましょう

129
00:08:32,188 --> 00:08:37,545
埋め込みベクトルの第1軸を

130
00:08:37,545 --> 00:08:43,210
これら性別 高貴さ 年齢 食べ物 の意味の軸の中に
はめ込むことが出来る ということは保証できます

131
00:08:43,210 --> 00:08:46,480
具体的には

132
00:08:46,480 --> 00:08:51,850
例えば 学習アルゴリズムがこの矢印を
第1次元の軸に選ぶかもしれません

133
00:08:51,850 --> 00:08:54,343
単語のコンテクストが与えられて

134
00:08:54,343 --> 00:08:58,993
第1次元はこう
第2軸はこうなるかもしれませんね

135
00:08:58,993 --> 00:09:00,895
もしくは直交すら
していないかもしれません

136
00:09:00,895 --> 00:09:05,420
この非直交軸が第2軸で

137
00:09:05,420 --> 00:09:09,230
学習した単語埋め込みベクトルの
第2要素となります

138
00:09:09,230 --> 00:09:10,530
このとき

139
00:09:10,530 --> 00:09:17,105
線形代数のことをよくご存知であれば
わかると思いますが

140
00:09:17,105 --> 00:09:19,850
もし正則行列 A があれば

141
00:09:19,850 --> 00:09:26,330
この部分が（θi の転置掛ける ej）が
簡単に次のように書き換えられます

142
00:09:26,330 --> 00:09:35,604
A 掛ける θi の転置
A の逆転置 掛けるej

143
00:09:35,604 --> 00:09:36,790
なぜなら これを展開すれば

144
00:09:36,790 --> 00:09:44,008
θi の転置  A の転置  A の逆転置  掛ける  ej

145
00:09:44,008 --> 00:09:46,215
真ん中の項が打ち消し合って

146
00:09:46,215 --> 00:09:49,530
θi の転置 ej が残り
前と同じになりますね

147
00:09:49,530 --> 00:09:51,993
もし線形代数がよくわからなくても
心配しなくて構いませんが

148
00:09:51,993 --> 00:09:56,540
こういったアルゴリズムにおいて

149
00:09:56,540 --> 00:10:00,965
特徴量を表すのに使われている軸は
人間が解釈しやすい軸に

150
00:10:00,965 --> 00:10:06,975
上手く変換できることが保証されている
ということの簡単な証明をお見せしています

151
00:10:06,975 --> 00:10:10,205
具体的には 学習された特徴量は
色々なものの組み合わせの可能性があります

152
00:10:10,205 --> 00:10:11,565
性別 高貴さ 年齢 食べ物

153
00:10:11,565 --> 00:10:14,575
コスト サイズ

154
00:10:14,575 --> 00:10:16,781
名詞か否か 動作動詞かどうか

155
00:10:16,781 --> 00:10:17,975
その他色々な特徴量がありえます

156
00:10:17,975 --> 00:10:21,620
埋め込み行列の各要素

157
00:10:21,620 --> 00:10:27,670
つまり各行を見て それに人間の解釈を
直接当てはめるのは非常に難しいのですが

158
00:10:27,670 --> 00:10:30,710
この手の線形変換でないとしても

159
00:10:30,710 --> 00:10:33,620
アナロジーとして記述する上では

160
00:10:33,620 --> 00:10:36,660
平行四辺形座標が上手く機能するのです

161
00:10:36,660 --> 00:10:42,950
つまり 学習語獲得した特徴量を
どのように線形変換しようが

162
00:10:42,950 --> 00:10:51,220
類推のための平行四辺形座標が
依然機能するのです

163
00:10:51,220 --> 00:10:54,170
単語埋め込み学習については以上です

164
00:10:54,170 --> 00:10:57,860
これら単語埋め込みを学習するための
様々なアルゴリズムを見てきました

165
00:10:57,860 --> 00:11:01,802
今週のプログラミング演習でも
もう少し取り扱うことになっています

166
00:11:01,802 --> 00:11:03,680
次回は 感情分析をする上で

167
00:11:03,680 --> 00:11:06,640
これらのアルゴリズムを
どう使っていくかについてお見せしていきたいと思います

168
00:11:06,640 --> 00:11:08,000
次のビデオに進みましょう