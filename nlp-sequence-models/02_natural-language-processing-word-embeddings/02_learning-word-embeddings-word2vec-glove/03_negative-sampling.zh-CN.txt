上个视频中 你看到了如何利用Skip-Gram模型构建一个监督学习任务 使得我们建立从上下文到目标词语的映射 并基于此训练出一个有用的词嵌入 但是这种方法的弊端在于Softmax函数训练起来比较慢 在这个视频中，你将看到一种改进的学习方法，
叫做“负采样法” 它使你达到和刚才看到的Skip-Gram模型类似的结果 但采用一种更加有效的学习算法 让我们看看如何做到这一点 这段视频中介绍的想法大多数基于
Tomas Mikolov, Ilya Sutskever Kai Chen, Greg Corrado, 以及Jeff Dean的成果 在这个算法中，我们要做的就是 建立一个新的监督学习问题 这个问题就是，给定一对词语，
例如“橙子"和"果汁" 我们要预测这是否是一对上下文和目标词语 在这个例子中，“橙子果汁”是一个正样本 那么“橙子”和“国王”呢？ 对，这是一个负样本，
因此我们在目标这一栏写上0 那么我们要做的就是采样一个上下文 以及一个目标词语 在这个例子中，我们采样的是“橙子”和“果汁” 并且我们给这对词语标记为1
我们把中间这一栏记为“词语” 那么，用和我们之前视频中一样的方法 生成一个正样本之后 即采样一个上下文词语，然后在附近的
一个窗口，例如前后10个词语 选取一个目标词语 那么这就是你生成这个表格第一行的方法 接下来，要生成一个负样本
你首先选取和之前一样的上下文词语 然后从字典中随机抽取一个词语 在这个例子中，我随机选取了“国王”这个词语
并且将这对词语标记为0 然后，我们仍然选取“橙子”
并且从字典中再随机抽取一个词语 我们假设了
如果我们确实是随机抽取了一个词语 它很大可能不会和“橙子”这个词语相关
那么记为“橙子”、“书”、0 让我们再选取一些其他的，首先“橙子”，或许因为偶然 你选到了“这”这个词，记为0 再来，“橙子”，或许再次偶然地，我们选到了“的” 我们标记为0 注意到，尽管“的”被标记为0 但“的”实际上也会出现在“橙子”的后面 总结下来，我们生成这个数据集的方法是
我们选择一个上下文词语 然后选取一个目标词语
这就是这个表格的第一行 这就给我们一个正样本 那么上下文、目标，然后标记为1 然后我们要做的是
对于某个次数，k 我们选取同样的上下文词语，
然后从字典中随机抽取词语 “国王”、“书”、“这”、“的”，
只要是从字典中随机抽取的就行 把这些全部标记为0，这样就得到了我们的负样本 即使我们从字典中随机抽取的某个词 恰好出现在上下文词语附近的一个窗口 前后十个词语内，例如紧接着上下文词语“橙汁”
也是没关系的 接下来，我们要设计一个监督学习问题 在这个问题中，这个学习算法输入为x，输入这对词语 然后它通过预测这个标记，来预测输出y 那么这个问题就是，给定一对词语，
例如“橙子”和“果汁” 你认为它们出现在一起吗？ 你认为要得到这对词语，
我是通过采样两个彼此接近的词语？ 还是通过从文本中选取一个词 然后从字典中随机再选取一个词？ 实际上就是要努力区分这两种 生成一对词语的分布 这就是你生成训练集的方法 至于如何选取k，Mikolov等人 建议对于小的数据集，或许k介于5到20 如果你有一个很大的数据集，
那么选取较小的k 也就是，对于大的数据集，k介于2到5 对于小的数据集选取更大的k值 好了，在这个例子中，我们选取k=4 接下来我们要描述这个监督学习算法 从x映射到y 这是我们从之前视频中学习的Softmax模型 这是我们从前一张幻灯片中得到的训练集 再强调一下，这将会是新的输入x 这将是你想要预测的y值 那么要定义这个模型，我将用c来表示 上下文词语，用t来表示可能的目标词语 用y来表示0或1，这就是一对上下文-目标词语 接下来我们要做的是定义一个逻辑回归模型 例如，在给定c、t的条件下，y=1的概率， 我们要把这个建模为一个逻辑回归模型 但具体我们采用的公式是Sigmoid函数，应用在 theta_t，转置，乘以e_c 这些参数和以前差不多 对每一个可能的目标词语，
你有一个参数向量，theta 以及对于每一个上下文词语，另一个参数向量 实际上也就是词嵌入向量 我们将用这个公式来估计 y=1的概率 假设你有k个样本，
那么你可以看成是有 一个k1的负样本-正样本比率 对每一个正样本，你有 k个负样本来训练这个类似逻辑回归的模型 那么把这个转换成一个神经网络，
如果输入词语是“橙子” 也就是第6257个词语，你要做的是 输入这个one-hot向量 传递给e，利用向量乘法，得到嵌入向量，e_6275 那么你实际上得到了10,000个可能的 逻辑回归分类问题 其中一个就是对应于 目标词语是否是“果汁”的分类器 接下来还会有其他的词语，例如 可能在这里某个地方，
你要预测这个词语是不是“国王” 这些都是你字典中的词语 那么，想象这是有10,000个二进制逻辑回归分类器 但是，并非要在每次迭代中
训练全部10,000个分类器 我们只训练其中的5个 也就是对应于实际的目标词语的那个 以及对应于其他4个
随机抽取的负样本的分类器 这就是k等于4时的情形 那么，不同于训练一个非常难于训练的 巨大的具有10,000个可能结果的Softmax模型，
我们将它转换为 10,000个二元分类问题 它们每一个计算起来都特别容易 并且在每次迭代中，我们只训练其中的5个，
或者更一般地说 其中的k+1个，k个负样本，1个正样本 这就是为什么这个算法的计算成本显著更低 因为你在训练k+1个逻辑回归模型 或者说k+1个二元分类问题 它们在每次迭代中计算起来都相对容易 而不是训练一个具有10,000个
可能结果的Softmax分类器 在这周的编程练习中，你将有机会 来熟悉这种算法 这种技巧叫做负采样法，因为你做的正是 得到一个正样本，”橙子“，然后”果汁“ 然后你会故意地生成一些负样本 这就是为什么它叫做”负采样法“ 你将用这些负样本来训练4个二元分类器 在每次迭代中，你选择4个不同的 随机负样本，并用它们来训练你的算法 现在，在我们总结之前，
对于这个算法很重要的一个细节就是 你如何选取这些负样本？ 在选取上下文词语“橙子”后 你要如何抽取这些词语来生成负样本呢？ 一个可能的做法是，抽取中间的词语 作为候选目标词语 一个可能的做法是，根据你的文本中 词语的经验频率分布来抽样 也就是根据词语出现的频率来抽样 这么做的问题在于，你会得到很多例如 “这”、“的”、“和”之类的词语 另一个极端就是，你根据字典词汇量的倒数 均匀地抽取负样本，但是这么做 也很不能够代表英语中单词的分布 因此，作者Mikolov等人表示，
根据经验，他们认为最好的方法是 采用这种启发式的值，也就是介于 从经验频率分布中采样，即 观察到的英语文本中的分布，
到从均匀分布中采样 他们所做的，就是依词频的3/4次幂 来抽样 那么例如f(w_i)表示某个观察到的
英语中的词频 或者说你训练集中的词频，
那么取它的3/4次幂 这样它就介于取均匀分布 和取经验分布的 两个极端之间 我不确定这是否在理论上站得住脚，但是 许多研究者现在正在用这种启发式方法，
并且似乎效果非常不错 那么，总结一下，
你学到了如何用Softmax分类器
来训练词向量 但这样计算上成本很高 在这个视频中，你学到了如何把它变为一系列 二元分类问题，并且非常高效地训练词向量 如果你运行这种算法，你会得到非常好的词向量 当然，就像深度学习中的其他领域一样 有开源的实现 也有预训练过的词向量，
这些都是其他人训练过， 并且通过许可证的方式分享到网上的 因此如果你想快速上手一个自然语言处理问题， 合理的做法是下载其他人的词向量，
并以此作为出发点 这样我们就讲完了Skip-Gram模型 在下一个视频中，我将与你们分享另外一种 可能比你现在看到的更加简单的词嵌入算法 在下个视频中，我们将学习GloVe算法