1
00:00:00,000 --> 00:00:03,645
序列到序列模型的其中一个激动人心的进展

2
00:00:03,645 --> 00:00:08,780
是非常准确的语音识别的出现

3
00:00:08,780 --> 00:00:10,140
我们的课程接近尾声

4
00:00:10,140 --> 00:00:13,800
我想借几个视频来介绍一下

5
00:00:13,800 --> 00:00:19,195
序列到序列模型是如何运用到声音数据,例如语音上的

6
00:00:19,195 --> 00:00:22,110
那么什么是语音识别呢?

7
00:00:22,110 --> 00:00:24,505
给定一个声音片段 x

8
00:00:24,505 --> 00:00:31,058
你的任务是自动找出相应的文字 y

9
00:00:31,058 --> 00:00:32,565
那么你对一个声音片段

10
00:00:32,565 --> 00:00:34,649
这样作图表示

11
00:00:34,649 --> 00:00:37,211
横坐标是时间轴

12
00:00:37,211 --> 00:00:43,313
麦克风测量气压的微小变化

13
00:00:43,313 --> 00:00:46,470
现在你之所以听到我的声音

14
00:00:46,470 --> 00:00:50,130
是因为你的耳朵探测到

15
00:00:50,130 --> 00:00:55,065
由扬声器或耳机生成的微小的气压变化

16
00:00:55,065 --> 00:01:01,145
这种音频图是用气压对时间作图

17
00:01:01,145 --> 00:01:06,192
如果这段声频片段是我在说

18
00:01:06,192 --> 00:01:08,835
那只飞快的棕色狐狸

19
00:01:08,835 --> 00:01:14,175
那么语音识别算法就可以用个声频片段作为输入
然后输出对应文本

20
00:01:14,175 --> 00:01:18,398
即使人的耳朵也不处理原始的声波形式

21
00:01:18,398 --> 00:01:21,090
人的耳朵有生理结构

22
00:01:21,090 --> 00:01:25,183
能够测量不同频率的强度

23
00:01:25,183 --> 00:01:29,745
所以一个常见的声频数据的预处理步骤是

24
00:01:29,745 --> 00:01:34,985
用声频片段生成一个频谱图

25
00:01:34,985 --> 00:01:39,100
这个图里横坐标是时间

26
00:01:39,100 --> 00:01:42,390
纵坐标是频率

27
00:01:42,390 --> 00:01:46,820
不同颜色的深浅表示能量大小

28
00:01:46,820 --> 00:01:51,385
在不同的时间不同的频率下，音量大小是多少?

29
00:01:51,385 --> 00:01:55,390
这种频谱图

30
00:01:55,390 --> 00:01:58,810
或者你也许听到人们说false back outputs

31
00:01:58,810 --> 00:02:01,870
是常用的

32
00:02:01,870 --> 00:02:06,470
把音频输入到算法之前的预处理步骤

33
00:02:06,470 --> 00:02:12,990
这种预处理步骤也与人耳的一个计算步骤非常相似

34
00:02:12,990 --> 00:02:18,164
语音识别的其中一个最激动人心的趋势是

35
00:02:18,164 --> 00:02:19,735
以前

36
00:02:19,735 --> 00:02:27,072
语音识别系统过去基于音素

37
00:02:27,072 --> 00:02:31,135
我想说，那是手工设计的细胞基本单元

38
00:02:31,135 --> 00:02:34,175
"飞快的棕色狐狸"用音素表示

39
00:02:34,175 --> 00:02:36,074
我要简化一下

40
00:02:36,074 --> 00:02:38,960
"The"有一个"de"音 "e"音

41
00:02:38,960 --> 00:02:42,578
"Quick"有一个"ku"音 "u"音 "ik"音 "k"音

42
00:02:42,578 --> 00:02:46,270
语言学家写下这些基本的声音单元

43
00:02:46,270 --> 00:02:50,293
并尝试把语言拆分为这些基本的声音单元

44
00:02:50,293 --> 00:02:52,670
那么，棕色的

45
00:02:52,670 --> 00:02:57,400
这些不是用更复杂的符号写的官方音素

46
00:02:57,400 --> 00:03:02,620
但语言学家假设

47
00:03:02,620 --> 00:03:04,475
用音素，声音的基本单元表示音频

48
00:03:04,475 --> 00:03:08,290
是语音识别的最佳方式

49
00:03:08,290 --> 00:03:10,776
然而，用端到端的深度学习

50
00:03:10,776 --> 00:03:15,505
我们发现用音素表示声音不再是必要的

51
00:03:15,505 --> 00:03:21,010
你可以开发一个系统,，输入一个音频片段

52
00:03:21,010 --> 00:03:27,795
并直接输出文字 而不需要使用像这样的手工设计的表示方法

53
00:03:27,795 --> 00:03:33,420
使这成为可能的事情之一是要更大的数据集

54
00:03:33,420 --> 00:03:43,425
语音识别的学术数据集可能是300小时长 在学术界

55
00:03:43,425 --> 00:03:49,220
3000小时的音频数据集将被认为是合理的大小

56
00:03:49,220 --> 00:03:50,965
很多研究

57
00:03:50,965 --> 00:03:56,065
很多学术论文，是基于上千个小时的数据集的

58
00:03:56,065 --> 00:03:59,890
但是，最好的商业性的系统是在

59
00:03:59,890 --> 00:04:04,681
超过1万小时 有时超过10万小时的音频数据集上训练的

60
00:04:04,681 --> 00:04:10,325
而且，音频数据集还会越来越大

61
00:04:10,325 --> 00:04:12,925
已经标注的声频数据集，包含x和y

62
00:04:12,925 --> 00:04:15,487
和深度学习算法

63
00:04:15,487 --> 00:04:18,745
很大地推动语音识别地进步

64
00:04:18,745 --> 00:04:22,310
那么，如何开发一个语音识别系统呢

65
00:04:22,310 --> 00:04:23,634
上个视频中

66
00:04:23,634 --> 00:04:25,870
我们说到注意力模型

67
00:04:25,870 --> 00:04:28,768
所以，你可以做的一件事就是

68
00:04:28,768 --> 00:04:30,655
在横坐标上

69
00:04:30,655 --> 00:04:34,604
你输入不同时间帧的音频

70
00:04:34,604 --> 00:04:38,905
然后你有一个注意力机制尝试输出文字

71
00:04:38,905 --> 00:04:40,975
比如 "那 只 飞 快 的 棕 色 狐 狸"

72
00:04:40,975 --> 00:04:42,873
或者什么其它的

73
00:04:42,873 --> 00:04:47,933
另一个似乎很好的方法是使用CTC损失函数来做语音识别

74
00:04:47,933 --> 00:04:53,540
CTC代表的是联结主义时间分类，是由于Alex Graves

75
00:04:53,540 --> 00:04:57,220
Santiago Fernandes, Faustino Gomez, 和Jürgen Schmidhuber发表的

76
00:04:57,220 --> 00:05:01,398
是这样的，假如，一个音频片段是有人说

77
00:05:01,398 --> 00:05:02,915
那只飞快的棕色狐狸

78
00:05:02,915 --> 00:05:07,090
我们将使用一个新的网络结构

79
00:05:07,090 --> 00:05:11,460
像这样输入x和输出y数量相等

80
00:05:11,460 --> 00:05:17,350
我已经画了一个简单的单向的RNN，但在实践中

81
00:05:17,350 --> 00:05:20,518
这通常是一个双向的LSTM

82
00:05:20,518 --> 00:05:23,847
或是双向的GRU，通常是一个更深层的模型

83
00:05:23,847 --> 00:05:30,280
但是请注意，这里的时间步骤数量非常大

84
00:05:30,280 --> 00:05:32,950
并且在语音识别中，通常输入时间步骤的数量

85
00:05:32,950 --> 00:05:35,790
大大超出输出时间步骤的数量

86
00:05:35,790 --> 00:05:39,940
例如，如果您有10秒的音频

87
00:05:39,940 --> 00:05:44,437
特征是100 hz 即每秒100个样本数据

88
00:05:44,437 --> 00:05:48,750
那么10秒的音频将会是1000个输入向量

89
00:05:48,750 --> 00:05:51,050
对，所以它是100赫兹乘以10秒

90
00:05:51,050 --> 00:05:53,553
所以有1000个输入向量

91
00:05:53,553 --> 00:05:56,828
但是你的输出可能没有1000个字母

92
00:05:56,828 --> 00:05:59,910
可能没有1000个字母

93
00:05:59,910 --> 00:06:01,790
那你怎么做?

94
00:06:01,790 --> 00:06:08,788
CTC损失函数允许RNN生成这样的输出序列 ttt

95
00:06:08,788 --> 00:06:10,810
有一个特殊的字符被称为

96
00:06:10,810 --> 00:06:12,860
空白字符，这里我们要把它写为下划线

97
00:06:12,860 --> 00:06:22,645
h_eee _ _ _<br />然后是一个空格

98
00:06:22,645 --> 00:06:32,995
我们要这样写<br />一个空格然后 _ _ _ qqq _ _

99
00:06:32,995 --> 00:06:40,615
这被认为是"the q"部分的一个正确的输出

100
00:06:40,615 --> 00:06:42,345
quick 以 q 开头

101
00:06:42,345 --> 00:06:46,460
CTC损失函数的基本原理

102
00:06:46,460 --> 00:06:51,488
是折叠那些没有被空白隔开的重复字符

103
00:06:51,488 --> 00:06:52,956
明确一下

104
00:06:52,956 --> 00:06:55,135
我使用这个下划线来表示

105
00:06:55,135 --> 00:07:01,620
一个特殊的空白字符，这与空格不同

106
00:07:01,620 --> 00:07:05,181
这里在"the"和"quick"中间有一个空格

107
00:07:05,181 --> 00:07:07,025
所以我应该输出一个空格

108
00:07:07,025 --> 00:07:09,605
但是通过折叠

109
00:07:09,605 --> 00:07:11,025
没有被空白隔开的重复字符

110
00:07:11,025 --> 00:07:18,142
它实际上把序列折叠成 t, h,

111
00:07:18,142 --> 00:07:20,686
e, 然后空格, q,

112
00:07:20,686 --> 00:07:26,400
这使你的网络

113
00:07:26,400 --> 00:07:31,380
重复同样字符很多次,有1000个输出序列

114
00:07:31,380 --> 00:07:34,830
所以,插入一堆空白字符

115
00:07:34,830 --> 00:07:38,990
最终仍然能输出很多更短的文字串

116
00:07:38,990 --> 00:07:42,150
所以，这里的这个短语"飞快的棕色狐狸"包括空格

117
00:07:42,150 --> 00:07:46,140
实际上有19个字符
如果以某种方式

118
00:07:46,140 --> 00:07:48,090
神经网络必须要输出1000个输出向量

119
00:07:48,090 --> 00:07:52,714
并允许空白字符和重复字符插入

120
00:07:52,714 --> 00:07:55,171
它就仍然可以

121
00:07:55,171 --> 00:08:00,480
用这1000个Y的输出值来表示
这19个字符的组成的字串

122
00:08:00,480 --> 00:08:03,615
所以，这篇由Alex Grace发表的文章

123
00:08:03,615 --> 00:08:08,323
以及那些我参与其中的

124
00:08:08,323 --> 00:08:10,120
深度语音识别系统

125
00:08:10,120 --> 00:08:14,190
都使用这个方法建立有效的语音识别系统

126
00:08:14,190 --> 00:08:19,935
所以，我希望这能让你对语音识别模型
的工作方式有一个初步的理解

127
00:08:19,935 --> 00:08:23,415
基于注意力的模型，CTC模型
都是可行的方案

128
00:08:23,415 --> 00:08:27,430
这展示了两种不同的方法来构建这些系统

129
00:08:27,430 --> 00:08:30,400
现今，构建这些高效的系统

130
00:08:30,400 --> 00:08:33,330
其中生成技能语音识别系统是

131
00:08:33,330 --> 00:08:37,585
需要相当大的努力，需要一个非常大的数据集

132
00:08:37,585 --> 00:08:40,688
但是，我想在下一个视频中与你分享

133
00:08:40,688 --> 00:08:43,610
如何搭建一个触发词检测系统

134
00:08:43,610 --> 00:08:47,115
其中的关键字检测系统
实际上可以更容易地搭建

135
00:08:47,115 --> 00:08:50,995
可以用小的数据集来做

136
00:08:50,995 --> 00:08:53,000
那么，让我们在下一个视频中讨论它吧