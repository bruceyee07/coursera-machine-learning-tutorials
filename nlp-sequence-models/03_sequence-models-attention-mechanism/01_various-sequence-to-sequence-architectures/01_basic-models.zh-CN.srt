1
00:00:00,000 --> 00:00:03,398
嗨，欢迎学习这门课最后一周的内容

2
00:00:03,398 --> 00:00:05,805
同时也是深度学习系列

3
00:00:05,805 --> 00:00:09,215
最后一周的课程内容

4
00:00:09,215 --> 00:00:11,430
你们即将到达终点线

5
00:00:11,430 --> 00:00:14,875
这周你将学习Seq2Seq模型

6
00:00:14,875 --> 00:00:19,590
该模型对实施机器翻译和语言识别很有帮助

7
00:00:19,590 --> 00:00:22,710
现在我们先引入基本模型，在之后的课程中

8
00:00:22,710 --> 00:00:23,738
你将会学到集束搜索（Beam Search）、

9
00:00:23,738 --> 00:00:25,847
注意力模型（Attention model）

10
00:00:25,847 --> 00:00:29,540
最后讨论如何对语音数据建模作为收尾

11
00:00:29,540 --> 00:00:32,160
如何对数据进行运算

12
00:00:32,160 --> 00:00:38,770
假如你现在输入一句法语
如：Jane visite l'Afrique en septembre

13
00:00:38,770 --> 00:00:41,620
然后你想把这句法语翻译成英文

14
00:00:41,620 --> 00:00:44,160
简9月份正在访问非洲
(Jane is visiting Africa in September.)

15
00:00:44,160 --> 00:00:48,380
像之前那样，我们用x<1>到x<5>

16
00:00:48,380 --> 00:00:49,655
来表示作为输入的这句法语句子里面

17
00:00:49,655 --> 00:00:52,445
的每个单词

18
00:00:52,445 --> 00:00:58,460
然后我们用y<1>到y<6>来表达输出句子里每个单词

19
00:00:58,460 --> 00:01:05,895
现在你该如何构造一个神经网络，
使得输入是序列X，输出是序列y呢？

20
00:01:05,895 --> 00:01:08,010
对于这个问题，你可以这么做：

21
00:01:08,010 --> 00:01:14,630
顺便一提，我接下去要说的这个方法主要由Sutskever

22
00:01:14,630 --> 00:01:16,580
Oriol Vinyals和Quoc Le的一篇论文提供

23
00:01:16,580 --> 00:01:23,023
另一篇是由Kyunghyun Cho、

24
00:01:23,023 --> 00:01:26,023
Bart van Merrienboer, Caglar Gulcehre, 
Dzmitry Bahdanau, Fethi Bougares

25
00:01:26,023 --> 00:01:29,023
Hogler Schwen和Yoshua Bengio提供

26
00:01:29,023 --> 00:01:31,875
首先我们先定义一个神经网络

27
00:01:31,875 --> 00:01:38,060
该网络使用RNN模型来搭建一个编码器网络

28
00:01:38,060 --> 00:01:41,473
这部分可能是GRU或者LSTM

29
00:01:41,473 --> 00:01:44,945
每次向模型喂入一个法语单词

30
00:01:44,945 --> 00:01:49,135
然后通过理解输入序列的内容

31
00:01:49,135 --> 00:01:55,770
随后RNN将输出一个向量来表示输入序列

32
00:01:55,770 --> 00:02:00,755
在此之后，需要再建立一个解码器网络

33
00:02:00,755 --> 00:02:02,525
该网络将编码器的输出作为输入

34
00:02:02,525 --> 00:02:07,220
也就是图中黑色部分画出的部分

35
00:02:07,220 --> 00:02:10,900
而这些输入将通过神经网络进行训练

36
00:02:10,900 --> 00:02:15,720
并且按一次一个单词的顺序

37
00:02:15,720 --> 00:02:22,300
将翻译结果输出

38
00:02:22,300 --> 00:02:25,783
而当解码器停止时，也就意味着句子翻译结束

39
00:02:25,783 --> 00:02:29,900
像之前一样，我们可以将每一次产生的输出

40
00:02:29,900 --> 00:02:34,460
作为后一个产生单词的RNN模型的输入

41
00:02:34,460 --> 00:02:39,640
就像我们之前在序列中用语言模型合成文本一样

42
00:02:39,640 --> 00:02:45,915
在深度学习中应用证明这个方法是有效的

43
00:02:45,915 --> 00:02:49,725
当给到你足够多的，且一一对应的法语和英语句子

44
00:02:49,725 --> 00:02:52,815
如果你训练一个模型，输入一句法语句子

45
00:02:52,815 --> 00:02:57,144
然后输出一个相应的英语翻译

46
00:02:57,144 --> 00:02:59,695
这个方法目前看来确实是行之有效的

47
00:02:59,695 --> 00:03:03,270
并且这个模型仅仅用了一个编码器网络

48
00:03:03,270 --> 00:03:08,400
它的作用是将输入的法语句子先编码

49
00:03:08,400 --> 00:03:13,860
再然后用解码器将来产生相应的英语翻译

50
00:03:13,860 --> 00:03:18,630
同样相似的结构也可以用在图像标注

51
00:03:18,630 --> 00:03:24,060
比如给出这样一张照片

52
00:03:24,060 --> 00:03:29,260
也许希望可以自动辨识出这张图里，一只猫坐在椅子上

53
00:03:29,260 --> 00:03:32,790
那么你该如何构建一个神经网络，通过输入一张图片

54
00:03:32,790 --> 00:03:38,405
再输出一段辨识文字：
a cat sitting on a chair(一只猫坐在椅子上)

55
00:03:38,405 --> 00:03:43,715
你可以这样做：在之前讲卷积网络时

56
00:03:43,715 --> 00:03:48,453
你学会了如何输入一张图片到卷积网络中去

57
00:03:48,453 --> 00:03:50,555
也许是预训练好的AlexNet

58
00:03:50,555 --> 00:03:56,360
并且AlexNet能够学习输入图片的编码结果或一系列特征

59
00:03:56,360 --> 00:03:57,440
所以

60
00:03:57,440 --> 00:04:04,295
这是AlexNet模型，如果我们不要这最后的Softmax部分

61
00:04:04,295 --> 00:04:06,770
预训练好的AlexNet模型最后可以得到

62
00:04:06,770 --> 00:04:12,705
一个4096维的特征向量来表达这张关于猫的图像

63
00:04:12,705 --> 00:04:18,760
现在这个预训练好的网络可以作为该图片的编码器网路部分

64
00:04:18,760 --> 00:04:25,120
而你将有一个4096维的向量来表示这张图

65
00:04:25,120 --> 00:04:28,870
你可以将这个向量喂入RNN模型

66
00:04:28,870 --> 00:04:36,878
其作用是每次产生一个说明文字

67
00:04:36,878 --> 00:04:43,985
类似于我们看到的机器翻译，从法语到英语的翻译

68
00:04:43,985 --> 00:04:50,590
你现在可以输入一个特征向量来描述输入

69
00:04:50,590 --> 00:04:58,180
随后产生一个输出集，或者说一个文字输出序列
并且是每次一个单词

70
00:04:58,180 --> 00:05:01,941
这个方法对图像标识而言，效果不错

71
00:05:01,941 --> 00:05:04,610
尤其是当产生的说明文字不长的时候

72
00:05:04,610 --> 00:05:07,730
据我所知

73
00:05:07,730 --> 00:05:10,115
这个模型一开始是由

74
00:05:10,115 --> 00:05:12,731
Junhua Mao, Wei Xu, Yi Yang, 
Jiang Wang, Zhiheng Huang,

75
00:05:12,731 --> 00:05:18,185
和Alan Yuille提出

76
00:05:18,185 --> 00:05:20,030
尽管有很多研究组互相独立，

77
00:05:20,030 --> 00:05:25,080
同时独立提出了相似的模型

78
00:05:25,080 --> 00:05:29,560
另外两个研究组在同一时间做了相似的工作

79
00:05:29,560 --> 00:05:33,260
他们是Oriol Vinyals

80
00:05:33,260 --> 00:05:35,025
Alexander Toshev, Samy Bengio

81
00:05:35,025 --> 00:05:38,750
Dumitru Erhan, Andrej Karpathy, 李菲菲

82
00:05:38,750 --> 00:05:45,040
现在你已经看到一个基本的Seq2seq模型是如何运行的

83
00:05:45,040 --> 00:05:49,340
以及基本的图像-序列（图像标识）模型是如何建立的

84
00:05:49,340 --> 00:05:53,460
但在你跑这样一个模型来产生一个序列

85
00:05:53,460 --> 00:05:56,550
和用语言模型合成文本相比，

86
00:05:56,550 --> 00:06:00,495
仍然有很大的区别

87
00:06:00,495 --> 00:06:02,050
关键区别之一是：

88
00:06:02,050 --> 00:06:04,887
你不希望输出一个随意的翻译

89
00:06:04,887 --> 00:06:07,626
你希望输出一个你想要的翻译结果

90
00:06:07,626 --> 00:06:10,345
或者说你不希望输出一个随意的图片标识

91
00:06:10,345 --> 00:06:13,870
你希望输出一个最好，或者说最符合的标识

92
00:06:13,870 --> 00:06:18,000
下节课我将告诉你，你该如何获得你想要的结果
GTC字幕组翻译