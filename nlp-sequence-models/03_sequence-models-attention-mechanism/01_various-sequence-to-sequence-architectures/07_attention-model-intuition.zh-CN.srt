1
00:00:00,000 --> 00:00:01,910
本周大部分时间,

2
00:00:01,910 --> 00:00:07,100
大家都在用Encoder-Decoder框架
解决机器翻译问题

3
00:00:07,100 --> 00:00:11,805
也就是一个RNN读入输入语句，
一个RNN输出翻译后的语句

4
00:00:11,805 --> 00:00:15,538
而注意力模型（attention model）

5
00:00:15,538 --> 00:00:18,425
是这个模型的一个改进。

6
00:00:18,425 --> 00:00:21,050
注意力模型，

7
00:00:21,050 --> 00:00:23,855
是深度学习里最具影响力的思想之一。

8
00:00:23,855 --> 00:00:26,410
让我们看看它如何工作的。

9
00:00:26,410 --> 00:00:29,770
比如说，我们有一个很长的法语句子，

10
00:00:29,770 --> 00:00:33,800
我们想让网络中绿色的
编码器做的是

11
00:00:33,800 --> 00:00:36,230
读取并记住整个句子

12
00:00:36,230 --> 00:00:40,755
并存储在激励函数里

13
00:00:40,755 --> 00:00:42,670
对于紫色的网络

14
00:00:42,670 --> 00:00:44,720
解码网络

15
00:00:44,720 --> 00:00:47,727
生成英语翻译

16
00:00:47,727 --> 00:00:50,702
“Jane去年九月去了非洲，很享受那里的文化也结识了很多
很棒的人“

17
00:00:50,702 --> 00:00:52,355
“她回来之后激动的说她的旅途有多美妙，“

18
00:00:52,355 --> 00:00:53,687
“并且诱惑我也去。“

19
00:00:53,687 --> 00:00:56,855
然而，人类翻译的方法

20
00:00:56,855 --> 00:01:00,260
并不是先读整个法语句子

21
00:01:00,260 --> 00:01:02,390
并且记住整个句子

22
00:01:02,390 --> 00:01:06,215
然后返回一个整个的句子，

23
00:01:06,215 --> 00:01:11,686
而是先读一部分

24
00:01:11,686 --> 00:01:14,430
翻译出一部分

25
00:01:14,430 --> 00:01:17,270
再看第二部分，再翻译一部分

26
00:01:17,270 --> 00:01:18,960
再看，再翻译

27
00:01:18,960 --> 00:01:20,305
反复如此

28
00:01:20,305 --> 00:01:23,105
人们会一部分一部分地翻译整个句子

29
00:01:23,105 --> 00:01:29,041
因为记住整个句子太难了。

30
00:01:29,041 --> 00:01:34,835
所以像在这里看到上面的 encoder-decoder结构，

31
00:01:34,835 --> 00:01:37,760
对于短句子来说，这个模型表现的很好，

32
00:01:37,760 --> 00:01:40,225
所以我们会得到一个相对较高的Bleu 分数

33
00:01:40,225 --> 00:01:43,215
但是对于长句子来说，

34
00:01:43,215 --> 00:01:45,440
比如说大于30，40个单词的句子，

35
00:01:45,440 --> 00:01:47,320
这个模型的表现就会下降。

36
00:01:47,320 --> 00:01:51,346
Bleu分数与句子长短的关系可能看起来是这样的

37
00:01:51,346 --> 00:01:56,219
非常短的句子是很难翻译的,

38
00:01:56,219 --> 00:01:59,015
很难把所有单词都翻译准确

39
00:01:59,015 --> 00:02:03,080
长句也不好翻译,是因为

40
00:02:03,080 --> 00:02:07,138
你的网络很难把一个特别长的句子记忆下来。

41
00:02:07,138 --> 00:02:08,990
在这个视频以及下一段视频中，

42
00:02:08,990 --> 00:02:14,705
你将看到注意力模型也许更像一个真正的人可能会采用的翻译方法，

43
00:02:14,705 --> 00:02:19,030
注意力模型在某个时间点只看句子的一部份，

44
00:02:19,030 --> 00:02:22,508
（使用注意力模型的）机器翻译系统的性能看上去是这样的

45
00:02:22,508 --> 00:02:26,255
因为在一个时间点只针对句子的一部份进行处理，

46
00:02:26,255 --> 00:02:29,000
你不会看到这个大下降

47
00:02:29,000 --> 00:02:32,060
因为这其实是对神经网络记忆能力的度量。

48
00:02:32,060 --> 00:02:38,606
而记忆长句子，并不是我们最想要让神经网络来做的事情

49
00:02:38,606 --> 00:02:43,325
这段视频中，我将给你们一些直观的理解，关于

50
00:02:43,325 --> 00:02:49,149
注意力模型是如何工作的。
而在之后的视频中我将对模型的细节详细说明。

51
00:02:49,149 --> 00:02:53,190
注意力模型由以下几位发明：
Dimitri,Bahdanau,Camcrun Cho,

52
00:02:53,190 --> 00:03:00,060
Yoshe Bengio,虽然它主要是用于机器翻译，

53
00:03:00,060 --> 00:03:03,685
但它如今也在其他应用被使用。

54
00:03:03,685 --> 00:03:06,480
这个模型确实非常有影响力。

55
00:03:06,480 --> 00:03:10,580
我认为这是深度学习理论中有深远意义的一篇论文。（模型最初以论文形式提出）

56
00:03:10,580 --> 00:03:14,385
用一个短句的翻译来说明，

57
00:03:14,385 --> 00:03:18,840
虽然这些模型更多是为长句子开发的，

58
00:03:18,840 --> 00:03:22,440
还是用些简单的例子更容易说明问题。

59
00:03:22,440 --> 00:03:24,165
这是我们常见的(法语)句式，

60
00:03:24,165 --> 00:03:26,470
“Jane visite l'Afrique en Septembre.”

61
00:03:26,470 --> 00:03:30,200
比方我们使用RNN

62
00:03:30,200 --> 00:03:34,140
比如使用一个双向RNN,

63
00:03:34,140 --> 00:03:37,845
为了为每一个输入的字符计算一些特征，

64
00:03:37,845 --> 00:03:42,925
并且你需要理解到，

65
00:03:42,925 --> 00:03:46,555
双向RNN输出Y1，Y2

66
00:03:46,555 --> 00:03:51,240
一直到Y5,但是我们并不需要逐字翻译,

67
00:03:51,240 --> 00:03:54,497
让我去掉上面的Y

68
00:03:54,497 --> 00:03:56,220
但是当我们使用双向RNN网络时

69
00:03:56,220 --> 00:03:59,580
我们在做的事情实际上

70
00:03:59,580 --> 00:04:02,528
是针对句子中五个不同的位置

71
00:04:02,528 --> 00:04:06,475
计算出一组丰富的

72
00:04:06,475 --> 00:04:12,060
关于句子中每个词以及它们周围词的特征

73
00:04:12,060 --> 00:04:17,125
现在，我们将生成英文翻译

74
00:04:17,125 --> 00:04:21,690
我们会用另外一个RNN网络去生成英文翻译

75
00:04:21,690 --> 00:04:29,055
这里是我的RNN标记，<br />并且这次我会用一个不同的标记（而不是常用的A）

76
00:04:29,055 --> 00:04:32,830
以免和这里的激活因子混淆

77
00:04:32,830 --> 00:04:34,605
所以我会用另外一个标记

78
00:04:34,605 --> 00:04:39,915
我要用S去标记上面这个RNN的隐藏状态

79
00:04:39,915 --> 00:04:45,270
所以我会写S1，而不是A1

80
00:04:45,270 --> 00:04:50,750
我们希望在这个模型中第一个生成的词是Jane

81
00:04:50,750 --> 00:04:54,277
并且整句话是：Jane visits Africa in September

82
00:04:54,277 --> 00:04:57,120
那么现在问题来了

83
00:04:57,120 --> 00:04:59,835
当你想要去生成第一个词时

84
00:04:59,835 --> 00:05:05,720
你应该去看下面这个法语句子中的哪一个部分呢？

85
00:05:05,720 --> 00:05:07,960
看起来你应该先关注第一个词

86
00:05:07,960 --> 00:05:11,385
以及它周围的几个词吧

87
00:05:11,385 --> 00:05:15,100
但是你不用去关注句子末尾的那些词

88
00:05:15,100 --> 00:05:18,875
注意力模型会计算的

89
00:05:18,875 --> 00:05:25,350
是一系列注意力权重(参数)，我们会一用alpha(1,1)去标记

90
00:05:25,350 --> 00:05:29,670
你生成的第一个词时

91
00:05:29,670 --> 00:05:36,145
你需要多少注意力在这里的第一个信息

92
00:05:36,145 --> 00:05:41,525
然后我们会生成第二个注意力权重(参数)

93
00:05:41,525 --> 00:05:46,360
Alpha(1,2)，该权重将告诉我们

94
00:05:46,360 --> 00:05:51,740
在第二个词上放多少注意力

95
00:05:51,740 --> 00:05:57,050
以此类推到第三个词，等等

96
00:05:57,050 --> 00:06:02,147
然后这些一起会准确告诉我们

97
00:06:02,147 --> 00:06:08,610
我们应该放多少注意力在这个位置所对应的句子上下文语境C

98
00:06:08,610 --> 00:06:14,990
这就是这个RNN网络的输入

99
00:06:14,990 --> 00:06:16,590
这就是RNN网络的一个步骤

100
00:06:16,590 --> 00:06:19,880
我们会在下一个部分中详细讲解

101
00:06:19,880 --> 00:06:23,366
那么这个RNN网络的第二步

102
00:06:23,366 --> 00:06:25,755
我们将会有一个新的隐藏状态S2

103
00:06:25,755 --> 00:06:31,585
以及一系列新的注意力权重(参数)

104
00:06:31,585 --> 00:06:38,325
我们将会有Alpha(2,1)，它会告诉生成第二个词的一个权重

105
00:06:38,325 --> 00:06:42,065
这个代表的可能是visite这个词

106
00:06:42,065 --> 00:06:47,760
并且alpha(2,1)会告诉我们
到底需要放多少注意力在第一个词身上

107
00:06:47,760 --> 00:06:50,880
然后和之前一样, 一直到.....alpha(2,2)等等

108
00:06:50,880 --> 00:06:53,020
我们应该放多少注意力在visite这个词上面

109
00:06:53,020 --> 00:06:55,851
以及应该在free上花多少时间

110
00:06:55,851 --> 00:07:01,054
当然啦，我们生成的第一个词Jane也是这个注意力权重的输入

111
00:07:01,054 --> 00:07:05,850
然后我们的注意力权重会对上下文有一个大致了解

112
00:07:05,850 --> 00:07:09,646
第二步它也会是一个输入，用于帮助生成第二个词

113
00:07:09,646 --> 00:07:14,930
然后紧接着它带我们进入第三步，S3

114
00:07:14,930 --> 00:07:19,700
这里我们有新的输入词以及新的

115
00:07:19,700 --> 00:07:24,730
基于alpha3的上下文环境

116
00:07:24,730 --> 00:07:27,860
然后它进一步会告诉我们应该放

117
00:07:27,860 --> 00:07:32,633
多少关注在新的法语输入词上面

118
00:07:32,633 --> 00:07:35,365
我可能还没有明确讲太多

119
00:07:35,365 --> 00:07:38,650
但是下一个视频中我会更详细地讲解

120
00:07:38,650 --> 00:07:43,750
什么是上下文环境以及针对第三个词

121
00:07:43,750 --> 00:07:46,300
应该有一个什么样的上下文环境

122
00:07:46,300 --> 00:07:50,720
可能我们需要看原句中的这一部分吧

123
00:07:50,720 --> 00:07:55,710
生成这个上下文环境C的公式

124
00:07:55,710 --> 00:08:01,177
以及你计算注意力权重的方法会在下一个视屏中讲解

125
00:08:01,177 --> 00:08:05,000
并且在下一个视频中，你会看到alpha3,t

126
00:08:05,000 --> 00:08:07,300
这是你在生成第三个词时出现的

127
00:08:07,300 --> 00:08:11,390
我猜这可能是African，就是想猜一下正确的输出

128
00:08:11,390 --> 00:08:16,306
RNN的这一步当中

129
00:08:16,306 --> 00:08:21,640
在时间t时需要对法语词所付出的注意力

130
00:08:21,640 --> 00:08:27,749
取决于这个双向RNN在时间t的激活值

131
00:08:27,749 --> 00:08:32,677
我猜这也可能基于第四个激活值

132
00:08:32,677 --> 00:08:37,900
以及在时间t的反向激活值，还有前一步的状态

133
00:08:37,900 --> 00:08:39,420
以及S2，这些会共同影响

134
00:08:39,420 --> 00:08:47,640
法语句子中不同词语的注意力值

135
00:08:47,640 --> 00:08:49,250
但是我们在下一个视频中都会详细讲解

136
00:08:49,250 --> 00:08:52,775
这里最重要的知识点就是

137
00:08:52,775 --> 00:08:58,510
通过这种架构，RNN会向前每次生成一个词

138
00:08:58,510 --> 00:09:04,275
直到生成句子末尾，并且每一步

139
00:09:04,275 --> 00:09:06,445
都会包含这些注意力权重

140
00:09:06,445 --> 00:09:09,100
Alpha,t,t'，这些会告诉你

141
00:09:09,100 --> 00:09:11,340
在你生成第t个英文词句时

142
00:09:11,340 --> 00:09:16,915
你应该在第t'个法语词上放多少注意力

143
00:09:16,915 --> 00:09:20,710
这使得我们在我们翻译句子时

144
00:09:20,710 --> 00:09:24,835
我们的模型在每一步都可以

145
00:09:24,835 --> 00:09:28,135
只关注局部一小部分的法语词

146
00:09:28,135 --> 00:09:31,210
我希望这个视频可以传达一些

147
00:09:31,210 --> 00:09:34,825
关于注意力权重的想法，我们现在有了一个

148
00:09:34,825 --> 00:09:36,590
关于这个算法如何运行的大概的感觉

149
00:09:36,590 --> 00:09:41,000
让我们进入下一个视频去看注意力模型具体是如何运行的