本周大部分时间, 大家都在用Encoder-Decoder框架
解决机器翻译问题 也就是一个RNN读入输入语句，
一个RNN输出翻译后的语句 而注意力模型（attention model） 是这个模型的一个改进。 注意力模型， 是深度学习里最具影响力的思想之一。 让我们看看它如何工作的。 比如说，我们有一个很长的法语句子， 我们想让网络中绿色的
编码器做的是 读取并记住整个句子 并存储在激励函数里 对于紫色的网络 解码网络 生成英语翻译 “Jane去年九月去了非洲，很享受那里的文化也结识了很多
很棒的人“ “她回来之后激动的说她的旅途有多美妙，“ “并且诱惑我也去。“ 然而，人类翻译的方法 并不是先读整个法语句子 并且记住整个句子 然后返回一个整个的句子， 而是先读一部分 翻译出一部分 再看第二部分，再翻译一部分 再看，再翻译 反复如此 人们会一部分一部分地翻译整个句子 因为记住整个句子太难了。 所以像在这里看到上面的 encoder-decoder结构， 对于短句子来说，这个模型表现的很好， 所以我们会得到一个相对较高的Bleu 分数 但是对于长句子来说， 比如说大于30，40个单词的句子， 这个模型的表现就会下降。 Bleu分数与句子长短的关系可能看起来是这样的 非常短的句子是很难翻译的, 很难把所有单词都翻译准确 长句也不好翻译,是因为 你的网络很难把一个特别长的句子记忆下来。 在这个视频以及下一段视频中， 你将看到注意力模型也许更像一个真正的人可能会采用的翻译方法， 注意力模型在某个时间点只看句子的一部份， （使用注意力模型的）机器翻译系统的性能看上去是这样的 因为在一个时间点只针对句子的一部份进行处理， 你不会看到这个大下降 因为这其实是对神经网络记忆能力的度量。 而记忆长句子，并不是我们最想要让神经网络来做的事情 这段视频中，我将给你们一些直观的理解，关于 注意力模型是如何工作的。
而在之后的视频中我将对模型的细节详细说明。 注意力模型由以下几位发明：
Dimitri,Bahdanau,Camcrun Cho, Yoshe Bengio,虽然它主要是用于机器翻译， 但它如今也在其他应用被使用。 这个模型确实非常有影响力。 我认为这是深度学习理论中有深远意义的一篇论文。（模型最初以论文形式提出） 用一个短句的翻译来说明， 虽然这些模型更多是为长句子开发的， 还是用些简单的例子更容易说明问题。 这是我们常见的(法语)句式， “Jane visite l'Afrique en Septembre.” 比方我们使用RNN 比如使用一个双向RNN, 为了为每一个输入的字符计算一些特征， 并且你需要理解到， 双向RNN输出Y1，Y2 一直到Y5,但是我们并不需要逐字翻译, 让我去掉上面的Y 但是当我们使用双向RNN网络时 我们在做的事情实际上 是针对句子中五个不同的位置 计算出一组丰富的 关于句子中每个词以及它们周围词的特征 现在，我们将生成英文翻译 我们会用另外一个RNN网络去生成英文翻译 这里是我的RNN标记，<br />并且这次我会用一个不同的标记（而不是常用的A） 以免和这里的激活因子混淆 所以我会用另外一个标记 我要用S去标记上面这个RNN的隐藏状态 所以我会写S1，而不是A1 我们希望在这个模型中第一个生成的词是Jane 并且整句话是：Jane visits Africa in September 那么现在问题来了 当你想要去生成第一个词时 你应该去看下面这个法语句子中的哪一个部分呢？ 看起来你应该先关注第一个词 以及它周围的几个词吧 但是你不用去关注句子末尾的那些词 注意力模型会计算的 是一系列注意力权重(参数)，我们会一用alpha(1,1)去标记 你生成的第一个词时 你需要多少注意力在这里的第一个信息 然后我们会生成第二个注意力权重(参数) Alpha(1,2)，该权重将告诉我们 在第二个词上放多少注意力 以此类推到第三个词，等等 然后这些一起会准确告诉我们 我们应该放多少注意力在这个位置所对应的句子上下文语境C 这就是这个RNN网络的输入 这就是RNN网络的一个步骤 我们会在下一个部分中详细讲解 那么这个RNN网络的第二步 我们将会有一个新的隐藏状态S2 以及一系列新的注意力权重(参数) 我们将会有Alpha(2,1)，它会告诉生成第二个词的一个权重 这个代表的可能是visite这个词 并且alpha(2,1)会告诉我们
到底需要放多少注意力在第一个词身上 然后和之前一样, 一直到.....alpha(2,2)等等 我们应该放多少注意力在visite这个词上面 以及应该在free上花多少时间 当然啦，我们生成的第一个词Jane也是这个注意力权重的输入 然后我们的注意力权重会对上下文有一个大致了解 第二步它也会是一个输入，用于帮助生成第二个词 然后紧接着它带我们进入第三步，S3 这里我们有新的输入词以及新的 基于alpha3的上下文环境 然后它进一步会告诉我们应该放 多少关注在新的法语输入词上面 我可能还没有明确讲太多 但是下一个视频中我会更详细地讲解 什么是上下文环境以及针对第三个词 应该有一个什么样的上下文环境 可能我们需要看原句中的这一部分吧 生成这个上下文环境C的公式 以及你计算注意力权重的方法会在下一个视屏中讲解 并且在下一个视频中，你会看到alpha3,t 这是你在生成第三个词时出现的 我猜这可能是African，就是想猜一下正确的输出 RNN的这一步当中 在时间t时需要对法语词所付出的注意力 取决于这个双向RNN在时间t的激活值 我猜这也可能基于第四个激活值 以及在时间t的反向激活值，还有前一步的状态 以及S2，这些会共同影响 法语句子中不同词语的注意力值 但是我们在下一个视频中都会详细讲解 这里最重要的知识点就是 通过这种架构，RNN会向前每次生成一个词 直到生成句子末尾，并且每一步 都会包含这些注意力权重 Alpha,t,t'，这些会告诉你 在你生成第t个英文词句时 你应该在第t'个法语词上放多少注意力 这使得我们在我们翻译句子时 我们的模型在每一步都可以 只关注局部一小部分的法语词 我希望这个视频可以传达一些 关于注意力权重的想法，我们现在有了一个 关于这个算法如何运行的大概的感觉 让我们进入下一个视频去看注意力模型具体是如何运行的