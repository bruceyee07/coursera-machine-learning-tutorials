1
00:00:00,000 --> 00:00:05,525
序列与序列机器翻译模型与

2
00:00:05,525 --> 00:00:11,058
本课程第一周所了解到的语言模型,
有相似之处

3
00:00:11,058 --> 00:00:14,060
但同时也存在着一些显著的差异。

4
00:00:14,060 --> 00:00:16,335
让我们来看一看，你可以

5
00:00:16,335 --> 00:00:20,625
将机器翻译看做是建立有条件的语言模型

6
00:00:20,625 --> 00:00:23,378
我的意思是,在语言建模中,

7
00:00:23,378 --> 00:00:27,473
这是我们在第一周中建立的网络。

8
00:00:27,473 --> 00:00:35,350
这个模型可以估算一个句子的概率。

9
00:00:35,350 --> 00:00:38,175
这就是语言模型的功能。

10
00:00:38,175 --> 00:00:42,235
你也可以用它来产生新的句子,

11
00:00:42,235 --> 00:00:46,450
有时候，当你在这里写 x1 和 x2 时,

12
00:00:46,450 --> 00:00:47,740
在这个例子中,

13
00:00:47,740 --> 00:00:53,210
x2 等于 y1 或等于 y^, 这只是个反馈。

14
00:00:53,210 --> 00:00:56,040
但 x1、x2 等等都不重要。

15
00:00:56,040 --> 00:00:57,950
那就为了这张幻灯片清理一下

16
00:00:57,950 --> 00:00:59,800
我要把这些都划去

17
00:00:59,800 --> 00:01:02,660
X1可以是全零向量，x2向量

18
00:01:02,660 --> 00:01:06,995
x3都只是之前你生成的输出。

19
00:01:06,995 --> 00:01:09,243
这就是语言模型。

20
00:01:09,243 --> 00:01:12,315
机器翻译模型如下所示,

21
00:01:12,315 --> 00:01:14,200
我要用几种不同的颜色,

22
00:01:14,200 --> 00:01:17,490
绿色和紫色,分别表示

23
00:01:17,490 --> 00:01:22,369
编码网络（绿色）和解码网络（紫色）

24
00:01:22,369 --> 00:01:27,820
你会注意到解码网络看起来

25
00:01:27,820 --> 00:01:33,938
与我们上面的语言模型相似。

26
00:01:33,938 --> 00:01:35,520
所以机器翻译模型

27
00:01:35,520 --> 00:01:38,715
与语言模型非常相似,

28
00:01:38,715 --> 00:01:43,690
但是，它不总是以全零向量开始,

29
00:01:43,690 --> 00:01:46,385
而是有一个编码的网络

30
00:01:46,385 --> 00:01:49,960
它算出了输入句子的一些表示,

31
00:01:49,960 --> 00:01:54,425
它处理输入句,并在启动解码网络时

32
00:01:54,425 --> 00:02:01,915
使用输入句子的表示
而不是以全零向量。

33
00:02:01,915 --> 00:02:07,355
因此，这就是为什么
我称之为有条件的语言模型,

34
00:02:07,355 --> 00:02:11,345
而非以所有句子的概率去建模,

35
00:02:11,345 --> 00:02:14,790
它现在建模的是

36
00:02:14,790 --> 00:02:17,490
输出的英语翻译的概率,

37
00:02:17,490 --> 00:02:22,745
在一些法语句子输入的条件下

38
00:02:22,745 --> 00:02:28,780
换句话说,就是你正在试图估计出英语翻译的可能性。

39
00:02:28,780 --> 00:02:33,795
比如,翻译成Jane is visiting Africa in September（"简在9月要访问非洲"）,的可能性

40
00:02:33,795 --> 00:02:38,870
但在输入法语审查的条件下,

41
00:02:38,870 --> 00:02:42,271
"Jane visite I'Afrique en septembre."
（法语Jane is visiting Africa in September
（"简在9月要访问非洲"））

42
00:02:42,271 --> 00:02:46,830
那么，这才是翻译为英语句子的概率

43
00:02:46,830 --> 00:02:51,710
在输入是法语句子的条件下,
这就是为什么它是一个有条件的语言模型。

44
00:02:51,710 --> 00:02:54,830
现在,如果你想把这个模型应用到实际中

45
00:02:54,830 --> 00:02:58,790
把法语的一句话翻译成英语,

46
00:02:58,790 --> 00:03:02,045
有了这个法语句子作为输入,

47
00:03:02,045 --> 00:03:05,285
模型可能会告诉你

48
00:03:05,285 --> 00:03:08,550
英语翻译的概率差异。

49
00:03:08,550 --> 00:03:11,900
俺么，x是法语句,

50
00:03:11,900 --> 00:03:13,669
"Jane visite l'Afrique en septembre."（法语）
Jane is visiting Africa in September
（"简在9月要访问非洲"）

51
00:03:13,669 --> 00:03:17,330
现在,这会告诉你

52
00:03:17,330 --> 00:03:22,058
法语句不同英语翻译的概率。

53
00:03:22,058 --> 00:03:28,235
你需要的不是随机抽样输出。

54
00:03:28,235 --> 00:03:31,970
如果你从这个分布中取样的话,

55
00:03:31,970 --> 00:03:36,343
P(y|x),也许某一次你得到的翻译质量很好

56
00:03:36,343 --> 00:03:38,081
"Jane is visiting Africa in September.""简在9月访问非洲。”

57
00:03:38,081 --> 00:03:40,850
但是,也许下一次你的翻译就会不一样,

58
00:03:40,850 --> 00:03:42,770
"Jane is going to be visiting Africa in September. "
"简将在9月访问非洲。"

59
00:03:42,770 --> 00:03:46,805
听起来有点尴尬,但不是个非常差的翻译,

60
00:03:46,805 --> 00:03:48,120
只是不是最好的翻译方式。

61
00:03:48,120 --> 00:03:49,970
有时,偶尔,

62
00:03:49,970 --> 00:03:52,432
你会得到："In September,"在9月,

63
00:03:52,432 --> 00:03:54,055
Jane will visit Africa."简将访问非洲。”

64
00:03:54,055 --> 00:03:55,580
也许只是凑巧了

65
00:03:55,580 --> 00:03:57,930
有时会合成一个非常糟糕的翻译:

66
00:03:57,930 --> 00:04:00,055
"Her African friend welcomed Jane in September."
"她的非洲朋友欢迎简在9月。”

67
00:04:00,055 --> 00:04:04,475
因此，当你使用这个模型进行机器翻译时,

68
00:04:04,475 --> 00:04:08,810
你不是要从这个分布随机抽样。

69
00:04:08,810 --> 00:04:13,130
而是找到英语句子y,

70
00:04:13,130 --> 00:04:16,910
使条件概率最大化。

71
00:04:16,910 --> 00:04:20,660
因此,在开发机器翻译系统时,

72
00:04:20,660 --> 00:04:25,910
你需要做的一件事就是给出一个算法,找到

73
00:04:25,910 --> 00:04:31,805
y的值,把它在这里最大化。

74
00:04:31,805 --> 00:04:34,810
最常用的算法是beam search（定向搜索）,

75
00:04:34,810 --> 00:04:37,565
你在下一个视频中会看到相关内容。

76
00:04:37,565 --> 00:04:39,730
但是,在学习beam search之前,

77
00:04:39,730 --> 00:04:43,700
你可能在想为什么不用greedy search（贪婪搜索机制）？
那么,什么是greedy search（贪婪搜索机制）呢？

78
00:04:43,700 --> 00:04:49,310
greedy search（贪婪搜索机制）是计算机科学的一种算法,

79
00:04:49,310 --> 00:04:50,960
就是第一个字挑的是可能性最大的单词

80
00:04:50,960 --> 00:04:55,390
基于你的有条件的语言模型。

81
00:04:55,390 --> 00:05:01,160
进入机器翻译模型,选择了第一个单词之后,

82
00:05:01,160 --> 00:05:04,685
然后挑出可能性最高的第二个单词,

83
00:05:04,685 --> 00:05:07,335
然后选出可能性最高的第三个单词。

84
00:05:07,335 --> 00:05:10,685
这种算法称为greedy search（贪婪搜索机制）。

85
00:05:10,685 --> 00:05:16,759
而且,你真正要的是选择整个序列的单词,y1,y2,

86
00:05:16,759 --> 00:05:21,973
到y^Ty,

87
00:05:21,973 --> 00:05:27,705
使整个句子的联合概率最大化。

88
00:05:27,705 --> 00:05:30,176
事实证明,贪婪搜索机制,

89
00:05:30,176 --> 00:05:31,620
就是选出最佳的第一个单词

90
00:05:31,620 --> 00:05:34,280
在选择后,

91
00:05:34,280 --> 00:05:36,104
试着选出最好的第二个单词,

92
00:05:36,104 --> 00:05:37,395
然后

93
00:05:37,395 --> 00:05:39,318
试着选出最好的第三个单词,

94
00:05:39,318 --> 00:05:41,545
这种方法真的行不通。

95
00:05:41,545 --> 00:05:44,990
为了证明这一点,让我们考虑以下两个翻译。

96
00:05:44,990 --> 00:05:46,920
第一个是更好的翻译,

97
00:05:46,920 --> 00:05:50,610
所以希望在我们的机器翻译模型中,

98
00:05:50,610 --> 00:05:56,330
在给定x时更大时
p(y)是第一个句子的概率

99
00:05:56,330 --> 00:05:59,907
这是一个较好的,更简洁的法语翻译。

100
00:05:59,907 --> 00:06:02,394
第二个翻译也不是很差,

101
00:06:02,394 --> 00:06:03,665
它只是更啰嗦

102
00:06:03,665 --> 00:06:05,970
有不必要的词。

103
00:06:05,970 --> 00:06:10,485
但是,如果算法
选择了"jane is"作为前两个字,

104
00:06:10,485 --> 00:06:14,298
因为"going"是一个更常见的英语单词,

105
00:06:14,298 --> 00:06:21,930
对于特定的法语输入，
"jane is going,"的可能性

106
00:06:21,930 --> 00:06:26,610
实际上概率是高于"Jane is visiting“

107
00:06:26,610 --> 00:06:32,710
在这个法语句子中。

108
00:06:32,710 --> 00:06:35,760
所以,如果你只是

109
00:06:35,760 --> 00:06:40,215
在前三个单词最大的概率基础上,
去选择第三个单词

110
00:06:40,215 --> 00:06:43,670
最后很有可能会选择第二个翻译。

111
00:06:43,670 --> 00:06:50,710
但这最终会得出一个不太理想的句子

112
00:06:50,710 --> 00:06:55,325
在这个模型中，
给定 x 时，由 y 的可能性 p 来比较

113
00:06:55,325 --> 00:07:01,280
得出了一个不太理想的翻译，
这可能是一个的论点

114
00:07:01,280 --> 00:07:05,110
但是,这是一个更广泛的现象例子,

115
00:07:05,110 --> 00:07:08,860
如果你想找到单词序列, y1, y2,

116
00:07:08,860 --> 00:07:13,822
一直到最后一个词,使联合概率最大化,

117
00:07:13,822 --> 00:07:17,845
每次只选一个单词并不总是最好的。

118
00:07:17,845 --> 00:07:21,730
当然,英语句子中的单词组合总数

119
00:07:21,730 --> 00:07:25,660
是指数级别的。

120
00:07:25,660 --> 00:07:30,365
所以,如果你只有1万字的字典,然后你

121
00:07:30,365 --> 00:07:35,211
要进行长达十个单词的翻译,

122
00:07:35,211 --> 00:07:42,085
然后有10000的10次方这些可能的句子
,是十个单词。

123
00:07:42,085 --> 00:07:44,955
从词汇量中提取单词,

124
00:07:44,955 --> 00:07:47,970
字典的大小为10000词。

125
00:07:47,970 --> 00:07:51,260
所以,句子空间的可能性是巨大的,

126
00:07:51,260 --> 00:07:53,455
对所有可能评分,是不可能的

127
00:07:53,455 --> 00:08:00,880
这就是为什么最常见的事情是
使用一个近似搜索出来。

128
00:08:00,880 --> 00:08:02,540
而且,近似搜索算法所做的,

129
00:08:02,540 --> 00:08:03,724
是它会尝试,

130
00:08:03,724 --> 00:08:05,135
但不会总是成功,

131
00:08:05,135 --> 00:08:07,737
它会选择

132
00:08:07,737 --> 00:08:11,900
使条件概率最大化的句子 y 。

133
00:08:11,900 --> 00:08:16,925
而且, 即使它不保证找出 y ,
使概率最大化,

134
00:08:16,925 --> 00:08:19,025
但它通常表现不错。

135
00:08:19,025 --> 00:08:20,570
综上所述, 在这个视频中,

136
00:08:20,570 --> 00:08:26,430
您看到了如何将机器翻译作为条件语言建模问题来考虑。

137
00:08:26,430 --> 00:08:28,880
但这两者之间的一个主要区别

138
00:08:28,880 --> 00:08:31,460
早期的语言建模问题是

139
00:08:31,460 --> 00:08:34,313
随机生成一个句子,

140
00:08:34,313 --> 00:08:37,910
而不是试着找到最有可能的英语句子,

141
00:08:37,910 --> 00:08:40,130
最有可能的英语翻译。

142
00:08:40,130 --> 00:08:43,655
但一定长度的所有英语句子的集合

143
00:08:43,655 --> 00:08:47,420
太大, 无法详尽地列举出。

144
00:08:47,420 --> 00:08:51,155
所以, 我们必须求助于搜索算法。

145
00:08:51,155 --> 00:08:53,369
所以, 让我们去到下一个视频,

146
00:08:53,369 --> 00:08:56,000
你将学习波束搜索算法。