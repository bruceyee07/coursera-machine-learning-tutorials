序列与序列机器翻译模型与 本课程第一周所了解到的语言模型,
有相似之处 但同时也存在着一些显著的差异。 让我们来看一看，你可以 将机器翻译看做是建立有条件的语言模型 我的意思是,在语言建模中, 这是我们在第一周中建立的网络。 这个模型可以估算一个句子的概率。 这就是语言模型的功能。 你也可以用它来产生新的句子, 有时候，当你在这里写 x1 和 x2 时, 在这个例子中, x2 等于 y1 或等于 y^, 这只是个反馈。 但 x1、x2 等等都不重要。 那就为了这张幻灯片清理一下 我要把这些都划去 X1可以是全零向量，x2向量 x3都只是之前你生成的输出。 这就是语言模型。 机器翻译模型如下所示, 我要用几种不同的颜色, 绿色和紫色,分别表示 编码网络（绿色）和解码网络（紫色） 你会注意到解码网络看起来 与我们上面的语言模型相似。 所以机器翻译模型 与语言模型非常相似, 但是，它不总是以全零向量开始, 而是有一个编码的网络 它算出了输入句子的一些表示, 它处理输入句,并在启动解码网络时 使用输入句子的表示
而不是以全零向量。 因此，这就是为什么
我称之为有条件的语言模型, 而非以所有句子的概率去建模, 它现在建模的是 输出的英语翻译的概率, 在一些法语句子输入的条件下 换句话说,就是你正在试图估计出英语翻译的可能性。 比如,翻译成Jane is visiting Africa in September（"简在9月要访问非洲"）,的可能性 但在输入法语审查的条件下, "Jane visite I'Afrique en septembre."
（法语Jane is visiting Africa in September
（"简在9月要访问非洲"）） 那么，这才是翻译为英语句子的概率 在输入是法语句子的条件下,
这就是为什么它是一个有条件的语言模型。 现在,如果你想把这个模型应用到实际中 把法语的一句话翻译成英语, 有了这个法语句子作为输入, 模型可能会告诉你 英语翻译的概率差异。 俺么，x是法语句, "Jane visite l'Afrique en septembre."（法语）
Jane is visiting Africa in September
（"简在9月要访问非洲"） 现在,这会告诉你 法语句不同英语翻译的概率。 你需要的不是随机抽样输出。 如果你从这个分布中取样的话, P(y|x),也许某一次你得到的翻译质量很好 "Jane is visiting Africa in September.""简在9月访问非洲。” 但是,也许下一次你的翻译就会不一样, "Jane is going to be visiting Africa in September. "
"简将在9月访问非洲。" 听起来有点尴尬,但不是个非常差的翻译, 只是不是最好的翻译方式。 有时,偶尔, 你会得到："In September,"在9月, Jane will visit Africa."简将访问非洲。” 也许只是凑巧了 有时会合成一个非常糟糕的翻译: "Her African friend welcomed Jane in September."
"她的非洲朋友欢迎简在9月。” 因此，当你使用这个模型进行机器翻译时, 你不是要从这个分布随机抽样。 而是找到英语句子y, 使条件概率最大化。 因此,在开发机器翻译系统时, 你需要做的一件事就是给出一个算法,找到 y的值,把它在这里最大化。 最常用的算法是beam search（定向搜索）, 你在下一个视频中会看到相关内容。 但是,在学习beam search之前, 你可能在想为什么不用greedy search（贪婪搜索机制）？
那么,什么是greedy search（贪婪搜索机制）呢？ greedy search（贪婪搜索机制）是计算机科学的一种算法, 就是第一个字挑的是可能性最大的单词 基于你的有条件的语言模型。 进入机器翻译模型,选择了第一个单词之后, 然后挑出可能性最高的第二个单词, 然后选出可能性最高的第三个单词。 这种算法称为greedy search（贪婪搜索机制）。 而且,你真正要的是选择整个序列的单词,y1,y2, 到y^Ty, 使整个句子的联合概率最大化。 事实证明,贪婪搜索机制, 就是选出最佳的第一个单词 在选择后, 试着选出最好的第二个单词, 然后 试着选出最好的第三个单词, 这种方法真的行不通。 为了证明这一点,让我们考虑以下两个翻译。 第一个是更好的翻译, 所以希望在我们的机器翻译模型中, 在给定x时更大时
p(y)是第一个句子的概率 这是一个较好的,更简洁的法语翻译。 第二个翻译也不是很差, 它只是更啰嗦 有不必要的词。 但是,如果算法
选择了"jane is"作为前两个字, 因为"going"是一个更常见的英语单词, 对于特定的法语输入，
"jane is going,"的可能性 实际上概率是高于"Jane is visiting“ 在这个法语句子中。 所以,如果你只是 在前三个单词最大的概率基础上,
去选择第三个单词 最后很有可能会选择第二个翻译。 但这最终会得出一个不太理想的句子 在这个模型中，
给定 x 时，由 y 的可能性 p 来比较 得出了一个不太理想的翻译，
这可能是一个的论点 但是,这是一个更广泛的现象例子, 如果你想找到单词序列, y1, y2, 一直到最后一个词,使联合概率最大化, 每次只选一个单词并不总是最好的。 当然,英语句子中的单词组合总数 是指数级别的。 所以,如果你只有1万字的字典,然后你 要进行长达十个单词的翻译, 然后有10000的10次方这些可能的句子
,是十个单词。 从词汇量中提取单词, 字典的大小为10000词。 所以,句子空间的可能性是巨大的, 对所有可能评分,是不可能的 这就是为什么最常见的事情是
使用一个近似搜索出来。 而且,近似搜索算法所做的, 是它会尝试, 但不会总是成功, 它会选择 使条件概率最大化的句子 y 。 而且, 即使它不保证找出 y ,
使概率最大化, 但它通常表现不错。 综上所述, 在这个视频中, 您看到了如何将机器翻译作为条件语言建模问题来考虑。 但这两者之间的一个主要区别 早期的语言建模问题是 随机生成一个句子, 而不是试着找到最有可能的英语句子, 最有可能的英语翻译。 但一定长度的所有英语句子的集合 太大, 无法详尽地列举出。 所以, 我们必须求助于搜索算法。 所以, 让我们去到下一个视频, 你将学习波束搜索算法。