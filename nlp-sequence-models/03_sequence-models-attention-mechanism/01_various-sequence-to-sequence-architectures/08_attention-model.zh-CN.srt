1
00:00:00,000 --> 00:00:04,500
在上一个视频中，你了解到注意力模型如何让

2
00:00:04,500 --> 00:00:06,780
神经网络在生成翻译时

3
00:00:06,780 --> 00:00:11,310
只注意输入句子的某一部分，

4
00:00:11,310 --> 00:00:14,080
就更像人工翻译人员所做的那样。

5
00:00:14,080 --> 00:00:16,470
现在让我们将这种直觉形式化为

6
00:00:16,470 --> 00:00:20,800
关于如何实现注意模型的具体细节。

7
00:00:20,800 --> 00:00:23,700
与之前的视频相同，

8
00:00:23,700 --> 00:00:30,160
假设你有一个输入句子，并且使用双向循环神经网络（RNN），

9
00:00:30,160 --> 00:00:35,775
或者双向门控制循环单元（GRU），
或者长短时记忆网络（LSTM），
来计算每一个单词的特征。

10
00:00:35,775 --> 00:00:40,660
实际上，GRUs和LSTMs经常用在这里，

11
00:00:40,660 --> 00:00:43,655
或许LSTMs会更普遍一些。

12
00:00:43,655 --> 00:00:46,770
因此，对向前发生的传播（forward occurence）

13
00:00:46,770 --> 00:00:51,625
你有一个向前的第一次时间步骤。

14
00:00:51,625 --> 00:00:55,140
激活向后传播（backward occurence）
的第一次时间步长。

15
00:00:55,140 --> 00:00:58,620
激活向前传播（forward occurence）
的第二次时间步长。

16
00:00:58,620 --> 00:01:02,070
激活向后的传播等。

17
00:01:02,070 --> 00:01:09,120
对于仅在向前传播和向后传播的第五时间步长，

18
00:01:09,120 --> 00:01:13,260
我们有一个零点，我想我们也可以有

19
00:01:13,260 --> 00:01:19,580
一个向后的第六个全是零的因子，

20
00:01:19,580 --> 00:01:22,190
实际上这个因子全部由零组成。

21
00:01:22,190 --> 00:01:29,034
接下来，为了简化每个时间步长的符号，

22
00:01:29,034 --> 00:01:32,030
即使你有通过在双向RNN中

23
00:01:32,030 --> 00:01:37,850
向前和向后传播中计算得到的所有特征。

24
00:01:37,850 --> 00:01:46,640
我只是想用a<t>标示这两个激活单元连接在一起。

25
00:01:46,640 --> 00:01:50,345
所以，a<t>是一个在时间步长t的特征向量。

26
00:01:50,345 --> 00:01:55,300
尽管为了保持符号一致，

27
00:01:55,300 --> 00:01:58,190
我们把第二个激活单元用t'来表示。

28
00:01:58,190 --> 00:02:04,035
实际上，t‘用来表示法语句子中的单词。

29
00:02:04,035 --> 00:02:08,930
接下来，我们只有一个向前的传播，

30
00:02:08,930 --> 00:02:16,255
所以这个单独的双向RNN的状态s
是来产生翻译结果的。

31
00:02:16,255 --> 00:02:18,100
所以在第一次时间步长，

32
00:02:18,100 --> 00:02:24,765
它应该产生y1，只是将文本c作为输入。

33
00:02:24,765 --> 00:02:30,022
如果你想用时间索引它，

34
00:02:30,022 --> 00:02:36,750
我想你可以写作c1，
但是有时候我只是写没有上标1的c。

35
00:02:36,750 --> 00:02:43,390
这将取决于注意力参数α<1,1>，

36
00:02:43,390 --> 00:02:50,960
α<1,2>等等，这些注意力参数α

37
00:02:50,960 --> 00:02:57,610
将告诉我们有多少的上下文取决于

38
00:02:57,610 --> 00:03:01,275
我们从不同的时间步长中

39
00:03:01,275 --> 00:03:05,820
获得的激活函数。

40
00:03:05,820 --> 00:03:10,760
所以，我们定义的上下文实际上只是一种方法，

41
00:03:10,760 --> 00:03:16,770
来等待不同的时间步长的注意力特征。

42
00:03:16,770 --> 00:03:25,120
因此，这些注意力(attention waits)会满足这一点，
它们都是非负的，

43
00:03:25,120 --> 00:03:29,005
因此，这将是一个零正数，它们的和等于1.

44
00:03:29,005 --> 00:03:32,100
稍后我们会介绍如何确保这一点。

45
00:03:32,100 --> 00:03:36,690
我们将有在时间1的内容

46
00:03:36,690 --> 00:03:41,625
经常删除上标,这将是 t_prime 的总和,

47
00:03:41,625 --> 00:03:45,620
所有的 t_prime 的注意力权重

48
00:03:45,620 --> 00:03:54,358
这些激活的总和。

49
00:03:54,358 --> 00:04:03,530
所以这里的这个部分是注意力权重,
这个部分来自这里。

50
00:04:03,530 --> 00:04:14,250
所以alpha (t_prime) 

51
00:04:14,250 --> 00:04:26,480
是yt应给的关注度

52
00:04:26,480 --> 00:04:27,740
换言之

53
00:04:27,740 --> 00:04:30,630
在生成输出词的 t 时,

54
00:04:30,630 --> 00:04:35,540
你应该注意 t_prime 输入字多少呢。

55
00:04:35,540 --> 00:04:41,875
因此,这是生成输出的一个步骤,
然后在下一次步骤中,

56
00:04:41,875 --> 00:04:47,070
你生成第二个输出,并再次执行

57
00:04:47,070 --> 00:04:52,725
现在你有了一组新的注意力权重，
他们要找到一个新的方法来求和。

58
00:04:52,725 --> 00:04:55,385
这样生成一个新的内容。

59
00:04:55,385 --> 00:05:00,175
这也是输入,允许你生成第二个单词。

60
00:05:00,175 --> 00:05:05,930
只是现在这样的总和成为

61
00:05:05,930 --> 00:05:12,910
第二时间点的内容，
t_prime alpha (2, t_prime) 的总和。

62
00:05:12,910 --> 00:05:16,500
因此,使用这些内容向量。

63
00:05:16,500 --> 00:05:18,163
C1就在那后面

64
00:05:18,163 --> 00:05:24,630
C2,等等。

65
00:05:24,630 --> 00:05:30,590
这里的网络看起来像一个相当标准的 RNN 序列

66
00:05:30,590 --> 00:05:33,890
以内容向量作为输出,我们

67
00:05:33,890 --> 00:05:37,880
一次只能生成一个单词的翻译。

68
00:05:37,880 --> 00:05:42,590
我们还定义了如何计算内容向量,从

69
00:05:42,590 --> 00:05:46,815
这些注意力权重和输入句子的那些特征。

70
00:05:46,815 --> 00:05:49,220
所以剩下的事就是

71
00:05:49,220 --> 00:05:53,322
定义如何实际计算这些注意力权重。

72
00:05:53,322 --> 00:05:55,015
让我们在下一张幻灯片上这样做。

73
00:05:55,015 --> 00:05:57,454
所以只是重述一遍, alpha (t,

74
00:05:57,454 --> 00:06:01,065
t_prime) 是你应该付出的关注力

75
00:06:01,065 --> 00:06:07,225
a (t_prime) 当你试图在输出翻译中生成 t 的单词时。

76
00:06:07,225 --> 00:06:11,268
所以,让我把公式写下来,
我们探讨它是如何工作的。

77
00:06:11,268 --> 00:06:14,395
这是公式,你可以使用计算 alpha (t,

78
00:06:14,395 --> 00:06:18,136
t_prime) 这将计算这些术语 e (t,

79
00:06:18,136 --> 00:06:21,735
t_prime), 然后使用本质上是一个软最大化, 以确保

80
00:06:21,735 --> 00:06:25,770
如果您对 t_prime 求和, 这些权重和为1。

81
00:06:25,770 --> 00:06:28,555
所以对于每一个固定值的 t,

82
00:06:28,555 --> 00:06:34,765
如果你对 t_prime 的求和, 这些东西加起来为1。

83
00:06:34,765 --> 00:06:38,795
使用这个软最大优先级,

84
00:06:38,795 --> 00:06:41,405
来确保这和为1。

85
00:06:41,405 --> 00:06:44,760
现在,我们如何计算这些因素 e。

86
00:06:44,760 --> 00:06:48,960
一个方法是使用一个小的神经网络如下。

87
00:06:48,960 --> 00:06:56,315
所以 s t -1是神经网络的前一步。

88
00:06:56,315 --> 00:06:59,134
所以这里是我们的网络。

89
00:06:59,134 --> 00:07:04,950
如果你试图生成的yt,
然后 st 减去1是来自之前的隐藏状态

90
00:07:04,950 --> 00:07:07,598
将落入 st

91
00:07:07,598 --> 00:07:12,005
这是一个非常小的神经网络的输入。

92
00:07:12,005 --> 00:07:16,260
通常,神经网络只有一个隐藏层,
因为你需要计算这些很多。

93
00:07:16,260 --> 00:07:23,015
然后 a (t_prime) 时间步长 t_prime 的特征是另一个输入。

94
00:07:23,015 --> 00:07:24,795
直觉上来讲,

95
00:07:24,795 --> 00:07:31,465
如果你想决定多大程度注意激活 t_prime。

96
00:07:31,465 --> 00:07:35,220
看起来它应该主要取决于

97
00:07:35,220 --> 00:07:39,345
你自己的从上一次的时间步骤的隐藏状态激活。

98
00:07:39,345 --> 00:07:41,545
你还没有当前状态激活

99
00:07:41,545 --> 00:07:43,680
因为你还没有计算。

100
00:07:43,680 --> 00:07:47,269
但看看无论你这 RNN隐藏的阶段生成

101
00:07:47,269 --> 00:07:51,260
上部翻译然后为每个位置,

102
00:07:51,260 --> 00:07:53,531
每个单词，都看它们的特征。

103
00:07:53,531 --> 00:07:57,126
所以这看起来很自然, alpha(t,

104
00:07:57,126 --> 00:08:03,015
t_prime) 和 e (t, t_prime) 应取决于这两个数量。

105
00:08:03,015 --> 00:08:04,820
但我们不知道这个函数是什么。

106
00:08:04,820 --> 00:08:07,805
所以有一件事你可以做的是
只训练一个非常小的神经网络

107
00:08:07,805 --> 00:08:10,710
学习这个功能应该是什么。

108
00:08:10,710 --> 00:08:18,080
并且相信反向传播算法通过
梯度下降法会找到对的函数。

109
00:08:18,080 --> 00:08:21,700
事实证明如果你实现了

110
00:08:21,700 --> 00:08:25,660
这整个模型和用梯度下降训练它,

111
00:08:25,660 --> 00:08:27,490
整个过程真的管用

112
00:08:27,490 --> 00:08:31,270
这个小神经网络可以很好的告诉你

113
00:08:31,270 --> 00:08:35,545
应给a (t_prime)多少关注

114
00:08:35,545 --> 00:08:40,000
并且这个公式可以确保

115
00:08:40,000 --> 00:08:45,713
注意力总和等于1,
然后当你一个人在每次产生一个单词时,

116
00:08:45,713 --> 00:08:50,200
这个神经网络实际上关注的是正确的部分

117
00:08:50,200 --> 00:08:54,973
的输入语句，通过使用渐变下降
自动学习所有这一切。

118
00:08:54,973 --> 00:08:58,765
现在,这个算法的一个缺点是,

119
00:08:58,765 --> 00:09:03,950
运行该算法需要二次时间或二次成本。

120
00:09:03,950 --> 00:09:09,550
如果你有 tx 个输入词和ty

121
00:09:09,550 --> 00:09:12,655
个输出那么注意力参数总数

122
00:09:12,655 --> 00:09:17,025
将是 tx 乘以 ty。

123
00:09:17,025 --> 00:09:21,940
因此,该算法以二次成本运行。

124
00:09:21,940 --> 00:09:26,450
虽然在机器翻译应用中

125
00:09:26,450 --> 00:09:29,290
输入和输出语句都不

126
00:09:29,290 --> 00:09:33,430
长，所以也许二次成本实际上是可接受的。

127
00:09:33,430 --> 00:09:38,230
虽然也有一些研究工作试图降低成本。

128
00:09:38,230 --> 00:09:47,575
到目前为止,
在描述的注意力算法下进行机器翻译。

129
00:09:47,575 --> 00:09:53,160
如果不深入细节,
这个想法也被应用到其他问题上。

130
00:09:53,160 --> 00:09:55,100
比如图像字幕。

131
00:09:55,100 --> 00:09:58,420
因此在图像字母问题中,任务是

132
00:09:58,420 --> 00:10:02,155
查看图片并为该图片写一个描述。

133
00:10:02,155 --> 00:10:06,015
所以在这篇文章中, Kevin Chu

134
00:10:06,015 --> 00:10:08,835
Jimmy Barr, Ryan Kiros, Kelvin 
Shaw, Aaron Korver,

135
00:10:08,835 --> 00:10:11,190
Russell Zarkutnov, Virta Zemo,

136
00:10:11,190 --> 00:10:16,620
and Andrew Benjo 也表明, 你可以有一个非常相似的架构。

137
00:10:16,620 --> 00:10:21,220
在为图片编写标题时的图片。

138
00:10:21,220 --> 00:10:26,500
只注意部分图像

139
00:10:26,500 --> 00:10:30,745
所以,如果你有兴趣,
那么我鼓励你去看看那篇论文

140
00:10:30,745 --> 00:10:36,310
在编程练习中,
你可以使用所有这些。

141
00:10:36,310 --> 00:10:42,100
尽管机器翻译是一个非常复杂的问题,
在练习中,你

142
00:10:42,100 --> 00:10:44,910
将实施和发挥的注意力算法,而你

143
00:10:44,910 --> 00:10:48,002
自己的日期标准化问题。

144
00:10:48,002 --> 00:10:50,625
所以这个问题输入这样的日期。

145
00:10:50,625 --> 00:10:55,390
这实际上有一个日期的
阿波罗月球着陆和标准化成

146
00:10:55,390 --> 00:11:01,165
标准格式或这样的日期,
并有一个神经网络

147
00:11:01,165 --> 00:11:04,250
序列模型将其规范化为此格式。

148
00:11:04,250 --> 00:11:07,710
这是威廉.莎士比亚的生日。

149
00:11:07,710 --> 00:11:09,433
被认为是。

150
00:11:09,433 --> 00:11:12,430
和你在练习中看到的,你可以训练

151
00:11:12,430 --> 00:11:15,940
一个神经网络输入任何格式日期

152
00:11:15,940 --> 00:11:18,975
使它使用注意模型

153
00:11:18,975 --> 00:11:23,470
为这些日期生成规范化格式。

154
00:11:23,470 --> 00:11:26,500
另一件事,有时有趣的是

155
00:11:26,500 --> 00:11:29,775
看看可视化的注意力权重。

156
00:11:29,775 --> 00:11:36,437
所以这里有一个机器翻译的例子,
这里绘制了不同的颜色。

157
00:11:36,437 --> 00:11:39,605
不同注意的权重大小。

158
00:11:39,605 --> 00:11:42,865
我不想花太多的时间在这一点上,
但你发现,

159
00:11:42,865 --> 00:11:46,930
相应的输入和输出词

160
00:11:46,930 --> 00:11:51,460
你会发现注意力权重将会很高。

161
00:11:51,460 --> 00:11:56,005
因此,建议当它在输出中生成一个特定的词时,

162
00:11:56,005 --> 00:12:01,885
通常注意输入的正确的词和所有这一切,包括

163
00:12:01,885 --> 00:12:04,660
学习什么时候注意什么

164
00:12:04,660 --> 00:12:08,560
学会了使用反向传播与注意模型。

165
00:12:08,560 --> 00:12:11,310
所以这就是注意模型

166
00:12:11,310 --> 00:12:15,255
真的是深入学习中最有力的思想之一。

167
00:12:15,255 --> 00:12:18,153
我希望你喜欢在本周的编程练习中实施

168
00:12:18,153 --> 00:12:22,000
这些想法