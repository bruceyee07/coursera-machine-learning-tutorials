在上一个视频中，你了解到注意力模型如何让 神经网络在生成翻译时 只注意输入句子的某一部分， 就更像人工翻译人员所做的那样。 现在让我们将这种直觉形式化为 关于如何实现注意模型的具体细节。 与之前的视频相同， 假设你有一个输入句子，并且使用双向循环神经网络（RNN）， 或者双向门控制循环单元（GRU），
或者长短时记忆网络（LSTM），
来计算每一个单词的特征。 实际上，GRUs和LSTMs经常用在这里， 或许LSTMs会更普遍一些。 因此，对向前发生的传播（forward occurence） 你有一个向前的第一次时间步骤。 激活向后传播（backward occurence）
的第一次时间步长。 激活向前传播（forward occurence）
的第二次时间步长。 激活向后的传播等。 对于仅在向前传播和向后传播的第五时间步长， 我们有一个零点，我想我们也可以有 一个向后的第六个全是零的因子， 实际上这个因子全部由零组成。 接下来，为了简化每个时间步长的符号， 即使你有通过在双向RNN中 向前和向后传播中计算得到的所有特征。 我只是想用a<t>标示这两个激活单元连接在一起。 所以，a<t>是一个在时间步长t的特征向量。 尽管为了保持符号一致， 我们把第二个激活单元用t'来表示。 实际上，t‘用来表示法语句子中的单词。 接下来，我们只有一个向前的传播， 所以这个单独的双向RNN的状态s
是来产生翻译结果的。 所以在第一次时间步长， 它应该产生y1，只是将文本c作为输入。 如果你想用时间索引它， 我想你可以写作c1，
但是有时候我只是写没有上标1的c。 这将取决于注意力参数α<1,1>， α<1,2>等等，这些注意力参数α 将告诉我们有多少的上下文取决于 我们从不同的时间步长中 获得的激活函数。 所以，我们定义的上下文实际上只是一种方法， 来等待不同的时间步长的注意力特征。 因此，这些注意力(attention waits)会满足这一点，
它们都是非负的， 因此，这将是一个零正数，它们的和等于1. 稍后我们会介绍如何确保这一点。 我们将有在时间1的内容 经常删除上标,这将是 t_prime 的总和, 所有的 t_prime 的注意力权重 这些激活的总和。 所以这里的这个部分是注意力权重,
这个部分来自这里。 所以alpha (t_prime) 是yt应给的关注度 换言之 在生成输出词的 t 时, 你应该注意 t_prime 输入字多少呢。 因此,这是生成输出的一个步骤,
然后在下一次步骤中, 你生成第二个输出,并再次执行 现在你有了一组新的注意力权重，
他们要找到一个新的方法来求和。 这样生成一个新的内容。 这也是输入,允许你生成第二个单词。 只是现在这样的总和成为 第二时间点的内容，
t_prime alpha (2, t_prime) 的总和。 因此,使用这些内容向量。 C1就在那后面 C2,等等。 这里的网络看起来像一个相当标准的 RNN 序列 以内容向量作为输出,我们 一次只能生成一个单词的翻译。 我们还定义了如何计算内容向量,从 这些注意力权重和输入句子的那些特征。 所以剩下的事就是 定义如何实际计算这些注意力权重。 让我们在下一张幻灯片上这样做。 所以只是重述一遍, alpha (t, t_prime) 是你应该付出的关注力 a (t_prime) 当你试图在输出翻译中生成 t 的单词时。 所以,让我把公式写下来,
我们探讨它是如何工作的。 这是公式,你可以使用计算 alpha (t, t_prime) 这将计算这些术语 e (t, t_prime), 然后使用本质上是一个软最大化, 以确保 如果您对 t_prime 求和, 这些权重和为1。 所以对于每一个固定值的 t, 如果你对 t_prime 的求和, 这些东西加起来为1。 使用这个软最大优先级, 来确保这和为1。 现在,我们如何计算这些因素 e。 一个方法是使用一个小的神经网络如下。 所以 s t -1是神经网络的前一步。 所以这里是我们的网络。 如果你试图生成的yt,
然后 st 减去1是来自之前的隐藏状态 将落入 st 这是一个非常小的神经网络的输入。 通常,神经网络只有一个隐藏层,
因为你需要计算这些很多。 然后 a (t_prime) 时间步长 t_prime 的特征是另一个输入。 直觉上来讲, 如果你想决定多大程度注意激活 t_prime。 看起来它应该主要取决于 你自己的从上一次的时间步骤的隐藏状态激活。 你还没有当前状态激活 因为你还没有计算。 但看看无论你这 RNN隐藏的阶段生成 上部翻译然后为每个位置, 每个单词，都看它们的特征。 所以这看起来很自然, alpha(t, t_prime) 和 e (t, t_prime) 应取决于这两个数量。 但我们不知道这个函数是什么。 所以有一件事你可以做的是
只训练一个非常小的神经网络 学习这个功能应该是什么。 并且相信反向传播算法通过
梯度下降法会找到对的函数。 事实证明如果你实现了 这整个模型和用梯度下降训练它, 整个过程真的管用 这个小神经网络可以很好的告诉你 应给a (t_prime)多少关注 并且这个公式可以确保 注意力总和等于1,
然后当你一个人在每次产生一个单词时, 这个神经网络实际上关注的是正确的部分 的输入语句，通过使用渐变下降
自动学习所有这一切。 现在,这个算法的一个缺点是, 运行该算法需要二次时间或二次成本。 如果你有 tx 个输入词和ty 个输出那么注意力参数总数 将是 tx 乘以 ty。 因此,该算法以二次成本运行。 虽然在机器翻译应用中 输入和输出语句都不 长，所以也许二次成本实际上是可接受的。 虽然也有一些研究工作试图降低成本。 到目前为止,
在描述的注意力算法下进行机器翻译。 如果不深入细节,
这个想法也被应用到其他问题上。 比如图像字幕。 因此在图像字母问题中,任务是 查看图片并为该图片写一个描述。 所以在这篇文章中, Kevin Chu Jimmy Barr, Ryan Kiros, Kelvin 
Shaw, Aaron Korver, Russell Zarkutnov, Virta Zemo, and Andrew Benjo 也表明, 你可以有一个非常相似的架构。 在为图片编写标题时的图片。 只注意部分图像 所以,如果你有兴趣,
那么我鼓励你去看看那篇论文 在编程练习中,
你可以使用所有这些。 尽管机器翻译是一个非常复杂的问题,
在练习中,你 将实施和发挥的注意力算法,而你 自己的日期标准化问题。 所以这个问题输入这样的日期。 这实际上有一个日期的
阿波罗月球着陆和标准化成 标准格式或这样的日期,
并有一个神经网络 序列模型将其规范化为此格式。 这是威廉.莎士比亚的生日。 被认为是。 和你在练习中看到的,你可以训练 一个神经网络输入任何格式日期 使它使用注意模型 为这些日期生成规范化格式。 另一件事,有时有趣的是 看看可视化的注意力权重。 所以这里有一个机器翻译的例子,
这里绘制了不同的颜色。 不同注意的权重大小。 我不想花太多的时间在这一点上,
但你发现, 相应的输入和输出词 你会发现注意力权重将会很高。 因此,建议当它在输出中生成一个特定的词时, 通常注意输入的正确的词和所有这一切,包括 学习什么时候注意什么 学会了使用反向传播与注意模型。 所以这就是注意模型 真的是深入学习中最有力的思想之一。 我希望你喜欢在本周的编程练习中实施 这些想法