上个视频中 你看到了
我们将用来定义序列学习问题的符号 现在，我们来谈论一下怎么建立一个 用于学习从 x 到 y 的映射的神经网络模型 现在你可以尝试一下
用一个标准的神经网络来完成这个任务 在我们之前的例子中，
我们有9个输入词，所以你可以 尝试把这9个输入词，
或许是9个独热向量 传入到一个标准的神经网络
或一些隐藏层中 然后，他们将会输出9个包括0或1的值 来告诉你，每个单词是人名的一部分。 但事实证明，这样做效果不好，并且 其中存在两个主要的问题 第一个问题是，输入和输出 在不同的例子中，长度可能会不同 所以，并不是每个个例
都有相同的输入长度Tx或 相同的输出长度Ty 另外，如果每个句子都有最大长度，你可能要扩充或 零扩充每个输入项，
使其达到最大长度，但是 这仍然不是高质量的表示方法。 另外，第二个问题可能更严重 像这样的朴素神经网络结构 不会将那些从不同文本位置学到的特征，
进行共享 特别是，假如神经网络学到了“Harry”这个词 如果它出现在了位置1，
就会有标识指出这是人名的一部分 如果它能自动确定在某个位置 Xt 出现的 "Harry" 同样表明这个词有可能是一个人的名字<br />那就再好不过了 这也许和你在卷积神经网络课程
中看到的过程很相似 在那门课程中，
模型从图片的某一部分学到的模式 可以更快地应用到图片的其他部分 我们希望在序列数据可以实现相似的效果 如同你从卷积神经网络中看到的<br />使用更好的神经网络结构 有利于减少模型中的参数数量 在标准神经网络中这些 x 均是
长度为10,000的 独热编码的向量<br />就是一个非常庞大的输入层 假设整个输入语句的长度达到单词数量上限<br />再乘以10,000 第一层的权值矩阵最终会包含大量参数 因此我们下一张幻灯片中
将要描述的循环神经网络 不存在上述的缺点 那么，什么是循环神经网络？ 我们来创建一个循环神经网络，
如果你从左向右读一个句子 你读的第一个词语我们将其表示为x1 我们将第一个词语 输入一个神经网络层 画一条这样的线，因此形成第一个
神经网络的隐藏层 我们可以用这个神经网络来预测输出 这是否是一个人名的一部分 这里循环神经网络所做的是 当其继续读取句子中的第二个词语的时候 比如说x2, （LSTM）不仅仅是
通过x2去预测y2 它还会把计算机第一步的计算结果
作为其输入信息（的一部分） 所以具体地来说 第一步的激活数值会传递到第二步 在下一个步骤中 循环神经网络输入第三个词x3并预测 输出y_hat_3等 直到最后一次步骤, 它输入x<tx> 然后输出y_hat_ty
在这个例子里面Tx=Ty 架构会略有变化 当Tx不同于Ty时 在每一步，循环神经网络将 它的激活数值传递到下一步，
供其使用 要开始整个计算过程 我们会在第0步的时候使用
一些人为制造的激活数值 通常为一个全零的向量 有的研究者会随机的为a0设置初始值 你还可以通过别的方法来
为a0设置初始值 但是使用全零的向量作为一个
第0步的虚激活数值 是最常用的选择用来作为神经网络的输入 在有些学术文章和书当中 你会看到循环神经网络通过以下的图形来表示： 每一步你输入一个x，输出一个ŷ 有时会使用一个T指数 有时会画这样一个循环
表示循环的链接 有时候会用阴影箱线图来表示 这里阴影箱线图表示时间上，一步的延迟 我个人认为这些循环图形很难解释清楚 因此，这个课程中我更趋向于使用类似
左侧图中那样展开的图形来表示 但是，如果你在教科书 或者研究论文中看到右图中那样的图形示意 我会将它想象成展开成左侧示意图那样，
 以便正真理解其含义。 循环神经网络从左向右扫描数据 每一步所用的参数是共享的 因此，这里有一组参数，我们会在 下一张幻灯片中详细叙述 控制从x1到隐藏层链接的一组参数 我们用W_ax表示它们 这一组W_ax参数同时
也会被用于每一个步骤 我想我可以在那里写上W_ax 激活函数，各层间水平的链接是由 一组特定参数W_aa来控制 同样的W_aa也将被用于每一个步骤中 同样，W_ya控制输出预测 我将在下一张幻灯片中详细叙述
这些参数是如何运作的 这个循环神经网络的含义是 在对y3进行预测时，它不仅从x3获取信息 同时也考虑到x1和x2 因为x1可以从这条路径
帮助预测y3 这个RNN的缺点之一是，它只使用序列中 先前的信息来做出预测（这里指x1,x2,x3），具体来说
当我们预测y3时 它不会使用关于词语x4, x5, x6等等的信息 当你看到一句话的时候
 这将会造成困扰 他说“Teddy Roosevelt was a great President”
（泰迪罗斯福是一位伟大的总统） 当需要判定Teddy是否是人名的一部分时 如果不仅知道有关头两个词的信息 还知道句中后面的词
 将有助于做出判断 因为有可能出现这样的句子：
 "Teddy bears are on sale"
（泰迪熊在打折） 如果只知道头三个词的话 我们不能确信Teddy是否是
人名的一部分 第一个里面是人名，
第二个例子里面不是 如果你光看头三个词语，
你将无从分辨 这个特定的神经网络结构的一个限制是 在序列中对某一时间的预测仅使用 之前的输入，
 而不使用序列中之后的信息 以后，在有关双向循环神经网络 BRNNS视频中
我们来讨论这个问题 目前这个简单的单向神经网络 架构足够说明一些重要的概念 之后我们将对这些想法
做出一些简洁快速的修正 让我们在判断ŷ3的时候同时考虑到 序列中（当前时间点）
之前和之后的信息 我们将在后面的视频中学习 我们把这个神经网络所做的计算写出来 （神经网络计算公式） 这是一个整理过的神经网络的示意图 就像我前面说到的 通常开始时，你将输入一个全零的向量
作为a0的初始值 接下来，这是正向传播的示意。 要计算a1, 我们通过激活函数g 中使用参数Waa乘以a0 加上Wax乘以x1， 加上偏置ba 然后计算ŷ1来做出对第1步的预测 这里会使用和上面不一样的激活函数 （将g函数）用于Wya乘以 a1加上by 这里我使用的标记惯例来替代这些矩阵如Wax 第二个下标表示Wax乘以 像xy那样的数 a表示这是用来计算a那样的数 类似的，这里Wya乘以 类似a的数来计算类似y的数 用来计算激活数值的激活函数 常常使用tanh函数以及RNN，
 有时候也会使用ReLU tanh函数是最常见的选择 我们有其他方法来避免梯度消失的问题 我们会在这周晚些时候讨论。 根据输出y的性质 如果是二分类问题，
你可能会使用Sgimoid激活函数 如果是k类的分类问题
 你会使用softmax函数 但是激活函数的选取
还是要根据输出y的性质 来确定 所以前面所讲的名字识别问题，
输出y不是0就是1 这里的第二个g可以使用
sigmoid激活函数 这里可以写作g2来和
前面的激活函数加以区别 但是我通常不会这么做 通常，在步骤t的时候 t等于激活函数Waa的g乘以上一步的a 加上目前步骤的Wax再加上b_a 和ŷt等于g g可以是不同的激活函数，
 W_ya的g 乘以a（t），加上by 这些等式定义了神经网络中的正向传播 开始（计算）时将a_0的初始值设为全零向量 用a0, x1计算出a1和ŷ1 然后用a1，x_2计算a2和ŷ2,
以此类推 这样你就在这个图中从左到右
进行了正向传播的计算 的图形 为了帮助我们建立更加复杂的神经网络 我将简化这部分的表示方法 我把这两个等式拷贝到下一张幻灯片 疑问词也会待在它们本身应该在的地方 我要做的就是简化表示方法 我把这部分用简单的方式的改写 记作at等于g乘以 矩阵Wa乘以一个新数值 at-1, xt 加上ba 左面和右面带下划线的数值应该是一样的 我们使用Waa和 和Wax放在一起来表示Wa 我把他们水平叠加起来便得到矩阵Wa 例如，如果a是100维的 在我们例子中
x是10000维 那么Waa就是100X100的矩阵 Wax就是100X100000维度的矩阵 把两个矩阵叠加在一起，
 这个是100维 这里是100和10000维 Wa就是一个100X10100的矩阵 左边的矩阵画的不等比
因为Wax是一个很宽的矩阵 这个记号表示 它有两个向量x 把他们叠加在一起，
 我们用这个表示方法来说明 我们用向量at-1 它是100维的，
 将其叠加在at上 我们得到一个 10100维的向量 我希望大家可以自己验证一下 这个矩阵乘以这个向量
 我们可以得到原来的数值 因为Waa矩阵Wax矩阵相乘 再与向量[a^(t-1)，x^t]相乘 等于Waa乘以a^(t-1) 加上Wax乘以x^(t)
 和这里的值完全相同 所以这种表示方法的好处是
 不再需要两个作为参数的矩阵 我们可以把Waa，
Wax压缩成一个参数的矩阵Wa 这样，在我们建立更复杂模型时
便可以简化表示 同样的，我将重写ŷ^9(t) Wy,a(t)加上by 我们在Wy和by里面用了下标y 解释了输出量的类型是 Wy表示计算输出y的权重矩阵 Wa和ba表示计算激活数值a^(t) 激活函数的参数。 那么你了解了关于RNN的基础知识 接下来我们讨论反向传递 以及RNN如何学习