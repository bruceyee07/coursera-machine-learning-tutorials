1
00:00:00,000 --> 00:00:02,825
在上一个视频中, 你了解了 GRU, 

2
00:00:02,825 --> 00:00:04,170
门控循环单元，

3
00:00:04,170 --> 00:00:09,770
以及GRU如何使你能在序列中
学习较长的(序列)链接。

4
00:00:09,770 --> 00:00:12,765
另外一种允许你很好的处理（长距离序列链接）的

5
00:00:12,765 --> 00:00:16,028
是LSTM，或者长短期记忆单元

6
00:00:16,028 --> 00:00:20,110
这个方法比GRU更强大
 让我们来看一下

7
00:00:20,110 --> 00:00:23,161
这里是前一个视频里面看到的关于GRU的等式

8
00:00:23,161 --> 00:00:24,354
GRU中

9
00:00:24,354 --> 00:00:27,210
我们有a_t等于c_t

10
00:00:27,210 --> 00:00:31,615
和两个门： 更新门和相关性门，c_tilde_t

11
00:00:31,615 --> 00:00:34,946
是替代记忆单元的候选（数值）

12
00:00:34,946 --> 00:00:38,267
然后我们使用更新门gamma_u

13
00:00:38,267 --> 00:00:43,408
来决定是否要使用c_tilde_t来修正c_t

14
00:00:43,408 --> 00:00:49,985
LSTM比GRU强大一些，并且更为泛化

15
00:00:49,985 --> 00:00:55,360
（LSTM算法）归功于Sepp Hochreiter和Jurgen Schimdhuber （的论文）

16
00:00:55,360 --> 00:00:57,415
这篇论文是一篇真正的开创性的论文

17
00:00:57,415 --> 00:01:01,555
对序列模型产生了巨大的影响

18
00:01:01,555 --> 00:01:04,740
我认为这篇论文属于众多艰涩难懂
 的论文中的一篇

19
00:01:04,740 --> 00:01:08,280
这篇论文用很大篇幅来论述梯度消失理论

20
00:01:08,280 --> 00:01:12,555
因此，我觉得更多的人是通过别的渠道来学习

21
00:01:12,555 --> 00:01:15,510
LSTM的细节而不是通过这篇特定的文章

22
00:01:15,510 --> 00:01:19,345
这篇论文对深度学习领域产生了深厚的影响

23
00:01:19,345 --> 00:01:23,176
然这里的等式便是LSTM的核心

24
00:01:23,176 --> 00:01:26,775
我们继续看记忆单元c

25
00:01:26,775 --> 00:01:30,480
以及用作其修正值的候选数层c_tilde_t

26
00:01:30,480 --> 00:01:34,810
是这个

27
00:01:34,810 --> 00:01:38,930
注意在LSTM中

28
00:01:38,930 --> 00:01:45,800
不会再出现a_t等于c_t的情况出现

29
00:01:45,800 --> 00:01:48,000
因此这个是我们使用的

30
00:01:48,000 --> 00:01:51,830
这个只是左边的等式，除了现在

31
00:01:51,830 --> 00:01:56,095
这里特定使用a_t或者a_(t-1)
 而非c_(t-1)

32
00:01:56,095 --> 00:01:59,710
以及我们不再使用gamma或者这个相关性门

33
00:01:59,710 --> 00:02:03,615
但是有LSTM的变体会把这些因子重新包括进去

34
00:02:03,615 --> 00:02:06,110
但是通常形式的LSTM

35
00:02:06,110 --> 00:02:07,900
不会那么做

36
00:02:07,900 --> 00:02:12,320
接着我们和前面一样使用更新门

37
00:02:12,320 --> 00:02:21,110
更新W并且使用a_(t-1)
 x_t加b_u

38
00:02:21,110 --> 00:02:25,340
LSTM的一个新特性是

39
00:02:25,340 --> 00:02:28,725
不再使用一个单一的更新门控

40
00:02:28,725 --> 00:02:29,970
包括这两个项

41
00:02:29,970 --> 00:02:32,315
我们将采用两个不同的项

42
00:02:32,315 --> 00:02:35,795
不使用gamma_u和1-gamma_u，

43
00:02:35,795 --> 00:02:37,415
我们这里会使用gamma_u

44
00:02:37,415 --> 00:02:41,910
并将遗忘门控称作gamma_f

45
00:02:41,910 --> 00:02:43,790
因此这个门控，gamma_f

46
00:02:43,790 --> 00:02:48,695
是一个关于x_t （a_(t-1)），加上b_u的sigmoid函数

47
00:02:48,695 --> 00:02:54,866
就像你预料的一样

48
00:02:54,866 --> 00:03:01,180
我们将会得到一个新的输出门控
关于W_o的sigma函数

49
00:03:01,180 --> 00:03:09,239
然后就像你们预期一样，加上b_o

50
00:03:09,239 --> 00:03:17,560
记忆单元的更新值将是c_t等于gamma_u

51
00:03:17,560 --> 00:03:21,785
这个星号表示元素积/点积操作

52
00:03:21,785 --> 00:03:24,575
这个是向量-向量点积操作

53
00:03:24,575 --> 00:03:27,525
而且，用于代替1-gamma_u

54
00:03:27,525 --> 00:03:30,470
我们将另外得到一个独立的遗忘门控，gamma_f

55
00:03:30,470 --> 00:03:34,475
乘以c_t减去1

56
00:03:34,475 --> 00:03:37,970
这为记忆单元提供了一个新的选项来

57
00:03:37,970 --> 00:03:41,720
保留老的值c_t减去1
 并且加上（前面的项）

58
00:03:41,720 --> 00:03:43,160
这个新值c_tilde_t

59
00:03:43,160 --> 00:03:48,445
使用不同的更新和遗忘门

60
00:03:48,445 --> 00:03:53,960
这个表示更新，遗忘和输出门

61
00:03:53,960 --> 00:04:02,413
最后，a_t不再等于c_t

62
00:04:02,413 --> 00:04:10,700
a_t等于输出门和c_t的点积

63
00:04:10,700 --> 00:04:13,170
这些便是LSTM的核心等式

64
00:04:13,170 --> 00:04:18,070
你可以发现它们具有三个控制门
 而不是两个

65
00:04:18,070 --> 00:04:23,025
这个（LSTM）相对复杂一点，
 它将门控放置在略为不同的地方

66
00:04:23,025 --> 00:04:29,805
这里就是控制LSTM行为的核心等式

67
00:04:29,805 --> 00:04:33,450
使用图来解释事物是一种常用的方式

68
00:04:33,450 --> 00:04:35,505
因此让我在这里画个示意图

69
00:04:35,505 --> 00:04:38,995
如果图形太复杂，不用担心

70
00:04:38,995 --> 00:04:42,940
我个人认为等式比图画更为容易理解

71
00:04:42,940 --> 00:04:46,680
但是我将通过图形来传递其直观的含义

72
00:04:46,680 --> 00:04:52,210
这个大图的灵感来源于Chris Ola的一篇博客文章

73
00:04:52,210 --> 00:04:54,580
文章的题目是理解LSTM网络
 这里画的图形

74
00:04:54,580 --> 00:04:58,540
和他在博客文章里画的很相似

75
00:04:58,540 --> 00:05:02,460
从这图里面能够学习到的重要信息是

76
00:05:02,460 --> 00:05:06,900
你使用a_t-1和x_t来计算所有门控的数值

77
00:05:06,900 --> 00:05:08,970
在图中，你有a_t-1

78
00:05:08,970 --> 00:05:11,940
和x_t被一起用来计算遗忘门

79
00:05:11,940 --> 00:05:13,665
更新门以及

80
00:05:13,665 --> 00:05:16,140
输出门的数值

81
00:05:16,140 --> 00:05:21,765
他们同样使用一个tanh函数得到c_tilde_t

82
00:05:21,765 --> 00:05:23,790
这些数据通过

83
00:05:23,790 --> 00:05:27,384
点积计算等复杂的方式被揉合在一起

84
00:05:27,384 --> 00:05:33,000
并从c_t-1中计算得到c_t

85
00:05:33,000 --> 00:05:37,020
有个很有趣的事情是，你可以生成
一堆类似的单元

86
00:05:37,020 --> 00:05:39,605
这里是其中一个，你把他们连接起来

87
00:05:39,605 --> 00:05:41,633
你可以根据时序将他们连接起来

88
00:05:41,633 --> 00:05:45,545
它先处理x_1 然后处理x_2 x_3

89
00:05:45,545 --> 00:05:51,020
你可以把这些单元更具以下方式结合起来

90
00:05:51,020 --> 00:05:56,860
前一个时间点的（单元）输出是
下一个时间点的（单元）输入

91
00:05:56,860 --> 00:06:00,180
c也是同样的计算。我在下图中将其简化。

92
00:06:00,180 --> 00:06:03,350
你可能意识到一个很酷的事实是

93
00:06:03,350 --> 00:06:07,185
图中上部有一条流程直线表示如何计算

94
00:06:07,185 --> 00:06:12,180
只要你恰当的设置了遗忘和更新门

95
00:06:12,180 --> 00:06:15,450
LSTM可以相对简单的将

96
00:06:15,450 --> 00:06:21,380
c_0值传递到图的右侧

97
00:06:21,380 --> 00:06:23,434
例如使得c_3等于c_0

98
00:06:23,434 --> 00:06:25,095
这就是为什么LSTM

99
00:06:25,095 --> 00:06:26,895
以及GRU

100
00:06:26,895 --> 00:06:31,585
可以长时间很好的记住某些数值

101
00:06:31,585 --> 00:06:39,975
对于存储在记忆单元的某些实数值
 （记忆）可以维持很多步骤

102
00:06:39,975 --> 00:06:41,970
这就是关于LSTM的内容

103
00:06:41,970 --> 00:06:44,965
你可以想象

104
00:06:44,965 --> 00:06:48,690
人们同时使用几种（LSTM）变体

105
00:06:48,690 --> 00:06:52,410
最普遍常用的（变体）可能是

106
00:06:52,410 --> 00:06:57,094
门控的值不仅仅取决于a_t-1, x_t

107
00:06:57,094 --> 00:07:05,745
有时候人们会把c_t-1的值也用上

108
00:07:05,745 --> 00:07:10,311
这个叫做窥孔连接

109
00:07:10,311 --> 00:07:13,595
不是一个很好的名称，但是你可以看到

110
00:07:13,595 --> 00:07:19,975
窥孔连接的意思是，
 门控数值不仅仅取决于a_t-1, x_t

111
00:07:19,975 --> 00:07:23,090
同时也和前面记忆单元的值相关

112
00:07:23,090 --> 00:07:28,615
窥孔连接可以用作所有
这三个门控的计算中去

113
00:07:28,615 --> 00:07:32,985
这是一个常见的LSTM变体

114
00:07:32,985 --> 00:07:38,200
一个技术细节是，例如这个是一个100维的向量

115
00:07:38,200 --> 00:07:41,975
因此你有一个100维的隐藏记忆单元

116
00:07:41,975 --> 00:07:46,450
例如，第五个元素

117
00:07:46,450 --> 00:07:51,045
的c_t-1只会影响相关门控的第五个元素

118
00:07:51,045 --> 00:07:54,070
因此，他们的关系是一对一的

119
00:07:54,070 --> 00:07:56,035
不是每一个关于100维c_t-1的元素

120
00:07:56,035 --> 00:07:59,850
能够影响到门控单元的元素

121
00:07:59,850 --> 00:08:04,720
因此，c_t-1的第一个元素影响
 门控的第一个元素

122
00:08:04,720 --> 00:08:07,395
第二个元素影响第二个元素，以此类推

123
00:08:07,395 --> 00:08:10,870
当你在科技论文中读到窥孔连接的时候

124
00:08:10,870 --> 00:08:16,630
他们表达的意思是，c_t-1会被用来影响门控数值

125
00:08:16,630 --> 00:08:19,720
这就是LSTM

126
00:08:19,720 --> 00:08:21,350
什么时候你应该使用GRU？

127
00:08:21,350 --> 00:08:23,285
什么时候你应该使用LSTM？

128
00:08:23,285 --> 00:08:25,865
现在没有一个普适一致的观点S

129
00:08:25,865 --> 00:08:28,755
虽然我首先介绍了GRU

130
00:08:28,755 --> 00:08:30,730
在深度学习的历史中

131
00:08:30,730 --> 00:08:33,020
LSTM要远远早于GRU

132
00:08:33,020 --> 00:08:37,092
GRU是一个相对近期的发明

133
00:08:37,092 --> 00:08:41,725
用来作为复杂的LSTM模型的简化

134
00:08:41,725 --> 00:08:44,860
研究者在多种不同的问题上使用这两种方法

135
00:08:44,860 --> 00:08:46,350
在不同的问题上

136
00:08:46,350 --> 00:08:47,840
不同的算法各有千秋

137
00:08:47,840 --> 00:08:51,070
所以，不存在一个普适的优秀算法

138
00:08:51,070 --> 00:08:53,630
这就是我为什么要把这两种方法
都介绍给大家的原因

139
00:08:53,630 --> 00:08:56,970
当我使用这些方法的时候

140
00:08:56,970 --> 00:09:00,170
GRU的有点是其模型的简单性

141
00:09:00,170 --> 00:09:03,465
因此更适用于构建较大的网络

142
00:09:03,465 --> 00:09:04,780
它只有两个门控

143
00:09:04,780 --> 00:09:06,940
从计算角度看，它的效率更高

144
00:09:06,940 --> 00:09:10,630
它的可扩展性有利于
构筑较大的模型

145
00:09:10,630 --> 00:09:15,465
但是LSTM更加的强大和灵活，因为它具有三个门控

146
00:09:15,465 --> 00:09:17,875
如果你要从两中方法中选取一个

147
00:09:17,875 --> 00:09:21,550
我认为LSTM是经过历史检验的方法

148
00:09:21,550 --> 00:09:23,015
因此，如果你要选取一个

149
00:09:23,015 --> 00:09:28,510
我认为大多数人会把LSTM作为
默认第一个去尝试的方法

150
00:09:28,510 --> 00:09:30,490
虽然，在过去几年

151
00:09:30,490 --> 00:09:35,050
GRU的势头越来越猛，
我感觉越来越多的团队

152
00:09:35,050 --> 00:09:39,835
同时也用GRU，因为其简单而且效果可以（和LSTM）比拟

153
00:09:39,835 --> 00:09:43,920
可以更容易的将其扩展到更大的问题

154
00:09:43,920 --> 00:09:46,065
LSTM就介绍到这里

155
00:09:46,065 --> 00:09:48,607
不管用是GRU还是LSTM

156
00:09:48,607 --> 00:09:53,430
你可以用他们来构筑一个
可以获取更长范围相关性的神经网络