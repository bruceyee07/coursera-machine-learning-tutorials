1
00:00:00,400 --> 00:00:02,910
語言建模是最基本也是

2
00:00:02,910 --> 00:00:06,080
最重要的自然語言處理任務之一

3
00:00:06,080 --> 00:00:09,640
是可以充分運用RNN的領域

4
00:00:09,640 --> 00:00:14,880
在這段影片, 你會學到如何建構一個RNN語言模型

5
00:00:14,880 --> 00:00:18,700
這會帶領你進入這周有趣的程式練習

6
00:00:18,700 --> 00:00:20,770
你會建構一個語言模型

7
00:00:20,770 --> 00:00:24,730
並用來產生一段莎士比亞式的文本, 或者其他風格

8
00:00:24,730 --> 00:00:25,720
讓我們開始吧

9
00:00:25,720 --> 00:00:28,150
那甚麼是語言模型呢

10
00:00:28,150 --> 00:00:30,340
假設我們要建構一個語音辨識系統

11
00:00:30,340 --> 00:00:34,945
當你聽到以下句子: 「蘋果和梨子沙拉很好吃」

12
00:00:34,945 --> 00:00:37,395
所以你聽到了甚麼

13
00:00:37,395 --> 00:00:43,475
我是說「蘋果和一對沙拉很好吃」還是「蘋果和梨子沙拉很好吃」(注: 英文中一對(pair)和梨子(pear)同音)

14
00:00:45,282 --> 00:00:49,642
你應該會認為第二句較為可能

15
00:00:49,642 --> 00:00:52,092
而這就是一個好的語音辨識系統能做到的事

16
00:00:52,092 --> 00:00:56,472
即使聽起來相同, 卻能幫我們判斷出是哪一句

17
00:00:58,080 --> 00:01:00,830
而語音辨識系統挑出第二句的方法

18
00:01:00,830 --> 00:01:03,800
是透過語言模型

19
00:01:03,800 --> 00:01:08,280
來告訴我們這兩句子各自的機率為何

20
00:01:08,280 --> 00:01:12,438
例如, 一個語言模型可能會說第一句的

21
00:01:12,438 --> 00:01:16,027
機率是3.2乘以10的-13次方

22
00:01:16,027 --> 00:01:21,800
第二句的機率是5.7乘以10的-10次方

23
00:01:21,800 --> 00:01:27,210
有了這些機率, 第二句是更為可能的

24
00:01:27,210 --> 00:01:32,140
因為和第一句相比10的指數多3

25
00:01:32,140 --> 00:01:35,840
所以系統會挑出第二句

26
00:01:36,980 --> 00:01:42,280
語言模型做的事是給定特定句子

27
00:01:42,280 --> 00:01:46,710
它能告訴你特定句子的機率為何

28
00:01:46,710 --> 00:01:51,481
我說的機率是指, 如果你想拿起隨機一份報紙,

29
00:01:51,481 --> 00:01:54,148
隨機打開一個電子郵件或一個網頁或

30
00:01:54,148 --> 00:01:57,890
聽你的朋友說的下一件事

31
00:01:57,890 --> 00:02:02,377
此特定句子, 比如說剛提到的蘋果和梨子沙拉

32
00:02:02,377 --> 00:02:06,878
在這世界中被使用到機率為何

33
00:02:06,878 --> 00:02:11,351
這是以下兩者的基礎要件: 剛提到的語音辨識系統

34
00:02:11,351 --> 00:02:16,050
和機器翻譯系統

35
00:02:16,050 --> 00:02:20,780
我們會期望它輸出最有可能出現的句子

36
00:02:21,790 --> 00:02:27,547
因此, 語言模型的基礎工作是輸入一個

37
00:02:27,547 --> 00:02:34,103
我會將其寫成y^<1>, y^<2> 到 y^<Ty>的句子

38
00:02:34,103 --> 00:02:37,280
語言模型做的是

39
00:02:37,280 --> 00:02:42,400
將句子表示為 y 而不是 x

40
00:02:42,400 --> 00:02:46,918
但語言模型做的事是預測

41
00:02:46,918 --> 00:02:48,962
該特定字詞序列的機率

42
00:02:53,401 --> 00:02:55,560
那該如何建構語言模型呢

43
00:02:58,530 --> 00:03:01,530
用RNN建構模型

44
00:03:01,530 --> 00:03:06,990
你會需要一個包含許多英文文本語料庫(corpus)的訓練資料

45
00:03:06,990 --> 00:03:10,710
或其他任何你想套用你的語言模型的語言上

46
00:03:10,710 --> 00:03:15,410
而「語料庫(corpus)」是自然語言分析領域的術語,
代表非常大的文體

47
00:03:15,410 --> 00:03:19,330
或是一套包含非常多英文句子的文本

48
00:03:19,330 --> 00:03:23,040
假設你從訓練集裡得到了以下句子

49
00:03:23,040 --> 00:03:25,360
「貓平均一天睡十五小時」

50
00:03:25,360 --> 00:03:29,970
你要做的第一件事是解析標記(tokenize)該句子

51
00:03:29,970 --> 00:03:35,380
這代表你要組成一個像是先前影片中看過的字典

52
00:03:35,380 --> 00:03:40,740
並對應每個單字至, 例如說one-hot向量

53
00:03:40,740 --> 00:03:44,200
或是在你字典中的索引值

54
00:03:44,200 --> 00:03:48,230
有一件你可能也想做的事是
建構句子何時結束的模型

55
00:03:48,230 --> 00:03:53,735
因此, 加入一個稱為EOS的標記(token)是很常見的

56
00:03:53,735 --> 00:04:00,160
那代表「句子結束」(End of Sentence), 
幫助你找出句子何時結束

57
00:04:00,160 --> 00:04:01,330
我們之後會再回頭講這件事

58
00:04:01,330 --> 00:04:06,320
如果你希望你的模型可以判斷句子何時結束

59
00:04:06,320 --> 00:04:11,150
你可以把EOS標記加到你訓練集當中每一個句子的最後

60
00:04:11,150 --> 00:04:15,685
這周末的程式作業中, 我們並不會使用EOS標記

61
00:04:15,685 --> 00:04:19,823
但在某些應用中你可能會想使用它

62
00:04:19,823 --> 00:04:22,740
稍後我們會看到它在哪裡發揮效用

63
00:04:22,740 --> 00:04:27,970
在這個例子當中, 我們有y^<1>到y^<9>九個輸入

64
00:04:27,970 --> 00:04:33,240
如果你把EOS標記加到每一個句子最後

65
00:04:33,240 --> 00:04:35,840
並進行標記解析, 你可以決定

66
00:04:35,840 --> 00:04:38,190
是否把句末當作標記

67
00:04:38,190 --> 00:04:41,270
在這個例子當中, 我會直接忽略標點符號

68
00:04:41,270 --> 00:04:43,938
所以「天(day)」是一個標記

69
00:04:43,938 --> 00:04:48,861
然後忽略句號, 但如果你把句號(或其他標點符號)當作

70
00:04:48,861 --> 00:04:54,070
標記, 那你也可以把句號加到你的字典中

71
00:04:54,070 --> 00:04:58,240
現在, 另一個細節是如果有一些訓練集中字詞

72
00:04:58,240 --> 00:04:59,850
並不在你的字典裏面

73
00:04:59,850 --> 00:05:04,717
假設你的字典包含一萬字, 也許是最常見的一萬個

74
00:05:04,717 --> 00:05:09,032
英文字, 那麼「埃及貓(Mau)」這個字

75
00:05:09,032 --> 00:05:12,343
可能不在這一萬字當中

76
00:05:12,343 --> 00:05:16,773
在這情況下, 你可以把「埃及貓(Mau)」這個字

77
00:05:16,773 --> 00:05:21,128
用一個獨特的標記<UNK>來代替,
代表著不存於字典的字詞

78
00:05:21,128 --> 00:05:25,366
模型當中就把所有字典裡沒有的字詞統一當作一個字詞來處理

79
00:05:25,366 --> 00:05:30,260
進行標記步驟基本上意味著

80
00:05:30,260 --> 00:05:33,500
輸入句子並將其映射到各個獨立標記, 或者說

81
00:05:33,500 --> 00:05:36,390
你字典當中的獨立字詞

82
00:05:36,390 --> 00:05:41,606
接下來, 讓我們建立一個 RNN 以模擬這些不同的序列的機率

83
00:05:41,606 --> 00:05:46,702
我們將在下一張投影片看到的是

84
00:05:46,702 --> 00:05:53,470
你會設定 x^<t><t> 為 y^<t-1><t-1>

85
00:05:53,470 --> 00:05:56,388
那我們將繼續建立 RNN 模型,

86
00:05:56,388 --> 00:06:01,263
我會繼續用這個貓的句子當作範例

87
00:06:01,263 --> 00:06:03,984
這會是RNN架構

88
00:06:03,984 --> 00:06:08,528
在時間0, 你會計算一些啟動函數值 a^<1>

89
00:06:08,528 --> 00:06:13,527
是輸入數值 x^<1> 的函數

90
00:06:13,527 --> 00:06:19,338
而且 x^<1> 會初始為全為零的向量

91
00:06:19,338 --> 00:06:25,685
前面的 a^<0> 習慣上來說也會設定成零向量

92
00:06:25,685 --> 00:06:30,792
但是 a^<1> 做的事情是softmax預測

93
00:06:30,792 --> 00:06:36,574
試著找出第一個單詞的概率 y

94
00:06:36,574 --> 00:06:40,810
所以會寫成 y^<1>

95
00:06:40,810 --> 00:06:45,189
所以這步驟做的其實是嘗試用softmax以預測

96
00:06:45,189 --> 00:06:49,492
字典中字詞的機率為何

97
00:06:49,492 --> 00:06:53,596
第一字是 "a" 或第二字 "Aaron" 的機率是多少

98
00:06:53,596 --> 00:06:58,841
然後句子裡第一字是「貓(Cat)」的機率是多少

99
00:06:58,841 --> 00:07:01,697
或第一字是「祖魯(Zulu)」的機率是多少

100
00:07:01,697 --> 00:07:04,985
或第一字是個未知字詞的機率是多少

101
00:07:04,985 --> 00:07:09,461
或第一字是代表句子結尾的EOS標記的機率是多少

102
00:07:09,461 --> 00:07:10,858
但這應該不太可能會出現, 對吧

103
00:07:10,858 --> 00:07:15,482
y_hat^<1> 是softmax的輸出

104
00:07:15,482 --> 00:07:19,875
他預測了第一字出現特定字詞的機率

105
00:07:19,875 --> 00:07:24,882
在我們的例子中, 出現「貓(Cat)」這個字

106
00:07:24,882 --> 00:07:29,120
所以這是個一萬個softmax輸出, 因為字典還有一萬字

107
00:07:29,120 --> 00:07:33,079
或者說10,002個輸出, 因為把未知字詞和

108
00:07:33,079 --> 00:07:35,893
句子結尾當作標記

109
00:07:35,893 --> 00:07:39,298
然後, RNN前進到下一步

110
00:07:39,298 --> 00:07:43,260
而且a^<1>也傳到下一步

111
00:07:43,260 --> 00:07:47,670
而在這一步, 要嘗試找出第二個字詞為何

112
00:07:48,730 --> 00:07:54,480
但是我們會給他實際的第一個字詞

113
00:07:54,480 --> 00:07:57,666
我們要告訴他

114
00:07:57,666 --> 00:08:01,304
第一個字是「貓(Cat)」 這裡是 y^<1>

115
00:08:01,304 --> 00:08:06,540
用 y^<1> 等於 x^<2> 來告訴它這是「貓」

116
00:08:06,540 --> 00:08:14,860
第二步中同樣使用softmax輸出預測值

117
00:08:14,860 --> 00:08:18,561
RNN的工作是, 不管在句子何處, 無論先前輸入了甚麼

118
00:08:18,561 --> 00:08:23,143
它會估計不同字詞在該處出現的機率

119
00:08:23,143 --> 00:08:26,950
預測是否為"a", "Aaron", "Cats", "Zulu", 未知字詞標記還是句尾標記

120
00:08:26,950 --> 00:08:27,700
那麼這樣的話

121
00:08:27,700 --> 00:08:33,120
在這裡的答案應該是「平均(average)」, 因為句子的開頭是「貓平均」

122
00:08:33,120 --> 00:08:36,418
然後, 你會繼續做RNN下一步

123
00:08:36,418 --> 00:08:39,913
計算出 a^<3>

124
00:08:39,913 --> 00:08:42,760
但要預測第三字詞, 「15」

125
00:08:42,760 --> 00:08:44,800
我們要輸入前面兩個字詞

126
00:08:44,800 --> 00:08:48,175
我們要告訴它「貓(Cats)」和「平均(Average)」是前面兩字詞

127
00:08:48,175 --> 00:08:54,741
所以這裡的輸入 x^<3> 等於代表「平均(average)」的 y^<2>

128
00:08:54,741 --> 00:08:59,839
然後嘗試預測接下來出現甚麼字

129
00:08:59,839 --> 00:09:04,030
換句話說, 當已經出現「貓平均 (Cat average)」,

130
00:09:04,030 --> 00:09:07,005
我們要找出字典中字詞

131
00:09:08,783 --> 00:09:10,142
在那之後出現的機率

132
00:09:10,142 --> 00:09:13,159
這個例子下是「15」機率最大, 接下來也是同樣做法

133
00:09:14,403 --> 00:09:19,513
直到最後的

134
00:09:19,513 --> 00:09:25,061
我猜是在第九步的時間9, 你會需要輸入 x^<9>

135
00:09:25,061 --> 00:09:31,501
它等於 y^<8>, 代表著「天(day)」

136
00:09:31,501 --> 00:09:37,200
然後這裡是 a^<9>, 他的工作是輸出 y_hat^<9>

137
00:09:37,200 --> 00:09:40,690
它代表著 EOS 標記 (句尾標記)

138
00:09:40,690 --> 00:09:45,621
不論句子前面出現了哪些字

139
00:09:45,621 --> 00:09:49,694
我們會預期它將預測EOS在這裡會有最高出現機率

140
00:09:49,694 --> 00:09:50,988
代表著句尾標記

141
00:09:50,988 --> 00:09:55,945
RNN當中的每一步會看看前面出現過的單詞, 例如說

142
00:09:55,945 --> 00:10:01,285
給定三個字, 下一個出現字詞的機率為何?

143
00:10:01,285 --> 00:10:06,193
所以這個RNN模型是從左至右, 一字一字做預測

144
00:10:06,193 --> 00:10:10,995
我們訓練到一個網路之後, 我們將定義成本函數(cost function)。

145
00:10:10,995 --> 00:10:15,004
所以, 在特定的時間 t, 如果真正的字 y^<t>

146
00:10:15,004 --> 00:10:18,741
然後神經網路的softmax函數輸出預測值 y_hat

147
00:10:18,741 --> 00:10:24,963
那這個就是softmax損失函數(loss function), 
你可能已經對它很熟悉

148
00:10:24,963 --> 00:10:29,298
然後總損失函數就是加總在每一步當中

149
00:10:29,298 --> 00:10:32,640
獨立預測單詞的損失函數

150
00:10:32,640 --> 00:10:35,520
當你用一個很大的訓練集來訓練這個RNN

151
00:10:35,520 --> 00:10:42,010
你可以做的是, 給定任何一組字詞,

152
00:10:42,010 --> 00:10:47,120
像是"Cats average 15 hours", 它會預測特定字詞在接下來出現的機率

153
00:10:48,280 --> 00:10:52,150
例如說, 簡單起見, 給定一個含有 y^<1>, y^<2>

154
00:10:52,150 --> 00:10:56,020
和 y^<3>三個詞的例句

155
00:10:56,020 --> 00:11:01,883
找出這整個句子的機率是什麼的方法是

156
00:11:01,883 --> 00:11:06,200
第一個 softmax 函數告訴你 y^<1>的機率

157
00:11:06,200 --> 00:11:08,230
這是第一個輸出

158
00:11:08,230 --> 00:11:15,329
第二個 softmax 告訴你給定 y^<1> 情況之下, y^<2> 機率為何

159
00:11:15,329 --> 00:11:23,895
第三個 softmax 表示給定 y^<1> 和 y^<2>, y^<3> 機率為何

160
00:11:23,895 --> 00:11:27,786
然後將這些機率相乘

161
00:11:27,786 --> 00:11:31,450
你會在前面的程式練習當中見到更多細節

162
00:11:31,450 --> 00:11:36,930
將這三個機率相乘, 你會得到整個句子

163
00:11:36,930 --> 00:11:39,280
這個包含三個單字句子的機率

164
00:11:39,280 --> 00:11:45,230
那這就是一個用RNN訓練語言模型的基礎結構

165
00:11:45,230 --> 00:11:49,120
如果有些地方對你來說很抽象, 請不要擔心

166
00:11:49,120 --> 00:11:52,606
你會在程式作業當中獲得更多練習

167
00:11:52,606 --> 00:11:56,490
但接下來你可以用語言模型做的一個很有趣的事是

168
00:11:56,490 --> 00:11:59,150
是從模型當中取樣序列

169
00:11:59,150 --> 00:12:00,880
讓我們在下一段影片看看它