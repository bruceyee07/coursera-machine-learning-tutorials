1
00:00:00,000 --> 00:00:03,420
你已经学习了RNN如何运作

2
00:00:03,420 --> 00:00:06,840
如何将其应用于识别人名之类的问题

3
00:00:06,840 --> 00:00:09,537
还有语言建模问题，

4
00:00:09,537 --> 00:00:12,330
并且你也看到了反向传播
在训练RNN中的使用

5
00:00:12,330 --> 00:00:15,400
事实证明，
一个基本的 RNN算法的问题就是

6
00:00:15,400 --> 00:00:20,660
它会产生梯度消失问题。

7
00:00:20,660 --> 00:00:23,245
让我们来探讨这个问题，
然后在接下来几节视频中

8
00:00:23,245 --> 00:00:27,102
我们谈谈解决这个问题的几种方法。

9
00:00:27,102 --> 00:00:30,710
你已经看过这样的 RNN照片。

10
00:00:30,710 --> 00:00:34,585
让我们以一个语言建模作为例子。

11
00:00:34,585 --> 00:00:36,975
假设你看到这个句子，

12
00:00:36,975 --> 00:00:45,400
“the cat which already ate and 
maybe already ate a bunch of food
 that was delicious...

13
00:00:45,400 --> 00:00:50,033
...was full"

14
00:00:50,033 --> 00:00:52,280
所以，为了保持一致

15
00:00:52,280 --> 00:00:53,540
因为 cat 是单数形式，

16
00:00:53,540 --> 00:00:56,300
所以应该是‘cat was’

17
00:00:56,300 --> 00:01:01,665
"the cats which already 
ate a bunch of food was delisious

18
00:01:01,665 --> 00:01:02,730
and apples, and pears

19
00:01:02,730 --> 00:01:06,930
and so on, were full"

20
00:01:06,930 --> 00:01:08,100
所以，为了保持一致

21
00:01:08,100 --> 00:01:14,381
应该是‘cats were’

22
00:01:14,381 --> 00:01:19,285
这是一个语言有长期依赖性的例子，

23
00:01:19,285 --> 00:01:22,020
句子前面的部分可能会

24
00:01:22,020 --> 00:01:26,435
影响句子后面的部分。

25
00:01:26,435 --> 00:01:29,610
但是事实证明，
我们迄今为止遇到的基本的RNN

26
00:01:29,610 --> 00:01:33,355
不擅长捕捉长期的依赖关系。

27
00:01:33,355 --> 00:01:36,465
原因就是，你可能会记起

28
00:01:36,465 --> 00:01:40,035
我们早期关于深层神经网络的讨论。

29
00:01:40,035 --> 00:01:43,590
当时，我们谈到了梯度消失的问题

30
00:01:43,590 --> 00:01:46,040
在非常深的神经网络情况下

31
00:01:46,040 --> 00:01:51,720
100层甚至更深，你执行前向传播过程

32
00:01:51,720 --> 00:01:54,311
从左到右，然后反向传播。

33
00:01:54,311 --> 00:01:57,315
当时我们假设，如果这是一个非常深的神经网络

34
00:01:57,315 --> 00:01:59,530
那么，从输出y 得到的梯度

35
00:01:59,530 --> 00:02:02,460
很难反向传播去

36
00:02:02,460 --> 00:02:05,550
影响前期层的权重，

37
00:02:05,550 --> 00:02:07,865
很难影响前期层的计算。

38
00:02:07,865 --> 00:02:10,545
对于RNN，有一个类似的问题，

39
00:02:10,545 --> 00:02:13,560
你执行前向传播，从左到右，

40
00:02:13,560 --> 00:02:14,985
然后反向传播，

41
00:02:14,985 --> 00:02:18,160
从右到左。

42
00:02:18,160 --> 00:02:20,560
这可能是相当困难的，

43
00:02:20,560 --> 00:02:23,020
由于类似的梯度消失问题，

44
00:02:23,020 --> 00:02:26,500
由于错误输出关系到

45
00:02:26,500 --> 00:02:32,685
之后的步骤，
这影响前期的计算。

46
00:02:32,685 --> 00:02:34,675
所以在实践中，这代表

47
00:02:34,675 --> 00:02:38,140
很难去让一个神经网络意识到，它需要

48
00:02:38,140 --> 00:02:41,759
记住看到了一个单数名词还是复数名词，

49
00:02:41,759 --> 00:02:45,715
所以，随后在序列中决定生成‘was
’还是‘were’

50
00:02:45,715 --> 00:02:49,435
这取决于它是单数还是复数。

51
00:02:49,435 --> 00:02:50,975
请注意，在英语中，

52
00:02:50,975 --> 00:02:53,910
中间部分的长度是任意的，对吧？

53
00:02:53,910 --> 00:02:58,030
所以你可能需要记住单数/复数

54
00:02:58,030 --> 00:03:04,227
很长一段时间后, 直到你可以使用
这一点信息为止。

55
00:03:04,227 --> 00:03:06,185
因此, 由于这个问题,

56
00:03:06,185 --> 00:03:10,240
基本 RNN 模型有附近效应,

57
00:03:10,240 --> 00:03:19,295
这意味着，输出 y ^ <3>
主要受接近于 y ^ <3>的值的影响.

58
00:03:19,295 --> 00:03:23,926
而那里的值主要受它附近的值影响。

59
00:03:23,926 --> 00:03:27,160
这里的输出很难

60
00:03:27,160 --> 00:03:30,950
被输入影响到, 
因为输入是非常早期的序列。

61
00:03:30,950 --> 00:03:33,415
这是因为无论输出是什么,

62
00:03:33,415 --> 00:03:35,180
不管这是否正确, 是否错误,

63
00:03:35,180 --> 00:03:37,600
对于这个区域，很难做到

64
00:03:37,600 --> 00:03:40,475
通过各种方式反向传播到序列的开始，

65
00:03:40,475 --> 00:03:43,620
从而去修改神经网络

66
00:03:43,620 --> 00:03:46,745
在序列前期做的计算。

67
00:03:46,745 --> 00:03:50,590
所以这是基本 RNN 算法的一个缺点。

68
00:03:50,590 --> 00:03:54,385
在下几个视频中还无法解决这个问题

69
00:03:54,385 --> 00:03:57,025
但如果我们不解决它, 那么 RNN

70
00:03:57,025 --> 00:04:01,920
往往不擅长捕获远程依赖关系。

71
00:04:01,920 --> 00:04:06,165
尽管这个讨论集中在梯度消失上,

72
00:04:06,165 --> 00:04:09,160
当我们谈到非常深的神经网络时

73
00:04:09,160 --> 00:04:11,935
我们也谈到了爆炸梯度。

74
00:04:11,935 --> 00:04:13,175
我们做反向传播过程，

75
00:04:13,175 --> 00:04:15,795
梯度不只是指数递减,

76
00:04:15,795 --> 00:04:19,432
它们也可能随着经历的层数指数增加。

77
00:04:19,432 --> 00:04:24,213
事实证明, 梯度消失
往往是训练 RNNs 时更严重的问题,

78
00:04:24,213 --> 00:04:26,920
尽管发生梯度爆炸时,

79
00:04:26,920 --> 00:04:28,630
这可能是灾难性的, 因为

80
00:04:28,630 --> 00:04:31,870
指数级大的梯度可能导致参数

81
00:04:31,870 --> 00:04:37,252
变得很大，以至于
你的神经网络参数变得非常混乱。

82
00:04:37,252 --> 00:04:40,780
事实证明, 爆炸梯度更容易被发现, 因为

83
00:04:40,780 --> 00:04:44,350
参数只是爆炸的话, 
你可能会经常看到 NaN

84
00:04:44,350 --> 00:04:45,950
或者非数字,

85
00:04:45,950 --> 00:04:52,018
这意味着在你的神经网络
计算中数值溢出的结果。

86
00:04:52,018 --> 00:04:53,995
如果你看到了爆炸的梯度,

87
00:04:53,995 --> 00:04:58,015
一个解决方案是应用 gradient clipping

88
00:04:58,015 --> 00:04:59,440
这到底是什么意思呢？

89
00:04:59,440 --> 00:05:02,400
就是看看你的梯度向量,

90
00:05:02,400 --> 00:05:07,025
如果它大于某个临界值

91
00:05:07,025 --> 00:05:10,240
重新缩放某些梯度向量, 使其不那么大

92
00:05:10,240 --> 00:05:13,675
所以这是依据一些最大值的缩减

93
00:05:13,675 --> 00:05:16,335
所以如果你看到了梯度爆炸,

94
00:05:16,335 --> 00:05:18,758
如果你的导数爆炸或者你看到 NaN

95
00:05:18,758 --> 00:05:21,355
只需应用 gradient clipping

96
00:05:21,355 --> 00:05:26,910
这是一个相当好用的解决方案,
它将处理爆炸的梯度。

97
00:05:26,910 --> 00:05:30,365
但是梯度消失更难解决

98
00:05:30,365 --> 00:05:34,227
并且这将是接下来几个视频的主题

99
00:05:34,227 --> 00:05:36,730
综上所述, 在早期的课程中,

100
00:05:36,730 --> 00:05:39,470
你看到了如何训练非常深的神经网络,

101
00:05:39,470 --> 00:05:43,950
利用导数，你可能会
陷入到梯度消失或爆炸的问题中,

102
00:05:43,950 --> 00:05:46,480
要么呈指数递减, 要么

103
00:05:46,480 --> 00:05:50,070
呈指数递增，成为关于层数的函数。

104
00:05:50,070 --> 00:05:54,708
在 RNN 中, RNN 处理数据超过1000次集,

105
00:05:54,708 --> 00:05:56,038
超过1万次集,

106
00:05:56,038 --> 00:06:00,490
这基本上是使用1000层
或者1万层神经网络,

107
00:06:00,490 --> 00:06:04,075
因此, 也会遇到这类问题。

108
00:06:04,075 --> 00:06:08,875
梯度爆炸, 你可以使用
gradient clipping 解决，

109
00:06:08,875 --> 00:06:12,185
但是我们需要更多操作
来解决梯度消失问题

110
00:06:12,185 --> 00:06:14,650
所以我们在下一个视频中
所谈论的是 GRU,

111
00:06:14,650 --> 00:06:16,315
称作 greater recurrent units

112
00:06:16,315 --> 00:06:19,282
这是一个非常有效的方案, 用以解决

113
00:06:19,282 --> 00:06:21,690
梯度消失问题, 并将允许

114
00:06:21,690 --> 00:06:25,805
神经网络来捕获更长范围的依赖性

115
00:06:25,805 --> 00:06:28,000
所以, 让我们继续下一个视频。