Şimdiye kadar RNN'lerin (Özyineli Sinirsel Ağların) temel yapı taşlarını öğrendiniz. Fakat çok daha kuvvetli modeller üretebileceğiniz iki fikir daha var. Biri Çift Yönlü RNN'ler. Bunlar sayesinde dizinin hem önceki zamanlarından hem de ileriki zamanlardan bilgileri kullanabiliyorsunuz. Bu videoda bundan bahsedeceğiz. İkincisi ise Derin RNN'ler. Bunu da bir sonraki videoda göreceksiniz. Çift Yönlü RNN'ler ile bağlayalım. Çift Yönlü RNN'lere neden ihtiyaç duyduğumuzu anlamak için gelin Varlık İsmi Tanıma ile ilgili olarak daha önce de gördüğümüz şu ağa bakalım. Bu ağın sorunlarından biri şuydu, üçüncü kelime olan Teddy'nin bir insan ismi olup olmadığını anlamak için cümlenin ilk bölümüne bakmak yeterli olmuyordu. Yani y_3 çıktısının sıfır mı yoksa bir mi olacağını bilebilmek için ilk üç kelimeden fazlasına ihtiyacınız vardı. Çünkü ilk üç kelime size oyuncak ayının mı (teddy bear) yoksa ABD başkanı Teddy Roosevelt'in mi kastettiğini söylemiyordu. Buradaki sadece tek yönlü ya da ileri yönlü bir RNN. Ve bu son söylediğim şey, bu birimler standart RNN de olsa GRU veya LSTM de olsa geçerli. Tüm bu bloklar sadece ileri yönlü işliyor. Yani Çift Yönlü RNN ya da BRNN dediğimiz şeyin yaptığı bu sorunu çözmek. Çöft Yönlü RNN şöyle çalışır. Basitleştirilmiş 4 sözcüklü bir cümle kullanacağım. Yani 4 girdimiz var. x_1'den x_4'e kadar. Bu ağın gizli katmanı ileri yönlü özyineli öğelere sahip olacak. Bunlara a_1, a_2, a_3 ve a_4 diyeceğim. Ve şuraya sağa doğru giden bir ok çizeceğim. Bu ok bunun ileri yönlü bir öğe olduğunu gösterecek. Bu yüzden bu şekilde bağlanacak. Ve bu özyineli birimlerin her biri o anki x değerini alarak y^_1, y^_2 y^_3 ve y^_4'ü tahmin edebilmek için kullanacak. Yani şu ana kadar hiçbir şey yapmadım. Önceki slaytlardaki RNN'i tekrar çizdik. Ve tuhaf yerlere oklar koyduk. Fakat bu okları bu tuhaf yerlere koydum çünkü geri yönlü bir özyineli katman daha ekleyeceğiz. Yani yine a_1 olacak, ama bu sefer sola bakan okla geri yönlü bağlantıyı simgeleyecek. Sonra a_2, geri yönlü. a_3 geri yönlü. Ve a_4 geri yönlü. Yani sola bakan ok geri yönlü bağlantı demek. Sonra ağı şu şekilde bağlantılandırıyoruz. Ve tüm bu geri yönlü a bağlantıları zamanda geriye doğru birbirine bağlanacak. Dikkat ederseniz bu ağ Döngüsüz Düz Ağaç formunda (Acyclic Graph). Yani x_1'den x_4'e bir girdi dizisi verildiğinde ileri yönlü dizi önce a_ileri_1'i sonra a_ileri_2'yi sonra a_ileri_3'ü ve sonra a_ileri_4'ü hesaplayacak. Geri yönlü dizi a_geri_4'den başlayıp a_geri_3'e doğru geri gidecek. Dikkat edin bu şekilde ağ aktivasyonlarını hesaplıyoruz. Yani bu Geriye Yayılım (back-prop) değil İleri Yayılım. Ama İleri Yayılım hesaplamasının bir bölümü soldan sağa doğru giderken diğer bir bölümü sağdan sola doğru gidiyor. a_geriye_3'ü hesapladıktan sonra bu aktivasyonları kullanarak a_geriye_2'yi ve a_geriye_1'i hesaplayabilirsiniz. Ve tüm aktivasyonları hesapladıktan sonra artık tahminlerinizi hesaplayabilirsiniz. Yani örneğin tahmin yapmak için ağınız t zamanındaki y^ tahminini yapacaksa aktivasyon fonksiyonu içinde W_y, hem t anındaki ileri yönlü aktivasyona hem de t anındaki geri yönlü aktivasyona uygulanacak. Diyelim t'nin 3'e eşit olduğu zamandaki tahmine bakarsak x_1'den gelen bilgi buradan akabilir, ileri 1 den ileri 2'ye, x_2'den ayrıca bilgi alacak, sonra ileri 3'e ve y^_3'e kadar gelecek. Yani x_1, x_2, x_3'ten gelen bilgilerin hepsi dikkate alınacak. x_4'ten gelen bilgi ise geriye 4, geriye 3 üzerinden y^_3'e akacak. Böylece t=3 zamanı için yapılacak tahminin hem geçmişten bilgi alması hem şimdiki zamandan bilgi alması, ki hem ileri hem de geri yönlü adımlarda şimdiki zaman kullanılıyor, hem de gelecekten bilgi alması sağlanıyor. Örneğimize dönersek "He said, Teddy Roosevelt..." diye bir cümle parçası verildiğinde Teddy'nin bir isim olup olmadığını tahmin etmek için gelecek ve geçmişten bilgi kullanıyorsunuz. Çift Yönlü Özyineli Sinirsel Ağ işte bu ve burada gördüğünüz bloklar sadece standart RNN blokları değil GRU veya LSTM blokları da olabilir. İşin aslı birçok Doğal Dil İşleme (DDİ) probleminde, doğal dil işleme gerektiren metin bazlı problemlerde LSTM içeren Çift Yönlü RNN'ler sıklıkla kullanılıyor. Yani bir DDİ problemimiz varsa, örneğin bir cümleyi tamamlamak veya cümledeki şeyleri etiketlemek istiyorsanız LSTM bloklu Çift Yönlü RNN'ler ilk denenecek mantıklı şeylerden biri. Çift Yönlü RNN'ler için bu kadar. Bu temel RNN, GRU veya LSTM mimarisine yapılan bir değişiklik. Ve bu değişik sayesinde RNN, GRU ve LSTM'ler dizinin ortasındaki bir tahmin için bile bütün diziden gelen bilgiyi kullanma potansiyeline sahip. Çift Yönlü RNN'lerin dezavantajı ise tahmin üretebilmek için bütün veri dizisinin elinizde olması gerektiği. Örneğin bir Konuşma Tanıma sistemi yapıyorsanız BRNN (Çift Yönlü RNN) tüm konuşmanın tamamındaki sesleri kullanmanıza izin veriyor ama buradaki basit uygulamayı kullanırsanız tüm sesleri toplayıp, işlemeye başlayıp bir konuşma tanıma tahmini üretmeden önce konuşan kişinin konuşmasını bitirmesini beklemeniz gerekir. Gerçek hayattaki konuşma tanıma uygulamaları için burada gördüğünüz standart Çift Yönlü RNN yerine daha karmaşık modüller de kullanılabiliyor. Fakat tüm cümleyi aynı anda aldığınız bir çok doğal dil işleme uygulaması için buradaki standart BRNN algoritmaları oldukça etkili. BRNN'ler için bu kadar. Sıradaki bu haftanın son videosunda tüm bu fikirleri, RNN'leri, LSTM'leri ve GRU'ları ve bunların Çift Yönlü versiyonlarını alalım ve Derin versiyonlarını yapalım.