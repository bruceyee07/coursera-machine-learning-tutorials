1
00:00:00,000 --> 00:00:03,585
你现在已经学习了RNN的基本结构

2
00:00:03,585 --> 00:00:08,730
在这个视频中，你将会看到循环神经网络是如何反向传播的

3
00:00:08,730 --> 00:00:12,570
通常情况下，当你在一个框架中实现这个的时候，很多时候，

4
00:00:12,570 --> 00:00:16,848
编程框架会自动处理好反向传播

5
00:00:16,848 --> 00:00:21,887
但是我认为对RNN中反向传播是如何工作的有一个粗略的理解也是很有用的

6
00:00:21,887 --> 00:00:23,575
让我们来看看

7
00:00:23,575 --> 00:00:25,719
你已经看到过了，对于一个前向传播，

8
00:00:25,719 --> 00:00:33,965
在神经网络中会进行从左到右的激活值计算，

9
00:00:33,965 --> 00:00:37,365
直到输出所有的预测值。

10
00:00:37,365 --> 00:00:40,605
在反向传播中，你可能已经猜到了，

11
00:00:40,605 --> 00:00:42,375
你最终会在

12
00:00:42,375 --> 00:00:46,830
与正向传播相反的方向上

13
00:00:46,830 --> 00:00:49,071
进行反向传播的计算。

14
00:00:49,071 --> 00:00:52,815
让我们来进行正向传播的计算。

15
00:00:52,815 --> 00:00:56,120
给定输入的序列x_1, x_2,

16
00:00:56,120 --> 00:01:01,690
x_3...x_tx

17
00:01:01,690 --> 00:01:06,385
然后使用 x_1和 a_0

18
00:01:06,385 --> 00:01:11,390
计算激活值，乘以这个值

19
00:01:11,390 --> 00:01:17,490
然后加在一起，又使用 x_2 和 a_1 去计算 a_2

20
00:01:17,490 --> 00:01:24,853
然后是 a_3......，直到 a_tx。

21
00:01:24,853 --> 00:01:27,753
实际上计算 a_1,

22
00:01:27,753 --> 00:01:29,760
同样需要参数。

23
00:01:29,760 --> 00:01:32,595
W_a 和 b_a(我们用绿色符号表示)，

24
00:01:32,595 --> 00:01:35,570
这些就是用来计算 a_1 的参数。

25
00:01:35,570 --> 00:01:39,950
事实上，这些参数被用在每一个时间步上，即是说

26
00:01:39,950 --> 00:01:45,035
这些参数被用来计算 a_2, a_3，

27
00:01:45,035 --> 00:01:52,329
等等。直到最后一个时间步，所有的激活值取决于参数 W_a 和 b_a。

28
00:01:52,329 --> 00:01:55,090
让我们继续完善这张图

29
00:01:55,090 --> 00:02:01,980
现在，给定 a_1, 你的网络可以计算第一个预测值， y-hat_1,

30
00:02:01,980 --> 00:02:07,340
然后是第二个时间步，y-hat_2, y-hat_3,

31
00:02:07,340 --> 00:02:14,985
等等，包括 y-hat_ty。

32
00:02:14,985 --> 00:02:18,600
我将继续用不同的颜色来表示参数。

33
00:02:18,600 --> 00:02:22,590
因此，为了计算 y-hat，

34
00:02:22,590 --> 00:02:24,700
你需要参数

35
00:02:24,700 --> 00:02:27,350
W_y 和 b_y,

36
00:02:27,350 --> 00:02:32,230
这两个参数在这个节点中，跟其他的一样。

37
00:02:32,230 --> 00:02:34,900
所以同样用绿色的表示。

38
00:02:34,900 --> 00:02:36,670
接下来，为了计算反向传播，

39
00:02:36,670 --> 00:02:39,361
你需要一个损失函数。

40
00:02:39,361 --> 00:02:42,370
我们先定义一个逐元素的损失，

41
00:02:42,370 --> 00:02:45,625
该损失假设一个特定的词语出现在序列中。

42
00:02:45,625 --> 00:02:46,975
这是一个人名，

43
00:02:46,975 --> 00:02:48,220
则 y_t=1。

44
00:02:48,220 --> 00:02:51,395
你的网络输出一个可能性，比如

45
00:02:51,395 --> 00:02:55,303
0.1的可能性这个词是一个人名。

46
00:02:55,303 --> 00:03:00,385
 所以我要把它定义为标准的逻辑回归损失，

47
00:03:00,385 --> 00:03:04,045
也称为交叉熵损失。

48
00:03:04,045 --> 00:03:07,240
这跟我们之前遇到过的

49
00:03:07,240 --> 00:03:10,502
二分类问题很相似。

50
00:03:10,502 --> 00:03:12,190
所以这个损失表达的是

51
00:03:12,190 --> 00:03:15,671
对一个单位置的预测，或者说是

52
00:03:15,671 --> 00:03:17,661
对于一个单词单一时间步t的预测。

53
00:03:17,661 --> 00:03:21,700
现在来定义整个序列的整体损失，

54
00:03:21,700 --> 00:03:28,235
L 定义为 t 从1到 T_x 或 T_y

55
00:03:28,235 --> 00:03:29,940
的总和。

56
00:03:29,940 --> 00:03:33,890
在这个例子中，T_x 等于 T_y

57
00:03:33,890 --> 00:03:38,605
对于单一时间步来说。

58
00:03:38,605 --> 00:03:42,712
然后，只需要这个没有

59
00:03:42,712 --> 00:03:47,575
上标 T 的 L。这就是整个序列的损失。

60
00:03:47,575 --> 00:03:48,910
因此，在一个计算图中，

61
00:03:48,910 --> 00:03:52,225
为了计算损失，

62
00:03:52,225 --> 00:03:54,380
给定了y-hat_1，你可以计算第一个时间步的损失，

63
00:03:54,380 --> 00:03:59,785
给定了这个(y-hat_2)，可以计算第二个时间步的损失，

64
00:03:59,785 --> 00:04:03,094
计算第三个时间步的损失，

65
00:04:03,094 --> 00:04:07,265
等等......直到最后一个时间步的损失。

66
00:04:07,265 --> 00:04:11,945
最后，为了计算全局损失，

67
00:04:11,945 --> 00:04:19,525
我们使用该公式，将它们全部加起来计算最终的 L，

68
00:04:19,525 --> 00:04:23,550
即每个时间步的损失总和。

69
00:04:23,550 --> 00:04:25,975
所以这就是一个计算问题，

70
00:04:25,975 --> 00:04:29,950
你在之前的很多例子中已经见过了反向传播，

71
00:04:29,950 --> 00:04:34,390
所以你并不会感到惊讶，反向传播仅仅是要求做一个

72
00:04:34,390 --> 00:04:39,880
反方向的计算或者说传递信息。

73
00:04:39,880 --> 00:04:43,700
即这4个时间步的箭头，

74
00:04:43,700 --> 00:04:46,605
最后是这么做。

75
00:04:46,605 --> 00:04:53,095
这使得你能够计算合适数量的

76
00:04:53,095 --> 00:04:55,705
参数的梯度，

77
00:04:55,705 --> 00:04:59,305
然后使用梯度下降更新参数。

78
00:04:59,305 --> 00:05:02,070
在这个反向传播的过程中，

79
00:05:02,070 --> 00:05:08,575
最重要的消息或最重要的传递计算就是这个，

80
00:05:08,575 --> 00:05:11,260
它从右到左，

81
00:05:11,260 --> 00:05:13,600
这也是为了给这个算法起了这么个名字：

82
00:05:13,600 --> 00:05:17,380
基于时间的反向传播。

83
00:05:17,380 --> 00:05:20,180
这个算法名字的动机是：对于前向传播，

84
00:05:20,180 --> 00:05:22,305
你从左到右扫描，

85
00:05:22,305 --> 00:05:25,075
增加了时间索引 t，

86
00:05:25,075 --> 00:05:27,695
对于反向传播，则是从右到左，

87
00:05:27,695 --> 00:05:29,380
同样是根据时间。

88
00:05:29,380 --> 00:05:31,870
这就给这个算法一个我认为特别酷的名字，

89
00:05:31,870 --> 00:05:36,180
基于时间的反向传播(在时间步上传播）。

90
00:05:36,180 --> 00:05:40,039
 这句话真的会让你感觉你需要一台时间机来实现这个输出，

91
00:05:40,039 --> 00:05:42,460
但是我认为这只是

92
00:05:42,460 --> 00:05:45,360
该算法众多名字中最酷的一个。

93
00:05:45,360 --> 00:05:50,890
我希望这能让你了解到RNN中的前向和后向是如何工作的。

94
00:05:50,890 --> 00:05:54,766
现在，到目前为止，你只是看了RNN中的一个主要的案例，

95
00:05:54,766 --> 00:06:00,220
这个案例的输入和输出的序列长度是相等的。

96
00:06:00,220 --> 00:06:01,660
在下一个视频中，

97
00:06:01,660 --> 00:06:06,033
我想要给你展示更广泛的RNN结构，

98
00:06:06,033 --> 00:06:10,880
因此我将让你解决更广泛的应用集。 让我们继续。