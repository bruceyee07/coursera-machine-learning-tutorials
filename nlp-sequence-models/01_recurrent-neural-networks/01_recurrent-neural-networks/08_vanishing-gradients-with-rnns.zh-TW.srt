1
00:00:00,000 --> 00:00:03,420
你已經了解RNN是如何運作的

2
00:00:03,420 --> 00:00:06,840
也知道如何將其運用到具體問題上, 如實體識別問題

3
00:00:06,840 --> 00:00:09,537
和語言模型

4
00:00:09,537 --> 00:00:12,330
你也看到了如何將反向傳播運用於RNN訓練

5
00:00:12,330 --> 00:00:15,400
但基本RNN演算法還有一個問題

6
00:00:15,400 --> 00:00:20,660
那就是梯度消失問題

7
00:00:20,660 --> 00:00:23,245
我們會在之後的影片中討論它

8
00:00:23,245 --> 00:00:27,102
我們會討論一些解決這個問題的方法

9
00:00:27,102 --> 00:00:30,710
你已經知道RNN的圖形就像這樣

10
00:00:30,710 --> 00:00:34,585
現在我們將會舉個語言模型的例子

11
00:00:34,585 --> 00:00:36,975
假設你看到這個句子

12
00:00:36,975 --> 00:00:45,400
「那隻貓已經吃了很多好吃食物

13
00:00:45,400 --> 00:00:50,033
等等...,然後吃飽了」

14
00:00:50,033 --> 00:00:52,280
前後應該保持一致

15
00:00:52,280 --> 00:00:53,540
因為貓是單數

16
00:00:53,540 --> 00:00:56,300
所以應該是用單數的「是(was)」

17
00:00:56,300 --> 00:01:01,665
「那隻貓已經吃了很多好吃的食物

18
00:01:01,665 --> 00:01:02,730
有蘋果和梨子

19
00:01:02,730 --> 00:01:06,930
...等等, 然後吃飽了」

20
00:01:06,930 --> 00:01:08,100
為了保持一致

21
00:01:08,100 --> 00:01:14,381
單數的貓(cat)要用單數的是(was), 或複數的貓(cats)和是(were)

22
00:01:14,381 --> 00:01:19,285
這是個範例, 說明句子中位置相差很大的字詞仍可能有相關

23
00:01:19,285 --> 00:01:22,020
這裡很早出現的字詞

24
00:01:22,020 --> 00:01:26,435
仍會對後面出現的字詞有影響

25
00:01:26,435 --> 00:01:29,610
但我們目前見到的基本RNN模型

26
00:01:29,610 --> 00:01:33,355
仍然不擅長捕捉這種長期依賴關係

27
00:01:33,355 --> 00:01:36,465
這裡解釋一下, 你可能還記得

28
00:01:36,465 --> 00:01:40,035
之前討論過在訓練很深的網路時

29
00:01:40,035 --> 00:01:43,590
會有梯度消失的問題

30
00:01:43,590 --> 00:01:46,040
所以這是一個非常深的網路

31
00:01:46,040 --> 00:01:51,720
假設有100層或更多, 你會從左至右做正向傳播

32
00:01:51,720 --> 00:01:54,311
然後再做反向傳播

33
00:01:54,311 --> 00:01:57,315
我們曾說過, 如果這是很深的網路

34
00:01:57,315 --> 00:01:59,530
那從輸出 y 得到的梯度

35
00:01:59,530 --> 00:02:02,460
會非常難以做反向傳播

36
00:02:02,460 --> 00:02:05,550
並進而難以影響前面幾層的權重

37
00:02:05,550 --> 00:02:07,865
難以影響這裡的計算

38
00:02:07,865 --> 00:02:10,545
對於有著類似問題的RNN

39
00:02:10,545 --> 00:02:13,560
也是一樣要從左至右做正向傳播

40
00:02:13,560 --> 00:02:14,985
然後再由右至左

41
00:02:14,985 --> 00:02:18,160
做反向傳播

42
00:02:18,160 --> 00:02:20,560
但反向傳播的進行較為困難

43
00:02:20,560 --> 00:02:23,020
因為同樣有梯度消失問題

44
00:02:23,020 --> 00:02:26,500
後面幾層輸出的誤差

45
00:02:26,500 --> 00:02:32,685
很難影響前面幾層中的權重

46
00:02:32,685 --> 00:02:34,675
它代表的是, 實際上

47
00:02:34,675 --> 00:02:38,140
很難讓一個神經網路了解到它

48
00:02:38,140 --> 00:02:41,759
需要去注意名詞是單數或者複數

49
00:02:41,759 --> 00:02:45,715
然後在序列模型中根據它參考到的名詞

50
00:02:45,715 --> 00:02:49,435
判斷是要生成 was 或者 were

51
00:02:49,435 --> 00:02:50,975
而且在英文中

52
00:02:50,975 --> 00:02:53,910
句子中間的這個部分其實可以是任意長度的

53
00:02:53,910 --> 00:02:58,030
所以需要把名詞是單數抑或複數

54
00:02:58,030 --> 00:03:04,227
記住相當長一段時間, 直到你需要用到這項資訊

55
00:03:04,227 --> 00:03:06,185
因為這個問題

56
00:03:06,185 --> 00:03:10,240
基本RNN模型會有很多在局部的影響

57
00:03:10,240 --> 00:03:19,295
代表這裡的輸出 ŷ˂³˃ 主要是受到它附近的值所影響

58
00:03:19,295 --> 00:03:23,926
而這裡的值則會被附近的輸入值所影響

59
00:03:23,926 --> 00:03:27,160
這讓這裡的輸出很難受到

60
00:03:27,160 --> 00:03:30,950
序列中較為前面的輸入所影響

61
00:03:30,950 --> 00:03:33,415
這是因為不管輸出是甚麼

62
00:03:33,415 --> 00:03:35,180
不管這是否正確

63
00:03:35,180 --> 00:03:37,600
這裡的結果都很難透過反向傳播

64
00:03:37,600 --> 00:03:40,475
去影響到較為前面的序列

65
00:03:40,475 --> 00:03:43,620
進而去改進神經網路中

66
00:03:43,620 --> 00:03:46,745
前面序列的計算

67
00:03:46,745 --> 00:03:50,590
這是基本RNN演算法的弱點

68
00:03:50,590 --> 00:03:54,385
我們會在接下來幾個影片當中處理這個問題

69
00:03:54,385 --> 00:03:57,025
如果不處理這個問題, 那麼

70
00:03:57,025 --> 00:04:01,920
RNN在這些具有長期依賴關係的問題
就沒辦法表現得很好

71
00:04:01,920 --> 00:04:06,165
儘管我們這裡都是在討論梯度消失問題

72
00:04:06,165 --> 00:04:09,160
你還記得我們也提到非常深層的神經網路

73
00:04:09,160 --> 00:04:11,935
也有提到梯度爆炸的問題

74
00:04:11,935 --> 00:04:13,175
當進行反向傳播時

75
00:04:13,175 --> 00:04:15,795
梯度不僅可能呈現指數下降

76
00:04:15,795 --> 00:04:19,432
當計算到越深層時, 也可能呈現指數上升

77
00:04:19,432 --> 00:04:24,213
事實上, 當訓練RNN網路時, 儘管梯度消失是較為嚴重的問題

78
00:04:24,213 --> 00:04:26,920
但梯度爆炸也是有可能發生

79
00:04:26,920 --> 00:04:28,630
這可能使神經網路崩潰

80
00:04:28,630 --> 00:04:31,870
因為指數項非常大的梯度可能會造成

81
00:04:31,870 --> 00:04:37,252
參數也隨之變的非常大, 讓神經網路無法被使用

82
00:04:37,252 --> 00:04:40,780
而梯度爆炸很容易被發現

83
00:04:40,780 --> 00:04:44,350
因為你的參數可能會變成 NaN 或者是

84
00:04:44,350 --> 00:04:45,950
顯示為非數字的情況

85
00:04:45,950 --> 00:04:52,018
代表神經網路計算中出現數值溢位問題

86
00:04:52,018 --> 00:04:53,995
如果你真的遇到了梯度爆炸

87
00:04:53,995 --> 00:04:58,015
有可解決方法是運用梯度修剪(gradient clipping)

88
00:04:58,015 --> 00:04:59,440
它代表的是

89
00:04:59,440 --> 00:05:02,400
觀察你的梯度向量

90
00:05:02,400 --> 00:05:07,025
如果它大於某個閾值

91
00:05:07,025 --> 00:05:10,240
重新縮放梯度向量, 確保它不會太大

92
00:05:10,240 --> 00:05:13,675
是修剪它最大值的方法

93
00:05:13,675 --> 00:05:16,335
如果你遇到梯度爆炸

94
00:05:16,335 --> 00:05:18,758
如果導數非常大或者出現NaN

95
00:05:18,758 --> 00:05:21,355
那就使用梯度修剪

96
00:05:21,355 --> 00:05:26,910
這是梯度爆炸問題的解決方法中, 較為確實好用的

97
00:05:26,910 --> 00:05:30,365
但梯度消失就更難以解決

98
00:05:30,365 --> 00:05:34,227
它會是之後幾個影片的主題

99
00:05:34,227 --> 00:05:36,730
綜上所述, 在前面課程中

100
00:05:36,730 --> 00:05:39,470
我們看到在非常深層的神經網路的訓練中

101
00:05:39,470 --> 00:05:43,950
隨著層數越趨增加

102
00:05:43,950 --> 00:05:46,480
你可能遇到梯度消指數上升而爆炸

103
00:05:46,480 --> 00:05:50,070
或是指數下降而消失的問題

104
00:05:50,070 --> 00:05:54,708
而RNN, 假使要處理含有1,000

105
00:05:54,708 --> 00:05:56,038
甚至10,000個以上時間序列的數據集

106
00:05:56,038 --> 00:06:00,490
那就至少需要1,000或10,000層的神經網路

107
00:06:00,490 --> 00:06:04,075
也遇到了這些類型的問題

108
00:06:04,075 --> 00:06:08,875
梯度爆炸問題可以直接使用梯度修剪方法來解決

109
00:06:08,875 --> 00:06:12,185
但梯度消失就需要更多手續來處理

110
00:06:12,185 --> 00:06:14,650
下一部影片中我們會提到GRU

111
00:06:14,650 --> 00:06:16,315
即為「門閘遞迴單元」

112
00:06:16,315 --> 00:06:19,282
這是個對於梯度消失問題

113
00:06:19,282 --> 00:06:21,690
非常有效的方法

114
00:06:21,690 --> 00:06:25,805
會讓你的神經網路能將資訊記的更為長久

115
00:06:25,805 --> 00:06:28,000
讓我們進入下一段影片