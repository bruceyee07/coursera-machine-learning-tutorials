1
00:00:00,000 --> 00:00:05,595
在上一段影片中，
您看過了我們在序列學習問題中將會使用到的符號

2
00:00:05,595 --> 00:00:08,250
我們現在要談談如何建立一個模型

3
00:00:08,250 --> 00:00:11,470
一個神經網路來將 x 對應到 y

4
00:00:11,470 --> 00:00:16,950
你可以嘗試用一個標準的神經網路來做這項任務

5
00:00:16,950 --> 00:00:19,410
所以, 在前面的例子中

6
00:00:19,410 --> 00:00:21,570
我們有九個輸入單字

7
00:00:21,570 --> 00:00:26,070
您可以想像試著將這九個輸入單字

8
00:00:26,070 --> 00:00:31,790
將九個 one-hot 向量，
餵進一個標準神經網路

9
00:00:31,790 --> 00:00:33,110
也許用一些隱藏層

10
00:00:33,110 --> 00:00:37,040
最終會輸出 9 個數字 0 或者 1

11
00:00:37,040 --> 00:00:41,335
告訴您是否每個字是
一個人的名字的一部分

12
00:00:41,335 --> 00:00:46,580
但這實際上並不可行，
這樣做真的有兩個很重要的問題

13
00:00:46,580 --> 00:00:52,225
首先，輸入跟輸出在不同的例子可能有不同長度

14
00:00:52,225 --> 00:00:57,290
所以並不是所有例子的輸入長度都是Tx或

15
00:00:57,290 --> 00:01:03,250
同樣的輸出長度 Ty, 如果每個句子有一個最大長度

16
00:01:03,250 --> 00:01:06,770
或許您可以填入 0 到每個輸入使得每個輸入

17
00:01:06,770 --> 00:01:11,350
長度都是最大長度，但這似乎不是一個好的表示法

18
00:01:11,350 --> 00:01:14,280
第二個，或許是比較嚴重的問題是

19
00:01:14,280 --> 00:01:17,820
像這樣單純的神經網路架構

20
00:01:17,820 --> 00:01:21,870
它無法在不同位置的單字上共享一些學習的特徵

21
00:01:21,870 --> 00:01:25,605
特別是當神經網路已經學到或許單字

22
00:01:25,605 --> 00:01:31,995
哈利(Harry) 出現在一個位置上
已經給了一個提示這是一個人的名字一部分

23
00:01:31,995 --> 00:01:36,240
如果它能自動地找出哈利(Harry) 出現在

24
00:01:36,240 --> 00:01:41,500
其他位置 xt 時，也代表是一個人的名字，
這樣不是很棒嗎？

25
00:01:41,500 --> 00:01:47,145
這或許類似您在卷積神經網路見過

26
00:01:47,145 --> 00:01:50,100
您想從一個影像其中一個部分學到的

27
00:01:50,100 --> 00:01:53,145
能夠很快地應用到影像其他部分

28
00:01:53,145 --> 00:01:57,310
我們在序列資料也想這樣做

29
00:01:57,310 --> 00:02:00,870
類似您在卷積網路看到的

30
00:02:00,870 --> 00:02:06,435
一個好的表示式會讓您
減少您模型裡大量的參數

31
00:02:06,435 --> 00:02:10,330
前面談過，我們說每一個這些都是 10,000

32
00:02:10,330 --> 00:02:13,920
維度 one-hot 向量，所以這是一個

33
00:02:13,920 --> 00:02:17,190
相當大的輸入層

34
00:02:17,190 --> 00:02:22,750
整個輸入大小會是最大數量的單字乘 10,000

35
00:02:22,750 --> 00:02:27,950
而這第一層的權重矩陣
將會是非常龐大數量的參數

36
00:02:27,950 --> 00:02:31,890
所以在我們即將描述的遞迴神經網路中

37
00:02:31,890 --> 00:02:36,735
在下一個投影片中描述的，就不會有這些缺點

38
00:02:36,735 --> 00:02:39,925
什麼是遞迴神經網路？

39
00:02:39,925 --> 00:02:41,895
我們來建立這個模型

40
00:02:41,895 --> 00:02:46,175
當您從左到右讀一段句字

41
00:02:46,175 --> 00:02:50,535
讀到的第一個單字假設是 X1

42
00:02:50,535 --> 00:02:53,570
我們要做的是，拿第一個單字

43
00:02:53,570 --> 00:02:56,480
餵進一個神經網路層

44
00:02:56,480 --> 00:02:59,005
我這樣畫

45
00:02:59,005 --> 00:03:03,080
這是第一個神經網路的隱藏層

46
00:03:03,080 --> 00:03:07,330
我們可以試著讓這個神經網路預估一個輸出

47
00:03:07,330 --> 00:03:09,960
預估這是否是一個人姓名的一部分

48
00:03:09,960 --> 00:03:14,599
遞迴神經網路做的事是

49
00:03:14,599 --> 00:03:19,890
當它繼續讀取句子中第二個字詞x^<2>時

50
00:03:19,890 --> 00:03:26,730
除了只用 X^<2> 做為預估 y2 輸出外

51
00:03:26,730 --> 00:03:33,700
它同時也將第一時間步計算的結果作為輸入

52
00:03:33,700 --> 00:03:40,365
特別是，從第一時間步產生的啟動值也將傳遞到第二時間步

53
00:03:40,365 --> 00:03:43,065
接著在下一時間步驟

54
00:03:43,065 --> 00:03:47,920
遞迴神經網路輸入

55
00:03:47,920 --> 00:03:53,715
第三個單字，試著輸出一些預估

56
00:03:53,715 --> 00:04:02,305
y-hat<3> 等等

57
00:04:02,305 --> 00:04:11,650
直到最後一步，輸入為 x<Tx> 輸出為 y-hat<Ty>

58
00:04:11,650 --> 00:04:15,290
至少在這個例子

59
00:04:15,290 --> 00:04:22,000
Tx 跟 Ty 相等，
當 Tx 跟 Ty 不同時，架構會稍許變更

60
00:04:22,000 --> 00:04:24,365
在每個時間步驟

61
00:04:24,365 --> 00:04:27,340
遞迴神經網路傳遞

62
00:04:27,340 --> 00:04:30,460
啟動值給下一個時間步驟使用

63
00:04:30,460 --> 00:04:33,715
要使這整件事情開始動起來

64
00:04:33,715 --> 00:04:38,100
我們也需要一些手工打造的啟動值在第零時間步驟

65
00:04:38,100 --> 00:04:41,025
通常使用零向量

66
00:04:41,025 --> 00:04:44,030
有一些研究人員會使用隨機值當做 a_zero

67
00:04:44,030 --> 00:04:48,480
您可以使用其他方式來初始 a_zero 
但說實在

68
00:04:48,480 --> 00:04:54,185
使用零向量當做第零時間步驟的啟動值是最常見的選擇

69
00:04:54,185 --> 00:04:56,530
所以使用這個輸入到神經網路中

70
00:04:56,530 --> 00:05:00,605
有一些研究論文或一些書

71
00:05:00,605 --> 00:05:05,370
您會見到這樣畫法的神經網路，如以下圖形

72
00:05:05,370 --> 00:05:10,425
每個步驟您輸入 x 輸出 y-hat

73
00:05:10,425 --> 00:05:11,880
假設這裡是

74
00:05:11,880 --> 00:05:17,020
T索引，然後為了表示遞迴連結

75
00:05:17,020 --> 00:05:19,860
有時候人們會畫一個返回箭頭像這樣

76
00:05:19,860 --> 00:05:21,340
表示這一層會餵回同一個單元

77
00:05:21,340 --> 00:05:27,320
有時候他們會畫一個實心方塊，畫一個實心方塊在這裡

78
00:05:27,320 --> 00:05:29,665
來表示延遲一個時間步驟

79
00:05:29,665 --> 00:05:32,495
我個人認為這樣的遞迴圖形

80
00:05:32,495 --> 00:05:35,390
很難解釋，所以在這個課程

81
00:05:35,390 --> 00:05:39,560
我會傾向於使用
像在左邊的未展開圖形

82
00:05:39,560 --> 00:05:41,540
但如果您看到像右邊這樣的圖形

83
00:05:41,540 --> 00:05:43,960
在課本或者研究論文上

84
00:05:43,960 --> 00:05:47,135
它的意思是，或者說我試著將它想成

85
00:05:47,135 --> 00:05:50,545
把它展開變成像左邊一樣的圖形

86
00:05:50,545 --> 00:05:55,800
遞迴神經網路從左到右掃描資料

87
00:05:55,800 --> 00:05:59,560
每次使用的參數都共享

88
00:05:59,560 --> 00:06:01,890
所以會有一組的參數

89
00:06:01,890 --> 00:06:04,680
我們將在下一張投影片詳細描述

90
00:06:04,680 --> 00:06:09,150
但控制從 x1 連結到隱藏層的參數

91
00:06:09,150 --> 00:06:13,400
會是一組我們寫為 Wax 的參數

92
00:06:13,400 --> 00:06:18,420
同時也將會是同一組參數用在每一個時間步驟上

93
00:06:18,420 --> 00:06:22,090
我想這裡也可以寫上Wax

94
00:06:22,090 --> 00:06:26,220
啟動值的部分，水平連結的部分由

95
00:06:26,220 --> 00:06:29,970
一組參數稱為 Waa 控制

96
00:06:29,970 --> 00:06:35,400
同時也會是同樣的一組參數
使用在每一個時間步驟

97
00:06:35,400 --> 00:06:43,390
同樣地，還有一些參數 Wya 來控制輸出預測

98
00:06:43,840 --> 00:06:48,400
我將在下一張投影片中描述這些參數的運作方式

99
00:06:48,400 --> 00:06:50,940
所以在這個遞迴神經網路

100
00:06:50,940 --> 00:06:55,210
它的意思是當預估 y3 時

101
00:06:55,210 --> 00:07:01,480
它不只使用 x3 的資訊，
同時也使用了 x1 跟 x2 的資訊

102
00:07:01,480 --> 00:07:07,980
因為 x1 的資訊可以用這種方式傳遞，來幫助預估 y3

103
00:07:07,980 --> 00:07:11,580
這樣的遞迴神經網路有一個缺點，它只使用

104
00:07:11,580 --> 00:07:15,625
在序列前方的資訊來做預估

105
00:07:15,625 --> 00:07:18,500
特別是，當預估 y3 時

106
00:07:18,500 --> 00:07:22,125
它並不會使用到 x4

107
00:07:22,125 --> 00:07:24,250
x5, x6 等等單字

108
00:07:24,250 --> 00:07:29,405
這會是一個問題，因為如果您有一個句字

109
00:07:29,405 --> 00:07:32,489
「他說：『泰迪羅斯福是個偉大的總統』」
(He said "Teddy Roosevelt was a great President")

110
00:07:32,489 --> 00:07:36,710
要決定「泰迪(Teddy)」是否為人名的一部分

111
00:07:36,710 --> 00:07:41,380
除了參考前面兩字的資訊

112
00:07:41,380 --> 00:07:44,100
得到後面字詞的資訊也會相當有用

113
00:07:44,100 --> 00:07:47,035
因為這句話也可能是

114
00:07:47,035 --> 00:07:49,410
"他說, 泰迪熊, 正在特賣“ 
"He said teddy bears are on sale"

115
00:07:49,410 --> 00:07:53,340
所以只看前三個單字是無法

116
00:07:53,340 --> 00:07:57,090
確定 “泰迪” (Teddy) 這個單字 是人名一部分

117
00:07:57,090 --> 00:07:58,530
在第一個例子，是如此

118
00:07:58,530 --> 00:08:00,130
第二個例子則不是

119
00:08:00,130 --> 00:08:05,395
但您無法只看前面三個單字來作區別

120
00:08:05,395 --> 00:08:07,280
這是一種限制

121
00:08:07,280 --> 00:08:11,920
在這樣的遞迴神經網路架構下，在某一個時間步驟做預估時

122
00:08:11,920 --> 00:08:15,740
只使用序列前面的資訊來當做輸入

123
00:08:15,740 --> 00:08:19,580
而不使用序列後面的資訊

124
00:08:19,580 --> 00:08:22,895
我們會在後面的影片中解決這個問題

125
00:08:22,895 --> 00:08:28,390
我們會談到的雙向遞迴神經網路 
(bi-directional recurrent neural networks)

126
00:08:28,390 --> 00:08:30,315
但在目前的階段

127
00:08:30,315 --> 00:08:35,300
這樣的簡單單向的神經網路架構

128
00:08:35,300 --> 00:08:38,475
足夠讓我們來解釋主要的概念

129
00:08:38,475 --> 00:08:42,760
到時我們只需要做一些快速的修改
來讓我們可以

130
00:08:42,760 --> 00:08:46,099
做 y-hat<3> 的預估時
使用到兩邊的資訊

131
00:08:46,099 --> 00:08:49,750
序列前面的資訊
跟序列後面的資訊

132
00:08:49,750 --> 00:08:51,815
我們在後面的影片中會談到

133
00:08:51,815 --> 00:08:57,305
現在讓我們來詳細寫下這個神經網路的計算

134
00:08:57,305 --> 00:09:01,700
這個是清乾淨的神經網路圖形

135
00:09:01,700 --> 00:09:04,185
正如我先前提到過的

136
00:09:04,185 --> 00:09:09,510
您從輸入 a<0> 等於零向量開始

137
00:09:09,510 --> 00:09:13,810
接下來, 他的正向傳播看起來像這樣

138
00:09:13,810 --> 00:09:22,580
為了計算 a<1>您需要用一個啟動函數 g 來計算

139
00:09:22,580 --> 00:09:28,185
Waa 乘 a0 加上

140
00:09:28,185 --> 00:09:34,010
Wax 乘上 x1 加偏差值 (bias)

141
00:09:34,010 --> 00:09:36,405
我將它寫為 ba

142
00:09:36,405 --> 00:09:39,310
然後來計算 y-hat

143
00:09:39,310 --> 00:09:42,190
<1> 在第一時間步驟的預估值

144
00:09:42,190 --> 00:09:44,915
那會是一些啟動函數 

145
00:09:44,915 --> 00:09:49,450
或許跟上面用的是
不同的啟動函數

146
00:09:49,450 --> 00:09:59,140
應用到 Wya 乘 a<1> 加 by

147
00:09:59,140 --> 00:10:01,960
我用的符號在下標的約定是

148
00:10:01,960 --> 00:10:05,810
在矩陣中，像是 Wax

149
00:10:05,810 --> 00:10:11,960
第二個索引在這個 Wax 指的是
這個將跟一些 x 相關的值做乘積

150
00:10:11,960 --> 00:10:18,270
而下標的 a 代表這個計算跟 a 的值相關 

151
00:10:18,270 --> 00:10:20,665
同樣地，您注意到這個

152
00:10:20,665 --> 00:10:29,395
Wya 將會跟一個 a 的值相乘，
來計算一個跟 y 有關的值

153
00:10:29,395 --> 00:10:32,545
使用到的啟動函數來計算

154
00:10:32,545 --> 00:10:39,355
啟動值通常在 RNN 中是使用 tanh

155
00:10:39,355 --> 00:10:46,320
有時候也會使用 ReLU, 但 tanh 實際上

156
00:10:46,320 --> 00:10:49,630
是相當常用的

157
00:10:49,630 --> 00:10:53,825
我們還有其他方式來避免梯度消失問題

158
00:10:53,825 --> 00:10:56,275
我們將在之後的課程談到

159
00:10:56,275 --> 00:10:59,470
根據您的輸出

160
00:10:59,470 --> 00:11:02,205
如果是二元分類問題

161
00:11:02,205 --> 00:11:05,725
我猜您會使用 S型啟動函數

162
00:11:05,725 --> 00:11:09,850
或者如果是 k 類分類問題可能使用 softmax

163
00:11:09,850 --> 00:11:15,230
啟動函數的選擇
在於您輸出 y 的種類

164
00:11:15,230 --> 00:11:19,300
所以對於人名辨識任務，y 為 0 或 1

165
00:11:19,300 --> 00:11:23,915
我猜這第二個 g 函數會是 S型啟動函數

166
00:11:23,915 --> 00:11:28,120
我想這裡您可以寫成 g2，
如果您想區別

167
00:11:28,120 --> 00:11:32,280
這可以是不同的啟動函數，
但通常我不這麼做

168
00:11:32,280 --> 00:11:35,330
更一般而言，在 t 時

169
00:11:35,330 --> 00:11:41,715
a<t> 會是 g of Waa 乘 a

170
00:11:41,715 --> 00:11:49,440
從前一個步驟來的，加上 Wax 乘目前的 x 加 ba

171
00:11:49,440 --> 00:11:54,460
而 y-hat 等於 g

172
00:11:54,460 --> 00:12:02,380
可以是不同的啟動函數， g of Wya 乘 a<t> 加 by

173
00:12:03,090 --> 00:12:07,210
所以這個方程式定義了正向傳播

174
00:12:07,210 --> 00:12:12,110
在神經網路上，
您會從 a<0> 是全為 0 的向量開始

175
00:12:12,110 --> 00:12:14,340
然後使用 a<0> 跟 x<1> 

176
00:12:14,340 --> 00:12:16,810
您可以計算 a<1> 跟 y-hat<1>

177
00:12:16,810 --> 00:12:24,610
然後您用 x<2> 跟 a<1> 來計算 a<2> 跟 y-hat<2>

178
00:12:24,610 --> 00:12:26,480
等等，您繼續執行

179
00:12:26,480 --> 00:12:30,595
正向傳播，從圖形的左邊到右邊

180
00:12:30,595 --> 00:12:34,560
現在，為了幫助我們發展更複雜的神經網路

181
00:12:34,560 --> 00:12:38,690
我會使用這些符號並將其些微簡化

182
00:12:38,690 --> 00:12:45,490
讓我將這兩個方程式
複製到下一張投影片

183
00:12:45,490 --> 00:12:48,825
我要做的是，

184
00:12:48,825 --> 00:12:50,770
為了稍微簡化記號

185
00:12:50,770 --> 00:12:55,970
我會用更間單的方式來表達

186
00:12:55,970 --> 00:13:01,890
我會寫成 a<t> 等於 g of 一個矩陣

187
00:13:01,890 --> 00:13:12,500
Wa 乘一個新的量，也就是 a<t-1>, x<t>

188
00:13:12,500 --> 00:13:16,880
然後加上 ba

189
00:13:16,880 --> 00:13:22,865
所以在左、右邊畫底線的部分，應該是相等的 

190
00:13:22,865 --> 00:13:28,000
而我們定義 Wa 的方式是，拿矩陣 Waa

191
00:13:28,000 --> 00:13:30,130
跟這個矩陣 Wax

192
00:13:30,130 --> 00:13:33,235
然後將他們並排放在一起

193
00:13:33,235 --> 00:13:36,180
水平疊在一起如下

194
00:13:36,180 --> 00:13:40,260
這就是矩陣 Wa

195
00:13:40,280 --> 00:13:47,385
舉個例子，如果 a 是一個 100 維度

196
00:13:47,385 --> 00:13:51,920
而在我們的例子中，x 是一個 10,000 維度

197
00:13:51,920 --> 00:13:57,520
Waa 會是一個 100乘100 維度的矩陣

198
00:13:57,520 --> 00:14:03,755
而 Wax 會是一個　100乘10,000 維度的矩陣

199
00:14:03,755 --> 00:14:06,300
所以當我們將這兩個矩陣疊在一起

200
00:14:06,300 --> 00:14:08,945
這個會是100 維度

201
00:14:08,945 --> 00:14:10,800
這個會是 100

202
00:14:10,800 --> 00:14:14,005
而這個會是 10,000 個元素

203
00:14:14,005 --> 00:14:22,295
所以 Wa 會是一個 100乘10100 維度的矩陣

204
00:14:22,295 --> 00:14:25,730
我想這個圖形有點不符比例

205
00:14:25,730 --> 00:14:29,295
因為 Wax 會是一個很寬的矩陣

206
00:14:29,295 --> 00:14:31,665
而這個符號的意義是

207
00:14:31,665 --> 00:14:36,635
就只是拿這兩個向量疊在一起

208
00:14:36,635 --> 00:14:39,145
當您使用這樣的符號時

209
00:14:39,145 --> 00:14:42,010
我們將會用 a<t-1> 這個向量

210
00:14:42,010 --> 00:14:48,215
所以這是個 100 維度，疊在 a<t> (應該是 x<t>) 上面

211
00:14:48,215 --> 00:14:55,425
所以這會是一個 10,100 維度的向量

212
00:14:55,425 --> 00:14:59,700
希望您自己檢查一下，這個矩陣

213
00:14:59,700 --> 00:15:05,705
乘上這個向量，會得到跟原來一樣的量

214
00:15:05,705 --> 00:15:11,340
因為現在，這個矩陣 Waa跟

215
00:15:11,340 --> 00:15:17,935
Wax 乘上這個 a<t-1> 跟 x<t> 向量

216
00:15:17,935 --> 00:15:27,685
這個就等於 Waa 乘 a<t-1> 加上 Wax 乘 x<t>

217
00:15:27,685 --> 00:15:32,260
也正是跟我們原先這邊一樣

218
00:15:32,260 --> 00:15:35,690
用這個符號的好處是
與其到處

219
00:15:35,690 --> 00:15:40,005
使用兩個參數矩陣 Waa 跟 Wax

220
00:15:40,005 --> 00:15:44,105
我們可以將它們壓縮成為一個參數矩陣 Wa

221
00:15:44,105 --> 00:15:48,850
而這可以讓我們簡化記號，讓我們建立更複雜的模型

222
00:15:48,850 --> 00:15:51,645
然後，同樣的方式

223
00:15:51,645 --> 00:15:54,125
我將稍微重新寫一下

224
00:15:54,125 --> 00:16:00,210
將寫為 Wy a<t> 加 by

225
00:16:00,210 --> 00:16:06,200
我們現在也簡化了兩個下標為 Wy 跟 by

226
00:16:06,200 --> 00:16:09,140
它標記為我們要計算的輸出量

227
00:16:09,140 --> 00:16:13,345
所以 Wy 標示了一個權重矩陣，跟計算 y 相關

228
00:16:13,345 --> 00:16:17,820
而上面這裡的 Wa 跟 ba  標示了

229
00:16:17,820 --> 00:16:22,475
這個參數的計算跟 a 啟動輸出量相關

230
00:16:22,475 --> 00:16:26,680
所以這就是基本的遞迴神經網路

231
00:16:26,680 --> 00:16:31,210
接下來，我們來談談反向傳播
跟您如何讓這個 RNN 學習