1
00:00:00,000 --> 00:00:03,420
RNN'lerin nasıl çalıştıklarını öğrendiniz ve nasıl

2
00:00:03,420 --> 00:00:06,840
isim varlık tanıma gibi problemlere uygulanabileceklerini öğrendiniz,

3
00:00:06,840 --> 00:00:09,537
hem de dil modellemesi gibi,

4
00:00:09,537 --> 00:00:12,330
Ve RNN'de eğitmek için geri yayılımın nasıl kullanılabileceğini gördünüz.

5
00:00:12,330 --> 00:00:15,400
Temel bir RNN algoritması ile ilgili problemlerden biri

6
00:00:15,400 --> 00:00:20,660
kaybolan eğim problemleri karşılaşılmasıdır.

7
00:00:20,660 --> 00:00:23,245
Sonraki birkaç videolarda hadi bunu tartışalım,

8
00:00:23,245 --> 00:00:27,102
Bu sorunları çözmenize yardımcı olacak bazı problemler hakkında konuşacağız.

9
00:00:27,102 --> 00:00:30,710
Buna benzer RNN'nın fotoğraflarını gördünüz.

10
00:00:30,710 --> 00:00:34,585
Ve bir dil modelleme örneğini ele alalım.

11
00:00:34,585 --> 00:00:36,975
Diyelim ki bu cümleyi görüyorsunuz,

12
00:00:36,975 --> 00:00:45,400
"Yemek çoktan yemiş kedi .... ve belki de çoktan lezzetli olan birçok yemekleri yemiş nokta,

13
00:00:45,400 --> 00:00:50,033
nokta,nokta,nokta, doydu."

14
00:00:50,033 --> 00:00:52,280
Ve böylece, uygun olması için

15
00:00:52,280 --> 00:00:53,540
çünkü kedi(cat) tekildir,

16
00:00:53,540 --> 00:00:56,300
Kedi doydular yerine doydu olmalıdır.

17
00:00:56,300 --> 00:01:01,665
"Çoktan birçok lezzetli yemekler yiyen kediler,

18
00:01:01,665 --> 00:01:02,730
ve elmalar ve armutlar,

19
00:01:02,730 --> 00:01:06,930
ve benzeri, doydular"

20
00:01:06,930 --> 00:01:08,100
Yani tutarlı olması için

21
00:01:08,100 --> 00:01:14,381
Kedi doydu ya da kediler doydular.

22
00:01:14,381 --> 00:01:19,285
Ve bu dilin çok uzun süren şartlara sahip olabileceği örneğidir,

23
00:01:19,285 --> 00:01:22,020
Daha önce de çalıştığı yer olabilir

24
00:01:22,020 --> 00:01:26,435
cümlede daha sonra gelmesi gereken yerleri etkileyebilir.

25
00:01:26,435 --> 00:01:29,610
Ama şimdiye kadar gördüğümüz, ortaya çıkan temel RNN öyle değildir

26
00:01:29,610 --> 00:01:33,355
uzun süren şartları yakalamada çok iyi değidir.

27
00:01:33,355 --> 00:01:36,465
Nedenini açıklamak için, hatırlayabilirsiniz

28
00:01:36,465 --> 00:01:40,035
bizim önceden tartıştığımız çok derin yapay sinir ağları hatırlayabilirsiniz,

29
00:01:40,035 --> 00:01:43,590
Biz kaybolan eğimler problemi hakkında konuştuk.

30
00:01:43,590 --> 00:01:46,040
Demek ki bu çok, çok derin yapay sinir ağı olduğunu söyler,

31
00:01:46,040 --> 00:01:51,720
100 katman ya da hatta daha derin ileri yayılım uygulayabilirsiniz,

32
00:01:51,720 --> 00:01:54,311
Soldan sağa ve sonuç geri yayılım.

33
00:01:54,311 --> 00:01:57,315
Ve biz buna dedik, eğer bu çok derin yapay sinir ağ ise, 

34
00:01:57,315 --> 00:01:59,530
daha sonrasında eğim çıktı y tarafından

35
00:01:59,530 --> 00:02:02,460
geri yaymak daha zor zaman alır

36
00:02:02,460 --> 00:02:05,550
önceki katmanları ağırlıklarını etkiler,

37
00:02:05,550 --> 00:02:07,865
önceki katmalarda hesaplamaları etkilemek için.

38
00:02:07,865 --> 00:02:10,545
Ve RNN için benzer problem ile,

39
00:02:10,545 --> 00:02:13,560
Soldan sağa gelen ileri yayılım var,

40
00:02:13,560 --> 00:02:14,985
ve sonra geri yayılım,

41
00:02:14,985 --> 00:02:18,160
sağdan sola gider.

42
00:02:18,160 --> 00:02:20,560
Ve bu gerçekten zor olabilir,

43
00:02:20,560 --> 00:02:23,020
çünkü kaybolan eğilim problemi yüzünden,

44
00:02:23,020 --> 00:02:26,500
hataların çıktısı için bağlantılı

45
00:02:26,500 --> 00:02:32,685
daha sonraki zaman adımları önceki hesaplamalarımızı etkiler.

46
00:02:32,685 --> 00:02:34,675
Ve pratikte, bunun anlamı,

47
00:02:34,675 --> 00:02:38,140
bir sinir ağı elde etmek zor olabilir

48
00:02:38,140 --> 00:02:41,759
ezberlemesi gerektiğinin farkına varması için tekil isim ya da çoğul isim görmesi,

49
00:02:41,759 --> 00:02:45,715
böylece daha sonra dizide hem doydu hem doydular,

50
00:02:45,715 --> 00:02:49,435
tekil veya çoğul durumuna bağlıdır.

51
00:02:49,435 --> 00:02:50,975
Ve İngilizcede farket,

52
00:02:50,975 --> 00:02:53,910
Ortadaki cümle uzun olabilirdi değil mi?

53
00:02:53,910 --> 00:02:58,030
Bu yüzden tekil/çoğul ezberlemesi gerekebilir

54
00:02:58,030 --> 00:03:04,227
bilgiyi kullanmadan uzun süre önce.

55
00:03:04,227 --> 00:03:06,185
Bu problem yüzünden,

56
00:03:06,185 --> 00:03:10,240
Temel RNN modeli birçok yerel etkeni vardır,

57
00:03:10,240 --> 00:03:19,295
Bunun anlamı y^<3 çıktısı. y^<3>. değerine yakın değerler tarafından etkilenir

58
00:03:19,295 --> 00:03:23,926
Ve değer esas olarak buradaki yakın bir yerde olan girdilerden etkilenir.

59
00:03:23,926 --> 00:03:27,160
Ve buradaki çıktı için

60
00:03:27,160 --> 00:03:30,950
dizinin önceki bir girişinden etkinlenmesi zordur.

61
00:03:30,950 --> 00:03:33,415
Bunun sebebi çıktı ne olursa olsun,

62
00:03:33,415 --> 00:03:35,180
Bunun doğru olup olmadığı,

63
00:03:35,180 --> 00:03:37,600
Bu o alan için çok zordur

64
00:03:37,600 --> 00:03:40,475
dizinin başlangıcından beri geri yayılım,

65
00:03:40,475 --> 00:03:43,620
ve bu sebeple sinir ağını nasıl değiştirmek için

66
00:03:43,620 --> 00:03:46,745
önceki dizide hesaplamalar yapılıyor.

67
00:03:46,745 --> 00:03:50,590
Yani bu temel RNN algoritmasının zayıflığıdır.

68
00:03:50,590 --> 00:03:54,385
Bir sonraki birkaç videoda ele alınmadı.

69
00:03:54,385 --> 00:03:57,025
Ama biz bunu ele almazsak sonrasında RNN

70
00:03:57,025 --> 00:04:01,920
uzun vadeli bağımlılıkları yakalamada konusunda çok iyi olmama eğilimindedir.

71
00:04:01,920 --> 00:04:06,165
Ve bu tartışma kaybolan eğilime odaklansa bile,

72
00:04:06,165 --> 00:04:09,160
Hatırlayacaksınız, Biz derin yapay sinir ağı hakkında konuştuk

73
00:04:09,160 --> 00:04:11,935
Biz ayrıca patlayan eğimler hakkında konuştuk.

74
00:04:11,935 --> 00:04:13,175
Biz geri yayılım yapıyoruz,

75
00:04:13,175 --> 00:04:15,795
Eğim sadece üssel olarak azalmamalıdır,

76
00:04:15,795 --> 00:04:19,432
Üzerinde durduğumuz katman sayısı ile üssel olarak artabilir. 

77
00:04:19,432 --> 00:04:24,213
Kaybolan eğimler daha büyük problem olmaya meğilli RNN'lerin eğitimlerinde ortaya çıktı.

78
00:04:24,213 --> 00:04:26,920
patlayan eğimler olmasına rağmen,

79
00:04:26,920 --> 00:04:28,630
felaket olabilir çünkü

80
00:04:28,630 --> 00:04:31,870
üstel olarak büyük eğimler değişkenlere neden olabilir

81
00:04:31,870 --> 00:04:37,252
değişkenler büyük olması için sizin yapay ağınızdaki parametreler kötü olabilir.

82
00:04:37,252 --> 00:04:40,780
Bu yüzden patlayan eğim fark etmenin daha kolay olduğu anlaşılıyor çünkü

83
00:04:40,780 --> 00:04:44,350
parametreler sadece hızlı bir şekilde ortaya çıktığı ve sık sık Nan'ler görüldüğüdür,

84
00:04:44,350 --> 00:04:45,950
ya da sadece numaralar değildir,

85
00:04:45,950 --> 00:04:52,018
senin yapay ağında hesaplamanının sayısal taşma sonuçlarının anlamıdır.

86
00:04:52,018 --> 00:04:53,995
Ve eğer patlayan eğimleri görüyorsanız,

87
00:04:53,995 --> 00:04:58,015
bir çözüm gradient kırpması uygulamaktır.

88
00:04:58,015 --> 00:04:59,440
Ve bunun anlamı şudur,

89
00:04:59,440 --> 00:05:02,400
bütün bunların anlamı eğim vektörlerine bakmaktır.

90
00:05:02,400 --> 00:05:07,025
ve eğer doyma bölgesinden daha yüksek ise,

91
00:05:07,025 --> 00:05:10,240
tekrar eğim vektörünü ölçekle böylece çok büyük olmaz.

92
00:05:10,240 --> 00:05:13,675
Bu yüzden maksimum değerlere göre kırpmalar vardır.

93
00:05:13,675 --> 00:05:16,335
Bu yüzden eğer patlayan eğim görüyorsanız,

94
00:05:16,335 --> 00:05:18,758
Eğer türevleriniz patlama yaparsa ya da Nan'ler görüyorsanız,

95
00:05:18,758 --> 00:05:21,355
sadece eğim kırpması uygula,

96
00:05:21,355 --> 00:05:26,910
ve bu patlayan eğimlere dikkat edecek, oldukça sağlam bir çözümdür.

97
00:05:26,910 --> 00:05:30,365
Ama kaybolan eğimleri çözmek çok daha zordur

98
00:05:30,365 --> 00:05:34,227
ve bu gelecek haftalardaki videoların konusu olacaktır.

99
00:05:34,227 --> 00:05:36,730
Özetlemek gerekirse, daha önceki bir kursta,

100
00:05:36,730 --> 00:05:39,470
Çok derin sinir ağlarının eğitiminin nasıl yapıldığını gördünüz,

101
00:05:39,470 --> 00:05:43,950
Siz kaybolan eğim ya da patlayan eğim problemleri türevi ile birlikte karşılaşabilirsiniz,

102
00:05:43,950 --> 00:05:46,480
ya katlanarak azalır ya da büyür

103
00:05:46,480 --> 00:05:50,070
üstel olarak katman sayısının bir fonksiyonu olarak.

104
00:05:50,070 --> 00:05:54,708
Ve RNN'de, bin defadan fazla RNN veri işleme kümesinde,

105
00:05:54,708 --> 00:05:56,038
10.000'den fazla küme,

106
00:05:56,038 --> 00:06:00,490
bu temel olarak 1.000 katman ya da 10.000 katmanlı sinir ağına gidebilir,

107
00:06:00,490 --> 00:06:04,075
ve bu yüzden, bu tip problemler ile karışılaşılır.

108
00:06:04,075 --> 00:06:08,875
Patlayan eğimler, sadece eğim kırpması kullanarak adres sıralayabilirsiniz,

109
00:06:08,875 --> 00:06:12,185
ama kaybolan eğimleri ele almak için daha fazla çalışma yapacaktır.

110
00:06:12,185 --> 00:06:14,650
Bir sonraki videoda yapacağımız şey GRU hakkında konuşmak,

111
00:06:14,650 --> 00:06:16,315
daha büyük tekrarlayan birimler,

112
00:06:16,315 --> 00:06:19,282
adresleme için çok etkili bir çözümdür

113
00:06:19,282 --> 00:06:21,690
kaybolan eğim problemi ve buna izin verecek

114
00:06:21,690 --> 00:06:25,805
Sinir ağınız, daha uzun süren bağımlılıkları yakamak için.

115
00:06:25,805 --> 00:06:28,000
Öyleyse bir sonraki videoya geçelim.