1
00:00:00,000 --> 00:00:01,980
前回のビデオでは

2
00:00:01,980 --> 00:00:05,047
シーケンスモデルを適用できる様々な応用例から
幾つかをご紹介しました

3
00:00:05,047 --> 00:00:10,470
これらシーケンスモデルを構築していく上で使用する
表記法についてまずは定義していきます

4
00:00:10,470 --> 00:00:12,360
取っ掛かりの例として

5
00:00:12,360 --> 00:00:16,290
このような文を入力とする
シーケンスモデルを構築したいとしましょう

6
00:00:16,290 --> 00:00:19,361
「ハリー・ポッターとハーマイオニー・グレンジャーは
新しい呪文を発明した」

7
00:00:19,361 --> 00:00:21,210
ちなみにこれらは J・K・ローリングの

8
00:00:21,210 --> 00:00:25,595
ハリー・ポッターシリーズに出てくるキャラクターです

9
00:00:25,595 --> 00:00:29,220
この文のどの部分が人の名前であるか
自動的に判別してくれるような

10
00:00:29,220 --> 00:00:34,120
シーケンスモデルが欲しいとしましょう

11
00:00:34,120 --> 00:00:36,760
これは固有表現抽出と呼ばれる問題で

12
00:00:36,760 --> 00:00:40,125
サーチエンジンなどにも用いられています

13
00:00:40,125 --> 00:00:43,800
例えば過去24時間のニュースに
登場したすべての人を見出し化して

14
00:00:43,800 --> 00:00:49,245
記事を適切に検索できるようにする
といったようなことです

15
00:00:49,245 --> 00:00:52,050
固有表現抽出は

16
00:00:52,050 --> 00:00:54,765
人名 会社名

17
00:00:54,765 --> 00:00:57,210
時間 場所 国名

18
00:00:57,210 --> 00:01:01,370
通貨 その他
様々なタイプの文字列抽出に用いられます

19
00:01:01,370 --> 00:01:04,650
さてこの x を入力したとき

20
00:01:04,650 --> 00:01:08,250
出力 y として
1単語につき値を1つ出力するような

21
00:01:08,250 --> 00:01:11,355
モデルを考えます

22
00:01:11,355 --> 00:01:17,130
ターゲットの出力 y は
対応する入力語が人名かどうかを表します

23
00:01:17,130 --> 00:01:21,030
細かいことを言うと
この表現はベストではありません

24
00:01:21,030 --> 00:01:23,520
もう少しまともな出力表現として

25
00:01:23,520 --> 00:01:26,558
単純に人名かどうかの情報だけでなく

26
00:01:26,558 --> 00:01:30,775
人名がどこで始まってどこで終わっているかを
表現したものもあります

27
00:01:30,775 --> 00:01:33,297
"Harry Potter" はここからここまで

28
00:01:33,297 --> 00:01:35,550
次の名前はここからここまで
といったような具合です

29
00:01:35,550 --> 00:01:37,990
ただここでの取っ掛かりの例として

30
00:01:37,990 --> 00:01:41,900
よりシンプルな出力表現で進めでいきましょう

31
00:01:41,900 --> 00:01:44,790
さて 入力は9個の単語列です

32
00:01:44,790 --> 00:01:51,345
最終的にはこれらの単語に対応した
9個の特徴量を使うことになります

33
00:01:51,345 --> 00:01:53,858
シーケンス内におけるインデックスを表現するのに

34
00:01:53,858 --> 00:01:59,797
x に上付きの山括弧〈〉を使い
x〈1〉 x〈2〉 x〈3〉のように表します

35
00:01:59,797 --> 00:02:06,955
この例では x〈9〉までで全ての位置が指定できます

36
00:02:06,955 --> 00:02:13,180
x〈t〉つまりインデックス t によって

37
00:02:13,180 --> 00:02:15,270
シーケンスの中のインデックスを表現します

38
00:02:15,270 --> 00:02:17,400
t は temporal から来ています
（temporal: 時間的な）

39
00:02:17,400 --> 00:02:21,735
シーケンスが時間的なものであろうがなかろうが

40
00:02:21,735 --> 00:02:27,805
シーケンスの中の位置を指定する
インデックスとして t を使います

41
00:02:27,805 --> 00:02:29,895
出力についても同様に

42
00:02:29,895 --> 00:02:34,040
これらの値をそれぞれ示すのに

43
00:02:34,040 --> 00:02:39,725
y〈1〉 y〈2〉 y〈3〉・・・・y〈9〉 を使います

44
00:02:39,725 --> 00:02:44,553
また入力長を表すのに
Tx (T下付きx) を使いましょう

45
00:02:44,553 --> 00:02:46,540
この例だと9単語ありますので

46
00:02:46,540 --> 00:02:53,025
Tx = 9 となります
同様に出力長を表すのにTyを使います

47
00:02:53,025 --> 00:02:56,130
この例では Tx = Ty ですが

48
00:02:56,130 --> 00:02:59,410
前回のビデオで見たとおり
Tx と Ty は必ずしも同じではありません

49
00:02:59,410 --> 00:03:03,690
これまでに使ってきた表記法として

50
00:03:03,690 --> 00:03:09,080
i 番目のトレーニングデータを表す
x(i) を覚えているでしょうか

51
00:03:09,080 --> 00:03:10,710
この場合

52
00:03:10,710 --> 00:03:17,160
i 番目のトレーニングデータシーケンス内の
t 番目の要素を表すのに

53
00:03:17,160 --> 00:03:21,870
この表記 x(i)〈t〉を使います

54
00:03:21,870 --> 00:03:26,700
またトレーニングセットのデータによって
入力長は異なります

55
00:03:26,700 --> 00:03:33,368
つまり Tx(i) は i 番目のトレーニングデータの
入力長を表すことになります

56
00:03:33,368 --> 00:03:40,930
同様に y(i)〈t〉は i 番目のトレーニングデータの
出力シーケンス内の t 番目の要素を表し

57
00:03:40,930 --> 00:03:49,842
Ty(i) は i 番目のトレーニングデータの
出力シーケンスの長さを表すことになります

58
00:03:49,842 --> 00:03:51,570
なのでこの例では

59
00:03:51,570 --> 00:03:57,015
Tx(i) = 9 ですが
違うトレーニングデータとして

60
00:03:57,015 --> 00:04:03,370
15単語の文が入力されたとすると
そのデータに対する Tx(i) は 15 となるわけです

61
00:04:03,370 --> 00:04:09,645
さてNLP（自然言語処理）を始めていきましょう

62
00:04:09,645 --> 00:04:16,410
NLP（自然言語処理）に対する本格的な取り組みとしては
これが最初にあたります

63
00:04:16,410 --> 00:04:18,915
決めなければならないこととして

64
00:04:18,915 --> 00:04:22,470
シーケンス内の各単語をどう表現するか
ということがあります

65
00:04:22,470 --> 00:04:25,195
例えば"Harry"という語をどう表現するか

66
00:04:25,195 --> 00:04:30,330
つまり x〈1〉がどのようになっていればいいのか

67
00:04:30,330 --> 00:04:35,580
ここからは シーケンス内の各単語をどう表現するか
について話していきましょう

68
00:04:35,580 --> 00:04:38,530
シーケンス内の単語を表現するにあたり
まず思いつくのは

69
00:04:38,530 --> 00:04:42,075
ボキャブラリーを用意することでしょうか

70
00:04:42,075 --> 00:04:46,090
辞書と呼ばれることもありますが つまりは

71
00:04:46,090 --> 00:04:50,740
表現に使われる単語のリストを
作ることを意味します

72
00:04:50,740 --> 00:04:53,160
ボキャブラリー内の最初の単語は "a" です

73
00:04:53,160 --> 00:04:55,270
辞書内の最初の単語です

74
00:04:55,270 --> 00:05:00,805
第2語目は "aaron"
それからもう少し下の方に "and"

75
00:05:00,805 --> 00:05:08,035
どこかに "harry" や "potter" が入っているでしょう

76
00:05:08,035 --> 00:05:16,895
そして一番下まで来て
辞書の最後の単語は "zulu" とかでしょうか

77
00:05:16,895 --> 00:05:19,415
"a" は1番目の単語

78
00:05:19,415 --> 00:05:21,302
"aaron" 2番目の単語

79
00:05:21,302 --> 00:05:29,385
この辞書では "and" 367番目

80
00:05:29,385 --> 00:05:34,925
"harry" は4075番目

81
00:05:34,925 --> 00:05:37,775
"potter" は6830番目

82
00:05:37,775 --> 00:05:44,690
最終語のzuluは
例えば10,000番目に現れることになります

83
00:05:44,690 --> 00:05:46,295
この例において

84
00:05:46,295 --> 00:05:51,380
1万語サイズの辞書が使われている事になりますが

85
00:05:51,380 --> 00:05:55,880
これは近年のNLPのアプリケーションからすると
かなり小さいと言えます

86
00:05:55,880 --> 00:06:00,460
商用のアプリケーションで言えば

87
00:06:00,460 --> 00:06:07,485
3〜5万語サイズの辞書が一般的ですし
10万語のものも珍しくありません

88
00:06:07,485 --> 00:06:09,920
大規模なインターネット企業などは

89
00:06:09,920 --> 00:06:14,310
100万語を上回るサイズの辞書を
使っているところもあります

90
00:06:14,310 --> 00:06:17,695
ただ 商用アプリケーションとしてよく見る辞書のサイズは

91
00:06:17,695 --> 00:06:21,660
おそらく3〜5万語といったところでしょう

92
00:06:21,660 --> 00:06:27,790
説明用としては1万語で十分ですので
この値で進めましょう

93
00:06:27,790 --> 00:06:33,520
さて1万語の辞書を選びました
この辞書を構築するには

94
00:06:33,520 --> 00:06:35,630
トレーニングセットをすべて調べ

95
00:06:35,630 --> 00:06:40,115
登場回数上位1万語を使うという方法があります

96
00:06:40,115 --> 00:06:43,940
またオンライン辞書をいくつか調べれば

97
00:06:43,940 --> 00:06:48,099
英語で使われる一般的な1万語がわかります

98
00:06:48,099 --> 00:06:54,320
そしてone-hot表現を使ってこれら各語を
表現することが出来るようになります

99
00:06:54,320 --> 00:07:02,960
例えば "harry" を表している x〈1〉は
次のようなベクトルになります

100
00:07:02,960 --> 00:07:12,080
"harry" の辞書内での位置つまり
4075番目の要素のみ 1 それ以外全て 0

101
00:07:12,080 --> 00:07:18,110
次に x〈2〉も同様に

102
00:07:18,110 --> 00:07:24,757
6830番目が 1 であることを除き
全て 0 のベクトル

103
00:07:24,757 --> 00:07:30,620
"and" は367番目の単語として表現されていましたので

104
00:07:30,620 --> 00:07:38,000
x〈3〉は367番目が 1 であることを除き
全て 0 のベクトルになります

105
00:07:38,000 --> 00:07:40,835
これらは辞書サイズが1万語であれば

106
00:07:40,835 --> 00:07:45,830
それぞれ1万次元のベクトルとなります

107
00:07:45,830 --> 00:07:50,570
そして "a" は辞書の1番目の単語でしょうから

108
00:07:50,570 --> 00:07:54,258
その "a" に相当する x〈7〉は

109
00:07:54,258 --> 00:07:57,625
最初が 1 で

110
00:07:57,625 --> 00:08:03,060
残りが全て 0 というベクトルになります

111
00:08:03,060 --> 00:08:07,760
この表現では

112
00:08:07,760 --> 00:08:12,800
文の t 番目の要素の値として
x〈t〉がone-hotベクトルとして表されます

113
00:08:12,800 --> 00:08:16,760
one-hotというのはベクトルの唯一の要素が 1 で
それ以外は全て 0 というところから来ています

114
00:08:16,760 --> 00:08:22,075
この文では9個のone-hotベクトルを使って
9単語を表しています

115
00:08:22,075 --> 00:08:25,995
ゴールは この x に対するこの表現が与えられた時

116
00:08:25,995 --> 00:08:31,325
シーケンスモデルを使って
ターゲット y へのマッピングを学習することです

117
00:08:31,325 --> 00:08:33,875
これは教師あり学習の問題として扱います

118
00:08:33,875 --> 00:08:37,460
つまり x y 両方が揃ったラベル付きデータが必要です

119
00:08:37,460 --> 00:08:39,137
もう一つだけ細かい話をします

120
00:08:39,137 --> 00:08:42,665
後のビデオで詳しく説明されますが

121
00:08:42,665 --> 00:08:46,545
もしボキャブラリーにない単語が出てきたら
どうすればいいでしょうか

122
00:08:46,545 --> 00:08:52,400
答えを言うと Unknown Word という
新しいトークンというか偽単語をつくることになります

123
00:08:52,400 --> 00:08:58,391
表記としては〈UNK〉のように表し
ボキャブラリーにない単語を意味します

124
00:08:58,391 --> 00:09:00,985
後ほど詳しく説明します

125
00:09:00,985 --> 00:09:02,660
まとめると

126
00:09:02,660 --> 00:09:05,570
トレーニングセットの x y を
シーケンスデータとして扱う場合の

127
00:09:05,570 --> 00:09:09,205
表記方法について説明しました

128
00:09:09,205 --> 00:09:11,390
次のビデオでは
x から y へのマッピング方法として

129
00:09:11,390 --> 00:09:15,420
再帰型ニューラルネットワークについて
説明をしていきます