Bu zamana kadar gördüğünüz farklı RNN versiyonları kendi başlarına oldukça iyi çalışıyorlar. Fakat çok karmaşık fonksiyonları öğrenmek için bazen RNN'lerin birden çok katmanını birleştirip bu modellerin daha derin versiyonlarını yapmamız gerekebilir. Bu videoda bu derin RNN'leri nasıl yapacağınızı öğreneceksiniz. Haydi başlayalım. Hatırlarsanız standart bir Sinirsal Ağ için x girdisi vardı, bir gizli katmanı besliyordu, bu katmanın aktivasyonları vardı, ilk gizli katman için bu aktivasyona a_1 diyelim. Bu sonraki gizli katmana bağlıydı. Aktivasyonları a_2. Sonra belki bir katman daha. a_3 aktivasyonları. Ve sonunda çıktı tahmini y^. Derin RNN de buna benziyor. Elle çizdiğim bu ağın zaman boyunca açılmışı gibi. Bir bakalım. Burada daha önce gördüğünüz standart RNN var. Fakat gösterimi biraz değiştirdim. Sıfır zamanındaki aktivasyonu a_0 olarak yazmak yerine şu köşeli parantez içinde 1'i ekledim. Bunun birinci katman için olduğunu simgeliyor. Yani kullanacağımız gösterim, l'inci katmanın aktivasyonu için a[l] ve zamanı göstermek için <t> <t> Yani bunun aktivasyonu birinci katman zaman 1, bunlar da ilk katman zaman 2,3 ve 4. Sonra bunları üst üste ekliyoruz. Bu 3 gizli katmanlı bir sinirsel ağ oldu. Şuradaki değer nasıl hesaplanıyor bir bakalım. a[2]<3>'nin iki girdisi var. Bir alttan gelen girdisi var, bir tane de soldan gelen. Aktivasyon fonksiyonu g içinde, bir ağırlık matrisi olacak, bu matrise Wa diyeceğiz çünkü a, yani aktivasyon değerini hesaplıyor. Ve ikinci katman için olacak. Bu matrise şunları vereceğim: a[2]<2> virgül a[1]<3> artı ikinci katman için olan ba. Akitvasyon değerini bu şekilde hesaplıyorsunuz. Yani bu katmandaki tüm hesaplamalarda aynı Wa[2] ve ba[2] parametreleri kullanılıyor. İlk katmanın ise kendi parametreleri olacak: Wa[1] ve ba[1]. Soldaki gibi standart sinirsel ağlar bildiğiniz gibi çok derin olabiliyor. Belki 100'den fazla katmanlı olabiliyor. RNN'ler için ise 3 katman bile çok fazla. Çünkü zaman boyutu için halihazırda bu ağlar çok büyük olabiliyor. Az katman olsa bile. Genelde 100 katman gibi çok katman kullanıldığını görmezsiniz. Bazen görebileceğiniz bir şey ise, özyineli katmanların üst üste eklenmesi. Fakat o zaman çıktıyı buradan alabilirsiniz, şundan kurtulalım. Yatay olarak bağlı olmayan derin katmanlar olur, ve sonunda y<1>'i tahmin eden derin bir ağ oluştururlar. y<2>'i tahmin eden bir derin ağ da burada olabilir. Bu gördüğünüz ağ mimarisinde zaman boyunca bağlı 3 özyineli birim, sonra bir ağ, sonra tekrar bir ağ, y<3> ve y<4> için gördüğümüz gibi. Derin bir ağ var ama yatay bağlantıları yok. Daha sık görmeye başladığımız bir mimari bu. Bu bloklar standart RNN, yani basit RNN modeli olmak zorunda değil. GRU veya LSTM blokları da olabilirler. Son olarak Çift Yönlü RNN'lerin de derin versiyonlarını yapabilirsiniz. Derin RNN'lerin eğitilmesi hesaplama açısından pahalı olduğu için, çünkü halihazırda zaman ekseninde çok genişler, çok derin özyineli katmanlar görmezsiniz. Burada zaman ekseninde bağlı 3 derin katman var. Normal derin ağlarda gördüğünüz kadar çok sayıda katman göremezsiniz. Derin RNN'ler için bu kadar. Bu hafta temel RNN'lerden temel özyineli birimlerden, GRU'lara, LSTM'lere ve Çift Yönlü RNN'lere, az önce gördüğünüz gibi bunların derin versiyonlarına kadar öğrendiklerinizle elinizde zengin bir alet çantası oluştu. Artık dizi modellerinde çok güçlü modeller oluşturabilirsiniz. Umarım bu haftanın videolarını beğenmişsinizdir. Programlama egzersizleri için bol şans. Haftaya tekrar görüşmek üzere.