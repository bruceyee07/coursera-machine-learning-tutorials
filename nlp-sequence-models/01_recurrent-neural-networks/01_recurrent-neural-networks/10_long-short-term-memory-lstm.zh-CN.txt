在上一个视频中, 你了解了 GRU, 门控循环单元， 以及GRU如何使你能在序列中
学习较长的(序列)链接。 另外一种允许你很好的处理（长距离序列链接）的 是LSTM，或者长短期记忆单元 这个方法比GRU更强大
 让我们来看一下 这里是前一个视频里面看到的关于GRU的等式 GRU中 我们有a_t等于c_t 和两个门： 更新门和相关性门，c_tilde_t 是替代记忆单元的候选（数值） 然后我们使用更新门gamma_u 来决定是否要使用c_tilde_t来修正c_t LSTM比GRU强大一些，并且更为泛化 （LSTM算法）归功于Sepp Hochreiter和Jurgen Schimdhuber （的论文） 这篇论文是一篇真正的开创性的论文 对序列模型产生了巨大的影响 我认为这篇论文属于众多艰涩难懂
 的论文中的一篇 这篇论文用很大篇幅来论述梯度消失理论 因此，我觉得更多的人是通过别的渠道来学习 LSTM的细节而不是通过这篇特定的文章 这篇论文对深度学习领域产生了深厚的影响 然这里的等式便是LSTM的核心 我们继续看记忆单元c 以及用作其修正值的候选数层c_tilde_t 是这个 注意在LSTM中 不会再出现a_t等于c_t的情况出现 因此这个是我们使用的 这个只是左边的等式，除了现在 这里特定使用a_t或者a_(t-1)
 而非c_(t-1) 以及我们不再使用gamma或者这个相关性门 但是有LSTM的变体会把这些因子重新包括进去 但是通常形式的LSTM 不会那么做 接着我们和前面一样使用更新门 更新W并且使用a_(t-1)
 x_t加b_u LSTM的一个新特性是 不再使用一个单一的更新门控 包括这两个项 我们将采用两个不同的项 不使用gamma_u和1-gamma_u， 我们这里会使用gamma_u 并将遗忘门控称作gamma_f 因此这个门控，gamma_f 是一个关于x_t （a_(t-1)），加上b_u的sigmoid函数 就像你预料的一样 我们将会得到一个新的输出门控
关于W_o的sigma函数 然后就像你们预期一样，加上b_o 记忆单元的更新值将是c_t等于gamma_u 这个星号表示元素积/点积操作 这个是向量-向量点积操作 而且，用于代替1-gamma_u 我们将另外得到一个独立的遗忘门控，gamma_f 乘以c_t减去1 这为记忆单元提供了一个新的选项来 保留老的值c_t减去1
 并且加上（前面的项） 这个新值c_tilde_t 使用不同的更新和遗忘门 这个表示更新，遗忘和输出门 最后，a_t不再等于c_t a_t等于输出门和c_t的点积 这些便是LSTM的核心等式 你可以发现它们具有三个控制门
 而不是两个 这个（LSTM）相对复杂一点，
 它将门控放置在略为不同的地方 这里就是控制LSTM行为的核心等式 使用图来解释事物是一种常用的方式 因此让我在这里画个示意图 如果图形太复杂，不用担心 我个人认为等式比图画更为容易理解 但是我将通过图形来传递其直观的含义 这个大图的灵感来源于Chris Ola的一篇博客文章 文章的题目是理解LSTM网络
 这里画的图形 和他在博客文章里画的很相似 从这图里面能够学习到的重要信息是 你使用a_t-1和x_t来计算所有门控的数值 在图中，你有a_t-1 和x_t被一起用来计算遗忘门 更新门以及 输出门的数值 他们同样使用一个tanh函数得到c_tilde_t 这些数据通过 点积计算等复杂的方式被揉合在一起 并从c_t-1中计算得到c_t 有个很有趣的事情是，你可以生成
一堆类似的单元 这里是其中一个，你把他们连接起来 你可以根据时序将他们连接起来 它先处理x_1 然后处理x_2 x_3 你可以把这些单元更具以下方式结合起来 前一个时间点的（单元）输出是
下一个时间点的（单元）输入 c也是同样的计算。我在下图中将其简化。 你可能意识到一个很酷的事实是 图中上部有一条流程直线表示如何计算 只要你恰当的设置了遗忘和更新门 LSTM可以相对简单的将 c_0值传递到图的右侧 例如使得c_3等于c_0 这就是为什么LSTM 以及GRU 可以长时间很好的记住某些数值 对于存储在记忆单元的某些实数值
 （记忆）可以维持很多步骤 这就是关于LSTM的内容 你可以想象 人们同时使用几种（LSTM）变体 最普遍常用的（变体）可能是 门控的值不仅仅取决于a_t-1, x_t 有时候人们会把c_t-1的值也用上 这个叫做窥孔连接 不是一个很好的名称，但是你可以看到 窥孔连接的意思是，
 门控数值不仅仅取决于a_t-1, x_t 同时也和前面记忆单元的值相关 窥孔连接可以用作所有
这三个门控的计算中去 这是一个常见的LSTM变体 一个技术细节是，例如这个是一个100维的向量 因此你有一个100维的隐藏记忆单元 例如，第五个元素 的c_t-1只会影响相关门控的第五个元素 因此，他们的关系是一对一的 不是每一个关于100维c_t-1的元素 能够影响到门控单元的元素 因此，c_t-1的第一个元素影响
 门控的第一个元素 第二个元素影响第二个元素，以此类推 当你在科技论文中读到窥孔连接的时候 他们表达的意思是，c_t-1会被用来影响门控数值 这就是LSTM 什么时候你应该使用GRU？ 什么时候你应该使用LSTM？ 现在没有一个普适一致的观点S 虽然我首先介绍了GRU 在深度学习的历史中 LSTM要远远早于GRU GRU是一个相对近期的发明 用来作为复杂的LSTM模型的简化 研究者在多种不同的问题上使用这两种方法 在不同的问题上 不同的算法各有千秋 所以，不存在一个普适的优秀算法 这就是我为什么要把这两种方法
都介绍给大家的原因 当我使用这些方法的时候 GRU的有点是其模型的简单性 因此更适用于构建较大的网络 它只有两个门控 从计算角度看，它的效率更高 它的可扩展性有利于
构筑较大的模型 但是LSTM更加的强大和灵活，因为它具有三个门控 如果你要从两中方法中选取一个 我认为LSTM是经过历史检验的方法 因此，如果你要选取一个 我认为大多数人会把LSTM作为
默认第一个去尝试的方法 虽然，在过去几年 GRU的势头越来越猛，
我感觉越来越多的团队 同时也用GRU，因为其简单而且效果可以（和LSTM）比拟 可以更容易的将其扩展到更大的问题 LSTM就介绍到这里 不管用是GRU还是LSTM 你可以用他们来构筑一个
可以获取更长范围相关性的神经网络