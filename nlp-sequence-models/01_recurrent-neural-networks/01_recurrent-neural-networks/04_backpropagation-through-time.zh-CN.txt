你已经学习了一些RNN的基础知识 在这个视频中 我将介绍
反向传播在循环神经网络是如何工作的。 像往常一样,<br /> 当您在一个编程框架中实现此方法时, 通常，编程框架将自动处理反向。 但我认为, 对 backprop 在 RNNs 的工作方式有一个粗略的了解还是有用的。 让我们一起来看看让我们一起来看看 你已经看到了反向传播如何工作, 从左向右计算这些激活值, 如神经网络中所示, 之后将会输出所有的预测值。 在反向传播中, 你可能已经猜到了, 你最后保留 反向传播的值，在相反的方向 如图中向前的箭头。 让我们看一下前向传播计算的过程。 得到这个输入序列 x_1, x_2, x_3, 到 x_tx。 然后 x_1 激活值表示为 a_0 计算激励值，和这个值相乘， 然后, x_2 和 a_1 一起用来计算 a_2, 然后 a_3, 直到 a_tx。 好吧。实际上计算的是 a_1, 您还需要参数。 我们把这个画成绿色的 W_a 和 b_a 这些是用于计算参数 a_1 的。 然后, 这些参数实际上用于每一步计算 这些参数实际上用于计算 a_2、a_3、 等等, 直到最后一个，所有激活值的计算<br />都取决于参数 W_a 和 b_a。 让我们继续充实这个图表。 现在, 给定 a_1, 您的神经网络可以<br />计算第一个预测值, y-hat_1, 然后计算第二步，<br />y-hat_2, y-hat_3, 就这样下去, 得到hat_ty。 再用一个不同颜色写一个参数。 所以, 为了计算 y_hat, 你需要参数， W_y 和 b_y, 这一次进入这个节点以及其他所有节点。 我也要用绿色画这个。 接下来, 为了计算反向, 你需要一个损失函数。 因此, 让我们定义一个逐元素的损失值。 假设序列中一个确定的个词。 它是一个人的名字, 所以 y_t 是1。 你的神经网络输出是一写概率值， 或许特定词是一个人的名字的概率是 0.1 。 把它定义为标准的逻辑回归损失, 也称为交叉熵损失。 这可能要和我们以前看过的很眼熟。 像是一个二分类问题。 所以这个损失相关的是， 单个位置或单个时间集的单个预测, 对于一个字。 现在让我们定义整个序列的整体损失, 所以我将 L 定义为, t 从 i 到 T_x 或 T_Y 的求和。 T_x 等于 T_y , 在这个例子中的损失, 对于单个时间戳, y_t。 然后, 值取 L 值，不需要它。 上标 T。这是整个序列的损失。 所以, 在一个计算图中, 为了计算给定 y-hat_1 的损失, 可以通过给定的第一个时间戳 来计算出第二个时间戳的损失值。 以及第三个时间戳的损失, 一直到最后一个时间戳的损失。 最后, 计算整体损失值, 我们将把这些所有的计算值求和，<br />最终的 L 使用该方程, L是每一步时间戳的损失总和。 所以, 这是一个计算问题， 从以前见过的反向传播的例子来看, 显而易见，反向传播要求 在相反的方向进行计算和传递信息。 所有的四个反向传播箭头, 最终这样来处理。 这么做可以计算所有适当的数量参数, 然后，就像铆钉枪一样逐个计算所有参数。 并使用梯度下降法来更新参数。 在这个反向传播的过程中, 最重要的信息传递和递归计算<br />是这一步, 从右向左计算, 这就是它提出反向传播算法的原因， 一个非常快速的计算方法，<br />全名叫“基于时间的反向传播算法”。 去这个名字的动机来源是“前馈传播”， 从左向右扫描, 增加的时间变量, t, 与此相反，<br />反向传播从右向左计算, 有点像是时间倒退的样子。 所以收这些的启发,<br /> 我想起了一个很酷的名字, “基于时间的反向传播算法”，沿时间轴反向传播。 这个命名听起来就像是<br />你需要一个时间机器来实现这个输出, 但我，我觉得“基于时间的反向传播” 只是算法中最酷的名字之一。 我希望这些能给你一个关于<br /> RNN 前馈传播和反向传播的认识。 到目前为止,<br /> 你只看到了 RNN 的一个主要的例子, 输入序列的长度等于输出序列的长度。 在下一视频中, 我想向您展示更广泛的 RNN 体系结构, 因此, 这将让您可以处理更广泛的应用集。<br />让我们继续下一堂课。