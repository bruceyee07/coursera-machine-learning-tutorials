1
00:00:00,450 --> 00:00:01,190
上个视频中

2
00:00:01,190 --> 00:00:05,820
你看到了
我们将用来定义序列学习问题的符号

3
00:00:05,820 --> 00:00:08,370
现在，我们来谈论一下怎么建立一个

4
00:00:08,370 --> 00:00:11,760
用于学习从 x 到 y 的映射的神经网络模型

5
00:00:11,760 --> 00:00:17,066
现在你可以尝试一下
用一个标准的神经网络来完成这个任务

6
00:00:17,066 --> 00:00:22,425
在我们之前的例子中，
我们有9个输入词，所以你可以

7
00:00:22,425 --> 00:00:27,697
尝试把这9个输入词，
或许是9个独热向量

8
00:00:27,697 --> 00:00:33,079
传入到一个标准的神经网络
或一些隐藏层中

9
00:00:33,079 --> 00:00:37,593
然后，他们将会输出9个包括0或1的值

10
00:00:37,593 --> 00:00:41,282
来告诉你，每个单词是人名的一部分。

11
00:00:41,282 --> 00:00:43,460
但事实证明，这样做效果不好，并且

12
00:00:43,460 --> 00:00:46,435
其中存在两个主要的问题

13
00:00:46,435 --> 00:00:49,315
第一个问题是，输入和输出

14
00:00:49,315 --> 00:00:52,595
在不同的例子中，长度可能会不同

15
00:00:52,595 --> 00:00:57,840
所以，并不是每个个例
都有相同的输入长度Tx或

16
00:00:57,840 --> 00:00:59,968
相同的输出长度Ty

17
00:00:59,968 --> 00:01:04,708
另外，如果每个句子都有最大长度，你可能要扩充或

18
00:01:04,708 --> 00:01:08,184
零扩充每个输入项，
使其达到最大长度，但是

19
00:01:08,184 --> 00:01:11,765
这仍然不是高质量的表示方法。

20
00:01:11,765 --> 00:01:14,025
另外，第二个问题可能更严重

21
00:01:14,025 --> 00:01:17,915
像这样的朴素神经网络结构

22
00:01:17,915 --> 00:01:22,185
不会将那些从不同文本位置学到的特征，
进行共享

23
00:01:22,185 --> 00:01:26,766
特别是，假如神经网络学到了“Harry”这个词

24
00:01:26,766 --> 00:01:31,997
如果它出现在了位置1，
就会有标识指出这是人名的一部分

25
00:01:31,997 --> 00:01:36,290
如果它能自动确定在某个位置 Xt 出现的 "Harry"

26
00:01:36,290 --> 00:01:41,579
同样表明这个词有可能是一个人的名字<br />那就再好不过了

27
00:01:41,579 --> 00:01:46,276
这也许和你在卷积神经网络课程
中看到的过程很相似

28
00:01:46,276 --> 00:01:48,398
在那门课程中，
模型从图片的某一部分学到的模式

29
00:01:48,398 --> 00:01:53,320
可以更快地应用到图片的其他部分

30
00:01:53,320 --> 00:01:57,840
我们希望在序列数据可以实现相似的效果

31
00:01:57,840 --> 00:02:02,497
如同你从卷积神经网络中看到的<br />使用更好的神经网络结构

32
00:02:02,497 --> 00:02:06,567
有利于减少模型中的参数数量

33
00:02:06,567 --> 00:02:12,065
在标准神经网络中这些 x 均是
长度为10,000的

34
00:02:12,065 --> 00:02:16,825
独热编码的向量<br />就是一个非常庞大的输入层

35
00:02:16,825 --> 00:02:22,418
假设整个输入语句的长度达到单词数量上限<br />再乘以10,000

36
00:02:22,418 --> 00:02:28,290
第一层的权值矩阵最终会包含大量参数

37
00:02:28,290 --> 00:02:33,150
因此我们下一张幻灯片中
将要描述的循环神经网络

38
00:02:33,150 --> 00:02:37,010
不存在上述的缺点

39
00:02:37,010 --> 00:02:40,346
那么，什么是循环神经网络？

40
00:02:40,346 --> 00:02:46,348
我们来创建一个循环神经网络，
如果你从左向右读一个句子

41
00:02:46,348 --> 00:02:50,738
你读的第一个词语我们将其表示为x1

42
00:02:50,738 --> 00:02:53,951
我们将第一个词语

43
00:02:53,951 --> 00:02:56,935
输入一个神经网络层

44
00:02:56,935 --> 00:03:02,645
画一条这样的线，因此形成第一个
神经网络的隐藏层

45
00:03:02,645 --> 00:03:07,358
我们可以用这个神经网络来预测输出

46
00:03:07,358 --> 00:03:10,210
这是否是一个人名的一部分

47
00:03:10,210 --> 00:03:14,607
这里循环神经网络所做的是

48
00:03:14,607 --> 00:03:20,392
当其继续读取句子中的第二个词语的时候

49
00:03:20,392 --> 00:03:25,946
比如说x2, （LSTM）不仅仅是
通过x2去预测y2

50
00:03:25,946 --> 00:03:33,957
它还会把计算机第一步的计算结果
作为其输入信息（的一部分）

51
00:03:33,957 --> 00:03:35,216
所以具体地来说

52
00:03:35,216 --> 00:03:40,675
第一步的激活数值会传递到第二步

53
00:03:40,675 --> 00:03:46,292
在下一个步骤中

54
00:03:46,292 --> 00:03:52,731
循环神经网络输入第三个词x3并预测

55
00:03:52,731 --> 00:03:57,804
输出y_hat_3等

56
00:04:00,920 --> 00:04:07,134
直到最后一次步骤, 它输入x<tx>

57
00:04:07,134 --> 00:04:13,346
然后输出y_hat_ty
在这个例子里面Tx=Ty

58
00:04:13,346 --> 00:04:18,731
架构会略有变化

59
00:04:18,731 --> 00:04:21,380
当Tx不同于Ty时

60
00:04:22,520 --> 00:04:27,029
在每一步，循环神经网络将

61
00:04:27,029 --> 00:04:30,550
它的激活数值传递到下一步，
供其使用

62
00:04:31,650 --> 00:04:34,090
要开始整个计算过程

63
00:04:34,090 --> 00:04:38,060
我们会在第0步的时候使用
一些人为制造的激活数值

64
00:04:38,060 --> 00:04:41,250
通常为一个全零的向量

65
00:04:41,250 --> 00:04:44,679
有的研究者会随机的为a0设置初始值

66
00:04:44,679 --> 00:04:47,382
你还可以通过别的方法来
为a0设置初始值

67
00:04:47,382 --> 00:04:52,230
但是使用全零的向量作为一个
第0步的虚激活数值

68
00:04:52,230 --> 00:04:56,710
是最常用的选择用来作为神经网络的输入

69
00:04:56,710 --> 00:05:00,800
在有些学术文章和书当中

70
00:05:00,800 --> 00:05:05,170
你会看到循环神经网络通过以下的图形来表示：

71
00:05:05,170 --> 00:05:09,840
每一步你输入一个x，输出一个ŷ

72
00:05:10,840 --> 00:05:14,720
有时会使用一个T指数

73
00:05:14,720 --> 00:05:19,730
有时会画这样一个循环
表示循环的链接

74
00:05:19,730 --> 00:05:25,040
有时候会用阴影箱线图来表示

75
00:05:25,040 --> 00:05:30,006
这里阴影箱线图表示时间上，一步的延迟

76
00:05:30,006 --> 00:05:34,790
我个人认为这些循环图形很难解释清楚

77
00:05:34,790 --> 00:05:39,590
因此，这个课程中我更趋向于使用类似
左侧图中那样展开的图形来表示

78
00:05:39,590 --> 00:05:42,840
但是，如果你在教科书

79
00:05:42,840 --> 00:05:46,360
或者研究论文中看到右图中那样的图形示意

80
00:05:46,360 --> 00:05:50,830
我会将它想象成展开成左侧示意图那样，
 以便正真理解其含义。

81
00:05:50,830 --> 00:05:55,620
循环神经网络从左向右扫描数据

82
00:05:55,620 --> 00:05:59,530
每一步所用的参数是共享的

83
00:05:59,530 --> 00:06:03,540
因此，这里有一组参数，我们会在

84
00:06:03,540 --> 00:06:04,770
下一张幻灯片中详细叙述

85
00:06:04,770 --> 00:06:09,050
控制从x1到隐藏层链接的一组参数

86
00:06:09,050 --> 00:06:13,050
我们用W_ax表示它们

87
00:06:13,050 --> 00:06:18,424
这一组W_ax参数同时
也会被用于每一个步骤

88
00:06:18,424 --> 00:06:21,850
我想我可以在那里写上W_ax

89
00:06:21,850 --> 00:06:26,910
激活函数，各层间水平的链接是由

90
00:06:26,910 --> 00:06:29,980
一组特定参数W_aa来控制

91
00:06:29,980 --> 00:06:35,220
同样的W_aa也将被用于每一个步骤中

92
00:06:35,220 --> 00:06:42,475
同样，W_ya控制输出预测

93
00:06:44,559 --> 00:06:47,820
我将在下一张幻灯片中详细叙述
这些参数是如何运作的

94
00:06:48,860 --> 00:06:53,355
这个循环神经网络的含义是

95
00:06:53,355 --> 00:06:58,913
在对y3进行预测时，它不仅从x3获取信息

96
00:06:58,913 --> 00:07:03,736
同时也考虑到x1和x2

97
00:07:03,736 --> 00:07:08,021
因为x1可以从这条路径
帮助预测y3

98
00:07:08,021 --> 00:07:13,148
这个RNN的缺点之一是，它只使用序列中

99
00:07:13,148 --> 00:07:18,616
先前的信息来做出预测（这里指x1,x2,x3），具体来说
当我们预测y3时

100
00:07:18,616 --> 00:07:24,019
它不会使用关于词语x4, x5, x6等等的信息

101
00:07:24,019 --> 00:07:28,430
当你看到一句话的时候
 这将会造成困扰

102
00:07:28,430 --> 00:07:32,417
他说“Teddy Roosevelt was a great President”
（泰迪罗斯福是一位伟大的总统）

103
00:07:32,417 --> 00:07:36,512
当需要判定Teddy是否是人名的一部分时

104
00:07:36,512 --> 00:07:40,932
如果不仅知道有关头两个词的信息

105
00:07:40,932 --> 00:07:44,920
还知道句中后面的词
 将有助于做出判断

106
00:07:44,920 --> 00:07:49,540
因为有可能出现这样的句子：
 "Teddy bears are on sale"
（泰迪熊在打折）

107
00:07:49,540 --> 00:07:53,470
如果只知道头三个词的话

108
00:07:53,470 --> 00:07:57,160
我们不能确信Teddy是否是
人名的一部分

109
00:07:57,160 --> 00:08:00,390
第一个里面是人名，
第二个例子里面不是

110
00:08:00,390 --> 00:08:05,890
如果你光看头三个词语，
你将无从分辨

111
00:08:05,890 --> 00:08:09,900
这个特定的神经网络结构的一个限制是

112
00:08:09,900 --> 00:08:15,000
在序列中对某一时间的预测仅使用

113
00:08:15,000 --> 00:08:19,690
之前的输入，
 而不使用序列中之后的信息

114
00:08:19,690 --> 00:08:24,450
以后，在有关双向循环神经网络

115
00:08:24,450 --> 00:08:29,220
BRNNS视频中
我们来讨论这个问题

116
00:08:29,220 --> 00:08:33,495
目前这个简单的单向神经网络

117
00:08:33,495 --> 00:08:38,670
架构足够说明一些重要的概念

118
00:08:38,670 --> 00:08:41,830
之后我们将对这些想法
做出一些简洁快速的修正

119
00:08:41,830 --> 00:08:46,110
让我们在判断ŷ3的时候同时考虑到

120
00:08:46,110 --> 00:08:49,790
序列中（当前时间点）
之前和之后的信息

121
00:08:49,790 --> 00:08:51,280
我们将在后面的视频中学习

122
00:08:52,390 --> 00:08:55,965
我们把这个神经网络所做的计算写出来

123
00:08:55,965 --> 00:08:56,680
（神经网络计算公式）

124
00:08:57,892 --> 00:09:02,601
这是一个整理过的神经网络的示意图

125
00:09:02,601 --> 00:09:05,376
就像我前面说到的

126
00:09:05,376 --> 00:09:10,093
通常开始时，你将输入一个全零的向量
作为a0的初始值

127
00:09:10,093 --> 00:09:13,924
接下来，这是正向传播的示意。

128
00:09:13,924 --> 00:09:20,082
要计算a1, 我们通过激活函数g

129
00:09:20,082 --> 00:09:26,763
中使用参数Waa乘以a0

130
00:09:26,763 --> 00:09:30,693
加上Wax乘以x1，

131
00:09:30,693 --> 00:09:36,851
加上偏置ba

132
00:09:36,851 --> 00:09:42,093
然后计算ŷ1来做出对第1步的预测

133
00:09:42,093 --> 00:09:48,277
这里会使用和上面不一样的激活函数

134
00:09:48,277 --> 00:09:53,353
（将g函数）用于Wya乘以

135
00:09:53,353 --> 00:09:58,429
a1加上by

136
00:09:58,429 --> 00:10:06,230
这里我使用的标记惯例来替代这些矩阵如Wax

137
00:10:06,230 --> 00:10:10,200
第二个下标表示Wax乘以

138
00:10:10,200 --> 00:10:12,220
像xy那样的数

139
00:10:12,220 --> 00:10:18,290
a表示这是用来计算a那样的数

140
00:10:18,290 --> 00:10:23,980
类似的，这里Wya乘以

141
00:10:23,980 --> 00:10:29,730
类似a的数来计算类似y的数

142
00:10:29,730 --> 00:10:37,130
用来计算激活数值的激活函数

143
00:10:37,130 --> 00:10:42,558
常常使用tanh函数以及RNN，
 有时候也会使用ReLU

144
00:10:42,558 --> 00:10:48,630
tanh函数是最常见的选择

145
00:10:48,630 --> 00:10:53,840
我们有其他方法来避免梯度消失的问题

146
00:10:53,840 --> 00:10:56,420
我们会在这周晚些时候讨论。

147
00:10:56,420 --> 00:11:00,900
根据输出y的性质

148
00:11:00,900 --> 00:11:05,520
如果是二分类问题，
你可能会使用Sgimoid激活函数

149
00:11:05,520 --> 00:11:09,390
如果是k类的分类问题
 你会使用softmax函数

150
00:11:09,390 --> 00:11:14,600
但是激活函数的选取
还是要根据输出y的性质

151
00:11:14,600 --> 00:11:15,390
来确定

152
00:11:15,390 --> 00:11:19,340
所以前面所讲的名字识别问题，
输出y不是0就是1

153
00:11:19,340 --> 00:11:23,240
这里的第二个g可以使用
sigmoid激活函数

154
00:11:24,730 --> 00:11:28,807
这里可以写作g2来和
前面的激活函数加以区别

155
00:11:28,807 --> 00:11:32,730
但是我通常不会这么做

156
00:11:32,730 --> 00:11:38,002
通常，在步骤t的时候

157
00:11:38,002 --> 00:11:43,954
t等于激活函数Waa的g乘以上一步的a

158
00:11:43,954 --> 00:11:49,395
加上目前步骤的Wax再加上b_a

159
00:11:49,395 --> 00:11:54,009
和ŷt等于g

160
00:11:54,009 --> 00:11:57,828
g可以是不同的激活函数，
 W_ya的g

161
00:11:59,099 --> 00:12:03,910
乘以a（t），加上by

162
00:12:03,910 --> 00:12:08,960
这些等式定义了神经网络中的正向传播

163
00:12:08,960 --> 00:12:14,603
开始（计算）时将a_0的初始值设为全零向量

164
00:12:14,603 --> 00:12:19,355
用a0, x1计算出a1和ŷ1

165
00:12:19,355 --> 00:12:25,417
然后用a1，x_2计算a2和ŷ2,
以此类推

166
00:12:25,417 --> 00:12:29,722
这样你就在这个图中从左到右
进行了正向传播的计算

167
00:12:29,722 --> 00:12:30,857
的图形

168
00:12:30,857 --> 00:12:34,577
为了帮助我们建立更加复杂的神经网络

169
00:12:34,577 --> 00:12:39,187
我将简化这部分的表示方法

170
00:12:39,187 --> 00:12:41,897
我把这两个等式拷贝到下一张幻灯片

171
00:12:43,820 --> 00:12:44,630
疑问词也会待在它们本身应该在的地方

172
00:12:45,930 --> 00:12:50,710
我要做的就是简化表示方法

173
00:12:50,710 --> 00:12:55,840
我把这部分用简单的方式的改写

174
00:12:55,840 --> 00:13:01,341
记作at等于g乘以

175
00:13:01,341 --> 00:13:06,845
矩阵Wa乘以一个新数值

176
00:13:06,845 --> 00:13:12,197
at-1, xt

177
00:13:12,197 --> 00:13:17,552
加上ba

178
00:13:17,552 --> 00:13:22,340
左面和右面带下划线的数值应该是一样的

179
00:13:23,340 --> 00:13:28,070
我们使用Waa和

180
00:13:28,070 --> 00:13:33,120
和Wax放在一起来表示Wa

181
00:13:33,120 --> 00:13:39,190
我把他们水平叠加起来便得到矩阵Wa

182
00:13:41,360 --> 00:13:46,840
例如，如果a是100维的

183
00:13:47,960 --> 00:13:53,520
在我们例子中
x是10000维

184
00:13:53,520 --> 00:13:59,104
那么Waa就是100X100的矩阵

185
00:13:59,104 --> 00:14:03,760
Wax就是100X100000维度的矩阵

186
00:14:03,760 --> 00:14:09,570
把两个矩阵叠加在一起，
 这个是100维

187
00:14:09,570 --> 00:14:14,195
这里是100和10000维

188
00:14:14,195 --> 00:14:20,170
Wa就是一个100X10100的矩阵

189
00:14:20,170 --> 00:14:27,070
左边的矩阵画的不等比
因为Wax是一个很宽的矩阵

190
00:14:28,420 --> 00:14:31,767
这个记号表示

191
00:14:31,767 --> 00:14:35,080
它有两个向量x

192
00:14:35,080 --> 00:14:40,910
把他们叠加在一起，
 我们用这个表示方法来说明

193
00:14:40,910 --> 00:14:42,330
我们用向量at-1

194
00:14:42,330 --> 00:14:46,981
它是100维的，
 将其叠加在at上

195
00:14:46,981 --> 00:14:50,468
我们得到一个

196
00:14:50,468 --> 00:14:55,752
10100维的向量

197
00:14:55,752 --> 00:15:00,952
我希望大家可以自己验证一下

198
00:15:00,952 --> 00:15:06,402
这个矩阵乘以这个向量
 我们可以得到原来的数值

199
00:15:06,402 --> 00:15:11,765
因为Waa矩阵Wax矩阵相乘

200
00:15:11,765 --> 00:15:17,717
再与向量[a^(t-1)，x^t]相乘

201
00:15:17,717 --> 00:15:23,669
等于Waa乘以a^(t-1)

202
00:15:23,669 --> 00:15:31,659
加上Wax乘以x^(t)
 和这里的值完全相同

203
00:15:31,659 --> 00:15:37,974
所以这种表示方法的好处是
 不再需要两个作为参数的矩阵

204
00:15:37,974 --> 00:15:43,910
我们可以把Waa，
Wax压缩成一个参数的矩阵Wa

205
00:15:43,910 --> 00:15:49,692
这样，在我们建立更复杂模型时
便可以简化表示

206
00:15:49,692 --> 00:15:54,514
同样的，我将重写ŷ^9(t)

207
00:15:54,514 --> 00:16:00,298
Wy,a(t)加上by

208
00:16:00,298 --> 00:16:05,503
我们在Wy和by里面用了下标y

209
00:16:05,503 --> 00:16:10,383
解释了输出量的类型是

210
00:16:10,383 --> 00:16:14,760
Wy表示计算输出y的权重矩阵

211
00:16:14,760 --> 00:16:19,330
Wa和ba表示计算激活数值a^(t)

212
00:16:19,330 --> 00:16:24,030
激活函数的参数。

213
00:16:24,030 --> 00:16:28,200
那么你了解了关于RNN的基础知识

214
00:16:28,200 --> 00:16:32,480
接下来我们讨论反向传递

215
00:16:32,480 --> 00:16:36,500
以及RNN如何学习