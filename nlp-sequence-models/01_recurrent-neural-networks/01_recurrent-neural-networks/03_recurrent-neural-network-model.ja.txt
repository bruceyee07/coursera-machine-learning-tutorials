前回の動画では 今後使っていく表現方法を学び
シーケンス学習問題を定義しました ではどうやって
ニューラルネットワークのモデルを構築して x から y へのマッピングを学習するか
について学んでいきましょう まず検討としてスタンダードなニューラルネットワークを
このタスクに試してみましょう 前回の例では9個の入力語がありました
例えばこんなことを考えてみましょう その9個の入力語に対応した
9個のone-hotベクトルを受け取り スタンダードなニューラルネットワークに入力され
隠れ層をいくつか通って 最終的には9個の0/1の値が
アウトプットとして出力されます それを見て
どの語が人名かを判断できるわけです しかしこれでは
うまくいかないことがわかります 問題が大きく2つあります 1つは 入力と出力は データによって長さが異なる
可能性があるということです つまり全てのデータにおいて
入力長 Tx と出力長 Ty が 等しいと仮定することができません もし文の最大長がわかっていれば すべての入力に0パディングして
最大長に揃えるという手があるかもしれません しかしこれも
あまりいい方法ではなさそうです 2つ目の問題は
こちらのほうがより深刻かもしれませんが こういった単純な
ニューラルネットワークの構造だと 文の色々な場所で学習した特徴量を
共有しません つまり もしニューラルネットワークが
1番目に登場した単語 "Harry" が 人名の一部である
と判断するよう学習したならば 別の場所 x〈t〉に登場した "Harry" も たぶん人名だろうと自動的に
判断してくれたら良さそうですよね これは畳み込みニューラルネットワークのところで
学んだことに似ているかもしれません 画像のある一部分で学習されたことが 一般化されて画像の他の部分にも
そのまま適用されるような仕組みです 同じような効果がシーケンスデータにも
欲しいわけです またこれも畳み込みネットで
学んだことですが 良い表現を用いることで
モデル内のパラメタ数を減らすことができます この例において
これら入力の1つ1つは 1万次元のone-hotベクトルであり
非常に大きな入力層です もし入力サイズがトータルで
最大語長×10,000だとすると 第1層の重み行列は とんでもない数の
パラメタを持つことになってしまいます そこで再帰型ニューラルネットワークは
次のスライドから説明していきますが このどちらの欠点もありません それでは
再帰型ニューラルネットワークとは何でしょうか 1つ作ってみましょう
今 左から右へ文を読んでいるとします 出てきた一番最初の単語を
x〈1〉としましょう 何をするのかというと
その最初の単語を取って ニューラルネットワーク層に与えます 図に描きましょう
これが最初のニューラルネットワークの隠れ層です 出力を予測するような
ニューラルネットワークがあってもいいですね つまり「これは人名か否か？」
ということですね 再帰型ニューラルネットワークが
何をしているのかというと 続いて文中の2番目の単語
x〈2〉が入力された時 x〈2〉だけを使って y〈2〉を予測するのではなく タイムステップ1で計算されたものから
何かしら情報を加えるということをします 具体的に言うと タイムステップ1からアクティベーション値（a〈1〉）が
タイムステップ2に渡されます そして次のタイムステップでは 再帰型ニューラルネットワークは
3番目の単語 x〈3〉を入力して 予測値 y^〈3〉を出力します
その後も同様に 最終ステップまで続きます<tx> 入力は x〈Tx〉出力は y^〈Ty〉です この例では Tx = Ty となっていますが 等しくない場合は
アーキテクチャが少し変わります 各タイムステップで
再帰型ニューラルネットワークは アクティベーション値を
次のタイムステップで使えるよう渡します このプロセスの起点には タイム0のアクティベーション値を
用意しておく必要があります 普通は要素0のベクトルを使いますが a〈0〉を乱数で初期化する研究者もいますし 他にも a〈0〉を初期化する方法はあります 実際には タイム0の疑似アクティベーション値として 要素0のベクトルを使うのが依然最も一般的ですので
それをニューラルネットワークの入力として使いましょう 論文や書籍によっては この種のニューラルネットワークを
次のようなダイアグラムで記述している事があります 全てのタイムステップで
x を入力し y を出力します インデックス〈t〉がついているかもしれませんね そして再帰型の接続を表現するために
このようなループを描くことがあります 自身へのフィードバックを表しています
黒塗りの四角が描かれることもあり タイムステップが1遅れることが表現されています 個人的にはこの再帰型ダイアグラムは
解釈がしにくいので このコースでは左側に描いてあるような
展開したダイアグラムを使うことにします 教科書や研究論文によっては
右のようなダイアグラムを見ることがありますが 何を意味しているのか考えるときは 心のなかで左のようなダイアグラムに
展開して考えるようにしています 再帰型ニューラルネットワークは
データを左から右へスキャンします そして各タイムステップで使われるパラメタは
共有されます 次のスライドでもっと詳しく説明しますが 幾つかのパラメタのセットがあります x〈1〉から隠れ層への接続を制御している
パラメタがありますが これもパラメタのセットで
Waxと書くことにします 全タイムステップで
同じパラメタWaxが使われます 右のダイアグラムでもWaxと書いていいでしょう そして アクティベーション値の接続
つまり横方向の接続も 別のパラメタのセットWaaで制御されます これも どのタイムステップにおいても
同じパラメタWaaが使われます 同様に 出力値つまり予測値を制御するWyaがあります 次のスライドでこれらのパラメタがどのように働くのか
きちんと説明しましょう この再帰型ニューラルネットワークで
これがどういうことを意味するのかというと y〈3〉予想する時
ネットワークはx〈3〉だけではなく x〈1〉や x〈2〉からも情報を受け取っています
なぜかというとx〈1〉の情報は このルートを通って y〈3〉の予測に寄与出来るからです RNN (再帰型ニューラルネットワーク) の弱点は
予測をするときに シーケンス内の早い方の
情報しか使わないということです 例えば y〈3〉の予測をする時
x〈4〉 x〈5〉 x〈6〉 ・・・の情報は使いません なぜこれが問題になるのかというと
次のような文が与えられたとき （彼は「テディ・ルーズベルトは偉大な大統領」だと言った） "Teddy" が人名の一部かどうかを
判断するのに 最初の2単語の情報だけでなく その後の単語の情報も知っていると
役に立つからです というのも こんな文もあり得るからです
（彼は「テディベア セール中！」と言った） 最初の3単語を与えられただけでは "Teddy" が人名かどうか
自身を持って判断することができません 1つ目の文では人名ですが
2つ目の文では人名ではありません しかし最初の3単語を見ただけでは
その違いがわかりません この種のニューラルネットワーク構造における
弱点の一つです ある時点での予想は 入力データのうち シーケンスの早い方の情報は使われますが
後ろの方は使われないのです この問題はもっと後のビデオで扱います 双方向再帰型ニューラルネットワーク (BRNN)
と呼ばれるものです 現時点ではよりシンプルな
この単方向のニューラルネットワークで 充分キーコンセプトを説明出来ると思います 後ほど簡単な修正を加えて 例えば y^〈3〉の予測をするときに シーケンス内の前後の情報を
両方使えるようにします 後ほど詳しく見ていきます ではこのニューラルネットワークがやっている計算を
詳しく書いていきましょう   こちらが先程のニューラルネットワークを
キレイに書き直したものです 先に説明したとおり 大抵の場合 要素が全て0のベクトル a〈0〉から始めます 次に順伝播がどのようになっているか
お見せします a〈1〉を計算するには
活性化関数 g に次のようなものを入れます Waa 掛ける a〈0〉 足す Wax 掛ける x〈1〉 足すバイアス
ここでは ba と書きましょう そして y^〈1〉
つまり時間1での予測値を計算します 活性化関数 おそらくさっきのとは
違う活性化関数に次のものを入れます Wya 掛ける a〈1〉 足す by ちなみに 表記上の習慣として
Waxなどの行列に使われている下付き文字について 2番目の文字 x は Wax が
x 関連の数に掛けられる ということを意味し 1番目の文字 a は Wax が
a 関連の数を算出するということを意味します 同様に もうおわかりかと思いますが
Wya は a 関連の数に掛けられ y 関連の数を算出する
ということになります アクティベーション値を計算するのに使われる
活性化関数としては RNN では tanh をよく見かけます
ReLU が使われることもありますが 実際のところ tanh が主流です また勾配消失問題については
別の対策を行います この週の後半にご説明します また アウトプット y の種類によって もし2クラス分類問題であれば
シグモイド活性化関数を使うでしょうし kクラス分類問題であれば
ソフトマックスを使うかもしれません いずれにせよ ここでの活性化関数の選択は
どういう種の出力 y を考えているかによって 異なります 固有表現抽出タスクであれば
ここで y は0か1としますが 2つ目の g は
シグモイド活性化関数になるでしょう また これらは違う活性化関数であると
明確に区別したければ g1 g2 などと書くことも出来るのですが
私は普通そのようにはしません またより一般的に 時間 t において a〈t〉は g の Waa 掛ける
一つ前のタイムステップの a〈t-1〉 足す Wax 掛ける 現在のタイムステップの x〈t〉
足す ba また y^〈t〉は 活性化関数 g 同じ活性化関数とは限らないですが
 g の Wya 掛ける a〈t〉 足す by となります これらの数式がこのニューラルネットワークにおける
順伝播の定義です 要素が全て0のベクトル a〈0〉を起点に
a〈0〉, x〈1〉を使って a〈1〉, y^〈1〉を計算し
その後 x〈2〉を受け取って x〈2〉, a〈1〉を使って a〈2〉, y^〈2〉を計算し ・・・とこのように図の左から右へ進みながら
順伝播を行っていきます ・・・とこのように図の左から右へ進みながら
順伝播を行っていきます さてより複雑なニューラルネットワークを
楽に作れるよう この表記方法を少し単純化することにします この2つの数式を次のスライドにコピーしましょう こちらです さて実際に単純化していくわけですが この部分を少しシンプルに
書いていきます a〈t〉イコール g の 行列 Wa 掛ける 新たな数 a〈t-1〉, x〈t〉 足す ba 下線を引いた部分が等しい部分です Wa は次のように定義できます 行列 Waa と Wax を取り出して横に並べ こんな風に水平に積んでいる感じでしょうか
これを行列 Wa とします 例えば a が100次元だったとします また x〈t〉を10000次元だとします すると Waa は 100×100 次元の行列 Wax は 100×10000 次元の行列となります この2つの行列をくっつけると
縦が100次元 横方向左側が100次元
右側が10000次元ですので Wa は 100×10100 次元の行列となります この図だとスケールどおりでないですね
Wax は非常に幅が広い行列ですので またこっちの表記がどういう意味かというと 単に2つのベクトルを持ってきて くっつけただけです
この表記が表しているのは a〈t-1〉 100次元のベクトルでしたね
それを x〈t〉の上に積みます
すると 10100次元ベクトルが出来上がります ぜひご自身でチェックしてほしいのですが
この行列に このベクトルを掛ければ
元と同じものが得られます なぜなら行列 Waa Wax に a〈t-1〉 x〈t〉ベクトルを掛けると これは単に
Waa 掛ける a〈t-1〉 足す Wax 掛ける x〈t〉に等しくなります
これはまさにこの元々の部分に等しいわけです この表記のいいところは
2つのパラメタ行列を連れ回さなくて済むことです Waa Wax を一つのパラメタ行列 Wa に
詰め込んでいるわけです これによって 複雑なモデルを作るときにも
シンプルな表現にすることができます 同様にこちらも少し書き直して Wy a〈t〉プラス by
としましょう これで Wy by の下付き文字が 何の値を算出しているかを
表すだけになりました つまり Wy はy関連の値を算出するための
重み行列であることを表しており Wa や ba は
a つまりアクティベーション関連の 値を算出するための
パラメタであることを表しています 以上で 基本的な再帰型ニューラルネットワークが
どんなものか分かったと思います 次はRNNにおける逆伝播と
その学習方法についてお話しましょう  