1
00:00:00,000 --> 00:00:02,704
到目前为止你所见过的 RNNs 的不同版本

2
00:00:02,704 --> 00:00:05,055
已经可以很好的解决问题了。

3
00:00:05,055 --> 00:00:09,240
但对于学习非常复杂的函数，有时候

4
00:00:09,240 --> 00:00:13,860
把多层 RNNs 堆在一起形成更深层的这些模型的版本会很有帮助。

5
00:00:13,860 --> 00:00:19,195
在本视频中, 你将看到如何构建这些更深层的 RNNs。让我们来看看。

6
00:00:19,195 --> 00:00:20,850
所以你记得，对于一个标准的神经网络,

7
00:00:20,850 --> 00:00:23,520
你将有一个输入 x。

8
00:00:23,520 --> 00:00:29,580
然后, 这是堆积到一些隐藏的层, 所以可能有激活

9
00:00:29,580 --> 00:00:33,610
a1 来自于第一个隐藏层,

10
00:00:33,610 --> 00:00:36,790
然后将其堆叠到下一层,并且激活 a2,

11
00:00:36,790 --> 00:00:40,000
然后也许还有一层,

12
00:00:40,000 --> 00:00:42,880
激活 a3 然后你做一个预测ŷ。

13
00:00:42,880 --> 00:00:45,290
所以一个深层 RNN 有点像这样,

14
00:00:45,290 --> 00:00:47,900
通过采用这个我刚刚手画的神经网络并且

15
00:00:47,900 --> 00:00:51,370
按时间展开。让我们来看看。

16
00:00:51,370 --> 00:00:54,070
这是你所看到过的标准 RNN。

17
00:00:54,070 --> 00:00:56,395
但我改变了一点点符号, 改变的是,

18
00:00:56,395 --> 00:01:01,450
我并没有把时刻0的激活写为 a0 ,

19
00:01:01,450 --> 00:01:06,605
我添加了这个正方形括号1
表示这是来自于第一层的。

20
00:01:06,605 --> 00:01:12,970
所以我们将要使用a[l]来表示它是

21
00:01:12,970 --> 00:01:20,285
对应第l层上面的激活，
并且用<t>来表示它对应时间t

22
00:01:20,285 --> 00:01:23,830
所以这会在a[1]<1>上激活,

23
00:01:23,830 --> 00:01:32,688
这是(a[1]<2>) (a[1]<3>) (a[1]<4>)。

24
00:01:32,688 --> 00:01:38,950
然后我们可以把这些叠在上面

25
00:01:38,950 --> 00:01:45,697
这将会是一个包含三个隐藏层的新网络。

26
00:01:45,697 --> 00:01:51,740
让我们来看一个如何计算这个数值的例子。

27
00:01:51,740 --> 00:01:56,440
a[2]<3>有两个输入。

28
00:01:56,440 --> 00:01:58,750
它有一个来自底部的输入,

29
00:01:58,750 --> 00:02:03,005
另外还有一个来自左边的输入。

30
00:02:03,005 --> 00:02:09,055
这里计算机会把一个激活函数g代入到一个矩阵上。

31
00:02:09,055 --> 00:02:14,140
也就是Wa,因为要计算a的值，还有一个激活后的值。

32
00:02:14,140 --> 00:02:16,097
对于第二层,

33
00:02:16,097 --> 00:02:19,510
我要给它a[2]<2>的值，

34
00:02:19,510 --> 00:02:25,045
也就是这个，逗号a[1]<3>，也就是这个值

35
00:02:25,045 --> 00:02:34,653
加上第二层相应的ba

36
00:02:34,653 --> 00:02:37,400
这就是你计算激活值的方式。

37
00:02:37,400 --> 00:02:41,985
因此相同的参数Wa[2]以及

38
00:02:41,985 --> 00:02:48,515
ba[2]会在这一层的每一次计算中被使用。

39
00:02:48,515 --> 00:02:57,150
与之相反，第一层会有它自己的参数Wa[1]和ba[1]。

40
00:02:57,150 --> 00:03:01,625
但是对于像左边这样的标准RNN，

41
00:03:01,625 --> 00:03:03,580
我们见过非常深的神经网络，

42
00:03:03,580 --> 00:03:05,575
甚至可能超过100层。

43
00:03:05,575 --> 00:03:10,970
对于RNN来说，有三层已经算很多了。

44
00:03:10,970 --> 00:03:12,720
由于时间这一维度的存在,

45
00:03:12,720 --> 00:03:17,260
即使只有很少的层数这些网络也已经变得很大。

46
00:03:17,260 --> 00:03:22,535
你很少会看见这样的神经网络堆叠到100层。

47
00:03:22,535 --> 00:03:26,080
你有时会看到的是

48
00:03:26,080 --> 00:03:30,040
有一些互相堆叠的循环层。

49
00:03:30,040 --> 00:03:32,988
但是你可能取的是这里的输出，我们先不说这个，

50
00:03:32,988 --> 00:03:36,730
然后有一些没有水平连接的深层

51
00:03:36,730 --> 00:03:41,495
但是在这里有一个深度网络最后会预测y<1>。

52
00:03:41,495 --> 00:03:48,000
然后这里也可以有同样的深度网络用来预测y<2>。

53
00:03:48,000 --> 00:03:51,270
这就是我们比较多的时候会见到的一种网络结构当你

54
00:03:51,270 --> 00:03:55,065
有三个以时间连接的循环单元，

55
00:03:55,065 --> 00:03:56,655
然后跟着一个网络，

56
00:03:56,655 --> 00:03:58,285
后面跟着另一个网络，

57
00:03:58,285 --> 00:04:00,705
就像我们看到的y<3>和y<4>。

58
00:04:00,705 --> 00:04:04,105
这里有一个深度神经网络，但没有水平的连接。

59
00:04:04,105 --> 00:04:08,095
这是一种我们经常看见的结构。

60
00:04:08,095 --> 00:04:12,410
并且很经常的，这些模块不一定要是标准RNN，

61
00:04:12,410 --> 00:04:14,390
简单RNN模型。

62
00:04:14,390 --> 00:04:17,770
它们也可以是GRU模块或者LSTM模块。

63
00:04:17,770 --> 00:04:24,110
最后，你也可以建立一个双向RNN的深层版本。

64
00:04:24,110 --> 00:04:30,085
因为深层RNN在训练时计算力花费很大，

65
00:04:30,085 --> 00:04:32,715
经常有很大的时间范围,

66
00:04:32,715 --> 00:04:37,700
虽然你不会看到那么多深度循环层，

67
00:04:37,700 --> 00:04:42,320
这有三个在时间上相连接的深度循环层。

68
00:04:42,320 --> 00:04:45,530
你不会看到那么多深度循环层，

69
00:04:45,530 --> 00:04:48,940
不会像在一个传统的深层神经网络中那么多层。

70
00:04:48,940 --> 00:04:51,510
所以这就是深层 RNNs的所有内容。

71
00:04:51,510 --> 00:04:53,810
这周你所看到的

72
00:04:53,810 --> 00:04:55,621
从基本的 RNN,

73
00:04:55,621 --> 00:04:57,050
基本的循环单位,

74
00:04:57,050 --> 00:04:58,149
到 GRU,到 LSTM, 到双向 RNN,

75
00:04:58,149 --> 00:05:01,770
以及你刚才看到的这些深层的版本,

76
00:05:01,770 --> 00:05:04,685
你现在有一个非常丰富的工具箱来构建

77
00:05:04,685 --> 00:05:08,530
非常强大的模型学习序列模型。

78
00:05:08,530 --> 00:05:11,450
我希望你喜欢这周的视频。

79
00:05:11,450 --> 00:05:16,000
祝你编程练习好运, 我期待下周见到你。