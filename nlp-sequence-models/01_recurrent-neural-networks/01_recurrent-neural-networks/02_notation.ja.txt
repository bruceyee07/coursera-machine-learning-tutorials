前回のビデオでは シーケンスモデルを適用できる様々な応用例から
幾つかをご紹介しました これらシーケンスモデルを構築していく上で使用する
表記法についてまずは定義していきます 取っ掛かりの例として このような文を入力とする
シーケンスモデルを構築したいとしましょう 「ハリー・ポッターとハーマイオニー・グレンジャーは
新しい呪文を発明した」 ちなみにこれらは J・K・ローリングの ハリー・ポッターシリーズに出てくるキャラクターです この文のどの部分が人の名前であるか
自動的に判別してくれるような シーケンスモデルが欲しいとしましょう これは固有表現抽出と呼ばれる問題で サーチエンジンなどにも用いられています 例えば過去24時間のニュースに
登場したすべての人を見出し化して 記事を適切に検索できるようにする
といったようなことです 固有表現抽出は 人名 会社名 時間 場所 国名 通貨 その他
様々なタイプの文字列抽出に用いられます さてこの x を入力したとき 出力 y として
1単語につき値を1つ出力するような モデルを考えます ターゲットの出力 y は
対応する入力語が人名かどうかを表します 細かいことを言うと
この表現はベストではありません もう少しまともな出力表現として 単純に人名かどうかの情報だけでなく 人名がどこで始まってどこで終わっているかを
表現したものもあります "Harry Potter" はここからここまで 次の名前はここからここまで
といったような具合です ただここでの取っ掛かりの例として よりシンプルな出力表現で進めでいきましょう さて 入力は9個の単語列です 最終的にはこれらの単語に対応した
9個の特徴量を使うことになります シーケンス内におけるインデックスを表現するのに x に上付きの山括弧〈〉を使い
x〈1〉 x〈2〉 x〈3〉のように表します この例では x〈9〉までで全ての位置が指定できます x〈t〉つまりインデックス t によって シーケンスの中のインデックスを表現します t は temporal から来ています
（temporal: 時間的な） シーケンスが時間的なものであろうがなかろうが シーケンスの中の位置を指定する
インデックスとして t を使います 出力についても同様に これらの値をそれぞれ示すのに y〈1〉 y〈2〉 y〈3〉・・・・y〈9〉 を使います また入力長を表すのに
Tx (T下付きx) を使いましょう この例だと9単語ありますので Tx = 9 となります
同様に出力長を表すのにTyを使います この例では Tx = Ty ですが 前回のビデオで見たとおり
Tx と Ty は必ずしも同じではありません これまでに使ってきた表記法として i 番目のトレーニングデータを表す
x(i) を覚えているでしょうか この場合 i 番目のトレーニングデータシーケンス内の
t 番目の要素を表すのに この表記 x(i)〈t〉を使います またトレーニングセットのデータによって
入力長は異なります つまり Tx(i) は i 番目のトレーニングデータの
入力長を表すことになります 同様に y(i)〈t〉は i 番目のトレーニングデータの
出力シーケンス内の t 番目の要素を表し Ty(i) は i 番目のトレーニングデータの
出力シーケンスの長さを表すことになります なのでこの例では Tx(i) = 9 ですが
違うトレーニングデータとして 15単語の文が入力されたとすると
そのデータに対する Tx(i) は 15 となるわけです さてNLP（自然言語処理）を始めていきましょう NLP（自然言語処理）に対する本格的な取り組みとしては
これが最初にあたります 決めなければならないこととして シーケンス内の各単語をどう表現するか
ということがあります 例えば"Harry"という語をどう表現するか つまり x〈1〉がどのようになっていればいいのか ここからは シーケンス内の各単語をどう表現するか
について話していきましょう シーケンス内の単語を表現するにあたり
まず思いつくのは ボキャブラリーを用意することでしょうか 辞書と呼ばれることもありますが つまりは 表現に使われる単語のリストを
作ることを意味します ボキャブラリー内の最初の単語は "a" です 辞書内の最初の単語です 第2語目は "aaron"
それからもう少し下の方に "and" どこかに "harry" や "potter" が入っているでしょう そして一番下まで来て
辞書の最後の単語は "zulu" とかでしょうか "a" は1番目の単語 "aaron" 2番目の単語 この辞書では "and" 367番目 "harry" は4075番目 "potter" は6830番目 最終語のzuluは
例えば10,000番目に現れることになります この例において 1万語サイズの辞書が使われている事になりますが これは近年のNLPのアプリケーションからすると
かなり小さいと言えます 商用のアプリケーションで言えば 3〜5万語サイズの辞書が一般的ですし
10万語のものも珍しくありません 大規模なインターネット企業などは 100万語を上回るサイズの辞書を
使っているところもあります ただ 商用アプリケーションとしてよく見る辞書のサイズは おそらく3〜5万語といったところでしょう 説明用としては1万語で十分ですので
この値で進めましょう さて1万語の辞書を選びました
この辞書を構築するには トレーニングセットをすべて調べ 登場回数上位1万語を使うという方法があります またオンライン辞書をいくつか調べれば 英語で使われる一般的な1万語がわかります そしてone-hot表現を使ってこれら各語を
表現することが出来るようになります 例えば "harry" を表している x〈1〉は
次のようなベクトルになります "harry" の辞書内での位置つまり
4075番目の要素のみ 1 それ以外全て 0 次に x〈2〉も同様に 6830番目が 1 であることを除き
全て 0 のベクトル "and" は367番目の単語として表現されていましたので x〈3〉は367番目が 1 であることを除き
全て 0 のベクトルになります これらは辞書サイズが1万語であれば それぞれ1万次元のベクトルとなります そして "a" は辞書の1番目の単語でしょうから その "a" に相当する x〈7〉は 最初が 1 で 残りが全て 0 というベクトルになります この表現では 文の t 番目の要素の値として
x〈t〉がone-hotベクトルとして表されます one-hotというのはベクトルの唯一の要素が 1 で
それ以外は全て 0 というところから来ています この文では9個のone-hotベクトルを使って
9単語を表しています ゴールは この x に対するこの表現が与えられた時 シーケンスモデルを使って
ターゲット y へのマッピングを学習することです これは教師あり学習の問題として扱います つまり x y 両方が揃ったラベル付きデータが必要です もう一つだけ細かい話をします 後のビデオで詳しく説明されますが もしボキャブラリーにない単語が出てきたら
どうすればいいでしょうか 答えを言うと Unknown Word という
新しいトークンというか偽単語をつくることになります 表記としては〈UNK〉のように表し
ボキャブラリーにない単語を意味します 後ほど詳しく説明します まとめると トレーニングセットの x y を
シーケンスデータとして扱う場合の 表記方法について説明しました 次のビデオでは
x から y へのマッピング方法として 再帰型ニューラルネットワークについて
説明をしていきます