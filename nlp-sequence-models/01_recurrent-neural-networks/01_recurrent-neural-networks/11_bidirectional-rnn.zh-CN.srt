1
00:00:00,000 --> 00:00:05,285
到目前为止, 你已经了解了 RNNs 的大部分组成部分。

2
00:00:05,285 --> 00:00:09,910
但是, 还有两个想法
 可以让你建立更强大的模型。

3
00:00:09,910 --> 00:00:12,005
一个是双向 RNNs,

4
00:00:12,005 --> 00:00:14,370
这让你在一个时间点获得

5
00:00:14,370 --> 00:00:17,368
序列中前部分的和后部分的信息,

6
00:00:17,368 --> 00:00:19,500
所以我们将在这个视频中讨论这部分内容。

7
00:00:19,500 --> 00:00:21,870
第二, 是深度 RNNs,

8
00:00:21,870 --> 00:00:23,770
你将会在下一个视频中看到。

9
00:00:23,770 --> 00:00:26,817
因此, 让我们从双向 RNNs 开始。

10
00:00:26,817 --> 00:00:29,230
为了激发双向 RNNs,

11
00:00:29,230 --> 00:00:32,010
看一下这个网络,
 在之前识别人名的上下文识别中

12
00:00:32,010 --> 00:00:35,570
已经看到过几次。

13
00:00:35,570 --> 00:00:38,115
这个网络的一个问题是,

14
00:00:38,115 --> 00:00:43,280
为了弄清第三个单词
“泰迪“是否是人名的一部分,

15
00:00:43,280 --> 00:00:46,760
仅仅考虑句子的前半部分是不够的。

16
00:00:46,760 --> 00:00:50,910
所以为了判断 Y ^<3>是 0 还是 1,

17
00:00:50,910 --> 00:00:52,585
你需要更多的信息

18
00:00:52,585 --> 00:00:56,935
不只是前三个单词, 因为
前三个单词不足以告诉你, 比如他们会

19
00:00:56,935 --> 00:01:03,565
谈论泰迪熊或者前美国总统泰迪. 罗斯福。

20
00:01:03,565 --> 00:01:09,510
因此, 这是单向或正向 RNN。

21
00:01:09,510 --> 00:01:13,890
我刚才说的都成立，

22
00:01:13,890 --> 00:01:16,020
无论这些部分是

23
00:01:16,020 --> 00:01:24,130
标准 RNN单位，GRU 单位,
 还是 LSTM 单位。

24
00:01:24,130 --> 00:01:29,245
所有这些单位都是单向且朝前的。

25
00:01:29,245 --> 00:01:32,010
相对而言，一个双向 RNN（BRNN）

26
00:01:32,010 --> 00:01:34,865
可以解决此问题。

27
00:01:34,865 --> 00:01:38,473
双向 RNN 的工作原理如下：

28
00:01:38,473 --> 00:01:45,405
我将用一个简化的四维输入,
 或者说是一个四个单词的句子。

29
00:01:45,405 --> 00:01:47,080
所以我们有四个输入矢量。

30
00:01:47,080 --> 00:01:49,224
x1 到 x4

31
00:01:49,224 --> 00:01:56,110
因此, 这个网络的头部层将有一个前向递归的部分。

32
00:01:56,110 --> 00:02:04,135
所以我们要叫这个 A1, A2,

33
00:02:04,135 --> 00:02:09,465
A3，A4，

34
00:02:09,465 --> 00:02:12,955
然后我会画一个右箭头

35
00:02:12,955 --> 00:02:17,000
去表示这是前向递归的组成,

36
00:02:17,000 --> 00:02:22,985
因此, 它们将连接如下。

37
00:02:22,985 --> 00:02:28,512
因此, 这四个单位
每次都输入当前的 X,

38
00:02:28,512 --> 00:02:33,709
然后喂入数据

39
00:02:33,709 --> 00:02:42,730
去帮助预测 Y^1,

40
00:02:42,730 --> 00:02:46,928
Y^2, Y^3 和 Y^4

41
00:02:46,928 --> 00:02:50,241
所以，到目前为止我还什么都没做。

42
00:02:50,241 --> 00:02:54,220
基本上, 我们绘制了上张幻灯片中的 RNN,

43
00:02:54,220 --> 00:02:57,675
但箭头放置在了有趣的位置。

44
00:02:57,675 --> 00:03:00,970
我把箭头的位置

45
00:03:00,970 --> 00:03:03,550
做了调整, 因为我们还要

46
00:03:03,550 --> 00:03:06,656
添加一个后向递归层。

47
00:03:06,656 --> 00:03:08,650
所以我们有 A1,

48
00:03:08,650 --> 00:03:12,994
左箭头表示这是一个后向连接,

49
00:03:12,994 --> 00:03:15,410
A2，向后，

50
00:03:15,410 --> 00:03:20,725
A3, 向后，

51
00:03:20,725 --> 00:03:25,730
A4，向后, 因此左箭头
表示它是后向连接的。

52
00:03:25,730 --> 00:03:32,780
因此, 我们将网络连接成如下。

53
00:03:32,780 --> 00:03:41,627
而这后向的连接将连接彼此，
向后传播。

54
00:03:41,627 --> 00:03:48,493
因此, 请注意, 
此网络定义了一个无循环图。

55
00:03:48,493 --> 00:03:51,472
因此, 给定一个输入序列, x1到 x4,

56
00:03:51,472 --> 00:03:55,310
前向序列将
首先计算A(前向）^1，

57
00:03:55,310 --> 00:03:57,116
然后用它计算A(前向）^2,

58
00:03:57,116 --> 00:03:59,240
然后 A(前向)^3, A(前向)^4,

59
00:03:59,240 --> 00:04:03,634
相反, 向后序列
将从计算A(后向)^ 4开始,

60
00:04:03,634 --> 00:04:06,160
然后向后计算A(向后)^3,

61
00:04:06,160 --> 00:04:08,645
然后当计算到激活层时,

62
00:04:08,645 --> 00:04:10,840
注意这整个过程不是反向传播, 而是前向传播。

63
00:04:10,840 --> 00:04:15,710
前向传播的关系图中有一部分

64
00:04:15,710 --> 00:04:17,810
计算是从左向右，

65
00:04:17,810 --> 00:04:20,875
有一部分是从右向左。

66
00:04:20,875 --> 00:04:23,061
计算了A(向后)^3以后,

67
00:04:23,061 --> 00:04:27,200
您可以使用这些激活部分
来计算A(向后)^2,

68
00:04:27,200 --> 00:04:30,580
然后 A(向后)^1 ,
然后计算所有的激活部分,

69
00:04:30,580 --> 00:04:34,290
然后你就可以做出预测了。

70
00:04:34,290 --> 00:04:36,050
比如

71
00:04:36,050 --> 00:04:37,590
为了做出预测,

72
00:04:37,590 --> 00:04:44,790
你的网络在时间 t ，
将得到 Y ^t，这是个对于W(y)来说的激活函数

73
00:04:44,790 --> 00:04:54,345
在时间 t，有前向激活函数,

74
00:04:54,345 --> 00:05:00,160
也有后向激活函数

75
00:05:00,160 --> 00:05:07,690
一起被喂入，来做预测。

76
00:05:07,690 --> 00:05:11,390
所以，举例一下
假设在时间设为3的位置上,

77
00:05:11,390 --> 00:05:15,910
然后从 X 得到的信息可以流到这里,

78
00:05:15,910 --> 00:05:18,270
从前向 1 到 2 ，

79
00:05:18,270 --> 00:05:24,140
激活函数中都有考虑,
再由前向 3 得到 Y ^3。

80
00:05:24,140 --> 00:05:27,150
所以 x ^1, x ^2,

81
00:05:27,150 --> 00:05:31,700
x ^3中的信息都被考虑了进去，
并且 x ^4中的信息能通过

82
00:05:31,700 --> 00:05:37,110
A(反向)^4 到 A(反向)^3 传入

83
00:05:37,110 --> 00:05:40,590
因此, 这允许在 t3 时刻的预测将

84
00:05:40,590 --> 00:05:44,925
过去的信息作为输入,

85
00:05:44,925 --> 00:05:47,430
以及目前的通过前向作为输入，以及

86
00:05:47,430 --> 00:05:49,935
后项的信息

87
00:05:49,935 --> 00:05:54,215
以及来自未来的信息同样作为输入。

88
00:05:54,215 --> 00:05:57,853
所以, 如果给出一个类似的短语, 
"他说,

89
00:05:57,853 --> 00:06:05,680
泰迪. 罗斯福...”来预测

90
00:06:05,680 --> 00:06:08,573
‘泰迪’是否是人名字的一部分,

91
00:06:08,573 --> 00:06:15,220
你会考虑过去和未来的信息。

92
00:06:15,220 --> 00:06:21,260
这就是双向递归神经网络（BRNN）

93
00:06:21,260 --> 00:06:24,530
这里可以不只是标准的 RNN 块,

94
00:06:24,530 --> 00:06:28,245
也可以是 GRU 块或 LSTM 块。

95
00:06:28,245 --> 00:06:30,670
事实上, 对于许多 NLP 问题,

96
00:06:30,670 --> 00:06:33,440
对于处理NLP问题的大量文本,

97
00:06:33,440 --> 00:06:40,520
带有 LSTM 的双向 RNN 
是非常常用的。

98
00:06:40,520 --> 00:06:45,100
所以, 我们有 NLP 问题, 你有完整的句子,

99
00:06:45,100 --> 00:06:47,000
你试着把句子里的东西标注出来,

100
00:06:47,000 --> 00:06:50,395
具有 LSTM 块的双向 RNN

101
00:06:50,395 --> 00:06:55,235
既向前又向后，这值得首先尝试。

102
00:06:55,235 --> 00:06:59,240
所以, 这是双向 RNN, 这是

103
00:06:59,240 --> 00:07:05,710
对基本的 RNN 结构
或 GRU 或 LSTM 进行的修改,

104
00:07:05,710 --> 00:07:07,910
通过做这个改变, 你可以用

105
00:07:07,910 --> 00:07:11,360
使用RNN 或 GRU 或 LSTM的模型

106
00:07:11,360 --> 00:07:14,640
预测任何地方, 甚至在一个序列的中间,

107
00:07:14,640 --> 00:07:18,830
利用来自整个序列的信息。

108
00:07:18,830 --> 00:07:22,640
双向 RNN 的缺点是

109
00:07:22,640 --> 00:07:26,750
需要整个数据序列, 然后才能在任何地方进行预测。

110
00:07:26,750 --> 00:07:30,170
例如, 如果要构建语音识别系统,

111
00:07:30,170 --> 00:07:33,140
然后 BRNN 会让你考虑

112
00:07:33,140 --> 00:07:37,595
整个演讲的内容,
 但如果你使用简单的前向实现,

113
00:07:37,595 --> 00:07:40,280
你需要等待人停止说话, 得到

114
00:07:40,280 --> 00:07:43,100
整个话语, 你才可以

115
00:07:43,100 --> 00:07:46,400
实际处理它，并进行语音识别预测。

116
00:07:46,400 --> 00:07:48,560
所以对于一个实时类型的语音识别应用程序,

117
00:07:48,560 --> 00:07:52,340
他们有更复杂的模块, 而不是仅仅

118
00:07:52,340 --> 00:07:56,730
使用标准的双向 RNN, 正如在这里看到的。

119
00:07:56,730 --> 00:07:59,510
但对于许多自然语言处理应用, 其中

120
00:07:59,510 --> 00:08:02,870
你可以同时得到整个句子,

121
00:08:02,870 --> 00:08:06,560
标准 BRNN 算法实际上是非常有效的。

122
00:08:06,560 --> 00:08:10,715
所以, 这是 BRNNs ，
在下一个也是本周最后的视频中,

123
00:08:10,715 --> 00:08:13,010
让我们来谈谈
如何利用 RNNs,

124
00:08:13,010 --> 00:08:19,000
LSTMs GRUs 和双向版本, 去构建它们的深层版本。