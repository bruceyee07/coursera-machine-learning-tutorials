在上一段影片中, 你學到了 GRU 即「門閘遞迴單元」 還有它如何學習連接在序列中相距很遠的字詞 另一個也能夠做到這樣事情的單元是 LSTM 即「長短期記憶單元」 甚至比 GRU 更為強大, 那我們就來看看 這些是上一段影片當中 GRU 的公式 對於 GRU 我們有 a<t> 等於 c<t> 另有兩個門閘, 分別是視覺門閘和相關門閘, 即 c~<t> 這是個取代記憶細胞的候選數值 然後使用更新門閘, Γᵤ 以決定是否用 c~<t> 去更新 c<t> LSTM 是比 GRU 更為強大而且通用的 多虧 Sepp Hochreiter 和 Jurgen Schmidhuber 是非常具有開創性的論文 對於序列模型具有深遠影響 但我認為這篇論文的閱讀難度是非常高的 它更聚焦在梯度消失的理論上 我想大部分讀懂 LSTM 的人
並非透過這篇論文來了解它的細節 而是來自於其他資源 但我仍認為它對於深度學習社群仍有重大影響 以下是 LSTM 主要的方程式 我們繼續來探討記憶細胞 c 還有它的更新候選數值, c~<t> 將會是這樣 注意到 LSTM 中 我們不再用到 a<t> 等於 c<t> 這是我們取而代之的 比較像左邊的式子 我們是用 a<t> 或者 a<t-1> 來取代 c<t-1> 我們也不再用到相關門閘, Γᵣ 當然你也可以使用 LSTM 的變異型
把這些功能都拿回來用 但這裡提供的是較為常見的 LSTM 版本 我們先只考慮這個 我們會像之前一樣有個更新門閘 裡面有表示更新的參數 Wᵤ
並使用 a<t-1>, x<t> 再加上 bᵤ LSTM 的一個新特性是 並非只用一個更新門閘來控制記憶過程 如這裡的兩個項 我們是用兩個不同來源的項 我們不再只用 Γᵤ 和 (1-Γᵤ) 而是在這裡用 Γᵤ 然後用遺忘門閘 Γf 那這個 Γf 門閘 用到了 sigmoid S型函數<br />其它和你之前看到的差不多 這裡是 x<t> 並加上 bf 然後我們會有一個新的輸出門閘 一樣是sigmoid S型函數, 用到 Wₒ 最後加上 bₒ 然後, 記憶細胞的更新值 c<t> 等於 Γᵤ 這個 ＊ 表示矩陣中逐元素的乘積 這是個向量間逐元素乘積 再加上, 取代 (1-Γᵤ) 用的是另一項遺忘門閘 Γf 乘以 c<t-1> 這樣給了記憶細胞選擇權 去決定使用多少舊數值 c<t-1> 並直接加上新的值 c~<t> 所以這裡是使用兩個分開的項, 更新門閘和遺忘門閘 這些代表更新門閘, 遺忘門閘, 還有輸出門閘 最後, 取代 GRU 中的 a<t>=c<t> a<t> 等於輸出門閘和 c<t> 去做逐元素乘積 那這就是 LSTM 的主要方程式 它用了三個門閘, 而並非兩個 並把三個門閘用到了不同的地方, 較為複雜 這裡再次列出統御 LSTM 行為的方程式 我們用圖片來解釋這些東西 像這裡的圖片 如果覺得這張圖很複雜, 請不要擔心 我自己也覺得從式子來看會比較好懂 但我還是用圖示來表達他想傳達的概念 這裡的圖片的靈感是來自 Chris Ola 的部落格文章 文章標題為「了解 LSTM 網路」 這裡的圖和它文章內的圖片很相似 但關鍵的不同點是 三個門閘輸出的值都是透過計算 a<t-1> 和 x<t> 而得 如這張圖中, 用 a<t-1> 和 x<t> 來計算遺忘門閘 更新門閘 還有輸出門閘 然後再透過 tanh 函數來計算 c~<t> 而這些值透過一些複雜的方式組合 如逐元素乘積等 從前一時序的 c<t-1> 去計算得到 c<t> 這裡有件事很有趣, 如果你將數個單元組合再一起 就像這個組合範例 是依照時間順序相連接 那這裡是輸入 x‹¹›, x‹²›, 和 x‹³› 可以將這些單元前後相互連接 前一單元的輸出 a 會成為下一單元的輸入 a 而 c 也是同理, 像下面這張圖, 是個簡化過的作法 有件有趣的事是 在上面這邊可以連成一條線 說明只要你能適當設置遺忘門閘和更新門閘的值 LSTM 是可以輕易做到 由左而右將 c<0> 的值一路傳遞下去 也許 c<3> 可以等於 c<0> 這就是為何 LSTM 和 GRU 擅於將資訊長久記憶的原因 即使經過多著時序, 它們仍可以將資訊記憶在細胞中 那這就是 LSTM 你可以想像 這裡和人們經常使用的 LSTM 仍有些微不同 最為常見的應該是 並非只由 a<t-1> 和 x<t> 來計算這些門閘的輸出值 在這裡也可能會去偷看一下 c<t-1> 的值 這被稱做「窺視孔連接」 並不是一個很棒的名稱, 但你可以看到 它代表這些門閘的輸出值並不單單取決於 a<t-1> 和 x<t> 同時會再去參考前一個記憶細胞的輸出值 那窺視孔連接就可以結合三個門閘的輸出值來做計算 那這是和常見的 LSTM 有所不同的地方 有個技術細節是, 假設這是個100維的向量 你有個100維的隱藏記憶細胞單元 比如說第五個 c<t-1> 只會去影響第五個細胞對應的那些門閘 所以他們的關係是一對一的 並不是100維中所有的 c<t-1> 都可以去影響所有的記憶細胞 第一個 c<t-1> 只會影響第一個記憶細胞 而第二個 c<t-1> 值也只會影響第二個記憶細胞, 依此類推 但如果你曾經讀過論文, 或和其他人談論過窺視孔連接 它代表門閘的輸出也會受到 c<t-1> 所影響 那這就是 LSTM 那何時該使用 GRU? 又何時該去使用 LSTM 呢? 這並沒有統一的說法 雖然我在課程中先講解 GRU 但是在深度學習的歷史中 LSTM 其實是更早出現的 GRU 是相對近期的研究 可能源自於 Pavia 對於複雜的 LSTM 做出的簡化 研究學者們已經嘗試過將這兩個模型<br />運用在各種不同的問題上 而在不同的問題中 會有不同模型勝出 在這兩者當中, 並沒有哪個是能將另一個比下去的 所以我將他們都一併呈現出來 但當我使用他們的時候 GRU 的優點就是它是個相對簡單的模型 所以更容易去建構出較大的網路 一個單元只有兩個門閘 所以計算速度也較快 對於擴大神經網路較為簡便 而 LSTM 表現更為強大而有效
因為它有三個門閘 如果你想要從中選擇一個使用 我想 LSTM 在過去表現上應該是更優先的選擇 所以若你要從中選擇 我相信人們會預設優先嘗試 LSTM 但是在過去幾年當中 GRU 也持續受到更多支持, 有越來越多團隊使用它 因為它的結構更簡單<br />而表現經常和 LSTM 不相上下 它也許也更能運用在規模較大的問題上 那以上就是 LSTM 的內容 不管是 GRU 或是 LSTM 都能夠被用於需要長期記憶的神經網路