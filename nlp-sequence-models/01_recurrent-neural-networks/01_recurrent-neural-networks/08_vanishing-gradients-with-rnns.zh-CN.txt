你已经学习了RNN如何运作 如何将其应用于识别人名之类的问题 还有语言建模问题， 并且你也看到了反向传播
在训练RNN中的使用 事实证明，
一个基本的 RNN算法的问题就是 它会产生梯度消失问题。 让我们来探讨这个问题，
然后在接下来几节视频中 我们谈谈解决这个问题的几种方法。 你已经看过这样的 RNN照片。 让我们以一个语言建模作为例子。 假设你看到这个句子， “the cat which already ate and 
maybe already ate a bunch of food
 that was delicious... ...was full" 所以，为了保持一致 因为 cat 是单数形式， 所以应该是‘cat was’ "the cats which already 
ate a bunch of food was delisious and apples, and pears and so on, were full" 所以，为了保持一致 应该是‘cats were’ 这是一个语言有长期依赖性的例子， 句子前面的部分可能会 影响句子后面的部分。 但是事实证明，
我们迄今为止遇到的基本的RNN 不擅长捕捉长期的依赖关系。 原因就是，你可能会记起 我们早期关于深层神经网络的讨论。 当时，我们谈到了梯度消失的问题 在非常深的神经网络情况下 100层甚至更深，你执行前向传播过程 从左到右，然后反向传播。 当时我们假设，如果这是一个非常深的神经网络 那么，从输出y 得到的梯度 很难反向传播去 影响前期层的权重， 很难影响前期层的计算。 对于RNN，有一个类似的问题， 你执行前向传播，从左到右， 然后反向传播， 从右到左。 这可能是相当困难的， 由于类似的梯度消失问题， 由于错误输出关系到 之后的步骤，
这影响前期的计算。 所以在实践中，这代表 很难去让一个神经网络意识到，它需要 记住看到了一个单数名词还是复数名词， 所以，随后在序列中决定生成‘was
’还是‘were’ 这取决于它是单数还是复数。 请注意，在英语中， 中间部分的长度是任意的，对吧？ 所以你可能需要记住单数/复数 很长一段时间后, 直到你可以使用
这一点信息为止。 因此, 由于这个问题, 基本 RNN 模型有附近效应, 这意味着，输出 y ^ <3>
主要受接近于 y ^ <3>的值的影响. 而那里的值主要受它附近的值影响。 这里的输出很难 被输入影响到, 
因为输入是非常早期的序列。 这是因为无论输出是什么, 不管这是否正确, 是否错误, 对于这个区域，很难做到 通过各种方式反向传播到序列的开始， 从而去修改神经网络 在序列前期做的计算。 所以这是基本 RNN 算法的一个缺点。 在下几个视频中还无法解决这个问题 但如果我们不解决它, 那么 RNN 往往不擅长捕获远程依赖关系。 尽管这个讨论集中在梯度消失上, 当我们谈到非常深的神经网络时 我们也谈到了爆炸梯度。 我们做反向传播过程， 梯度不只是指数递减, 它们也可能随着经历的层数指数增加。 事实证明, 梯度消失
往往是训练 RNNs 时更严重的问题, 尽管发生梯度爆炸时, 这可能是灾难性的, 因为 指数级大的梯度可能导致参数 变得很大，以至于
你的神经网络参数变得非常混乱。 事实证明, 爆炸梯度更容易被发现, 因为 参数只是爆炸的话, 
你可能会经常看到 NaN 或者非数字, 这意味着在你的神经网络
计算中数值溢出的结果。 如果你看到了爆炸的梯度, 一个解决方案是应用 gradient clipping 这到底是什么意思呢？ 就是看看你的梯度向量, 如果它大于某个临界值 重新缩放某些梯度向量, 使其不那么大 所以这是依据一些最大值的缩减 所以如果你看到了梯度爆炸, 如果你的导数爆炸或者你看到 NaN 只需应用 gradient clipping 这是一个相当好用的解决方案,
它将处理爆炸的梯度。 但是梯度消失更难解决 并且这将是接下来几个视频的主题 综上所述, 在早期的课程中, 你看到了如何训练非常深的神经网络, 利用导数，你可能会
陷入到梯度消失或爆炸的问题中, 要么呈指数递减, 要么 呈指数递增，成为关于层数的函数。 在 RNN 中, RNN 处理数据超过1000次集, 超过1万次集, 这基本上是使用1000层
或者1万层神经网络, 因此, 也会遇到这类问题。 梯度爆炸, 你可以使用
gradient clipping 解决， 但是我们需要更多操作
来解决梯度消失问题 所以我们在下一个视频中
所谈论的是 GRU, 称作 greater recurrent units 这是一个非常有效的方案, 用以解决 梯度消失问题, 并将允许 神经网络来捕获更长范围的依赖性 所以, 让我们继续下一个视频。