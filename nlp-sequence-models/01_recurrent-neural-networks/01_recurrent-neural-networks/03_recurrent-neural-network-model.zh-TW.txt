在上一段影片中，
您看過了我們在序列學習問題中將會使用到的符號 我們現在要談談如何建立一個模型 一個神經網路來將 x 對應到 y 你可以嘗試用一個標準的神經網路來做這項任務 所以, 在前面的例子中 我們有九個輸入單字 您可以想像試著將這九個輸入單字 將九個 one-hot 向量，
餵進一個標準神經網路 也許用一些隱藏層 最終會輸出 9 個數字 0 或者 1 告訴您是否每個字是
一個人的名字的一部分 但這實際上並不可行，
這樣做真的有兩個很重要的問題 首先，輸入跟輸出在不同的例子可能有不同長度 所以並不是所有例子的輸入長度都是Tx或 同樣的輸出長度 Ty, 如果每個句子有一個最大長度 或許您可以填入 0 到每個輸入使得每個輸入 長度都是最大長度，但這似乎不是一個好的表示法 第二個，或許是比較嚴重的問題是 像這樣單純的神經網路架構 它無法在不同位置的單字上共享一些學習的特徵 特別是當神經網路已經學到或許單字 哈利(Harry) 出現在一個位置上
已經給了一個提示這是一個人的名字一部分 如果它能自動地找出哈利(Harry) 出現在 其他位置 xt 時，也代表是一個人的名字，
這樣不是很棒嗎？ 這或許類似您在卷積神經網路見過 您想從一個影像其中一個部分學到的 能夠很快地應用到影像其他部分 我們在序列資料也想這樣做 類似您在卷積網路看到的 一個好的表示式會讓您
減少您模型裡大量的參數 前面談過，我們說每一個這些都是 10,000 維度 one-hot 向量，所以這是一個 相當大的輸入層 整個輸入大小會是最大數量的單字乘 10,000 而這第一層的權重矩陣
將會是非常龐大數量的參數 所以在我們即將描述的遞迴神經網路中 在下一個投影片中描述的，就不會有這些缺點 什麼是遞迴神經網路？ 我們來建立這個模型 當您從左到右讀一段句字 讀到的第一個單字假設是 X1 我們要做的是，拿第一個單字 餵進一個神經網路層 我這樣畫 這是第一個神經網路的隱藏層 我們可以試著讓這個神經網路預估一個輸出 預估這是否是一個人姓名的一部分 遞迴神經網路做的事是 當它繼續讀取句子中第二個字詞x^<2>時 除了只用 X^<2> 做為預估 y2 輸出外 它同時也將第一時間步計算的結果作為輸入 特別是，從第一時間步產生的啟動值也將傳遞到第二時間步 接著在下一時間步驟 遞迴神經網路輸入 第三個單字，試著輸出一些預估 y-hat<3> 等等 直到最後一步，輸入為 x<Tx> 輸出為 y-hat<Ty> 至少在這個例子 Tx 跟 Ty 相等，
當 Tx 跟 Ty 不同時，架構會稍許變更 在每個時間步驟 遞迴神經網路傳遞 啟動值給下一個時間步驟使用 要使這整件事情開始動起來 我們也需要一些手工打造的啟動值在第零時間步驟 通常使用零向量 有一些研究人員會使用隨機值當做 a_zero 您可以使用其他方式來初始 a_zero 
但說實在 使用零向量當做第零時間步驟的啟動值是最常見的選擇 所以使用這個輸入到神經網路中 有一些研究論文或一些書 您會見到這樣畫法的神經網路，如以下圖形 每個步驟您輸入 x 輸出 y-hat 假設這裡是 T索引，然後為了表示遞迴連結 有時候人們會畫一個返回箭頭像這樣 表示這一層會餵回同一個單元 有時候他們會畫一個實心方塊，畫一個實心方塊在這裡 來表示延遲一個時間步驟 我個人認為這樣的遞迴圖形 很難解釋，所以在這個課程 我會傾向於使用
像在左邊的未展開圖形 但如果您看到像右邊這樣的圖形 在課本或者研究論文上 它的意思是，或者說我試著將它想成 把它展開變成像左邊一樣的圖形 遞迴神經網路從左到右掃描資料 每次使用的參數都共享 所以會有一組的參數 我們將在下一張投影片詳細描述 但控制從 x1 連結到隱藏層的參數 會是一組我們寫為 Wax 的參數 同時也將會是同一組參數用在每一個時間步驟上 我想這裡也可以寫上Wax 啟動值的部分，水平連結的部分由 一組參數稱為 Waa 控制 同時也會是同樣的一組參數
使用在每一個時間步驟 同樣地，還有一些參數 Wya 來控制輸出預測 我將在下一張投影片中描述這些參數的運作方式 所以在這個遞迴神經網路 它的意思是當預估 y3 時 它不只使用 x3 的資訊，
同時也使用了 x1 跟 x2 的資訊 因為 x1 的資訊可以用這種方式傳遞，來幫助預估 y3 這樣的遞迴神經網路有一個缺點，它只使用 在序列前方的資訊來做預估 特別是，當預估 y3 時 它並不會使用到 x4 x5, x6 等等單字 這會是一個問題，因為如果您有一個句字 「他說：『泰迪羅斯福是個偉大的總統』」
(He said "Teddy Roosevelt was a great President") 要決定「泰迪(Teddy)」是否為人名的一部分 除了參考前面兩字的資訊 得到後面字詞的資訊也會相當有用 因為這句話也可能是 "他說, 泰迪熊, 正在特賣“ 
"He said teddy bears are on sale" 所以只看前三個單字是無法 確定 “泰迪” (Teddy) 這個單字 是人名一部分 在第一個例子，是如此 第二個例子則不是 但您無法只看前面三個單字來作區別 這是一種限制 在這樣的遞迴神經網路架構下，在某一個時間步驟做預估時 只使用序列前面的資訊來當做輸入 而不使用序列後面的資訊 我們會在後面的影片中解決這個問題 我們會談到的雙向遞迴神經網路 
(bi-directional recurrent neural networks) 但在目前的階段 這樣的簡單單向的神經網路架構 足夠讓我們來解釋主要的概念 到時我們只需要做一些快速的修改
來讓我們可以 做 y-hat<3> 的預估時
使用到兩邊的資訊 序列前面的資訊
跟序列後面的資訊 我們在後面的影片中會談到 現在讓我們來詳細寫下這個神經網路的計算 這個是清乾淨的神經網路圖形 正如我先前提到過的 您從輸入 a<0> 等於零向量開始 接下來, 他的正向傳播看起來像這樣 為了計算 a<1>您需要用一個啟動函數 g 來計算 Waa 乘 a0 加上 Wax 乘上 x1 加偏差值 (bias) 我將它寫為 ba 然後來計算 y-hat <1> 在第一時間步驟的預估值 那會是一些啟動函數 或許跟上面用的是
不同的啟動函數 應用到 Wya 乘 a<1> 加 by 我用的符號在下標的約定是 在矩陣中，像是 Wax 第二個索引在這個 Wax 指的是
這個將跟一些 x 相關的值做乘積 而下標的 a 代表這個計算跟 a 的值相關 同樣地，您注意到這個 Wya 將會跟一個 a 的值相乘，
來計算一個跟 y 有關的值 使用到的啟動函數來計算 啟動值通常在 RNN 中是使用 tanh 有時候也會使用 ReLU, 但 tanh 實際上 是相當常用的 我們還有其他方式來避免梯度消失問題 我們將在之後的課程談到 根據您的輸出 如果是二元分類問題 我猜您會使用 S型啟動函數 或者如果是 k 類分類問題可能使用 softmax 啟動函數的選擇
在於您輸出 y 的種類 所以對於人名辨識任務，y 為 0 或 1 我猜這第二個 g 函數會是 S型啟動函數 我想這裡您可以寫成 g2，
如果您想區別 這可以是不同的啟動函數，
但通常我不這麼做 更一般而言，在 t 時 a<t> 會是 g of Waa 乘 a 從前一個步驟來的，加上 Wax 乘目前的 x 加 ba 而 y-hat 等於 g 可以是不同的啟動函數， g of Wya 乘 a<t> 加 by 所以這個方程式定義了正向傳播 在神經網路上，
您會從 a<0> 是全為 0 的向量開始 然後使用 a<0> 跟 x<1> 您可以計算 a<1> 跟 y-hat<1> 然後您用 x<2> 跟 a<1> 來計算 a<2> 跟 y-hat<2> 等等，您繼續執行 正向傳播，從圖形的左邊到右邊 現在，為了幫助我們發展更複雜的神經網路 我會使用這些符號並將其些微簡化 讓我將這兩個方程式
複製到下一張投影片 我要做的是， 為了稍微簡化記號 我會用更間單的方式來表達 我會寫成 a<t> 等於 g of 一個矩陣 Wa 乘一個新的量，也就是 a<t-1>, x<t> 然後加上 ba 所以在左、右邊畫底線的部分，應該是相等的 而我們定義 Wa 的方式是，拿矩陣 Waa 跟這個矩陣 Wax 然後將他們並排放在一起 水平疊在一起如下 這就是矩陣 Wa 舉個例子，如果 a 是一個 100 維度 而在我們的例子中，x 是一個 10,000 維度 Waa 會是一個 100乘100 維度的矩陣 而 Wax 會是一個　100乘10,000 維度的矩陣 所以當我們將這兩個矩陣疊在一起 這個會是100 維度 這個會是 100 而這個會是 10,000 個元素 所以 Wa 會是一個 100乘10100 維度的矩陣 我想這個圖形有點不符比例 因為 Wax 會是一個很寬的矩陣 而這個符號的意義是 就只是拿這兩個向量疊在一起 當您使用這樣的符號時 我們將會用 a<t-1> 這個向量 所以這是個 100 維度，疊在 a<t> (應該是 x<t>) 上面 所以這會是一個 10,100 維度的向量 希望您自己檢查一下，這個矩陣 乘上這個向量，會得到跟原來一樣的量 因為現在，這個矩陣 Waa跟 Wax 乘上這個 a<t-1> 跟 x<t> 向量 這個就等於 Waa 乘 a<t-1> 加上 Wax 乘 x<t> 也正是跟我們原先這邊一樣 用這個符號的好處是
與其到處 使用兩個參數矩陣 Waa 跟 Wax 我們可以將它們壓縮成為一個參數矩陣 Wa 而這可以讓我們簡化記號，讓我們建立更複雜的模型 然後，同樣的方式 我將稍微重新寫一下 將寫為 Wy a<t> 加 by 我們現在也簡化了兩個下標為 Wy 跟 by 它標記為我們要計算的輸出量 所以 Wy 標示了一個權重矩陣，跟計算 y 相關 而上面這裡的 Wa 跟 ba  標示了 這個參數的計算跟 a 啟動輸出量相關 所以這就是基本的遞迴神經網路 接下來，我們來談談反向傳播
跟您如何讓這個 RNN 學習