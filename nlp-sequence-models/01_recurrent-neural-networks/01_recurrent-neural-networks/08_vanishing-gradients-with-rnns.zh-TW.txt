你已經了解RNN是如何運作的 也知道如何將其運用到具體問題上, 如實體識別問題 和語言模型 你也看到了如何將反向傳播運用於RNN訓練 但基本RNN演算法還有一個問題 那就是梯度消失問題 我們會在之後的影片中討論它 我們會討論一些解決這個問題的方法 你已經知道RNN的圖形就像這樣 現在我們將會舉個語言模型的例子 假設你看到這個句子 「那隻貓已經吃了很多好吃食物 等等...,然後吃飽了」 前後應該保持一致 因為貓是單數 所以應該是用單數的「是(was)」 「那隻貓已經吃了很多好吃的食物 有蘋果和梨子 ...等等, 然後吃飽了」 為了保持一致 單數的貓(cat)要用單數的是(was), 或複數的貓(cats)和是(were) 這是個範例, 說明句子中位置相差很大的字詞仍可能有相關 這裡很早出現的字詞 仍會對後面出現的字詞有影響 但我們目前見到的基本RNN模型 仍然不擅長捕捉這種長期依賴關係 這裡解釋一下, 你可能還記得 之前討論過在訓練很深的網路時 會有梯度消失的問題 所以這是一個非常深的網路 假設有100層或更多, 你會從左至右做正向傳播 然後再做反向傳播 我們曾說過, 如果這是很深的網路 那從輸出 y 得到的梯度 會非常難以做反向傳播 並進而難以影響前面幾層的權重 難以影響這裡的計算 對於有著類似問題的RNN 也是一樣要從左至右做正向傳播 然後再由右至左 做反向傳播 但反向傳播的進行較為困難 因為同樣有梯度消失問題 後面幾層輸出的誤差 很難影響前面幾層中的權重 它代表的是, 實際上 很難讓一個神經網路了解到它 需要去注意名詞是單數或者複數 然後在序列模型中根據它參考到的名詞 判斷是要生成 was 或者 were 而且在英文中 句子中間的這個部分其實可以是任意長度的 所以需要把名詞是單數抑或複數 記住相當長一段時間, 直到你需要用到這項資訊 因為這個問題 基本RNN模型會有很多在局部的影響 代表這裡的輸出 ŷ˂³˃ 主要是受到它附近的值所影響 而這裡的值則會被附近的輸入值所影響 這讓這裡的輸出很難受到 序列中較為前面的輸入所影響 這是因為不管輸出是甚麼 不管這是否正確 這裡的結果都很難透過反向傳播 去影響到較為前面的序列 進而去改進神經網路中 前面序列的計算 這是基本RNN演算法的弱點 我們會在接下來幾個影片當中處理這個問題 如果不處理這個問題, 那麼 RNN在這些具有長期依賴關係的問題
就沒辦法表現得很好 儘管我們這裡都是在討論梯度消失問題 你還記得我們也提到非常深層的神經網路 也有提到梯度爆炸的問題 當進行反向傳播時 梯度不僅可能呈現指數下降 當計算到越深層時, 也可能呈現指數上升 事實上, 當訓練RNN網路時, 儘管梯度消失是較為嚴重的問題 但梯度爆炸也是有可能發生 這可能使神經網路崩潰 因為指數項非常大的梯度可能會造成 參數也隨之變的非常大, 讓神經網路無法被使用 而梯度爆炸很容易被發現 因為你的參數可能會變成 NaN 或者是 顯示為非數字的情況 代表神經網路計算中出現數值溢位問題 如果你真的遇到了梯度爆炸 有可解決方法是運用梯度修剪(gradient clipping) 它代表的是 觀察你的梯度向量 如果它大於某個閾值 重新縮放梯度向量, 確保它不會太大 是修剪它最大值的方法 如果你遇到梯度爆炸 如果導數非常大或者出現NaN 那就使用梯度修剪 這是梯度爆炸問題的解決方法中, 較為確實好用的 但梯度消失就更難以解決 它會是之後幾個影片的主題 綜上所述, 在前面課程中 我們看到在非常深層的神經網路的訓練中 隨著層數越趨增加 你可能遇到梯度消指數上升而爆炸 或是指數下降而消失的問題 而RNN, 假使要處理含有1,000 甚至10,000個以上時間序列的數據集 那就至少需要1,000或10,000層的神經網路 也遇到了這些類型的問題 梯度爆炸問題可以直接使用梯度修剪方法來解決 但梯度消失就需要更多手續來處理 下一部影片中我們會提到GRU 即為「門閘遞迴單元」 這是個對於梯度消失問題 非常有效的方法 會讓你的神經網路能將資訊記的更為長久 讓我們進入下一段影片