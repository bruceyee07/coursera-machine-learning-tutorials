Önceki videoda GRU'ları, yani Geçitlenmiş Özyinelemeli Birim'leri (Gated Recurrent Unit) ve bunlar sayesinde bir dizi içerisinde nasıl uzun vadeli bağlantılar kurabileceğinizi öğrendiniz. Aynı şeyi yapmanıza izin veren bir diğer birim ise LSTM yani Uzun-Kısa Vadeli Bellek (Long Short-Term Memory) birimleridir. Hatta bunlar GRU'lardan daha da güçlüdür. Gelin inceleyelim. Önceki videoda GRU için yazdığımız denklemler bunlardı. GRU için a_t eşittir c_t demiştik. Ve iki geçit vardı, güncelleme geçidi ve ilgililik geçidi. Bellek hücresinin üstüne yazma adayı olan c_tilde_t. Güncelleme geçidi, gamma_u'yu ise c_t'yi c_tilde_t ile güncelleyip güncellemeyeceğimize karar veriyordu. LSTM, GRU'nun biraz daha güçlü ve daha genel bir versiyonu. Sepp Hochreiter and Jurgen Schmidhuber tarafından geliştirildi. Bu gerçekten çığır açıcı bir makaleydi ve Dizi modellemesi konusunda büyük etkisi oldu. Bence okuması biraz daha zor olan bir makale. Kaybolan gradyanların teorisine biraz fazla giriyor. Bu yüzden çoğu kişi LSTM'lerin detaylarını başka kaynaklardan öğrenmiştir diye düşünüyorum. Yine de bu makale Derin Öğrenme camiasında büyük etki yaratmıştır. LSTM'i kontrol eden denklemlere gelirsek. Bellek hücresine c ve güncelleme için kullanacağımız yeni bellek adayına c_tilde_t demeye devam ediyoruz. Şu şekilde tanımlıyoruz. Dikkat ederseniz LSTM için artık a_t eşittir c_t durumunu kullanmıyoruz. Artık bunu kullanıyoruz. Bu yine soldaki denkleme benziyor. Tek farkı artık a'yı kullanıyoruz. c_<t-1> yerine a_<t-1> yazdık. Ve artık bu gamayı yani bu ilgililik geçidini kullanmıyoruz. Aslında bunu ekleyebileceğiniz bir LSTM çeşidi de var ama daha sık kullanılan LSTM versiyonunda bununla ilgilenmiyoruz. Yine önceki gibi bir Güncelleme Geçidi olacak. Güncelleme ağırlıkları W_u. Burada a_t-1 kullanıyoruz. LSTM'in yeni bir özelliği, bu iki terimi de yöneten tek bir güncelleme geçidi yerine iki ayrı terimimiz olacak. Yani gamma_u ve 1-gamma_u yerine burada bir gamma_u olacak. Ve bir de gamma_f diyeceğimiz Unutma Geçidi (Forget Gate) olacak. Bu gamma_f geçidi, tahmin ettiğiniz gibi şunların sigmoidi olacak. x_t artı b_f Sonra bir de yeni Çıktı Geçidimiz olacak. Sigma içinde W_o, ve yine aynı şeyler. Artı b_o. Sonra belleğin güncelleme değeri. c_t, eşittir gamma_u yıldız burada her elemanı karşılıklı çarpmayı belirtiyor. Yani iki vektörün elemanlarını çarpıyoruz. Artı. 1-gamma_u yerine ayrı bir Unutma Geçidimiz var, gamma_f, çarpı c_t-1. Bu, Bellek hücresine eski c_t değerini tutma ve buna yeni değeri, c_tilde_t'yi ekleme imkanı veriyor. Bu yüzden ayrı güncelleme ve unutma geçitleri kullandık. Bunlar güncelleme, unutma ve çıktı geçitleri. Ve son olarak, a_t eşittir c_t yerine, a_t eşittir Çıktı Geçidiyle c_t'nin elemanlarının çarpımı. LSTM'in denklemleri işte bunlar. İki yerine üç tane geçit olduğunu görüyorsunuz. Biraz daha karmaşık ve geçitleri biraz farklı yerlere koyuyor. LSTM davranışını tanımlayan denklemler tekrar karşınızda. Bunları resimlerle açıklamak adettendir. Şuraya bir tane çizdim. Bu resim çok karmaşık göründüyse dert etmeyin. Ben şahsen denklemleri resimden daha anlaşılır buluyorum. Yine de bu resim sezgisel olarak daha iyi kavramanıza yardımcı olacaktır. Buradaki büyük resim Chris Ola'nın "Understanding LSTM Network" (LSTM Ağlarını Anlamak) adlı blog yazısından esinlendi. Bu diyagram o yazıdakinin bir benzeri. Ama bu resimden aklınızda kalmasını beklediğim bütün geçit değerlerinin a_t-1 ve x_t ile hesaplandığı. Bu resimde a_t-1 ve x_t bir araya gelerek unutma geçidini, güncelleme geçidini, ve çıktı geçidini hesaplıyor. Ayrıca tanh'ten geçerek c_tilde_t'yi de hesaplıyorlar. Ve bu değerler eleman boyunca çarpım gibi karmaşık yollarla birleşip bir önceki c_t-1'i c_t'ye dönüştürüyorlar. Şimdi bunlardan bir kaç tanesini paralel bağlayarak ilginç bir özelliğini görelim. Bunları teker teker alalım ve geçici olarak bağlayalım. Bu girdi x_1, sonra x_2 ve x_3. Sonra bu birimleri şu şekilde birleştirebilirsiniz. a'nın bir zaman dilimindeki çıktısı sonraki zaman dilimindeki girdisi. c için de aynı şekilde. Diyagramların alt kısmını biraz sadeleştirdim. Burada fark edebileceğiniz güzel bir özellik yukarıdaki şu çizgi boyunca unutma ve güncelleme geçitlerini doğru ayarladığınız sürece LSTM'in baştaki c_0'ın değerini en sağdaki c_3'e kadar değişmeden geçirmesi görece kolay. İşte bu yüzden LSTM'ler ve benzer şekilde GRU'lar da, bazı değerleri uzun süre hafızada tutma konusunda oldukça iyiler. Bazı gerçek değerler bellek hücrelerinde çok uzun süre saklanabilir. LSTM bu kadar. Tahmin edebileceğiniz gibi bunun insanların kullandığı birkaç varyasyonu daha var. Sanırım en yaygın değişiklik, geçit değerlerinin sadece a_t-1 ve x_t'ye bağlı olması yerine, bezen insanlar araya c_t-1 değerini de sıkıştırıyorlar. Buna gözetleme deliği bağlantısı deniyor. Süper bir isim değil belki ama gözetleme deliğinin anlamı, geçit değerleri sadece a_t-1 ve x_t'ye bağlı olmakla kalmıyor. Aynı zamanda önceki bellek hücresi değerine de bağlı. Bu gözetleme deliği bağlantısı her üç geçidin hesaplanmasında da kullanılabiliyor. Bu LSTM'lerde görebileceğiniz sık kullanılan bir varyasyon. Bir teknik detay ise şu. Bunlar diyelim 100 boyutlu vektörler olsun. Eğer 100 boyutlu bir gizli hafıza hücresi birimin varsa böyle oluyor. Ve c_t-1'in diyelim ki beşinci elemanı bağlı geçitlerin sadece beşinci elemanlarını etkiliyor. Yani bu ilişki bire bir. 100 elemanlı c_t-1'in her elemanı, geçitlerdeki her elemanı etkilemiyor. İlk eleman sadece ilk elemanı etkiliyor, ikinci eleman ikinciyi etkiliyor, bu şeklinde gidiyor. Eğer bir makale gözetleme deliği bağlantısından bahsedildiğini görürseniz, c_t-1'in geçit değerini etkilemek için kullanıldığını kastediyorlardır. LSTM için bu kadar. Ne zaman GRU kullanmalısız? Ve ne zaman LSTM kullanmalısınız? Bu konuda geniş tabanlı bir uzlaşma yok. Her ne kadar bu derste GRU'ları daha önce anlattıysam da derin öğrenme tarihinde LSTM'ler çok daha önce ortaya çıktı. GRU'lar görece daha yeni icatlar. Daha karmaşık olan LSTM modelini basitleştirme çabasının ürünü olması olası. Araştırmacılar her iki modeli de farklı problemlerde denediler ve farklı problemler için farklı algoritmalar kazandı. Yani evrensel olarak daha üstün bir algoritma yok. Bu yüzden size ikisini de göstermek istedim. Fakat bunları kullanırken benim hissetiğim, GRU'nun avantajı, daha basit bir model olması nedeniyle daha büyük bir ağ oluşturmanın daha kolay olması. Sadece iki geçidi var. Bu yüzden biraz daha hızlı çalışıyor. Daha büyük modeller için ölçeklenebiliyor. Fakat LSTM daha güçlü ve daha esnek çünkü iki yerine üç geçidi var. Birini seçmeniz gerekirse sanırım LSTM tarihsel olarak daha kendini ispat etmiş bir seçenek. Yani birini seçmek zorundaysanız sanırım çoğu kişi ilk denenecek şey olarak LSTM'i seçer. Yine de son birkaç yıldır GRU'lar büyük bir momentum kazanıyor ve bana öyle geliyor ki giderek daha fazla takım GRU'ları kullanıyor. Çünkü daha basitler ama çoğunlukla eşit derecede iyi çalışıyorlar. Ve daha büyük problemler için ölçeklenmeleri de daha kolay. LSTM'ler için bu kadar. GRU da olsa LSTM de olsa çok daha uzun bir aralıktaki bağlantıları yakalayan bir sinirsel ağ yapabilecek durumdasınız.