1
00:00:01,090 --> 00:00:06,380
在我们训练了一个序列模型以后，

2
00:00:06,380 --> 00:00:09,950
我们可以通过采样新的序列非正式地
了解它学到了什么

3
00:00:09,950 --> 00:00:11,330
让我们看看应该怎么做

4
00:00:12,520 --> 00:00:17,550
记住，一个序列模型模拟

5
00:00:17,550 --> 00:00:22,860
在任何序列模型中的词出现的机会如下，
我们现在想

6
00:00:22,860 --> 00:00:27,610
在这个分布中采样，以生成新词序列

7
00:00:28,980 --> 00:00:35,850
以上结构训练了这个网络

8
00:00:35,850 --> 00:00:40,830
但是采样方法略有不同

9
00:00:40,830 --> 00:00:45,790
首先为你希望模型生成的第一个字采样

10
00:00:45,790 --> 00:00:50,380
为此你输入x1=0，a0=0

11
00:00:50,380 --> 00:00:54,010
现在你的第一个时间标记

12
00:00:54,010 --> 00:00:58,720
有一些可能输出的最大概率

13
00:00:58,720 --> 00:01:05,690
然后根据softmax的分布随机采样

14
00:01:05,690 --> 00:01:09,740
softmax 分布会告诉你第一个词指向a的概率

15
00:01:09,740 --> 00:01:13,850
指向Aaron的概率

16
00:01:13,850 --> 00:01:16,570
指向Zulu的概率

17
00:01:16,570 --> 00:01:20,350
第一个单词是未知单词标记的几率是什么。

18
00:01:20,350 --> 00:01:23,840
也许这是一个句子标记的结束。

19
00:01:23,840 --> 00:01:29,418
然后你用这个向量使其比如运行numpy指令

20
00:01:29,418 --> 00:01:35,090
np.random.choice以采样

21
00:01:35,090 --> 00:01:40,850
通过这个向量概率定义的分布，
然后你就可以为这些首先出现的词采样了

22
00:01:40,850 --> 00:01:45,765
然后你继续，前进到第二步时间点

23
00:01:45,765 --> 00:01:53,690
记住第二步时间需要输入是这个y1

24
00:01:53,690 --> 00:01:58,660
现在你用刚刚采样过的y1

25
00:01:58,660 --> 00:02:02,718
在这里传递它到下一步时间点

26
00:02:02,718 --> 00:02:03,450
不管什么方式，管用就行

27
00:02:03,450 --> 00:02:08,570
你只是选择第一步，把这个输入传递到第二个位置

28
00:02:08,570 --> 00:02:14,227
然后softwax会预测y^2

29
00:02:14,227 --> 00:02:17,000
例如,,假设在你对第一个单词进行取样后,

30
00:02:17,000 --> 00:02:21,850
第一个词恰好是 z, 这个词作为第一个词很常见。

31
00:02:21,850 --> 00:02:26,112
然后你传递 v 作为 x2,

32
00:02:26,112 --> 00:02:32,420
x2现在等于 y^1。

33
00:02:32,420 --> 00:02:37,260
现在你正在试图找出

34
00:02:37,260 --> 00:02:40,500
当第一个词是d时，第二词的几率

35
00:02:40,500 --> 00:02:42,830
会是y^2

36
00:02:42,830 --> 00:02:48,610
然后你用同样的采样方法为y^2采样

37
00:02:48,610 --> 00:02:51,130
然后在下一个时间标记

38
00:02:51,130 --> 00:02:55,940
你把你有的选择当成硬编码

39
00:02:55,940 --> 00:03:00,220
然后传给下一个时间步骤

40
00:03:00,220 --> 00:03:05,480
然后你为第三个词取样，不管你选了什么

41
00:03:05,480 --> 00:03:08,990
你一直继续直到最后一个时间标记

42
00:03:08,990 --> 00:03:11,900
那你怎么知道序列结束了呢

43
00:03:11,900 --> 00:03:16,940
那么, 你可以一直采样直到你取得EOS标记

44
00:03:16,940 --> 00:03:21,970
如果标记的结尾是词汇的一部分

45
00:03:21,970 --> 00:03:25,140
这会告诉你句子结束了，你可以停止了

46
00:03:25,140 --> 00:03:28,650
或者，如果你的词汇里没有EOS标记

47
00:03:28,650 --> 00:03:33,520
你也可以只是去采20词或100词的样

48
00:03:33,520 --> 00:03:37,300
然后一直继续直到你时间步骤的上限

49
00:03:37,300 --> 00:03:43,260
这个特定的过程有时会产生一个未知词标记。

50
00:03:43,260 --> 00:03:47,710
如果要确保算法从不生成此标记,

51
00:03:47,710 --> 00:03:51,360
你可以做的一件事就是拒绝所有

52
00:03:51,360 --> 00:03:55,540
未知词标记样本，只从剩余词汇重新取样

53
00:03:55,540 --> 00:03:59,240
直到没有未知词为止

54
00:03:59,240 --> 00:04:02,860
或者你可以将其放入输出项中

55
00:04:02,860 --> 00:04:04,260
如果你不介意有个未知词输出

56
00:04:04,260 --> 00:04:08,580
这是如何生成一个随机选择的句子

57
00:04:08,580 --> 00:04:11,280
从你的 RNN 语言模型里。

58
00:04:11,280 --> 00:04:15,150
到目前为止,我们已经建立了一个字级 RNN,

59
00:04:15,150 --> 00:04:19,840
词汇来自英语。

60
00:04:19,840 --> 00:04:21,640
根据你的应用,

61
00:04:21,640 --> 00:04:26,070
你也可以建立一个字符级别 RNN。

62
00:04:26,070 --> 00:04:30,168
在这种情况下,你的词汇将只是字母。

63
00:04:30,168 --> 00:04:35,990
到 z, 也许还有空格

64
00:04:35,990 --> 00:04:40,820
标点符号,如果你需要的话,数字0到9。

65
00:04:40,820 --> 00:04:44,370
如果你想区分大写和小写,

66
00:04:44,370 --> 00:04:47,990
也可以包括大写字母以及

67
00:04:47,990 --> 00:04:52,800
你可以看看你的训练集和

68
00:04:52,800 --> 00:04:57,250
里面的字符,并使用它来定义词汇表。

69
00:04:57,250 --> 00:05:02,250
如果你建立字符级语言模型而不是词汇级的

70
00:05:02,250 --> 00:05:06,779
那么你的序列 y1, y2, y3,

71
00:05:06,779 --> 00:05:12,440
将是您的训练数据中的单个字符,

72
00:05:12,440 --> 00:05:15,430
而不是训练数据中的个别单词。

73
00:05:15,430 --> 00:05:22,950
像我们之前的例子一样,
句子“猫平均每天睡15小时”。

74
00:05:22,950 --> 00:05:27,160
在这个例子中, c 会是 y1, a 会是 y2,

75
00:05:27,160 --> 00:05:33,050
t 会是 y3, 空格会是 y4 等等。

76
00:05:33,050 --> 00:05:37,080
使用字符级语言模型有一些利弊。

77
00:05:37,080 --> 00:05:40,760
你不必担心未知的单词标记。

78
00:05:40,760 --> 00:05:44,246
字符级语言模型

79
00:05:44,246 --> 00:05:48,811
能够分配一个像mau一样的非零概率的序列。

80
00:05:48,811 --> 00:05:53,419
而如果mau不在你的词汇水平语言模型中

81
00:05:53,419 --> 00:05:56,451
您只需将它标为未知的单词标记。

82
00:05:56,451 --> 00:06:01,051
但字符级语言模型的主要缺点

83
00:06:01,051 --> 00:06:04,570
是你最终会有更长的序列。

84
00:06:04,570 --> 00:06:08,450
很多英语句子有10到20字，

85
00:06:08,450 --> 00:06:11,640
但可能有非常多的字符。

86
00:06:11,640 --> 00:06:16,850
因此,，字符语言模型不如词级语言模型好

87
00:06:16,850 --> 00:06:19,567
在找出句首对句尾的

88
00:06:19,567 --> 00:06:23,650
长距离的关系上

89
00:06:23,650 --> 00:06:28,730
字符级模型在计算训练上也更加复杂。

90
00:06:28,730 --> 00:06:32,500
所以我在自然语言处理中看到的趋势是,

91
00:06:32,500 --> 00:06:36,510
大多数情况下,还在使用字级语言模型

92
00:06:36,510 --> 00:06:41,710
但是随着计算机速度的加快,人们的应用越来越多,

93
00:06:41,710 --> 00:06:47,460
至少在某些特殊情况下,是字符级别模型。

94
00:06:47,460 --> 00:06:50,410
但它们往往需要大量的硬件和更复杂的计算训练,

95
00:06:50,410 --> 00:06:53,690
因此今天还没有被广泛使用。

96
00:06:53,690 --> 00:06:57,810
除非你可能需要特殊处理

97
00:06:57,810 --> 00:07:00,210
很多未知单词或其他词汇词的程序。

98
00:07:00,210 --> 00:07:02,340
或者,它们也被用于更专业的应用程序中

99
00:07:02,340 --> 00:07:06,460
这些程序需要更专业词汇。

100
00:07:06,460 --> 00:07:08,890
因此,有了这些方法,

101
00:07:08,890 --> 00:07:13,356
你现在能做的就是建立一个RNN
来观察英文文本

102
00:07:13,356 --> 00:07:18,682
构建单词级别和字符级别的模型

103
00:07:18,682 --> 00:07:23,610
在你培训的语言模型中采样。

104
00:07:23,610 --> 00:07:27,930
下面是一个语言模型中的文本示例,

105
00:07:27,930 --> 00:07:30,690
这是一个有文化级别的语言模型。

106
00:07:30,690 --> 00:07:34,080
你可以在练习中实现这样的东西。

107
00:07:34,080 --> 00:07:36,850
如果这个模型是在新闻文章上受训的,

108
00:07:36,850 --> 00:07:39,240
它会生成左边的文本。

109
00:07:39,240 --> 00:07:44,460
这看起来有点像新闻文本,
不是语法层面上来讲

110
00:07:44,460 --> 00:07:48,630
但也许听起来有点像新闻里的东西,

111
00:07:48,630 --> 00:07:50,490
比如这句
“concussion epidemic to be 
examined.”

112
00:07:50,490 --> 00:07:52,394
如果在莎士比亚的文本上训练

113
00:07:52,394 --> 00:07:55,750
然后它会写出像莎士比亚写出的东西。

114
00:07:55,750 --> 00:07:57,510
像是这句“终有一死的月亮
陷入爱河的月食”。

115
00:07:57,510 --> 00:08:00,100
“And subject of this thou
art another this fold.”

116
00:08:00,100 --> 00:08:02,210
“When besser be my love 
to me see sabl's.”

117
00:08:02,210 --> 00:08:04,910
"For whose are ruse of mine eyes 
heaves."

118
00:08:06,390 --> 00:08:11,470
这是基本的 RNN, 
如何用它建立一个语言模型,

119
00:08:11,470 --> 00:08:15,680
以及你所训练的语言模型中的样本。

120
00:08:15,680 --> 00:08:20,180
在接下来的几个视频中, 
我想进一步讨论一些训练RNN的挑战。

121
00:08:20,180 --> 00:08:24,908
以及如何应对这些挑战,尤其是

122
00:08:24,908 --> 00:08:28,780
通过建立更强大的 RNN 模型
使梯度消失

123
00:08:28,780 --> 00:08:32,760
所以在接下来的视频中
让我们来谈谈梯度消失的问题

124
00:08:32,760 --> 00:08:38,300
我们将继续谈论 GRU, 
门复发单元以及 LSTM 模型。