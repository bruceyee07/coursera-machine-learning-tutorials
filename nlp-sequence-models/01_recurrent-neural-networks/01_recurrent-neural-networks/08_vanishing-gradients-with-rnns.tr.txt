RNN'lerin nasıl çalıştıklarını öğrendiniz ve nasıl isim varlık tanıma gibi problemlere uygulanabileceklerini öğrendiniz, hem de dil modellemesi gibi, Ve RNN'de eğitmek için geri yayılımın nasıl kullanılabileceğini gördünüz. Temel bir RNN algoritması ile ilgili problemlerden biri kaybolan eğim problemleri karşılaşılmasıdır. Sonraki birkaç videolarda hadi bunu tartışalım, Bu sorunları çözmenize yardımcı olacak bazı problemler hakkında konuşacağız. Buna benzer RNN'nın fotoğraflarını gördünüz. Ve bir dil modelleme örneğini ele alalım. Diyelim ki bu cümleyi görüyorsunuz, "Yemek çoktan yemiş kedi .... ve belki de çoktan lezzetli olan birçok yemekleri yemiş nokta, nokta,nokta,nokta, doydu." Ve böylece, uygun olması için çünkü kedi(cat) tekildir, Kedi doydular yerine doydu olmalıdır. "Çoktan birçok lezzetli yemekler yiyen kediler, ve elmalar ve armutlar, ve benzeri, doydular" Yani tutarlı olması için Kedi doydu ya da kediler doydular. Ve bu dilin çok uzun süren şartlara sahip olabileceği örneğidir, Daha önce de çalıştığı yer olabilir cümlede daha sonra gelmesi gereken yerleri etkileyebilir. Ama şimdiye kadar gördüğümüz, ortaya çıkan temel RNN öyle değildir uzun süren şartları yakalamada çok iyi değidir. Nedenini açıklamak için, hatırlayabilirsiniz bizim önceden tartıştığımız çok derin yapay sinir ağları hatırlayabilirsiniz, Biz kaybolan eğimler problemi hakkında konuştuk. Demek ki bu çok, çok derin yapay sinir ağı olduğunu söyler, 100 katman ya da hatta daha derin ileri yayılım uygulayabilirsiniz, Soldan sağa ve sonuç geri yayılım. Ve biz buna dedik, eğer bu çok derin yapay sinir ağ ise, daha sonrasında eğim çıktı y tarafından geri yaymak daha zor zaman alır önceki katmanları ağırlıklarını etkiler, önceki katmalarda hesaplamaları etkilemek için. Ve RNN için benzer problem ile, Soldan sağa gelen ileri yayılım var, ve sonra geri yayılım, sağdan sola gider. Ve bu gerçekten zor olabilir, çünkü kaybolan eğilim problemi yüzünden, hataların çıktısı için bağlantılı daha sonraki zaman adımları önceki hesaplamalarımızı etkiler. Ve pratikte, bunun anlamı, bir sinir ağı elde etmek zor olabilir ezberlemesi gerektiğinin farkına varması için tekil isim ya da çoğul isim görmesi, böylece daha sonra dizide hem doydu hem doydular, tekil veya çoğul durumuna bağlıdır. Ve İngilizcede farket, Ortadaki cümle uzun olabilirdi değil mi? Bu yüzden tekil/çoğul ezberlemesi gerekebilir bilgiyi kullanmadan uzun süre önce. Bu problem yüzünden, Temel RNN modeli birçok yerel etkeni vardır, Bunun anlamı y^<3 çıktısı. y^<3>. değerine yakın değerler tarafından etkilenir Ve değer esas olarak buradaki yakın bir yerde olan girdilerden etkilenir. Ve buradaki çıktı için dizinin önceki bir girişinden etkinlenmesi zordur. Bunun sebebi çıktı ne olursa olsun, Bunun doğru olup olmadığı, Bu o alan için çok zordur dizinin başlangıcından beri geri yayılım, ve bu sebeple sinir ağını nasıl değiştirmek için önceki dizide hesaplamalar yapılıyor. Yani bu temel RNN algoritmasının zayıflığıdır. Bir sonraki birkaç videoda ele alınmadı. Ama biz bunu ele almazsak sonrasında RNN uzun vadeli bağımlılıkları yakalamada konusunda çok iyi olmama eğilimindedir. Ve bu tartışma kaybolan eğilime odaklansa bile, Hatırlayacaksınız, Biz derin yapay sinir ağı hakkında konuştuk Biz ayrıca patlayan eğimler hakkında konuştuk. Biz geri yayılım yapıyoruz, Eğim sadece üssel olarak azalmamalıdır, Üzerinde durduğumuz katman sayısı ile üssel olarak artabilir. Kaybolan eğimler daha büyük problem olmaya meğilli RNN'lerin eğitimlerinde ortaya çıktı. patlayan eğimler olmasına rağmen, felaket olabilir çünkü üstel olarak büyük eğimler değişkenlere neden olabilir değişkenler büyük olması için sizin yapay ağınızdaki parametreler kötü olabilir. Bu yüzden patlayan eğim fark etmenin daha kolay olduğu anlaşılıyor çünkü parametreler sadece hızlı bir şekilde ortaya çıktığı ve sık sık Nan'ler görüldüğüdür, ya da sadece numaralar değildir, senin yapay ağında hesaplamanının sayısal taşma sonuçlarının anlamıdır. Ve eğer patlayan eğimleri görüyorsanız, bir çözüm gradient kırpması uygulamaktır. Ve bunun anlamı şudur, bütün bunların anlamı eğim vektörlerine bakmaktır. ve eğer doyma bölgesinden daha yüksek ise, tekrar eğim vektörünü ölçekle böylece çok büyük olmaz. Bu yüzden maksimum değerlere göre kırpmalar vardır. Bu yüzden eğer patlayan eğim görüyorsanız, Eğer türevleriniz patlama yaparsa ya da Nan'ler görüyorsanız, sadece eğim kırpması uygula, ve bu patlayan eğimlere dikkat edecek, oldukça sağlam bir çözümdür. Ama kaybolan eğimleri çözmek çok daha zordur ve bu gelecek haftalardaki videoların konusu olacaktır. Özetlemek gerekirse, daha önceki bir kursta, Çok derin sinir ağlarının eğitiminin nasıl yapıldığını gördünüz, Siz kaybolan eğim ya da patlayan eğim problemleri türevi ile birlikte karşılaşabilirsiniz, ya katlanarak azalır ya da büyür üstel olarak katman sayısının bir fonksiyonu olarak. Ve RNN'de, bin defadan fazla RNN veri işleme kümesinde, 10.000'den fazla küme, bu temel olarak 1.000 katman ya da 10.000 katmanlı sinir ağına gidebilir, ve bu yüzden, bu tip problemler ile karışılaşılır. Patlayan eğimler, sadece eğim kırpması kullanarak adres sıralayabilirsiniz, ama kaybolan eğimleri ele almak için daha fazla çalışma yapacaktır. Bir sonraki videoda yapacağımız şey GRU hakkında konuşmak, daha büyük tekrarlayan birimler, adresleme için çok etkili bir çözümdür kaybolan eğim problemi ve buna izin verecek Sinir ağınız, daha uzun süren bağımlılıkları yakamak için. Öyleyse bir sonraki videoya geçelim.