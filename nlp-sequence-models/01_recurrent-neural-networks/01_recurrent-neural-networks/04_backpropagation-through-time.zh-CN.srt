1
00:00:00,000 --> 00:00:03,585
你已经学习了一些RNN的基础知识

2
00:00:03,585 --> 00:00:08,730
在这个视频中 我将介绍
反向传播在循环神经网络是如何工作的。

3
00:00:08,730 --> 00:00:12,570
像往常一样,<br /> 当您在一个编程框架中实现此方法时,

4
00:00:12,570 --> 00:00:16,848
通常，编程框架将自动处理反向。

5
00:00:16,848 --> 00:00:21,887
但我认为, 对 backprop 在 RNNs 的工作方式有一个粗略的了解还是有用的。

6
00:00:21,887 --> 00:00:23,575
让我们一起来看看让我们一起来看看

7
00:00:23,575 --> 00:00:25,719
你已经看到了反向传播如何工作,

8
00:00:25,719 --> 00:00:33,965
从左向右计算这些激活值, 如神经网络中所示,

9
00:00:33,965 --> 00:00:37,365
之后将会输出所有的预测值。

10
00:00:37,365 --> 00:00:40,605
在反向传播中, 你可能已经猜到了,

11
00:00:40,605 --> 00:00:42,375
你最后保留

12
00:00:42,375 --> 00:00:46,830
反向传播的值，在相反的方向

13
00:00:46,830 --> 00:00:49,071
如图中向前的箭头。

14
00:00:49,071 --> 00:00:52,815
让我们看一下前向传播计算的过程。

15
00:00:52,815 --> 00:00:56,120
得到这个输入序列 x_1, x_2,

16
00:00:56,120 --> 00:01:01,690
x_3, 到 x_tx。

17
00:01:01,690 --> 00:01:06,385
然后 x_1 激活值表示为 a_0

18
00:01:06,385 --> 00:01:11,390
计算激励值，和这个值相乘，

19
00:01:11,390 --> 00:01:17,490
然后, x_2 和 a_1 一起用来计算 a_2,

20
00:01:17,490 --> 00:01:24,853
然后 a_3, 直到 a_tx。

21
00:01:24,853 --> 00:01:27,753
好吧。实际上计算的是 a_1,

22
00:01:27,753 --> 00:01:29,760
您还需要参数。

23
00:01:29,760 --> 00:01:32,595
我们把这个画成绿色的 W_a 和 b_a

24
00:01:32,595 --> 00:01:35,570
这些是用于计算参数 a_1 的。

25
00:01:35,570 --> 00:01:39,950
然后, 这些参数实际上用于每一步计算

26
00:01:39,950 --> 00:01:45,035
这些参数实际上用于计算 a_2、a_3、

27
00:01:45,035 --> 00:01:52,329
等等, 直到最后一个，所有激活值的计算<br />都取决于参数 W_a 和 b_a。

28
00:01:52,329 --> 00:01:55,090
让我们继续充实这个图表。

29
00:01:55,090 --> 00:02:01,980
现在, 给定 a_1, 您的神经网络可以<br />计算第一个预测值, y-hat_1,

30
00:02:01,980 --> 00:02:07,340
然后计算第二步，<br />y-hat_2, y-hat_3,

31
00:02:07,340 --> 00:02:14,985
就这样下去, 得到hat_ty。

32
00:02:14,985 --> 00:02:18,600
再用一个不同颜色写一个参数。

33
00:02:18,600 --> 00:02:22,590
所以, 为了计算 y_hat,

34
00:02:22,590 --> 00:02:24,700
你需要参数，

35
00:02:24,700 --> 00:02:27,350
W_y 和 b_y,

36
00:02:27,350 --> 00:02:32,230
这一次进入这个节点以及其他所有节点。

37
00:02:32,230 --> 00:02:34,900
我也要用绿色画这个。

38
00:02:34,900 --> 00:02:36,670
接下来, 为了计算反向,

39
00:02:36,670 --> 00:02:39,361
你需要一个损失函数。

40
00:02:39,361 --> 00:02:42,370
因此, 让我们定义一个逐元素的损失值。

41
00:02:42,370 --> 00:02:45,625
假设序列中一个确定的个词。

42
00:02:45,625 --> 00:02:46,975
它是一个人的名字,

43
00:02:46,975 --> 00:02:48,220
所以 y_t 是1。

44
00:02:48,220 --> 00:02:51,395
你的神经网络输出是一写概率值，

45
00:02:51,395 --> 00:02:55,303
或许特定词是一个人的名字的概率是 0.1 。

46
00:02:55,303 --> 00:03:00,385
把它定义为标准的逻辑回归损失,

47
00:03:00,385 --> 00:03:04,045
也称为交叉熵损失。

48
00:03:04,045 --> 00:03:07,240
这可能要和我们以前看过的很眼熟。

49
00:03:07,240 --> 00:03:10,502
像是一个二分类问题。

50
00:03:10,502 --> 00:03:12,190
所以这个损失相关的是，

51
00:03:12,190 --> 00:03:15,671
单个位置或单个时间集的单个预测,

52
00:03:15,671 --> 00:03:17,661
对于一个字。

53
00:03:17,661 --> 00:03:21,700
现在让我们定义整个序列的整体损失,

54
00:03:21,700 --> 00:03:28,235
所以我将 L 定义为,

55
00:03:28,235 --> 00:03:29,940
t 从 i 到 T_x 或 T_Y 的求和。

56
00:03:29,940 --> 00:03:33,890
T_x 等于 T_y , 在这个例子中的损失,

57
00:03:33,890 --> 00:03:38,605
对于单个时间戳, y_t。

58
00:03:38,605 --> 00:03:42,712
然后, 值取 L 值，不需要它。

59
00:03:42,712 --> 00:03:47,575
上标 T。这是整个序列的损失。

60
00:03:47,575 --> 00:03:48,910
所以, 在一个计算图中,

61
00:03:48,910 --> 00:03:52,225
为了计算给定 y-hat_1 的损失,

62
00:03:52,225 --> 00:03:54,380
可以通过给定的第一个时间戳

63
00:03:54,380 --> 00:03:59,785
来计算出第二个时间戳的损失值。

64
00:03:59,785 --> 00:04:03,094
以及第三个时间戳的损失,

65
00:04:03,094 --> 00:04:07,265
一直到最后一个时间戳的损失。

66
00:04:07,265 --> 00:04:11,945
最后, 计算整体损失值,

67
00:04:11,945 --> 00:04:19,525
我们将把这些所有的计算值求和，<br />最终的 L 使用该方程,

68
00:04:19,525 --> 00:04:23,550
L是每一步时间戳的损失总和。

69
00:04:23,550 --> 00:04:25,975
所以, 这是一个计算问题，

70
00:04:25,975 --> 00:04:29,950
从以前见过的反向传播的例子来看,

71
00:04:29,950 --> 00:04:34,390
显而易见，反向传播要求

72
00:04:34,390 --> 00:04:39,880
在相反的方向进行计算和传递信息。

73
00:04:39,880 --> 00:04:43,700
所有的四个反向传播箭头,

74
00:04:43,700 --> 00:04:46,605
最终这样来处理。

75
00:04:46,605 --> 00:04:53,095
这么做可以计算所有适当的数量参数,

76
00:04:53,095 --> 00:04:55,705
然后，就像铆钉枪一样逐个计算所有参数。

77
00:04:55,705 --> 00:04:59,305
并使用梯度下降法来更新参数。

78
00:04:59,305 --> 00:05:02,070
在这个反向传播的过程中,

79
00:05:02,070 --> 00:05:08,575
最重要的信息传递和递归计算<br />是这一步,

80
00:05:08,575 --> 00:05:11,260
从右向左计算,

81
00:05:11,260 --> 00:05:13,600
这就是它提出反向传播算法的原因，

82
00:05:13,600 --> 00:05:17,380
一个非常快速的计算方法，<br />全名叫“基于时间的反向传播算法”。

83
00:05:17,380 --> 00:05:20,180
去这个名字的动机来源是“前馈传播”，

84
00:05:20,180 --> 00:05:22,305
从左向右扫描,

85
00:05:22,305 --> 00:05:25,075
增加的时间变量, t,

86
00:05:25,075 --> 00:05:27,695
与此相反，<br />反向传播从右向左计算,

87
00:05:27,695 --> 00:05:29,380
有点像是时间倒退的样子。

88
00:05:29,380 --> 00:05:31,870
所以收这些的启发,<br /> 我想起了一个很酷的名字,

89
00:05:31,870 --> 00:05:36,180
“基于时间的反向传播算法”，沿时间轴反向传播。

90
00:05:36,180 --> 00:05:40,039
这个命名听起来就像是<br />你需要一个时间机器来实现这个输出,

91
00:05:40,039 --> 00:05:42,460
但我，我觉得“基于时间的反向传播”

92
00:05:42,460 --> 00:05:45,360
只是算法中最酷的名字之一。

93
00:05:45,360 --> 00:05:50,890
我希望这些能给你一个关于<br /> RNN 前馈传播和反向传播的认识。

94
00:05:50,890 --> 00:05:54,766
到目前为止,<br /> 你只看到了 RNN 的一个主要的例子,

95
00:05:54,766 --> 00:06:00,220
输入序列的长度等于输出序列的长度。

96
00:06:00,220 --> 00:06:01,660
在下一视频中,

97
00:06:01,660 --> 00:06:06,033
我想向您展示更广泛的 RNN 体系结构,

98
00:06:06,033 --> 00:06:10,880
因此, 这将让您可以处理更广泛的应用集。<br />让我们继续下一堂课。