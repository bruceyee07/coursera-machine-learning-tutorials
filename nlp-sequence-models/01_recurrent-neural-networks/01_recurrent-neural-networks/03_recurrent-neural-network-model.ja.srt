1
00:00:00,450 --> 00:00:01,190
前回の動画では

2
00:00:01,190 --> 00:00:05,820
今後使っていく表現方法を学び
シーケンス学習問題を定義しました

3
00:00:05,820 --> 00:00:08,370
ではどうやって
ニューラルネットワークのモデルを構築して

4
00:00:08,370 --> 00:00:11,760
x から y へのマッピングを学習するか
について学んでいきましょう

5
00:00:11,760 --> 00:00:17,066
まず検討としてスタンダードなニューラルネットワークを
このタスクに試してみましょう

6
00:00:17,066 --> 00:00:22,425
前回の例では9個の入力語がありました
例えばこんなことを考えてみましょう

7
00:00:22,425 --> 00:00:27,697
その9個の入力語に対応した
9個のone-hotベクトルを受け取り

8
00:00:27,697 --> 00:00:33,079
スタンダードなニューラルネットワークに入力され
隠れ層をいくつか通って

9
00:00:33,079 --> 00:00:37,593
最終的には9個の0/1の値が
アウトプットとして出力されます

10
00:00:37,593 --> 00:00:41,282
それを見て
どの語が人名かを判断できるわけです

11
00:00:41,282 --> 00:00:43,460
しかしこれでは
うまくいかないことがわかります

12
00:00:43,460 --> 00:00:46,435
問題が大きく2つあります

13
00:00:46,435 --> 00:00:49,315
1つは 入力と出力は

14
00:00:49,315 --> 00:00:52,595
データによって長さが異なる
可能性があるということです

15
00:00:52,595 --> 00:00:57,840
つまり全てのデータにおいて
入力長 Tx と出力長 Ty が

16
00:00:57,840 --> 00:00:59,968
等しいと仮定することができません

17
00:00:59,968 --> 00:01:04,708
もし文の最大長がわかっていれば

18
00:01:04,708 --> 00:01:08,184
すべての入力に0パディングして
最大長に揃えるという手があるかもしれません

19
00:01:08,184 --> 00:01:11,765
しかしこれも
あまりいい方法ではなさそうです

20
00:01:11,765 --> 00:01:14,025
2つ目の問題は
こちらのほうがより深刻かもしれませんが

21
00:01:14,025 --> 00:01:17,915
こういった単純な
ニューラルネットワークの構造だと

22
00:01:17,915 --> 00:01:22,185
文の色々な場所で学習した特徴量を
共有しません

23
00:01:22,185 --> 00:01:26,766
つまり もしニューラルネットワークが
1番目に登場した単語 "Harry" が

24
00:01:26,766 --> 00:01:31,997
人名の一部である
と判断するよう学習したならば

25
00:01:31,997 --> 00:01:36,290
別の場所 x〈t〉に登場した "Harry" も

26
00:01:36,290 --> 00:01:41,579
たぶん人名だろうと自動的に
判断してくれたら良さそうですよね

27
00:01:41,579 --> 00:01:46,276
これは畳み込みニューラルネットワークのところで
学んだことに似ているかもしれません

28
00:01:46,276 --> 00:01:48,398
画像のある一部分で学習されたことが

29
00:01:48,398 --> 00:01:53,320
一般化されて画像の他の部分にも
そのまま適用されるような仕組みです

30
00:01:53,320 --> 00:01:57,840
同じような効果がシーケンスデータにも
欲しいわけです

31
00:01:57,840 --> 00:02:02,497
またこれも畳み込みネットで
学んだことですが

32
00:02:02,497 --> 00:02:06,567
良い表現を用いることで
モデル内のパラメタ数を減らすことができます

33
00:02:06,567 --> 00:02:12,065
この例において
これら入力の1つ1つは

34
00:02:12,065 --> 00:02:16,825
1万次元のone-hotベクトルであり
非常に大きな入力層です

35
00:02:16,825 --> 00:02:22,418
もし入力サイズがトータルで
最大語長×10,000だとすると

36
00:02:22,418 --> 00:02:28,290
第1層の重み行列は とんでもない数の
パラメタを持つことになってしまいます

37
00:02:28,290 --> 00:02:33,150
そこで再帰型ニューラルネットワークは
次のスライドから説明していきますが

38
00:02:33,150 --> 00:02:37,010
このどちらの欠点もありません

39
00:02:37,010 --> 00:02:40,346
それでは
再帰型ニューラルネットワークとは何でしょうか

40
00:02:40,346 --> 00:02:46,348
1つ作ってみましょう
今 左から右へ文を読んでいるとします

41
00:02:46,348 --> 00:02:50,738
出てきた一番最初の単語を
x〈1〉としましょう

42
00:02:50,738 --> 00:02:53,951
何をするのかというと
その最初の単語を取って

43
00:02:53,951 --> 00:02:56,935
ニューラルネットワーク層に与えます

44
00:02:56,935 --> 00:03:02,645
図に描きましょう
これが最初のニューラルネットワークの隠れ層です

45
00:03:02,645 --> 00:03:07,358
出力を予測するような
ニューラルネットワークがあってもいいですね

46
00:03:07,358 --> 00:03:10,210
つまり「これは人名か否か？」
ということですね

47
00:03:10,210 --> 00:03:14,607
再帰型ニューラルネットワークが
何をしているのかというと

48
00:03:14,607 --> 00:03:20,392
続いて文中の2番目の単語
x〈2〉が入力された時

49
00:03:20,392 --> 00:03:25,946
x〈2〉だけを使って y〈2〉を予測するのではなく

50
00:03:25,946 --> 00:03:33,957
タイムステップ1で計算されたものから
何かしら情報を加えるということをします

51
00:03:33,957 --> 00:03:35,216
具体的に言うと

52
00:03:35,216 --> 00:03:40,675
タイムステップ1からアクティベーション値（a〈1〉）が
タイムステップ2に渡されます

53
00:03:40,675 --> 00:03:46,292
そして次のタイムステップでは

54
00:03:46,292 --> 00:03:52,731
再帰型ニューラルネットワークは
3番目の単語 x〈3〉を入力して

55
00:03:52,731 --> 00:03:57,804
予測値 y^〈3〉を出力します
その後も同様に

56
00:04:00,920 --> 00:04:07,134
最終ステップまで続きます<tx>

57
00:04:07,134 --> 00:04:13,346
入力は x〈Tx〉出力は y^〈Ty〉です

58
00:04:13,346 --> 00:04:18,731
この例では Tx = Ty となっていますが

59
00:04:18,731 --> 00:04:21,380
等しくない場合は
アーキテクチャが少し変わります

60
00:04:22,520 --> 00:04:27,029
各タイムステップで
再帰型ニューラルネットワークは

61
00:04:27,029 --> 00:04:30,550
アクティベーション値を
次のタイムステップで使えるよう渡します

62
00:04:31,650 --> 00:04:34,090
このプロセスの起点には

63
00:04:34,090 --> 00:04:38,060
タイム0のアクティベーション値を
用意しておく必要があります

64
00:04:38,060 --> 00:04:41,250
普通は要素0のベクトルを使いますが

65
00:04:41,250 --> 00:04:44,679
a〈0〉を乱数で初期化する研究者もいますし

66
00:04:44,679 --> 00:04:47,382
他にも a〈0〉を初期化する方法はあります

67
00:04:47,382 --> 00:04:52,230
実際には タイム0の疑似アクティベーション値として

68
00:04:52,230 --> 00:04:56,710
要素0のベクトルを使うのが依然最も一般的ですので
それをニューラルネットワークの入力として使いましょう

69
00:04:56,710 --> 00:05:00,800
論文や書籍によっては

70
00:05:00,800 --> 00:05:05,170
この種のニューラルネットワークを
次のようなダイアグラムで記述している事があります

71
00:05:05,170 --> 00:05:09,840
全てのタイムステップで
x を入力し y を出力します

72
00:05:10,840 --> 00:05:14,720
インデックス〈t〉がついているかもしれませんね

73
00:05:14,720 --> 00:05:19,730
そして再帰型の接続を表現するために
このようなループを描くことがあります

74
00:05:19,730 --> 00:05:25,040
自身へのフィードバックを表しています
黒塗りの四角が描かれることもあり

75
00:05:25,040 --> 00:05:30,006
タイムステップが1遅れることが表現されています

76
00:05:30,006 --> 00:05:34,790
個人的にはこの再帰型ダイアグラムは
解釈がしにくいので

77
00:05:34,790 --> 00:05:39,590
このコースでは左側に描いてあるような
展開したダイアグラムを使うことにします

78
00:05:39,590 --> 00:05:42,840
教科書や研究論文によっては
右のようなダイアグラムを見ることがありますが

79
00:05:42,840 --> 00:05:46,360
何を意味しているのか考えるときは

80
00:05:46,360 --> 00:05:50,830
心のなかで左のようなダイアグラムに
展開して考えるようにしています

81
00:05:50,830 --> 00:05:55,620
再帰型ニューラルネットワークは
データを左から右へスキャンします

82
00:05:55,620 --> 00:05:59,530
そして各タイムステップで使われるパラメタは
共有されます

83
00:05:59,530 --> 00:06:03,540
次のスライドでもっと詳しく説明しますが

84
00:06:03,540 --> 00:06:04,770
幾つかのパラメタのセットがあります

85
00:06:04,770 --> 00:06:09,050
x〈1〉から隠れ層への接続を制御している
パラメタがありますが

86
00:06:09,050 --> 00:06:13,050
これもパラメタのセットで
Waxと書くことにします

87
00:06:13,050 --> 00:06:18,424
全タイムステップで
同じパラメタWaxが使われます

88
00:06:18,424 --> 00:06:21,850
右のダイアグラムでもWaxと書いていいでしょう

89
00:06:21,850 --> 00:06:26,910
そして アクティベーション値の接続
つまり横方向の接続も

90
00:06:26,910 --> 00:06:29,980
別のパラメタのセットWaaで制御されます

91
00:06:29,980 --> 00:06:35,220
これも どのタイムステップにおいても
同じパラメタWaaが使われます

92
00:06:35,220 --> 00:06:42,475
同様に 出力値つまり予測値を制御するWyaがあります

93
00:06:44,559 --> 00:06:47,820
次のスライドでこれらのパラメタがどのように働くのか
きちんと説明しましょう

94
00:06:48,860 --> 00:06:53,355
この再帰型ニューラルネットワークで
これがどういうことを意味するのかというと

95
00:06:53,355 --> 00:06:58,913
y〈3〉予想する時
ネットワークはx〈3〉だけではなく

96
00:06:58,913 --> 00:07:03,736
x〈1〉や x〈2〉からも情報を受け取っています
なぜかというとx〈1〉の情報は

97
00:07:03,736 --> 00:07:08,021
このルートを通って y〈3〉の予測に寄与出来るからです

98
00:07:08,021 --> 00:07:13,148
RNN (再帰型ニューラルネットワーク) の弱点は
予測をするときに

99
00:07:13,148 --> 00:07:18,616
シーケンス内の早い方の
情報しか使わないということです

100
00:07:18,616 --> 00:07:24,019
例えば y〈3〉の予測をする時
x〈4〉 x〈5〉 x〈6〉 ・・・の情報は使いません

101
00:07:24,019 --> 00:07:28,430
なぜこれが問題になるのかというと
次のような文が与えられたとき

102
00:07:28,430 --> 00:07:32,417
（彼は「テディ・ルーズベルトは偉大な大統領」だと言った）

103
00:07:32,417 --> 00:07:36,512
"Teddy" が人名の一部かどうかを
判断するのに

104
00:07:36,512 --> 00:07:40,932
最初の2単語の情報だけでなく

105
00:07:40,932 --> 00:07:44,920
その後の単語の情報も知っていると
役に立つからです

106
00:07:44,920 --> 00:07:49,540
というのも こんな文もあり得るからです
（彼は「テディベア セール中！」と言った）

107
00:07:49,540 --> 00:07:53,470
最初の3単語を与えられただけでは

108
00:07:53,470 --> 00:07:57,160
"Teddy" が人名かどうか
自身を持って判断することができません

109
00:07:57,160 --> 00:08:00,390
1つ目の文では人名ですが
2つ目の文では人名ではありません

110
00:08:00,390 --> 00:08:05,890
しかし最初の3単語を見ただけでは
その違いがわかりません

111
00:08:05,890 --> 00:08:09,900
この種のニューラルネットワーク構造における
弱点の一つです

112
00:08:09,900 --> 00:08:15,000
ある時点での予想は 入力データのうち

113
00:08:15,000 --> 00:08:19,690
シーケンスの早い方の情報は使われますが
後ろの方は使われないのです

114
00:08:19,690 --> 00:08:24,450
この問題はもっと後のビデオで扱います

115
00:08:24,450 --> 00:08:29,220
双方向再帰型ニューラルネットワーク (BRNN)
と呼ばれるものです

116
00:08:29,220 --> 00:08:33,495
現時点ではよりシンプルな
この単方向のニューラルネットワークで

117
00:08:33,495 --> 00:08:38,670
充分キーコンセプトを説明出来ると思います

118
00:08:38,670 --> 00:08:41,830
後ほど簡単な修正を加えて

119
00:08:41,830 --> 00:08:46,110
例えば y^〈3〉の予測をするときに

120
00:08:46,110 --> 00:08:49,790
シーケンス内の前後の情報を
両方使えるようにします

121
00:08:49,790 --> 00:08:51,280
後ほど詳しく見ていきます

122
00:08:52,390 --> 00:08:55,965
ではこのニューラルネットワークがやっている計算を
詳しく書いていきましょう

123
00:08:55,965 --> 00:08:56,680
 

124
00:08:57,892 --> 00:09:02,601
こちらが先程のニューラルネットワークを
キレイに書き直したものです

125
00:09:02,601 --> 00:09:05,376
先に説明したとおり 大抵の場合

126
00:09:05,376 --> 00:09:10,093
要素が全て0のベクトル a〈0〉から始めます

127
00:09:10,093 --> 00:09:13,924
次に順伝播がどのようになっているか
お見せします

128
00:09:13,924 --> 00:09:20,082
a〈1〉を計算するには
活性化関数 g に次のようなものを入れます

129
00:09:20,082 --> 00:09:26,763
Waa 掛ける a〈0〉

130
00:09:26,763 --> 00:09:30,693
足す Wax 掛ける x〈1〉

131
00:09:30,693 --> 00:09:36,851
足すバイアス
ここでは ba と書きましょう

132
00:09:36,851 --> 00:09:42,093
そして y^〈1〉
つまり時間1での予測値を計算します

133
00:09:42,093 --> 00:09:48,277
活性化関数 おそらくさっきのとは
違う活性化関数に次のものを入れます

134
00:09:48,277 --> 00:09:53,353
Wya 掛ける a〈1〉

135
00:09:53,353 --> 00:09:58,429
足す by

136
00:09:58,429 --> 00:10:06,230
ちなみに 表記上の習慣として
Waxなどの行列に使われている下付き文字について

137
00:10:06,230 --> 00:10:10,200
2番目の文字 x は Wax が
x 関連の数に掛けられる

138
00:10:10,200 --> 00:10:12,220
ということを意味し

139
00:10:12,220 --> 00:10:18,290
1番目の文字 a は Wax が
a 関連の数を算出するということを意味します

140
00:10:18,290 --> 00:10:23,980
同様に もうおわかりかと思いますが
Wya は

141
00:10:23,980 --> 00:10:29,730
a 関連の数に掛けられ y 関連の数を算出する
ということになります

142
00:10:29,730 --> 00:10:37,130
アクティベーション値を計算するのに使われる
活性化関数としては

143
00:10:37,130 --> 00:10:42,558
RNN では tanh をよく見かけます
ReLU が使われることもありますが

144
00:10:42,558 --> 00:10:48,630
実際のところ tanh が主流です

145
00:10:48,630 --> 00:10:53,840
また勾配消失問題については
別の対策を行います

146
00:10:53,840 --> 00:10:56,420
この週の後半にご説明します

147
00:10:56,420 --> 00:11:00,900
また アウトプット y の種類によって

148
00:11:00,900 --> 00:11:05,520
もし2クラス分類問題であれば
シグモイド活性化関数を使うでしょうし

149
00:11:05,520 --> 00:11:09,390
kクラス分類問題であれば
ソフトマックスを使うかもしれません

150
00:11:09,390 --> 00:11:14,600
いずれにせよ ここでの活性化関数の選択は
どういう種の出力 y を考えているかによって

151
00:11:14,600 --> 00:11:15,390
異なります

152
00:11:15,390 --> 00:11:19,340
固有表現抽出タスクであれば
ここで y は0か1としますが

153
00:11:19,340 --> 00:11:23,240
2つ目の g は
シグモイド活性化関数になるでしょう

154
00:11:24,730 --> 00:11:28,807
また これらは違う活性化関数であると
明確に区別したければ

155
00:11:28,807 --> 00:11:32,730
g1 g2 などと書くことも出来るのですが
私は普通そのようにはしません

156
00:11:32,730 --> 00:11:38,002
またより一般的に 時間 t において

157
00:11:38,002 --> 00:11:43,954
a〈t〉は g の Waa 掛ける
一つ前のタイムステップの a〈t-1〉

158
00:11:43,954 --> 00:11:49,395
足す Wax 掛ける 現在のタイムステップの x〈t〉
足す ba

159
00:11:49,395 --> 00:11:54,009
また y^〈t〉は 活性化関数 g

160
00:11:54,009 --> 00:11:57,828
同じ活性化関数とは限らないですが
 g の Wya 掛ける a〈t〉

161
00:11:59,099 --> 00:12:03,910
足す by となります

162
00:12:03,910 --> 00:12:08,960
これらの数式がこのニューラルネットワークにおける
順伝播の定義です

163
00:12:08,960 --> 00:12:14,603
要素が全て0のベクトル a〈0〉を起点に
a〈0〉, x〈1〉を使って

164
00:12:14,603 --> 00:12:19,355
a〈1〉, y^〈1〉を計算し
その後 x〈2〉を受け取って

165
00:12:19,355 --> 00:12:25,417
x〈2〉, a〈1〉を使って a〈2〉, y^〈2〉を計算し

166
00:12:25,417 --> 00:12:29,722
・・・とこのように図の左から右へ進みながら
順伝播を行っていきます

167
00:12:29,722 --> 00:12:30,857
・・・とこのように図の左から右へ進みながら
順伝播を行っていきます

168
00:12:30,857 --> 00:12:34,577
さてより複雑なニューラルネットワークを
楽に作れるよう

169
00:12:34,577 --> 00:12:39,187
この表記方法を少し単純化することにします

170
00:12:39,187 --> 00:12:41,897
この2つの数式を次のスライドにコピーしましょう

171
00:12:43,820 --> 00:12:44,630
こちらです

172
00:12:45,930 --> 00:12:50,710
さて実際に単純化していくわけですが

173
00:12:50,710 --> 00:12:55,840
この部分を少しシンプルに
書いていきます

174
00:12:55,840 --> 00:13:01,341
a〈t〉イコール g の

175
00:13:01,341 --> 00:13:06,845
行列 Wa 掛ける 新たな数

176
00:13:06,845 --> 00:13:12,197
a〈t-1〉, x〈t〉

177
00:13:12,197 --> 00:13:17,552
足す ba

178
00:13:17,552 --> 00:13:22,340
下線を引いた部分が等しい部分です

179
00:13:23,340 --> 00:13:28,070
Wa は次のように定義できます

180
00:13:28,070 --> 00:13:33,120
行列 Waa と Wax を取り出して横に並べ

181
00:13:33,120 --> 00:13:39,190
こんな風に水平に積んでいる感じでしょうか
これを行列 Wa とします

182
00:13:41,360 --> 00:13:46,840
例えば a が100次元だったとします

183
00:13:47,960 --> 00:13:53,520
また x〈t〉を10000次元だとします

184
00:13:53,520 --> 00:13:59,104
すると Waa は 100×100 次元の行列

185
00:13:59,104 --> 00:14:03,760
Wax は 100×10000 次元の行列となります

186
00:14:03,760 --> 00:14:09,570
この2つの行列をくっつけると
縦が100次元

187
00:14:09,570 --> 00:14:14,195
横方向左側が100次元
右側が10000次元ですので

188
00:14:14,195 --> 00:14:20,170
Wa は 100×10100 次元の行列となります

189
00:14:20,170 --> 00:14:27,070
この図だとスケールどおりでないですね
Wax は非常に幅が広い行列ですので

190
00:14:28,420 --> 00:14:31,767
またこっちの表記がどういう意味かというと

191
00:14:31,767 --> 00:14:35,080
単に2つのベクトルを持ってきて

192
00:14:35,080 --> 00:14:40,910
くっつけただけです
この表記が表しているのは

193
00:14:40,910 --> 00:14:42,330
a〈t-1〉

194
00:14:42,330 --> 00:14:46,981
100次元のベクトルでしたね
それを

195
00:14:46,981 --> 00:14:50,468
x〈t〉の上に積みます
すると

196
00:14:50,468 --> 00:14:55,752
10100次元ベクトルが出来上がります

197
00:14:55,752 --> 00:15:00,952
ぜひご自身でチェックしてほしいのですが
この行列に

198
00:15:00,952 --> 00:15:06,402
このベクトルを掛ければ
元と同じものが得られます

199
00:15:06,402 --> 00:15:11,765
なぜなら行列 Waa Wax に

200
00:15:11,765 --> 00:15:17,717
a〈t-1〉 x〈t〉ベクトルを掛けると

201
00:15:17,717 --> 00:15:23,669
これは単に
Waa 掛ける a〈t-1〉

202
00:15:23,669 --> 00:15:31,659
足す Wax 掛ける x〈t〉に等しくなります
これはまさにこの元々の部分に等しいわけです

203
00:15:31,659 --> 00:15:37,974
この表記のいいところは
2つのパラメタ行列を連れ回さなくて済むことです

204
00:15:37,974 --> 00:15:43,910
Waa Wax を一つのパラメタ行列 Wa に
詰め込んでいるわけです

205
00:15:43,910 --> 00:15:49,692
これによって 複雑なモデルを作るときにも
シンプルな表現にすることができます

206
00:15:49,692 --> 00:15:54,514
同様にこちらも少し書き直して

207
00:15:54,514 --> 00:16:00,298
Wy a〈t〉プラス by
としましょう

208
00:16:00,298 --> 00:16:05,503
これで Wy by の下付き文字が

209
00:16:05,503 --> 00:16:10,383
何の値を算出しているかを
表すだけになりました

210
00:16:10,383 --> 00:16:14,760
つまり Wy はy関連の値を算出するための
重み行列であることを表しており

211
00:16:14,760 --> 00:16:19,330
Wa や ba は
a つまりアクティベーション関連の

212
00:16:19,330 --> 00:16:24,030
値を算出するための
パラメタであることを表しています

213
00:16:24,030 --> 00:16:28,200
以上で 基本的な再帰型ニューラルネットワークが
どんなものか分かったと思います

214
00:16:28,200 --> 00:16:32,480
次はRNNにおける逆伝播と
その学習方法についてお話しましょう

215
00:16:32,480 --> 00:16:36,500
 