你现在已经学习了RNN的基本结构 在这个视频中，你将会看到循环神经网络是如何反向传播的 通常情况下，当你在一个框架中实现这个的时候，很多时候， 编程框架会自动处理好反向传播 但是我认为对RNN中反向传播是如何工作的有一个粗略的理解也是很有用的 让我们来看看 你已经看到过了，对于一个前向传播， 在神经网络中会进行从左到右的激活值计算， 直到输出所有的预测值。 在反向传播中，你可能已经猜到了， 你最终会在 与正向传播相反的方向上 进行反向传播的计算。 让我们来进行正向传播的计算。 给定输入的序列x_1, x_2, x_3...x_tx 然后使用 x_1和 a_0 计算激活值，乘以这个值 然后加在一起，又使用 x_2 和 a_1 去计算 a_2 然后是 a_3......，直到 a_tx。 实际上计算 a_1, 同样需要参数。 W_a 和 b_a(我们用绿色符号表示)， 这些就是用来计算 a_1 的参数。 事实上，这些参数被用在每一个时间步上，即是说 这些参数被用来计算 a_2, a_3， 等等。直到最后一个时间步，所有的激活值取决于参数 W_a 和 b_a。 让我们继续完善这张图 现在，给定 a_1, 你的网络可以计算第一个预测值， y-hat_1, 然后是第二个时间步，y-hat_2, y-hat_3, 等等，包括 y-hat_ty。 我将继续用不同的颜色来表示参数。 因此，为了计算 y-hat， 你需要参数 W_y 和 b_y, 这两个参数在这个节点中，跟其他的一样。 所以同样用绿色的表示。 接下来，为了计算反向传播， 你需要一个损失函数。 我们先定义一个逐元素的损失， 该损失假设一个特定的词语出现在序列中。 这是一个人名， 则 y_t=1。 你的网络输出一个可能性，比如 0.1的可能性这个词是一个人名。  所以我要把它定义为标准的逻辑回归损失， 也称为交叉熵损失。 这跟我们之前遇到过的 二分类问题很相似。 所以这个损失表达的是 对一个单位置的预测，或者说是 对于一个单词单一时间步t的预测。 现在来定义整个序列的整体损失， L 定义为 t 从1到 T_x 或 T_y 的总和。 在这个例子中，T_x 等于 T_y 对于单一时间步来说。 然后，只需要这个没有 上标 T 的 L。这就是整个序列的损失。 因此，在一个计算图中， 为了计算损失， 给定了y-hat_1，你可以计算第一个时间步的损失， 给定了这个(y-hat_2)，可以计算第二个时间步的损失， 计算第三个时间步的损失， 等等......直到最后一个时间步的损失。 最后，为了计算全局损失， 我们使用该公式，将它们全部加起来计算最终的 L， 即每个时间步的损失总和。 所以这就是一个计算问题， 你在之前的很多例子中已经见过了反向传播， 所以你并不会感到惊讶，反向传播仅仅是要求做一个 反方向的计算或者说传递信息。 即这4个时间步的箭头， 最后是这么做。 这使得你能够计算合适数量的 参数的梯度， 然后使用梯度下降更新参数。 在这个反向传播的过程中， 最重要的消息或最重要的传递计算就是这个， 它从右到左， 这也是为了给这个算法起了这么个名字： 基于时间的反向传播。 这个算法名字的动机是：对于前向传播， 你从左到右扫描， 增加了时间索引 t， 对于反向传播，则是从右到左， 同样是根据时间。 这就给这个算法一个我认为特别酷的名字， 基于时间的反向传播(在时间步上传播）。  这句话真的会让你感觉你需要一台时间机来实现这个输出， 但是我认为这只是 该算法众多名字中最酷的一个。 我希望这能让你了解到RNN中的前向和后向是如何工作的。 现在，到目前为止，你只是看了RNN中的一个主要的案例， 这个案例的输入和输出的序列长度是相等的。 在下一个视频中， 我想要给你展示更广泛的RNN结构， 因此我将让你解决更广泛的应用集。 让我们继续。