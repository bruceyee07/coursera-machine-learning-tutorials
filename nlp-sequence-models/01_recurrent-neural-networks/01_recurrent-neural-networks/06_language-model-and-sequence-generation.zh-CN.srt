1
00:00:00,400 --> 00:00:02,910
语言建模(language modeling)<br />是自然语言处理(natural language processing)中

2
00:00:02,910 --> 00:00:06,080
最基础和最重要的任务之一

3
00:00:06,080 --> 00:00:09,640
同时也是RNN非常擅长的领域

4
00:00:09,640 --> 00:00:14,880
这一节你会学到如何用RNN构建一个语言模型

5
00:00:14,880 --> 00:00:18,700
其内容将把我们带到本周有趣的编程作业

6
00:00:18,700 --> 00:00:20,770
在作业中你将构建一个语言模型

7
00:00:20,770 --> 00:00:24,730
使其生成莎士比亚风格或其他风格的文字

8
00:00:24,730 --> 00:00:25,720
如何对数据进行运算

9
00:00:25,720 --> 00:00:28,150
那么什么是语言模型？

10
00:00:28,150 --> 00:00:30,340
假设你正在构建一个语音识别系统

11
00:00:30,340 --> 00:00:34,945
你听到了这个句子<br />The apple and pear salad was delicious

12
00:00:34,945 --> 00:00:37,395
我刚刚说的是什么？

13
00:00:37,395 --> 00:00:43,475
我说的是 apple and pair salad<br />还是 apple and pear salad？

14
00:00:45,282 --> 00:00:49,642
你也许会认为第二个句子的可能性大得多

15
00:00:49,642 --> 00:00:52,092
实际上，高质量语音识别系统能帮助你

16
00:00:52,092 --> 00:00:56,472
即使在这两个句子听起来一模一样的时候

17
00:00:58,080 --> 00:01:00,830
语音识别系统之所以能够挑选出第二个句子

18
00:01:00,830 --> 00:01:03,800
正是因为它使用了语言模型(language model)

19
00:01:03,800 --> 00:01:08,280
该模型给出这两个句子的概率

20
00:01:08,280 --> 00:01:12,438
例如，一个语言模型可能会说

21
00:01:12,438 --> 00:01:16,027
第一个句子的概率是3.2*10^(-13)

22
00:01:16,027 --> 00:01:21,800
第二个句子的概率是5.7*10^(-10)

23
00:01:21,800 --> 00:01:27,210
依照这两个概率，第二个句子的可能性要大得多

24
00:01:27,210 --> 00:01:32,140
有10^3倍的差距

25
00:01:32,140 --> 00:01:35,840
所以语音识别系统会选择第二个选项

26
00:01:36,980 --> 00:01:42,280
所以语言模型的功能在于给定任意句子

27
00:01:42,280 --> 00:01:46,710
它都会给出这个特定句子的概率

28
00:01:46,710 --> 00:01:51,481
句子的概率的意思是，如果你随意拿起一份报纸

29
00:01:51,481 --> 00:01:54,148
或打开一份电邮或打开一个网页

30
00:01:54,148 --> 00:01:57,890
听到别人或朋友随意的话语

31
00:01:57,890 --> 00:02:02,377
即在整个时空中你听到的下一句特定的话是

32
00:02:02,377 --> 00:02:06,878
apple and pear salad 的概率

33
00:02:06,878 --> 00:02:11,351
这是语音识别和

34
00:02:11,351 --> 00:02:16,050
机器翻译的基础构成

35
00:02:16,050 --> 00:02:20,780
因为机器翻译系统需要过滤出合适的翻译

36
00:02:21,790 --> 00:02:27,547
所以语言模型的工作是，输入一个句子

37
00:02:27,547 --> 00:02:34,103
写成 y(1) y(2)直到y(Ty)

38
00:02:34,103 --> 00:02:37,280
语言模型将被作用于输出值y 使之表达成一个句子

39
00:02:37,280 --> 00:02:42,400
而不是作用于输入值x

40
00:02:42,400 --> 00:02:46,918
但是语言模型所做的就是估计

41
00:02:46,918 --> 00:02:48,962
特定单词序列的概率

42
00:02:53,401 --> 00:02:55,560
那么如何构建一个语言模型？

43
00:02:58,530 --> 00:03:01,530
用RNN构建一个语言模型

44
00:03:01,530 --> 00:03:06,990
你首先需要一个由一个大文集(corpus)构成的训练集

45
00:03:06,990 --> 00:03:10,710
可以是英语或任意你想构建语言模型的语言

46
00:03:10,710 --> 00:03:15,410
文集(corpus)这个词是NLP的术语表示

47
00:03:15,410 --> 00:03:19,330
大体量的或海量英语语句构成的英语文本(text)的集合

48
00:03:19,330 --> 00:03:23,040
假设你的训练集中有这样一句话

49
00:03:23,040 --> 00:03:25,360
Cats average 15 hours of sleep a day<br />（猫平均每天睡15个小时）

50
00:03:25,360 --> 00:03:29,970
你要做的第一件事是将这个句子标记化(tokenize)

51
00:03:29,970 --> 00:03:35,380
从而得到一个你在之前的视频中见过的单词表

52
00:03:35,380 --> 00:03:40,740
然后将各个单词映射到一个 一位有效矢量(one hot vector)

53
00:03:40,740 --> 00:03:44,200
将对应索引值的矢量元素值置一

54
00:03:44,200 --> 00:03:48,230
另外要做的就是标记句子的结束

55
00:03:48,230 --> 00:03:53,735
通常我们会加一个叫做EOS(End Of Sentence)的标记

56
00:03:53,735 --> 00:04:00,160
它将帮助你定位句子的结束

57
00:04:00,160 --> 00:04:01,330
在后面的课程中我们再讨论这个

58
00:04:01,330 --> 00:04:06,320
但是如果你想让你的模型明确捕获每个句子的结尾

59
00:04:06,320 --> 00:04:11,150
可以把EOS标记加在训练集中每个句子的后面

60
00:04:11,150 --> 00:04:15,685
本周的练习我们不会用到EOS

61
00:04:15,685 --> 00:04:19,823
但是某些应用场景中你可能会需要

62
00:04:19,823 --> 00:04:22,740
后面的课程中我们会看到使用它的时候

63
00:04:22,740 --> 00:04:27,970
这个例子中我们有y1 y2 y3 4 5 6 7 8 9

64
00:04:27,970 --> 00:04:33,240
加上EOS标记一共9个输入

65
00:04:33,240 --> 00:04:35,840
在标记化这一步骤中，你可以决定

66
00:04:35,840 --> 00:04:38,190
是否计入句号标记

67
00:04:38,190 --> 00:04:41,270
这个例子中，我们忽略标点

68
00:04:41,270 --> 00:04:43,938
所以我将 day 当作一个标记(token)

69
00:04:43,938 --> 00:04:48,861
并忽略句号，如果你想将句号或其他标点作为标记

70
00:04:48,861 --> 00:04:54,070
你可以将句号加入你的单词表

71
00:04:54,070 --> 00:04:58,240
现在另一个细节是，如果有些单词在你的训练集中

72
00:04:58,240 --> 00:04:59,850
但不在单词表中，要怎么处理

73
00:04:59,850 --> 00:05:04,717
如果你的单词表有10,000个单词

74
00:05:04,717 --> 00:05:09,032
也许是最常用的10,000个英语单词。那么Mau这个词<br />Egyptian Mau是一个猫的品种

75
00:05:09,032 --> 00:05:12,343
可能不在你的10,000个最常用的标记中

76
00:05:12,343 --> 00:05:16,773
这种情况下你可以把Mau用一个全局唯一的标记代替

77
00:05:16,773 --> 00:05:21,128
叫做UNK 表示未知单词

78
00:05:21,128 --> 00:05:25,366
并对UNK的概率建模，而不是对某个特定未知词建模

79
00:05:25,366 --> 00:05:30,260
完成标记化的步骤基本上就是

80
00:05:30,260 --> 00:05:33,500
将输入句子映射成各个标记

81
00:05:33,500 --> 00:05:36,390
或单词表中单词的集合

82
00:05:36,390 --> 00:05:41,606
下面我们构建RNN来为这些不同序列的概率建模

83
00:05:41,606 --> 00:05:46,702
下一页ppt将体现的一点是

84
00:05:46,702 --> 00:05:53,470
最终你将输入<br />x(<t>)置为y(<t-1>)

85
00:05:53,470 --> 00:05:56,388
那么我们来构建这个RNN模型

86
00:05:56,388 --> 00:06:01,263
我将继续用这个句子作为样例

87
00:06:01,263 --> 00:06:03,984
这是RNN架构

88
00:06:03,984 --> 00:06:08,528
在时间0 你要计算输入x1的某个激活函数值

89
00:06:08,528 --> 00:06:13,527
在时间0nbsp你要计算输入x1的某个激活函数值

90
00:06:13,527 --> 00:06:19,338
x1将被置为全0即0矢量

91
00:06:19,338 --> 00:06:25,685
前面的a0按照惯例，也置为全0矢量

92
00:06:25,685 --> 00:06:30,792
a(1)用softmax预测

93
00:06:30,792 --> 00:06:36,574
尝试得到第一个单词y的概率

94
00:06:36,574 --> 00:06:40,810
即y(1)

95
00:06:40,810 --> 00:06:45,189
这一步实际上就是用softmax尝试预测

96
00:06:45,189 --> 00:06:49,492
单词表中每个单词的概率

97
00:06:49,492 --> 00:06:53,596
第一个是a，这个词是Aaron的概率?

98
00:06:53,596 --> 00:06:58,841
是cats的概率?

99
00:06:58,841 --> 00:07:01,697
直到最后一个单词Zulu的概率

100
00:07:01,697 --> 00:07:04,985
或者它是UNK的概率是多少？

101
00:07:04,985 --> 00:07:09,461
或者说这个句子第一个单词

102
00:07:09,461 --> 00:07:10,858
的概率分布是怎样的？

103
00:07:10,858 --> 00:07:15,482
所以y^1输出到一个softmax，它给出

104
00:07:15,482 --> 00:07:19,875
第一个单词的概率的预测，不论它最后是哪个单词

105
00:07:19,875 --> 00:07:24,882
我们的例子中，这个单词是cats

106
00:07:24,882 --> 00:07:29,120
所以如果单词表有10,000个单词<br />这将是一个10,000路softmax输出

107
00:07:29,120 --> 00:07:33,079
或者10,002 我想UNK和EOS也算在内

108
00:07:33,079 --> 00:07:35,893
作为额外两个标记

109
00:07:35,893 --> 00:07:39,298
然后，RNN前向推进到下一步

110
00:07:39,298 --> 00:07:43,260
运行某个激活函数，得到a<1>输入给下一步

111
00:07:43,260 --> 00:07:47,670
在这一步，它试图算出第二个单词是什么

112
00:07:48,730 --> 00:07:54,480
同时我们也给它第一个单词正确项

113
00:07:54,480 --> 00:07:57,666
所以我们将告诉他，实际上

114
00:07:57,666 --> 00:08:01,304
第一个单词其实是Cats也就是y1

115
00:08:01,304 --> 00:08:06,540
告诉它cats，也就是为什么y1=x2

116
00:08:06,540 --> 00:08:14,860
第二步的输出仍然由softmax预测出来

117
00:08:14,860 --> 00:08:18,561
RNN要做的是预测某个单词的概率，无论这个单词是什么

118
00:08:18,561 --> 00:08:23,143
它可以是a或者Aaron或Cats或Zulu或UNK或EOS

119
00:08:23,143 --> 00:08:26,950
可以是任意之前在单词表中给出的单词

120
00:08:26,950 --> 00:08:27,700
所以 在这种情况下所以 在这种情况下

121
00:08:27,700 --> 00:08:33,120
我猜正确的答案是average，因为这句话的开头是cats average

122
00:08:33,120 --> 00:08:36,418
然后继续RNN的下一步

123
00:08:36,418 --> 00:08:39,913
现在要计算a3

124
00:08:39,913 --> 00:08:42,760
但是要预测第三个单词即“15”

125
00:08:42,760 --> 00:08:44,800
我们现在可以给它前面2个单词

126
00:08:44,800 --> 00:08:48,175
我们告诉它前2个单词是cats average

127
00:08:48,175 --> 00:08:54,741
那么下一个输入有x<3>=y<2><br />即单词average是输入

128
00:08:54,741 --> 00:08:59,839
它要做的是要给出这个序列的下一个单词

129
00:08:59,839 --> 00:09:04,030
换句话说要做的就是给定前面的单词
是cats average的条件下

130
00:09:04,030 --> 00:09:07,005
字典中任意单词作为下一个单词的概率

131
00:09:08,783 --> 00:09:10,142
是什么，对吗？

132
00:09:10,142 --> 00:09:13,159
这个例子中，正确答案是“15”，
然后继续下一个单词

133
00:09:14,403 --> 00:09:19,513
直到最后，我猜是在第9步

134
00:09:19,513 --> 00:09:25,061
你输入x(9)=y(8)

135
00:09:25,061 --> 00:09:31,501
即单词day

136
00:09:31,501 --> 00:09:37,200
则它得到a<9>，然后它要输出y^9

137
00:09:37,200 --> 00:09:40,690
也就正好是EOS

138
00:09:40,690 --> 00:09:45,621
在所有已知条件下，
即给出所有前面的单词

139
00:09:45,621 --> 00:09:49,694
希望它会预测出这些单词
EOS还有句子标记

140
00:09:49,694 --> 00:09:50,988
他们高概率的结果

141
00:09:50,988 --> 00:09:55,945
所以，RNN的每一步
都会看之前的几组单词，例如

142
00:09:55,945 --> 00:10:01,285
给出前3个单词，
第四个单词的分布概率是什么？

143
00:10:01,285 --> 00:10:06,193
即RNN学习从左至右每次预测一个单词

144
00:10:06,193 --> 00:10:10,995
接下来，为了训练这个神经网络，
我们要定义一个代价函数

145
00:10:10,995 --> 00:10:15,004
在任一给定时间t，如果真实值为yt

146
00:10:15,004 --> 00:10:18,741
神经网络softmax预测值为y^t

147
00:10:18,741 --> 00:10:24,963
它就是softmax的代价函数，
你应该已经熟悉这个函数了

148
00:10:24,963 --> 00:10:29,298
整体损失就是所有时间步骤单个预测的

149
00:10:29,298 --> 00:10:32,640
损失总和

150
00:10:32,640 --> 00:10:35,520
如果你用上一个训练集训练RNN

151
00:10:35,520 --> 00:10:42,010
你能做的是，给定任意初始单词集<br />如cats average 15 hours of

152
00:10:42,010 --> 00:10:47,120
它能预测下一个单词的概率分布

153
00:10:48,280 --> 00:10:52,150
给定一个新句子 y(1) y(2) y(3)

154
00:10:52,150 --> 00:10:56,020
简单一些的话，只有这三个单词

155
00:10:56,020 --> 00:11:01,883
你能给出整个句子的概率分布的方法是

156
00:11:01,883 --> 00:11:06,200
第一个softmax告诉你y(1)的概率分布

157
00:11:06,200 --> 00:11:08,230
这是第一个输出

158
00:11:08,230 --> 00:11:15,329
第二个softmax告诉你
给定y(1)的条件下y(2)的概率分布

159
00:11:15,329 --> 00:11:23,895
第三个softmax告诉你
给定y(1)y(2)的条件下y(3)的概率分布

160
00:11:23,895 --> 00:11:27,786
然后将这三个概率相乘

161
00:11:27,786 --> 00:11:31,450
你应该已经在之前的练习中
了解到更多细节了

162
00:11:31,450 --> 00:11:36,930
通过这3个的乘积最终得到这

163
00:11:36,930 --> 00:11:39,280
三个单词组成三个句子的概率

164
00:11:39,280 --> 00:11:45,230
这是如何使用
RNN训练语言模型的基本结构

165
00:11:45,230 --> 00:11:49,120
如果这些理论看上去有些抽象，
不必担心

166
00:11:49,120 --> 00:11:52,606
你将会在编程练习中运用这些概念

167
00:11:52,606 --> 00:11:56,490
下一节将会是语言模型运用中
最有趣的部分

168
00:11:56,490 --> 00:11:59,150
即对模型进行单词序列的抽样<br />(sample sequences from the model)

169
00:11:59,150 --> 00:12:00,880
我们就在下一个视频中看看它是怎么工作的吧
GTC字幕组翻译